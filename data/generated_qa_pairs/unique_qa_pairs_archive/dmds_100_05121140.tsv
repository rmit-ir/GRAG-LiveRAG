qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	I'm doing a project about communication technology. Could you explain how electronic systems ensure reliable data transmission, particularly in space missions like Cassini?	Electronic systems use multiple mechanisms to ensure reliable data transmission. In the Cassini RADAR system, the Radio Frequency Electronics Subsystem (RFES) handles transmission and reception of signals, while maintaining signal integrity through various components. It uses an ultra-stable oscillator as the system timing source, and employs both digital chirp generation and signal conversion processes. The signals are protected from interference using a Faraday cage (electrostatic shield). For communication interface design, systems like RS-232 use voltage level conversion with regulated positive and negative voltages to ensure reliable signal transmission. This is achieved through charge-pump circuits that generate the required voltage levels, with feedback loops monitoring and regulating the output voltages to maintain signal integrity.	"['The RS-232 interface is popular in single-ended low-speed serial communications. The interface was originally designed to connect the serial port of a computer to peripheral devices. The Electronics Industry Alliance (EIA)-232 levels are defined as up to ±15V into a 5kΩ load. Thus, RS-232 transceivers need three supplies from the system. Due to the limited on-board supply, single-supply RS-232 integrated circuits (ICs) have been available since the 1980s. These transceivers integrate charge-pump circuitries to generate the required positive and negative voltage levels internally on the IC. The charge pumps need external capacitors to store and transfer the energy. In this post, I’ll explain how the charge-pump circuitry works and how the output voltage is regulated based on it.\nHow charge-pump circuitry works\nFigure 1 is a diagram of the charge-pump circuitry. An internal oscillator generates a clock signal, which is used to control the switches (SW1, SW2, SW3 and SW4).\nFigure 1: Charge-pump circuitry diagram\nFor illustration purposes, let’s focus on the V+ voltage first. The mechanism of generating the V- voltage is similar. With the internal clock, the voltage will charge on C1 and C3 alternatively (in some literature C1 is called flying capacitor and C3 is called storage capacitor). In the first cycle, C1 charges to VCC (Figure 2).\nFigure 2: The diagram of generating V+ voltage (cycle one)\nIn the next cycle, one end of C1 connects to VCC and the other end to V+ (Figure 3). Since the voltage difference on C1 cannot be changed instantly, V+ becomes two times VCC, with one from the SW1 and the other from the existing charge of C1. Here you assume no loss in the circuitry.\nFigure 3: The diagram of generating V+ voltage (cycle two)\nIn the same manner, V- is generated in cycles (refer back to Figure 1). In one cycle, C2 charges to V+. In the next cycle, the charge dumps to V- with the polarity reversed. Therefore, V- maintains the same amount of V+ voltage level but below ground.\nHow regulation works\nWith charge-pump circuitry, V+ becomes 2Vcc and V- becomes -2Vcc. As long as the clock keeps running, the charging keeps happening. To make the device have better efficiency and produce a better regulated output voltage, a regulator circuitry monitors the output voltage with a feedback loop to control the charge pump (Figure 4).\nFigure 4: The regulator circuitry diagram\nIf the V+ voltage is higher than a pre-set positive threshold, or V- is lower than a negative threshold, the charge pump stops working. As one of the voltages drops beyond its threshold, the charge pump starts pumping again. Due to this regulation, some ripples are shown on the V+ voltage (Figure 5).\nFigure 5: V+ waveform\nLet’s analyze the amplitude of the ripples. Assume the ends of C3 connect to V+ and Vcc (Figure 2). Look at the moment just before time t1 (Figure 4), where the V+ voltage is higher than Vr (threshold) and the charge pump is not yet turned on. V+ droops down by discharging C3. At t1, the voltage decreases to Vr; therefore V+ is equal to Vr. The charge pump starts working and transfers the charge stored on C1 to C3. Without any loss, the charge should be equal in two states. Before the charge transfer, the voltage on C3 is Vr-Vcc and Vcc on C1. After the transfer, both C1 and C3 share the same voltage potential Vp-Vcc. The total charge before and after the transfer is calculated in Equation 1.\nThus, Vp is calculated by Equation 2 with all known values:\nAssuming Vr = 5.5V, Vcc = 3.3V and C1 = C3, Vp is 6.05V.\nI have several comments about these equations. First, if you assume C1 = C3, the charge can be considered distributed equally between C1 and C3. Therefore. the peak voltage Vp is halfway of Vr (5.5V) and 2Vcc (6.6V). Also, from the equations you can see that from the ripple’s point of view, what matters is not the absolute values of C1 and C3 but the ratio of C1 to C3. In some applications, C3 connects to GND instead of Vcc. In this case, charge Equation 1 becomes Vr*C3+Vcc*C1 = (Vp-Vcc)*C1+V+*C3, resulting in the same Vp.\nSimilarly, you can calculate the ripple voltage of the V- pin. Equation 3 calculates the peak voltage of V- as:\nTo make the variables consistent, Vr and Vp are the threshold voltage and peak voltage of V+ used in Equations 1 and 2. Replacing them with the values from the previous calculation and again assuming C2 = C4 results in Equation 4:\nThe V- voltage shows less amplitude of ripple than V+, since the charge is filtered one more time by external capacitors.\nThe voltage on the capacitors goes above Vcc with regulation and could become 2Vcc without regulation. Therefore, to play it safe, choose MLCC (X5r or X7r) capacitors with a voltage rating of at least 10V or even 16V.\nThe impact of variable capacitor values\nNow that you understand how the charge pump and regulator circuitries work, you can play with the external capacitor value to see the impact on the output waveform. The rule of thumb is that if C3 is bigger than C1, then less charge will be dumped to C3. Therefore, less ripples will show in the V+ waveform.\nFigures 6 through 8 are some V+ waveforms of various capacitor values from bench testing.\nFigure 6: V+ waveform, C1 = 0.47µF, C3 = 0.1µF\nFigure 7: V+ waveform, C1 = 0.1µF, C3 = 0.1µF\nFigure 8: V+ waveform, C1 = 0.01µF, C3 = 0.1µF\nIn these room-temperature tests of TI’s TRS3232E RS-232 line transceivers, the supply is 3.3V. The C3 value remains 0.1µF and C1 value varies from 0.01µF to 0.47µF. Figure 6 shows the highest ripples, with C1 = 0.47µF, and Figure 8 shows the lowest ripples, with C1 = 0.01µF. Be aware that the regulation cycles vary with the amplitude of the ripple as well. Assume that the discharge happens at the same rate on C3. Higher ripples take longer to discharge (44µs in Figure 6), while low ripples have a higher frequency (36µs in Figure 7). The cycle of Figure 8 is mainly governed by the charging time rather than the discharging time. In an additional testing, Figure 9 shows the case when C1 and C3 have the same ratio but with a different absolute value 0.47µF. Comparing Figure 9 with Figure 7, the amplitude of the ripples is close and the frequency is higher (20µs) due to the faster charging and discharging capability of larger capacitors.\nFigure 9: V+ waveform, C1 = 0.47µF, C3 = 0.47µF\nPlease be aware in the testing above, the device was running with no load. If a current load is added such as a resistor (RL in Figure 4), the calculation would be different and in general the output voltage would be lower.\n- Read the application report, Design for a Discrete Charge Pump.', 'Team Lead: Dr. Charles Elachi\nRADAR General Description:\nThe Cassini Radar (RADAR) uses the five-beam Ku-band antenna feed assembly associated with the spacecraft high gain antenna to direct radar transmissions toward targets, and to capture blackbody radiation and reflected radar signals from targets.\nRADAR Scientific Objectives:\n- To determine whether oceans exist on Titan, and, if so, to determine their distribution.\n- To investigate the geologic features and topography of the solid surface of Titan.\n- To acquire data on non-Titan targets (rings, icy satellites) as conditions permit.\nRADAR Sensing Instruments:\n- Synthetic Aperture Radar Imager [SAR] (13.78 GHz Ku-band; 0.35 to 1.7 km resolution)\n- Altimeter (13.78 GHz Ku-band; 24 to 27 km horizontal, 90 to 150 m vertical resolution)\n- Radiometer (13.78 GHz passive Ku-band; 7 to 310 km resolution)\nRADAR Instrument Characteristics:\n- Mass (current best estimate) = 41.43 kg\n- Peak Operating Power (current best estimate) = 108.40 W\n- Peak Data Rate (current best estimate) = 364.800 kilobits/sec\nThe Cassini Radar (RADAR) will be used to investigate the surface of Saturn\'s moon Titan by taking four types of observations: imaging, altimetry, backscatter, and radiometry. In the imaging mode of operation, the RADAR instrument will bounce pulses of microwave energy off the surface of Titan from different incidence angles and record the time it takes the pulses to return to the spacecraft. These measurements, when converted to distances (by dividing by the speed of light), will allow the construction of visual images of the target surface. Radar will be used to image Titan because the moon\'s surface is hidden from optical view by a thick, cloud-infested atmosphere: radar can ""see"" through such cloud cover.\nRadar altimetry similarly involves bouncing microwave pulses off the surface of the target body and measuring the time it takes the ""echo"" to return to the spacecraft. In this case, however, the goal will not be to create visual images but rather to obtain numerical data on the precise altitude of the surface features of Titan. In the backscatter mode of operation, the RADAR will act as a scatterometer. That is, it will bounce pulses off Titan\'s surface and then measure the intensity of the energy returning. This returning energy or backscatter, is always less than the original pulse, because surface features inevitably reflect the pulse in more than one direction. From the backscatter measurements, scientists can infer the composition of the surface of Titan.\nFinally, in the radiometry mode, the RADAR will operate as a passive instrument, simply recording the energy emanating from the surface of Titan. This information will tell scientists the amount of latent heat (i.e.. moisture) in the moon\'s atmosphere, a factor that has an impact on the precision of the other measurements taken by the instrument. During imaging, altimetry, and backscatter operations, the RADAR instrument will transmit linear frequency-modulated Ku-band pulsed signals toward the surface of Titan using the high-gain antenna (HGA). These signals, after reflection from the surface, will be captured by the same antenna and detected by the RADAR Radio Frequency Electronics Subsystem. During radiometry operations, the instrument will not transmit any radar signals, but the HGA will again be used for radiometric observations.\nTo improve the surface coverage by radar imaging, a switched, multiple Ku-band antenna feed array structure is part of the HGA and permits the formation of five antenna beam patterns. Each of these beams will have a different pointing angle relative to the antenna reflector\'s focal axis.\nThe major functional components of the RADAR Subsystem are:\nFor information on these components, click on their names.\nThe Radio Frequency Electronics Subsystem (RFES) has three principal functions: the transmission of high-power frequency-modulated and unmodulated pulses, the reception of both reflected energy from the target and passive radiometric data, and the routing of calibration signals. The RFES has a fully enclosed structural housing and Faraday cage (i.e., an electrostatic shield). The RFES electronics units are individually enclosed and are mounted to the RFES housing wall opposite the wall that mounts to the spacecraft. For thermal control, heat flows conductively from the units to the housing wall and is then radiated away from the RFES.\nThe RFES consists of the following components:\nThe frequency generator (FG) contains an ultra-stable oscillator that is the system timing source for the RADAR instrument.\nThe digital chirp generator (DCG) generates the low-power, baseband frequency, modulated pulse upon request from the RADAR Digital Subsystem. Both the bandwidth and the pulse width of this pulse can be varied in accordance with the parameters received from the Digital Subsystem.\nThe chirp up-converter and amplifier (CUCA) converts the baseband chirp pulse to Ku band and provides the up-converted pulse to the high-power amplifier.\nThe high-power amplifier (HPA) receives a low-power Ku-band chirp pulse from the CUCA and amplifies that pulse to the required power level for transmission.\nThe purpose of the front-end electronics (FEE) is to route the high-power transmission pulses, the returning low-energy echoes and radiometric signals, and the calibration signals. The FEE receives the high-power pulse from the HPA and routes the signal to one of five different antenna ports on the RFES via an antenna switch module. The echo returns and radiometric signals are routed from one of the five antenna ports to the RFES microwave receiver. The FEE also steers the selected calibration signal to the microwave receiver during periods of calibration mode operation.\nThe microwave receiver (MR) receives signals at Ku band and down-converts these to baseband so that they can be properly sampled. The sources of these signals are the echo returns, radiometric signals, and calibration signals routed through the FEE. The MR receives the re-routed chirp calibration signal from the CUCA and passes that signal to the FEE for proper routing. The MR is also the source of the noise diode calibration signal that is provided to the FEE for routing. MR gain and bandwidth information is provided to the MR from the DSS.\nThe RFES power supply converts the (approximately) 30-volt d.c. input from the Power and Pyrotechnic Subsystem to the required voltages for the RFES.\nThe RADAR Digital Subsystem (DSS) performs three principal functions: reception and depacketization of RADAR commands from the Command and Data Subsystem (CDS), configuration control and timing signal generation for RADAR, and the packetization of RADAR housekeeping (i.e., hardware status) data and science data for transfer to the CDS.\nDSS subassemblies are contained within a spacecraft bay and will be supported in shear by shear plates and the top and bottom rings of the Cassini spacecraft bus. Electronic harnesses, which face inboard on the spacecraft and be supported by the inboard shear plate, are used to provide interconnections between the RADAR subassemblies and the spacecraft.\nThe DSS uses two primary modes of heat transfer in its design. These are (1) the conduction of heat from the electronic components to the subchassis and the outboard shear plate, and (2) the radiation of heat from the outboard shear plate to the space environment. High-power heat dissipation components are mounted on a special heatsink bracket, which is bolted directly to the outboard shear plate to optimize heat transfer. Thermal compounds were applied between the components and the heatsink to minimize contact thermal resistance.\nThe DSS consists of the following components:\nFor information on these components, click on their names.\nThe bus interface unit (BUI) is the interface between RADAR and the CDS. On the RADAR side, the BIU interfaces to the flight computer unit for command, software, and data transfers.\nThe flight computer unit (FCU) receives commands and software from the CDS and sends data and status to CDS by way of the BIU. It depacketizes the commands and provides the RADAR configuration and timing information to the control and timing unit. It also receives housekeeping values in a predetermined order from the low-speed A/D converter and packetizes the housekeeping and science data to be passed to the CDS by way of the BIU. In addition, the FCU receives spacecraft time broadcasts and RADAR software uploads from CDS by way of the BIU. The FCU is built around an engineering flight computer (EFC) with additional banks of ROM and internal interface circuitry.\nThe purpose of the control and timing unit (CTU) is to control the hardware configuration and the timing of control signals within RADAR. The parameters for determining RADAR configuration and timing are passed to the CTU from the FCU. The CTU provides the configuration and timing control signals to the RFES and to other portions of the DSS. In addition, the CTU is responsible for updating to millisecond resolution the spacecraft time received from the CDS.\nThe signal conditioner unit (SCU) consists of a science data buffer and high- and low-speed analog-to-digital (A/D) converters. The science data buffer (SDB) is the digital data rate buffer for RADAR. The sole purpose of the SDB is to receive and store the high-rate digital science data from the high-speed A/D converter during the proper receive window period (as determined by the CTU) and then to provide this data upon request to the FCU at a slower rate. The high-speed A/D converter digitizes the imaging data output from the RFES microwave receiver and provides the data to the SDB for buffering. The low-speed A/D converter performs two tasks. It digitizes the analog housekeeping telemetry values from throughout RADAR at predetermined times and provides these digitized values to the FCU upon request. It also digitizes the radiometer output from the RFES microwave receiver and provides those values to the FCU upon request.\nThe DSS power supply converts the (approximately) 30-volt d.c. input from the Power and Pyrotechnic Subsystem to the voltages required for the DSS.\nThe RADAR Energy Storage Subsystem (ESS) converts the (approximately) 30-volt d.c. input from the PPS to a higher voltage, stores energy in a capacitor bank, and provides a regulated voltage to the high-power amplifier (HPA) of the RFES. The ESS subassemblies are contained within a spacecraft bay and are supported in shear by shear plates and the top and bottom rings of the Cassini spacecraft bus. High-strength fasteners will be used to tie the electronics assemblies to the spacecraft. Electronic harnesses, which face inboard on the spacecraft and be supported by the inboard shear plate, are used to provide interconnections between the RADAR subassemblies and the spacecraft.\nThe ESS uses two primary modes of heat transfer in its design. These are (1) the conduction of heat from the electronic components to the subchassis and the outboard shear plate, and (2) the radiation of heat from the outboard shear plate to the space environment. High-power heat dissipation components will be mounted on a special heatsink bracket, which will be bolted directly to the outboard shear plate to optimize heat transfer. Thermal compounds will be applied between the components and the heatsink to minimize contact thermal resistance.\nThe ESS consists of:\nFor information on these components, click on their names.\nThe boost circuitry increases the (approximately) 30-volt d.c. input power to approximately 85 volts d.c. for more efficient energy storage by the capacitor bank. Soft-start circuitry limits the current draw from the power source, and an input voltage filter prevents electromagnetic interference (EMI) from being conducted back into the source.\nThe capacitor bank stores energy to supply to the buck regulator (and the HPA) during RADAR pulse bursts. The capacitor bank voltage drops during each burst but returns to normal before the next burst.\nThe buck regulator regulates the varying capacitor bank voltage for the HPA.']"	['<urn:uuid:70d576e0-e6d8-42f5-8c0b-b89d0274bb6b>', '<urn:uuid:0f27616d-d94e-493f-a127-beed88703822>']	open-ended	with-premise	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-12T11:40:26.317848	23	113	3075
2	Do TV mounts and whiteboards have similar placement restrictions?	Yes, both have placement restrictions. Whiteboards should not be installed near heat sources or in direct sunlight, while TV mounts require careful placement consideration for optimal viewing angles, avoiding glare from windows, and ensuring the wall can support the weight.	['The question “how to hang a whiteboard” is not among the rarely asked ones. Now it’s quite difficult to imagine an office without a whiteboard. Many offices and companies hold regular seminars and presentations, so the board is simply an indispensable assistant in their arsenal. This is not only a suitable surface for drawing diagrams and graphs but also a convenient object for fixing various visual materials with a magnet.\nThe whiteboard can be used also in your everyday life at home. They are popular among office workers, teachers of educational institutions, and also can become the irreplaceable assistant in your regular doings.\nHow To Put A Whiteboard On The Wall\nBefore the whiteboard installation, lay it down at room temperature in a horizontal position at a flat surface for at least 8 hours. The deformation of the plane of the board is allowed (after installation, it will straighten after some time).\nRead the following instructions to be aware of how to mount whiteboard:\nCheck the list\nMake sure that the list of complication is available and corresponding. All the items must be present.\nPick up the place for mounting\nIt is not recommended to install the whiteboard near heat sources and in places exposed to direct sunlight.\nGet the installation place ready\nPrepare a place around the installation point. Remove the board and its accessories from the package. Remove carefully to avoid the damage.\nMake a markup\nMark the bottom and side borders of the board on the wall. Make a mark of the holes location on the bottom border to mount the brackets. Use the building ruler if necessary.\nDrill the holes in the wall and fix the brackets with screws and dowels, mounting through a round hole of the bracket. For the large whiteboards it is usually installed an additional bracket in the middle of the board.\nMount the bottom and the upper parts\nInstall the board on the bottom brackets and mark the upper limit for further installation. Don’t forget to hold the board while marking the upper border to avoid markup errors. Mark the the location of the holes on the upper border to mount the brackets. Drill the holes in the wall and fix the brackets with screws and dowels. Mount the board through the oval hole of the bracket, don’t fully tighten the screw. After mounting, the bracket should move with force within the oval hole. For the large boards, it is usually installed an additional bracket in the middle of the board.\nSet up the board\nInstall the board on the lower brackets, pressing it against the wall, lower the upper brackets, and fix the board to be installed. Ensure the reliability of the construction. Hold the board during installation.\nMount the shelf\nPlace the shelf on the lower end of the board by pressing until it clicks.\nRemove the protective film from the working surface of the board\nBefore removing it is recommended to cut the film around the perimeter with a stationery knife.\nThis is how to put a whiteboard on the wall by yourself.', 'A full motion TV mount is one of the best ways to enhance your television viewing experience. It allows you to tilt, swivel, and extend your TV, giving you the flexibility to find the perfect viewing angle. However, installing a full motion TV mount on a metal wall can be quite challenging. In this article, we’ll show you the step-by-step process of how to install a full motion TV mount on a metal wall, from measuring the wall to choosing the right screws and anchors for a secure installation.\nThe benefits of using a full motion TV mount\nBefore we dive into the installation process, let’s first discuss the benefits of using a full motion TV mount. As mentioned earlier, a full motion TV mount allows you to adjust the viewing angle. This flexibility means you can have a comfortable viewing experience no matter where you’re seated in the room. Additionally, a full motion TV mount can help reduce glare on the screen, which can help improve the picture quality. It also creates a sleek and modern look in your room, effectively saving space.\nAnother benefit of using a full motion TV mount is that it can help prevent neck and eye strain. With the ability to adjust the viewing angle, you can position the TV at a height and angle that is comfortable for your eyes and neck. This is especially important if you spend a lot of time watching TV or using your TV as a computer monitor. Additionally, a full motion TV mount can be a great option for families with children or pets, as it allows you to easily adjust the TV to a safe and secure position.\nTools required for the installation process\nNow that we have an idea of what a full motion TV mount is and its benefits, let’s discuss the tools you’ll need for the installation process. Here are the tools you’ll need:\n- Electric Drill\n- Tape Measure\n- Metal Screws\n- Wall Anchors\nIt’s important to note that the specific tools you’ll need may vary depending on the type of wall you’ll be mounting the TV on. For example, if you’re mounting the TV on a concrete wall, you may need a hammer drill and masonry bit in addition to the tools listed above.\nAdditionally, it’s a good idea to have a second person assist you during the installation process, especially if you’re mounting a larger TV. This can help ensure that the mount is level and secure, and can also make the process easier and safer overall.\nMeasuring the wall to determine the ideal mounting location\nThe first step in installing a full motion TV mount on a metal wall is to measure the wall for the ideal mounting location. You’ll want to find the studs in the wall first to ensure a secure installation. If you have a stud finder, use it, but if not, you can use a strong magnet to find the studs in the metal wall. Once you find them, mark the location with a pencil. After identifying the stud, decide where you want your TV to be mounted on the wall. Mark this spot on the wall with a pencil, and ensure it is level for optimal viewing.\nIt’s important to also consider the viewing angle when determining the ideal mounting location for your TV. You want to make sure that the TV is mounted at a height and angle that allows for comfortable viewing from your desired seating area. A good rule of thumb is to mount the TV at eye level when seated. Additionally, consider any potential glare or reflections from windows or lights in the room, and adjust the mounting location accordingly.\nPreparing the metal wall for mounting\nThe next step is to prepare the metal wall for mounting. This involves removing any paint or debris from the wall at the chosen location. Once the wall is clean, take the mounting bracket and position it over the mark you made earlier. Ensure that the bracket’s holes line up with the stud. Also, double-check that the bracket is level.\nAfter ensuring that the bracket is level, mark the holes on the wall with a pencil. Then, drill pilot holes into the wall at the marked locations. This will make it easier to screw in the mounting screws later on.\nOnce the pilot holes are drilled, attach the mounting bracket to the wall using screws and a screwdriver. Make sure that the screws are tight and secure. Finally, test the bracket by gently pulling on it to ensure that it is firmly attached to the wall.\nChoosing the right screws and anchors for a secure installation\nSelecting the correct screws and anchors is crucial when mounting a TV on a metal wall. The screws and anchors you choose should be able to support the weight of your TV. Therefore, you should select screws and anchors that are suitable for your specific TV’s size and weight. Using screws and anchors that are either too weak or too powerful can cause your TV to fall or damage the wall.\nWhen selecting screws and anchors, it is also important to consider the type of metal wall you are mounting your TV on. Different types of metal walls require different types of screws and anchors. For example, if you are mounting your TV on a steel stud, you will need to use self-tapping screws that are specifically designed for steel studs. On the other hand, if you are mounting your TV on a metal plate, you will need to use toggle bolts that can expand behind the plate to provide a secure hold.\nAnother factor to consider when choosing screws and anchors is the location of the installation. If you are mounting your TV in an area that is prone to earthquakes or high winds, you will need to use screws and anchors that are designed to withstand these conditions. Additionally, if you are mounting your TV in a high-traffic area, you may want to consider using screws and anchors that have a tamper-resistant design to prevent theft or vandalism.\nAttaching the mounting bracket to the metal wall\nNow that you have the right screws and anchors, attach the mounting bracket to the metal wall. Insert the screws into the holes in the bracket and tighten them until they are secure. Make sure that the bracket is level and flush against the wall.\nIt is important to note that if the metal wall is particularly thick or hard, you may need to use a drill to create pilot holes before inserting the screws. This will make the process easier and prevent the screws from bending or breaking. Additionally, if the bracket is large or heavy, it may be necessary to use multiple screws and anchors to ensure that it is securely attached to the wall.\nHow to mount the TV onto the bracket safely and securely\nWith the bracket secure, it’s time to install the TV onto the bracket. Make sure that the TV’s mounting holes match up with the holes on the bracket. Align them carefully and screw them into place. Ensure the TV is level and hangs straight. Once secured, test the strength of the mount before releasing it completely.\nIt’s important to note that the weight of the TV should be taken into consideration when choosing a bracket. Make sure the bracket is rated to hold the weight of your TV. If the bracket is not strong enough, it could result in the TV falling and causing damage or injury.\nAdditionally, it’s a good idea to have someone assist you when mounting the TV onto the bracket. This will make the process easier and safer. One person can hold the TV while the other person screws it into place. This will also help ensure that the TV is level and straight.\nTesting and adjusting the TV mount for optimal viewing angles\nNow that the TV is mounted securely, you can test the full motion mount’s capabilities by adjusting it to your ideal viewing angle. Tilt, swivel, and extend the TV as needed to get the optimal viewing angles.\nIt is important to note that the optimal viewing angle may vary depending on the room’s lighting and seating arrangement. Take some time to experiment with different angles to find the one that works best for you.\nAdditionally, if you have multiple people watching the TV from different angles, consider investing in a mount that allows for multi-angle viewing. This will ensure that everyone in the room has a comfortable viewing experience.\nTips for concealing cables and cords after installation\nThe last thing you want after installing your TV is to have unsightly cords and cables hanging down from it. To avoid this, consider using cable sleeves or conduits to conceal the cords and provide a more organized look. Alternatively, you may want to consider hiring an electrician to install power outlets behind the TV to hide the cords.\nCommon mistakes to avoid during installation\nAvoid common mistakes during installation by ensuring that you have the right tools, screws and anchors, and that the TV is level. Check the capabilities of your bracket and TV before ensuring that your chosen screw and wall anchors are sufficient. Additionally, don’t tilt the TV too far, which could cause it to become unbalanced and fall.\nTroubleshooting tips in case of mounting issues\nIn some cases, certain complications may arise during installation. Here are some troubleshooting tips to follow to fix mounting issues:\n- If your TV is not level, adjust it accordingly.\n- If the bracket is crooked, remove it from the wall, and remount it after determining the correct mounting location.\n- If the screws do not seem to be going into the wall material securely, consider using larger ones or different anchors.\nFrequently asked questions about full motion TV mounts on metal walls\nHere are some frequently asked questions.\n- Q: Can full motion TV mounts be placed on any type of wall?\n- A: No, full motion TV mounts should be installed on walls strong enough to support the weight and with mounting spaces well-positioned.\n- Q: Will a full motion TV mount damage the wall?\n- A: A correctly installed full motion TV mount should not cause any damage to the wall. By following the installation guidelines and choosing the correct screws and anchors, you can avoid damage efficiently.\nConclusion: Enjoy your newly installed full motion TV mount on a metal wall!\nInstalling a full motion TV mount on a metal wall can be challenging, but by following the steps mentioned above, the process should be more comfortable. Remember that selecting the appropriate tools, screws, and anchors are essential. Doing so ensures a secure installation that can withstand the weight of the TV. Making sure the TV is level before releasing it completely is vital to avoid mishaps. Lastly, don’t forget to conceal any visible cables or cords for a cleaner look. Complete installation of your full motion TV mount and sit back and enjoy your new viewing experience from your couch or bed on your beautifully mounted TV.']	['<urn:uuid:ba8c9b1f-55bd-42f8-9782-dfd8508bff32>', '<urn:uuid:f7b365c7-925f-4e7f-87b9-2059c4a66309>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T11:40:26.317848	9	40	2379
3	As an energy infrastructure analyst, I'm curious about how energy storage facilities can benefit the power grid while also facing distribution equity challenges. What are the proven advantages of large-scale battery storage, and what socioeconomic disparities need to be addressed in energy infrastructure deployment?	Large-scale battery storage has demonstrated significant benefits, as shown by the Leighton Buzzard facility which can power 6,000 homes for 1.5 hours at peak times and has supported the National Grid for over 7,500 hours. The facility has proven both technically and commercially viable, helping to address network challenges and support the transition to a low-carbon economy. However, similar to EV charging infrastructure deployment patterns, there are concerning equity issues in energy infrastructure distribution. Data shows that higher-income areas and communities with higher educational attainment levels are significantly more likely to receive new energy infrastructure. For instance, areas with above-median household incomes ($68,703) are 2.24 times more likely to have access to such infrastructure, and areas with higher education levels are 3.55 times more likely to receive these installations. These disparities need to be addressed through more equitable planning and deployment strategies to ensure all communities benefit from emerging energy technologies.	"[""Britain’s largest grid-scale battery ‘could transform the energy grid’\nAn £18.4m grid-scale battery system in Bedfordshire has proved the technical and commercial viability of energy storage in Britain following an extensive two-year trial, according to the facility's operator.\nDistribution network operator UK Power Networks has claimed that the fully-automated 6MW/10MWh 6MW ‘big battery’, based in Leighton Buzzard, can potentially transform the energy grid and play a major role in the transition towards a low-carbon economy.\nThe Smarter Network Storage (SNS) facility – reportedly the only one of its kind currently operating on the energy network – can store enough electricity to power 6,000 homes for 1.5 hours at peak times.\nThe facility has supported National Grid for more than 7,500 hours and fed the local electricity network on more than 180 occasions, standing ready to provide power in the event of a national or local problem on the network. The project concluded grid-scale energy storage could be commercially viable as battery costs continue to fall and revenue streams become accessible.\nUK Power Networks director of safety, strategy and support services Suleman Alli said: “As we move towards a low-carbon, decentralized, digital energy system, all eyes are on the role of storage – especially batteries – in Britain’s electricity network. We believe that grid-scale storage has a huge role to play in addressing the challenges the industry faces.\n“The trial has drawn attention to the fact that the UK’s regulatory framework needs to evolve to help exploit its full potential. For example, energy storage currently incurs a double carbon levy – both when it stores energy and when it releases it.”\nSNS was awarded £13.2m of funding from the Low Carbon Networks Fund for the project. This was supplemented with £4m from UK Power Networks and £1.2m from project partners – a mix of businesses and academic institutions which are helping to deliver SNS.\nThe building itself is approximately 8,200sq.ft – about the size of three tennis courts – and is divided into two main rooms. One houses the transformers and inverter units that convert electricity from direct current to alternating current, while the other room houses the battery racks and modules where the energy is actually stored.\nUK Power Networks conducted two set of trials – one was successfully completed in March and these included looking at the storage value streams. In order to deliver greater value to customers, the team decided to undertake additional trials in the last six months from June to December 2016 to obtain further learning of the capability of SNS before the project concludes at the end of the month.\nUK Power Networks will continue to operate the battery in Leighton Buzzard, helping to meet the continuing demands for electricity in the Bedfordshire town.\nThe recently published Government smart energy consultation, which outlines the opportunity for businesses to actively balance their energy needs through storage, makes reference to the issues the trial highlighted.\nLast month, a Policy Exchange report found that energy storage, alongside other emerging technologies such as demand response, could help establish a smarter, more flexible energy system that creates savings for the UK in the tune of £8bn by 2030.\nThe UK has “multiple gigawatts” of energy storage capacity that is proposed or in the development pipeline. But according to the Renewable Energy Association (REA), this will fail to come to fruition without a joined-up and more supportive policy structure from Government.\nIn its last report before being disbanded, the Energy and Climate Change Committee (ECC) urged the Government to “move quickly” to address the regulatory barriers faced by energy storage.\n© Faversham House Ltd 2022 edie news articles may be copied or forwarded for individual use only. No other reproduction or distribution is permitted without prior written consent."", 'As electric vehicles (EVs) roll onto the roads in large volumes across the U.S, there has been a corresponding demand for more robust charging infrastructure. Despite the appealing environmental and economic benefits of EVs, the convenience of charging stations heavily influences adoption. And there begins the problem. While electric vehicle use is growing rapidly in well-to-do, predominantly White communities, minority neighborhoods have largely been left behind to date.\nThe U.S. government has emphasized the importance of equity when planning infrastructure investments in bills like the Build Back Better plan, and has incentivized a large portion of EV infrastructure funding in programs like NEVI and policies that seek to ensure EV charging infrastructure is deployed equitably.\nBut how well are these policies performing?\nTaking a granular approach to assess the current state of EV infrastructure\nIn an effort to understand the current state of equity in charger deployment, identify gaps that may exist, and add to the body of knowledge surrounding EV infrastructure deployment – we decided to leverage our vast data core, analytical expertise, and powerful software platform to conduct an analysis of Public EV Level 2 chargers in Columbus, Ohio.\nIn the sections below, we explore where access to EV chargers is most prevalent based on variables like population density, various socio-demographic statistics, and with different definitions of what constitutes a charging gap.\nEV Charging Infrastructure Basics\nFor EVs to achieve broad adoption and utilization, drivers need easy access to charging infrastructure. While many EV owners charge at home, people with longer commutes or irregular driving habits are unlikely to see themselves in EV ownership without excellent access to public charging stations.\nThere are three levels of charging equipment, determined by charging speed.\n- Level 1 (L1) – less than 2% of public EV chargers in the U.S. are L1.\n- Level 2 (L2) – the most common type of public EV chargers, accounting for more than 80% of public EV chargers in the U.S.\n- Level 3 (L3) – more than 15% of public EV chargers in the U.S. are L3.\nDue to the overwhelming preference for and majority of public charging stations being Level 2, we focused our analysis on L2 chargers.\nThe Landscape of EV Charging Infrastructure in Columbus\nAt first glance, you see that Columbus has Level 2 EV chargers spread across the city. A high concentration of chargers are located in the downtown area. So during our study, we took population density into consideration when drawing any conclusions regarding charger placement.\nRacial Factors and public EV Charger Locations\nCloser inspection of the distribution of public chargers in Columbus reveals disparities when comparing majority White areas to majority non-White areas. This is in line with other recent studies on public EV charging distribution. For example, Axios did an analysis of 35 U.S. cities and found that majority-White census tracts are 1.4 times as likely to have a charger.\nGiven UrbanFootprint’s unique ability to aggregate data across all census resolutions, we looked a level of granularity deeper, analyzing census block groups in Columbus using our Analyst application. It revealed that the EV charging locations in our study area are even more heavily skewed towards majority-White areas than what Axios had found in other cities around the country.\nIn Columbus, majority-White block groups are 2 times more likely to have a charger, and 2.3 times more likely to have at least three chargers.\nBut we knew there were likely other factors that may be more strongly correlated with the prevalence of charging stations than race and ethnicity.\nRelationship between EV Charger Presence and Educational Level\nOur study found an even stronger correlation between high education levels and the presence of EV chargers. It was immediately apparent from looking at the map that the median US educational attainment level (36% with at least a bachelor’s degree) was a tipping point for whether a block group would likely contain an L2 charger.\nTo be specific, around 80% of L2 chargers in our study were located in block groups with above the US median for bachelor’s degree attainment – and those block groups were 3.55 times as likely to contain a charger than those below the median.\nThe really shocking piece of this statistic is that these same block groups accounted for only 51% of population and only 40% of area.\nMedian Income and its Impact on EV Charger Distribution\nMedian income also appears to play a significant role in charger presence. Within our study area, block groups with chargers had median incomes 1.1 times higher than those without. Moreover, block groups with average incomes above the US median for household income ($68,703) were 2.24 times as likely to have a charger.\nThis finding supports the argument that historically, charger placement has favored higher income areas, and raises concerns for how access to EV infrastructure will lead to income-based disparities in future EV usage.\nEquitable Access to EV Chargers\nThe end goal of public EV charging infrastructure is to serve the public. That is, having enough chargers in a given block group to satisfy demand. We wanted to get an overall picture of who is being “served” versus “unserved” in Columbus. We chose 4 L2 chargers as a threshold for which to consider a block group “served.”\nWhen we looked at the data through this lens, all three ‘metrics’ (education level, median income, and racial composition) showed equity-related differences.\nConclusions and Recommendations\nOverall, our findings indicate that education level and median income are the most closely related metrics to the distribution of EV chargers in Columbus, Ohio. While racial factors are not quite as strong, there is still a trend for chargers to be more present in predominantly White block groups – in line with studies of other major cities across the country.\nAdditionally, the results were progressively more compelling as we peeled back all of the layers of data. For example, when limiting the analysis to the areas of the city with the highest population density, and increasing the threshold of what counts as a charging gap, the trends are magnified significantly.\nPolicymakers, local governments, utilities, and private companies should consider these findings when incentivizing, funding, planning, and placing future EV infrastructure. By focusing on ensuring equitable access to chargers, they can support the wider adoption of EVs. This includes prioritizing charger installation in diverse neighborhoods and areas with lower educational attainment and income levels.\nBy leveraging the right data at the intersections of climate, community, and the built environment, we can surface the actionable insights that will ensure a more equitable distribution of EV chargers – that will ultimately contribute significantly to higher EV adoption rates, pushing us closer towards a more resilient, sustainable, and inclusive future. Resilience Insights, when paired with our comprehensive Analyst application for data visualization, provide answers to many complex questions related to Infrastructure & Mobility for any location in the United States. Contact us if you want to learn more!']"	['<urn:uuid:ff9ee6b9-87ad-4987-b56f-4a05e98b02fc>', '<urn:uuid:5fbf5a0f-b48f-437c-a774-25770944f520>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T11:40:26.317848	44	151	1784
4	Could you explain the advantages and functionality of using the publish/subscribe pattern in distributed applications with Dapr?	Dapr's pub/sub API provides a platform-agnostic way to send and receive messages with at-least-once delivery guarantee. It integrates with various message brokers and queuing systems, with the specific message broker being configurable as a Dapr pub/sub component at runtime, making services more portable and flexible. A key benefit is that multiple services can subscribe to topics published by a single service - for example, both shipping and email services can receive messages published by a cart service through the same pub/sub message broker component.	"[""In this case, we'll be wrapping things up and talking about what else we can learn. Believe me or not, there is still much to discover.\nWe should review what has been accomplished so far. We began by discussing what the Dapr is and how it is constructed, and the hands-on installation and configuration was the next stage. We also mentioned that Dapr has hosting options. We have used the self-hosted one. But there are two more, and trying them out is an excellent idea for new Dapr challenges.\nFirst, there is an option for Dapr to run on the Kubernetes cluster. In this scenario, four Dapr parts are executing on Kubernetes: operators, sidecar injectors, locations, and sentries.\nThe last option is to run in serverless mode. You can choose a serverless cloud offering if you want to run your Dapr applications without managing any underlying infrastructures such as VMs or Kubernetes. These platforms integrate with Dapr to make deploying and managing your applications easy. At the time of writing this article, the sole option for operating Dapr in serverless mode was to use the Azure Container Apps serverless platform.\nAzure Container Apps is a serverless application hosting service where users do not see nor manage any underlying VMs, orchestrators, or other cloud infrastructure. Azure Container Apps enables you to run your application code in multiple containers and is unopinionated about runtimes or programming models used.\nDapr is built-in to Container Apps, enabling you to use the Dapr API building blocks without any manual deployment of the Dapr runtime. You simply deploy your services with their Dapr components.\nThis is another area covered by Dapr and is an excellent place to the extent of your knowledge. This is a prevalent issue, and the one present is most of the microservices-based solutions. The challenge is to make a call from one service to another and manage the addresses of the environments that the microservices are running on. There is a set of common challenges in this process. The first is no call service discovery to track the URL and IP address of the given microservice - this is challenging, though, because we know the code we like to address service by name rather than by an IP. The second challenge lies in communication security: we need to prevent calls from external sources if the service is to be used only internally in the solution. We also have to ensure that the communication is encrypted so no one can listen. Third, observability can be challenging in the microservices infrastructure.Dapr also handles that with Open telemetry tracing.\nLast but not least, there is a challenge of retries in case of a mistake. We always have to be ready for an error in network communication. At the time of writing this article, the sole option for operating Dapr in serverless mode was to use the Azure Container Apps serverless platform.\nThis is how Dapr addresses this case. With the Service invocation block. Service invocation block provides several features to make it easy to call methods between applications. First of all, we have an mTLS authentication to keep the communication secure. In the event of call failures and transient errors, service invocation provides a resiliency feature that performs automatic retries with backoff periods. By default, all calls between applications are traced, and metrics are gathered to provide insights and diagnostics for applications. This is especially important in production scenarios, providing call graphs and metrics on the calls between your services.\nDirect calls aren’t the only type of possible service-to-service communication.There is also pub/sub messaging. In this approach, the messages aren’t being sent directly. Instead, the sender sends them to a message broker and, in due time, receives them from the receiver. The example solution with this Dapr block can look like this.\nBut why should we use Dapr pub/sub API? Good question. The block provides us with a platform-agnostic API to send and receive messages and offers an at-least-once message delivery guarantee. It also integrates with various message brokers and queuing systems. This point is essential because the specific message broker used by your service is pluggable and configured as a Dapr pub/sub-component at runtime - this removes the dependency from your service and makes your service more portable and flexible to changes.\nThe application often works like this when we go with the pub/sub. Your service makes a network call to a Dapr pub/sub-building block API. The pub/sub building block makes calls into a Dapr pub/sub component that encapsulates a specific message broker.To receive messages on a topic, Dapr subscribes to the pub/sub-component on behalf of your service with a topicand delivers the messages to an endpoint on your service when they arrive.\nAs shown in the diagram above, the pub/sub approach has one huge benefit over the direct API call. In the graph below, a “shipping” service and an “email” service have both subscribed to topics published by a “cart” service. Each service loads pub/sub-component configuration files that point to the same pub/sub message broker component. So one message is received by the recipients, which is highly convenient when designing internal application communication.\nSo the Pub/Sub Dapr Messaging should be another point on the Dapr roadmap.\nIt is going to be the last roadmap item we are going to discuss. There are many more, of course, but we must draw a line somewhere. So in my honest opinion, the last item to look at should be the monitoring block. In the case of the Dapr solution, it is called observability and comprises three areas: tracing, view metrics and logging.\nThis diagram shows the introduction of observability to our Dapr infrastructure - as we can see the Dapr sidecars are using the collector. The advantage of it is that the approach to internal communication is consistent. Always sidecar based. Once the data is collected, we can freely monitor and visualize it as we like. This is huge because it blends perfectly into Dapr infrastructure and, as always withDapr doesn’t restrict us to a predefined set of tools.\nThat is it when it comes to Dapr series. I hope you will find it enjoyable and educative. Please remember that the work we did was only for one block. The series' goal was to show its approach to using Dapr and the design decision made by its creators. I believe we met this goal and encourage you to keep using and exploring Dapr usage in your day-to-day work.\nIf you have any questions, please drop me a line at firstname.lastname@example.org.\nTill next time. Keep coding.""]"	['<urn:uuid:c4e9a78d-fce7-4d1c-9c96-a17d7342d420>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T11:40:26.317848	17	84	1099
5	stage depth vs width rock band	When mixing audio, width refers to lateral positioning of instruments across the soundstage (left to right), typically spanning 40 feet wide in a live setting. Depth refers to how close or far away each instrument sounds in relation to the front of the soundstage. In a standard rock band setup, vocals and bass are placed up front in the center, drums slightly behind them, and guitarists spread out 15 feet to the left and right.	"['Panning is how the instruments are arranged within the stereo image. By properly using this mixing feature, you can create a very realistic mix. Mixing engineer should mix tracks with a live sound stage/concert stage perspective. A vocal is always in the center because the band lead singer is the star of the show. At the back of the vocals is the bass player. Typically at the back of the bass player is the drummer. The band’s guitarists are on the left and the right. Bear in mind that different panning arrangement are possible for additional instruments (such a band with piano or others). But this concept illustrates the basic things on how to do panning in audio mixing to create a very realistic stereo image such as the diagram shown below:\nLive sound monitors for vocals and bass are placed up front facing the audience, so that fans can get a great feel of the vocals and the bass groove. On the left and right loud sound monitors are the guitars.\nIn the commercial audio production and using a recording software, panning can be controlled between -100 to +100. Where mostly -100 is the leftmost part of the stage and +100 is the rightmost part of the stage. Mixing from the real live stage perspective, a mixer can set:\na. Vocals to panning= 0 (center)\nb. Kick drums = 0 (center)\nc. Bass guitar= 0 (center)\nd. 1st guitarist=75 (hard right)\ne. 2nd guitarist= -75 (hard left)\nf. Whole drum set (crash cymbals to ride cymbals)= -12.5 to 12.5 (this is the correct one)\nUsing ratio and proportion, typical sound stage width is about 40 feet wide. Using recording software, this whole panning width is 200 panning units. (Length of -100 to +100).\nTherefore the ratio of panning units to feet is:\n200 panning units/ 40 feet = 5 panning units/feet (For a 40 feet sound stage)\nTo check how realistic is this conversion, we will use the width of the real drum set.\nReal drum set needs 5 feet width space when fully set-up. Converting 5 feet to panning units in recording software is about :\n5 feet x 5 panning units/feet = 25 panning units, so our panning specifications are correct.\nThis means that for a 40 feet sound stage, to create a real stereo image of drums, it should be panned between -12.5 to 12.5 (this is the correct specification).\nGuitarist are placed +75 to -75 respectively. This means they are both located :\n1st guitarist: -75 panning units/ 5 panning units= 15 feet from the left of the vocals.\n2nd guitarist: +75 panning units/5 panning units= 15 feet from the right of the vocals.\nOne important thing to take note on panning is the energy level with respect to panning distance. Rule of thumb is that, the lowest frequencies should be pan on the center except for the vocals. And the higher the frequencies, the farther you can place them away from the center. It is because, low frequencies such as bass occupies massive energies and needs to placed at the center for maximum volume.\nThe following are advantages of proper panning in mixing :\na. Create a real stereo image of an actual live sound stage.\nb. Avoid battling the same frequencies in the same location of the stereo image.\nBy placing the vocals in the center of the mix, means not in conflict in low frequencies and those guitars that occupy the same frequencies as the vocals are placed away from the center. Thus panning improves the clarity of the mix.\nFor details about panning settings for different musical instruments, you can refer to the following tutorials:', ""The drum kit presents a mix engineer with a variety of tracks to deal with, but the content of those tracks is a real grab bag of sounds. Between kick, snare, toms, and cymbals, the drum set covers the entire audible frequency range, as well as extremes in dynamics. It's no wonder that drums are one of the most difficult instruments to get right in a mix.\nOf course, mixing is an art, and art can't really be taught — especially within the confines of one short article. But mixing is also a trade, and as such has spawned various tricks of the trade, many of which can be taught.\nTHE VISION THING\nA danger of talking about drum mixing as a separate topic is that the inexperienced might be seduced into thinking that a killer-sounding drum mix is an end in itself. It's not. A great drum mix is still just a part of the total mix; like any of the parts, it must be appropriate for, and supportive of, the song and arrangement.\nThe first step to mixing any song, then, is to familiarize yourself with it. Start by pushing up all the faders and listening to it over and over. Make the obvious level adjustments while you're listening, of course, so that the thing has the general shape of a mix. But don't start processing tracks yet — just listen. What does the song need from the drums? What is it asking for? A rock ballad, for example, might cry out for a huge backbeat and thunderous toms, whereas a jazz ballad might need only for the drums to whisper and caress. You also need to ask yourself whether the drums are going to be the focal point of the rhythm section, or whether they'll be subservient to some other element such as the rhythm guitar or bass.\nIt's helpful to start a mix with a clear vision of the song, a mental final mix of how you think it should sound. Only when you have some direction for the song can you properly assess the role of the drums. It's all about context.\nEvery great stereo mix also sounds three-dimensional — tall, deep, and wide. Tall refers to frequency range (how well all the frequencies are represented), wide to lateral positioning (where the instruments “appear” left to right across the soundstage), and deep to depth of soundstage (how close up or far away each instrument sounds in relation to the “front” of the soundstage).\nTherefore, think in 3-D terms, for not only the whole mix but also for the drums. A drum kit, after all, comprises a bunch of parts spread out all over the place. Reflecting a (believable) sense of that spread-outness will deepen the dimensionality of the whole mix.\nOne of my personal mix tricks is to take the three dimensions one at a time: width first, height second, and depth third. That helps me focus on maximizing each dimension. Here's the strategy, laid out in that order.\nSTART AS IS\nOnce you have the vision for the song, see how close you can get to realizing it by manipulating volume levels and pan positions only. Starting that way keeps you focused on the song as a whole and helps avoid the easy distraction of soloing and processing individual tracks, only to find that they don't work once reintroduced to the mix.\nDon't worry — in the course of shaping the mix using only the tracks as they are, you will soon become aware of what's missing, what's clashing, what needs tweaking or muting, and so on.\nSEE THE KIT\nWhen panning drums, envision an area on the soundstage where the kit is set up. Note that you can mix as if looking at the drums from behind or in front of the kit — there's no right or wrong here.\nWhat's important is to choose one perspective and stick with it. To that end, make sure the panning of the overhead mics corresponds with the panned positions of the toms (assuming they are individually close-miked). Simply solo the panned overheads (during a tom-fill passage), note which direction the tom fill goes, make a mental note of where each tom hits, and then pan the individual tom tracks accordingly. This will tighten the drum mix and will help clarify the location and punch of each tom in the stereo field.\nThe usual goal when panning is to create an uncluttered soundstage in which each instrument can clearly be heard and its position identified. Assuming you're going for a natural drum perspective (the best place to start, usually), keep the overall “width” of the drum image consistent with the number of instruments on the soundstage. In the case of an 18-piece big band, for example, the drum kit would likely be panned into a small area (between, say, 11 and 12 o'clock), so it won't “step on” the other instruments. A power-trio mix, on the other hand, would allow for wider drum pans.\nLISTEN, DON'T LOOK\nWhen setting pans, don't rely visually on the location of pan pots. Pan pots used in budget consoles are not very exacting, with the result that their positions on the mixer don't necessarily correspond to the position of the instrument in the stereo field. The solution is to make panning decisions based only on the sound. (I often close my eyes while panning, to better focus on what I'm hearing.)\nA very useful technique is to finalize drum-pan tweaks with the mix summed to mono. That might not seem to make sense, but it works. For example, to find the best location for the hi-hats, sum to mono, turn the hi-hat pan knob slowly back and forth (in the general area where you had assigned it), and listen for a spot where the hats seem to step forward a bit or come into clearer focus. What you're doing, basically, is combing the mono sound field in search of a slice of free space.\nThe mono-panning technique is helpful for positioning kick and snare, too. Though it's customary to pan kick and snare drums on top of one another, typically dead center, try panning them ever so slightly apart, again while in mono, and listen for a spot where you get a sense of separation (the same as you get when listening to a live drum kit). By putting a slight bit of space between the two drums, you unmask the signals from one another, which helps clarify each in the mix.\nIf the tracks you're mixing include stereo overheads, make sure to check the phase relationships between the two mic signals — poor overhead-mic placement is not uncommon, and its results can ruin a mix. Sum the overhead tracks to mono and listen closely from top to bottom. If the sound collapses, loses frequency content, or in any way sounds weird, you likely have a phase problem.\nIn that case, there are a couple of solutions. One is to go into a DAW and manually align the stereo sound waves. If that's not feasible, the easy fix is to eliminate (mute) one of the overhead channels. You can always create a faux-stereo image later by panning an effect opposite the mono track, delaying it a bit, and equalizing it differently.\nIt also pays to check phase relationships between the overhead mics and any close-miked drums. Sometimes the overheads are positioned at such a height above the kit that the sound waves are practically phase-inverted when they reach the mics (in relation to the sound that's captured by the close mics).\nIn that case, you might find that reversing the polarity (“flipping the phase,” as it's called) on the overhead-mic signals improves the drum sound — for example, by making the kick and snare sound fuller. But before making a decision which way to go, be sure to solo all the drum-kit tracks and listen closely, in mono, throughout the song. There are often trade-offs you must make when changing phase relationships, and you should take those into account.\nAfter levels, pans, and phase checks, the logical next step is dynamics processing. Compressors tend to alter tonality a bit, so it makes sense to compress before getting into EQ and other processing.\nTypically, the most important drums in a mix are kick and snare, so it's wise to reserve your better compressors for those tracks. Unfortunately, most inexpensive compressors don't have what it takes to get awesome kick and snare sounds. This is one of those instances in which the gear can make a big difference. (This explains why so many pro mixers favor UREI 1176LN peak limiters for kick and snare — they sound marvelous.)\nIn my experience, the best affordable pick for kick and snare drums is a full-featured, VCA-based compressor. One midpriced unit I've had great results with, especially on kick drum, is the Aphex 661 Expressor.\nFor overheads, you can usually get by with a cheaper compressor (again, VCA based). Lower ratios — 1.5:1 to 3:1 — are good here, and only slight to moderate gain reduction. The goal is to bring up the low-level stuff enough to make subtleties audible, but without crushing the life out of the performance.\nIf you're working in a DAW-based studio, you also have the option of using one of the many plug-in compressors on the market. (This holds true for any of the compression applications discussed here.) Though in many cases I still prefer the sonic character of hardware compressors, especially when I'm going for sounds that are more radical, there are many engineers who swear by their plug-ins. The Waves Renaissance Compressor is an example of a sonically flexible and relatively inexpensive compressor plug-in. It sounds good, is easy to use, runs on Mac and PC in a variety of formats, and can be set to emulate either a VCA-based or optical-controlled compressor (see Fig. 1).\nIf the toms are individually miked, some form of noise gating is almost essential. In the analog realm, my favorite affordable gates are the 4-channel Drawmer MX40 Punch Gate (see Fig. 2) and those found on the Drawmer MX30 Gated/Compressor/Limiter (an excellent all-around 2-channel dynamics processor). Of course, if you're working on an automated console or a DAW (or both), you can cut or reduce noise any number of ways — with automated mutes, deletes and crossfades, noise-gate plug-ins, and so forth.\nTAKE A BUS\nTo help make the drums sound huge and extra solid without sounding overly compressed, try compressing all (or most) of the drum tracks a second time through a stereo compressor. Either bus the drums to a subgroup with a stereo compressor on the inserts or return the compressor outputs to a pair of faders, to allow for equalizing. Hit the compressor fairly hard and bring up the compressed channels just beneath the other drum tracks. Mix to taste.\nI prefer photo-optical-controlled compressors for this duty. A very affordable opto unit that works remarkably well is the Joemeek MC2 (see Fig. 3). The MC2 is the least expensive unit I've found that gets close to a high-end compression sound on snare drum. Opto-controlled compressors are also great for processing room mics. Another low-cost analog compressor capable of giving near-high-end results is the FMR Audio RNC1773, better known as the Really Nice Compressor. A VCA-based stereo unit, the versatile 1773 is useful in many drum applications, including stereo overheads, kick drum, snare, and subgroups.\nTHE GREAT EQUALIZER\nBack to the vision thing; it helps to know why you are equalizing a signal, or to at least know what you're going for. There are two main kinds of reasons to equalize, and both describe types of EQ: corrective and creative.\nTrying to make an instrument sound clearer, better defined, better balanced, or just more natural all fall into the corrective camp. So does carving out space by juggling frequencies between competing instruments (kick and bass, for example). Creative EQ, on the other hand, moves beyond mere correction of frequency imbalances. Here, you try to make the sound into something different — bigger than life, as if recorded through a telephone, or whatever.\nWhen applying corrective EQ, work to strike a balance between boosting and cutting; too much of either will typically make the instrument sound unnatural. The idea is to keep the envelope as linear and consistent as possible. If you must favor one or the other, favor cutting — cutting never adds noise, but boosting often does.\nBring up the entire rhythm section when dialing-in drum-kit EQ. It's fine to solo here and there from time to time in order to focus in on a particular instrument. For the most part, though, keep the bass and other rhythm instruments in there with the drums. Remember, the concept of equalization pertains to a relationship among elements. You need to balance the frequency content of the whole foundation — not of just one element. The goal is to strike a musical balance between all the elements, leaving each more audible in the mix by ensuring that it isn't fighting other elements for the same sonic territory.\nNote that effects can be used to help equalize a signal. For example, if the overheads sound too bright, processing the tracks with a dark room rather than a bright one can help take some of the edge off. Better yet, route the effects returns back through spare fader channels so that you can EQ the effect.\nDRUM 'N' BASS\nIn terms of juggling frequencies, pay particular attention to how the kick drum and bass guitar work together. Solo the two and listen closely. Which produces the predominantly lower note and which the higher?\nTry accentuating this low/high relationship: carve out some space by equalizing so that like frequencies don't overlap. For example, if you boost the kick at 60 Hz, cut the bass at 60 Hz. If you cut the kick at 400 Hz (usually not a bad idea), try boosting the bass a bit in the same range. Again, the goal is to find a musical balance between the elements.\nKNOW YOUR PATCHES\nIt pays to be well acquainted with your effects processors, too, as they usually play a big part in a drum mix. Keep a list of favorite drum patches handy, or save your favorite presets for your plug-ins, so that you know you can always dial up something that will work.\nDrums tend to allow for more creative processing than do other instruments, however, so it's good to keep your ears (and mind) open. Sometimes the unorthodox — a lead-guitar patch on a snare drum, for instance — can be just the ticket.\nPersonal-studio operators are blessed these days with a plethora of first-rate effects at low prices, be they plug-ins for DAW systems or inexpensive outboard multi-effects units. My current favorite multi-effects units for drum processing are the Lexicon MPX 110 and the T.C. Electronic M300 (see Fig. 4).\nTWEAK THE TIMING\nTime-domain effects, such as reverb, and delay are where it's at for making a drum kit come alive in the mix. There are many ways to use such effects to create depth in a mix, but they all boil down to the same idea: positioning some instruments in the foreground, others in the background, and others in between.\nSome engineers are quite finicky about this process, to the point that they calculate minutely different early-reflection (predelay) times for different ends (channels) of the same instrument. Other engineers, including moi, do it more by feel.\nWith multimiked drums, you might need to factor in ambient sound captured by overhead and room mics. Even leakage between close-miked drums can affect drum ambience. The trick is integrating the natural ambiences with any artificial ones you dial in.\nOne thing I like to play with that can enhance drum-kit dimensionality is the amount of effect in each channel. After all, the wetter the signal, the farther back it tends to sound. Even using only a single effect, you can help dimensionalize a kit simply by varying the amount of effect returning to the different channels. Try, for example, putting no effect on the kick, just a bit on the hats, a moderate amount on the overheads, even more on the toms, and the most on the snare.\nGET A REFERENCE\nAny tricks of the trade that help with mixing in general will help with mixing drums. Those tricks include honoring Fletcher and Munson by monitoring at different levels, employing multiple monitors (including a cheapie system such as a boom box), and remembering to check things in mono from time to time.\nI'll close with the most important trick for those who are serious about improving their drum-mixing chops: compare your mixes with those of the masters. When your drum mixes can hold a candle to those of seasoned pros such as Elliot Scheiner, Frank Filipetti, and Bob Clearmountain (to name just a few), you can put the reference discs away. Until then, keep 'em spinning. They will keep you honest.\nBrian Knaveis a former senior associate editor atEM.He lives in Salvador, Bahia, Brazil, where the drums never stop.""]"	['<urn:uuid:b2eac978-3889-4f7b-a4fc-5164463872ad>', '<urn:uuid:6ee690d9-2bfd-4f17-a23c-b2a49cdf6cd1>']	factoid	with-premise	short-search-query	distant-from-document	comparison	novice	2025-05-12T11:40:26.317848	6	75	3481
6	modern urban style bedroom arrangement sleep quality factors	Urban modern style focuses on creating comfortable, calm environments using neutral colors and comfortable furniture, often incorporating industrial elements like steel and brick with pastel accents. When considering sleep quality, this design approach should be balanced with evidence-based factors that promote good sleep: maintaining cool temperatures (below 75°F), minimizing noise through white noise or earplugs, and creating a dark environment. The style's emphasis on comfort aligns with sleep science showing that environmental comfort significantly impacts sleep quality and overall health outcomes.	"['Interior Architecture and Popular Styles\nInterior architecture has changed throughout history and was mostly determined by the way of life, social position, property status, etc. Today, the situation is a little different and mostly everything depends on the users of the space – what they like and where they feel most comfortable. The space adapts to each homeowner thanks to the ideas, tools and techniques offered by interior design. Contemporary interior architecture the effects rousing new world culture, and unit designs that integrate an all global influences alike.There are many unique styles in interior design, some are completely different, while some complement each other. Through practice, there are also cases of combining elements of several different styles to achieve an ideal living environment. Otherwise, it is very difficult to single out the most common used styles because the needs of users and tastes are very different, but we tried to single out the 5 very popular styles that appear in interiors around the world.\nAs a concept of minimalism, we can take the sentence “less is more” by the world famous architect Mies Van Der Rohe. The main feature of this style is the most functional and open space. It’s also, characterised by simplicity, clean lines, flat, smooth surfaces without ornamentals and geometric shapes which exude elegantly.\nModern minimalist interiors use a monochromatic palette that are easy to work with, including white, beige, black and gray. If you are still a fan of colors, when decorating a minimalist interior, you can also use pastel colors used as an accent, such as mint, peach color, etc. Wall decorations such as wallpaper and relief are not recommended.\nIndustrial interior design is based on an open space with high ceilings. The main characteristics are visible building elements that we normally hide in the living space, such as bare pipes for ventilation, heating. The point is in the accents of completely raw materials – rusty metal, wood, brick, concrete. High ceilings as a synonym of industrial design come from huge industrial halls and factories that have been turned into apartments. Old brick, concrete, steel girders are the details that actually give a real industrial touch to an environment. That is why in these interiors, we can single out earthy colors and neutral tones (like all shades of gray) that imitate these “industrial” materials. As accents and an integral part of the industrial style, we can also single out large metal chandeliers, spotlights and lamps, as well as light bulbs with visible cables.\nScandinavian style in interior design is becoming an increasingly current trend in contemporary architecture. It is defined by simplicity, pure form, and irresistible elegance. Natural lighting plays a key role in designing a space in this style. The space is minimally equipped with furniture which emphasizes the spaciousness. White color dominates this style, which is logical considering that it best reflects light. Also, in order not to obscure the natural light curtains are generally non-existent, while those used in the interior are white, almost transparent or light. To add contrast to the interior, color accents are applied, it can be a piece of furniture, pillows, paintings framed by a thin black frame or small decors that complete the space. Scandinavian style connects the ambience with nature, so the use of natural wood is inevitable in this design. Furniture and floors are made of wood mostly light tones.To add warmth to this space, textile accents are used, such as woolen rugs, artificial animal fur, etc.\nIt can also be said that the Scandinavian style is the definition of the interior with the use of quality natural materials, ergonomics and aesthetics that accompany the function.\nThe rustic style of interior design is very diverse and easily recognizable. Wooden beams on the ceiling, rough-hewn woodenly or stone cladding on the wall or floor, are the main features of this style. Perfectly fit the design of a country house, cottage, but also modern apartments. Rough stone fireplace, as the main accent is an unmistakable choice when decorated in this style. The rustic style of interior design creates a warm, comfortable, cozy, and very interesting ambience. Natural materials in their original form such as stone, brick and wood combined with natural textiles such as leather, linen and wool give warmth to the space.The color palette that is most suitable for decorating in this style is usually light tones because they create a good combination with the colors of natural materials.\nUrban modern style in interior design is very outstanding and modern in the arrangement of city apartments and lofts. In the urban modern style, everything is about creating a comfortable, calm, and well-arranged interior that contains the charm of everyday life. This style puts equal focus on comfort and brilliant design. It is a combination of different interior design styles. It can be defined as a style influenced by industrial and contemporary design. Roughly processed wood, steel or brick, combined with neutral and pastel colors and comfortable furniture are the embodiment of this style. The seating furniture is mostly massive but also elegant and monochrome. Accents wich is added to make a contrast is such as brightly colored pillows, mirrors, interestingly designed pieces of furniture, a combination of gold or silver metals on furniture.', ""New Information to Sleep on\nNew Information to Sleep on\nWe often fail to recognize or acknowledge its importance, but sleep, like a high-nutrient diet and exercise, is a crucial component of excellent health. As our lives become increasingly busy, we are sacrificing sleep: in 1942, mean reported sleep time for adults was 7.6 hours, and by 2001, the average was down to 6.7 hours.1 According to a poll by the National Sleep Foundation, 63% of American adults report that their sleep needs are not being met, and 43% report that on weeknights they rarely or never get a good night’s sleep.2But we need adequate sleep to work optimally in our daily activities.3\nWhy do we need sleep? Our brains work best when we are well-rested: during sleep, our brains stabilize newly formed memories, and adequate sleep promotes learning and cognitive performance the next day.3,4 Sleep is also essential for proper immune function.5 Getting adequate sleep regularly may reduce the severity of cold symptoms and also may maintain sufficient numbers of natural killer cells.6 In fact, there is some evidence that poor sleep could impair the immune system’s ability to eliminate, small, newly established tumors before they become dangerous.7 In addition, melatonin, which is a hormone produced in response to darkness and during sleep, is an antioxidant and an inhibitor of cancer cell growth.8-10 Allowing the body to produce sufficient melatonin is essential. Inadequate sleep is associated with impaired learning ability, faster aging of the brain, impaired driving and work performance, overeating, obesity, elevated cholesterol, and increased risk of diabetes, hypertension and death from all causes. Plus, lack of sleep negatively affects our appearance and emotional state.4,11-21\nAre you getting enough, good quality sleep? The precise amount of sleep required for adults has been debated, and differs between individuals. But, if you wake up to an alarm clock you are most likely sleep-deprived.6Here are some strategies to consider if you’d like to improve the quality of your sleep:6,22\nMinimize electronic device use at night. Smartphones, computer screens, televisions and tablets emit blue light, which suppresses melatonin production. Using these devices close to bedtime can disrupt sleep.23 If you wake in the middle of the night, don’t turn on your TV, smartphone or computer; the light will turn off melatonin and cause you to feel more alert. Instead, relax, read under low light or meditate until you feel sleepy again.\nMake your sleep environment as dark as possible. Don’t keep clocks that emit light or night lights in the bedroom; light-blocking curtains or a sleep mask can reduce exposure to outside light and enhance your sleep quality. Light exposure regulates our internal clock: bright light makes us alert in the morning, and a dark room at night promotes melatonin production and good sleep. Exposure to light soon before bed or during sleep reduces the depth and quality of sleep. Even a low level of light exposure through closed eyelids (such as a night light) can reduce melatonin production, and this disruption of our natural rhythms has ill health effects. Light exposure at night is associated with an increased risk of cancer, most strongly with breast cancer.24,25\nSleep on a consistent schedule, going to bed at the same time every night, and waking up at the same time every morning.\nDon’t wake up to an alarm clock if possible; the alarm clock wakes you abruptly and use of the snooze button can rob you of valuable REM sleep. When you wake naturally, your body prepares you during the final sleep cycles by shifting hormone production—reducing melatonin and increasing cortisol, which helps you to become alert.6,26\nMinimize noise. How noise affects one’s sleep is somewhat individual, based on what is familiar and typical. Earplugs or “white noise” (for example from a fan) may help to prevent noises from disrupting sleep.27\nExercise. Exercising regularly (especially vigorous exercise) promotes healthy sleep, but exercise close to bedtime is usually not recommended.\nSleep at a comfortable, but cooler temperature. Body temperature naturally drops during sleep. Sleeping in a warm room (above 75°F) or trapping in excessive heat with extra blankets may disrupt sleep.\nFollow a high-nutrient diet. A low intake of vegetables is associated with poor sleep.31 Those following aNutritarian diet may get better quality sleep than people eating poorly, and therefore may require fewer hours of sleep.\nFor those who experience difficulty sleeping, morning light exposure (or light therapy) helps normalize melatonin cycling as a means of establishing better sleep patterns and resolving insomnia. In the mornings open the shades wide and get in a sun lit room, go outside or use a therapeutic light. Tart cherry juice, a natural dietary source of melatonin, may be an effective addition.32,33 Supplementing to achieve adequate omega-3 fatty acids and zinc may also benefit sleep.34-36 These natural methods are preferable to prescription sleep drugs, which are linked to a dramatic increase in risk of death. These findings demonstrated a three-fold increased risk of death associated with regular use and a 35 percent higher risk of cancer.37\n1. Moore DW: Gallup. Eyes Wide Open: Americans, Sleep and Stress. 2002.\n2. National Sleep Foundation. Sleepy Connected Americans 2011.\n3. McCoy JG, Strecker RE: The cognitive cost of sleep lost. Neurobiol Learn Mem 2011, 96:564-582.\n4. Walker MP: The role of sleep in cognition and emotion. Ann N Y Acad Sci 2009, 1156:168-197.\n5. Opp MR: Sleeping to fuel the immune system: mammalian sleep and resistance to parasites. BMC Evol Biol 2009, 9:8.\n6. Dement WC, Vaughan C: The Promise of Sleep. New York: Delacorte Press; 1999.\n7. Hakim F, Wang Y, Zhang SX, et al: Fragmented sleep accelerates tumor growth and progression through recruitment of tumor-associated macrophages and TLR4 signaling. Cancer Res 2014, 74:1329-1337.\n8. Canaple L, Kakizawa T, Laudet V: The days and nights of cancer cells. Cancer Res 2003, 63:7545-7552.\n9. Blask DE, Brainard GC, Dauchy RT, et al: Melatonin-depleted blood from premenopausal women exposed to light at night stimulates growth of human breast cancer xenografts in nude rats. Cancer Res 2005, 65:11174-11184.\n10. Schernhammer ES, Schulmeister K: Melatonin and cancer risk: does light at night compromise physiologic cancer protection by lowering serum melatonin levels? Br J Cancer 2004, 90:941-943.\n11. Aldabal L, Bahammam AS: Metabolic, endocrine, and immune consequences of sleep deprivation. Open Respir Med J 2011, 5:31-43.\n12. Gangwisch JE, Heymsfield SB, Boden-Albala B, et al: Short sleep duration as a risk factor for hypertension: analyses of the first National Health and Nutrition Examination Survey. Hypertension 2006, 47:833-839.\n13. Gangwisch JE, Heymsfield SB, Boden-Albala B, et al: Sleep duration as a risk factor for diabetes incidence in a large U.S. sample.Sleep 2007, 30:1667-1673.\n14. Gangwisch JE, Malaspina D, Babiss LA, et al: Short sleep duration as a risk factor for hypercholesterolemia: analyses of the National Longitudinal Study of Adolescent Health. Sleep 2010, 33:956-961.\n15. Kim S, DeRoo LA, Sandler DP: Eating patterns and nutritional characteristics associated with sleep duration. Public health nutrition2011, 14:889-895.\n16. Mozaffarian D, Hao T, Rimm EB, et al: Changes in diet and lifestyle and long-term weight gain in women and men. N Engl J Med2011, 364:2392-2404.\n17. Patel SR, Malhotra A, White DP, et al: Association between reduced sleep and weight gain in women. Am J Epidemiol 2006,164:947-954.\n18. Spiegel K, Leproult R, Van Cauter E: Impact of sleep debt on metabolic and endocrine function. Lancet 1999, 354:1435-1439.\n19. Cappuccio FP, D'Elia L, Strazzullo P, et al: Sleep duration and all-cause mortality: a systematic review and meta-analysis of prospective studies. Sleep 2010, 33:585-592.\n20. Theorell-Haglow J, Berglund L, Berne C, et al: Both habitual short sleepers and long sleepers are at greater risk of obesity: a population-based 10-year follow-up in women. Sleep Med 2014.\n21. Williamson AM, Feyer AM: Moderate sleep deprivation produces impairments in cognitive and motor performance equivalent to legally prescribed levels of alcohol intoxication. Occup Environ Med 2000, 57:649-655.\n22. National Sleep Foundation. The Sleep Environment. [http://sleepfoundation.org/sleep-news/the-sleep-environment]\n23. National Sleep Foundation. See. [http://sleepfoundation.org/bedroom/see.php]\n24. Yang WS, Deng Q, Fan WY, et al: Light exposure at night, sleep duration, melatonin, and breast cancer: a dose-response analysis of observational studies. Eur J Cancer Prev 2014, 23:269-276.\n25. Stevens RG, Brainard GC, Blask DE, et al: Breast cancer and circadian disruption from electric lighting in the modern world. CA Cancer J Clin 2014, 64:207-218.\n26. Stepanski EJ: The effect of sleep fragmentation on daytime function. Sleep 2002, 25:268-276.\n27. Hu RF, Jiang XY, Zeng YM, et al: Effects of earplugs and eye masks on nocturnal sleep, melatonin and cortisol in a simulated intensive care unit environment. Crit Care 2010, 14:R66.\n28. Geoghegan P, O'Donovan MT, Lawlor BA: Investigation of the effects of alcohol on sleep using actigraphy. Alcohol Alcohol 2012,47:538-544.\n29. Feige B, Gann H, Brueck R, et al: Effects of alcohol on polysomnographically recorded sleep in healthy subjects. Alcohol Clin Exp Res 2006, 30:1527-1537.\n30. Drake C, Roehrs T, Shambroom J, et al: Caffeine effects on sleep taken 0, 3, or 6 hours before going to bed. J Clin Sleep Med 2013,9:1195-1200.\n31. Katagiri R, Asakura K, Kobayashi S, et al: Low Intake of Vegetables, High Intake of Confectionary, and Unhealthy Eating Habits are Associated with Poor Sleep Quality among Middle-aged Female Japanese Workers. J Occup Health 2014.\n32. Pigeon WR, Carr M, Gorman C, et al: Effects of a tart cherry juice beverage on the sleep of older adults with insomnia: a pilot study. J Med Food 2010, 13:579-583.\n33. Howatson G, Bell PG, Tallent J, et al: Effect of tart cherry juice (Prunus cerasus) on melatonin levels and enhanced sleep quality.Eur J Nutr 2012, 51:909-916.\n34. Montgomery P, Burton JR, Sewell RP, et al: Fatty acids and sleep in UK children: subjective and pilot objective sleep results from the DOLAB study--a randomized controlled trial. J Sleep Res 2014, 23:364-388.\n35. Scorza FA, Cavalheiro EA, Scorza CA, et al: Sleep Apnea and Inflammation - Getting a Good Night's Sleep with Omega-3 Supplementation. Front Neurol 2013, 4:193.\n36. Song CH, Kim YH, Jung KI: Associations of zinc and copper levels in serum and hair with sleep duration in adult women. Biol Trace Elem Res 2012, 149:16-21.\n37. Kripke DF, Langer RD, Kline LE: Hypnotics' association with mortality or cancer: a matched cohort study. BMJ Open 2012,2:e000850.""]"	['<urn:uuid:3193238e-0485-45e0-964a-f56787384d14>', '<urn:uuid:cded948b-e276-490c-b5b6-8684f5bf83a5>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T11:40:26.317848	8	81	2581
7	brain damage sociopaths how affects emotional behavior	Sociopaths with limbic cortex damage can be shrewd and emotionally intelligent, but lack the ability to relate to, empathize with, or express concern for others.	['This is the second section of your IELTS Academic Reading test. You should spend about twenty minutes on it. Read the passage and answer questions 14-26.\nThe Triune1 Brain\nThe first of our three brains to evolve is what scientists call the reptilian cortex. This brain sustains the elementary activities of animal survival such as respiration, adequate rest and a beating heart. We are not required to consciously “think” about these activities. The reptilian cortex also houses the “startle centre”, a mechanism that facilitates swift reactions to unexpected occurrences in our surroundings. That panicked lurch you experience when a door slams shut somewhere in the house, or the heightened awareness you feel when a twig cracks in a nearby bush while out on an evening stroll are both examples of the reptilian cortex at work. When it comes to our interaction with others, the reptilian brain offers up only the most basic impulses: aggression, mating, and territorial defence. There is no great difference, in this sense, between a crocodile defending its spot along the river and a turf war between two urban gangs.\nAlthough the lizard may stake a claim to its habitat, it exerts total indifference toward the well-being of its young. Listen to the anguished squeal of a dolphin separated from its pod or witness the sight of elephants mourning their dead, however, and it is clear that a new development is at play. Scientists have identified this as the limbic cortex. Unique to mammals, the limbic cortex impels creatures to nurture their offspring by delivering feelings of tenderness and warmth to the parent when children are nearby. These same sensations also cause mammals to develop various types of social relations and kinship networks. When we are with others of “our kind” – be it at soccer practice, church, school or a nightclub – we experience positive sensations of togetherness, solidarity and comfort. If we spend too long away from these networks, then loneliness sets in and encourages us to seek companionship.\nOnly human capabilities extend far beyond the scope of these two cortexes. Humans eat, sleep and play, but we also speak, plot, rationalise and debate finer points of morality. Our unique abilities are the result of an expansive third brain – the neocortex – which engages with logic, reason and ideas. The power of the neocortex comes from its ability to think beyond the present, concrete moment. While other mammals are mainly restricted to impulsive actions (although some, such as apes, can learn and remember simple lessons), humans can think about the “big picture”. We can string together simple lessons (for example, an apple drops downwards from a tree; hurting others causes unhappiness) to develop complex theories of physical or social phenomena (such as the laws of gravity and a concern for human rights).\nThe neocortex is also responsible for the process by which we decide on and commit to particular courses of action. Strung together over time, these choices can accumulate into feats of progress unknown to other animals. Anticipating a better grade on the following morning’s exam, a student can ignore the limbic urge to socialise and go to sleep early instead. Over three years, this ongoing sacrifice translates into a first class degree and a scholarship to graduate school; over a lifetime, it can mean ground-breaking contributions to human knowledge and development. The ability to sacrifice our drive for immediate satisfaction in order to benefit later is a product of the neocortex.\nUnderstanding the triune brain can help us appreciate the different natures of brain damage and psychological disorders. The most devastating form of brain damage, for example, is a condition in which someone is understood to be brain dead. In this state a person appears merely unconscious – sleeping, perhaps – but this is illusory. Here, the reptilian brain is functioning on autopilot despite the permanent loss of other cortexes.\nDisturbances to the limbic cortex are registered in a different manner. Pups with limbic damage can move around and feed themselves well enough but do not register the presence of their littermates. Scientists have observed how, after a limbic lobotomy2, “one impaired monkey stepped on his outraged peers as if treading on a log or a rock”. In our own species, limbic damage is closely related to sociopathic behaviour. Sociopaths in possession of fully-functioning neocortexes are often shrewd and emotionally intelligent people but lack any ability to relate to, empathise with or express concern for others.\nOne of the neurological wonders of history occurred when a railway worker named Phineas Gage survived an incident during which a metal rod skewered his skull, taking a considerable amount of his neocortex with it. Though Gage continued to live and work as before, his fellow employees observed a shift in the equilibrium of his personality. Gage’s animal propensities were now sharply pronounced while his intellectual abilities suffered; garrulous or obscene jokes replaced his once quick wit. New findings suggest, however, that Gage managed to soften these abrupt changes over time and rediscover an appropriate social manner. This would indicate that reparative therapy has the potential to help patients with advanced brain trauma to gain an improved quality of life.\n1 Triune = three-in-one\n2 Lobotomy = surgical cutting of brain nerves\nQuestions 14–22Classify the following as typical of the reptilian cortex\n|B||the limbic cortex|\nAnswer A, B or C, to questions 14–22.\n14) giving up short-term happiness for future gains\n|15) maintaining the bodily functions necessary for life|\n|16) experiencing the pain of losing another|\n|17) forming communities and social groups|\n|18) making a decision and carrying it out|\n|19) guarding areas of land|\n|20) developing explanations for things|\n|21) looking after one’s young|\n|22) responding quickly to sudden movement and noise|\nComplete the sentences below.\nUse no more than two words from the passage for each answer.\n|23) A person with only a functioning reptilian cortex is known as ...............|\n|24) ............... in humans is associated with limbic disruption.|\n|25) An industrial accident caused Phineas Gage to lose part of his ...............|\n|26) After his accident, co-workers noticed an imbalance between Gage’s ............... and higher-order thinking.|\nRemember, you have 60 minutes to complete the Reading test! You should spend about 20 minutes on each of the three sections.\nYou have completed the second section of your Reading test. Now move on to Reading passage 3.']	['<urn:uuid:33a235e0-ef81-4750-b19c-0fb143bdc472>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T11:40:26.317848	7	25	1061
8	How long can you safely take Ambien?	Those who have been prescribed Ambien to treat insomnia are directed not to take the drug for longer than ten consecutive days.	['Sleep medication like Ambien, Stilnox and Imovane (also known as Zimovane) are medications given for insomnia or sleeplessness and they are all categorised as depressant drugs. Ambien is the commercial name for the chemical zolpidem tartrate, Imovane is the chemical zopiclone and both are sometimes referred to as ‘z-drugs’. Sleep medications like Ambien and Imovane are referred to as sedatives or hypnotics and they affect the chemistry of the brain to induce drowsiness and cause the person taking the drugs to fall asleep. Sleep medication like Ambien comes in the form of pills or tablets that are swallowed. With Ambien, the pills are designed so that they have two active layers; the first is dissolved quickly and helps induce a state of sleep, while the second layer is designed to maintain a sleep and prevent the person waking up during the night.\nAmbien and Imovane are not an ‘over the counter’ medication and can only be prescribed by a doctor who has identified the genuine symptoms of insomnia in a patient.\nZolpidem tartrate was first synthesised as a medication for insomnia in 1988 and was subject to five years of clinical trials before being approved and trademarked as Ambien in 1993.\nZopiclone was introduced in 1986 and like Zolpidem was hailed as an improvement on benzodiazepine drugs, with fewer side effects. Zopliclone is marketed in the USA under the brand name Lunesta and is known by many different names all over the world including Imovane. Although widely prescribed for insomnia, these drugs carry with them the high risk of addiction, especially if used for long periods of time.\nAddiction to sleep medications can cause major problems, both physical and mental. Side effects of taking the drug can be to cause anxiety, nervousness and make addicts experience suicidal feelings. When Ambien is taken in doses that exceed the prescribed amount it has been associated with episodes of sleepwalking and extreme drowsiness the day after taking it.\nAmbien is designed as a short-term solution to sleeplessness. Those who have been prescribed the drug to treat insomnia are directed not to take the drug for longer than ten consecutive days. Although the drug aids sleep, there is evidence that the quality of sleep induced by Ambien is not as effectively restorative as natural sleep and can cause fatigue in someone who has taken it for extended periods.\nMixing with alcohol enhances the potency of these drugs, with potentially disastrous physical consequences.\nSomeone taking sleep medication long-term will develop a tolerance to it, meaning they must take more and more in order to sleep, leading to a risk of overdose.\nAmbien is the commercial name for the drug zolpidem tartrate. This drug may also be marketed under the name Stilnox. Zopiclone is marketed under a variety of different brand names worldwide. In South America, the medication is called Zalepla or Zetix, in the UK it is sold as Zimovane, in Ireland in might be known as Zileze, Zimoclone or Zorclone. In Australasia and some European countries it is marketed as Imovane or Imrest, in Germany Ximovan, in Egypt Hypnor and in Spain Limovan.\nRecreational users might refer to Ambien as ‘A-minus’, ‘zombie pills’ or ‘no-go pills’.\nThe drug Zoplicone may be referred to on the streets as ‘zimmers’, ‘zimmies, ‘zim-zims’ or ‘zoppies’.\nAs a depressant drug, sleep medications may also be generically referred to as ‘downers’ because of the way the combat the effects of stimulant drugs like cocaine and amphetamine (known as ‘uppers’).\nWhen prescribed for insomnia, sleeping medications like Ambien and Imovane have the effect of combating the causes of this condition and making the person taking it feel drowsy and fall asleep. In some cases, however, the drug can cause serious side effects. In the short term these might include weakness, feeling unsteady of the feet, a slowed heart rate, shallow breathing, nausea, vomiting, short-term memory loss and severe headache. These medications can also make someone feel ‘foggy’ as if they have been heavily drugged as well as causing twitching or shaking. In some cases taking Ambien can cause someone to sleepwalk, potentially putting them in harmful situations.\nIn most cases these side effects only occur if the drug is being abused and taken in doses that exceed the normal prescribed amount.\nBecause Ambien is meant for short-term use only, there are serious harmful effects that can arise if the drug is taken for longer periods.\nSomeone taking Ambien long-term may suffer from delusions, including thoughts of suicide. The drug may also impair the general judgement and reasoning abilities of the user. Long-term use of Ambien can lead to the body developing an ever increasing tolerance to the drug, which may lead to dependence as more and more is needed to induce sleep. This also increases the likelihood of overdose. A decrease in libido is another effect of long-term use of Ambien.\nAmbien can have the effect of impairing the motor functions of the user, making them unable to complete tasks that require a degree of coordination such as operating machinery or driving. Many of those who abuse sleep medication have difficulty talking properly, and will tend to slur their words and stagger around.\nSleeping medications based on Zopiclone like Imovane (aka Zimovane) may cause the side effects of indigestion, constipation, nausea and vomiting, diarrhoea, increased appetite and heart palpitations. Rashes on the skin may also form and, in rare cases, someone taking zopiclone will have a tingling sensation all over their body. In men, zopiclone may cause impotence or delayed ejaculation. Blurred vision and increased urination may also occur.\nSleep medication like Ambien, Stilnox and Imovane (or Zimovane) are legal prescription drugs that are manufactured under license in a wide variety of industrialised nations. The chemical zolpidem that is marketed as Ambien and Stilnox and is widely prescribed across the world, with the Netherlands being one of the countries where Stilnox is consumed the most. The patent for Ambien is held by the French pharmaceutical company Sanofi-Aventis, one of the largest companies of its kind. Sanofi-Aventis makes a variety of prescription and non-prescription drugs and vaccines and are represented throughout the world.\nThe drug zolpidem is also generically available in the USA and UK and is sold as a generic drug called Sandoz in South Africa, Ratiopharm in Germany and Teva in Israel.\nSanofi-Aventis is also the main manufacturer of the drug Zopiclone.\nSleeping medication like Ambien is produced exclusively by legitimate means and there are no cases of it being manufactured ‘on the streets’. As such all sleeping medications used for illicit purposes are those that are diverted from legitimate sources, i.e. from those who have been prescribed the drug to treat insomnia.\nIn countries with private health care like the USA, those addicted to sleeping medication often engage in ‘shopping around’ different doctors in order to increase their supply.\nIn the USA Ambien is the most widely prescribed sleep medication.\nSleeping medications are legally produced drugs that are intended for short term use in order to specifically treat the symptoms of insomnia. They are also abused by some who either take the drug in greater quantities or for a longer term than they have been prescribed. Here are some more essential facts about sleeping medications.\nAddiction to sleep medications can be gradual and difficult to detect. In many cases someone may have been prescribed Ambien for legitimate treatment of insomnia, but continuing to take the drug after the maximum recommended period can lead to dependency. This is because the body becomes tolerant of sleep medications, meaning that after a short while more and more is needed in order to get to sleep. Very soon a person regularly taking sleep medications can become addicted.\nSomeone dependent on medications like Ambien may show signs of constant fatigue or drowsiness and seem confused or ‘cloudy’ in their thinking. They may start slurring their words or show signs of a loss of control in their muscles. They may lose their balance often or develop a twitch. Someone addicted to Ambien may also display reduced or shallow breathing and their heart rate might be reduced as well. Addiction to sleep medication like Ambien can also stunt someone’s sexual urges.\nAmbien medication is not cheaply obtained, so someone with a dependency on the drug may show signs of financial difficulty and have trouble meeting their payments for other things. An Ambien addict who has difficulty obtaining the drug will often not be able to sleep at all for those periods they are without Ambien, so signs of sleeplessness like tired, bloodshot eyes, skin pallor and increased sweating might be observed.\nIn order to hide their sleep medication addiction, some will withdraw and detach themselves from friends, colleagues or family members. They may display signs of severe depression including suicidal thoughts which they may talk about.\nAmbien abuse affects short term memory so addicts will often have difficulty remembering things, missing appointments and so on.\nAnother sign of Ambien addiction may be frequent episodes of sleepwalking as the drug decreases the latency of ordinary sleep, despite lengthening its duration.\nOnce someone has become addicted to sleeping medication like Ambien, it is essential they seek professional help in order to manage their detoxification and rehabilitation. As soon as someone who has built a dependency towards Ambien stops taking the drug they will begin to experience unpleasant and painful withdrawal symptoms that, for their own welfare, should be managed under clinical observation and with medical support. Suddenly stopping taking sleep medication can result in seizure and, in some cases, death.\nWithout such medical support there is a risk that the person suffering withdrawal may turn to other drugs to alleviate their symptoms, which may include nausea, vomiting, dizziness, stomach cramps, sweating and convulsions. Attempting to treat these symptoms without advice can have potentially disastrous consequences.\nMedically supervised detoxification programmes will safely taper off the dose of Ambien to aid the comfort of the addict. During this time appropriate medication may be prescribed to relieve symptoms of withdrawal, including anti-depressants. A full medical examination of the addict is carried out in order to discover if there is any medical cause for the insomnia that the addict may have been attempting to treat with Ambien, that had not been previously identified. Throughout the initial detox phase, the addict’s nutritional intake is also strictly monitored and some foods or drinks may be banned, particularly those containing stimulants like caffeine.\nIn some cases a different sleep medication drug to the kind an addict has developed a dependency to may be substituted, for example a benzodiazepine in the case of non-benzodiazepine drug like Ambien. The detoxification stage of a rehabilitation programme will be tailored to an addict’s individual needs based on the severity of their addiction.\nThe initial detoxification phase of an addict’s rehabilitation is normally carried out in a residential centre and may last between one and two weeks. After detox, rehabilitation may continue with the addict either as an inpatient or an outpatient.\nThe benefit of inpatient treatment for sleeping medication addiction is the structure that a clinical facility can provide for every aspect of an addict’s rehabilitation. Private and group therapy sessions may be carried out intensively without distraction or temptation from the outside world. In this way there is a greater chance of complete recovery.\nAn outpatient programme can allow the addict to resume their daily life responsibilities including employment, whilst still engaging in support therapy. It is important to note, however, that rehabilitation is only effective in the long term when an addict fully admits to having a problem with the drug.\nTherapy sessions are designed to resolve the underlying issues that created the conditions for the abuse of Ambien or other sleeping medications and to develop coping strategies of dealing with life without recourse to abusing such drugs. Behavioural therapy will show the addict how to replace negative behaviours with positive ones.\nGroup therapy sessions, which are offered by a number of organisations may also be helpful long term. These are where ex-addicts can share their experiences and encourage others to remain drug-free.']	['<urn:uuid:322f948d-cb2d-4efc-9e2a-689090b52fb3>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T11:40:26.317848	7	22	2017
9	what climate regions exist peru	Peru has three distinct climate zones determined by the Andean mountains. These include the tropical Amazon jungle in the east with equatorial climate and heavy rainfall (39-157 inches annually), an arid coastal desert in the west that gets cooler towards the south, and an alpine/sub-alpine mountainous climate at elevations over 11,500 ft where temperatures decrease with altitude.	['Peru is known as the “Land of the Incas” since it was home to the once thriving pre-Columbian civilization that dominated the region for several hundred years. It was the jewel of the Spanish Empire, which held the country for more than 300 years. Peru contains some of the finest archaeological remains in the world along with sheer natural beauty, making it an altogether wholesome travel destination. The great Andes mountains dictate the landscape, while sloping down to the east is a thick rainforest and the mighty Amazon River. To the west spans a long Pacific shoreline, much of it desert, but there are popular surf spots along the coast.\nA trip to the lost city of Machu Picchu is a global phenomenon. The immaculate remains and the ambience of being steeped in cloud cover on a mountain top make it an ethereal experience. Many people chose to reach the site by hiking the Inca Trail, a multi-day trek which takes adventurous travelers to a number of Inca sites before reaching the Lost City. Further intriguing discoveries await at the Nazca lines, where a flight over these ancient markings provide intriguing insight into past lives.\nConsidering the vastness of the Peruvian Andes, trekking opportunities in the white snowy peaks are endless. For those less active, you can take a train up one of five lines through the Andes to still benefit from the scenery. If you ride into the altiplano, you will be awarded spectacular views of Lake Titicaca, the world’s highest navigable lake. The journey between Cusco and Puno is considered to be one of the greatest train rides in the world.\nWhite sandy beaches on the Pacific coast provide a relaxing atmosphere in warm tropical climes. You can go surfing on the northern beaches or sand-boarding down the world’s tallest dune near Ica. Food in Peru is renowned for being well-received by visitors, and the country’s capital, Lima, has an array of top-notch restaurants. Accommodations vary from western-style hotels in the larger cities to humble rustic cabins in the mountains and eco-lodges in the jungle. Wherever you go, Peruvians are known for their great hospitality.\nThe Andean mountains run down the spine of the country and dictate Peru’s three climate zones. To the east is the tropical Amazon jungle, significant of an equatorial climate. To the west is an arid, coastal desert - although things cool off the further south you travel. In the rainforest, conditions can get wet, with between 39 and 157 inches of rainfall per year feeding the rivers. A huge amount of Peru is within an alpine or sub-alpine mountainous climate, with altitudes over 11,500 ft. As a general rule, the higher you get, the colder the weather.\nThe country is easy to get around, with airports serving all major cities and attractions. The road network is developed, with a good number of private bus companies. Distances can be decieving though; and a journey from Lima to Cusco can take 24 hours due to the mountainous terrain. To cover this same distance by air is just a short hour long trip, and domestic flights are abundant.\n- Visit the magnificent Machu Picchu, the ancient lost city of the Inca civilization\n- Hike the culturally significant Inca Trail on a multi-day journey to see history in the making\n- Fly-over the mystical Nazca Lines and be bedazzled by their intriguing pictograms\n- Go rafting through the rainforest on a tributary river in the Amazon\n- Sand-board down the world’s tallest dune at Ica\n- Trek the snowy peaks of the high Andes on the Cordillera Blanca\n- Wind-down at sunset at Lake Titicaca, the world’s highest navigable lake\n- Ride the world-famous railway between Cusco and Puno for awe-inspiring views of the altiplano\n- Take a jungle trek in Manu National Park, a globally important biodiversity hotspot']	['<urn:uuid:723dd42f-d275-40ae-8aca-22c58a0562f5>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T11:40:26.317848	5	57	642
10	minimum wall thickness requirements fdm printing vs carbon fiber htpla comparison thickness limits	For FDM printing, the minimum wall thickness varies by build layer: 0.028 inches (0.71mm) for 0.007 inch builds, 0.040 inches (1.02mm) for 0.010 inch builds, and 0.052 inches (1.32mm) for 0.013 inch builds. For Carbon Fiber HTPLA, the resolution is limited by the maximum carbon fiber particle size of 0.15mm.	"['Fused Deposition Modeling (FDM)\nWhat is FDM?\nStratasys’ co-founder Scott Crump invented the Fused Deposition Modeling (FDM) technology and process. He patented FDM in 1989. FDM uses a thermoplastic filament that is heated then extruded layer by layer to create a 3D dimensional object.\nToday, FDM is the most widely used 3D printer because of the ease of use and that it runs engineering grade plastics. Over the last 10 years, FDM has become more widely known and affordable by providing desktop 3D FDM printers as well as industrial grade machines. Currently, there are hundreds of available 3D FDM printers in all different sizes, for different materials and applications.\nFDM runs 24/7 with connections to provide real-time status to our AM technicians. The ease of material changeouts allow flexibility to run all different materials. This flexibility, along with the minimal finishing required, allows the ability to turn projects around in as little as 1 day. The Stratasys Fortus machines are made for repeatable, industrial printing and provide the best parts and best finishes FDM can offer.\nMost FDM parts are used for fit and function prototypes, vacuum form templates, testing, jigs & fixtures and production parts. Engineers often choose the FDM printer because printed part strength can be roughly 75% a molded part of the same material (ex ABS).\nHow does FDM work?\nThe FDM printer builds a 3D model of a component from a 3D CAD file using a thermoplastic filament. That filament is then heated and extruded or deposited layer by layer until the part is complete. There is typically a heated nozzle for both support material and thermoplastic material. The machine reads the layer slice then deposits the support material and plastic materials as required. After the layer is complete, the build platform is lowered and the next layer is started. When the build is complete. The FDM parts are removed from the machine, removed from the platform and support material is removed often by hand or small tools.\nFDM Build Layers\n- .007” Build Layers – Best part finish, accuracy, detail, and strength but increases cost and build times*\n- .010” Build Layers – Best balance of finish, accuracy, detail, strength with cost and build times\n- .013” Build Layers – Best for quick low-cost parts but finish, accuracy, detail, and strength are not as great\n- Tough & Durable Prototype Parts\n- Real Thermoplastic Materials\n- End-Use Production Parts\n- Ease of Use\n- Test Parts Production Materials\n- No size limitation – Section and Bond Large Parts Together\nFDM Best Uses\n- Fit Checks\n- Material Testing\n- Master Patterns\n- Durability Testing\n- End Use Production\n- Heat Resistance\n- UL 94-Rated\n- Color Printing\nNot sure which additive process is best for your application?\nDownload our Best Additive Uses Chart (pdf).\nFDM Printer & Equipment\nLarge Industrial FDM Machines (Fortus 400 & 900)\nThese FDM printers have large build platforms from 16” x 14” x 16” (400mm x 350mm x 400mm) to 24” x 36” x 24” (900mm x 600mm x 900mm). These machines can build parts at different layers of .007”, .010” and 0.013” (125-330 micron) layers depending on the material chosen and requirements for parts. These are great for building large parts or loading up lots of smaller parts.\nDesktop 3D Printers\nThese 3d printers have smaller build platforms, roughly 11” x 7” x 6” (280mm x 175mm x 150mm). Most printers print in layers of 0.004”-0.010” (100-250 micron). Desktop printers are great for home projects, schools, and engineering offices. Parts are great for fit & function or gadgets.\nTTH offers (4) standard FDM materials based on the requirements of our customers and industries. Other materials are available for special requirements. Here is a list of standard materials offered:\n- BEST for STANDARD PROTOTYPING or ABS PRODUCTION PARTS\n- White ABS Material (available in other colors as requested)\n- Soluble Supports\n- Applications: Form, Fit and Function Prototyping, Master Patterns for Vacuum Castings, Testing, and Production\n- BEST for TOUGH & RIGID PROTOTYPES or PC PRODUCTION PARTS\n- White PC Material\n- Soluble Supports\n- Applications: Form, Fit and Function Prototyping, Jigs & Fixtures, Master Patterns for Vacuum Castings and Production\n- BEST for TOUGH & RIGID PRODUCTION MEDICAL COMPONENTS\n- White PC Material\n- Applications: Medical Device Components & Parts\n- BEST for HIGH HEAT PROTOTYPES & PRODUCTION PARTS\n- Ultem Material Colored Black or Tan\n- Tough, Durable & Rigid\n- Applications: Aerospace & Medical Parts, UL94V-O Rated Parts, High Heat Applications\nDon’t see the materials you want to use? It may be available with another additive process.\nDownload our Materials Availability Chart (pdf).\nFDM Finishes Available (*Standard*):\n- *Level 1 – Quick Finish = Support Removal Only*\n- Level 4 – Primer Finish = Sand/Remove/Fill All Build Lines & Primer\n- Level 5 – Painted Finish = Sand/Remove/Fill All Build Lines, Primer, and Paint/Mask/Texture as Needed (Show Quality Finish)\n- Metal Finish = Level 1 with Primer & Plated with Chrome, Nickel, Brushed Nickel, Gold, or Other\n- Minimum Wall Thickness & Feature Size\n- .007” Build = .028” (.71mm)\n- .010” Build = .040” (1.02mm)\n- .013” Build = .052” (1.32mm)\n- Tolerances = +/- 0.010” for first inch then +/- 0.002” per inch thereafter\n- Inserts = Preferred for threads. Install in Post-Processing with Heat Stake or Adhesives\n- Holes = Drill, Ream, and Tap (Print threads then chase or ream in finishing.', 'For exceptional accuracy, finish & performance, choose Carbon Fiber\nProto-pasta Carbon Fiber Composite HTPLA is a combination of milled carbon fibers and high-performance PLA. Resulting 3D printed prototypes and end-use parts are characterized by exceptionally stability of form and potential use up to 155 deg C (310 deg F) when heat treated.\nAdaptable to most PLA-compatible printers. Heated bed recommended for process ease, quality, and reliability, but not required. Printer should allow 3rd party filament, parameter adjustment, and nozzle replacement. Specialized machine adaptation and maintenance may be required for Proto-pasta materials particularly in continued use of abrasive materials.\nPlease consider all information below before purchase and use. More on getting started. More about our release of Carbon Fiber HTPLA.\n- Base material: Heat treatable PLA w/ high temp resistance\n- Characteristics: low odor, non-toxic, renewably-sourced\n- Molecular structure: Amorphous or partially crystalline\n- Amorphous as printed, part crystalline when heat treated\n- Melting resets crystalline structure to amorphous state\n- Additives: 10% by weight high-purity, milled carbon fiber\n- Max particle size: 0.15 mm (may limit resolution)\n- Density: approx. 1.3 g/cc\n- Length: approx. 360 m/kg (1.75 mm) & 136 m/kg (2.85 mm)\n- Min bend diameter: 40 mm (1.75 mm) & 100 mm (2.85 mm)\n- Glass transition (Tg) onset: approx. 60 deg C (140 deg F)\n- Melt point (Tm) onset: approx. 155 deg C (310 deg F)\n- Max use: Tg for amorphous, Tm for crystalline\n- Use limit is geometry, load & condition dependent\n- Material Safety Data Sheets (MSDS)\n- Hold filament end when unwrapping spool to avoid looping\n- Protect eyes when handling filament, particularly 2.85 mm\n- Coiled filament stores energy & may try to unwind\n- Do not bend tighter than min bend diameter (in tech specs)\n- Over-bending filament can cause breakage & projectile\n- When done printing, secure filament end to avoid looping\n- Store in cool, dry place away from UV light for peak performance\nProp 65 Warning! May cause cancer or reproductive harm.\nHowever, in an effort put this statement in perspective, consider our blog on this subject to better-understand actual risks.\nRoHS compliant - does not contain Cadmium (Cd), Lead (Pb), Mercury (Hg), Hexavalent Chromium: (Cr VI), Polybrominated Biphenyls (PBB), Polybrominated Diphenyl Ethers (PBDE), Bis(2-Ethylhexyl) phthalate (DEHP), Benzyl butyl phthalate (BBP), Dibutyl phthalate (DBP), or Diisobutyl phthalate (DIBP)\nRegarding food contact - though base resin may be safe for food contact, our process & additional ingredients may not be. Thus our materials are not certified for food contact even if the risk is low. Please consider additional coatings, treatments & testing before pursuing extended food contact or certification.\nRegarding skin contact - Not a known skin irritant, however, avoid recommending prolonged skin exposure without further testing.\n- Available in 1.75 & 2.85 (3.00) mm diameters\n- 50 g loose coil, 500 g on cardboard spool & 3 kg on plastic spool\n- Cardboard spool weight: up to 100 g\n- Cardboard spool dims: 20 cm dia x 6 cm wide w/ 5 cm opening\n- Carefully remove cardboard spool sides for Masterspool use\n- Loose coils also Masterspool compatible (instructions for use)\n- More details about Masterspool including download\n- Recycle cardboard spools locally\n- 3 kg spools weight: up to 1 kg weight\n- 3 kg spool dims: 30 cm dia x 10 cm wide w/ 5 cm opening\n- Return plastic spools for re-use\nSome machines may require specific considerations for filament placement, path, adjustments, settings, or other preparation & maintenance.\n- Spool should unwind with minimal resistance\n- Mount filament above machine or other unobstructed position\n- Filament path must not bend tighter than min bend diameter\n- Filament should remain clean, dry & dust-free\n- Check weight before printing to avoid material run out\n- Be careful not to place printer too cool or warm environment\n- Clean print surface with alcohol or water\n- Apply & reapply appropriate adhesion aid as required\n- Carefully control first layer gap for adhesion without jamming\n- Limit build rate to balance melting, cooling & movement limits\n- Adjust layer fan to balance part & nozzle cooling\n- Isolate/insulate heater block from layer fan for consistent heating\n- Maintain & replace nozzle & other components when worn\nAbrasive materials like Carbon Fiber and Metal Composites may cause premature wear of in line components such as bowden tubes, drive gears, nozzles, and other items in the filament travel path. Serviceable hardware including replaceable nozzles suggested. Wear resistant nozzles are recommended for extended use. Nozzles wear most quickly with flattening of the tip which affects nozzle diameter & distance to build plate. Inconsistent extrusion, inaccuracy & process instability. Extrusion width & first layer distance adjustments and/or replacement of nozzle. For more on nozzle replacement consider this blog demonstrating nozzle replacement & adjustments on a Prusa MK3. Reduce nozzle wear by minimizing over-extrusion & infill.\nProduct label suggest temperatures as a guideline based on typical nozzle set points. Appropriate settings can vary widely & in given good conditions, a wide range of temperatures can yield positive results. With relatively low print rates on hardware without hangups, HTPLA prints well at the low side of the recommended range. With high print rates on machines with hardware hangups, higher than labeled temperatures may be required for consistent extrusion. In some cases, oiling filament makes the difference between success & failure.\nOne specific problematic example is the Prusa MK3 which, by design, has a Prusa-specific heat break with an internal ledge that material can get caught on. To reduce need for nuance & risk of jamming, users should either replace the Prusa-specific heat break with a standard e3d v6 one, oil filament to help it slip past the Prusa-specific ledge, or print at an unusually high temperature. The trade off with high temperature as a solution is you should also match that with a high volume flow rate. Sounds great, right? It\'s okay except for the loss of detail when having to slow down for small part, fine feature, or high resolution printing.\nPrusa MK3-specific, Carbon Fiber HTPLA process recommendations:\n- 1st layer temp to overcome jamming on Prusa MK3: 255 deg C\n- Min temp to avoid jamming @ 9 cubic mm/s: approx. 240 deg C\n- Max recommended volume flow @ 240 deg C: 9 cubic mm/s\n- Min recommended volume flow @ 240 deg C: 1.5 cubic mm/s\nVolume flow = extrusion width x layer height x speed in mm.\nFor example, 0.5 mm extrusion width & 0.2 mm layer height for speed 20-90 mm/s.\nPoorly cooled cold sides of all metal hotends can yield a similar result & benefit from similar fixes to the Prusa MK3. Aggressive layer fans not isolated from heat blocks and/or nozzles can make for a jammy combination as well. Finding the balance between enough cooling fan when printing fast & a high enough nozzle set-point can be challenging. More isolated hotends with PTFE liners can allow slower printing with lower set points for more detail with less aggressive layer fan settings. Also, insulating your heater block and/or nozzle with a sock can help avoid unwanted layer fan cooling. Rapid changes in speed or print rate should also be avoided whenever possible.\nFor more on the subject of printing, consider our getting started guide.\nHTPLA is a semi-crystalline grade of PLA optimized for heat treating (also known as annealing or crystallizing) for higher temperature use. Without heat treating, ""as printed"" amorphous PLA loses significant stiffness (and the thus the ability to retain form) as the material approaches it\'s relatively low glass transition temperature. Heat treating creates a more crystalline molecular structure for maintaining stiffness to near melting, thus extending the useful range of HTPLA, but crystallization also creates shrinkage. HTPLA parts should be scaled in slicer to compensate for shrinkage when heat treating.\n- Typical heat treat temp: 95-110 deg C (200-230 deg F)\n- Typical heat treat time: 10+ minutes\nA large range of temperatures & times can yield acceptable results. With translucent grades and thin wall parts like a single wall vase, you can see a visual change from transparent to opaque begin in as little as 3 minutes with a full transition to opaque in 7 minutes. Parts with more mass will take more time. What\'s important is the core temp and time to ensure a comprehensive change in material structure to crystalline throughout the part.\n- Typical change in heat treating: -0.6% x/y, +1% z\n- Slicer scale in heat treating:1.006 or 100.6% x/y, 0.99 or 99% z\nHere\'s a demonstration of measuring shrinkage, determining change, and applying compensation in printing. Here\'s additional demonstration of application of scale, heat treating, and validation of form.\nAdditional post processes might include sanding or painting. The addition of Carbon Fiber can lend well to ease of sanding and adhesion of coatings like paint, however, there are also additional safety considerations when generating dust through sanding and fumes through coating. Please seek safe practices with appropriate personal protective equipment (PPE) and ventilation.\nExamples & Uses\nPrusa Extruder Assembly Housing\nHardinger Fiddle Musical Instrument\nFilament Testing Grips']"	['<urn:uuid:f93837c3-24bf-462f-aef5-472752a1a05b>', '<urn:uuid:0a72a8f7-d897-4570-bf0b-7a65e24dc76e>']	factoid	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-12T11:40:26.317848	13	50	2431
11	benefits of passive heating compared to mechanical heating systems home	Passive heating offers two main benefits: 1) It is cheaper in the long-run by eliminating costs of operating, maintaining and replacing heating systems over 25 years, despite some higher upfront construction costs. 2) It provides better comfort by maintaining consistent temperatures throughout the home, avoiding cold spots that can occur with mechanical heating systems, particularly in bedrooms at night.	"[""Passive Heating & Cooling\nThe house was designed to be warm without the need for heating, which realises two main benefits:\n- Financial. While some of the construction costs a little more up front, eliminating heating systems and the need to operate, maintain, and replace them over the next 25 years makes passive heating a cheaper option in the long-run.\n- Comfort. One issue with relying on mechanical heating systems for a home is that temperatures can be variable within the home. For example, heating systems might keep the living areas warm but then at night people retreat to bedrooms that are cold overnight. Designing a house to be warm throughout without heating eliminates these cold spots.\nPassive heating has been achieved through the following methods which are explained in detail on other pages on the website:\n- Solar access. The building has been located and orientated on the site to make the most of the site-specific sun angles. The internal layout of rooms has also been planned around which rooms need sun in the morning (kitchen), throughout the day (living areas), and evening (bedrooms). Heat is captured during the day and released overnight via a concrete slab thermal mass on the ground floor.\n- Windows. Most of the glazing is on the north side of the house to optimise daylight and solar gain, with glazing on the south side is minimised to reduce heat loss.\n- Walls. Heat is retained in the building through both high-performance glazing and a double layer of insulation, which takes the building to 50% above building code thermal performance.\nDesigned shading minimises overheating in summer and maximises solar gain in winter.\nShading is used to control solar gain into a building as and when required, typically in the summer time but also in the shoulder seasons. Without shading internal temperatures can reach uncomfortable levels, so shading needs to be considered along with ventilation for optimum temperature control on warm days.\nTypical shading features come in the following forms:\n- Vegetation, and\nIn summer the mid-afternoon western sun is the most warming and therefore in our house design we have kept western glazing to a minimum. In winter the early afternoon western sun is the most warming and in our house will enter through the northern glazing (as the sun trajectory in winter is further to the north).\nWe have included the following shading options on our house:\n- Fixed overhangs to reduce midday sun on the northern side, and\n- Corner fins to reduce some of the eastern sun on the northern elevation.\nWe are also considering sliding shades along the north elevation that can be extended, contracted, or moved as required, and vegetative shading on the eastern elevation to help reduce low level sun during the summer months. These shading features will be developed when we have lived in the house for a year and have a good idea how it performs in the summer. Whilst a computer model can be used to do this it cannot tell you what conditions you find comfortable.\nPlanning natural ventilation at the design stage avoids the need for costly ventilation systems.\nVentilation is important for the following reasons:\n- It removes unwanted heat from internal spaces on warm days. The movement of air over skin also helps to keep people cool.\n- It helps to provide fresh air to a space and remove stale air.\n- Building and furniture materials emit chemicals into the air which can be harmful to human health. Ventilation helps to remove these chemicals from the house.\n- It also helps remove moisture from the house - from cooking and showers, but also from people.\nOur design decisions on ventilation of the house were covered in the Passivation blog post. We decided to keep the ventilation simple but effective and use natural ventilation throughout with mechanical ventilation in high moisture areas such as bathrooms (extractor fan) and kitchen (rangehood).\nWe have included the following features to provide good natural ventilation:\n- Windows that can be left securely open throughout the day, even when the house is unoccupied,\n- Opening windows in every room, and\n- Opening windows on opposite sides of the room where possible to allow cross-ventilation.\nSecure opened windows allow ventilation throughout the day, avoiding overheating and a stuffy house in the evening.\nFor natural ventilation to be effective we've had to think about how we use the house. This is fairly easy for most of the year, but needs a little more consideration for the winter to maintain reasonable internal temperatures. For example, opening the windows when there are high internal gains (like when you have all your mates around or are cooking a big dinner) or on the morning of a sunny winters day when there will be sufficient solar gain to reheat the house.\nIn order to remove moisture in the house the following practises need to be considered:\n- Turning on the mechanical ventilation system as soon as moisture is produced, e.g., turning on the kitchen rangehood when cooking or the bathroom extractor fan when having a shower.\n- Keeping steaming cooking pots near the kitchen exhaust.\n- Opening windows in the kitchen when steam is being produced away from the kitchen exhaust, for example when pouring hot cooking water into the sink or washing up.""]"	['<urn:uuid:54f6a36d-e418-4792-bfec-a7aeb803e3d8>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T11:40:26.317848	10	59	890
12	how did competitive lawn mower racing originate british history establishment	Lawn mower racing originated in 1973 in Wisborough Green, West Sussex, England. It was founded by Jim Gavin, an Irishman and former car racer who had become disillusioned with the high costs of motorsport. While at the Cricketers Arms pub, Gavin and some locals saw a greenkeeper mowing the cricket patch and realized everyone had a lawn mower in their garden shed. They organized a Championship in a local farmer's field, which attracted about 80 competitors. This led to the formation of the British Lawn Mower Racing Association (BLMRA), which remains a non-profit organization that donates profits to charities while providing affordable motorsport with a strong social atmosphere.	['LAWN MOWER RACING HALL OF FAME\nThe USLMRA National Lawn Mower Racing Hall of Fame & Museum of America is located at Miller Lawn and Power, 1299 Harding Highway East State Road 309, Marion, Ohio.\nEvery legitimate sport and endeavor has a Hall of Fame. Now lawn mower racing takes its’ rightful place along such noble American pastimes as baseball, country music, motorcycle racing, pinball, juggling, trivia and quilting with a place to honor ingenuity and mowtivation. —Bruce Kaufman, Founder and President of USLMRA\nNational Lawn Mower Racing Hall of Fame Inductees\nThe Organizing Committee of the Twelve Mile 500 in Twelve Mile, Indiana\nThe Twelve Mile 500 is the home of America’s oldest organized riding lawn mower race. Founded in 1963 and hosted by the Twelve Mile Lions Club, the annual mow down takes place on July 4th each year. 33 mowers run 60 laps around a 1/4 mile track covering a distance of 15 miles, a bit longer than the original distance of 48 laps – twelve miles – due to increased speed in recent years.\nJim Gavin of Wisborough Green, West Sussex, England\nFounder and General Secretary of the British Lawn Mower Racing Association (BLMRA), founded in 1973 at the Cricketers Arms in Wisborough Green, West Sussex. Gavin, an Irishman who had raced and rallied cars across the world, became disillusioned with the increasing costs of motorsport. Whilst downing their pints in the village pub, Gavin and a group of locals looked across the village green to see the green keeper mowing the cricket patch. They realized that everyone had a lawn mower in their garden shed, so a Championship was held in a local farmer’s field. About 80 competitors turned up for what turned out to be a very successful event and the formation of the BLMRA. The BLMRA still sticks to its origins as a non-profit organization (any profits are given to charities or good causes) while providing well organized and inexpensive motor sport with a strong social atmosphere.\n“Racing Ray” Mrocks of Lake Zurich, Illinois\nThe first member of the USLMRA heeded the call to sod slinging glory on April 1, 1992, the birthday of national lawn mower race sanctioning in the United States. “Racing Ray” tore up the turf from 1992-2001, claiming BP Class Points Championships on his mower Sodzilla 1995-1997. “Racing Ray” served as Chief Technical Inspector, race course builder, rule maker and friend to lawn mower racers far and wide. “I like lawn mower racing because it’s fun and affordable,” said Ray in 1997. “Come to think of it, I like anything that is fun and affordable, that’s why I gave up my marriage, but I’m still racing lawn mowers.”\nChuck Miller of Marion, OH\nThe only USLMRA member to compete in every STA-BIL Nationals since 1992. The “Boogie Woogie Man” is a multi-year racing champion and is a Charter USLMRA Member. His record of consecutive starts in the annual STA-BIL Nationals is unmatched. Miller won the 1st BP Class Championship at the 1992 STA-BIL Nationals and the Factory Experimental Championship in 1993. In 2008 he took 2nd place in the STA-BIL Challenge of Champions Race. “I’ve seen the growth of the USLMRA since the first race,” said Miller. “It amazes me to see how this sport has grown so fast and how it has become part of American culture. I’ve stayed competitive over the years, and although I’m getting older, I’ve still got my MOWJo!”\nKen “Turfinator” Smolecki of West Chicago, Illinois\nHis mower, The Turfinator, was featured on the hit TV sitcom Home Improvement for the famous jet-powered lawn mower race pitting Tim Allen vs. Bob Vila. Smolecki joined the USLMRA in 1992, raced until 2000, served on the Rules Committee and as Technical Director and raced in the legendary British Lawn Mower Racing Association 12-Hour Race where he was honored as the first American to wave the checkered flag (1999).\nBill Simpson of Trevor, Wisconsin\nServed as the first USLMRA Race Director, adding consistency to race officiating by directing a staff of race stewards, technical directors and flaggers. Simpson created a Race Operations Manual, served as racer liaison, handled venue negotiations, implemented insurance programs and organized an annual Rules Committee Meeting.\nJohn Nelson “Sir Lawns-A-Lot”, Richardson, Texas\nAs a member of the USLMRA Steering Committee, Past President of the Lone Star Mower Racing Association (LSMRA) and the nations’ first “Mow Ambassador,” “Sir Lawns-A-Lot” epitomizes the spirit and unique culture of lawn mower racing. Also known as John Deere Nelson, John competed in five racing classes from 2001-2010. John still loves to show off in his John Deere 210, 14-foot, 5-engine machine he calls “Mow Longer.”\nDr. Art Neavill, Commerce Township, Michigan\nDr. Art Neavill is beloved for his spirit, love of competition and eagerness to help fellow racers. Just before his retirement from the Michigan public school system, Art became a lawn mower racer, touring the country with his son-in-law and wife by his side, while intermittently battling cancer. Lawn mower racers across the country have benefitted from Art’s generosity. Be it a spare part, extra engine or entire lawn mower, if Dr. Art is in the pits, what’s his is yours.\nCharles Powell – “Mr. Mowjangles” (1950-2006), Forney, Texas\nThe late Charles Powell was USLMRA VP of Operations, LSMRA Founding President, Winner of the 2005 Pain in the Butt Award and simply, Mr. Mowjangles. He was a MOWtivated racer, chapter president and race steward. The annual USLMRA Charles Powell Mr. Mowjangles Award was named in his honor.\nBruce Kaufman, Mr. “Mow It All”\nAs Founder and President of the USLMRA, Bruce Kaufman of Northbrook, Illinois is on the “cutting edge” of American motorsports and has been kicking grass since Day One. With tongue planted firmly in cheek, the man known as “Mr. Mow-It-All” leads the USLMRA, the nations’ oldest and largest national lawn mower racing sanctioning body, and is responsible for association management, event and TV production, media relations, marketing, sponsorship and finance. He also serves as TV color commentator on the Fox Sports Net “Race Freaks” Lawn Mower Racing telecasts, where his “Mowtivation” is on full display.\nGold Eagle® Company, Makers of STA-BIL® Brand Fuel Stabilizer\nIn 1992, Chicago-based Gold Eagle® Co. brought lawn mower racing from England to America as a promotion for STA-BIL®, America’s top selling fuel stabilizer. With the support of the British Lawn Mower Racing Association, Gold Eagle® Co. founded the USLMRA to sanction and present organized lawn mower races for fairs, towns and non-profit organizations across the USA. With one STA-BIL National Championship race in 1992, the STA-BIL Lawn & Garden Mower Racing Series has expanded to an annual 20-race circuit while the USLMRA sanctions 50 Chapters and Clubs nationwide. Gold Eagle® Co.’s 20 years of support is one of the longest consecutive sponsorships in all of American motorsports. Gold Eagle® Co. and the USLMRA have won numerous awards including accolades from Brandweek Magazine, The Publicity Club of Chicago, The International Festivals and Events Association, Inside PR magazine and the Public Relations Society of America.\nKevin “Cooter” Penne, Lake Villa, Illinois\n21 years ago Kevin Penne was a spectator at the inaugural STA-BIL Nationals in Grayslake, Illinois on Sept. 5, 1992 when he realized he was on the wrong side of the fence. By the following year he built his first racing mower and began lawn mower racing career, which has seen him become one of the most winning drivers in STA-BIL Racing history. “Cooter” is a pioneer of many the innovations used in the sport today. He designs and builds his own racing engines and chassis, is a 12-time STA-BIL Series National Champion, holds numerous track records and has been featured on Discovery Channel, ESPN2, Fox Sports Net and many TV news programs. Penne has also been banned at every horse track in Ohio.\nMike “Hedge Hog” Miller, Kenosha, Wisconsin\nLike Penne, Mike Miller was also a spectator at the first STA-BIL Nationals in 1992 and also knew he had to build a mower and compete. In 1993 Miller met Penne at their first race and one of the longest-lasting friendships in the USLMRA was formed, with “Hedge Hog” driving his 1968 T.S. Rugg to STA-BIL National Championships in 1996, 2005 and 2007.\nBobby Cleveland “The STA-BIL Engine Answerman”\nBobby Cleveland, aka STA-BIL’s Engine Answerman, hails from Locust Grove, Georgia, where he’s always loved to ride motorcycles, race lawn mowers, build hot rods and tinker in the garage. His passion for motors and what makes them work runs as deep as his appetite for Southern BBQ, sweet tea and being on the road.Bobby is one of the most decorated Lawn Mower Racing Champions in the history of the STA-BIL National Lawn Mower Racing Series, having won more than 75 First Place victories and taken 9 STA-BIL Series National Championships. He’s also the World Land Speed Record on a Lawn Mower, and is the builder of World’s First “Monster Mower” and the World Record Holder for Monster Mower Jumping!\nUSLMRA Vice President, and Director of Timing & Scoring, Kerry Evans has served the USLMRA for 18 years as Race Director, Engine Builder, Tech Advisor and “Mow To Guy.” Based in Wilmer, AL, Evans runs “Mow South,” the USLMRA’s southern office and serves as an advisor to the Dixie Outlaws, one of the founding USLMRA local chapters.\nUSLMRA National Chief Steward and Technical Director, Manny Torres is responsible for racer safety, rules writing and enforcement and race day operations. Based in Port Deposit, Maryland, Torres has served the USLMRA since 2002. He is a master mechanic, engine builder and racing champion.']	['<urn:uuid:5f09035c-b12c-4a04-8c52-40bf302caa28>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T11:40:26.317848	10	108	1599
13	I teach new riders. What makes weather more dangerous for motorcyclists?	Weather conditions affect motorcyclists more severely than other motorists. High winds, snow, ice, and low sun can impact motorcycle handling and rider concentration. Rain significantly reduces tire grip, especially on painted road markings and drain covers. Weather can also affect riders by making them too hot, too cold or wet, which impacts concentration.	['The previous elements will have seen you learning practical training and theory off-road at your training site. Element D is classroom based theory and will prepare you for final the on-road element.\nElement D will last for around 45 minutes and will further your knowledge on legal and safety aspects of riding a motorcycle on public roads. Detailed in this section are the various topics you can expect to cover with your trainer in the classroom-theory based element D.\nYou will have already covered some of the legalities involved with riding a motorcycle on public roads at the beginning of the CBT course. Your trainer will run the provisional motorcycle entitlements and motorcycle legal requirements such as insurance, tax and MOT testing. Other legal requirements discussed are safety helmets and displaying L-plates as a learner rider.\nA significant danger of any motorcyclist is to not be seen. Being conspicuous is essential for other road users to see you. This topic will see your trainer discuss situations where you may not be seen and how you can make it easier to be seen. this will cover:\n- Visual aids and the differences between fluorescent (daytime visual aid) and reflective (low light visual aid)\n- Use of headlights\n- Use of indicators\n- Road positioning\n- Changes in road surfaces\n- Keeping motorcycle / lights clean\nRules of the Road\nThe Highway Code is a reference in paper book or digital format that all road users must read. When applying for a provisional licence, you will have agreed and signed confirming that you have read the Highway Code. You’ll be expected to have a reasonable level of knowledge on the rules of the road before attending your CBT.\nAs a motorcycle rider, you are more vulnerable than many other motorists due to the lack of protection. Your trainer will explain many of the dangers associated with riding a motorcycle including:\n- different types of road surfaces and how they affect the motorcycle\n- weather conditions\nTo help in reducing injuries and the effect of weather conditions on the body, your trainer will also explain:\n- legal safety helmets that must be in good condition\n- appropriate protective clothing\nDefensive Driving and Hazard Perception\nDefensive driving is a skill that enables you to see potential hazards and developing hazards well in advance and to take appropriate action. Defensive driving requires constant concentration and riding skills including road positioning, safe, appropriate speed and awareness and anticipation. Your trainer will explain defensive driving and provide examples.\nMany of us know the dangers of riding too fast. Your trainer will explain why it’s important to ride within the legal speed limits and at an appropriate speed for the traffic, road and weather conditions. You’ll also have an explanation of the danger or riding too slowly and how it can affect other road users.\nFor all-round observation whilst riding a motorcycle, you must use a combination of mirrors and looking behind you. You will need to understand why you need to use this combination, how and when to use rear observation effectively. Your trainer will also explain when it is an inappropriate time to be looking around when observation should be concentrated ahead,\nBlind spots (the areas that aren’t seen in your mirrors) will be explained in more depth with the observation know as the ‘lifesaver glance’. This observation is in reference to the blind spot and is a final check all is clear. This can be just before changing lanes for example. For further information on motorcycle blind spots and those of other vehicles, see:\nThe safest road position for a motorcyclists is around the centre of your side of the road. Riding too close to the kerb or too close to the centre of the road can prove hazardous. Your trainer will cover this along with the correct road position for bends and junctions. Forward planning is essential as you’ll need to avoid hazards such as drain covers and manhole covers.\nKeeping your distance\nMany accidents occur due to drivers and riders traveling too closely to the vehicle in front due to a lack of thinking and braking distance and the inability for the drivers of other vehicles, particularly large vehicles to see you in their mirrors. This will be discussed by your trainer with the many dangers associated with traveling too close to the vehicle in front. Keeping a safe distance and using the two second rule will also be discussed along with the advantages of keeping a safe following distance, including:\n- an increased stopping distance that allows for more time to slow down\n- forward planning is improved due to the ability to past the vehicle in front\n- other road users can see you easier\nThe two second rule is a simple system used by many motorists to gauge a safe following distance in varied weather conditions. The two second rule will be discussed by your trainer.\nAll motorists are affected by weather conditions, motorcyclists more so than most. You’ll discuss how various weather conditions can impact the handling of the motorcycle and how it can affect the riders concentration when becoming too hot, too cold or wet. You’ll discuss:\n- High wind\n- Snow and ice\n- Low sun and a bright sun can affect levels of observation\nRain can substantially change the level of grip your tyres have on a particular surface and your trainer will explain those road surfaces that are best avoided if possible, such as painted road markings and drain covers.\nDifferent Road Surfaces\nAs with weather conditions, motorcyclists are affected to a greater extent on certain road surfaces. Road surfaces that represent a hazard include:\n- Road markings and drain covers in wet weather\n- Tramlines and railway lines\n- Loose road chippings and gravel\n- Mus and leaves\n- Well-worn, shiny road surfaces\nYour trainer will discuss the importance of forward planning and the ability to see these potential hazards in advance. Advice will include spotting hazards such as the rainbow effect that diesel or oil spillage can have on a road surface in wet weather and how the various surfaces affect braking, accelerating and cornering.\nCompetitive and Aggressive Attitudes\nRiding competitively or aggressively significantly increases your risk of an accident. Try to remain calm and focused even when other motorists make mistakes.\nAlcohol and Drugs\nTo ensure you remain legal and as safe as is possible, it’s best to never drink and ride, even if it’s within the legal limits. CBT element D will cover the legal limits of alcohol whilst riding within England, Scotland and Wales and that this limit varies on countries around the world. You’ll discus how alcohol and drugs affect your concentration and how it slows your reaction times and balance. You’ll cover also how long they can remain in your body.\nAfter the completion of element D, the final element E will begin. For further reading, see:']	['<urn:uuid:17f9a704-9551-426d-98fc-6feff39b7db7>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T11:40:26.317848	11	53	1159
14	what are common instruments used in dreamy alternative music	Dream pop typically features vocals, guitar, bass, drums, synthesizers, and keyboards as its primary instruments. These instruments are often used to create atmospheric textures and moods rather than traditional rock riffs, with guitars frequently employing various effects.	"['|Stylistic origins||Alternative rock, space rock, ethereal wave, noise pop, post-punk, neo-psychedelia|\n|Cultural origins||Late-1980s, United Kingdom|\n|Typical instruments||Vocals, guitar, bass, drums, synthesizers, keyboards|\n|Derivative forms||Shoegazing, ambient pop|\nDream pop is a subgenre of alternative rock.\nOrigin of the term\nThe term was coined in the late 1980s by Alex Ayuli to describe the music of his band A.R. Kane (later, he founded his own Dreampop record label). Shortly after, New York-based music journalist Simon Reynolds used the term to describe the shoegazing scene in the U.K. Reynolds is generally credited as being the first critic to use the term ""dream pop"" to describe a genre of music. In the 1990s, dream pop and shoegazing were regionally dependent and interchangeable terms.\nThe Allmusic Guide to Electronica defines dream pop as ""an atmospheric sub-genre of alternative rock that relies on sonic textures as much as melody"". Common characteristics are breathy vocals and use of guitar effects, often producing a ""wall of noise"". The term is often used, particularly in the United States, to describe bands who were part of the shoegazing scene, and shoegazing is seen as a part of dream pop. The term is thought to relate to the ""immersion"" in the music experienced by the listener. In the view of Simon Reynolds, dream pop ""celebrates rapturous and transcendent experiences, often using druggy and mystical imagery"". Dream pop tends to focus on textures and moods rather than propulsive rock riffs. Vocals are generally breathy or sung in a near-whisper, and lyrics are often introspective or existential in nature. Album art tends to consist of blurry pastel imagery or stark minimalist designs, or a combination of these two styles.\nLabels such as 4AD (Pale Saints, Lush, Swallow), Creation (My Bloody Valentine, Slowdive, Ride), Sub Pop (Velocity Girl), Sarah Records (Secret Shine), Slumberland (Black Tambourine, Swirlies), Cherry Red (Blind Mr. Jones), and Vernon Yard released significant records in the genre.\nHistory and influences\nInfluence over other styles\nA louder, more aggressive strain of dream pop came to be known as shoegazing; key bands of this style were Lush, Slowdive, My Bloody Valentine, Starflyer 59, Chapterhouse, Catherine Wheel, Ride, Medicine and Levitation. These bands kept the atmospheric qualities of dream pop, but added the intensity of post-punk-influenced bands such as Sonic Youth and The Jesus and Mary Chain. Shoegazing arose out of a love for dream pop\'s textures and moods, at the same time rejecting its more passive tendencies. Shoegaze was initially concentrated in England in the early 1990s.\nIn the late 1990s and early 2000s, bands like Sigur Rós, M83, Asobi Seksu, and The Radio Dept. have had the dream pop label attached to them. Groups like these are sometimes called nu-gaze bands. Singer-songwriter FKA Twigs, who rose to prominence in 2014, has been described as dream pop.\n- 4AD: ""The studio-based outfit comprised East London duo Alex Ayuli and Rudi Tambala, who described their music as ""dreampop""."" A.R. Kane short info\n- Nathaniel Wice / Steven Daly: ""The dream pop bands were lionized by the capricious British music press, which later took to dismissing them as ""shoegazers"" for their affectless stage presence."", Alt. Culture: An A-To-Z Guide to the \'90s-Underground, Online, and Over-The-Counter, p.73, HarperCollins Publishers 1995, ISBN 0-0627-3383-4\n- Pete Prown / Harvey P. Newquist: ""One faction came to be known as dream-pop or ""shoegazers"" (for their habit of looking at the ground while playing the guitars on stage). They were musicians who played trancelike, ethereal music that was composed of numerous guitars playing heavy droning chords wrapped in echo effects and phase shifters."", Hal Leonard 1997, ISBN 0-7935-4042-9\n- Bogdanov, Vladimir (2001) The Allmusic Guide to Electronica, Backbeat UK, ISBN 978-0-87930-628-1, p. ix\n- Reynolds, Simon (1991) ""POP VIEW; \'Dream-Pop\' Bands Define the Times in Britain"", The New York Times, 1 December 1991. Retrieved 5 September 2013\n- Goddard, Michael et al (2013) Resonances: Noise and Contemporary Music, Bloomsbury Academic, ISBN 978-1-4411-5937-3\n- Mendoza, Manuel (1992) ""Dream pop takes to the road: Swervedriver puts a modern twist on a classic rock image"", The Dallas Morning News, 23 April 1992\n- Bogdanov, Vladimir (2001). All Music Guide to Electronica: The Definitive Guide to Electronic Music (4th ed.). Backbeat Books. pp. ix. ISBN 978-0-87930-628-1.\n- John Bergstrom, ""George Harrison: All Things Must Pass"", PopMatters, 14 January 2011, http://www.popmatters.com/pm/review/135411-george-harrison-all-things-must-pass/ (retrieved 1 April 2012)\n- Reynolds, Simon (1 December 1991), ""Pop View; \'Dream-Pop\' Bands Define the Times in Britain"", The New York Times (The New York Times Company), retrieved 7 March 2010\n- ""Genre Profile - Post-Rock"". About.com Alternative Music. Retrieved 2012-05-24.']"	['<urn:uuid:de7728ce-9bc1-4826-9456-54b333793de1>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T11:40:26.317848	9	37	757
15	knee ligament strength anterior posterior compare	The posterior cruciate ligament is the knee's strongest ligament, while the anterior cruciate ligament is not specifically described as the strongest but serves two main functions: restricting tibial forward motion and excessive rotatory knee movements.	"['surgery with your own hamstring tendons\nThe first 5 BSc graduation projects...\nProf. RPA Janssen MD PhD will...\nJaap Tolk MD has won the prestigious Van Rens Prijs 2020 for the best scientific presentation...\nRemain up to date with all the latest knee facts via Facebook, Twitter and other social networks\nThe knee joint is the articulation between the femur and tibia. Bone is covered by cartilage to allow smooth knee motion. The inner (medial) and (outer) lateral meniscus are situated between the femur and tibia as shockabsorbers (see Information Meniscus). Stability of the knee joint is provided by the medial and lateral collateral ligaments as well as the cruciate ligaments. There are 2 cruciate ligaments: the anterior and posterior cruciate ligaments. The anterior cruciate ligament has 2 main functions: restriction of lower leg tibial forward motion as well as excessive rotatory knee movements.\nAnterior cruciate ligament rupture\nThe anterior cruciate ligament ruptures in a forced torsional or hyperextension movement of the knee. The lesion often occurs in pivoting sports such as football (soccer), field hockey or indoor sports. It occurs more frequently in women than men. The acute rupture of the anterior cruciate ligament may occur in patients of wide range in age: the youngest patient in my practice is 5 years old, the oldest 66. The majority of patients is 12-30 years old. In an acute lesion, patients often describe a ""popping"" sensation in the knee. Anterior cruciate deficient knees often lead to the feeling of giving way in daily or sports activities. Meniscus and other ligament lesions frequently occur in case of cruciate ligament rupture (see information meniscus). It is essential to make a correct diagnosis for adequate treatment, to limit rehabilitation time and optimize the prognosis for future (sports)activities.\nFirst of all, other knee injuries need to be adressed and treated. Scientific research has shown that the anterior cruciate ligament consists of 2 separate bundles, each important for specific stabilization of the knee. An anterior cruciate ligament rupture may be total (both bundles are torn) of partial (1 deficient bundle). The necessity for surgical reconstruction of the anterior cruciate ligament is determined by the degree of knee instability as well as (sports)activity requirements of the patient. A nonsurgical treatment by physiotherapy and/or brace may sometimes be sufiicient in patients with limited instability of the knee who do not wish to be involved in pivot sports. Treatment goal is to eliminate the giving way sensation of the knee.\nAnterior cruciate ligament reconstruction\nA ruptured anterior cruciate ligament does not heal. Reconstruction of the ligament is performed by using hamstring tendon or part of the patella tendon of the same leg. These tendons are very suitable for anterior cruciate ligament reconstruction. My PhD research has demonstrated that hamstring tendons regenerate after their removel for anterior cruciate ligament reconstruction (see Publications). The new cruciate ligament is fixed in femur and tibia through drilled bone tunnels and cortical fixation. This provides a rigid and stable reconstruction allowing accelerated rehabilitation. In case of severe rotatory instabilty or lateral ligament laxity, a reconstruction of the anterolateral ligament is performed on the outside of the knee during the same surgery. Early active exercises are encouraged to allow for better healing. A stabilizing leg brace is not necessary. Recent scientific evidence has shown that a correct surgical technique is essential for good results. In the last 2 years, new anatomic techniques have been developed leading to more than 90% succes rate. I am involved in scientific research on this subject as well as an instructor on a leading, yearly international course for orthopaedic surgeons on knee ligament reconstructions (see Biography).\nAnimation movie of all inside anterior cruciate ligament reconstruction.\nAfter the operation\nThe surgery is usually in daycare. The physiotherapist will give you instructions and teach you to walk with crutches. You are allowed to weight bear as well as bend and extend the knee freely. You will ambulate with the crutches till the first outpatient clinic appointment (2-3 weeks after surgery). Rehabilitation with your own physiotherapist can start within the first week after the operation. A referral will be provided before discharge from the hospital. I operate patients from all over the country. As such, we have developed a proven rehabilitation protocol that can be done by any physiotherapist near your residence (see protocol anterior cruciate ligament: in dutch only). I recommend to lift up your leg during 15 minutes, 5 times daily, in the first two weeks to limit lower leg swelling. For the same reason, you will be given a special stocking to be worn during daytime. The first outpatient check-up will be at 2-4 weeks. The stitches will dissolve by themselves and do not need removal. Experiences of patients are presented in Cases.\nWork and sports\nRehabilitation time after anterior cruciate ligament reconstruction takes 6-12 months. You may resume an office job between 2 to 4 weeks after surgery. Heavy labor may be undertaken after 4-6 weeks. These are general guidelines: it may take shorter or longer depending on knee swelling and pain. Driving a car is not allowed if you walk on crutches (till first appointment). You may cycle on a home trainer if your knee flexes 100º. Jogging is usually possible after 8 weeks if swelling and pain are limited. Return to pivot sports is possible after 6-8 months. Recent scientific research has demonstrated that return to sports activities is related to the knee cartilage condition.\nComplications after anterior cruciate ligament reconstruction are rare: wound disorders, infection, thrombosis, vascular or nerve injuries and arthrofibrosis (scarring of the knee). These complications occur in <1% of patients.\nif there is discharge from the wound\nif the knee becomes red and pain increases\nin case of fever >38.5º C.', ""Posterior Cruciate Ligament Tear\nThe Posterior Cruciate Ligament (PCL) is one of four main knee ligaments. It is a small band found at the back of the knee that links the tibia with the femur. Despite its size the PCL is the knee's strongest ligament, so Posterior Cruciate Ligament injuries are usually caused by harsh trauma or sudden damaging motions as opposed to more common sporting injuries. A PCL tear leads to knee instability and can make standing or bearing weight on that leg very difficult.\nSymptoms of Posterior Cruciate Ligament Tear\nThe symptoms can depend on the individual and the severity of the tear. Some people might experience knee pain or inflammation but this is not generally the case. This can make detecting the injury more difficult; however, the knee regularly feels unstable or unsteady and may collapse or wobble when bearing your weight. Other than this, the most reliable indicator is usually a traumatic incident involving the knee prior to the instability.\nCauses of Posterior Cruciate Ligament Tear\nVarious abrupt and sharp knee movements can lead to a Posterior Cruciate Ligament tear. Common causes include overextension or sudden twisting of the knee, as well as hard strikes to the shin or an impact that forces the knee backwards. These can all arise during fast paced contact sports such as football or rugby, although in many cases the injury is provoked by an incident not involving another athlete. Car accidents and other collisions are widespread causes outside of sport.\nDiagnosis of Posterior Cruciate Ligament Tear\nThe diagnostic process is the most important because the doctor will need to ascertain whether the Posterior Cruciate Ligament damage is isolated or linked to any other ligament injury. Physical exams are rarely enough to fully assess the injury, but an effective test involves the doctor pushing back on the tibia with the knee bent and judging whether the bone moved too far. Otherwise possible tests can include an x-ray, MRI or an arthroscopy procedure (with a small camera). These can help to show whether other ligaments are damaged, which is a common possibility with PCL tears. Fractures also need to be ruled out.\nMedical Treatment of Posterior Cruciate Ligament Tear\nIf the Posterior Cruciate Ligament tear occurs on its own, without any complications, then the treatment is likely to be conservative. This can mean a period of resting from activity and knee or leg immobilisation in order to keep the ligament in place as it heals. Pain medications and any other medicine will be prescribed at the doctor's discretion. A rehabilitation program of physical therapy will then gradually build up flexibility and power in the knee.\nSimultaneous damage to the Posterior Cruciate Ligament ligament and another knee ligament frequently necessitates a surgical procedure in which the injured ligaments are reconstructed by utilising a graft from an uninjured ligament or tendon. This surgery is often a choice and the doctor can talk you through the options and likely outcomes, but competitive athletes will usually opt for the surgery in an attempt to regain full strength in the knee. The other main option is a strengthening program. This can be arduous and last up to 6 months, but is the right option for some people.\n- Anterior Cruciate Ligament Tear\n- anterior knee pain\n- baker cyst\n- calf strain\n- fibula stress fracture\n- hamstring strain\n- hamstring tendinopathy\n- iliotibial band syndrome\n- lateral ligament sprain\n- medial ligament sprain\n- meniscus tear\n- patellar tendinopathy\n- patellofemoral instability\n- patellofemoral syndrome\n- pellegrini stieda syndrome\n- popliteus tendinitis\n- Posterior Cruciate Ligament Tear\n- sesamoid injuries\n- sever disease\n- sinding larsen and johansson syndrome\n- tibialis anterior tendinopathy\n- tibia fractures\n- tibialis posterior tendinopathy\n- Boot Stud Injuries\n- Pes Anserinus""]"	['<urn:uuid:ff74bfb7-b5e3-4b7a-943c-5a04e2ef1570>', '<urn:uuid:fc139d98-d5f7-4600-9b1f-9aef23a7284a>']	factoid	direct	short-search-query	distant-from-document	comparison	expert	2025-05-12T11:40:26.317848	6	35	1589
16	What security risks exist in centralized vs federated learning approaches for healthcare data?	In centralized learning, sensitive medical data must be aggregated in one place which creates privacy and security risks. In contrast, federated learning allows healthcare centers to exchange only model parameters without exposing patient data. The IBM fortiss Center demonstrates this with their Evidentia system that enables verifiable distributed learning while keeping data private. FL frameworks provide additional privacy mechanisms like differential privacy and cryptography to prevent information leakage between clients.	['Mr Pfeifer, the Center for AI was created two years ago with the aim of developing reliable and safe AI technologies for industry and society. What results have been achieved so far?\nWe established up a series of research lines in which we started multiple projects in areas such as human-centered machine learning, robotics, government services and distributed learning, as well as anomaly detection in building management.\nIn the human-centered machine learning area (HCML), we examine how applications that utilize machine learning can better meet the requirements of a human user. On the one hand, that means it will be easier for humans to understand and comprehend AI-based decisions, while on the other it allows the individual differences between the users to flow directly into the development process of the learning algorithms. These issues are being examined within a scenario that involves how firefighters manage stress in the field. Because people react differently depending on the stress situation, learning algorithms have to be personalized – in other words they have to be able to recognize the level of stress for firefighters on an individual basis. Conversely, the AI signals have to be presented to the platoon leaders in such a way that they can easily recognize which members of their platoon are currently in a highly stressful situation. In this area we are developing new stress recognition models on the basis of various biosignals and so-called self-supervised learning.\nIn the field of manufacturing robots, we are developing easy-to-manage and economically-viable configurations and analysis solutions that are also suitable for small-to-medium enterprises. To do that we developed semantic models that describe which work steps the robots are supposed to carry out. This information is coupled to data from the robot activities. By using machine learning, the system is then in a position to learn continuously, recognize deviations and when required to generate a warning, ideally before a malfunction occurs.\nIn terms of government services, we examined how these services can be designed such that citizens take advantage of them online without the hassle of filling out forms, and how they can be made available to citizens proactively, such as by eliminating the need to specifically apply for services with certain events. We demonstrated how this can work with two application scenarios: applying for child allowances and permits for opening restaurants.\nAnd finally, we recently began work on an application for recognizing water damage in buildings. We utilize sensors installed in the IBM Watson Center building in Munich to dynamically collect moisture data and link it to static building information and environmental data. By using special AI algorithms to detect events in the dynamic sensor data that could point to the intrusion of water, with the help of ontological data the water can not only be located more precisely, the cause can also be better qualified. That means you could determine if the event was caused by an open window or a broken pipe for example.\nYou have been collaborating with IBM for several years. What makes IBM a good partner for these issues and what is the objective of the effort?\nI believe it has to do with the fact that IBM is a global leader in the field of AI and boasts a large worldwide network of renowned research centers and business partners that understand what is required in practice. Apart from IBM’s leading industry expertise, the ethics aspect is also very important. IBM leaves it up to the partners and customers to decide which data the AI should use (transparency) and how they can use their data to build their own cognitive solutions.\nWhat challenges do you see with the current state of AI technologies?\nAI systems are learning-based, which means the functions and behaviors result from data that is used to train them. These methods also lead to unforeseeable behavior during operation, which presents a major challenge for the manageability of such systems. Furthermore, AI solutions are only as good as the data they are trained with. First off, this raises the question of the availability of suitable data and secondly, the quality. The keywords here are representative data and one-sidedness. AI systems can develop further through continuous learning and adapt or optimize themselves to new conditions based on experience. This makes development, and especially testing and validation, extremely difficult. Established software and systems engineering methods work under the premise that precise specifications related to the system behavior and operational environment are available, and that the systems will undergo extensive verification prior to going online, which is not feasible for AI systems. That’s why such methods are not directly transferrable. Furthermore, AI systems often operate in sometimes unknown or unreliable environments and must be robust enough to work around situations that have not been modeled in advance, or only partially modeled, despite imprecise or unreliable input data\nFor many people, AI is currently a black box with many unknowns. How do you want to gain the trust of people with this issue?\nA key aspect of trustworthiness is the extent to which AI decisions can be comprehended. As a user, can I understand how or why an AI system makes a particular decision? The focus here is on developing AI methods so that they can also provide the user an explanation of the results. At the IBM fortiss Center for AI, we recently launched a project to develop an intelligent driving assistant that recommends which driving lane to use and when to switch lanes. The system should also be able to explain why a particular recommendation is being made. Since the data that is used as a basis for the lane change recommendation likely stems from sections of the road that the driver is still unable to see, such as an accident or traffic congestion around a curve, in order to gain their trust it’s important that drivers be able to comprehend this.\nAt what point do you define AI as robust or trustworthy?\nWe don’t view any AI solution as robust, even if it still functions reliably and delivers good results in unknown environments or unpredictable situations. Here we might have to deal with a range of uncertainties – or better yet call them unknowns. This can be tied for instance to the fact that all possible situations or environmental influence was observed or even detected while developing and training the AI system.\nIt can also happen that an AI component makes a wrong decision if the input deviates only slightly from the known situation. We’re familiar with these kinds of problems in the area of image recognition for instance, where just a few minor pixel changes can lead to red traffic light being perceived as “green” or a pedestrian not being recognized. This of course opens up the risk of malicious manipulation in which a hacker tires to disrupt the AI through targeted input. To that extent then, it also becomes a question of the safety of the AI application and thus its trustworthiness. To employ AI-based applications in safety-critical environments such as autonomous driving or medical robotics, we have to trust that such misconduct is virtually impossible.\nWhat kind of solutions is the Center for AI working on to specifically design government services more directly and more service-oriented?\nThe future of public administration services lies in a proactive approach without interaction. Government services should be provided automatically, without the need for applications and without the citizen having to interact with an application. With our DR&P project (Digital Readiness Assessment and Piloting for German Public Services), we want to put the concept of proactive, non-interactive government services into practice. To do that we developed and applied an analysis method for the availability of specific services, enhanced existing software frameworks and created two demonstrators for children’s allowance and restaurant permit applications. Through our research we offer government practitioners a structured engineering approach that couples visionary service design with advanced technologies in order to achieve a higher level of service quality for individuals and companies. A government office that proactively delivers a service is viewed as user-friendly and improves the service quality because it delivers the user a service (user-centric) instead of merely approving it (government-centric). In order to make such proactive and non-interactive services available, the technology basis will combine machine learning-based intelligent data processing and accountability-based data exchange that relies on distributed ledger technology (DLT).\nHow can government agencies share information in a federated environment while respecting data privacy but without releasing specific data?\nThe keywords here are federated machine learning (FML) and accountability. FML is an approach that enables multiple parties to use their data to collaborate on the creation of a shared machine learning model without having to share the information. The idea is that all of the parties carry out machine learning tasks on their own private datasets and then share the resulting model updates in order to create a combined model of the entire data. This way the data remains private and the parties share only model updates and test data for evaluating the quality of the learned models. IBM Research developed a framework for federated learning that we have now linked with the fortiss Evidentia system as part of our collaborative AFML project (accountable federated machine learning). With Evidentia, you can specify distributed workflows such as federated learning and document execution of the steps so that they are tamper-proof. This makes it clear and verifiable at all times, which participant did precisely what and at what time, which decisions were made and why and whether the actual actions of the participants correspond to the agreed-upon learning process. This allows a consortium of players to verifiably account for government entitlements, even if there is no mutual trust.\nWhich sectors profit the most?\nAt the Center for AI, our initial research activities are industry-independent or they span all industries. There are countless fields of application for AI. Wherever data accumulates, AI, and especially machine learning, has the potential to be deployed in a profitably. There is significant potential especially in the manufacturing industry given its high share of recurring and predictable activities. And of course we’re also working on concrete applications in specific fields, such as the use of machine learning techniques for detecting anomalies in robot-based production.\nHow can companies gain access to these technologies?\nAs a collaborative research organization between IBM and fortiss, the Center for AI researches and develops AI solutions for specific challenges. For companies who wish to cooperate with us, the fortiss SME (small-to-medium enterprise) area is the starting point. This is where we bundle our services for companies and where we are happy to tell them about the different forms of cooperation that are available.', 'Do not let Big Bro in! – Security and privacy in elderly carepolaris_stable_admin\nIn our previous post we talked about how elderly care is becoming one of the most fundamental challenges across the world. It is clear that we will need all the tricks IT can offer, be it cloud computing, edge devices or AI. However, these shiny, new technologies are not without serious risks regarding privacy, material loss or even immediate danger of life. As elderly care regards, these risks are even more pronounced.\nPicture this. You think your grandma is fine as her well-being is monitored indoor (using cameras, lidars, etc) and outdoor (via wearable devices, etc). But what if someone can tap into the data communication and can see when the apartment is empty. Or by stealing the biometric data, it is a piece of cake to steal money or commit fraud. What is even worse, what if someone can fiddle with the smart pacemaker or the insulin pump remotely? Well, actually, it did already happen (shorturl.at/bkrX2).\nWhile all these hybrid (physical and cyber security) issues would worth a separate post on their own, we now want to introduce you to another aspect of privacy concerns: learning from highly sensitive data.\nAs we have talked about how data is important for learning complex patterns of the world, it is no surprise that health care monitoring or modeling behavioural patterns need lots of patient data. Those data can be as simple as the number of doctor-patient contacts a month or as complex as heart rate variation on a second by second basis. The problem is that we need to make sure that no personal information (“meta data”) gets mingled with the data needed to train the AI models. Why is that? Well, making such sensitive data open can pose direct threat to the participants. What is more, there is an indirect risk that can hurt even those who are not providing data to the training process, but are somewhat related to the patients.\nFor health monitoring, the problem is not limited to the model training phase. Continuous monitoring of the participants requires to maintain contact and repeated access to sensitive data. This data is then used to provide predictions as well as useful information to update (fine tune) the learning models (continual learning scenario). So how can we secure the flow of sensitive data? And how can we make sure that personal information is not getting into the wrong hands?\nThere are existing solutions that either try to hide or erase sensitive information (various kinds of anonymization) or try to deeply encrypt the communication channel.\nHowever, there is another smart idea that is designed to render the communication of sensitive info unnecessary. This approach is called federated learning. Let us see what this is all about.\nFederated Learning (FL)\nAccording to WIKI: “Federated learning is a machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging them. Federated learning enables multiple actors to build a common, robust machine learning model without sharing data, thus allowing to address critical issues such as data privacy, data security, data access rights and access to heterogeneous data. Its applications are spread over a number of industries including defense, telecommunications, IoT, and pharmaceutics.”\nThe term was coined by Google (https://arxiv.org/abs/1602.05629v3) back at 2017.\nLet us dive into this complex definition. The first interesting technology involved is called distributed learning. To brush up our knowledge, let us talk a bit about machine learning, in particular, supervised learning. Here the task is to learn to associate labels with data. Machine learning algorithms learn the association by incrementally tuning parameters that define the chain of transformations that make up the algorithm. Now we talk about millions or billions of those parameters! That explains why training is so tedious in most cases. However, if several machines can parallel work on different bunch of data, then training becomes much faster if the trained model variations are properly combined into one single solution. The other thing that pops up is that FL is ideal when privacy preserving is of central importance. The whole idea is about minimizing data exchange between the clients (unit that can train a model on local data) and the server (a unit that aggregates local model updates, organizes parameter exchange, but does not have access to data). This particular issue is getting so important that it makes FL a central part of all AI applications across various indsutries and business: Google, IBM, Intel, Baidu or Nvidia have all come up with their enterprise grade FL frameworks (shorturl.at/dgiy7)!\nThe original idea was based on the assumption that edge devices (like smart phones) can both collect and process data locally. In turn, if models can fit into the phone’s memory, than it is enough to exchange local updates with a central model. This concept is called cross-device FL. Personalized texting like Gboard is using this approach.\nWell, texting is fine on mobiles, but measuring blood sugar? So there is another real-life scenario. You already have shared your medical data with your doctor so as have all her other patients. In turn, the health care center can tune its own model using all the available data. Centers can exchange the model parameters without exposing their own patient data. This approach is called cross-silo FL.\nSee the picture below! Normally data are collected and aggregated across the different locations and a central unit trains a model using all the data collected. This setup definitely raises the red flag as sensitive medical records are moving around. But here comes cross-silo FL to the rescue! Privacy is preserved, well done!\nClearly, cross-device and cross-silo FL types define the scaling dimension of FL. In the last few years many new ideas have been discussed and now there are at least 6 factors that are needed to differentiate between the various solutions.\nData partitioning is about how participant and their features (data records) are treated across the different local models. While the original idea assumed that each client node has the very same representation on the participants, there are reak life scenarios where data gets partitioned by feature sets and not by user id. As an example, a bank and an insurance company may have access to different data on the very same user, yet they can mutually improve each other’s models.\nMachine Learning modeling is about the core model applied within FL. The more complex the model, the more update exchange is needed. As FL regards, the most important question is how to aggregate the local updates when facing reliability and communication bandwidth issues.\nPrivacy Mechanism is a core component of the FL frameworks. The basic idea is to avoid information leakage amongst clients. Differential privacy (that is to separate user specific and generally relevant information) and cryptography are two frequently used approaches, but this is a constantly evolving field.\nCommunication Architecture. The original idea suggested an orchestrated approach to model training where the central server holds the aggregated model that is mirrored in the local units. More recent solutions drop centrality and suggest various decentralized updating mechanisms. In these solutions client nodes communicate with a few peers and there is a particular policy on update propagation.\nWe talked about Scaling, and the last point is about the Motivating Factors for applying FL. In some cases, stringent regulations force us to turn to FL (consider GDPR in Europe, CCPA in the US or PIPL in China). In other cases shared cost and increased reliability could be the main driving forces.\nIf you wonder why we have so many factors to check just think about the immensely different challenges in e-commerce (personalized ad), finance (fraud detection) or healthcare (remote diagnostics, etc, see https://www.nature.com/articles/s41746-020-00323-1). Different requirements require different solutions.\nSo what are the main challenges that FL solutions meet?\nUpdating large models requires sending large messages. Another problem is limited bandwidth: when a large number of clients try to send data, many will fail. The solution for the first problem involves a form of compression, while the second one is addressed by the introduction of decentralized (peer2peer and gossip) networks, when updates are exchanged locally. One example solution is depicted in the figure next:\nPrivacy and data protection\nWhile raw data stay where it was generated, model updates can be attacked and reveal private information. Some solutions are built around differential privacy, where only statistical (general) data are extracted and used for model training (https://privacytools.seas.harvard.edu/differential-privacy). Another interesting idea is to perform computation on encrypted data only (“homomorphic encryption” for those who like scientific terms). Yet another idea goes to the opposite direction: let us spread the sensitive data across many data owners, but computations can only be done in a collaborative fashion. Cool, isn’t it?\nFor really large FL systems, nodes are most likely quite different in terms of storage capacity, computing power, and communication bandwidth. And only a handful of them participates in the update at a given time, resulting in biased training. Solution? Asynchronous communication, sampling of active devices and increased fault tolerance.\nClients may get different data in terms of quality (noisy, missing info, etc) and statistical properties (difference in distribution). That is big one and it is not easy to fix or even to detect. What is even worse, nodes with their local models can be compromised to enable a “model poisoning” attack (https://proceedings.mlr.press/v108/bagdasaryan20a.html): specially crafted data and local model updates drag the aggregated model toward an unwanted state causing erratic behavior and damage.\nIf you have read this far, you must share our enthusiasm for FL. If you are willing to get your hands dirty, here are some open-source FL frameworks to play with:\n- FATE (https://github.com/FederatedAI/FATE) supported by the Linux Foundation.\n- Substra: https://docs.substra.org/en/stable/\n- PySyft + PyGrid: https://blog.openmined.org/tag/pysyft/\n- Nvidia’s Clara: https://developer.nvidia.com/industries/healthcare\n- IBM’s solution: https://ibmfl.mybluemix.net/\n- OpenFL by Intel: https://openfl.readthedocs.io/en/latest/index.html\n- and the very user friendly Flower: https://flower.dev/\nIf you have any questions, have interesting ideas, or just want to talk about FL, just drop a mail!']	['<urn:uuid:924225df-0d7c-4ae1-94dc-c932450c26d9>', '<urn:uuid:74a87e46-c2b9-4423-a149-5b29495074be>']	factoid	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T11:40:26.317848	13	70	3469
17	Who found these islands first from Europe?	Christopher Columbus was the first European to discover this archipelago during his second voyage in 1493. He was so impressed by the islands that he named them 'Las Once Mil Las Virgines' after St. Ursula and her 11,000 virgins.	['Once home to pirates and buccaneers, the gorgeous British Virgin Islands now appeal to a decidedly different and more upscale crowd, including Sir Richard Branson.\nLocated some 90 miles east of Puerto Rico, the 50 or so Caribbean islands, islets and cays that make up the British Virgin Islands (BVI) are so pristine and unspoiled that they are often referred to as “Nature’s Little Secrets.” In fact, the region is blessed with so many green hills, turquoise-hued bays and palm-fringed beaches that it’s rated one of the most beautiful and relaxing destinations in the world.\nChristopher Columbus was the first European to discover this idyllic archipelago during his second voyage of discovery in 1493. The intrepid explorer was so impressed by the dozens of lush islands he saw on the horizon of the Caribbean that he named them “Las Once Mil Las Virgines” after the legendary St. Ursula and her 11,000 virgins.\nDespite their appeal, Europeans didn’t settle the Virgin Islands until the Dutch arrived on Tortola in the mid -17th century and built a stone fort on the shores of Road Town Harbour. The British annexed the archipelago in 1672 and eventually made it a protected territory, which it remains to this day.\nOver the centuries, the sheltered bays, gentle trade winds and strategic location next to the Spanish trade routes made the Virgin Islands an ideal base for pirates and privateers. In fact, the islands’ main channel is named for English privateer Sir Francis Drake who sailed through the area in 1595 and mustered a fleet there to attack the Spanish. And the infamous pirate Blackbeard is believed to have buried his treasure in the caves of Norman Island – a legend that may have inspired Robert Louis Stevenson to write “Treasure Island”.\nToday, there are several ways to explore the Virgin Islands including by cruise ship. However, while many ships call at the neighbouring island of St. Thomas in the U.S. Virgin Islands, only a handful stop in the BVI – usually at Tortola’s Road Town Harbour.\nIt’s a shame because while St. Thomas has great American-style shopping in the bustling duty-free port of Charlotte Amalie, the BVI offers a more relaxed and natural environment with a twist of British influence. The British islands also have some of the best beaches, diving, and open-air bars and restaurants in the Caribbean, including the legendary Foxy’s in Great Harbour, the Soggy Dollar Bar in White Bay, and The Last Resort in Trellis Bay.\nThe BVI is also one of the favourite haunts for British jetsetters, including Sir Richard Branson. The legendary business tycoon has a compound on the privately owned Necker Island, just north of Virgin Gorda. Mr. Branson is sometimes seen hanging out at local restaurants and bars, including Cow Wreck Beach Bar on Anegada, the Cooper Island Beach Bar, and CocoMaya and Hog Heaven on Virgin Gorda – the latter perched high up on a hill overlooking all of North Sound.\nIf you’re lucky enough to be on a ship that calls in the BVI, your choices for shore excursions will likely revolve around beaches, snorkeling, diving and sailing. Cane Garden Bay on the north shore of Tortola is the top choice for a day at the beach, while the Wreck of the Rhone off the coast of Salt Island is the best bet for diving. There are also some good snorkeling locations including the Indians near Pelican Island, the Caves at Norman Island, the Baths on Virgin Gorda, and Green Cay near Little Jost Van Dyke.\nSince a single day in Tortola usually isn’t enough to fully appreciate the beauty of these magical islands, it’s worth considering a pre- or post-cruise land stay, especially if you’re sailing out of nearby St. Thomas or San Juan. There are a variety of charming hotels in the BVI to choose from, including the Sugar Mill on Tortola’s Carrot Bay (rated one of the top 5 hotels in the Caribbean by Conde Nast Traveler magazine), Peter Island Resort and Spa on Peter Island (a private island with four beautiful beaches including Deadman’s Bay, considered one of the most beautiful in the Caribbean), Oil Nut Bay (a swanky private enclave that features gorgeous villas with plunge pools that hang over the edge of a cliff), and the Bitter End Yacht Club in North Sound, Virgin Gorda (which offers a great sailing and diving program).\nOf course, the best way to explore the BVI is by water, and there are a number of small ships that offer inter-island cruises of the region including some from Seabourn, SeaDream, Star Clipper and Windstar. These ships are small enough to anchor in some of the beautiful bays on Jost Van Dyke, Norman Island, and Virgin Gorda that large ships never get to visit.\nIf you have some boating experience or are prepared to hire a local skipper, there’s also the option of chartering your own sail or power boat from one of the many charter companies in Tortola (more on this in a future blog).\nHowever, no matter how you get there or how or how long you stay, the British Virgin Islands are bound to make a lasting impression. After all, what else would you expect from a group of Caribbean islands beautiful enough to be called “Nature’s Little Secrets?”']	['<urn:uuid:cce3f118-4b18-4dab-9ba1-5f45581bc38f>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T11:40:26.317848	7	39	886
18	improvements polio salmonella vaccine comparison vaccination programs	Both polio and Salmonella have seen dramatic improvements through vaccination programs, but at different times. For Salmonella, mass vaccination programs in poultry led to a significant decrease in human cases from 18,000 in 1993 to just 459 in 2010. In the case of polio, vaccination programs achieved even more dramatic results - while polio used to paralyze and kill up to half a million people yearly before vaccines, by 1961 cases in the US had dropped from 35,000 to just 161 thanks to immunization campaigns. The Salk vaccine was introduced in 1953 and the Sabin oral vaccine in 1955-1960, while Salmonella vaccination programs began later in the 1990s.	"[""Mass poultry vaccination programmes introduced to combat Salmonella infections have led to a dramatic fall in the number of cases since the late 1990s, according to a researcher at the University of Liverpool.\nSalmonella are important food-borne pathogens worldwide, causing diarrhoea, vomiting, nausea, fever and abdominal pain. There are currently around 6 million cases of illness from Salmonella across the EU each year, the majority of which are linked to food items such as eggs, chicken, beef, pork, salad vegetables and dairy products.\nBetween 1981 and 1991, the number of salmonella infections rose by 170% in the UK, driven primarily by an epidemic of Salmonella Enteritidis which peaked in 1993. A raft of control measures were introduced into the poultry industry including movement restrictions, compulsory slaughter and disinfection procedures, as well as a voluntary industry-led vaccination scheme that began in breeding flocks in 1994 and in laying flocks in 1998.\nCode of Practice\nLegislation requiring compulsory slaughter of poultry infected with Salmonella has now been revoked but the mass vaccination of poultry has continued by those breeders subscribing to the Lion Quality Code of Practice and using the Lion Mark on eggs. The code of practice requires mandatory vaccination of all young hens destined to lay Lion eggs against Salmonella, as well as traceability of hens, eggs and feed, a best-before date stamped on shells and hygiene controls at packing stations. Lion eggs now account for around 85% of the total market.\nSarah O’Brien, Professor of Epidemiology and Zoonoses, from the University’s Institute of Infection and Global Health, attributes a dramatic fall in the number of Salmonella cases in humans to this mass vaccination programme in poultry.\nProfessor O’Brien said: “We have seen a marked decline in the number of incidences of Salmonella infection, shown by two significant studies conducted 10 years apart. These studies found that the number of cases fell from 1.6 cases per 1,000 person years for a study conducted from 1993 to 1996 (1) to 0.2 cases per 1,000 person years for the same study conducted from 2008 to 2009 (2).\n“In addition, the number of laboratory-confirmed cases of illness dropped from more than 18,000 in 1993 to just 459 in 2010 (3).\n“The nature of public health interventions often means that evaluating their impact is complex as they are often implemented simultaneously. The decrease in laboratory confirmed human cases coincides quite closely with the introduction of vaccination programmes in breeder and laying flocks. It is probable that no single measure contributed to the decline in Salmonella cases but the relationship between vaccination programmes and the reduction in human disease is compelling and suggests these programmes have made a major contribution to improving public health.”\nThe research is published in Clinical Infectious Diseases.\nListen to Sarah O’Brien being interviewed on the subject of campylobacter in chickens on Radio 4’s Face the Facts programme: http://www.bbc.co.uk/programmes/b01ptzmf\n1. Tam CC, Rodrigues LC, Viviani L et al. Longitudinal study of infectious intestinal disease in the UK (IID2 study): incidence in the community and presenting to general practice. Gut 2012; 61(1): 69-77.\n2. Wheeler JG, Sethi D, Cowden JM et al. Study of infectious intestinal disease in England: rates in the community, presenting to general practice, and reported to national surveillance. The Infectious Intestinal Disease Study Executive. BMJ 1999; 318(7190): 1046-5\n3. Laboratory reports of human Salmonella cases in the UK, 1981 to 2010 – Health Protection Agency; Health Protection Scotland; Public Health Agency of Northern Ireland\nYou must be logged in to post a comment.\nAll recent news\nWellbeing Week: Liverpool Spring 10K\nApril: 4 fantastic volunteering opportunities you can apply for today\nDeputy Director of the Heseltine Institute appointed\nProfessor Michael Fisher awarded prestigious Chair in Emerging Technologies\nMedical students awarded at annual prize evening\nCongratulations to Prof Michael Fisher from the Dept of Computer Science on being awarded a highly prestigious Chair in Emerging Technologies by the Royal Academy of Engineering https://t.co/Q926B8k5VM\nThis time next week we will all be voting - won't we?\nA guide to what might happen in NW England...\nResearch led by @LivUni has played a key role in the development and introduction of the rotavirus vaccine. An evaluation of the impact of rotavirus vaccine in Malawi showed that it reduced infant deaths by nearly a third.\nFind out more here: https://t.co/0S7ZU3afmj #MadeAtUni"", 'Timeline of polio\n(Redirected from Timeline of poliomyelitis)Jump to navigation Jump to search\nThis article needs to be updated.March 2019)(\nThis is a timeline of polio, describing major events, such as vaccine releases, historic epidemics, and major organizations.\n|1955 (pre-vaccine era)||Before the vaccine, poliomyelitis paralyze and kill up to half a million people every year at its peak, according to calculations.|\n|1950s||The 1950s mark a change in the history of polio, with the development of both the Salk and Sabin vaccines.|\n|1960s||Massive cut in polio transmission is achieved in the United States, where the vaccines are first administered.|\n|1970s–1980s||Routine immunization is introduced worldwide as part of national immunization programs, helping to control poliomyelitis in many developing countries. Non–governmental organizations take action.|\n|1990s–2000s||WHO regions of the Americas and Europe achieve polio–free certifications, while the number of endemic countries plummet in the rest of the world.|\n|2010 onward||In the last years, massive immunization campaigns have proven to be successful, with an almost complete global eradication as of 2016. To date, only Afghanistan and Pakistan, remain endemic.|\n|Year/period||Type of event||Event||Location|\n|1789||Medical development||British physician Michael Underwood is the first to give a clinical description of poliomyelitis.||United Kingdom|\n|1840||Medical development||German orthopaedist Jakob Heine becomes the first to write a medical report on poliomyelitis, and the first to recognize the illness as a clinical entity.||Germany|\n|1894||Epidemic||First poliomyelitis epidemic breaks out in the United States. Eighteen deaths and 132 cases of permanent paralysis are reported.||United States (Vermont)|\n|1905||Epidemic||Poliomyelitis epidemic breaks out in the Scandinavian peninsula. 1,031 cases are reported.||Sweden, Norway|\n|1908||Scientific development||Austrian physicians Karl Landsteiner and Erwin Popper discover the etiologic agent of poliomyelitis by identifying a virus when transmitting the disease to a monkey.||Austria (Vienna)|\n|1910||Scientific development||American scientists Simon Flexner and Paul Lewis suggest that poliovirus gain access to the central nervous system via the nasal mucosa. This hypothesis is supported by experiments with monkeys performed by Flexner’s group and other researchers.|\n|1916||Epidemic||Large epidemic of poliomyelitis breaks out in the United States. In New York City 9,000 cases and 2,343 deaths are reported, while toll nationwide is reported at 27,000 cases and 6,000 deaths.||United States|\n|1927||Organization||American politician Franklin D. Roosevelt founds Warm Springs Foundation for polio rehabilitation. In 1980 the facility is renamed Roosevelt Warm Springs Institute for Rehabilitation. Today, the organization treats patients with post-polio symptoms, spinal cord injuries, strokes, and other disabilities.||United States (Georgia)|\n|1928||Medical development||American industrial hygienist Philip Drinker along with Louis Agassiz Shaw, Jr. develop the iron lung, a negative pressure ventilator that enables a person to breathe when normal muscle control has been lost or the work of breathing exceeds the person\'s ability. It becomes popular after its successful first use in a girl with poliomyelitis, showing dramatic recovery in a very short period of time.||United States|\n|1931||Scientific development||Australian scientists Frank Macfarlane Burnet and Jean Macnamara discover the existence of antigenic differences between strains of poliovirus, comparing the famous Rockefeller MV strain with a local strain isolated in Melbourne and further finding striking differences in cross-immunity experiments and neutralization tests in monkeys.||Australia|\n|1935||Crisis||Canadian researcher Maurice Brodie works in the development of a vaccine made from a killed strain of poliovirus. While successful in twenty laboratory monkeys, the trials fail when performed on humans. The same year, American researcher John A. Kolmer, working on his own vaccine using weakened poliovirus, fails even worse when a large number of children who were administered his vaccine become ill and many die.||United States|\n|1939||Medical development||American physician Charles Armstrong manages to adapt the Lansing strain of poliovirus to mice, making available for research purposes an animal far less expensive than the monkey.||United States|\n|1940||Medical development||Australian nurse Elizabeth Kenny introduces new treatment for polio, using warm compresses to relax painful, contracting muscles and massage for rehabilitation. Unconventional and controversial at the beginning, eventually this treatment becomes part of standard care for poliomyelitis.||United States|\n|1948||Scientific development||Team led by American biomedical scientist John Franklin Enders, at Harvard University, succeeds in culturing poliovirus in the laboratory outside of a living body. In 1949, the team publishes the experiments and findings, which would make mass production of vaccine possible. In 1954 the researchers were awarded the Nobel Prize.||United States|\n|1953||Medical development||American medical researcher Jonas Salk and his associates develop a potentially safe, inactivated (killed), injected polio vaccine. By 1955, the Salk vaccine is recognized as safe, effective and potent. Salk is hailed hero by the public and is granted a license to market his vaccine by the government.||United States|\n|1955–1960||Medical development||Polish American medical researcher Albert Sabin develops an oral polio vaccine. Initially overlooked due to Salk vaccine success, in 1957 the World Health Organization authorizes mass vaccination of children living in areas suffering from poliomyelitis epidemics. By 1959, about 4.5 million people in Russia have received the oral vaccine, making the incidence of polio in that country decrease markedly by 1959. Due to these results, the Sabin vaccine is licensed for use in the United States |\n|1961||Report||As a result of the first immunization campaigns, only 161 poliomyelitis cases are recorded in the United States, down from 35,000 in 1953 to 5,600 by 1957.||United States|\n|1962–1970||Medical development||The Salk vaccine is gradually replaced by the oral Sabin vaccine for most purposes because it is easier to administer and less expensive.||Worldwide|\n|1979||Program launch||Rotary International commits to provide oral polio vaccine to six million children in the Philippines as part of its new Health, Hunger and Humanity (3-H) program. Following the success of the program, Rotary begins to work with Albert Sabin on a plan to immunize all children against poliomyelitis.||Philippines|\n|1980||Program launch||Brazil implements National Immunization Days against infantile paralysis as part of the strategy to eliminate poliomyelitis. The initial objective is to achieve high coverage to interrupt transmission of poliovirus, with an established target of vaccinating 95% of children younger than five years with oral poliovirus vaccine. By 1989, a case of poliomyelitis in Brazil is confirmed for the last time.||Brazil|\n|1981||Scientific development||American researchers Vincent Racaniello and David Baltimore at Massachusetts Institute of Technology; and team led by German American virologist Eckard Wimmer at State University of New York, publish the poliovirus genome. The researchers used an enzyme to switch the single strands of viral RNA to double strands of DNA and then determined the sequence of adenine, thymine, guanine, and cytosine encoding the five molecules that are the substance of the virus’s existence.||United States|\n|1985||Program launch||Rotary International announces a US$120 million pledge to its new PolioPlus program as a twenty-year commitment to immunize all children of the world against poliomyelitis by 2005. So far, Rotary has been the largest private sector donor to polio eradication, committing over US$600 million to the cause.||Worldwide|\n|1985||Program launch||The Universal Childhood Immunization Initiative is launched jointly by UNICEF and WHO, with the purpose of reducing child mortality through effective immunization.||Worldwide|\n|1985||Program launch||The Pan American Health Organization launches initiative to eradicate the indigenous transmission of wild poliovirus from the WHO region of the Americas by the end of 1990.||Americas|\n|1988||Program launch||The World Health Organization resolutes to eradicate poliomyelitis globally by 2000 through several delivery strategies, including reinforcement of existing initiatives such as National Immunization Days (NIDs) and sub-national immunization days. By 2016, the resolution was not achieved.|\n|1988||Program launch||The World Health Assembly (WHA) launches a global goal to eradicate poliomyelitis by 2000. This goal is further moved to stopping transmission by end of 2005.||Worldwide|\n|1996||Program launch||South African politician Nelson Mandela launches Kick Polio out of Africa campaign, with aims at eradicating the disease from Africa. By 2003, poliomyelitis remains in only three countries (Nigeria, Niger and Egypt) out of 46.||Africa|\n|1999||Medical development||inactivated polio vaccine replaces oral polio vaccine as recommended method of polio immunization in the United States.|\n|1994||Achievement||Following successful eradication programs such as that of Brazil, poliomyelitis is confirmed eliminated in the Americas.||Western Hemisphere|\n|1999||Report||The last case of wild poliovirus (WPVs) type 2 is reported.||India (Aligarh)|\n|2002||Achievement||European WHO region is certified free of poliomyelitis.||Europe, Turkey, ex USSR|\n|2003||Epidemic||Political and religious leaders of Kano, Zamfara, and Kaduna states in Nigeria bring the immunization campaign to a halt by calling on parents not to allow their children to be immunized. Polio immunization is suspended, thus leading to poliomyelitis outbreak and reinfecting at least other six countries (Burkina Faso, Central African Republic, Chad, Côte d’Ivoire, Mali, and Sudan).||Africa|\n|2004||Program launch||Upon emergency meeting of Health Ministers committed to end polio transmission, a massive immunization campaign is conducted in 22 African countries, reaching 80 million children, becoming one of the World’s largest public health campaigns.||Africa|\n|2012||Report||Poliovirus serotype WPV3 is last reported.||Nigeria|\n|2012||Report||Poliomyelitis remains officially endemic in four countries.||Afghanistan, Nigeria, Pakistan, India|\n|2015||Achievement||Poliovirus serotype WPV2 is declared eradicated worldwide.|\n|2016||Report||The only three endemic countries as of 2016 are Afghanistan, Pakistan and Nigeria.||Afghanistan, Nigeria, Pakistan|\n- ""Kul Gautam – A History of Global Polio Eradication"" (PDF). unicef.org. Retrieved 10 January 2017.\n- ""POLIO: A REVIEW"". International Journal of Pharmaceutical Sciences and Research. 4 (5). 2013. doi:10.13040/IJPSR.0975-8232.4(5).1714-24.\n- ""History of Polio"". polioeradication.org. Retrieved 9 January 2017.\n- ""History of polio BBC"". BBC News. Retrieved 9 January 2017.\n- ""History of Polio ( Poliomyelitis )"". historyofvaccines.org. Retrieved 9 January 2017.\n- Gothefors, Leif (2008). ""The Impact of Vaccines in Low- and High-Income Countries"". Department of Clinical Sciences/Paediatrics, Umeå University, Umeå, Sweden. 66 (2): 55–69. doi:10.1159/000129623. Retrieved 14 January 2017.\n- ""Polio cases report"". historyofvaccines.org. Retrieved 9 January 2017.\n- Eggers, Hans J. ""Milestones in Early Poliomyelitis Research (1840 to 1949)"". PMC 112492.\n- Smallman-Raynor, Matthew (2006). Poliomyelitis: Emergence to Eradication. ISBN 9780199244744. Retrieved 9 January 2017.\n- ""Whatever happened to polio?"". amhistory.si.edu. Retrieved 9 January 2017.\n- ""Roosevelt Warm Springs Institute for Rehabilitation"". georgiaencyclopedia.org. Retrieved 9 January 2017.\n- ""Today in History: Iron Lung Used for the First Time (1928)"". english.tebyan.net. Retrieved 9 January 2017.\n- True Peters, Stephanie (2005). The Battle Against Polio. ISBN 9780761416357. Retrieved 10 January 2017.\n- ""Sister Kenny"". amhistory.si.edu. Retrieved 10 January 2017.\n- Pearce, J M S (2004). ""Salk and Sabin: poliomyelitis immunisation"". Journal of Neurology, Neurosurgery & Psychiatry. 75 (11): 1552. doi:10.1136/jnnp.2003.028530. PMC 1738787.\n- Persson, Sheryl (April 2010). Smallpox, Syphilis and Salvation: Medical Breakthroughs that Changed the World. ISBN 9781921497575. Retrieved 10 January 2017.\n- Rocha MelloI, Maria Lúcia; Moraes, José Cássio; Brendan, Helena Aparecida; Flannery, Brendan. ""Participation in national polio immunization days: results of a vaccine coverage survey among children in 27 Brazilian cities"". Revista Brasileira de Epidemiologia. 13 (2): 278–288. doi:10.1590/S1415-790X2010000200010. Retrieved 10 January 2017.\n- ""The polio genome"". amhistory.si.edu. Retrieved 10 January 2017.\n- The Rotarian. 2001. p. 53. Retrieved 10 January 2017.\n- ""International Notes Update: Progress Toward Eradicating Poliomyelitis from the Americas"". cdc.gov. Retrieved 10 January 2017.\n- ""National Immunization Day: a strategy to monitor health and nutrition indicators"". Retrieved 10 January 2017.\n- ""Address by President Nelson Mandela at the launch of the ""Kick Polio Out of Africa"" Campaign"". mandela.gov.za. Retrieved 10 January 2017.\n- Jegede, Ayodele Samuel (2007). ""What Led to the Nigerian Boycott of the Polio Vaccination Campaign?"". PLOS Medicine. 4 (3): e73. doi:10.1371/journal.pmed.0040073. PMC 1831725.\n- ""West Africa mobilizes for final assault against polio"". WHO. Retrieved 10 January 2017.\n- ""Global eradication of wild poliovirus type 2 declared"". polioeradication.org. Retrieved 9 January 2017.']"	['<urn:uuid:0b81580e-b308-4c95-9b70-27a54f818a84>', '<urn:uuid:feb4e80b-0a7f-4574-a0f4-23742048853c>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	novice	2025-05-12T11:40:26.317848	7	108	2563
19	How do the Living Archives project and the Digital Observatory for Protected Areas (DOPA) compare in their approaches to making information accessible to the public and decision-makers?	The Living Archives project and DOPA take different but complementary approaches to making information accessible. Living Archives focuses on revitalizing dormant digitized cultural heritage archives through social networking, mobile media, storytelling, gaming, and performance, aiming to create social change and cultural awareness. DOPA, on the other hand, processes and integrates biodiversity and protected areas data from various international institutions, providing web services and applications that allow users to access complex analyses. Both initiatives aim to make previously inaccessible information available, but while Living Archives emphasizes social and cultural engagement through interactive methods, DOPA focuses on providing scientific and statistical information to support policy-making and conservation efforts.	"['The purpose of the Living Archives project is to research, analyze and prototype how archives for public cultural heritage can become a significant social resource, creating social change, cultural awareness and collective collaboration pointing towards a shared future of a society.\n- Purpose and Aims\n- Research Strands\n- Research Strand: Performing Memory\n- Research Strand: Open Data\n- Research Methodologies\nToday most archives have been digitized or at least partly digitized, but many of them are still dormant and inaccessible to the broad spectra of citizens. Revitalizing public archives into living social resources implies shedding the conception that they are the dormant and disembodied narratives of a dominant culture. With the Living Archives project, we shake the digital dust off archives and open the process of archiving so that it embraces contemporary practices associated with open data, social networking, mobile media, storytelling, gaming, and performance. The purpose is both to see how public cultural heritage can become a significant social resource, creating social change, and how archival practices outside institutions can be facilitated, performed and valued.\nWHAT IS ARCHIVED? WHO DOES THE ARCHIVING? – Archiving is not just located in the past, it occurs in the present, and it impacts the future. As such, archiving faces the same general problems as knowledge circulation: call it narrowing or “shallowing”, to adapt Nicholas Carr’s popular term “The Shallows” (Carr 2010). We know that search engines serve as amplifiers of popularity, continually reinforcing a consensus about what information is important and what is not. Digital information that is marginal, unusual, or even simply associated with atypical keywords is left out of the wave of what constitutes knowledge. If we replace knowledge with archives in the previous sentence, then there is a risk that cultural memory becomes narrower, both in terms of accessing what already exists but also what is being generated now. The purpose of this project is not just to open up dormant archives, but to address the wider problems of a narrowing of content (what is archived?) and an narrowing of inclusion (who does the archiving?)\nARCHIVES AND PRACTICES OF ARCHIVING – We aim to address challenges facing the Digitized Society through the phenomena of archives and the practices of archiving. We approach archives and archiving through Performing Memory and Open Data with a collaborative and multidisciplinary team of researchers. Our researchers contribute knowledge and methodologies from the fields of History, Artistic Research, Interaction Design, Computer Science and Cultural and Critical Theory.\nOur specific aims are:\n- to explore the potentials of public digitized archives;\n- to view archive material (photographs, film/video, audio, text) as interactive social resources in the present;\n- to utilize the power of the collaborative media landscape by exploring how the gathering of live data can become instant archives as people move through geographical contexts;\n- to revitalize the co-production of shared public memory;\n- to explore how open data can become meaningful to specific communities of practice;\n- to foster a performative approach emphasizing the embodied and personal qualities of archiving as a living practice;\n- to create design activities, dedicated to exploring, prototyping and testing relevant possibilities for future digital archives.', ""Protected areas provide a significant percentage of the livelihoods of more than a billion people (UN Millennium Project 2005) and play a key role in biodiversity conservation as well as in the sustainable use of natural resources.\nTo support the European Union’s efforts to substantially reinforce the effectiveness of international governance for biodiversity and ecosystem services  and more generally for “strengthening the capacity to mobilize and use biodiversity data, information and forecasts so that they are readily accessible to policymakers, managers, experts and other users ”, the Joint Research Centre of the European Commission has developed the Digital Observatory for Protected Areas (DOPA) to provide a large variety of end users with means to assess and monitor the state of, and pressures on protected areas at local, national, regional and global scale.\nThe DOPA is designed to contribute to the ambitious Convention on Biological Diversity (CBD) mission for 2020: to halt the loss of biodiversity and to share the values and benefits of biodiversity and ecosystem services equitably. Among the targets set to 2020 by the CBD, Parties agreed to at least halve and where feasible bring close to zero the rate of loss of natural habitats including forests and they established a target of 17 % of terrestrial and inland water areas and 10 % of marine and coastal areas to be conserved through area-based conservation measures. Parties also agreed on a strategy on resource mobilization, with a substantial increase in the level of financial resources in support of implementation of the Convention. Parties agreed to translate this overarching international framework into revised National Biodiversity Strategies and Action Plans (NBSAPs) within two years.\nThe EU is strongly committed to strengthening the instruments for achieving the global biodiversity targets and to making sure that these are effectively implemented. To ensure transparency and reusability of our work, the DOPA has been developed using open standards for spatial data and using open source programming languages.\nThe DOPA is providing end-users, through a set of web services and applications, reference information on protected areas and conservation efforts. By processing and integrating a broad range of data coming from different international institutions, we provide end-users with simple means to access the results of powerful analyses which are often challenging to perform locally. Typically, such information can help national authorities to prepare strategies and reports such as the CBD National Biodiversity Strategies and Action Plans and CBD National Reports, donors to identify areas with critical values and needs, and decision-makers to define priorities for action. By facilitating the integrated access to regularly updated baseline information, our services can contribute to the monitoring of progress towards the 2020 Aichi Biodiversity Targets. More specialized applications can help researchers to identify unique ecological habitats, quantify connectivity between areas, or the expected climate shifts in any given location. In collaboration with our end-users, we aim at providing quick-and-easy access to critical information for the people who need this information for decision making.\nThe European Commission (EC)\nDG DEVCO - The Commission's Directorate-General (DG) for International Cooperation and Development is responsible for designing European international cooperation and development policy and delivering aid throughout the world. For the past 20 years, the European Commission has been an important donor for protected area conservation, especially in Africa. Commission projects and programmes aim to improve the management of protected areas and to develop conservation techniques. The EU also seeks to boost regional co-operation and help people to share information on good practice.\nDG ENV - The DG Environment makes sure that Member States correctly apply EU environmental law. In doing so it investigates complaints made by citizens and non-governmental organisations and can take legal action if it deems that EU law has been infringed. The DG also finances projects that contribute to environmental protection in the EU, in particular through LIFE, the EU's financial instrument for the environment.\nEEAS - The European External Action Service (EEAS) is in charge of the day-to-day management of EC policies, programmes and projects since the devolution process, which aims to bring decision making and implementation closer to the beneficiaries.\nFor these Directorates and Services, the DOPA can provide valuable information both for programming at the national and regional level and for implementing programmes and projects at the local level.\nUN organisations and Multilateral Environmental Agreements\nCBD - National Biodiversity Strategies and Action Plans (NBSAPs) are the principal instruments for implementing the Convention on Biological Diversity (CBD) at the national level. The Convention requires countries to prepare a national biodiversity strategy (or equivalent instrument) and to ensure that this strategy is mainstreamed into the planning and activities of all those sectors whose activities can have an impact (positive and negative) on biodiversity. The CBD is currently using the DOPA to support the preparation of country dossiers for Aichi Targets 11 and 12, and has also suggested its Parties consult DOPA in the revision of their NBSAPs.\nUN Environment - The United Nations Environment Programme (UN Environment) is the United Nations’ designated entity for addressing environmental issues at the global and regional level. Its mandate is to coordinate the development of environmental policy consensus by keeping the global environment under review and bringing emerging issues to the attention of governments and the international community for action.\nUNDP - The United Nations Development Programme (UNDP) is helping countries to develop policies, leadership skills, partnering abilities, institutional capabilities and build resilience in order to sustain development results. UNDP helps more than 140 countries to conserve and sustainably use biodiversity, and to secure ecosystem services. The DOPA continues to be developed to meet specific information needs of both UNEP and UNDP.\nGovernments have their own local, national and international biodiversity conservation projects. Through DOPA, national and regional services in charge of protected area management can easily access important information on biodiversity values and threats in a systematic way and prioritise their interventions in the same way than EC services, or simply compare their indicators with those proposed here.\nNon-Governmental Organisations (NGOs)\nNGOs have a long history in contributing to biodiversity conservation - from local to global activities. DOPA offers tools providing information at the level of individual protected areas, facilitating the definition of local priorities. Often active on the ground, NGOs will find in DOPA simple access to reference information which can sometimes be very different from the reality. Local experts can therefore easily assess how well the local situation is represented and, in a second stage, can communicate with the data providers to correct the information used by the decision makers.\nWe have paid much attention to the needs of researchers who will want, as far as possible, to access the raw information. The information delivered by the DOPA can be generally extracted in a variety of raw formats for further use. Our applications are also designed to ease, as much as possible, access to data which are usually time consuming to access and process.\nThe below links highlight the main policies underpinning the work done by the DOPA.\n- Post 2015-UN Global Development Goals\n- CBD Aichi Targets\n- EU development and cooperation for biodiversity and ecosystems services\n- EU biodiversity strategy to 2020\nReferences to the DOPA in Policy Documents\n- UNEP/CBD/COP/22/INF/30 (May 2018)\n- UNEP/CBD/COP/22/INF/10 (May 2018)\n- UNEP/CBD/COP/20/INF/43 (March 2016)\n- UNEP/CBD/COP/20/INF/44 (April 2016)\n- UNEP/CBD/COP/13/INF/17 (November 2016)\n- UNEP/CBD/COP/13/INF/18 (November 2016)\n- SCBD/SAM/DC/SBG/LJ/84384 (March 2015)\n- UNEP/CBD/COP/11/35, Decision XI/24 Protected Areas, Paragraph 8. (October 2012)\n EC/COM/2006/0216 final""]"	['<urn:uuid:d7a573ce-66c5-40d6-be2e-18df4f19af8d>', '<urn:uuid:3ea8991e-c7a8-49b1-90a9-079f25b8c03c>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T11:40:26.317848	27	106	1768
20	oldest known percussion instrument history location	The stone marimba might be the oldest known musical instrument. It's a type of xylophone, an instrument that uses wooden or metal bars to create music. The marimba is believed to have originated in Vietnam, and one was discovered in Vietnam in 1949 near a village called Ndut Lieng Krak. The stone marimba is made from a series of flat stones, which are placed on top of each other in graduated sizes.	['Percussion is available in all sizes and shapes. However even among the many most well-known forms of drums, there are some bizarre ones you’ve most likely by no means heard of. This weblog submit will have a look at among the most weird percussion devices. You most likely haven’t heard of most of them, however they’re price testing! So with out additional ado, let’s get began.\nDaf is a percussion instrument that originates from the Center East. It’s typically thought-about odd or bizarre on account of its distinctive design and sound. The Daf is performed with the palms, producing a loud, distinct noise that may be heard clearly over different devices.\nRegardless of its simplicity, the Daf is a really versatile instrument that can be utilized in numerous types. In recent times, the Daf has gained recognition outdoors of its conventional homeland and is now performed by musicians worldwide. Whether or not you think about it odd or wondrous, there’s no denying that the Daf is a very distinctive instrument.\n2) Tassa Drum\nTassa drums are percussion devices common in lots of components of India, Pakistan, and Trinidad and Tobago. They’re often created from pores and skin stretched over a wood body and have a particular sound typically utilized in music and dance.\nTassa drums are available in numerous sizes, and they are often performed with sticks or by hand. Tassa drums are sometimes utilized in wedding ceremony ceremonies and different festive events. As well as, the Tassa drum is an integral a part of the cultural heritage of many nations.\nAshiko is a percussion instrument of African origin. It’s sometimes created from a wood log and has a deep, resonant sound. Ashiko is usually utilized in conventional music, nevertheless it may also be performed in any model of music.\nAshiko is especially common in Brazil, the place it’s utilized in numerous genres, together with samba and funk. Ashiko can also be common in the USA and is usually utilized in jazz and rock. Ashiko is a flexible instrument that may be performed solo or in an ensemble. Ashiko is an satisfying instrument to play and can certainly add pleasure to any musical efficiency.\nCC BY-SA 3.0, Hyperlink\nTabla is a well-liked percussion instrument from India. It consists of two drums, the bigger one referred to as the bayan and the smaller one referred to as the dayan. Tabla is often performed with naked palms. The Tabla is believed to have originated in historic India, and it’s nonetheless broadly utilized in conventional Indian music.\nTabla can also be common in different components of Asia, together with Pakistan and Bangladesh. Tabla may be very versatile and can be utilized for various genres of music, from classical to people to common. Tabla has a variety of sounds that may be produced, from easy rhythms to advanced patterns. The Tabla is an important a part of Indian music and tradition.\nThe Cajon is a Latin American percussion instrument that originated in Peru. It’s sometimes created from a wood field, and the entrance face is performed with the palms. Cajons are sometimes utilized in conventional music however have additionally turn into common in different genres equivalent to rock and jazz.\nCajons are comparatively simple to play and may produce a variety of sounds, from deep bass notes to high-pitched snare drums. Cajons are additionally moveable, which makes them superb for avenue musicians and buskers.\nThe Flexatone is a musical instrument with a novel sound that’s good for including a little bit of aptitude to any efficiency. Initially invented within the early 1900s, the Flexatone is a small steel disc with a deal with hooked up.\nThe participant holds the disc in a single hand and makes use of the opposite hand to strike it, producing a high-pitched, vibrating sound. The Flexatone has been utilized by many well-known musicians, together with Duke Ellington, Frank Sinatra, and Miles Davis. Immediately, the Flexatone remains to be common amongst performers who need to add a little bit of enjoyable and pleasure to their music.\nGangsa is a kind of metallophone utilized in gamelan, a standard Indonesian musical ensemble. The phrase gamelan comes from the Javanese phrase which means “to strike,” and it refers back to the percussion devices which might be used on this music.\nGangsa is often performed on a Balinese gamelan, an orchestra consisting of bronze devices. The commonest instrument in a gamelan orchestra is the gong, which is available in numerous sizes and performs a spread of pitches. Gangsa is among the hottest forms of gamelan music and is usually carried out at ceremonies and festivals.\n8) Lion’s Roar\nThe lion’s roar is among the most spectacular sounds within the animal kingdom. However do you know that it additionally makes for an incredible musical instrument?\nLion’s roar percussion devices, also called membranophones, are drums that produce sound with friction utilizing a wire when moved backwards and forwards. The lion’s roar is a very well-suited instrument for this objective, as its thick conceal produces a deep, resonant sound.\nThe Mbira is a standard African instrument that consists of a wood board with steel tines of various lengths. The participant holds the Mbira of their lap and plucks the tines with their thumbs to create a rhythmic melody.\nThe Mbira is usually utilized in spiritual ceremonies and is claimed to have a chilled impact on the listener. Its development is straightforward, however the music it produces is advanced and delightful. The Mbira is an interesting instrument that can delight any music lover.\n10) Stone marimba\nThe stone marimba would be the oldest recognized musical instrument. It’s a sort of xylophone, an instrument that makes use of wood or steel bars to create music. The marimba is believed to have originated in Vietnam, and one was found in Vietnam in 1949 close to a village referred to as Ndut Lieng Krak.\n“Marimba” comes from the Bantu phrase for “wood xylophone.” The stone marimba is created from a collection of flat stones, that are positioned on prime of one another in graduated sizes. The performer strikes the stones with mallets to create a musical word.\n11) Wind machine\nThe wind machine is a percussion instrument utilized in many classical works like Gyorgy Ligeti’s Le Grand Macabre and Edward Elgar’s The Starlight Specific.\nAlso called an aeoliphone, the wind machine is performed by rotating a crank deal with that creates friction between wood slats. Wind machines are sometimes utilized in movie and tv scores, as their otherworldly tones can add a component of thriller or suspense to a scene.\nThe zabumba is a Brazilian bass drum that’s sometimes utilized in Samba music. It’s a massive, spherical drum performed with the palms, and it has a definite, deep sound that may be heard over the opposite devices in a Samba band.\nThe zabumba is believed to have originated within the African nation of Angola, and it was dropped at Brazil by enslaved Africans. Immediately, the zabumba is an important a part of Samba music, and it’s typically utilized in Carnaval celebrations.\nThe babendil is a standard Filipino gong instrument often created from bronze or brass and produces a metallic clang. It’s generally known as a “false gong.” Whereas the instrument is probably not used as a lot in kulintang ensembles anymore, it’s an integral a part of Filipino cultural heritage.\n14) Batá drum\nThe Batá drum is a sacred percussion instrument used within the spiritual ceremonies of the Yoruba individuals of Nigeria. The drum is historically created from the pores and skin of a cow or antelope, stretched over a wood body. Batá drums are available in numerous sizes, with the most important drum sometimes reserved for the Oba, or king.\nThe drums are historically embellished with Cowrie shells, that are believed to characterize the spirits of ancestors. The rhythms performed on the Batá drums are advanced and fluctuate relying on the precise ceremony being carried out.\nIn recent times, the Batá drum has gained recognition outdoors of its conventional cultural context and may now be present in music lecture rooms and live performance halls worldwide.\nThe bedug is a percussion instrument utilized in gamelan music. It’s a massive drum suspended from a body and performed with two sticks. The bedug has a deep, resonant sound that’s good for creating an environment of suspense or drama.\nAlong with its musical makes use of, the bedug can also be generally used as a signaling machine. For instance, it’s historically used to sign the start of prayer time at mosques. The bedug can also be generally utilized in wayang kulit performances, offering the puppets with sound results.\nThat’s a wrap on our record of bizarre percussion devices. We hope you loved studying about these distinctive devices as a lot as we loved writing about them! And don’t overlook to go away us a remark under, letting us know which is your favourite uncommon percussion instrument.']	['<urn:uuid:bc360e8b-ec5d-40c0-be27-a60004312b44>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T11:40:26.317848	6	72	1492
21	What were the unique characteristics of wine production in ancient Taianfu and how significant was it for the region?	Taianfu was unique in that it was the only place in all of Cathay where wine was produced. The region had many excellent vines that supplied great plenty of wine. The wine production was significant enough that it was distributed throughout the country. Historical records indicate that the wine of T'ai-yuan fu was so celebrated during the T'ang Dynasty that it was sent as tribute to the Emperors. Under Mongol rule, the use of this wine became more widespread. However, when the Ming Dynasty began, while the founder accepted wine offerings from T'ai-yuan in 1373, he subsequently prohibited such presentations.	"['The Travels of Marco Polo/Book 2/Chapter 37\nThe Kingdom of Taianfu\nAfter riding then those ten days from the city of Juju, you find yourself in a kingdom called TAIANFU, and the city at which you arrive, which is the capital, is also called Taianfu, a very great and fine city. [But at the end of five days\' journey out of those ten, they say there is a city unusually large and handsome called ACBALUC, whereat terminate in this direction the hunting preserves of the Emperor, within which no one dares to sport except the Emperor and his family, and those who are on the books of the Grand Falconer. Beyond this limit any one is at liberty to sport, if he be a gentleman. The Great Kaan, however, scarcely ever went hunting in this direction, and hence the game, particularly the hares, had increased and multiplied to such an extent that all the crops of the Province were destroyed. The Great Kaan being informed of this, proceeded thither with all his Court, and the game that was taken was past counting.]\nTaianfu is a place of great trade and great industry, for here they manufacture a large quantity of the most necessary equipments for the army of the Emperor. There grow here many excellent vines, supplying great plenty of wine; and in all Cathay this is the only place where wine is produced. It is carried hence all over the country. There is also a great deal of silk here, for the people have great quantities of mulberry-trees and silk-worms.\nFrom this city of Taianfu you ride westward again for seven days, through fine districts with plenty of towns and boroughs, all enjoying much trade and practising various kinds of industry. Out of these districts go forth not a few great merchants, who travel to India and other foreign regions, buying and selling and getting gain. After those seven days\' journey you arrive at a city called PIANFU, a large and important place, with a number of traders living by commerce and industry. It is a place too where silk is largely produced.\nSo we will leave it and tell you of a great city called Cachanfu. But stay--first let us tell you about the noble castle called Caichu.\n- Marsden translates the commencement of this passage, which is peculiar to Ramusio, and runs ""E in capo di cinque giornate delle predette dieci,"" by the words ""At the end of five days\' journey beyond the ten,"" but this is clearly wrong. The place best suiting in position, as halfway between Cho-chau and T\'ai-yuan fu, would be CHENG-TING FU, and I have little doubt that this is the place intended. The title of Ak-Baligh in Turki, or Chaghan Balghasun in Mongol, meaning ""White City,"" was applied by the Tartars to Royal Residences; and possibly Cheng-ting fu may have had such a claim, for I observe in the Annales de la Prop. de la Foi (xxxiii. 387) that in 1862 the Chinese Government granted to the R.C. Vicar-Apostolic of Chihli the ruined Imperial Palace at Cheng-ting fu for his cathedral and other mission establishments. Moreover, as a matter of fact, Rashiduddin\'s account of Chinghiz\'s campaign in northern China in 1214, speaks of the city of ""Chaghan Balghasun which the Chinese call Jintzinfu."" This is almost exactly the way in which the name of Cheng-ting fu is represented in \'Izzat Ullah\'s Persian Itinerary (Jigdzinfu, evidently a clerical error for Jingdzinfu), so I think there can be little doubt that Cheng-ting fu is the place intended. The name of Hwai-luh\'ien (see Note 2), which is the first stage beyond Cheng-ting fu, is said to mean the ""Deer-lair,"" pointing apparently to the old character of the tract as a game-preserve. The city of Cheng-ting is described by Consul Oxenham as being now in a decayed and dilapidated condition, consisting only of two long streets crossing at right angles. It is noted for the manufacture of images of Buddha from Shan-si iron. (Consular Reports, p. 10; Erdmann, 331.) [The main road turns due west at Cheng-ting fu, and enters Shan-si through what is known among Chinese travellers as the Ku-kwan, Customs\' Barrier.--H.C.] Between Cheng-ting fu and T\'ai-yuan fu the traveller first crosses a high and rugged range of mountains, and then ascends by narrow defiles to the plateau of Shan-si. But of these features Polo\'s excessive condensation takes no notice. The traveller who quits the great plain of Chihli [which terminates at Fu-ch\'eng-i, a small market-town, two days from Pao-ting.--H.C.] for ""the kingdom of Taianfu,"" i.e. Northern Shan-si, enters a tract in which predominates that very remarkable formation called by the Chinese Hwang-tu and to which the German name Loess has been attached. With this formation are bound up the distinguishing characters of Northern Interior China, not merely in scenery but in agricultural products, dwellings, and means of transport. This Loess is a brownish-yellow loam, highly porous, spreading over low and high ground alike, smoothing over irregularities of surface, and often more than 1000 feet in thickness. It has no stratification, but tends to cleave vertically, and is traversed in every direction by sudden crevices, almost glacier-like, narrow, with vertical walls of great depth, and infinite ramification. Smooth as the loess basin looks in a bird\'s-eye view, it is thus one of the most impracticable countries conceivable for military movements, and secures extraordinary value to fortresses in well-chosen sites, such as that of Tung-kwan mentioned in Note 2 to chap. xli. Agriculture may be said in N. China to be confined to the alluvial plains and the loess; as in S. China to the alluvial plains and the terraced hill-sides. The loess has some peculiar quality which renders its productive power self-renewing without manure (unless it be in the form of a surface coat of fresh loess), and unfailing in returns if there be sufficient rain. This singular formation is supposed by Baron Richthofen, who has studied it more extensively than any one, to be no subaqueous deposit, but to be the accumulated residue of countless generations of herbaceous plants combined with a large amount of material spread over the face of the ground by the winds and surface waters. [I do not agree with the theory of Baron von Richthofen, of the almost exclusive Eolian formation of loess; water has something to do with it as well as wind, and I think it is more exact to say that loess in China is due to a double action, Neptunian as well as Eolian. The climate was different in former ages from what it is now, and rain was plentiful and to its great quantity was due the fertility of this yellow soil. (Cf. A. de Lapparent, Lecons de Geographie Physique, 2\'e ed. 1898, p. 566.)--H.C.] Though we do not expect to find Polo taking note of geological features, we are surprised to find no mention of a characteristic of Shan-si and the adjoining districts, which is due to the loess; viz. the practice of forming cave dwellings in it; these in fact form the habitations of a majority of the people in the loess country. Polo has noticed a similar usage in Badakhshan (I. p. 161), and it will be curious if a better acquaintance with that region should disclose a surface formation analogous to the loess. (Richthofen\'s Letters, VII. 13 et passim.)\n- Taianfu is, as Magaillans pointed out, T\'AI-YUAN FU, the capital of the Province of Shan-si, and Shan-si is the ""Kingdom."" The city was, however, the capital of the great T\'ang Dynasty for a time in the 8th century, and is probably the Tajah or Taiyunah of old Arab writers. Mr. Williamson speaks of it as a very pleasant city at the north end of a most fertile and beautiful plain, between two noble ranges of mountains. It was a residence, he says, also of the Ming princes, and is laid out in Peking fashion, even to mimicking the Coal-Hill and Lake of the Imperial Gardens. It stands about 3000 feet above the sea [on the left bank of the Fen-ho.--H.C.]. There is still an Imperial factory of artillery, matchlocks, etc., as well as a powder mill; and fine carpets like those of Turkey are also manufactured. The city is not, however, now, according to Baron Richthofen, very populous, and conveys no impression of wealth or commercial importance. [In an interesting article on this city, the Rev. G. B. Farthing writes (North China Herald, 7th September, 1894): ""The configuration of the ground enclosed by T\'ai-yuan fu city is that of a \'three times to stretch recumbent cow.\' The site was chosen and described by Li Chun-feng, a celebrated professor of geomancy in the days of the T\'angs, who lived during the reign of the Emperor T\'ai Tsung of that ilk. The city having been then founded, its history reaches back to that date. Since that time the cow has stretched twice.... T\'ai-yuan city is square, and surrounded by a wall of earth, of which the outer face is bricked. The height of the wall varies from thirty to fifty feet, and it is so broad that two carriages could easily pass one another upon it. The natives would tell you that each of the sides is three miles, thirteen paces in length, but this, possibly, includes what it will be when the cow shall have stretched for the third and last time. Two miles is the length of each side; eight miles to tramp if you wish to go round the four of them.""--H. C.] The district used to be much noted for cutlery and hardware, iron as well as coal being abundantly produced in Shan-si. Apparently the present Birmingham of this region is a town called Hwai-lu, or Hwo-luh\'ien, about 20 miles west of Cheng-ting fu, and just on the western verge of the great plain of Chihli. [Regarding Hwai-lu, the Rev. C. Holcombe calls it ""a miserable town lying among the foot hills, and at the mouth of the valley, up which the road into Shan-si lies."" He writes (p. 59) that Ping-ting chau, after the Customs\' barrier (Ku Kwan) between Chih-li and Shan-si, would, under any proper system of management, at no distant day become the Pittsburg, or Birmingham, of China.--H.C.] (Richthofen\'s Letters, No. VII. 20; Cathay, xcvii. cxiii. cxciv.; Rennie, II. 265; Williamson\'s Journeys in North China; Oxenham, u.s. II; Klaproth in J. As. ser. II. tom. i. 100; Izzat Ullah\'s Pers. Itin. in J.R.A.S. VII. 307; Forke, Von Peking nach Ch\'ang-an, p. 23.) [""From Khavailu (Hwo-luh\'ien), an important commercial centre supplying Shansi, for 130 miles to Sze-tien, the road traverses the loess hills, which extend from the Peking-Kalgan road in a south-west direction to the Yellow River, and which are passable throughout this length only by the Great Central Asian trade route to T\'ai-yuan fu and by the Tung-Kwan, Ho-nan, i.e. the Yellow River route. (Colonel Bell, Proc.R.G.S. XII. 1890, p. 59.) Colonel Bell reckons seven days (218 miles) from Peking to Hwo-lu-h\'ien and five days from this place to T\'ai-yuan fu.""--H.C.]\n- Martini observes that the grapes in Shan-si were very abundant and the best in China. The Chinese used them only as raisins, but wine was made there for the use of the early Jesuit Missions, and their successors continue to make it. Klaproth, however, tells us that the wine of T\'ai-yuan fu was celebrated in the days of the T\'ang Dynasty, and used to be sent in tribute to the Emperors. Under the Mongols the use of this wine spread greatly. The founder of the Ming accepted the offering of wine of the vine from T\'aiyuan in 1373, but prohibited its being presented again. The finest grapes are produced in the district of Yukau-hien, where hills shield the plain from north winds, and convert it into a garden many square miles in extent. In the vintage season the best grapes sell for less than a farthing a pound. [Mr. Theos. Sampson, in an article on ""Grapes in China,"" writes (Notes and Queries on China and Japan, April, 1869, p. 50): ""The earliest mention of the grape in Chinese literature appears to be contained in the chapter on the nations of Central Asia, entitled Ta Yuan Chwan, or description of Fergana, which forms part of the historical records (Sze-Ki) of Sze-ma Tsien, dating from B.C. 100. Writing of the political relations instituted shortly before this date by the Emperor Wu Ti with the nations beyond the Western frontiers of China, the historian dwells at considerable length, but unluckily with much obscurity, on the various missions despatched westward under the leadership of Chang K\'ien and others, and mentions the grape vine in the following passage:--\'Throughout the country of Fergana, wine is made from grapes, and the wealthy lay up stores of wine, many tens of thousands of shih in amount, which may be kept for scores of years without spoiling. Wine is the common beverage, and for horses the mu-su is the ordinary pasture. The envoys from China brought back seeds with them, and hereupon the Emperor for the first time cultivated the grape and the mu-su in the most productive soils.\' In the Description of Western regions, forming part of the History of the Han Dynasty, it is stated that grapes are abundantly produced in the country of K\'i-pin (identified with Cophene, part of modern Afghanistan) and other adjacent countries, and referring, if I mistake not, to the journeys of Chang K\'ien, the same work says, that the Emperor Wu-Ti despatched upwards of ten envoys to the various countries westward of Fergana, to search for novelties, and that they returned with grape and mu-su seeds. These references appear beyond question to determine the fact that grapes were introduced from Western- or, as we term it, Central-Asia, by Chang K\'ien."" Dr. Bretschneider (Botanicon Sinicum, I. p. 25), relating the mission of Chang K\'ien (139 B.C. Emperor Wu-Ti), who died about B.C. 103, writes:--""He is said to have introduced many useful plants from Western Asia into China. Ancient Chinese authors ascribe to him the introduction of the Vine, the Pomegranate, Safflower, the Common Bean, the Cucumber, Lucerne, Coriander, the Walnut-tree, and other plants.""--H.C.] The river that flows down from Shan-si by Cheng-ting-fu is called ""Putu-ho, or the Grape River."" (J. As. u.s.; Richthofen, u.s.) [Regarding the name of this river, the Rev. C. Holcombe (l.c. p. 56) writes: ""Williamson states in his Journeys in North China that the name of this stream is, properly Poo-too Ho--\'Grape River,\' but is sometimes written Hu-t\'ou River incorrectly. The above named author, however, is himself in error, the name given above [Hu-t\'o] being invariably found in all Chinese authorities, as well as being the name by which the stream is known all along its course."" West of the Fan River, along the western border of the Central Plain of Shan-si, in the extreme northern point of which lies T\'ai-yuan fu, the Rev. C. Holcombe says (p. 61), ""is a large area, close under the hills, almost exclusively given up to the cultivation of the grape. The grapes are unusually large, and of delicious flavour.""--H.C.]\n- +In no part of China probably, says Richthofen, do the towns and villages consist of houses so substantial and costly as in this. Pianfu is undoubtedly, as Magaillans again notices, P\'ING-YANG FU. It is the Bikan of Shah Rukh\'s ambassadors. [Old P\'ing yang, 5 Lis to the south] is said to have been the residence of the primitive and mythical Chinese Emperor Yao. A great college for the education of the Mongols was instituted at P\'ing-yang, by Yeliu Chutsai, the enlightened minister of Okkodai Khan. [Its dialect differs from the T\'ai-yuan dialect, and is more like Pekingese.] The city, lying in a broad valley covered with the yellow loess, was destroyed by the T\'ai-P\'ing rebels, but it is reviving. [It is known for its black pottery.] The vicinity is noted for large paper factories. [""From T\'ai-yuan fu to P\'ing-yang fu is a journey of 185 miles, down the valley of the Fuen-ho."" (Colonel Bell, Proc.R.G.S. XII. 1890, p. 61.) By the way, Mr. Rockhill remarks (Land of the Lamas, p. 10): ""Richthofen has transcribed the name of this river Fuen. This spelling has been adopted on most of the recent maps, both German and English, but Fuen is an impossible sound in Chinese."" (Read Fen ho.)--H.C.] (Cathay, ccxi.; Ritter, IV. 516; D\'Ohsson, II. 70; Williamson, I. 336.)']"	['<urn:uuid:ade7a8e1-6128-41bb-a990-11b10afe4112>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T11:40:26.317848	19	100	2742
22	archaeology expert seeking compare collection focus istanbul archaeology museums national museum iran	The Istanbul Archaeology Museums focuses on classical artifacts spanning over 5000 years, including Hellenic, Hellenistic, Roman, and Ottoman collections, while the National Museum of Iran primarily focuses on Persian archaeological finds, with pieces from Persepolis, Shush, and other Iranian sites, divided into ancient Iran and Islamic Iran sections.	"['I’m sure a visit to Istanbul’s Archaeology Museums (Arkeoloji Müzeleri) is not exactly what you have in mind while planning your city trip.\nBut you should definitely consider adding this stunning complex of museums to your to-do list. For two reasons: its overwhelming state of the art antiquities collection spanning over 5000 years, and the fact that the items for once are well-lit and well-labeled.\nArchaeology Museums (Arkeoloji Müzeleri)\nOsman Hamdi Bey Yokuşu, Eminönü (down the slope at the left in Topkapi Palace’s first courtyard).\nTel: +90 212 520 77 40\non Map with Tourist Attractions in the Historical Part of Istanbul\nDaily between 09.00 and 17.00 (no entrance after 16.00). Closed on Monday. On the first day of religious holidays, the museum is closed until 12.00.\nThe entrance fee is TL 10. Credit cards are accepted.\nIstanbul Archaeology Museums actually consists of three museums in one complex: the Museum of Ancient Orient (Eski Şark Eserleri Müzesi), the Tiled Pavilion Museum (Çinili Köşk Müzesi) and the Archaeology Museum (Arkeoloji Müzesi) itself residing in the main building.\nThe main building was commissioned by archaeologist and painter Osman Hamdi (1881-1910). Late 19th century, the museum was founded to stop the flow of artifacts from the empire to Europe and house his discoveries. Osman Hamdi became the museum director. Soon after the inauguration, local governors spread out over the Ottoman Empire sent in a huge amount of objects. Today the museums have one of the world’s richest collections of classical artifacts on display. (also see the Archaeology Museums picture gallery)\nMuseum of the Ancient Orient\nThe Museum of the Ancient Orient is the first building on your left upon entering the museum complex. The building, built in 1883, houses pieces from the pre-Islamic Arabian peninsula, Mesopotamia (currently Iraq), Egypt and Anatolia (mainly Hittite empires). Don’t miss:\n- a Hittite copy of the famous Treaty of Kadesh (1269) between the Egyptian and the Hittite empires\n- the Ishtar gate of ancient Babylon, dating back to reign of Nebuchadnezzar II\n- the glazed brick panels depicting various animals\nThe Archaeology Museum is located in the biggest building in the complex and consists of four floors:\n- ground floor: (of the old building) classical archaeology, featuring a collection of Hellenic, Hellenistic and Roman statuary and sarcophagi (in the old building). Don’t miss:\n- a Roman statue of Bes, half-god of inexhaustible power and strength and the protector against evil.\n- a group of sarcophagi from the Royal Necropolis of Sidon, unearthed in 1887\n- the Alexander Sarcophagus (4th century B.C.), depicting him battling the Persians as well as a hunting scene\n- the Sarcophagus of the Mourning Women\n- ground floor: Thracian, Bithynian and Byzantine collections and the children’s museum, containing a huge Trojan Horse they can climb into\n- first floor: Istanbul through the ages. A nice chronological overview of Istanbul’s archaeological past. Don’t miss:\n- second floor: collections from Anatolia and Troy\n- third floor: Anatolia’s neighboring cultures, a gallery devoted to Cyprus and Syria-Palestine\nTiled Pavilion Museum\nThe third and last building in the complex is the tiled kiosk of Sultan Mehmet the Conqueror. The kiosk was built in 1472 and is one of the oldest examples of Ottoman civil architecture in Istanbul. The collection, on display in the six rooms and saloon, consists of various chinaware and ceramics from the Seljuk and Ottoman period.\nHelp, I’m on a Tight Schedule\nIf you want to see every item on display and read the excellent accompanying explanatory labels in both English and Turkish, you’ll need more than one day. So if you’re pressed for time, make sure you at least visit the breathtaking sarcophagi and Istanbul through the ages. If you have young children, also make a brief stop at their museum.', ""Detailing layers and layers of Iran's rich history, National Museum of Iran displays an impressive collection of masterpieces collected from Persepolis, Shush, and other major archeological sites across the country. Admire a masterfully sculpted mastiff found in Persepolis, and contemplate a marble statue of Darius. With two major sections--one devoted to ancient Iran and the other to Islamic Iran--the institution boasts a superb collection of prehistoric and early history pieces with pottery, ceramics, stone figures, and carvings (all labeled in Farsi and English). Set in a large brick building designed by a French architect, the museum tells a coherent story about the continuity of civilization in modern-day Iran. Guides and audio tours are available. Use our Tehran driving holiday planner to arrange your visit to National Museum of Iran and other attractions in Tehran.\nNational Museum of Iran reviews\nThe museum is a classified museum which clearly shows the Iran during the centuries and dynasties. Visiting it is highly recommended. more »\nif you have any interest in history, this museum is a must-see in Iran. it is practically the largest and reachest museum owing to the archeology of Persian lands through the history. more »\nAmazing collections. If you are in Tehran and want to learn more about the history of the Persian Empire. Come by this museum. One feedback i can give is that they can put up more English language caption for each artifacts? I went with a friend from Ahvaz and he became my guide. :) Outside of the musuem. You can find some really good food place. I forgot the name of the street. But yeah. Just walk around. It's fun!\nVery nice building. The architecture follows the Sasanid-era palaces. You can find some fine pieces from the Persepolis in the museum, but otherwise the museum is surprisingly small. There is an overpriced museum shop.\nPlan your trip to Tehran\nGet a personalized planA complete day by day itinerary\nbased on your preferences\nCustomize itRefine your plan. We'll find the\nbest routes and schedules\nBook itChoose from the best hotels\nand activities. Up to 50% off\nManage itEverything in one place.\nEveryone on the same page.\nSee all ›\nSee all ›\nSee all ›\nHidden gems in Tehran\nSide trips from Tehran\nPlans to National Museum of Iran by other users\n7 days in Iran BY A USER FROM IRAN November, culture, historic sites, shopping, wildlife, popular sights Preferences: November, culture, historic sites, shopping, wildlife Attraction style: Popular sights Pace: Medium 7 days in Iran BY A USER FROM IRAN November, popular sights Preferences: November Attraction style: Popular sights Pace: Medium 13 days in Iran BY A USER FROM GERMANY October, popular sights Preferences: October Attraction style: Popular sights Pace: Medium 19 days in Tehran BY A USER FROM GERMANY October, popular sights Preferences: October Attraction style: Popular sights Pace: Medium 4 days in Iran BY A USER FROM IRAN October, teens, kids, culture, romantic, shopping, wildlife, fast-paced, popular & hidden gems Preferences: October, teens, kids, culture, romantic, shopping, wildlife Attraction style: Popular & hidden gems Pace: Fast-paced 18 days in Iran BY A USER FROM ESTONIA July, historic sites, slow & easy, popular & hidden gems Preferences: July, historic sites Attraction style: Popular & hidden gems Pace: Slow & easy""]"	['<urn:uuid:ee18b9ba-d330-4be6-8668-851da466ee42>', '<urn:uuid:d377e6d8-c90c-4a76-95c2-1ddc3b4bb394>']	factoid	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-12T11:40:26.317848	12	48	1175
23	As a fashion historian studying traditional garments, I'm curious about how marriage status affected women's fashion choices in Korean hanbok and Ebdurian clothing - did unmarried women have specific colors or styles in both cultures?	Yes, marriage status influenced women's fashion in both cultures. In Korean tradition, unmarried women wore a specific combination of red skirt and yellow jacket in their hanbok. In Ebdurian culture, marriage affected clothing through the gender system - particularly with kanihā women who were often adopted into families for marriage arrangements and wore simpler clothing consisting of a short lajomàm and yanauta, different from the elaborate patchwork lajodàn worn by other women.	['The Late (Imperial) Third Concord (c. 1670-1808)\nThe dominant fashion of this period was for patchwork. Originally a sign of poverty, deployed by those who could not afford new clothes, they became popular among the rich, affected a new simplicity by contrast with the formality of the gentry. Patchwork was also, however, a way to display wealth – both because of the excessive labour required to construct clothes entirely from small, and later irregular, patches, and because those patches could each be valuable. Patches would be of cloth-of-gold, or rijnui, or silk, or sandflax, or be covered with delicate embroidery, or dyed violent hues.\nIn consequence of this mania for patchwork, the “hoops” of the lajodàn were thinned, and made less numerous, to create more space for the patches that were now the centre of attention and wealth.\nThe lajodàn also underwent several other dramatic changes, perhaps due to the amount of sewing that was now required in their creation. They were no longer convenient wraps of cloth, and this was realised: the shape of the cloth itself was now altered, with deep cuts being made to display the breasts, and flaps added to cover the shoulders, tied at the back, and tall collars to cover the back of the neck, constructed on light wooden frames. The strips of cloth used for the skirt part of the garment were now made longer than the upper parts, so that pleats could be sown in to them.\nFinally, the sheet of cloth that had formally descended from the hat to cover the back of the neck expanded around the brim until it covered the entire head from all directions – this simultaneously expressed the purity of the woman and her ability to afford delicate laces and thin gauzes through which she could see without being seen.\nHowever, not all women wore these clothes, and not only for class reasons. Around this time, a new gender was being created, the kanihā, whose nature is difficult to understand without reference to the marriage system. Ebdurian upper-class marriages are group marriages, where the core is a number of sisters, often partnered to a number of brothers. To balance numbers between families, and to build closer alliances, it became common for families with many daughters to send some of them away to be adopted into other families – marriage cells could thus include two tiers of women, the original daughters (the society being matrilineal) and their adopted sisters. Kanihā were originally simply those women who were adopted into other families – but in this period, this fact became associated with certain traits, and in the following century they were finally recognised as a distinct gender: the gender of the sort of female who would normally be adopted out, regardless of whether in fact they were. In this period, they were not a gender, but they were recognised as distinct.\nKanihā occupied a subordinate role in their families, and thus were clad rather more simply. In effect, the old clothing of the middle classes became the clothing of the kanihā: a short lajomàm and a yanauta. The yanauta, however, developed away from a simple band of cloth: the two front edge were now cut to line up, the corners were extended to reach over the shoulder without pulling the fabric, and the bottom hemline dropped down to the waist, ending the plebeian exposure of the stomach.\nKimyō were in this period prohibited from wearing the itàko and the lajotō, which were reserved now for true mahikò only. To circumvent regulations regarding the lajotō, they adopted the female lajomàm, which was in any case of a similar length – the only difference being that the lajotō had been of two colours, had been pieced together from several strips, and had several hoop-like hems. In order to retain continuity with this garment, they chose to decorate their new lajomàms with bands of design in imitation of the multiple hems of the lajotō. Prohibited from wearing the itàko, they simply wore no upper garment other than their banātiyosa – indeed, they considered that they had never worn any upper garment, as the itàko still was not incorporated into norms of “garments” – it was not a lajo, and they continued to wear no upper lajo.\nThe mahikò, meanwhile, refused to adopt the new patchwork mania of the lajodàn, seen as material and commercial, and retained the “purity” of their lajotō, even reducing their number of hems. They did, however, adopt the veil of the tàniko. Their banātiyosa expanded to cover the whole of the upper body, but most of it was hidden, as the itàko expanded down to the knees, and the sleeves became increasingly long and deep, to impractical degrees – this was by this time the costume of a religious elite.\nFor men, the great change in this time was the deprecation of the military – paradoxical, at a time of great military expansion. However, due to the land-pensions given to veterans, and the increasingly unified and clan-independent structure of the armed forces, the military was seen as a vehicle for the nouveau riche – and even military officers began to stress their respect for old norms and customs. Out went the fabric vambraces. Out went the giant banātiyosa – the new form would never extend higher than the naval. The ban on the male itàko was no problem at all – the same garment was made out of non-felt fabrics, and tucked under the banātiyosa, rather than hanging over it. Other changes were made at the same time – the sewn sleeves were unsewn, leaving drapes of cloth to hang over the upper arms, and the collars became looser, cut down to the sternum to expose the chest. The new garment was called an arnyahà, or “banner”, as it was marked with heraldic devices. Over this would often be worn a simple cloak, fastened at the neck.\nMen, however, were not quite so uniform as they had once been. A new gender was at this time being recognised – a new kind of male, disdainful of women and social mores, often dedicated to the armed forces, and generally more egocentric, due perhaps to the new freedom from their matrilineal families and duties to children and relatives. This ‘”type” of male had been recognised for some time, but it was only now that they became considered an independent gender, the ortu. The ortu dressed simply and without pretence – they discarded the lajomàm for the cheaper and more practical kinyajo, and did not bother, at least in their free time, with the arnyahà (which was, however, required formal dress in the armed forces), wearing only a cloak. They retained the wide banātiyosa out of military pride, but made it a simple cloth sash.\nFinally, the range of hat designs expanded exponentially in this period, as the basic straw structure was deformed in novel and ever-changing ways. Women’s hats began to be adorned with displays of exotic feathers – the most expensive sporting full plumages of feathers falling from the hat’s peak in an elaborate tail.\nDesign patterns, too, proliferated – but no garment was complete without a number of iratti, the symbol of the Empire and its faith – a number (usually seven or nine) of thick radiating branches, each sporting many thin and fractal fronds of ferns.', 'The Korean hanbok isn’t as well-known as other traditional costumes throughout Asia. But it has a long history and a heavy significance for the Korean culture. If you’re taking a trip to Korea, you might get the chance to wear hanbok. And if you’re going to wear it, then you need to do it right because it isn’t as simple as it might appear.\nWhat Is The Hanbok?\nThere are two main pieces to the hanbok. On the top, both men and women wear a jacket known as a jeogori. On the bottom, women wear a long skirt called a chima. The skirt is quite large, with billowing folds of fabric, and reaches all the way down to the ground. Men wear pants called baji. They’re large and roomy and both the men’s and the women’s version of hanbok is very comfortable.\nHanbok isn’t worn as an everyday garment anymore. These days, most people wear a hanbok only to formal occasions or special events. This includes weddings and New Year’s Day. However, perhaps because of the importance of the hanbok in Korean culture, this historical garment is making a comeback.\nThe History Of The Hanbok\nThe hanbok has a very long history in Korea, stretching back to the Goguryeo Dynasty. This dynasty was one of the Three Ancient Kingdoms of Korea and stretched from 37 BCE to 668 CE. However, hanboks today look very different to when they first become fashionable. In these early days of the hanbok, both men and women first wore short baji or trousers and a waist-length jacket.\nOver the years, the hanbok was influenced by cultures like the Mongolian empire and internal fashion trends. This resulted in changes to the hanbok. For example, towards the end of the Three Kingdoms era, women started wearing longer skirts and shorter jackets that cinched at the waist. And men followed these trends by wearing looser pants and jackets that bound at the waist.\nThe colours and patterns of a hanbok once held significant meanings. Class and marital status were the most important determiners of what people wore. Commoners wore white cotton most of the time, though they were sometimes allowed to wear pink, green, or grey. However, the upper classes had a huge range of choices.\nNoblemen and women wore more colours in their hanbok, but there were strict rules governing the use of these colours. Children and young girls could wear bright colours, but older women wore more subdued shades. Unmarried women wore a red skirt and yellow jacket, and married women wore green and red or blue after they had given birth to a son. And the patterns were tightly regulated to flowers or bats.\nMembers of the royal family got the most choice in hanbok colours and patterns. These held a variety of meanings. Queens wore images of phoenixes, kings wore fire, dragons, mountains, or water plants, and princesses wore butterflies, cranes and lotus flowers. Many of these patterns could only be worn by members of the royal family.\nThe Modern Hanbok\nWith all these influences, the modern hanbok is both different and similar to these earlier fashions. They’re still baggy and comfortable to wear. Men wear baggy pants that are cinched at the ankle and tunic-like jackets. Women wear the floor-length, voluminous skirt and a short jacket that ties at the waist.\nBut what has really changed is the colours and patterns available. The hanbok has been getting a facelift to fit with modern life. These days, bright colours and quirky details and patterns are all the rage as the hanbok goes to the streets of Korea. New fabrics such as denim and gingham are making the style more contemporary and more relevant to modern fashionistas.\nAnd it’s working. Young couples now show off their hanboks on tours and on the streets. It’s quite literally a rebirth and one that should be nurtured.\nToday, there are several different types of hanbok available today for different occasions. These include:\nThe Dol hanbok, worn on a child’s first birthday.\nThe Hollyebok or wedding hanbok, which is more ornamental than other types.\nHoegabyeon hanbok, to be worn on someone’s 61st birthday to wish them a long life.\nMyeongjeol hanbok, which is worn for the Lunar New Year.\nHow To Wear A Hanbok\nIf you travel to Korea, you might get the chance to wear these traditional clothings. It’s quite common for people to wear them on tours of historical palaces. In fact, many iconic landmarks in Korea offer free entry to anyone wearing a hanbok. So, look out for these deals when you’re travelling in South Korea.\nThe modern hanbok is fairly easy to wear, but not easy to put on. Here’s what you’ll need to do:\n1. Put on the undergarment dress, which usually fastens in the front.\n2. If it’s cold, you can wear jeans or trousers underneath.\n3. Wrap the skirt around you with the fabric out and the white surface inside.\n4. Cross the ribbon behind you and tie it in a bow at the front.\n5. Position the skirt at the bust level.\n6. Put the jacket around your shoulders and do up the clasp or button.\n7. Put the short ribbon on the long one, making an ‘X’ and tie a knot.\n8. Make a loop with the long ribbon and pass the short over, under, and grab it on the other side.\n9. Pull the short ribbon to tighten the knot, which should make both ribbons the same length and complete the outfit.\nYou can wear a vest over the top of the hanbok if it’s cold, or just go out and start exploring the city.\nPutting on a hanbok if you’re a man can be a little complicated too. Here’s how:\n1. Put a white t-shirt underneath.\n2. Gather the leftover fabric on the right side and cross it to the left.\n3. Hold the fabric down and cross the belt over it.\n4. Knot the belt in the centre and fold the waistband down to cover the belt.\n5. Pull-on the white socks that are worn with the hanbok, ensuring that the seams are straight.\n6. Position the pants’ seam on the ankle bone and pull the excess fabric outwards before wrapping it around your ankle.\n7. Wrap the ankle strap twice around the fabric to secure it and tie it on the inside of your leg.\n8. Repeat with the other foot.\n9. Put the jacket on.\n10. Tie the inner fastening ribbon on the left.\n11. Tie the outer ribbon.\n12. Wear the vest over the jacket and do up the buttons.\nHow To Tie A Hanbok?\nTying the Korean hanbok is probably the trickiest part of wearing traditional attire. Also known as Otkorum, most people get a lot of trouble in this part and often turn the experience into chaos. Remember the first time you did a necktie? Just like that, you might also get everything tangled. To help you pull-through, here’s a step-by-step guide of tying an Otkorum:\n1. The jeogori has two ribbons, a short one on the right and a long one on the left. Begin by letting them hang down.\n2. Cross the shorter ribbon over the longer one to form an “X”, then make a loop under the left ribbon using the right one to make a simple knot.\n3. Using the ribbon that now hangs on the left (formerly the right ribbon), forms a loop by wrapping it around your left palm.\n4. Fold the other ribbon by its center then tuck the edge inside the loop you made in your left palm before pulling it on all the way.\n5. Take off your hand from the loop and gently tighten it by pulling the folded edge of the long ribbon. The final result should look like a half-done bow tie with the two ribbons hanging on your right chest.\nWhen tying the hanbok, the knot shouldn’t be too tight and the loop should naturally lie on your chest parallel to the ceiling. This type of doing Otkorum applies to both men and women.\nWhere To Buy Modern Hanbok In Seoul\nRight before hanboks completely go out of fashion in Korea, designers reinvent the dress to match today’s fashion trend. From being a traditional attire, some are coming up with a simplified version tailored for daily wear. Some people see this as a downright affront to the tradition, but some argue that it is a way to preserve it.\nTrue enough, hanboks are starting to revive, but this time as part of the Korean street fashion. In Seoul alone, you’ll see a handful of girls wearing the twisted version of Korean hanbok. Matter of fact, you won’t recognize it as hanbok on a first glance until you look closely.\nPresently, there are limited stores providing the so-called modern hanboks in Seoul. Should you find one, they tend to be a bit pricey. If you are looking forward to purchasing one, here are some modern hanbok stores you can check out.\nKwangjang Market is one of the biggest traditional markets in Seoul so it won’t be impossible to find what you are looking for here. All you need is some digging.\nThere are a handful of stores in Kwangjang that offer ready-made hanboks for different occasions like birthdays and weddings. Ready-to-wear hanboks are a lot cheaper compared to tailor-made hanboks. Depending on the materials, a complete set of hanbok can cost from $172 to $350.\nWhile looking for a perfect fit hanbok, you can also try some of the food stalls in the market that offers traditional Korean cuisine.\nLocation: 88 Changgyeonggung-ro, Jongno 4(sa)-ga, Jongno-gu, Seoul\nCcomaque is a modern hanbok store along the Insadong cultural street in Seoul. It caters to both men and women and offers hanbok-inspired garments that you can proudly wear any time of the day. It has Sunday dresses for women as well as skirts and tops. For men, Ccomaque got jeogori-inspired overcoats and baji that resembles jogger pants. They go out of stock easily so you might want to follow them closely for new arrivals.\nLocation: 35 Insadong-gil, Gwanhun-dong, Jongno-gu, Seoul\nLocated in the Hapjeong-dong neighborhood, Soosulhwa is a popular modern hanbok store among young girls in Seoul. The store offers knee-length to longer modernized hanbok skirts with colorful embroidery. It also has a wide range collection of see-through hanbok inspired tops, as well as printed ones. Soosulhwa’s items are surprisingly cheaper given that most of them are made out of silk and other high-quality materials. Their pricing ranges from $45 to $150.\nLocation: 31-30 World cup-ro 3-gil, Hapjeong-dong, Mapo-gu, Seoul\nDailyHAN is reinventing the Korean tradition with its collection of casual hanboks and Korean accessories for women. Out of other modern hanbok stores in Seoul, DailyHAN has the closest collection of what a traditional hanbok looks like. With the touch of Sunday dresses, the store offers jeogori matched with knee-length skirts that feature prominent prints and embroideries. It also offers hanbok-inspired tops that you can pair with regular denim pants.\nLocation: 05624, Seongjeong Building, 474, Songpa-daero, Songpa-gu, Seoul\nWhere To Rent Hanbok In Seoul?\nPurchasing a hanbok is expensive and it’s not ideal to spend on something you would often use. If you are a tourist and want to experience wearing the traditional attire of Koreans, renting is a better option. Aside from it will save you from unnecessary expenses, renting will save you from the burden of extra baggage.\nThere are a pretty good number of boutiques where you can rent these traditional attires in Seoul or when you’re in Korea in general. You can find many of them near tourist attractions as they mostly target foreign visitors. Here are some places in Seoul where you can hire a set of Korean hanbok.\nSeohwa Hanbok Rental\nRenting a hanbok in Seohwa will not just allow you to wear Korean traditional attire, it will also allow you to experience royal life. While on a hanbok, you will have the chance to explore the Gyeongbokgung Palace – an important royal domain that dates back to the Joseon Dynasty.\nSeohwa Hanbok Rental has a wide selection of Hanbok designs and sizes. They also provide complimentary accessories like hair bands, ribbons, and handbags for the full look. You can book for a Seohwa Hanbok Rental on different booking sites like KKDay and Klook.\nLocation: 133-5 Sajik-ro, Sajik-dong, Jongno-gu, Seoul\nBukchon Oneday Hanbok\nAnother hanbok rental hotspot where you can find a perfect fit traditional Korean attire is the Oneday Hanbok rental in Seoul. This rental shop assists customers, from getting into the attire to completing their getaway look through accessories.\nFrom the rental shop, you can head off straight to the must-see places in the city. You can visit the nearby Bukchon Hanok Village, as well as the Gyeongbokgung and Changdeokgung palace.\nLocation: 2nd Floor, 4 Bukchonro5-gil, Jongno-gu, Seoul\nHanboknam is another rental shop in Seoul that can give you a wide range of options. The shop has different types of hanboks for different occasions like weddings, birthdays, new year, and festivals. From women’s hanboks to men’s, Hanboknam has a nice collection of Korean traditional attires. It also offers rentals for kids as well.\nHanboknam’s rental fee varies on what type of hanbok you like and how long you’re going to rent it. If you are a tourist, you can rent it for a couple of hours and explore the city while on the attire. Hanboknam has several locations near Gyeongbokgung, Changdeokgung, Jeonju, and Bukchon which famous for Hanok villages.\nLocation: 133-5 Sajik-ro, Sajik-dong, Jongno-gu, Seoul\nIf you want a day of hanbok experience, you can have it at YES Hanbok. This hanbok rental shop in Seoul has an impressive collection of traditional hanboks that resemble the attire of royals in the Joseon Dynasty. It offers the traditional baggy and loose clothing embroidered with prominent figures like dragons, floral patterns, and clouds.\nThe shop is conveniently located near Gyeongbok Palace, the main royal palace of the Joseon dynasty. While wearing a rented hanbok, you can explore the palace and other nearby historical sites for free.\nLocation: 133-6 Sajik-ro, Jongno-gu, Seoul\nPro-Tip: Although some of the hanbok rental shops in Seoul accept walk-ins, it is essential to book them beforehand. For your convenience, you can go to different online booking sites like Klook and search for the shop where you want to rent a hanbok. Aside from it’s hassle-free, you can also catch discount offers.\nPutting on or wearing a hanbok isn’t easy. But it’s worth it because this costume is comfortable to move in and will stand up to a range of temperatures. When you’re exploring Korea, make sure that you take the chance to experience this part of the local culture.']	['<urn:uuid:37955f9d-1bc7-4720-b9ef-ebfc3f755a2b>', '<urn:uuid:8b7446c5-6f44-4271-8163-4a4d9d0e11cd>']	factoid	with-premise	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T11:40:26.317848	35	72	3696
24	how many ww2 mosquito aircraft built vs us planes total	The Mosquito was built in 43 different marks with a total production of 7,781 aircraft, which was a small fraction compared to the United States' total aircraft production of about 294,000 for the war effort.	['de Havilland DH98 Mosquito Prototype\nThe de Havilland Aircraft Museum has received £41,000 from the Heritage Lottery Fund (HLF) to complete the restoration of the de Havilland Mosquito Prototype fighter-bomber.\nThe fast, high-flying Mosquito was for much of the War able to roam almost at will over enemy-occupied territory. Built of non-strategic materials (i.e. wood), it was designed for speed and range as a two-seat unarmed light bomber, unarmed reconnaissance aircraft and long range fighter. Its performance derived from a combination of;\n- careful packaging,\n- an aerodynamically clean shape,\n- a high wing loading,\n- and high power from two supercharged liquid-cooled V-12 Merlin engines.\nFor minimum frontal area, the engines are forward-mounted in closely tailored nacelles, and the intakes and ducts for the cabin heater, coolant radiators and oil coolers are built into the inboard wing leading edges. The fuselage is largely of balsa/plywood sandwich formed on moulds in left and right halves. The wing pair, of wood spars and stringers with plywood webs and skinning, is built as a single unit, mounted above the bomb bay. The Mosquito was built in 43 marks for a variety of roles, with a total production of 7,781 aircraft.\nPower Unit: Two 1,460 hp Rolls Royce Merlin 21\nWing Span: 54 ft 2 in (16.5 m)\nAll-up Weight (A.U.W): 19,670 lb (8,922 kg)\nMax Speed: 382 mph (615 kph)\nCeiling: 35,000 ft (10,670 m)\nRange: 2,180 miles (3,500 km)\nOn Display at the Museum:\nThis aircraft is the only surviving World War II prototype to be preserved in the World.\nDesigned and built at Salisbury Hall, the initial DH.98 design was presented to the Air Ministry in September 1938.Development was hampered by continued Air Ministry interference who tried to impose defensive armament-two rearward firing machine guns and provision for torpedoes to be carried for a maritime strike role. However de Havilland disregarded these potential changes believing that their design for a fighter and fighter-bomber roles were the best configuration for the Mosquito.\nThe company suffered repeated rejections from the Air Ministry, Air Marshall Freeman was not convinced that the type would outperform a Spitfire or German fighters that would be deployed in the forthcoming war.\nFinally ,in November 1938 an order to develop a Mosquito Prototype was granted but de Havilland were instructed that priority was to be given to the production of Tiger Moth Trainers, Rapide light Transports (called Dominies by the RAF) and essential variable speed Airscrews for several other aircraft manufacturers.\nThe Air Ministry persisted with their demands for defensive armament suggesting that the more powerful Griffon engine might permit the installation of a four gun turret without compromising top speed, but as neither Griffons nor turrets were available only mock ups were built.\nFinally on 12th December 1939 a prototype was ordered, but only for a photo –reconnaissance aircraft as the then Bomber Command chief, Sir Edgar Ludlow-Hewitt did not favour an unarmed bomber.\nThe prototype was to have a maximum speed of 397mph at 23,700 feet and cruise at 327mph at 26.600 feet. Range was to be 1,480 miles at 24,900 feet and a maximum ceiling of 32,100 feet.\nThe Dunkirk emergency held up detail design and construction as all Merlin Engines was need for fighters, and none could be spared for the prototype. Eventually Lord Beaverbrook intervened and made Merlin’s available for the prototype.\nThe de Havilland team at Salisbury worked through the Battle of Britain but E-0234, the bomber/reconnaissance prototype fitted with Merlin 21’s was moved by road to de Havilland’s factory on 3rd November 1940.\nIt was painted overall yellow and rolled out on November 19, 1940, just 10 months and 26 days after its inception.\nThe Mosquito prototype’s first flight was on 25th November 1940 piloted by Geoffrey de Havilland, Jr, accompanied by John Walker, designer of the engine installation.\nFollowing a demonstration to Lord Beaverbrook and other senior Government Ministers at Langley on 29th December an order for 150 aircraft was given to de Havilland.\nRenumbered W4050,the prototype reached an altitude of 22,000ft on 17th January 1941 and was delivered to Boscombe down on 19th February with camouflaged upper surfaces and yellow under surfaces for initial service trials.\nOn the 24th February while taxying on the rough surface of the airfield the tailwheel jammed and the fuselage fractured around the starboard access hatch. The damage was serious enough for the fuselage to be changed and the fuselage of the following prototype W4051 was used.\nDH98 Mosquito Prototype at The de Havilland Museum, London Colney, UK. #DH98 #Mosquito #prototype #ww2 #history #restoration #theta360 – Spherical Image – RICOH THETA', 'How did airplanes change warfare in the Pacific during World War II?\nTechnological advances in aviation during the war included improvements to long-range bombers, experimentation with jet-powered aircraft, and the use of radar to find enemy targets and detect enemy planes. Planes became faster, and fuel consumption also improved during the war.\nHow many planes were used in WW2?\nAddthis Share Tools. World War II witnessed tremendous growth in the size of American military aviation, from about 2,500 airplanes to nearly 300,000 by the war’s end.\nWhy were airplanes used in WW2?\nDuring WW2, the primary missions fulfilled by airplanes were air-to-air combat, bombing, aerial reconnaissance, and troop and supply transportation. Since many designs were based on pre-war civilian aircraft, many similar airplanes filled multiple roles.\nWhich country had the best planes in WW2?\nwhich country had the best fighter planes of WWII\n- Japan. 1.51%\n- Italy. 0.71%\n- France. 0.30%\n- Great Britain. 102. 10.29%\n- USA. 504. 50.86%\n- Russia. 2.02%\n- Germany. 333. 33.60%\n- China. 0.20%\nWhat was the most common plane in ww2?\nThe Spitfire is most likely the most famous aircraft from the World War II era. It was the king of low-altitudes, it will be known forever as the plane that turned the tide in the Battle of Britain.\nHow did the Allies win air superiority?\nThe Allies saw it, specifically long-range strategic bombing, as being a more important part of warfare which they believed capable of crippling Germany’s industrial centers. After the Battle of France, the Luftwaffe (Germany’s air force) achieved air supremacy over Western Europe.\nWhat ww2 planes are still flying?\nThe Collings’ two-seat TF-51D is only one the few Mustangs with flight controls for the passenger, and the Foundation’s B-24J is the only one of its kind still flying. The Boeing B-17 is one of only about a 15 still flying. The aircraft will be stationed at the Museum’s tarmac on Boeing Field.\nWho lost the most planes in ww2?\n- Soviet Union: Total losses were over 106,400 including 88,300 combat types.\n- United Kingdom: Total losses in Europe were 22,010, including 10,045 fighters and 11,965 bombers. …\n- United States: Total losses were nearly 45,000, including 22,951 operational losses (18,418 in Europe and 4,533 in the Pacific).\nHow long could ww2 planes fly?\n|Maximum Speed||Maximum Altitude||Maximum Range|\n|217 mph (349 kph)||25,000 ft (7,620 m)||683 mi (1,099 km)|\nHow many planes did us lose in ww2?\nThe United States had manufactured about 294,000 aircraft for the war effort. Of that number, 21,583 (7.34%) were lost in the United States in test flights, ferrying, training accidents, etc., and 43,581 were lost en route to the war and in overseas operations.\nWhat was the biggest air battle in history?\nThe largest single-day air battle of World War II was also an unmitigated disaster for the Allied forces.\nWhen did Germany lose air superiority?\nA study of the Luftwaffe’s efforts to cope with the loss of daylight air superiority in 1944-45 is of more than historical interest.\nCould ww2 planes cross the Atlantic?\nAlthough many air route surveys of the North Atlantic had been made in the 1930s, by the outbreak of World War II in Europe, civilian trans-Atlantic air service was just becoming a reality. It was soon suspended in favor of military activities.\nHow many planes from ww2 are still alive?\nOf the over 15,000 US P-51 Mustangs built, less than 200 are still flyable – about one percent of the production run. Of 12,571 F4U Corsairs built, roughly 50 are airworthy. Of 3,970 B-29 Superfortresses built, only two are flying today. Much of this is due to the ravages of time or accidents.\nCan you own a ww2 plane?\nYes it is possible to buy old military aircraft; they are frequently listed in the pages of aircraft sales periodicals like Controller, Trade-A-Plane, Barnstormers, etc.\nCan you buy an F-22?\nIn 1998, Congress even voted for an amendment that explicitly forbids the sale of F-22 Raptors to foreign nations. Unlike the vast majority of U.S. military aircraft, the F-22 was never designed to be exported.\nCan you buy a F-14?\nHighly unlikely. There were only two operators of the F-14: the US and Iran. When the US decommissioned theirs in 2006, aside from a few stripped airframes in museums, they were all destroyed to prevent their parts making their way to Iran.\nHow much does it cost to own a P-51?\nNow You Can (for $4 Million) The P-51 earned a solid reputation during the war as an air-superiority fighter. It was fast, maneuverable and most importantly, long-ranged enough to escort B-17 and B-24 bombers deep into Nazi-occupied Europe in the latter years of the Second World War.\nWhat kind of plane did Jimmy Stewart fly in World War II?\nB-24 Liberator bomber\nAlways from the copilot seat of a B-24 Liberator bomber, Stewart would command anywhere between 25 to 150 aircraft, depending on if he was lead, wing, or squadron commander. But even on the days he didn’t fly with the men he trained, he would brief his boys about the day’s missions.\nHow much does it cost to fly a p51 Mustang?\nHow much is it? Packages start at $2950 and include museum membership and benefits. Because our pilots and planes have busy performance schedules, advance reservations are mandatory. Longer flights are available for an additional price.\nHow many p51s are still flying?\nThere are only around 175 Mustangs still flying, with roughly 150 of those in the U.S. About 100 other Mustangs are on display in museums. More than 15,000 rolled off North American Aviation assembly lines in California and Texas during World War II.\nHow much does a stallion 51 flight cost?\nA little over $3100 for a 1 hour flight.\nHow hard is it to fly a P-51 Mustang?\nThe P-51D, which I flew, was a very straightforward airplane in every way. By that I mean it wasn’t difficult to fly or hard to handle, as long as you remembered a few basic things. First and foremost, you never forgot for a minute that it could bite hard if you got careless.\nWhy was the p51 Mustang so good?\nThe long-range P-51 Mustang fighter was invaluable to the Allied victory, enabling resumption of strategic bombing after heavy losses suffered by unescorted bombers in 1943. Developed for export to Britain, models modified by the British to use Rolls-Royce Merlin engines became America’s most capable wartime fighters.\nHow much runway does a p51 need?\nWith full fuel of 41 U.S. gallons (160 L; 34 imp gal) the payload for pilot and baggage is 354 lb (161 kg). The aircraft has fairly lengthy runway requirements with a standard day sea level take-off distance of 1,200 ft (366 m) and a landing distance of 1,500 ft (457 m).\nWhat was the fastest ww2 plane?\nMesserschmitt Me 163\nThe fastest aircraft to see any combat in WWII was the Messerschmitt Me 163, which set an airspeed record of 702 mph in 1944 (although this record was not recognized by the Fédération Aéronautique Internationale due to its secrecy during the war).\nCan you fly a p51 Mustang?\nP-51 Ride Flight Information\nFlight Experience is 30-minutes or 50-minutes (start up to shut down). Riders Must be at Least 12 years of Age to Ride. Riders Between the Age of 12 -17 years Must Have a Legal Guardian Present at Time of Check-in.\nHow much ammo did a P-51 carry?\nThe P-51D was armed with six 50 cal. Browning machine guns with a total of 1880, 400 rounds each inboard and 270 rounds each outboard.\nWhat was the fastest propeller plane in ww2?\nThe German Dornier DO-335 was unique in having a tractive (pulling) propeller in its nose and a propulsive (pushing) motor behind its cockpit, technology that at the time was totally new. It had a maximum sustained speed of 665 km/h (413 mph), increasing to 765 km/h (477 mph) with emergency boost.']	['<urn:uuid:71fa8d6f-4712-41c1-8429-e826d9c8ba5b>', '<urn:uuid:fc31caec-db1d-4cce-987e-b2d050e724dd>']	factoid	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-12T11:40:26.317848	10	35	2098
25	Is Mississippi EMT school longer than EMT-Basic national standard?	Mississippi EMT school takes two to six months to complete, which is longer than the national EMT-Basic standard duration of 3-11 weeks.	['If you are interested in the steps you will need to take to become an EMT in Mississippi, we have you covered with our latest resource below! All emergency medical technicians are regulated by the Mississippi State Department of Health and the steps laid out for you below will get you on track for certification!\nWhat does a Mississippi EMT do?\nAlthough the broad function of EMTs and Paramedics is to attend to the sick and seriously injured, there is much more to their duties. Here you’ll see some more-comprehensive descriptions of the EMT’s role.\nWhat is the process to become an EMT in Mississippi?\nEMTs and Paramedics in Mississippi will need to complete a post-secondary training course. Mississippi mandates that all EMTs and Paramedics in the state be professionally certified. Although every state could have differing requirements for EMT certification, the five steps below are very common.\nStep 1: Earn your high school diploma or G.E.D.\nWhen looking for an Emergency Medical Technician certification class, you can expect to also find that all programs require students to hold a G.E.D. or diploma before registration. So, regardless of your professional goals, make sure that you complete this step.\nStep 2: Graduate from an accredited EMT school in MS\nIf you wish to be eligible to test for certification, you must first complete a MS Dept. of Health accredited EMT program. Students must have finished the training in the previous 24 months and provide verification of successful program completion on the National Registry website (more info below). Junior colleges, trade schools, hospitals and universities all feature Emergency Medical Technician training programs. You can also take them at fire and police training academies.\nMississippi Department of Health Approved EMT Programs in MS\nBelow are all the EMT programs in Mississippi that are approved by the Mississippi Department of Health.\nMeridian Community College 910 Hwy 19 N Meridian MS 39307-5801 601-483-8241\nCoahoma Community College 3240 Friars Point Rd Clarksdale MS 38614 662-627-2571\nNorthwest Mississippi Community College 4975 Hwy 51 N Senatobia MS 38668-1714 662-562-3200\nEast Mississippi Community College 1512 Kemper Street Scooba MS 39358-0158 662-476-5000\nEast Central Community College 15738 Highway 15 Decatur MS 39327-0129 601-635-2111\nItawamba Community College 602 W Hill Street Fulton MS 38843-1099 601-862-8000\nJones County Junior College 900 South Court Street Ellisville MS 39437 601-477-4000\nHolmes Community College #1 Hill St Goodman MS 39079 662-472-2312\nHinds Community College 608 Hinds Boulevard Raymond MS 39154 601-857-5261\nCopiah-Lincoln Community College 1001 Co-Lin Lane Wesson MS 39191 601-643-5101\nMississippi Gulf Coast Community College 51 Main Street Perkinston MS 39573 866-735-1122\nMississippi Delta Community College PO Box 668 Moorhead MS 38761 662-246-6503\nJackson County EMS District 2204 Old Mobile Highway Pascagoula MS 39567 228-627-8727\nGulf Coast EMS District 2020 Intraplex Parkway Hattiesburg MS 39401 601-554-4947\nSouthwest Community College College Drive Summit MS 39666 601-276-3876\nSoutheast Trauma Care Region 207 South 28th Avenue Hattiesburg MS 39401 601-264-0342\nHow long is EMT School in MS?\nDepending on the particular training program in Mississippi, EMT training may often be finished in between two and six months.\nStep 3: Have a current CPR-BLS certificate for “Healthcare Provider” or equivalent\nStudents should have a CPR-BLS credential for “Healthcare Providers” for Emergency Medical Technician candidacy. The American Heart Association sets the standards for all Basic Life Support (BLS) and CPR programs. On the list of largest, and most-prominent providers of certification training is the American Red Cross.\nStep 4: Challenge the National Registry’s EMT Cognitive and Psychomotor exams\nThe Emergency Medical Technicians certification examination is given in two sections: the National Registry Cognitive (knowledge) and Psychomotor (skills) examinations. The Cognitive Exam is administered in an online, multiple-choice style, while the Psychomotor Exam is offered at a physical venue under the observation of an evaluator. We discuss the two test sections in depth below.\nStep 5: Have your name included on the National EMT Registry\nThe last step in becoming an Emergency Medical Technician is getting your name placed on the National EMT Registry within 30 days of finishing your coursework. A fee of $80 has to be paid so that you can get your name included on the registry. After becoming listed, you’ll be officially acknowledged as an Emergency Medical Technician.\nWhat is the median EMT salary in Mississippi?\nThe median pay for EMTs in Mississippi, along with extra regional statistics for Mississippi, is detailed in the following table. All data is from the website of bls.gov.\n|Gulfport-Biloxi, MS MSA||Hourly||$13.05||$20.87||$28.93|\n|Memphis, TN-MS-AR MSA||Hourly||$12.98||$18.84||$29.98|\n|Northeast Mississippi BOS||Hourly||$10.96||$17.77||$24.43|\n|Southwest Mississippi BOS||Hourly||$9.76||$16.20||$25.72|\n|Northwest Mississippi BOS||Hourly||$8.58||$15.04||$35.77|\n|Jackson, MS MSA||Hourly||$10.00||$14.88||$26.02|\n|Southeast Mississippi BOS||Hourly||$8.85||$14.50||$27.74|\nWhat is the job outlook for Emergency Medical Technicians and Paramedics in Mississippi?\nWith the population increasing and the Baby Boomers hitting retirement age in such large numbers, the healthcare industry in general is experiencing record growth in Mississippi. The Bureau of Labor Statistics projects a 18% rise in EMT and Paramedic jobs also over the subsequent decade.', 'EMTs are the first responders to provide medical care to the sick and injured during an emergency crisis. If you wish to have a career as an Emergency Medical Technician (EMT), you should take into account the commitment, dedication and time factor needed to be an EMT. It can be a full-time job that can take up to several months of rigorous and intense training. So, when determining how long does it take to become an EMT, you must take into account the EMT course duration, the length of different EMT programs, and the time taken for obtaining an EMT license.\nHow Long does it Take to Get EMT Certified?\nThere is no universal time duration to get certified as an EMT. An EMT course length varies with country, state, community colleges, and schools. Some states have their own regional Emergency Medical Services (EMS) agencies which differ from one to another. In the United States, obtaining an EMT Certification takes a minimum of three weeks to a maximum of two to four years depending upon your professional goals, national standards, and state requirements.\nIn order to become an EMT certified, the candidate must meet specific education and other relevant criteria. Further, the candidate must complete the necessary coursework and different level of the training program within the defined time. The time that each level takes depends on several factors like student dedication, class schedule and time taken for completing the licensing examinations. On successful completion of the training program, the candidate can appear for the certification tests and apply for the certification documents through an online portal to become an EMT certified.\nEMT Course Duration\nThe training and classes duration also depends upon the colleges, schools, and universities offering EMT courses. Some institutes may provide a certificate course of 3-6 months while some may offer a two or four-year associate or bachelor’s degree program. The number of hours is divided among class, lab, and field training that must be necessarily obtained. It depends upon the school or institute on how they structure the course to achieve the defined hours.\nMoreover, it also depends upon the course level you select. Generally, there are three levels of EMT certification course: EMT-Basic, EMT-Intermediate and EMT Paramedic. Each program requires its own set of training, lectures, and duration. For example, an EMT-Basic course requires 2 to 3 weeks to complete, having a class of minimum 8 to 10 hours a day, while an EMT Paramedic course may take two years to earn an associate degree.\nThe EMT-Basic has the shortest course duration as compared to the Intermediate and Paramedic. You need to cover all the theory and practical learning part in such a short time. On the other hand, a Paramedic course can take thousands of hours for completion. You will be attending two years of classes and clinical training to finally become an EMT Paramedic certified.\nDuration of Different EMT Programs\nEach EMT program level has its own pace. Generally, the basic level takes a shorter period to complete as compared to the advanced level of EMT. In many instances, the students are also required to complete the basic level to qualify for the advanced level of the EMT program.\nThe course duration for an EMT-Basic generally takes up to 3-11 weeks consisting of 120-150 credit hours. The basic level EMT program is not as challenging as the Intermediate and Paramedic programs. The program includes coursework and clinical training essential to become a professional EMT.\nEMT Intermediate program requires the student to attend another 300-350 credit hours of coursework and training. An EMT Intermediate takes on more responsibilities in comparison to the Basic level training. The course includes advanced learning in performing medical procedures on patients like conducting IV treatment, endotracheal intubations, etc.\n3. EMT Paramedic\nEMT Paramedic is the highest level of EMT certification program. It generally takes two years of excessive training, both classroom and clinical training at hospitals and fire departments. The coursework includes a variety of subjects like anatomy, physiology, cardiology, pharmacology, and other advanced medical treatments. The program requires 1,200 to 1,800 hours of training which leads to an associate or bachelor’s degree upon completion.\nGetting EMT License\nAfter completing different levels of EMT training course, the candidate may apply for the state’s licensing examination or the National Registry examination. It must be noted that the candidate must be an EMT certified before applying for an EMT licensure. Many states prefer the EMT certification to be obtained through the National Registry of Emergency Medical Technicians (NREMT) to qualify for the license. Further, the examination conducted by the NREMT requires the candidate to pass both cognitive and psychomotor exam.\nThe Cognitive exam is a computerized adaptive test which covers entire coursework taught in class during the training program. The candidate is given six opportunities to pass the exam. If not successful in the first attempt, the candidate may reapply within 15 days for the same. On the passing of the cognitive exam, the candidate may appear for the psychomotor exam. The exam tests the practical leaning of the candidate regarding medical procedures before getting an EMT certification.\nThe licensing procedure also varies with the state. Many states conduct background checks and may not give license to a candidate with a criminal record. Moreover, every two or three years EMTs are required to renew their licensure through continuing education.\nYou need to understand the amount of time and dedication required before applying for an EMT program. Ask yourself how long will it take for you to become an EMT professional and whether you can devote the amount of time required to complete the courses. Based on your career goals and priorities, you can opt for shorter or longer duration courses available.\nMore reading :- If you are looking for other career alternatives like nursing, medical assistant in healthcare sector. These fields offer diverse working environment and career advance opportunities. Know how long does it take to get your CMA?']	['<urn:uuid:d87868d1-8b5d-450b-80d0-f448635b3294>', '<urn:uuid:8cda8b19-0cb0-4bde-9022-d8017b092c46>']	factoid	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T11:40:26.317848	9	22	1816
26	Is almond milk just as healthy as regular milk?	No, almond milk and other plant-based beverages like rice and soy are not equivalent to cow's milk. While these drinks are often fortified with calcium, they lack many vitamins and minerals found naturally in cow's milk. Cow's milk naturally contains vitamin D, vitamin A, and potassium, and provides eight grams of high-quality protein per cup. In contrast, alternative beverages like almond, coconut, and rice milk contain only a small amount of protein.	['By Carol Witte, RDN, Nutrition Program Director.\nDairy foods are packed with important nutrients you need including calcium, vitamin D, potassium and protein. Three servings of low fat or fat free dairy items are recommended every day to improve bone health, lower blood pressure and reduce the risk of cardiovascular disease and type 2 diabetes. For adults over 60, 1200mg of calcium is recommended each day. The meals planned for Meals on Wheels, and our Dining & Activity Centers, provide an average minimum of one third of calcium needs or 400mg a meal. Read ingredient lists and nutrition labels to compare products that you enjoy. Here are some myths and misconceptions regarding dairy foods. Our goal is to empower you with nutrition information to make healthy food and beverage choices.\nMYTH: Many different foods contain calcium so there is no need to drink milk.\nFACT: Many foods do contain some calcium, although the best source is milk and milk products. Each glass or carton of milk provides almost 300mg of calcium. To obtain the same amount of calcium in 1 cup of milk you would need to eat 3½ cups broccoli, 1 cup almonds, 10 cups of spinach or 3 ounces of sardines with bones. The calcium in milk is easily absorbed and used by the body. Many greens contain oxalates which bind the calcium making it more difficult for the body to use.\nMYTH: Rice or almond milk is a good substitute for cow’s milk.\nFACT: Rice, almond and soy beverage actually come from plants. These drinks are often fortified with key nutrients like calcium, but lack many of the vitamins and minerals found naturally in cow’s milk. Milk also includes vitamin D, vitamin A and potassium and is a high quality source of protein, providing eight grams per cup. Alternate beverages, such as almond, coconut and rice, have only a small amount of protein.\nMYTH: All milk contains antibiotics, except organic milk.\nFACT: Milk produced on conventional dairy farms is strictly tested both on the farm and at the processing plant. Any milk that tests positive for antibiotics at any point is disposed of immediately and does not get into the food supply. Sometimes dairy cows require antibiotics to treat an illness. Milk is not used from a treated cow until all antibiotics are cleared from the cow’s system.\nMYTH: People who are sensitive to lactose should avoid all dairy foods.\nFACT: Lactose is the natural sugar in milk. If the body cannot easily break down and digest the sugar in milk to its simpler sugar form, then individuals may notice symptoms of gas, belly pain and bloating. They now have lactaid milk at the grocery store or individuals can take over-the-counter lactase enzyme tablets with dairy to help digest dairy’s lactose. For those who are lactose intolerant you can also include other dairy products without distress like yogurt with live and active cultures, natural hard cheeses such as cheddar, Colby, Monterey Jack and Swiss which contain little or no lactose.\nMYTH: People who are sensitive to lactose are allergic to milk.\nFACT: Lactose intolerance is not the same thing as a cow milk allergy. A cow milk allergy occurs because of a reaction by the body’s immune system to one or more milk proteins (casein or whey). Milk allergies can be life threatening and are usually diagnosed in the first year of life.']	['<urn:uuid:8eb74c9f-a8c2-40ff-a6c2-28a27fe5045e>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T11:40:26.317848	9	72	566
27	How much water can the solar-powered filtration system purify daily?	The solar-powered water filtration system can purify up to 1,500 gallons of water per day.	"['Twelve University of Arizona graduate students have teamed up with undergraduates on the Navajo Nation to design a solar-powered water filtration system that can provide 50 gallons of safe, clean water to 30 Navajo families per day.\nThe students are part of the first cohort of trainees to participate in a five-year, $3 million National Science Foundation grant that enables an unprecedented collaboration between the UA and Diné College, which serves a predominantly Navajo student population. The goal is to teach the next generation of STEM professionals how to confront food, energy and water challenges among indigenous communities while letting traditional values guide their work.\nThe project – called Indigenous Food, Energy and Water Security and Sovereignty, or Indige-FEWSS – combines research internships, teaching and cultural immersion.\n""We want to enable these trainees to tackle critical, real-world food, energy and water problems with an understanding of culture and sovereignty of indigenous people,"" said Karletta Chief, associate professor of environmental science in the College of Agriculture and Life Sciences, who is leading the project.\n""Through Indige-FEWSS, I\'m able to fulfill a promise I made to my family as a young girl, saying that I\'ll never forget where I come from and I\'ll never think that I am better than anybody, but instead be grounded and remember my place in the universe,"" said Nikki Tulley, a UA doctoral student in environmental science. ""My place in the universe will always be bringing resources and services to the Navajo Nation.""\nWith an area the size of West Virginia, the Navajo Nation is the largest reservation in the United States, but some areas have never been connected to central power and water due to rugged terrain and low population density. Today, the Navajo Nation estimates that of its 357,000 residents, up to 35% don\'t have running water in the home.\nA Navajo Nation Environmental Protection Agency policy prohibits human consumption of water from unregulated sources. Due to the lack of public water systems, Navajos living in rural areas regularly drive long distances to haul water for drinking, cooking and bathing. Often the water comes from unregulated and untested sources, such as livestock wells and springs that can exceed drinking water standards for contaminants like arsenic and uranium, an impact of earlier mining activities.\nAccording to the U.S. Environmental Protection Agency, the use of unregulated water sources is the greatest public health risk associated with drinking water for the Navajo Nation. A study published in 2018 by the Navajo Department of Health reported that compared with the non-Hispanic white population in Arizona, Navajos have a higher incidence and mortality of liver, kidney, stomach and gallbladder cancers.\nMeanwhile, the Indian Health Service reported that it would cost $200 million to provide access to safe drinking water in all Navajo homes. Without access to such funds, the Indige-FEWSS graduate students – half of whom are Native American – and undergraduate students from Diné College have set their sights on a mobile, solar-powered water purification system instead.\nLetting traditional Diné culture and ecological knowledge guide the process, the students designed a solar-powered water filtration system that can purify up to 1,500 gallons of water per day, removing nearly 100% of dissolved contaminants such as arsenic and uranium – well below the concentrations to meet drinking water standards – without the need to be connected to centralized water or power.\nTheir $25,000 solar-powered water filtration system largely comprises materials available at a hardware store, including, for example, two 275-gallon caged tote tanks, a few water filters and some PVC pipes. The system\'s design reflects lifestyles and culture on the reservation: It is mobile for hauling and can be built with readily available products, explained Christopher Yazzie, an environmental engineering graduate student in the Indige-FEWSS program.\n""Tó éí ííńá át\'é,"" said Benita Litson, director of Diné College\'s Land Grant Office. It\'s Diné for ""Water is life,"" a common refrain on the reservation. ""And purification makes it safe and clean.""\nUsing the system, communities on the reservation can continue to haul water from wells but can ensure its safety and cleanliness.\n""Trainees in the Indige-FEWSS program are not doing research to be put on the shelf, but we\'re doing research to have real impact and solve real problems that our community has,"" said Chief, who grew up on the Navajo Nation.\nThe Indige-FEWSS program is only a first step towards addressing food, energy and water insecurity among indigenous communities.\n""Our vision at the University of Arizona is to develop a diverse workforce with intercultural awareness and expertise in sustainable food, energy and water systems,"" she said.\nOther UA faculty involved in the program include Robert Arnold, professor emeritus of chemical and environmental engineering; Benedict Colombi, professor of American Indian studies; Murat Kacira, professor of biosystems engineering; Kimberly Ogden, professor of chemical and environmental engineering; Erin Ratcliff, assistant professor of materials science and engineering; Kelly Simmons-Potter, professor of electrical and computer engineering; and Valerie Shirley, assistant professor of teaching, learning and sociocultural studies.\nUnder their advisement, each UA student will receive a graduate degree in a science, technology, math or engineering field such as electrical engineering or materials science while also receiving a minor in Indige-FEWSS.\nFollowing graduation, both Tulley and Yazzie intend to create businesses that address their community\'s need for clean, safe water. Yazzie is considering a business focused on miniature versions of the filtration system, built more cheaply, for single-family use.\n""This work represents core aspects of the University of Arizona’s mission—working together, expanding potential, tackling big challenges and enriching lives,"" said UA President Robert C. Robbins. ""This partnership between the UA and Diné College gives students a meaningful, hands-on opportunity to make a real difference for people living on the Navajo Nation by collaborating on innovative solutions to meet the need for clean water. At the same time, these future scientists and engineers are gaining the skills they need to take on related problems involving food and energy. I look forward to seeing the impact of the important work being undertaken by Dr. Karletta Chief and her team.""\nThe Indige-FEWSS program is supported by grant award #DGE1735713 from the National Science Foundation.']"	['<urn:uuid:6f26af37-0e3d-4152-af7b-9421b988079a>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T11:40:26.317848	10	15	1023
28	main component black mustard india	In the black mustard seeds from India, the main compound was cis-Oleic acid.	"[""Evaluating Nepal's Roasted Mustard Oil - Chromatography Explores\nNov 01 2020\nKhokana is a small village with a population of under 5000 that is located a few miles south of Kathmandu in central Nepal. It is a simple Newari village, people who are the historical inhabitants of Kathmandu valley and its surrounding areas. They have a culture and lifestyle that is different from other areas of the Himalayan foothills. But what Khokana is really known for is its mustard oil. It was even nominated for the world heritage list by UNESCO for the ‘living heritage’ associated with its roasted mustard oil.\nRoasted mustard oil is a vegetable oil that has many professed health benefits. But there is little research into the composition and properties of different mustard oils from Khokana. In a paper - Chemical Profiling of Roasted Mustard Oils of Khokana, Nepal - published in the Asian Journal of Applied Chemistry Research, researchers from Tribhuvan University, Nepal investigated the detailed chemical composition of roasted mustard oils from Khokana.\nLiving museum to Newar identity\nRoasted mustard oil many medicinal uses in Nepal. New-born babies, cholesterol and blood-pressure are all treated with roasted mustard oil. It is used from birth to death to purify and treat ailments, it is also used in religious rituals. In Khokana, the oil mill is part of a living museum showcasing Newar civilization – it is an important part of Newar culture, tradition, ethnic identity, and uniqueness. Hence the UNESCO nomination in 1996.\nThere are around 40 different varieties of mustard plants, but the most common ones used in oils are black mustard (Brassica nigra L.), Oriental or brown (B. juncea L.), and white or yellow mustard (B. hirta). The oil is obtained from mustard seeds which are first powered using a heavy wooden beam, the powered seeds are then roasted before being crushed with the oil collected. The oil is naturally dark coloured because of carotenoids and chlorophyll. Lipase enzymes affect the taste and aroma of the oils by breaking the ester bonds present in the oils. The oil is usually consumed without any further processing after the oil extraction. Mustard oil pungency is due to allyl isothiocyanate which is unique to mustard oil compared with other vegetable oils.\nDetermine the composition the chromatography way\nThe researchers behind the paper referenced above analysed mustard oil produced from local mills in Khokana. The oils analysed were produced from brown, black and yellow mustard seeds. The oils were analysed using gas chromatography-mass spectrometry. The use of GC-MS to analyse food samples is discussed in the article, Analysis and Identification of Mezcal and Tequila Aromas by Ambient Ionisation MS, GC-MS, and GCxGC-MS.\nThe researchers found that in one sample, the black mustard seeds from India, the main compound was cis-Oleic acid. In the other two samples the main compound was Erucic acid. The analysis the team carried out provided the fatty acid profile of mustard seed oils and the variation in fatty acids.\nIn This Edition Modern & Practocal Applications - Accelerating ADC Development with Mass Spectrometry - Implementing High-Resolution Ion Mobility into Peptide Mapping Workflows Chromatogr...\nView all digital editions\nFeb 26 2023 Darmstadt, Germany\nMar 01 2023 Shanghai, China\nMar 09 2023 Guangzhou, China\nMar 18 2023 Philadelphia, PA, USA\nMar 26 2023 Indianapolis, IN, USA""]"	['<urn:uuid:4eea65c0-845d-40af-a1cc-5dd4b25207ba>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T11:40:26.317848	5	13	550
29	what does meek person mean characteristics of meek people in bible	A meek person is characterized by being kind, calm, peaceful, and gentle-voiced. They do not argue or quarrel, do not break up with anyone, and do not behave rudely. According to both the New and Old Testaments, Christ exemplified meekness as He would not quarrel, cry out, or let his voice be heard in the streets.	"['What is humbleness, and what is its significance? What did the fathers say commending it? What does the Holy Scripture say about it? What is its place among and its relationship with virtues? What is its relationship with high gifts, with grace, and with trials? How can a person be humble?\nAll this and more we would tackle – God willing – in a series of articles on this important topic, so that you may know what this great virtue is, and what other virtues it implies.\nHe has authority: What authority was that? It was the authority of the Legislator.\n• The words ""You have heard that it was said … But I say to you"" repeatedly came on the Lord\'s mouth, when speaking about murder, about adultery, about divorce, about oaths, about any eye for an eye and a tooth for a tooth, and about the relationship with the neighbor and the enemy (Mt 5: 21, 27, 28, 31, 32- 34, 38, 39, 43, 44)\n• In His words about the Sabbath, He said, ""For the Son of Man is Lord even of the Sabbath."" (Mt 5: 12: 8) As Lord of the Sabbath, He put its rules as He willed.\n• When He said to the paralytic, “Son, be of good cheer; your sins are forgiven you” the scribes grumbled within themselves, so He said to them, ""… that you may know that the Son of Man has power on earth to forgive sins … \'Arise, take up your bed, and go to your house.\'"" (Mt 9: 2- 6)\nThese words are repeated thrice in one paragraph of the Sermon on the Mount in (Mt 6: 25- 34).\nThe Lord says, ""Do not worry about your life, what you will eat or what you will drink; nor about your body, what you will put on …"" ""Do not worry about tomorrow …"" ""Do not worry about tomorrow … Sufficient for the day is its own trouble."" (Mt 6: 25- 34)\nIn case each master has a different tendency, one cannot serve both equally, or with the same degree of honesty. One\'s service will be true from all the heart to the one, and in flattery or hypocrisy to the other.\nIn case both have the same tendency, anybody can serve them equally. A person can serve God, the church, the community, the state, and knowledge, but cannot serve two opposite or competing masters, whether the master is a person or a thing.\nA meek person is kind, calm, peaceful, and gentle-voiced, neither argues nor quarrels, does not break up with anyone, nor behaves rudely.\nIn both the New and Old Testaments, it is said of Christ the Lord “ He will not quarrel nor cry out, nor will anyone hear his voice in the streets. A bruised reed he will not break, and smoking flax he will not quench(Matt 12: 19-20) (Isia 42: 2-3). He does not deprive anyone of hope. He does not quench smoking flax; perhaps a wind blows two years later and it may be kindled!\nThe wise never builds his house on the sand, on moving and unstable land, as the foolish does (Mt 7: 5), but rather builds it on the rock.\nBuilding on the rock:\nIt signifies building on deep faith in God, on sound understanding of His commandments, or on strong firm foundation of love for God, for people, and for good.\nSuch strong and firm foundation never falls.\nAs engineers do, build houses on deep foundation of reinforced concrete, which no winds or rains can shake or move from its place. This is the difference between building on the rock and building on the sand. Some people go to church and listen to God\'s word, just for knowledge, while others listen, with the intent to act accordingly. While the former turn the mind into a store of knowledge or a moving encyclopedia, having answers to any question, the latter listen, act, and turn the word into life.\nThe Lord says, ""Whoever hears these sayings of Mine, and does them, I will liken him to a wise man who built his house on the rock: and the rain descended, the floods came, and the winds blew and beat on that house; and it did not fall, for it was founded on the rock. But everyone who hears these sayings of Mine, and does not do them, will be like a foolish man who built his house on the sand: and the rain descended, the floods came, and the winds blew and beat on that house; and it fell. And great was its fall."" (Mt 7: 24- 27)\nHe promised His disciples to be with them always, even to the end of the age, and He fulfilled His promise. He presented Himself alive for forty days after His resurrection, speaking of the things pertaining to the kingdom of God (Acts 1: 3). He gave them peace and joy as He had promised (Jn 16: 22; 20: 20) and comforted them after the crucifixion they witnessed.\nThe period between the crucifixion and the resurrection:\nIn His resurrection, He sanctified the mortal human nature, giving it power to rise.\nBefore rising, with His body lying in the tomb, He was working for us.\nDeath caused His spirit to separate from His body, but His divinity never separated from His spirit or from His body. His spirit, united with His divinity, accomplished wonderful salvation for those lying in hope.\nThe journey back & Jacob\'s fear of his brother Esau\nAfter settling his family issues, whether the wrestling of his two wives, or the pursue of his uncle Laban, Jacob set on his way back to his father\'s house.\nTerror from Esau:\nHe was afraid, or rather terrified and trembling, in spite of all God\'s promises and support.\nProbably the words of Esau were still ringing in his ears, ""I will kill my brother Jacob"" (Gen 27: 41), or he remembered how he seized the opportunity of his brother\'s hunger and took his birthright, and made him swear (Gen 25: 31, 33). Maybe he recalled the deceit with which he took the birthright when he said to his father, ""I am Esau your firstborn"", and how his father said to Esau, ""Your brother came with deceit and has taken away your blessing."" (Gen 27: 19, 35)\nLessons from the Lord\'s Resurrection\nSo many miracles happened at the time of the Lord\'s crucifixion: there was darkness over all the land, the veil of the temple was torn in two, the earth quaked, and the rocks were split, and the graves were opened; but did everybody benefit from these lessons?\nJacob the Patriarch (8)\nBack to his home country\n& wrestling with his uncle Laban\nOur father Jacob submitted to the actual state, and accepted Leah as wife, then he married her sister Rachel, gathering between both sisters. He lived with the wife he loved, and the wife who loved him and sought his love. Both wives wrestled together.\nOur father Jacob avoided taking wives from unbelievers lest they turn his heart away from God as happened to Solomon the Wise afterwards (1 Kgs 11). He went to take wife from a holy family of his parents\' relatives, not knowing that problems might follow him even with those holy people, from his uncle Laban who deceived him, and from his two wrestling cousins, Leah the weak sighted, and the pretty Rachel.\nJacob met his cousin Rachel by the well where the shepherds watered their sheep. As there was a large stone on the well\'s mouth, they used to wait until they all gather and roll the stone away. When Jacob saw Rachel the daughter of his uncle, he went near and rolled the stone from the well\'s mouth, and watered the flock of Laban his uncle (Gen 29: 3, 10).\nThe covenant with God in Bethel\nSo many are the covenants made between God and man, among which was that with Jacob, where God said to him, ""Your descendants shall be as the dust of the earth … and in you and in your seed all the families of the earth shall be blessed. Behold, I am with you and will keep you wherever you go, and will bring you back to this land."" (Gen 28: 13- 15)\nJacob fled from the face of his brother Esau who had intended to kill him. Strange indeed was such hatred and strange his ignorance! Could he prevent the blessing that went to Jacob, that peoples serve Jacob, and nations bow down to him, that he become master over his brethren, and his mother’s sons bow down to him, that the older serve him though the younger? (Gen 27: 29; 25: 23) Esau was defying the divine dispensation, unlike his father who despite intending to bless Esau, submitted to the divine will when he remembered God\'s promise. Isaac said affirming, ""Indeed he shall be blessed."" (Gen 27: 33) Esau nevertheless disobeyed and showed ignorance, for the blessing implied the coming of Christ from the offspring of the firstborn, how then would he kill Jacob before the coming of Christ from his offspring! How would he kill him before the fulfillment of the other blessing Isaac had given him, that he multiply and be an assembly of peoples (Gen 28: 3)? It was impossible, but Jacob in fear fled from his face.\nJacob succeeded in obtaining the blessing of his father, a great treasure sought by the sons at that time.\nBlessings throughout human history came directly from God, from Him alone.\nHe blessed Adam and Eve (Gen 1: 28), Noah and his children (Gen 9: 1), and our father Abraham (Gen 12), the first to whom God said, ""You shall be a blessing"" (Gen 12: 2).\nBirthright was a great desirable thing in the days of our early fathers.\nThe firstborn before Aaron served as priest of the family after the decease of his father, as the Lord commanded Moses, ""Consecrate to Me all the firstborn, whatever opens the womb … it is Mine."" (Ex 13: 2) Christ was expected to come from the firstborn, according to the Lord\'s promise to our fathers Abraham and Isaac, ""In your seed all the nations of the earth shall be blessed."" (Gen 22: 18; 26: 4)\nGod chose Jacob before his birth, and gave him blessing and authority while still in his mother\'s womb, for God said to her, ""Two nations are in your womb, two peoples shall be separated from your body; one people shall be stronger than the other, and the older shall serve the younger."" (Gen 25: 23) The older is Esau, and the younger Jacob.\nWe mention in this context our first parents and grandparents Adam and Eve, the first human beings, the first sinners, the first to undergo punishment, and the first married couple forming the first family.\nThe type of the relationship between our mother Eve and our father Adam:\nShe is of the same nature, or rather ""bone of his bones"" and ""flesh of his flesh"" (Gen 2: 23) She is a helper comparable to him (Gen 2: 18) in nature, in mind, and in spirit. She is in God\'s image and likeness, responsible for her actions. He is the head, and she a helper, for God said, ""He shall rule over you"" (Gen 3: 16), and the Scripture says, ""The head of woman is man"" (1 Cor 11: 3; Eph 5: 22), and commands women to submit to their husbands in everything (Eph 5: 24).\nNehemiah built the walls of Jerusalem so that the people might no longer be a reproach. In his determination that knows no despair, he could turn darkness into light and grief into joy. Yet this was not all.\nThere were souls like Jerusalem, with walls broken down and gates burned with fire, souls trodden by enemies, and lost dignity. Having completed restoration of the city, Nehemiah then began restoration of those souls. He did not want for Jerusalem with the new walls to be like whitewashed tombs, while the inside full of dead men\'s bones (Mt 23: 27).\nThe Light of the Desert-Documentary on St Macarius Monastery, Egypt']"	['<urn:uuid:da00f8ee-7231-47dd-bb3f-08afdefb0de8>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T11:40:26.317848	11	56	2038
30	lord of the eagles motives helping gandalf group rescue them from wargs	The eagles' assistance in rescuing Gandalf's group was not purely altruistic. They had two main motivations: first, they were repaying a debt owed to Gandalf for previously healing their lord. Second, they wanted to harass the goblins and ruin their fun more than they actually wanted to save Gandalf, Bilbo, and the dwarves.	"['Course Hero. ""The Hobbit Study Guide."" Course Hero. 25 Aug. 2016. Web. 18 July 2018. <https://www.coursehero.com/lit/The-Hobbit/>.\nCourse Hero. (2016, August 25). The Hobbit Study Guide. In Course Hero. Retrieved July 18, 2018, from https://www.coursehero.com/lit/The-Hobbit/\n(Course Hero, 2016)\nCourse Hero. ""The Hobbit Study Guide."" August 25, 2016. Accessed July 18, 2018. https://www.coursehero.com/lit/The-Hobbit/.\nCourse Hero, ""The Hobbit Study Guide,"" August 25, 2016, accessed July 18, 2018, https://www.coursehero.com/lit/The-Hobbit/.\nAfter escaping the goblins, Bilbo Baggins catches up with the rest of his friends. He retells the story of his escape with great vigor—but leaves out the details of the ring. Gandalf reminds them the goblins will begin searching for them as soon as the sun goes down. They hurry along but are soon surrounded by wolves.\nThey climb into the trees; Dori again saves Bilbo and himself in the nick of time. The wargs (wolves) sometimes collaborate with goblins; the goblins may even ride the wolves like horses. A grey chief talks to the wolves, and Gandalf listens to their foreign language, explaining that the wargs and the goblins have a plan to kill many brave and strong humans who are returning from the south and setting up villages in the Wild.\nThere are guards placed around all of the trees, and Gandalf knows they must make their escape. Out of pinecones, he creates fireballs and throws them at the wargs. The wargs, of course, hate fire, and a great commotion ensues—such a commotion that the Lord of the Eagles takes notice. The eagles come and rescue Bilbo and his friends. Their assistance is in repayment of a debt owed to Gandalf for healing their lord at one time. The eagles take them to their eyrie (nest) and feed them rabbits and small sheep. They plan to take the group as far south as possible without creating conflict with the humans who disapprove of the eagles\' occasional raids on their sheep.\nBilbo\'s choice to find his friends illustrates both his loyalty and growing confidence. He retells the story of his escape but purposely leaves out the part about the ring. It is easy to determine his motives—that he is gaining some respect from the dwarves. He doesn\'t want them to know he escaped by using the power of invisibility.\nBilbo is sensitive to their criticisms. Even Gandalf seems impressed by Bilbo\'s escape, but the narrative also leads us to believe he is well aware of what is in Bilbo\'s pocket. Once trapped up in the trees by the wargs, a new problem begins. The group is safe for the moment but unable to escape. Tolkien uses humor to lighten the severity of the situation. The goblins come along and sing a morbid song about their plans for the dwarves, which is to ""Roast \'em alive."" While whimsical, it\'s clear the goblins are quite interested in torture. They aren\'t interested in killing to fulfill a basic need; they get pleasure from watching something die, making them the most evil species in Tolkien\'s world.\nThe dynamics between all the different species in Middle-earth can be a bit overwhelming, and the complexities are apparent in this chapter. During this portion of the road of trials, new enemies (wargs) and new allies (eagles) are introduced. Eagles appear more moral than goblins or wargs. The eagles end up saving Bilbo and his friends—though their motive was not purely altruistic. They want to harass the goblins and ruin their fun more than they want to save Gandalf, Bilbo, and the dwarves. And based on the narrator\'s description, the woodmen are inherently good, but they are not friends with the eagles, because of the birds\' desire to eat the occasional lamb.\nTolkien\'s world parallels the one he saw in World War I, where military alliances (England and France; humans, elves, and dwarves) fought against ferocious forces (Germany; goblins and wargs). Bilbo\'s loyalty to his friends may be influenced by Tolkien\'s loyalty to his group of school friends who entered military service in World War I. Of the group of four young men, only Tolkien and one other survived the war. Thorin will die in the conflict.']"	['<urn:uuid:2b1a6e48-3bca-40ff-ae2e-10d20f922d95>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T11:40:26.317848	12	53	685
31	As someone interested in emergency preparedness at sea, I'd like to know what roles crew members play during a ship fire and how the carbon dioxide system is built to handle these emergencies?	During a ship fire, each crew member has specific assigned duties. The Captain and Chief Engineer are in overall charge, with the Chief Officer and Second Engineer assisting them. Engineering staff handles engine room equipment, while navigation staff leads firefighting on deck. There's also a first aid team that prepares medical equipment and a support team that handles firefighting gear. For the CO2 system, it's built with specially designed features including bottles made of solid drawn steel that are hydraulically tested to 228 bars. The bottles contain liquid CO2 and have siphon tubes to ensure proper discharge. The system includes safety features like alarm signals when control cabinets are opened and master valves to stop accidental CO2 release.	['duty, emergency, captain, officer, hose, pump, foam, carbon dioxide, flooding,\nboundary cooling, spray, firefighting, first aid, breathing apparatus, ba, scba,\nFire suit for protecting against the radiant heat\nfrom a large fire\nFire fighting water valves can be operated at the engine room\nStringent fire prevention measures are taken for LNG\nIn the case of a fire on board ship, each person on board has to carry out his assigned duty. Each person has a specific task to do. Fire drills are conducted as soon as the ship departs on a long journey. Usually it is about one day after departure.\nEngineering staffs will attend to equipment in the engine room. In case of fire in the machinery space, the engineering staffs will play the major role in controlling the fire, and putting it out. In case of fire on the deck, the navigation staffs will be the frontline fire fighters, while the engineering staffs will play a supporting role.\nIn a normal situation, the Captain and the Chief Engineer will be overall in charge of the fire-fighting operation, and the Chief Officer and the Second Engineer Officer will assist them.\nThe engine department has to run the emergency fire pumps driven by diesel engines or the motor driven fire pumps to supply water to the hoses. This will usually be assigned to the Third Engineer Officer, and he will have a greaser to assist him. The emergency fire pump is driven by diesel engine that can be started by hand cranking.\nWhy hand cranking?\nIn a fire, electrical cables may be burnt and a motor driven fire pump may be rendered useless without electrical supply. A hand cranked diesel engine driving a fire pump, located at a secured location far away from the heat and fuel laden engine room is designed with that in mind.\nIn a fire, the water hoses are very important, and the people manning the hoses are very important. Cooling puts off the fire. However, in an oil fire in an engine room, this must be used very cautiously. This is because oil in an oil fire floats on water. By discharging water, on it, the fire may spread even more quickly because it can follow the flow of the water. The movement of the ship (rolling and pitching) will certainly make the water move about.\nWhile the frontline firefighters attack the seat of the fire, there will be another supporting gang who will assist by boundary cooling the surrounding walls adjacent to the fire. This is to prevent the heat from one room transmitting to another and so spreading the fire. They will also assist to remove combustibles that can cause the fire to spread. Fire fighters are trained to use hoses in spray water-wall protection and jet action effectively. The deck officer on the bridge will steer the ship, issue commands for slowing down the ship to take advantage of any wind direction if necessary. The engineer on duty at the engine room will respond to instructions for engine speed control, electrical power, and starting and control of fire pumps.\nThere will also be a group of people who are assigned for first aid duties. Whenever there is a fire and the general alarm has sounded, they will immediately go to the sick room and get stretchers, first aid kits, blankets, and other first aid equipment. Some people will also be assigned to go to specific locations to bring out fireman suit, axes, breathing apparatus, torches, lifelines and so on. Depending on the need for rescue operations, these items may be needed.\nFire drills are meant to prepare the crew for any fire that can occur in any part of the ship. By design, the engine room and machinery spaces are well protected by fire protection systems. Portable fire extinguishers provide the first line of defense for small fires. Fire hoses provide the second line of defense for a larger fire. Fixed foam piping over oil tanks, boilers, and oily areas provide protection for oil fires. Remote quick-closing valves are installed at oil tank outlets so that they can be closed at a secure position to cut off the supply of oil that feeds an oil fire.\nIf the fire has become too huge and cannot be controlled by the hoses, all the people will evacuate from the engine room. Doors, skylights, hatches and other openings will be tightly closed, and all the ventilation fans will be stopped. Carbon dioxide gas will be used to flood the whole engine room as a last resort. It is the last resort because once the CO2 is used to flooded the engine room, there will not be anymore left to do it a second time. When doing this total flooding, it is very important that all the openings are sealed properly, and that the CO2 gas can be contained in the compartment until the fire dies off due to starvation of oxygen. It is also very important to ensure that the fire has actually died off, and not smoldering when the doors are opened again. With a smoldering fire (or deep-seated fire), once it comes into contact with oxygen, it will burst into flame again.\nThe Captain and the Chief Engineer will monitor the whole fire episode and directs the actions from afar. They will have messengers to relay commands to the people fighting the fire.', 'In the previous article we learned about the various features of carbon dioxide gas and the advantages and disadvantages attached to it. We also studied what is a fixed CO2 fire extinguishing system and why it is so widely used.\nIn this article we would go a bit more deep into the constructional and safety specifications related to the installation and design of the fixed CO2 system. A CO2 system though efficient is extremely dangerous to the people working on the ship. Safety Of Life At Sea (SOLAS), has made special regulations regarding the usage of CO2.\nCO2 usage on ships has to abide by few safety regulations, as on ship there are lives at stake and measure to fight accidents are few .The main regulations are:\n- If the CO2 system is installed in the cargo spaces, the quantity of CO2 available should be sufficient enough to give at least a minimum of 30% of the total volume of the largest space that is protected by the CO2 system.\n- If the CO2 system is installed in machinery spaces, the quantity of CO2 available should be sufficient to give at least a volume equal to either of the following :\na)40% of the total volume of the largest machinery spaces that is protected by the CO2 system. (The volume should exclude that part of the casing where the horizontal area of the casing is 40% or less then the horizontal area of the space taken into consideration and measured midway, between tank top and lowest part of casing)\nb)35% of the total volume of the largest machinery spaces that are protected by the CO2 system including the area covered by the casing.\nIt is also a requirement that 85% of the required quantity of gas should be released into the spaces within two minutes of evacuating the fire affected space.\nConstruction of CO2 bottles for fixed fire fighting system.\nIt is imperative that the CO2 bottles are strong and sturdy due to the high internal pressure they are going to withstand. For this reason, the bottles are made from solid drawn steel and are also hydraulically tested upto 228 bars prior to installation.\nCO2 is retained inside the cylinder in the liquid form under pressure.A siphon tube is provided inside the bottle to ensure that the liquid CO2 is discharged from the bottle or else it would evaporate from the surface, giving a very slow discharge rate and taking away the latent heat would probably cause the remaining CO2 in the bottle to freeze.\nSome special features are provided to the system in order to increase the safety level and also to make operation smooth.\nThe control cabinet doors are installed with a special signaling system. Whenever a person opens the door of the control cabinet in order to operate the CO2 system, an alarm is sounded automatically. This is done to signal crew members of CO2 flooding on ship. This is also an indication to leave the fire affected place and assemble at the muster station.\nA master valve is also provided on the main pipe going to the machinery or cargo spaces, in order to stop the CO2 supply in case of accidental release.\nMarine Auxiliary Machinery, 7th Edition, 1995 by McGeorge\nSOLAS Regulations, Chapter II – 2 By International Maritime Organization(IMO) Publications.\nThis post is part of the series: CO2 Fire Extinguishing System\n- CO2 Fire Extinguishing System\n- Design Of Carbondioxide Fire Suppression System\n- Fire Fighting Arrangements in Cargo & Machinery Spaces']	['<urn:uuid:e69d84e0-fd97-488d-9fdf-5b9c7f12f8ae>', '<urn:uuid:7fd6539f-7bb9-489a-aaf8-550de5a8fee1>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T11:40:26.317848	33	118	1488
32	How do African stories blend entertainment and education today and traditionally?	African stories serve both entertainment and educational purposes. Currently, Netflix and UNESCO are supporting six emerging filmmakers to create modern African folktales, with stories ranging from fantasy to thriller, produced with $90,000 budgets and creative guidance from established mentors. These films, premiering globally on Netflix, feature diverse African talent and languages. Traditionally, as seen in places like Malawi, folktales played a central role in communities, being used to educate children in social morals and respect for tradition. The stories taught that everything - trees, animals, rivers, stones, mountains - were endowed with life and considered sacred. However, this oral tradition is vanishing due to lifestyle changes and aging storytellers, leading to initiatives to preserve these cultural elements through digital recording.	"['Netflix has announced the six short films that made the cut for The African Folktales Reimagined.\nThe collection of six short films was launched as part of Netflix’s partnership with UNESCO to support upcoming filmmakers who were provided with resources including a $90 000 budget and creative guidance by established filmmakers.\nThe emerging filmmakers were selected in 2021 following a call for submission that resulted in over 2000 applications from all over the continent. The 6 emerging storytellers from Nigeria, South Africa, Mauritania, Kenya, Uganda and Tanzania will see their respective short films, make their debut on Netflix, globally on March 29.\nThe African Folktales, Reimagined short film collection features a variety of African on-screen talent in stories by emerging African storytellers such as Mohamed Echkouna from Mauritania with Enmity Djinn; Walt Mzengi Corey from Tanzania with Katope; Korede Azeez from Nigeria with Zabin Halima; Voline Ogutu from Kenya with Anyango and the Ogre; Loukman Ali from Uganda with Katera of the Punishment Island and Gcobisa Yako from South Africa with MaMlambo.\nBelow are the film synopsis, key cast & crew:\nZabin Halima (Halima’s Choice) – by Korede Azeez – Nigeria\nWith 99% of the world population uploaded into virtual worlds, a young girl from a secluded Fulani village inadvertently elopes with an AI to escape an arranged marriage. Genre: Sci-fi, Fantasy; Language: Hausa; Director: Korede Azeez; Producer: Kenneth Gyang; Cast: Habiba Ummi Mohammed; Adam Garba, Produced by Cinema Kpatakpata.\nAnyango and the Ogre – by Voline Ogutu – Kenya\nWith the backdrop of a childhood folktale, 13-year-old Otis struggles to protect his younger siblings from a monster that lives inside their home. Genre: Fantasy, Drama; Language: KiSwahili and English; Director: Voline Ogutu; Producer: Sarah Hassan; Cast: Trevor Jones Kamau; Sarah Hassan, Produced by Alfajiri Productions.\nKatera of the Punishment Island – by Loukman Ali – Uganda\nAbandoned on an island, a woman grieving the loss of her baby exacts revenge on the powerful man who put her there. Genre: Thriller; Language: Runyankole and English; Director & Producer: Loukman Ali; Cast: Karababiito Tracy; Michael Wawuyo JR; Production company: Loukout Films.\nKatope – by Walt Mzengi Corey – Tanzania\nA young child with magical origins sets out on a journey to help end the drought that is devastating the community – even if it means risking their own life. Genre: Fantasy, Drama; Language: KiSwahili and ciGogo; Director: Walt Mzengi Corey; Producer: Petrus Van Staden; Rebecca Mzengi Corey Cast: Jene Mahenyela Mwalimu; Rahele Matete; Production company: Solela Art and Film\nEnmity Djinn – by Mohamed Echkouna – Mauritania\nThree generations after he was last summoned, an ancient Enmity Djinn finds himself in an unfamiliar city confronted by a familiar foe. Genre: Fantasy/Drama; Language: Hassaniya Arabic and French; Director & Producer: Mohamed Echkouna; Cast: Zainabou Ahmed Mohamed; Mamadou Mokhtar N’diaye Gueye.\nMaMlambo – by Gcobisa Yako – South Africa\nThe mystical river being, MaMlambo, watches over the sacred waters of discarded bodies. Genre: Drama; Language: isiXhosa; Director: Gcobisa Yako; Producer: Pakiso Albertus; Cast: Simphiwe Dana; Zikhona Bali\nEach storyteller was partnered with a local production company and under the guidance of Netflix-appointed supervising producer, Steven Markovich from Big World Cinema and industry mentors: Bongiwe Selane (mentor to Gcobisa Yako); Jenna Bass (mentorto Korede Azeez); Pape Boye (mentor to Loukman Ali); Femi Odugbemi (mentor to Mohamed Echkouna); Leila Afua Djansi (mentor to Voline Ogutu) and Tosh Gitonga (mentor to Walter Mzengi) who provided guidance and nurtured the filmmakers on their journey to bring their stories to life.\nShare your thoughts, Drop your comments below & Join the conversation, our Twitter or Facebook:', ""Malawi is endowed with a diversity of folktales and culture of storytelling handed down orally from one generation to the next. But because these stories exist only in oral form, very few are preserved in print or audio format. And with the aging of live performers, the precious intangible heritage is on the verge of extinction. The Malawi Folktales project aims to safeguard the Malawian cultural heritage before it disappears.\nMalawi is a landlocked country in southern Africa, bordering Tanzania in the east, Mozambique in the south, and Zambia in the west. Of its population of approximately 15 million, more than 80% is rural and dependent on agriculture for subsistence, and some 82% between the ages of 15 and 24 are illiterate.\n(Source: UNDP International Human Development Indications)\nMalawi represents some 16 different languages spoken by a diversity of ethnic groups, each of which has stories handed down from generation to generation. Folktales and story-telling play an integral, indeed central, role in the live of Malawians. And yet because they are circulated within communities by word of mouth for the purpose of education, either in social morals or for children, they are seldom recorded in print or audio format.\nThis oral tradition is all but vanishing in recent years due to the aging of community figures who can deliver live performances as well as changes in lifestyle. And because Malawi has made little effort to preserve the tradition, the unique culture of its ethnic groups is on the verge of extinction.\n“The emphasis is on respect for tradition as well as nature in general. The respect for tradition goes along with the belief that everything, according to the elders’ vision of the world, trees, animals, rivers, stones, mountains, are endowed with life, hence the interaction of humans and non humans in the folktales. Mountains, trees or stones were believed to be the abode of the spirits. Because today respect for these has disappeared, we see the wanton cutting down of trees, the destruction of sacred places and the disinterest in oral traditions,” said Professor Boston Soko of Oral Literature, University if Mzuzu. He provides a pool of consultancy services as an expert of the field.\nSony Corporation, at the request of Malawi National Commission for UNESCO and the GFCT, provides audio-visual recording equipment and technical training for local engineers in this project that aims to collect, edit, and digitize (document) the valuable and rich traditional culture of Malawians and pass it down to children of the next generation.\n■Director, National Librarian, National Library Service\nFolktales are very important because they help to preserve part of our cultural Heritage. I am very excited with this project because the documented Malawian Folk tales will enrich our information Resources that will be shared with readers at the National Library Service. The final product will also be digitized and added to our National Digital Repository Project at the National Library Service. Some of the folktales will eventually be published in children books in various languages. I look forward to the day when these folktales will be presented on our TV Stations.\n■Head Development Information Centre, Malawi National Library Service\nFolktales and fables are one of the oldest educational tools through which cultures have passed values and lore from one generation to the next. Storytelling based on traditional folktales is a gentle way to guide young people toward constructive personal values by presenting imaginative situations in which the outcome of both wise and unwise actions can be seen.\n■NLS ICT Specialist for data entry & documents preparation\nWhen I was 6 years old, my family and I happily sat around the fire listening to folktales told by my Mother. This stopped when we got our first television at home. By integrating the modern technology and tradition, this project will reintroduce the tradition of folk-telling to another generation.\nTristam Johnson, Boston Machika, Mathews Katoleza, Chimwemwe Sumani, and Sherpherd Phiri\nThe full high definition recording equipment provided by Sony is the latest on the market and has enhanced the quality of the recording as well as knowledge of the technical team.\n■Malawi National Commission for UNESCO\nC J Magomelo\nA nation without culture is worthless and folktales carry the basic principles of this culture which we must preserve\n■Global Future Charitable Trust\nThe project team trained by Sony is on a two-year tour of Malawi, collecting and documenting folktales circulated by word of mouth. The audio-visual recordings will be used not only in early and primary education to teach life skills to Malawian children, but also as a tool to disseminate the wisdom and values embodied in the oral traditions of the African continent to people across the world. We hope to draw on our experiences in Malawi and spread the project to other regions of southern Africa.\n■Global Future Charitable Trust (GFCT):\nGFCT is registered as a charitable entity by New Zealand Charities Commission under the charities Act 2005. GFCT aims to attain social change through sustainable human development as a means of expanding people's life opportunities and their capacity to make their own decisions.\n■Malawi National Commission for UNESCO:\nMalawi National Commission for UNESCO is a government statutory body established to coordinate interaction between UNESCO and relevant Ministries, Departments, institutions of higher learning, civil society organizations and non-governmental organizations in UNESCO's fields of competence i.e. Education, Science, Social and Human Science, Culture and Communication.""]"	['<urn:uuid:576c48d0-8786-48fb-a538-2e642563a5fc>', '<urn:uuid:1b44ca3c-e26d-44a0-8d3a-50f9a40ac153>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T11:40:26.317848	11	120	1497
33	What's special about Zadar's mix of old and new buildings?	Zadar features a unique architectural blend where centuries-old masonry stands alongside modern installations like the Greeting to the Sun. This mix resulted from post-war reconstruction efforts, where modernist buildings were integrated with preserved historical structures. For example, the concrete Rašica building along the Forum harmoniously coexists with older buildings, and the modern Boutique Hostel was created by adapting one of Bruno Milić's modernist blocks while preserving its original façade.	['The reasons for this architectural mix-up rests largely on the fact that Zadar was subjected to serious bombing raids by the allies in World War II, leaving the huge kind of holes in the urban fabric of the Old Town that had to be filled by post-war planners.\nOf all Croatia’s Adriatic cities, Zadar was the one that suffered most in terms of destruction and depopulation during the war, and the generations that rebuilt the city in the Fifties and Sixties were genuine urban pioneers. Maybe it’s because of Zadar’s post-war experience of being a city on the architectural frontier that makes it such a forward-looking and innovative place today.\nThe rebuilding of Zadar was a major talking point for Croatia’s post-war rulers. An initial plan put forward by architects Zenko Strižić and Božidar Rašica envisaged clearing much of the Old Town on the peninsula and building a modernist-inspired city in its place. However there was a competing desire among Croatian intellectuals to preserve as much of the city’s historical heritage as they could, and the radical plan was never enacted. The urge to revitalize Zadar’s cultural inheritance was another important plank in the city’s post-war reconstruction. Writer and intellectual Miroslav Krleža was sent to Zadar in 1949 to oversee an exhibition entitled Gold and Silver of Zadar, based on the artistic treasures salvaged from the city’s churches. Not only did this exhibition lead to the establishment of the The Gold and Silver of Zadar museum that still exists in the city’s Benedictine Convent of St Mary’s, it also helped create a Croatia-wide wave of interest in Zadar, boosting the reconstructing city’s morale.\nIt was Krleža that presided over the competition for a new urban plan. The winning proposal, by Bruno Milić, left more room for Zadar’s historic buildings, but again portrayed Zadar as a utopian city of the future, filled with rectangular blocks of concrete. Milić’s Zadar plan definitely caught the spirit of the age and was turned into an exhibition that toured Europe, visiting Paris, Milan, Moscow and London - before being shelved by the Croatian authorities as far too costly and ambitious.\nHowever the architectural visions of the Fifties, however over-optimistic, played a major part in determining contemporary Zadar’s appearance. Leading modernist architects were the ones who got the job of designing most of the new buldings. It was Rašica and Milić, for example, who built the boxes-on-stilts buildings that line Zadar’s main street, the Kalelarga.\nOf course a lot of people have felt over time that the buildings designed by Rašica and Milić were mistakes, concrete eyesores that took the heart and soul out of a historical city. However they were a logical response to Zadar’s wartime destruction, and nowadays have become classics in their own right.\nThey also fit in rather well with Zadar’s current transformation into an innovative Adriatic city of the present day, a place in which a piece of centuries-old masonry can stand alongside a sound-and-light installation like Greeting to the Sun. The so-called Rašica building, the pale concrete slab that runs along one side of the Forum, looks just as serene and well-proportioned as its significantly older neighbours, especially now that the Forum area has been thoroughly renovated and re-landscaped by architect Ante Uglešić.\nZadar’s Boutique Hostel, a modern-interior headline-grabber designed by Studio Up and Damir Gamulin Gamba, is an adaptation of one of Bruno Milić’s modernist blocks on the Kalelarga. An important aspect of the project was that Milić’s original façade had to remain unchanged.\nIncomplete and controversial though it is, the modernist projects of the Fifties and Sixties have had a profound effect on Zadar’s urban look. The upcoming projects of the future, such as the ambitious Gates of Zadar project entrusted to Sea-Organ author Nikola Bašić, can only be seen in the context of these earlier utopian predecessors.']	['<urn:uuid:91679c77-2608-42ec-8a00-cae0e2c54eda>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T11:40:26.317848	10	69	640
34	how often australian locusts numbers tracked	The Australian Plague Locust Commission (APLC) produces a monthly Locust Bulletin during times of locust activity (spring, summer and autumn). The Bulletin reports the current and previous month's locust situation and provides an outlook for likely developments in the following three months for Australian plague locust, spur-throated locust, and the migratory locust.	"['Terms and Descriptions used in the Locust Bulletin\nAbout the Locust Bulletin\nThe Australian Plague Locust Commission (APLC) produces a monthly Locust Bulletin during times of locust activity (spring, summer and autumn). The Bulletin reports the current and previous month\'s locust situation and weather events of potential significance to locust developments. The Bulletin also gives an outlook of likely developments in the following three months for Australian plague locust - Chortoicetes terminifera, spur-throated locust - Austracris guttulosa, and the migratory locust - Locusta migratoria.\nThe terms used in the Bulletin to describe the biology, behaviour and population density have defined meanings to increase accuracy and usefulness. The forecasts are dependent on current locust population distribution information, and weather and environmental conditions. Risk statements address both the probability and the potential consequences of an occurrence.\nThe majority of information documented in the Bulletin comes from regular surveys by the APLC. Additional information comes from landholders and the public, state primary industries departments and biosecurity agencies.\nLocust population densities\nA characteristic of locusts is gregarious behaviour and the formation of high density population units known as Bands of nymphs and Swarms of adults. Where higher densities occur, a large proportion of the regional population is concentrated in small areas occupied by these units and lower densities occur elsewhere. Therefore the high densities cannot be extrapolated to the area of an entire habitat area or region. Typically a range of density classes is found within surveyed regions with higher densities where habitat conditions are favourable. Where specific terms are used for density classes, the word is capitalised (Scattered or Concentration), while more general regional density descriptions (such as low or high) apply to a range of specific density categories.\nThe following terms are used to describe different density levels\nDensity classes for nymphs and adults\nNumber per m2\n1 - 5\n6 - 30\n31 - 80\nNumber per m2\nNumber per hectare\n0.03 - 0.1\n>200 - 1000\n0.2 - 0.5\n>1000 - 5000\n0.6 - 3.0\n>5000 - 30,000\nLow Density Swarm (LDS)\n4.0 - 10\n>30,000 - 100,000\nMedium Density Swarm (MDS)\n11 - 50\n>100,000 - 500,000\nHigh Density Swarm (HDS)\nGeneral regional density classes\nRegional Nymphal density\nvery low, occasional\nRegional Adult Density\nvery low, occasional\nA Band is a gregariously behaving group of nymphs at a density greater than 80 locust nymphs per square metre. Bands have well defined fronts and nymphs ""march"" in the same general direction.\nThree parameters are used to define bands of locust nymphs:\n- The size of a band as indicated by the length of the band front.\n- The infestation level which is measured by the length of bands per square km2 kilometre in a given area (most often a paddock).\n- The number of bands in a specified area (e.g. the Hillston-Hay area).\n|Term||Band length (m)||Term||km band front/km2||Term||No. of bands|\n|Infestation level||Number of bands in a specified area|\n|very small||<10||Light||0 - 0.5|\nOne or two\n|1 - 2|\n|small||10 - 100||Medium||0.6 - 2||A few||3 - 10|\n|medium||101 - 1000||Heavy||>2||A number||11 - 50|\n|large||1001 - 5000||Many, a large number||>50|\n|very large||> 5000|\nSwarms are gregariously behaving groups of adult locusts flying together as a unit at a density greater than four individuals/m2.\nThe size and number of swarms in a specified area are referred to when detailing an infestation.\n|Term||Swarm area (km2)||Term||Number|\n|Swarm Size||Number of swarms|\n|very small||<1||few, some||3 - 5|\n|small||1 - 2||a number||6 - 10|\n|medium||3 - 10||many||11 - 50|\n|large||11 - 20||very many, a large number, numerous||>50|\nOther terms mentioned in the Locust Bulletin\nOther terms used in the Bulletin to refer to locust biology and behaviour, rainfall and forecast probabilities. These are defined below.\n|Term||Rain falling in one week (mm)|\n|Light||0 - < 20|\n|Moderate||20 - 40|\n|Heavy||41 - 100|\n|Term||Chance that event will occur (%)|\n|Unlikely, low probability||<30|\n|May, moderate probability||30-70|\n|Likely, high probability||>70|\nLocust Biology and Behaviour\nA fully winged, mature locust capable of breeding and migrating.\nShort distance (up to 50 km/day) daytime migratory movement of gregariously behaving locusts, generally at low altitude (0 – 300 m), resulting in redistribution of a population.\nGeneral term covering all stages of the locust life cycle from egg to adult.\nAn over-wintering egg that does not develop for several months. Diapause ends by mid June and development resumes as soon as conditions are favourable.\nFirst and second instar.\nAn area containing more than 10 egg pods per square metre.\nLocusts leaving an area by migration.\nNewly moulted, soft-bodied adult incapable of sustained flight. This stage lasts approximately 5 days.\nFemales with mature eggs of 4-5 mm length.\nLocusts which fly into an area.\nStage of nymphal development separated by a moult. Australian plague and migratory locusts have five nymphal instars while the spur-throated locusts have 6-8 instars.\nFourth and fifth instars.\nFemales depositing eggs into the ground in egg pods containing up to 50 eggs for Australian plague and migratory locusts and 120 for spur-throated locusts.\nThird instar locusts.\nNocturnal, wind-assisted flight of locusts usually at higher altitudes (up to 1200 m), resulting in population displacement up to several hundred kilometres overnight.\nImmature locust without wings (though wing buds may be visible) and is therefore unable to fly. This stage in the locust life cycle follows hatching, lasts approximately five weeks and is often referred to as the hopper stage.\nEggs in an arrested state of development (diapause or quiescence).\nNymphs resulting from an autumn egg laying may develop to third instar and persist through winter in that stage. Development resumes in spring.\nLocusts that become adult in late autumn but do not mature and develop eggs until early spring.\nAn egg in which development has been arrested by the onset of dry conditions and which will resume development when sufficient rain falls.\nAn area of band or swarm density locusts at least 1km2 in size.\nFemale locusts bore into the ground with their abdomens to test the soil but do not lay eggs.\nForecasting districts referred to in the Locust Bulletin\nMap of forecast regions with main areas of potential locust habitat shown in yellow\nGeneral regions referred to in the Bulletin\nLocust Bulletin Subscription\nYou can subscribe online to receive email notification when each Locust Bulletin is released.\nIf you would like to be on the mailing list to receive printed copies of Locust Bulletins, please send your name and postal address to APLC by email or alternatively, provide them by post to the:\nForecasting & Information Officer\nAustralian Plague Locust Commission\nAustralian Government Department of Agriculture and Water Resources\nGPO Box 858\nCanberra ACT 2601']"	['<urn:uuid:0fa9113e-7955-4f20-af8d-cfc4af402a1b>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T11:40:26.317848	6	52	1101
35	How do the accessibility features for typing differ between physical keyboard modifications and mobile apps for people with motor impairments?	Physical keyboard modifications and mobile apps offer different solutions for people with motor impairments. For physical keyboards, options include keyguards (plastic/metal shields with holes), alternative keyboards (mini or extra-large keys), and built-in operating system features like StickyKeys and FilterKeys that help with sequential keystrokes and repeated keypresses. For mobile apps, the solutions include Switch Access/Switch Control which allows sequential navigation through screen items using physical switches, and AssistiveTouch which enables configuring multiple custom gestures according to user needs.	"['The keyboard can be the biggest obstacle to computing for a person with a mobility impairment. Fortunately, those who lack the dexterity or range of motion necessary to operate a standard keyboard have a wide range of options from which to choose. Pointers can be held in the mouth or mounted to a hat or headgear and used to press keys on a standard keyboard. Repositioning the keyboard to the floor can allow someone to use his feet instead of his hands for typing.\nBefore purchasing a complex keyboard option, evaluate the accessibility features that are built into current popular operating systems. For instance, the Accessibility Options control panel in current versions of Microsoft Windows™ contains settings that can make a standard keyboard easier to use. For a person who has a single point of entry (a single finger or a mouthstick), use of StickyKeys allows keystrokes that are usually entered simultaneously to be entered sequentially. FilterKeys can eliminate repeated keystrokes for a person who tends to keep a key pressed down too long or presses keys multiple times because of uncontrolled movements. Check the settings for these features and experiment with different time delays for optimum effect. Macintosh operating systems have similar features in the Easy Access control panel.\nConsider using the features common in popular word processors, such as Microsoft Word™, to ease text entry. The AutoCorrect™ feature of Word allows sentences or blocks of text, such as an address, to be represented by unique and brief letter sequences. For example, entering ""myaddr"" could be set to automatically display one\'s address in proper format. Long words can be abbreviated and entered into the AutoCorrect settings to increase typing speed and accuracy.\nA keyguard is a plastic or metal shield that fits over a standard keyboard. Holes are drilled into the guard to help an individual with poor dexterity or hand control press only the desired key without inadvertently pressing other keys. Keyguards are available from a variety of manufacturers.\nAlternative keyboards can be considered for a person who cannot effectively operate a regular keyboard despite changing settings or use of a keyguard. For someone who has a limited range of motion, a minikeyboard may be considered. If a person has good range of motion and poor dexterity, a keyboard with extra-large keys can offer a good solution. Several vendors offer an array of alternative keyboards, including those that are configured to relieve the effects of repetitive stress injuries (RSI).\nWhen physically activating a keyboard — whether through changing the settings or switching to an alternative keyboard — is not possible, evaluate the utility of a virtual keyboard. A virtual keyboard appears on the computer screen as a picture of a keyboard. A mouse, trackball, or alternative pointing system activates the keys on the screen and inserts the appropriate keystrokes into the desired program. A person can enter text by clicking on specific keys on the keyboard image. Modifier keys, such as CONTROL and ALT, can also be accessed, as can the function keys. Some virtual keyboards incorporate word prediction to increase entry speed and may include alternate layouts in addition to the traditional QWERTY layout found on standard keyboards.\nFor more information, consult Accessible Technology and Working Together: Computers and People with Mobility Impairments or view the video by the same title.', 'Making sure your app’s design doesn’t alienate users, whether they have physical disabilities or not\nYou have just developed your app and it looks extremely beautiful and sleek with all the bells and whistles! Woohoo! Everyone is going to love it right?\nWell, first it’s important to review if it is indeed well designed for as many users as possible. Can your grandma figure out how it works? Is it easy to use when you only have one hand available? Is it compatible with support products for the blind, deaf, etc?\nIn this post, we will examine some concrete examples of good and bad UI design that address the needs of people with certain physical disabilities.\nHow Do People with Disabilities Interact with Mobile Apps?\nFirstly, it is important to discuss just how do people with disabilities use mobile applications and overcome their challenges in using them that the rest of the population doesn’t face. Many times they rely on special support products, therefore it’s important to keep them in mind when designing an app to make sure that it works well alongside them. Below we’ll cover some of those products.\nFor the Visually Impaired:\n- TalkBack (Android) and Narrator (Windows): Read the current screen’s content\n- VoiceOver (iOS): Tells you out loud what is in the touch area\n- Zoom (Android, iOS, and Windows): Works like a magnifying glass, allowing you to enlarge content on the screen using touch gestures\n- Dictation (iOS and Android): Allows the user to input text by his or her voice\nFor the Hearing Impaired:\n- Subtitle (Android and iOS): Allows you to add and display subtitles for multimedia content\n- Visible and vibrating alerts (Android and iOS): Allows the user to be aware of alerts and messages through lights and vibrating notifications instead of sounds\nFor Those with Impaired Motor Skills:\n- Switch Access for Android and Switch Control for iOS: Allow you to navigate the screen by sequentially highlighting items and use a physical switch to select them\n- AssistiveTouch (iOS): Allows you to configure multiple gestures, according to the user’s needs\nTip: The accessibility API in iOS and Android provides a set of libraries for developers to create applications that leverage these phone support products.\nWe will now break down six examples of good and bad accessible mobile UI design, categorizing them by the 4 principles of mobile accessibility (outlined by the W3C): perceptibility, operability, understandability, and robustness.\nExamples 1&2: Perceptibility\nThis principle relates to the visibility of the elements in an application. It takes into account how information should be displayed regarding the size of the screen when the zoom action is performed or when the font size or contrast changes. Besides having images that represent an option or functionality, it is a must to have a compatible text alternative within the accessibility layer.\nTake this example showing poor perceptibility on the left and better perceptibility on the right when the TalkBack tool is being used.\nIf your user is utilizing one of the tools for the visually impaired, he or she will be read aloud the elements on the screen. On the left, the description, “Button 1,” is much more confusing than the description of the button on the right.\nAlways ensure that a vision impaired user receives the exact same information that is available visually by adding clear, descriptive text for every visual element.\nAnother important aspect of an app’s perceptibility is the use of color contrast.\nIn the first image, it’s difficult to see the text options since the text is gray on a gray background.\nIn the second picture, the text consists of white letters on either a blue or green background, which is far easier to see.\nWe must take caution with the colors that we use for our texts, buttons, and backgrounds for our application. By doing this we are keeping in mind and benefiting people with partial vision so that they can easily distinguish the buttons in applications and perceive contrasts differentiating between foreground and background colors.\nTo validate whether or not your app has good color contrast and if it’s highly perceptible, you can use these tools:\n- Colour Contrast Analyser: From the UI design stage, this tool helps you know if the text legibility and contrast of visual elements are acceptable.\n- Accessibility Scanner: A free app for Android launched by Google, this tool gives you suggestions for improvement for larger tactile elements, like increasing the contrast or providing descriptions so all users with accessibility needs can use the application without problems.\nExample 3: Operability\nThis principle relates to how easy it is for a user to carry out actions, gestures, and movements on the screen. It’s important to make it obvious at all times where the user is located within the app and make it easy for them to know what they are supposed to do next at all times.\nTake a look at these examples of operability:\nIn the first image, you can see that the title screen is ORDER, a title that doesn’t indicate to a user who may have stepped away for thirty minutes or so what it was they were in the middle of ordering. It’s also not very helpful to have fields in the form that do not provide cues as to what needs to be filled out in each.\nIn the second picture, we see that the screen contains a more specific title: what exactly they are ordering and how far in the process they are of ordering it. In addition, text fields are displayed letting you know that you must enter your phone number and its description, like “home” or “mobile” number. With these improvements, the user is always aware of what stage of the process they are in, what they ordering, and also what exactly is the information that is being asked of him or her.\nTitles for each screen and descriptions in the text fields benefit people with cognitive disabilities; users who may have difficulty understanding and assimilating information or face difficulties with interpreting the meaning of a selection or text field.\nExample 4: Understandability\nWhen speaking of an app’s “understandability” we are referring to how it adapts to changes in screen orientation. For example, does it maintain a consistent design and give us the proper cues as to how it works? Understandability, of course, also relates to the language used in notifications, instructions, buttons, etc. Everything should be clear and simple, not to mention free of grammatical errors.\nIn the first image, we can see that the four menu item is called FAQs. We should be careful with abbreviations and be ensure that users can access the expanded form of abbreviations.\nBy fulfilling these accessibility criteria for understandability, we can benefit users who have difficulty decoding words, rely on screen magnifiers, have limited memory or have difficulty using context to aid understanding.\nExample 5: Robustness\nThis principle is not actually directly related to the design, but as a designer, tester or developer, we can still check the behavior of the application with the use of support products.\nContent must be robust enough to be interpreted reliably by support products. A highly accessible mobile application must be compatible with support products, working just as well as when you do not have an activated support product.\nA user gives the “Next” button 3 taps\nWhen a user has enabled VoiceOver, TalkBack, or Narrator, in order to select an option, he or she must tap once to hear what it is and then double tap in order to select it.\nIn the first image, we see the user doing just that. The user taps on the “Next” option and the support app indicates that it’s the “Next” option and that it’s a button. He or she then double taps it, but the app becomes unresponsive and doesn’t display the next screen. This can often happen when an app is not compatible with the support product, certain buttons and functions don’t adhere to the alternate way of using them.\nOn the other hand, after tapping three times, the next corresponding page appears, meaning the application is compatible with the support product and performs the action as it would when the support product is not activated.\nAccessibility is Key for Good Mobile UI Design\nNow you know several examples of accessible mobile UI design and hopefully, you’ll only make apps in the future that take users with physical disabilities into account. At the least, I hope you take five minutes to test your app by using these support products or accessibility testing tools in order to put yourself in the shoes of someone else.\nDo you have any other do’s and don’ts of accessible mobile UI design to add to this list?\nRecommended for You\nMobile Application Testing Strategy: How to Take The Security of Your Mobile Apps to the Next Level of OWASP\nWhat is OWASP and why is it becoming increasingly relevant in the IT industry? What methods exist to complete validations according to its security standards? Take a look at this article from Matías Reina and take its input in mind for your mobile application testing…\nAbstracta Opens New Mobile Testing Center of Excellence\nA win-win for a local community and Uruguay’s growing technology services industry With the aim of providing high value-added software testing services for projects in the US, UK, Chile, Uruguay and the world, Abstracta continues on its path of growth! We’re pleased to announce the…']"	['<urn:uuid:8d7fc14d-b769-4401-8eb9-d640b106023f>', '<urn:uuid:4255695f-dd1e-4419-a730-ae5984d67921>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T11:40:26.317848	20	78	2141
36	What eco-friendly land management practices are being adopted in Lisle?	Lisle Park District is piloting four organic parks - Leask Lane, Abbeywoods Pond, Connelly and Oak Hill. They will manage these properties pesticide-free by implementing cultural controls including core aeration, overseeding and mowing high.	['MGG’s Ryan Anderson previews Chicago Grows Green Week and its theme of “Growing Pesticide-Free Lawns & Gardens for the Long-Term”\nYou “have to bring something to the party” when motivating park districts, schools or individuals to eliminate their lawn pesticide use as Healthy Communities Project (HCP) founder and Midwest Grows Green (MGG) friend Amy Joyce would say.\n“We’ve held firm to our belief that we would not just complain about pesticide use, but would focus on bringing sound solutions to the table through proper turf management training,” Amy explained regarding HCP’s approach to motivating the City of Whitefish Bay in Wisconsin to manage three pesticide-free parks.\nMost pesticide elimination policies do not happen overnight. They require continual community engagement, staff training, recordkeeping, long-term planning and improvement to achieve success and remain effective in future years. While seemingly cumbersome, MGG has learned and tracked many successful efforts from advocates and practitioners like Amy Joyce. We will share these lessons to both Greater Chicago residents and the entire Midwest during our upcoming Chicago Grows Green Week (CGG Week).\nWe kickoff the week on June 1st and June 2nd in the southeastern suburb of Riverdale and the western suburb of Lisle, respectively. Riverdale Park District has managed their parks pesticide-free for many years and plans to install a low-input eco-lawn at Cooper Park. Meanwhile, Lisle Park District will pilot four organic parks of Leask Lane, Abbeywoods Pond, Connelly and Oak Hill. Each district can manage their properties pesticide-free by increasing cultural controls of core aeration, overseeding and mowing high. Both districts will host a CGG Week Kickoff event that will demonstrate these essential cultural controls through a series of interactive games. Visit bit.ly/CGGriverdale19 to register for Riverdale’s demonstration at Riverdale Park and bit.ly/CGGlisle19 for Lisle’s event at Leask Lane Park.\nDuring the weekdays, CGG Week features communities that have enacted detailed plans to reduce outdoor pesticide use in the long-term. The highlight over the workdays is our MGG Lawn & Land Forum “Building Capacity” webinar that offers a unique look into the stakeholders to engage and messages to share to pass a comprehensive natural lawn care policy. Over two years, Midwest Grows Green and Go Green Park Ridge engaged the Park Ridge-Niles School District 64 on the issue of natural lawn care. This year we helped the district break its contract with TruGreen and manage all grounds in-house. We believe this action by District 64 is unique and other communities interested in reducing lawn pesticides can learn a great deal from this project. Go Green Park Ridge’s Amy Bartucci will join myself on Monday, June 3rd at noon as we cover the many meetings we held and attended, the recommendations we provided and the ultimate actions taken by District 64 to break their contract and manage all grounds pesticide-free. Register for FREE at bit.ly/CGGLLF19.\nFinally, we celebrate all the achievements covered by CGG Week and the successes to come during our CGG Week Celebration & Fundraiser at Dig Right In Landscaping, Inc’s warehouse. Organizations such as the Deep Roots Project, River Forest Sustainability Commission and Go Green Brookfield will share their sustainable landscaping progress alongside Midwest Grows Green. You can, also, beautify your yard with native plants sold by Natural, Communities, Native Plants or kick-start your lawn and garden with Dig Right In Compost Tea. 100% of compost tea sales will go towards MGG projects in 2019, while 10% of proceeds from native plants will go to MGG if pre-ordered at bit.ly/CGGnatives before May 19th. Register for this event at bit.ly/CGGcel19.\nOur CGG Week events should provide a full overview of both organic gardening and/or natural lawn care. However, the MGG Pledge offers one more resource if you feel unprepared to transition to organic land care after this week. Each month we share the latest updates, information and resources regarding natural lawn care and organic gardening. Thank you and I hope to see you in early June!']	['<urn:uuid:2debac0e-59a1-45cb-93bd-6aaca9d301c4>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T11:40:26.317848	10	34	654
37	what role music performance expression baroque period modern times	In the Baroque period, music was an event rather than a product, with performers being the only vehicle through which music could be heard. Each locality and time period had unique performance traditions, and composers were products of their cultural environment. Today, the high volume in sound that audiences are accustomed to tends to erase articulations and soft dynamics, with extreme levels of compression in recordings resulting in an impoverishment of musical expressivity.	"['Following the article about the musicians\' technique, Philippe raised an interesting and precise question about musical expressivity in Breton and Irish traditional fast tunes. An essential component in the performance of slow airs, what then is the expressivity in tunes such as reels, jigs, gavottes, ridées or other fast tunes? The expressive means is a wide topic that could be the subject of a Doctoral thesis. Let us focus on a couple of points, particularly the transposition of expressive tools from one music to another, keeping in mind an essential question: what are we talking about when evoking ""musical expressivity""?\nLet\'s go right to the point: musical expressivity is very relative and depends of the genre you play. If you are a classical musician and ask a talabarder to perform a tune with more ""expressivity"", you may be surprised by what you are going to hear. Don\'t expect pianissimo dynamics such as an oboist could play. The bombarde and oboe are part of the same family instrument but their range of expressivity is different in dynamics as well as in technical possibilities.\nIt is quite obvious that, during a musical session in a pub or during a fest-noz, the musicians don\'t perform with so much dynamics. One simple reason to explain it is the function of this music: in the first case, the musicians play ""for themselves"" – the session is a moment to share between them, rather than a public representation (read the article about the sessions). In the second case, the function is to make people dance, therefore the musical expressivity is to be found somewhere else.\nExploiting the instrument\'s musical possibilities\nSurely, it makes no sense to confine any musical genre in its functional foundations. The result would be a restriction of creativity. The influence – I would say the predominance – of some instruments in a music draws its outlines. Therefore it is quite easy to generalise and think that there is no nuance in traditional music. It is not because the bombarde can\'t do as many dynamics as the Celtic harp that it is not expressive. It is not because we perform traditional Breton music on the Celtic harp that we should restrict our dynamics with the pretext that the nuance range of the bombarde is small. Musical creativity has always benefited from instrumental complementarities whether in a symphonic orchestra, in a rock band, in a pipe band and so on. Why would the bagad de Lann-Bihoué be seeking to recruit a Celtic harp player if it wasn\'t for its expressive complementarities?\nThe range of the musician\'s expressivity has always been developing in the history of music, going to the extreme caricature – without surprise – in contemporary music. The result is a request for ""always more"" expressivity, often synonymous with ""always less"": ""always more loud"" (""always less soft""), ""always more staccato"" (""always less legato""), etc. Articulations and dynamics must be placed in their historical context: a fortissimo doesn\'t have the same intensity played by a Baroque musician as when played by a musician today; a pizzicato played in Classical times doesn\'t have the same sound as a Bartók pizz.\nThe high volume in sound the public now is accustomed to tends to erase the articulations and soft dynamics. The use of ever more powerful monitors during concerts and the extreme level of compression in recordings and radio and television broadcastings results in an impoverishment in musical expressivity. Dynamics are an important element in jazz and classical music. In any kind of music, but particularly in those genres, mastering a CD shouldn\'t consist in raising the volume against the dynamics.\nIn addition, there is the technical quality of each instrument: the sound volume of a pipe, the high precision of snare beats, the duration in resonance of Celtic harp strings… All instruments have specific expressive capacities. Regarding the resources at his or her disposal, the musician must adapt their musical expressivity. Creativity is, there, an important element.\nWhether caused by aesthetical affiliation or the instrument\'s technical capacities, the expressive means vary considerably from one musician to another one. Trying to compare them with external criteria is therefore never accurate. Musical expressivity is also subjective: within the same tradition, two musicians playing the same instruments won\'t use the same artefacts to express their feelings.\nThe musical expressivity in traditional music\nAmong all the technical capacities developed by traditional musicians, ornamentation is one of the main keys to musical expressivity. The ornamentation has not only the property to embellish the melody; it has the important capacity to structure the fundamental element in dance tunes: the rhythm. For expressive purposes, ornamentation is also a much more precise and subtle tool than dynamics. We find this expressive form in Celtic music – the Scottish pibroc\'h being an extreme example – as well as in the Harpsichord repertoire of the Baroque time.\nIn a slow airs, during a musical evening, and in a fast tune to get people dancing, in a Cèilidh, a large part of the traditional musician\'s expressivity will be conveyed through the principle of ornamentation. This ornamentation, or the variations thereof – the ones which cannot be written since they\'re played at will by the desire of the moment – gives the full meaning to the word ""traditional"" since it is always renewed.\nTo conclude, I would bring up the definition of the term ""expressivity"" applied to music: expressivity is the quality of being expressive, to express feelings and thoughts. Each society has created music with specific features, codes, functions, tools in order to reinforce its cohesion as well as to differentiate itself from others. Expressivity enables dissociating one music from another one, a society from another, which gives all the value of musical performances in their context. Learning this expressive code as well as the notes or the rhythm of a tune should require all our attention.\nI would like to thank Philippe for bringing up such an interesting subject. The principal idea of this blog being to exchange questions and answers, don\'t hesitate to share yours.', ""5 Myths of Baroque String Playing\n- Donʼt vibrate\n- Donʼt sustain or play legato\n- Swell on long notes\n- Play out of tune\n- Get a student violin, put gut strings on it, remove the chinrest, borrow a baroque bow and survive a Bach fugue.\n- Donʼt vibrate: Trying to play beautifully by eliminating your most immediate expressive tool is like eliminating soy sauce from Chinese cooking to make your recipes “French.” French cuisine doesnʼt use soy-sauce, so shouldnʼt that work? Not only are your new dishes not “French,” but you have diminished the quality of your old dishes. Trying to play beautifully and expressively simply by modes of subtraction is well, daft.\n- Donʼt sustain or play legato: Baroque music was born from vocal music. Have you ever heard a song? All songs, especially those from the 17th and 18th-centuries contain lots of legato and long tones. Often the notes the violin plays are exactly the same notes as a singer who is singing vowels! Legato, lyrical, beautiful playing and singing was PRIZED in the 18th-century. So was phrasing, dynamics, articulation, inﬂection, stress, rubato, accelerando, good intonation, varied bow strokes, passion, drama, emotion...sounds like they liked expressive music back then!\n- Swell on long notes: Its called Messa di Voce. Basically, forget about it. The ﬁrst early music scholars misunderstood 18th century bowing exercises which instructed string players how to develop their sound to make LONG SUSTAINED TONES (see no. 2). We owe a great debt to those scholars who opened up a world of information for us, but they got some things wrong. Messa di Voce is a beautiful, and very organic ornament that can be used if the moment really calls for it (just like a mordent, a tierce de coule, or a shot of vibrato...). But, the ubiquitous use of it is simply the confusion of this ornament with a bowing exercise, it was not something to do on every note. So extreme swelling? Take ibuprofen, it doesnʼt belong in early music.\n- Play out of Tune: As it turns out, 17th and 18th century string players were expected to achieve extremely detailed pitch accuracy. YES, REALLY! They understood that tuning one key perfectly (with pure intonation) on a keyboard will make other keys unusable. They devised various 'temperaments' making all keys usable, with some sounding better than others. In this way, each key had a unique quality and character. They reveled in those differences and players were expected to be able to display those tonal distinctions. Early violin primers suggested teaching major and minor semi-tones in the second lesson (i.e. d - eb, vs d - d#.) The French theorist, Joseph Sauveur, suggested limiting musicians to a 50-note octave in the “interest of practicality.”; a whole step should be divided into only 5 semi-tones (d,d#,d##,ebb,eb,e.) Equal temperament was the ingenious method of making everything equally out of tune and uniform. People were scandalized by it! So, it can be challenging when at ﬁrst working with wind and keyboard instruments at different temperaments, but in the end, this trial can hone your intonation to a new level of perfection. Try measuring the breadth of your beautiful vibrato. You might discover it spans several double sharps and ﬂats.\n- Get a student violin, put gut strings on it, remove the chinrest, borrow a baroque bow and survive a Bach fugue: Yes, It is expensive to buy a whole new instrument, but if you simply put weird strings on your ʻsecondʼ violin and try to play it with a strange bow with no chin or shoulder support, you have only succeeded in playing a handicapped violin. Just because something looks a bit like something else, and feels different, its not the something else. Ever taste those beautiful, realistic fake sushi rolls? And, as towering and magniﬁcent a composer JS BACH was, he was one of hundreds of composers during that period. Ever wonder why you havenʼt heard of the others? Were they all bad?\nAll kidding aside, what is baroque performance practice?\nFirstly, lets start with the understanding that there is no such thing as a single “baroque style.” There was as much, if not more, diversity of performance traditions between 1600 - 1800 as there were in the following two-hundred years. Of course, there was no internet or recordings, which meant there was no such thing as disembodied music. If you lived in Venice, you heard the music being played in Venice on that day. Maybe you got to hear a touring soloist from Paris, but the performance was the music and the player was the ONLY vehicle through which the music could be heard. Music was an event, not a product. Furthermore, any composer who heard someone play had to rely on his memory to recapture what ʻthat other musicʼ sounded like. If he wanted to incorporate this new style of music into his compositions, it was 100% ﬁltered through his own point of view and memory. Think of JS BACH writing French overtures. He had heard French music (ie Couperin) performed when he was studying in Luneburg at the age of 15. Where do we look to understand how to play those overtures? Do we just study his scores? French music? Couperin? There are so many layers to this story, and so much to learn and think about and experiment with!\nEvery composer was unique, a product of their locality (not simply their nationality) timeperiod and cultural environment. Communication among themselves was limited. However, they did write books and, it is mostly through these treatises that we can establish certain commonalities of notational and performance conventions of their time. But, these books are starting points. Every note is its own unique case, the books and scores left to us canʼt tell us the whole story.It turns out, Historical Performance Practice is a pretty extensive study. It is a wide array\nof varied languages, replete with immeasurable and nuanced expressions. Applying a few simple gimmicks to your current playing only cheapens an exquisite body of work by an important group of composers.\nIf you approach the study of early music with the same seriousness and integrity that you approached learning later music, the study will reward you with a world of personal, musical eloquence you may not have known lived within you. It was all about the expression back then. Is that really any different from now?\nFor more information on the study of early music, consider coming to the Institute for Early Music on Modern Instruments. Study baroque performance practice on your own modern instruments. June 10 -14, 2013 at George Mason University.\nNext page: About The Vivaldi Project""]"	['<urn:uuid:3d08d891-d4b7-43e6-b55b-a1e63a8a3e7a>', '<urn:uuid:95628e8f-5116-46aa-aaa1-75fe9d42a47d>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T11:40:26.317848	9	73	2127
38	What is the best way to deal with unwanted noise and echo when recording audio content in a regular room at home?	There are several quick fixes to help with echo: putting down an area rug, hanging sound-absorbing artwork or sound panels on walls, closing curtains to prevent sound bouncing off windows, and creating a barrier around your equipment with a portable booth.	['Setting up your podcast studio might sound like a big, scary project, but in reality, it’s one of the simplest things you’ll do when starting a podcast.\nSetting up Your Podcast Studio\nIt probably doesn’t seem simple to you right now because, well, you’re over-complicating it.\nYour podcast studio equipment\nI’ve been podcasting since August 2014, and since August 2014 my podcast studio has never had more 3 pieces of equipment that help me record my podcast:\n- MacBook Pro laptop\n- A USB mic (ATR2100) plugged into my laptop\n- Apple Earbuds\nI’ve recorded over 250 episodes with what I consider to be very high audio quality, and while that’s not just thanks to my 3-piece equipment setup, it does prove that you don’t have to have the most expensive microphone or even a mixer to do it right.\nYour recording environment\nSo if it’s not just thanks to my equipment, what else is involved in setting up your podcast studio?\nWell, your recording environment (where your podcast studio is physically set up) has A LOT to do with it, too.\nBut when I say it has a lot to do with it, I don’t mean that you have to have a fancy recording studio with padding on the walls and noise-cancelling windows.\nMy podcast studio is set up in an extra bedroom in our house (my office) – not in a recording studio or a room that cost me thousands of dollars to build. While this is super convenient and I love it, it sometimes means I have to get creative when I record.\nFor example, what if my neighbor decides to do yard work when I’m recording?\nMost podcast studio complications have to do with your environment, and they come into play when:\n- You’re not prepared\n- Something unexpected happens\nLogically, what can you do to avoid these complications?\n- Make sure you’re prepared\n- Have a backup plan for the unexpected\nBeing prepared to record\nLet’s cover being prepared first.\nBeing prepared to record a podcast episode has everything to do with having the right equipment setup, which we’ve already proven is super simple, and being in a conducive recording environment.\nA conducive recording environment is one where you can control:\n- The noise level,\n- The echo, and\n- Your surroundings (to a realistic extent).\nSo if your living room happens to be a place where your kids hang out, play, and watch TV – because there is no other place in the house for them to do those things, or for whatever reason – then guess what? That’s not a conducive recording environment.\nIf your kitchen table is next to the front door, and every time someone walks down your street your dog starts barking uncontrollably, then guess what? That’s not a conducive recording environment.\nIf your sitting area is a larger space with just one chair, a love seat, no carpeting and concrete walls, then the echo is likely going to be pretty intense. And guess what?…\nSo where is a space in your home or office where you have control over the noise level, the echo, and your surroundings?\nIf you’re able to find a space where you have control over the noise level and your surroundings, but the echo is giving you trouble, then there are loads of quick fixes that can help.\nSome examples include:\n- Putting down an area rug\n- Hanging sound-absorbing artwork on the walls or using sound panels\n- Closing your curtains so the sound doesn’t bounce off windows\n- Creating a barrier around your equipment with a portable booth\nMaking sure your podcast studio is prepared for recoding isn’t hard to do, and this preparation will save you from having to deal with recurring recording issues moving forward.\nHaving a backup plan\nAside from making sure your podcast studio is setup in a controlled environment, it’s also important to have a backup plan, you know, for those times when things happen that aren’t in your control.\nMaybe your 4-year old wasn’t able to go to daycare today because she’s sick, and so your day has suddenly shifted.\nMaybe your neighbor decided to do yard work on a different day than usual, and so you’re competing with lawnmowers and voices outside your window.\nWhatever the unforeseen circumstances may be, you should always have a backup plan.\nFor example, if your 4-year old is home instead of in daycare, then shift your recording schedule for after your significant other gets home and can watch them.\nIf you don’t have anyone like that, then schedule your recording for when your 4-year old is better and can go back to daycare.\nThis is a perfect example of why we recommend always being one month ahead of your recording schedule: you’ll never be a situation where your 4-year old getting sick causes you to miss publishing an episode on time.\nIf your neighbor is doing yard work on an off day, then think about another place in your house where you can record – perhaps your bedroom closet so you can shut the door and be in a tight, quiet space.\nSound super strange?\nIt’s really not; the closet is actually a very popular place to record episodes because it’s a very controlled environment (by default – they’re typically not that big), plus, you’ve got all your clothes in there to absorb the sound!\nI know people who literally choose to record every single episode in their closet – that is their recording studio.\nLuckily, with a 3-piece equipment setup, you’re able to pickup and move around the house when necessary.\nYour podcast studio setup\nYour podcast studio setup consists of two things:\n- Your equipment\n- Your environment\nKeep it simple, and be prepared. It really doesn’t get any simpler than that!\nWant to check out all of our equipment recommendations, including boom arms, pop filters, and mic flags? We share everything we use and recommend for your podcast equipment here!']	['<urn:uuid:ac4faeae-0c37-4f62-a273-c80b0047bb90>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T11:40:26.317848	22	41	1003
39	I'm really interested in racing history - who was the first driver to ever win a Formula One championship with a car that had the engine in the back?	Jack Brabham became the first driver to win the Formula One championship with a rear-engined car when he won the championship driving the Cooper T51 in 1959.	"['- Cooper T51\nCooper T51 Category Formula One, Formula Two Constructor Cooper Car Company Designer(s) Owen Maddock Technical specifications Chassis Steel spaceframe Suspension (front) Double wishbone, coil spring and damper Suspension (rear) Double wishbone, leaf spring and damper Axle track F: 55 in (1,397 mm)\nR: 53 in (1,346 mm)\nWheelbase 104 in (2,642 mm) Engine Climax, Maserati, Castellotti, Borgward or Ferrari 2.5 or 1.5 litre straight-4, naturally aspirated. Mid-engine, longitudinally mounted. Transmission Citroen, Colotti (and others) manual gearbox. Weight 1,545 lb (701 kg) Tyres Dunlop Competition history Notable entrants Cooper, Rob Walker Racing Team, Scuderia Centro Sud, Yeoman Credit Racing Team Notable drivers Stirling Moss\nWolfgang Von Trips\nDebut 1959 Monaco Grand Prix Races Wins Poles Fastest laps 24 5 6 6 Constructors\' Championships 1 (1959) Drivers\' Championships 1 (Jack Brabham, 1959)\nThe Cooper T51 was a Formula One and Formula Two racing car designed and built by the Cooper Car Company for the 1959 Formula One season. The T51 earned a significant place in motor racing history when Jack Brabham drove the car to become the first driver to win the championship with a rear-engined car, in 1959. The T51 was raced in several configurations by various entrants until 1963 and in all no less than 38 drivers were entered to drive T51s in Grand Prix races.\n- 1 The Chassis\n- 2 A historic season\n- 3 Into obscurity\n- 4 T51 Formula One results (Cooper & Walker entries only)\n- 5 Privateer entries\n- 6 References\n- 7 External links\nAesthetically and aerodynamically the T51 was a natural development of the T43 and T45 cars that had given Cooper their first two wins. The Coopers continued their practice of building spaceframe chassis that ignored orthodox design thinking in having several curved links and the rear-engine layout meant weight savings and aerodynamic advantages over the front-engined cars, which had to find room for propshafts to the rear wheels. Also the location of the fuel tanks on either side of the cockpit rather than at the rear meant the car handled more consistently with different fuel loads, a vital factor during races which lasted up to three hours. One notable throwback, however, was the car\'s leaf spring rear suspension, although it used a more modern coil spring and wishbone setup at the front.\nEngine & transmission\nThe standard F1 T51 was the first Cooper to be powered by the 2.5 litre 4 cylinder engine which Cooper and Lotus had commissioned Coventry Climax to build specifically for their rear-engined machines. The pioneering nature of this configuration created problems of its own, since there were so few rear-engined production cars from which a gearbox could be sourced. This shortage eventually created a niche in the market which paved the way for Hewland\'s prominence, but in the meantime many different solutions were tried with varying degrees of success. The works Coopers were fitted with modified Citroen gearboxes, while Rob Walker\'s team ran bespoke units from Italian specialist Valerio Colotti, although these proved much more fragile.\nIn all eight different engines were used in the back of T51s in championship races, with 2.2 and 1.5 litre Climax engines in addition to the standard 2.5: Scuderia Centro Sud and others used 2.5 and 1.5 litre engines from Maserati; the British Racing Partnership team used F2 powerplants from Borgward; Scuderia Eugenio Castellotti used their own Ferrari-derived 2.5 litre units; and one car used a Ferrari 2 litre engine lifted from a 625LM sportscar.\nA historic season\nThe T51 had already won the Glover Trophy at Goodwood and the Silverstone International Trophy before it made its first World Championship appearance in the 1959 Monaco Grand Prix, with no less than eight examples entered. The Cooper works team fielded Jack Brabham, Bruce McLaren and Masten Gregory, Stirling Moss and Maurice Trintignant were entered by Rob Walker, two 1.5 litre F2 cars were entered by Equipe Nationale Belge for Lucien Bianchi and Alain de Changy, with Ivor Bueb driving another F2 machine. Only the five 2.5 litre cars qualified, with Stirling Moss in pole position (Cooper\'s first) and Brabham starting in third place. Jean Behra took the lead at the start, but after his Ferrari developed engine problems after 21 laps the Coopers dominated, with Moss and Brabham running first and second until Moss\'s transmission gave up the ghost 19 laps from the end. Brabham cruised to his first World Championship win with Trintignant third and McLaren fifth.From Monaco on Cooper\'s season went from strength to strength, with Brabham leading the championship from start to finish. Brabham took his second win in the British Grand Prix, before Moss took a brace in Portugal and Italy and dominated the non-championship Gold Cup.\nBy the final race at Sebring Cooper already had the Constructors\' Championship in the bag, but the Drivers Championship was still up for grabs. Moss needed to beat Brabham and finish second or better to take the title, while Ferrari\'s Tony Brooks had a mathematical chance but needed both the win and fastest lap. Moss sprinted into the lead from pole position with Brabham in pursuit. After five laps Moss was a commanding ten seconds ahead, before his gearbox gave way again. Brabham led right up to the final lap, when a poor decision on Brabham\'s part meant he ran out of fuel. He managed to push his car across the line in fourth, but Cooper still won the race as Bruce McLaren became the youngest winner in Formula One history (a record he held until surpassed by Fernando Alonso in 2003), leaving Brabham the Drivers\' Champion.\nCooper travelled down to the 1960 Argentine Grand Prix seemingly at the peak of their powers, and Trintignant won the Buenos Aires F1 event that preceded the main championship race. However this turned out to be the last major win for a T51, as the speed of the new Lotus 18 raised eyebrows around the paddock. On the journey back, John Cooper made his mind up that to stay at the front he needed to build a new car, and at the next championship race at Monaco the lowline T53 made its debut. In the meantime Moss took the Walker T51 to second in the Glover Trophy and qualified on pole position for the International Trophy before retiring with wishbone failure. Rob Walker had already bought a Lotus 18 for Moss, but ironically the Englishman was to miss a large part of the season through injuries sustained when his notoriously fragile Lotus lost a wheel at speed in Belgium. Cooper entered the T51 just three more times, with Scarab refugee Chuck Daigh and journeyman Ron Flockhart retiring each time.\nT51 Formula One results (Cooper & Walker entries only)\n(key) (Races in italics indicate non-championship events, results in bold indicate pole position)\n1959 Entrant Engine Drivers GLV AIN INT MON 500 NED FRA GBR GER POR ITA IGC SCT USA Points WCC Cooper Car Company Climax 2.5l S4 Jack Brabham 2 Ret 1 1 2 3 1 Ret Ret 3 2 2 4 40 1st Masten Gregory 5 Ret Ret 3 Ret 7 Ret 2 Bruce McLaren 5 5 3 Ret Ret Ret Ret 1 Giorgio Scarlatti 12 RRC Walker Racing Team Climax 2.5l S4 Stirling Moss 1 Ret Ret Ret 1 1 1 Ret Maurice Trintignant Ret 3 8 11 5 4 4 9 2 1960 Entrant Engine Drivers RSA CBA ARG GLV INT MON 500 NED BEL FRA GBR SCT POR ITA LOM IGC USA Points WCC Cooper Car Company Climax 2.5l S4 Jack Brabham Ret Ret 0 NC Chuck Daigh Ret Ron Flockhart Ret Ret RRC Walker Racing Team Climax 2.5l S4 Stirling Moss 2 3s 2 19 Maurice Trintignant 1\nAs well as being a racing team Cooper was very much a business, as shown by the 1958 International Trophy where 19 Coopers of various types made up more than half the grid. Right from the outset the T51 was designed to be produced in large numbers and offered for sale to privateer teams, with Rob Walker getting full-works cars and the rest receiving slightly more workmanlike models. No fewer than ten other entrants ran T51s in Formula 1 during 1959, and with the works team switching to the T53, and Walker using Lotus 18s, many of the ex-works cars were sold and continued to be entered in 1960. Indeed, the T51 continued to appear on minor, non-championship F1 race entry lists as late as the 1967 Rhodesian Grand Prix!\nScuderia Centro Sud\nGuglielmo Dei set up Scuderia Centro Sud to publicise his business as distributor of Maserati cars to central and southern Italy, hence the name he chose for his team. Having fielded Maserati 250Fs for several years he switched to T51s midway through 1960, with the Climax replaced with a two-year-old 2.5 litre Maserati straight-4 250s engine which was used in Maserati\'s unspectacular 250S sportscar. In this configuration Centro Sud entered the T51 in 14 World Championship races, more than any other entrant. After converting to the 1.5 litre formula post-1960, Centro Sud\'s Cooper Maseratis still made the occasional appearance in minor Italian F1 races as late as 1963. Dei never kept his drivers for long, as they tended to be either journeymen, faded former stars like Maurice Trintignant, or up-and-coming racers like Wolfgang Von Trips who would soon be snapped up by bigger teams. In all, more than a dozen drivers drove T51s for Centro Sud.\nScuderia Centro Sud Championship Results (key)\nEntrant Engine Drivers MON 500 NED FRA GBR GER POR ITA USA Points WCC 1959 Scuderia Centro Sud Maserati 2.5l S4 Ian Burgess Ret Ret 6 14 0 - Colin Davies Ret 11 Hans Herrmann Ret Nicha Cabral 10 Entrant Engine Drivers ARG MON 500 NED BEL FRA GBR POR ITA USA Points WCC 1960 Scuderia Centro Sud Maserati 2.5l S4 Masten Gregory DNQ DNS 9 14 Ret 3 5th Ian Burgess DNQ 10 Ret Ret Maurice Trintignant Ret Ret Ret 15 Carlos Menditeguy 4 Roberto Bonomi 11 Nicha Cabral Ret Giorgio Scarlatti Ret Alfonso Thiele Ret Wolfgang Von Trips 9 Entrant Engine Drivers MON NED BEL FRA GBR GER ITA USA Points WCC 1961 Scuderia Centro Sud Maserati 1.5l S4 Massimo Natili Ret 0 -\nBritish Racing Partnership/Yeoman Credit Racing Team\nThe British Racing Partnership team was founded by Ken Gregory and Alfred Moss, Stirling\'s father, in late 1957. They spent 1958 and 1959 competing in Formula Two events, with the odd entry into F1 races, particularly in the UK, and experimented with 1.5 litre Borgward engines for their 1959-spec T51s, under an arrangement whereby Stirling Moss would use the Borgward-engined BRP cars with Rob Walker as the entrant.\nFor 1960 they had agreed for the three Samengo-Turner brothers, motor racing enthusiasts who owned Yeoman Credit Ltd., to bankroll their season with 2.5 litre Climax-engined F1 entries as well as a full slate of F2 entries with Borgward. BRP\'s list of drivers for 1960 was quite impressive, including at various points future champions Phil Hill and Denis Hulme, respected names like Tony Brooks and Dan Gurney, as well as Ferrari sportscar ace Olivier Gendebien. The Yeoman Credit cars were distinctive in having a mint green and red colourscheme and an air intake that stretched from the top of the car above the front axle along the left side of the cockpit to the engine\'s intake trumpets behind.\nBy the end of 1960 BRP had lost three drivers - Ivor Bueb, Harry Schell (who also ran his own T51 under the Ecurie Bleue banner) and Chris Bristow - to fatal accidents, and the Samengo-Turners decided to take their money to Reg Parnell\'s team instead, and since Parnell had filled up Cooper\'s order book BRP had to switch to Lotus for 1961.\nBritish Racing Partnership championship results (key)\nEntrant Engine Drivers MON 500 NED FRA GBR GER POR ITA USA Points WCC 1959 British Racing Partnership Climax 1.5l S4 /\nBorgward 1.5l S4\nIvor Bueb DNQ 13 0 - Chris Bristow 10 Entrant Engine Drivers ARG MON 500 NED BEL FRA GBR POR ITA USA Points WCC 1960 Yeoman Credit Racing Team Climax 2.5l S4 Tony Brooks 4 Ret Ret 5 5 Ret 0† -† Chris Bristow Ret Ret Ret Henry Taylor 7 4 8 DNS 14 Olivier Gendebien 3 2 9 7 Ret Bruce Halford 8 Phil Hill 6\n†In 1960 only the best-performing car for each chassis-engine combination scored Constructors\' points in each race, and on every occasion this was a works Cooper-Climax.\nScuderia Eugenio Castellotti\nNamed after the Ferrari driver who died in testing at Modena in 1957, Scuderia Eugenio Castellotti used modified Ferrari Tipo 553 2.0 litre engines bored out to 2.5 litres and rebranded as ""Castellotti"", with ""Eugenio"" on the cam covers. The cars were easily distinguishable from other F1 T51s in that their exhausts were fitted on the left, while all other engines used in T51s had their exhausts on the right. The team competed in four events in 1960 with lesser-known Italian drivers Gino Munaron, Giorgio Scarlatti and Giulio Cabianca.\nIn a cruel twist of fate, Cabianca was at the wheel of a T51-Castellotti at Modena in 1961 when he fell victim to one of Formula One\'s most bizarre accidents. With his throttle stuck open he was unable to stop, went out of the circuit onto public roads and collided with a taxicab.\nScuderia Eugenio Castellotti Championship Results (key)\nEntrant Engine Drivers ARG MON 500 NED BEL FRA GBR POR ITA USA Points WCC 1960 Scuderia Eugenio Castellotti Castellotti 2.5l S4 Giorgio Scarlatti DNQ 3 5th Gino Munaron Ret 15 Ret Giulio Cabianca 4\nThe T51 was entered into championship races by a further twelve organisations, the most notable being Reg Parnell, who later took over Yeoman Credit\'s racing activities from BRP, and Fred Armbruster, who entered Pete Lovely in the 1960 United States Grand Prix with a T51 uniquely fitted with a Ferrari Tipo 555 engine, a different engine to those used by Scuderia Eugenio Castelotti. The final World Championship appearance for a T51 came when Trevor Blokdyk took his Scuderia Lupini T51-Maserati to twelfth place in the 1963 South African Grand Prix.\nFounders: Charles Cooper • John Cooper\n- Ivan Rendall (1993). The Chequered Flag - 100 years of Motor Racing. Ted Smart. ISBN 0-297-83220-4 pgs 214-217\nDesigners: Owen Maddock • Jack Brabham • Eddie Stait • Derrick White\nFormula One/Two: 1000/1100 • Cooper-Bristol • T41 • T43 • T44 • T45 • T51 • T53 • T55 • T58 • T60 • T63 • T66 • T71 • T73 • T75 • T77 • T80 • T81 • T82 • T84 • T86 • T91\nTasman: T55 • T62 • T70 • T79\nFormula Three/Junior: 500 • T52 • T56 • T59 • T65/T67 • T72 • T76 • T83 • T85\nOther: T54 (IndyCar) • T87 (hillclimb) • T88 (Formula C) • T90 (Formula 5000)\nSportscars: Cooper-MG • Cooper-Jaguar • Bobtail • Monaco • Mini\nCars that competed in the 1959 Formula One season (ex. Indianapolis 500) Cars that competed in the 1960 Formula One season (ex. Indianapolis 500) Cars that competed in the 1961 Formula One season Cars that competed in the 1963 Formula One seasonATS 100 • Brabham BT3 • Brabham BT6 • Brabham BT7 • BRM P57 • BRM P61 • BRP • Cooper T51 • Cooper T53 • Cooper T55 • Cooper T60 • Cooper T66 • De Klerk Alfa Romeo Special • De Tomaso F1 • Ferrari 156 • Ferrari 156 Aero • Gilby • LDS Mk1 • Lola Mk4/Mk4A • Lotus 18 • Lotus 18/21 • Lotus 21 • Lotus 22 • Lotus 24 • Lotus 25 • Porsche 718 • Scirocco 01 • Scirocco 02 • Stebro MkIV\nWikimedia Foundation. 2010.\nСм. также в других словарях:\nCooper T51 — Der 1961 beim 500 Meilen Rennen v … Deutsch Wikipedia\nCooper T51 — du Rob Walker Racing à Donington Présentation Équipe … Wikipédia en Français\nCooper (automobile) — Cooper Car Company Cooper Car Company est une ancienne écurie de course automobile fondée par Charles et John Cooper. Charles a commencé à construire des monoplaces de course dans un petit garage de Surbiton dans le Surrey avant que son fils ne… … Wikipédia en Français\nCooper Car Company — est une ancienne écurie de course automobile fondée par Charles et John Cooper. Charles a commencé à construire des monoplaces de course dans un petit garage de Surbiton dans le Surrey avant que son fils ne le rejoigne pour fonder leur officine… … Wikipédia en Français\nCooper Car Company — Cooper Full name Cooper Car Company Base … Wikipedia\nCooper T53 — Présentation Équipe … Wikipédia en Français\nCooper T43 — The Cooper T43 was a Formula One and Formula Two racing car designed and built by the Cooper Car Company for the 1957 Formula One season, first appearing at the 1957 Monaco Grand Prix in a factory entered car for Jack Brabham. The T43 earned a… … Wikipedia\nCooper T45 — La Cooper T45 est la première Formule 1 ayant un moteur arrière. Née grâce à John Cooper en 1958 elle fut 2 fois titrée en 1959 et 1960. Cooper T45 La Cooper T45 à Donington … Wikipédia en Français\nCooper T53 — Das Cockpit des Cooper T53 E … Deutsch Wikipedia\nCooper T81 — Category Formula One Constructor Cooper Car Company Designer(s) Derrick White Technical specifications Chassis Aluminium monocoque … Wikipedia']"	['<urn:uuid:ad2c00e7-dfca-4581-b995-74de45b1985d>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T11:40:26.317848	29	27	2906
40	washington dc monuments stone maintenance protection	The historic monuments in Washington D.C. are built from various types of stone including fine-grained marble (Jefferson Memorial, Lincoln Memorial), coarse-grained marble (Corcoran Museum, Pan American Union Building), limestone (Architecture Hall), and red sandstone (Renwick Museum). These structures require specific maintenance approaches for protection. Vertical walls typically show less pollution damage than horizontal surfaces, which can begin showing roughening within years of installation. For maintenance and cleaning, specialized systems are employed - the DOFF system for removing biological matter and coatings, the TORC system for dealing with carbon sulphating and lime-based materials, and Clean-Film for safely removing carbon pollution and smoke marks from internal surfaces without water damage.	"['By Bruce R. Doe\nAbout The Product\nPublished by means of the yankee Geophysical Union as a part of the Field journey Guidebooks Series.\nThis afternoon journey in downtown Washington, D.C., will enable examine of toxins, structural, and different kinds of wear to buildings with emphasis on these developed of excellent- to coarse-grained marble yet with representatives of limestone and crimson sandstone. The travel will contain so as of exam [date of labor initiated (i) and/or occupied (o.) and stone type]: the Renwick Museum (1859o., crimson sandstone trim changed 1987), Corcoran Museum (1879c. enlarged 1927, coarse-grained marble), purple pass structures (1915i.-1917o., 1927i., fine-grained marble), Memorial Continental corridor of the Daughters of the yank Revolution, DAR (1904i.-1909o., fine-grained marble), Pan American Union development (1908i.-1910o., coarse-grained marble), structure corridor (DAR) (1929o., limestone), Washington Monument (1885o., marble), Jefferson Memorial (1943o., external of fine-grained marble), and Lincoln Memorial (1922o., very fine-grained marble). Coarse-grained marble is in general extra proof against pollutants harm than fine-grained marble. Direct publicity to the weather hurries up degradation. the main critical pollutants harm happens to balustrades and columns: runoff from bronze onto fine-grained marble may also reason critical degradation. Any type of overhang has a tendency to guard the underlying stone, yet exceptions can be visible on the Jefferson Memorial. ordinarily, vertical partitions have much less toxins harm than horizontal surfaces which may start to express roughening inside of years of deploy. Limestone in universal use is unusually proof against degradation owing, no less than partly, to the porous nature of the stone that inhibits runoff. If one of many journeys is made to the Powell construction of the U.S. Geological Survey in Reston, Virginia (1974o., concrete), will probably be noticeable that concrete behaves very similar to limestone and marble.\nRead Online or Download A Different View of Stone Monuments, Memorials and Buildings of Washington, D.C. PDF\nSimilar geophysics books\nHaving a look Into the Earth comprehensively describes the foundations and purposes of either \'global\' and \'exploration\' geophysics on all scales. It varieties an advent to geophysics compatible when you don\'t unavoidably intend to develop into specialist geophysicists, together with geologists, civil engineers, environmental scientists, and box archaeologists.\nIn regards to the ProductPublished by way of the yank Geophysical Union as a part of the Antarctic study sequence. content material:\nConcerning the ProductPublished by means of the yank Geophysical Union as a part of the sphere journey Guidebooks sequence. box journey T117 is designed to provide quite a few Quaternary geological reviews from part of the yankee Southwest. Specialties in the extensive scope of ""Quaternary geology"" that might be mentioned contain archaeology, climatology and paleoclimatology, geochemistry, geomorphology, paleoecology, paleontology, pedology, sedimentology, stratigraphy, tectonics, and volcanology.\nAdditional resources for A Different View of Stone Monuments, Memorials and Buildings of Washington, D.C.\nNon-convergent, margins are usually accompanied by a dipolar band of distinct positive anomalies above the shelf break and less distinct negative anomalies above the foot of the continental slope. The amplitudes are tens of milligals and the width corresponds to that of the continental margin. 5 Some Important Aspects of the Terrestrial Gravity Field and Internal Mass Distribution 17 slope, the more pronounced the gravity anomalies. This medium-scale feature is mostly suppressed by the Bouguer reduction by which the change from positive to negative BA values (from ocean to continent) is brought out.\nThe following descriptions will necessarily be somewhat subjective. These will include: the Earth’s figure and constitution (Sect. 2: ellipsoid, geoid, Earth’s density and shells crust, mantle, and core), continents and oceans (Sect. 3: isostasy of large geological structures, fold mountain ranges, limits to lateral density variations), plate tectonics and mantle flow (Sect. 4: mantle dynamics: convection, ridges, subduction), associated gravity anomalies (Sect. 5: scale laws and kind and size of gravity variations or anomalies to be expected), other large-scale gravity features (Sect.\nTo successfully apply gravity to the search for mineral resources, knowledge of the processes of mineral concentration and their geological associations is equally essential. In thinking about gravity it is critical to distinguish between the different kinds of anomalies: the customary Bouguer anomaly (BA), the Free Air anomaly (FA) and the isostatic anomalies. The different reference models used in defining the various anomalies must be taken into account (see Chap. 4); otherwise gross misinterpretations are the result; the relations with topographical, geological and tectonic features is very different; for example, mountain ranges are usually accompanied by generally positive, but highly scattered FA values and at the same time by a strongly negative smooth BA; spreading ocean ridges have a similar gravity expression, except that the BA is positive, if referenced to sea level, but negative in comparison with the deep sea basins.', 'Products and consultancy for sensitive stonework restoration and conservation\nOur expertise and systems are relied on by architects and building restoration companies across the UK for innovative and effective stone cleaning and masonry restoration.\nWe maintain an approved list of stone cleaning companies who have received our training and are using genuine systems.\nTHE DOFF SYSTEM EFFECTIVELY REMOVES\n- Paint coatings\n- Certain types of graffiti\n- Bird or vermin fouling\n- Algae, moss, fungi\n- Other biological matter\n- Wax coatings\nTHE TORC SYSTEM EFFECTIVELY REMOVES\n- Carbon sulphating\n- Lime & cement-based paints\n- Paint residues\n- Some old oil based paints\n- Lime scale\nCLEANFILM EFFECTIVELY REMOVES\n- Carbon pollution\n- Smoke marks\n- Smoke soiling\n- Without using water\n- From internal surfaces\nThe DOFF is an expertly engineered system that utilises a normal water supply to remove some of the more resistant forms of dirt or markings from a range of surfaces.\nThe water supply is fed into a hot-box where it is subjected to temperatures as high as 150℃. The resultant heated water, or vapour, is then filtered through the heat resistant hoses to the nozzle at a rate of between 3 to 10 litres per minute.\nOnce cleaned, the surface dries within minutes to prevent any long-term damage as a result of trapped moisture.\n“If you want a medical analogy for the DOFF it’s like the difference between leaches and penicillin!”\nGary Stapleton, Chairman of the Victoria Cross Trust\nThe TORC system is an evolution of the original Jos system, with its most distinctive feature being the revolving nozzle – creating a gentle swirling vortex to offset the concentration of the water and granulate mix.\nThe powder used is an inert substance that is applied with water simultaneously to achieve a ‘gentle clean’.\nIts ability to effectively clean surfaces is largely due to the innovation of nozzle, rather than the granulate running through it.\nClean-Film is a latex cleaning paste that is applied to the target area and, once left for a 24-48 hour period, can be peeled off – removing all dirt in the process.\nIt is made up of a combination of natural latex and ammonia. Once the film is removed, the ammonia evaporates from the surface without leaving behind any residue.\nThere are no special arrangements that need to be made for its disposal, as it can be done at landfill sites.\nOUR PRODUCTS HAVE BEEN USED ON\nSOME OF THE COUNTRIES MOST ICONIC BUILDINGS\nWestminster Abbey | Oxford University | Tower of London | St Pancras Station | The Albert Memorial | Canterbury Cathedral | Cambridge University\nMARKET-LEADING CLEANING SYSTEMS\nOur systems have been used for large-scale projects, including Westminster Abbey, Oxford University, and the Tower of London.\nKafften Limited has enjoyed a close working relationship with Stonehealth for many years, not only for the supply of specialist equipment and materials, but also for providing bespoke training. Their professionalism and attention to detail is second to none. Their office staff are always helpful and polite and willing to go the extra mile to ensure they provide exactly what is required, when it is required. Long may it continue!GEMMA PEARDON\nWe have worked with Stonehealth since we started up and the help and support we get from them is invaluable and gone a long way towards making us the professional / successful company we are today.STUART THOMAS\nAs someone with a long, proven, record working on the repair and restoration of historic masonry, I have been working with Stonehealth Ltd from the time that they first launched the Jos system into the UK during the 1980’s. Since then I have seen the introduction of the Doff and the Torc systems (which superseded Jos). The company’s experience and knowledge of masonry cleaning, in particular brickwork, has been most useful to myself and the many clients I have had occasion to refer them to. Whilst Stonehealth Ltd do not carry out contractual work they will arrange for supervised trials, produce reports, and assist architects and other specifiers with preparing masonry cleaning specifications. I have no hesitation in recommending this reliable professional organisation who have high standards and an impeccable reputation.DR. GERARD LYNCH\nSTONE CLEANING AND RESTORATION CONSULTANCY\nWe have been supplying systems and consultancy to the building conservation\nand restoration industry for over 20 years.\nArchitects, surveyors, conservation bodies and building and restoration companies regularly call on our expertise in stone restoration and masonry cleaning and conservation. Our involvement in a project as consultants can be independent of the use of our products i.e. it doesn’t necessarily mean our products will be used.\nAdvice on Stone Cleaning and Restoration\nWe offer advice on the most appropriate and sensitive methods of cleaning and restoring stonework and brickwork.\nFeasibility Studies for Stone Cleaning\nOur experience means we are often called on to carry out trials on sample areas of masonry. We then use the findings from those trials to create specifications for full projects.\nApproved Register of Stone Cleaning Contractors\nWe maintain a register of approved stone cleaning contractors. These contractors have been though our training program and are using equipment supplied by us.\nWorkshop and Field Engineers\nWe employ engineers who maintain and service our stone cleaning systems either on-site or in our workshop.']"	['<urn:uuid:cdf3a878-3b1e-4b0a-bddc-2957786b83cd>', '<urn:uuid:7cf77588-7fbb-406c-aa41-2022dc74bc2d>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T11:40:26.317848	6	108	1659
41	what percentage lifetime risk lung cancer brca2 mutation smokers survival rate early diagnosis	Smokers with BRCA2 mutations have a 25% lifetime risk of developing lung cancer, compared to the general 13% risk for smokers. When lung cancer is diagnosed at an early stage, the survival rate can be as high as 56%. However, most patients are not diagnosed early, and the overall 5-year survival rate is only 18.6%.	"['Cancer gene variation \'doubles lung cancer risk\' in smokers\nSmokers who carry a gene variant found in about 2 per cent of the population have an even higher risk of lung cancer, a large-scale UK-led study has shown.\nScientists found that a specific change in the gene almost doubles the overall risk of lung cancer.\nSmokers generally have a 13 per cent life-time risk of lung cancer, meaning that thirteen out of every hundred smokers tend to develop the disease. But this risk rose to 25 per cent if they carried the gene variant. This means that one in four gene carriers would be expected to get cancer if they smoked.\nOf the 10 million adults who smoke in the UK, up to 200,000 are thought to fall into this category, the researchers said.\nStudy leader Professor Richard Houlston, from the Institute of Cancer Research in London, said: ""Our results show that some smokers with BRCA2 mutations are at an enormous risk of lung cancer - somewhere in the region of 25 per cent over their lifetime.\n""Lung cancer claims more than a million lives a year worldwide and is by far the biggest cancer killer in the UK. We know that the single biggest thing we can do to reduce death rates is to persuade people not to smoke, and our new findings make plain that this is even more critical in people with an underlying genetic risk.""\nTo find gene variations linked to lung cancer, the scientists scoured existing databases containing information on the DNA of more than 17,000 Europeans with and without lung cancer.\nThey spotted a specific alteration in the genetic code of BRCA2 known as c.9976T that was more common in people who went on to develop the disease.\nA weaker link between lung cancer and changes in another gene, CHEK2, was also identified.\nThe results, published in the journal Nature Genetics, open up the possibility of personalised treatment for lung cancer patients with BRCA2 mutations.\nA family of experimental drugs called PARP inhibitors have shown signs of effectiveness in breast and ovarian cancer patients with defective versions of the gene. However, more trials are needed to ascertain whether these drugs could work against lung cancer.\nEarlier this year Cancer Research UK launched a trial to look at using these drugs in lung cancer patients.\nProfessor Peter Johnson, the charity\'s chief clinician, said: “We’ve known for two decades that inherited mutations in BRCA2 make people more likely to develop breast and ovarian cancer, but these new findings show a greater risk of lung cancer too, especially for people who smoke.\n“Importantly, this research suggests that treatments designed for breast and ovarian cancer may also be effective in lung cancer, where we urgently need new drugs.\nProfessor Johnson said research into the gene changes within many types of cancer is continuing to yield important information about how cancers develop, and how to find new treatments.\n“But, with or without one of these genetic flaws, the single most effective way to reduce the risk of lung cancer is to be a non-smoker,” he added.\nThe research was supported by a range of organisations, including Cancer Research UK.\nWang Y., McKay J.D., Rafnar T., Wang Z., Timofeeva M.N., Broderick P., Zong X., Laplana M., Wei Y. & Han Y. & (2014). Rare variants of large effect in BRCA2 and CHEK2 affect risk of lung cancer., Nature genetics, PMID: 24880342', 'Lung cancer is currently the most common cancer worldwide. In most countries, lung cancer is also the most malignant form of tumor with the lowest survival rates. As of 2018, in most countries worldwide, lung cancer is the primary cancer-related cause of death. A staggering 1.8 million deaths from lung cancer in both sexes combined were recorded in 2018 globally. In the USA lung cancer mortality peaked at 159,292 in 2005 but has since decreased by 6.5% to 148,945 in 2016.\nThe five-year survival rate for lung cancer in the U.S.A. as of 2015 is 18.6 percent, much lower than other types of cancers such as colorectal (64.5 percent), breast (89.6 percent) and prostate (98.2 percent). When the disease is diagnosed at an early stage, the survival rate of lung cancer can be as high as 56 percent. Unfortunately, most lung cancer patients are rarely, if ever, diagnosed at an early stage. As a result of the spreading of the malignancy, more than half of lung cancer patients die within one year of being diagnosed. The 5-year survival rate regardless of stage is only 15% among lung cancer patients.\nThe latest statistics provided by the World Cancer Research Fund International revealed a total of 2,093,876 new cases of lung cancer being diagnosed in 2018 for both sexes worldwide. Prevalence of lung cancer was the highest among men worldwide, representing 15.5% (1,368,524 new cases) of the total number of new cases diagnosed in 2018. Lung cancer ranked number 3 for women, representing 8.8% of the total number of new cancer cases diagnosed in 2018, or 725,352 new cases in lung cancer alone, worldwide.\nWhen individuals stop working due to cancer this represents a loss to society in the loss of productivity. The National Institutes of Health estimated that annual costs for the care of lung cancer in the U.S.A. reached $13.4 billion in 2015. Loss of earnings/productivity due to early death from lung cancer in the U.S.A adds another $21.3 billion in costs in 2015. In other countries, including Ireland, lung cancer is responsible for the highest productivity loss for cancer-related mortality.\nProjected productivity loss for cancer-related mortality 2011-2030 in Ireland\n[Source: Alison Pearce, Cathy Bradley, Paul Hanly, Ciaran O’Neill, Audrey Alforque Thomas, Michal Molcho and Linda Sharp Projecting productivity losses for cancer-related mortality 2011 – 2030 BMC Cancer. 2016 Oct 18;16(1):804.PMCID: PMC5069877 DOI: 10.1186/s12885-016-2854-4]\nIn the past, lung cancer was regarded as a disease for the elderly as the disease was found mostly in men over 50 years old, between the ages of 60-75 years. About 75% of lung cancer cases were attributed in part to tobacco smoking, with a higher estimate of 85% to 90% for the U.S.A. Compared to people who do not smoke, men who smoke are 23 times more likely to develop lung cancer, whereas women are 13 times more likely. Duration of smoking is considered to be the strongest determinant of lung cancer risk in smokers.\nEver since the early 1950’s when the carcinogenic effects of tobacco smoke on the lung were repeatedly shown in scientific studies, the rate of tobacco smoking began to decline sharply after the recognition by public health regulatory authorities in the mid-1960’s. In the U.S.A., the self-reported adult smoking rates peaked in 1954 at 45%, and remained at around 40% or more through the early 1970’s. The average rate of smoking across the decades began falling from 40% in the 1970s to 32% in the 1980s, 26% in the 1990s, and 24% since 2000. In 2008, the percentage of U.S. adults saying they smoked cigarettes was only 21%, a decrease of more than 50% from the peak at 45% in 1954.\nPER CAPITA CONSUMPTION OF CIGARETTES AMONG ADULTS 18 YEARS AND OLDER FROM 1900 – 2004 IN THE U.S.A.\n[SOURCE: THE NATIONAL ACADEMY PRESS, ENDING THE TOBACCO PROBLEM: A BLUEPRINT FOR THE NATION (2007) https://www.nap.edu/read/11795/chapter/4]\nDespite the dramatic reduction of close to 50% in smoking rates between 1975 to 2004 in the U.S.A., the age-adjusted cancer incidence rates for lung cancer actually INCREASED by 6.3 percent from 52.2 per 100,000 in 1975 to 55.5 per 100,000 in 2011.\nEven though the development of lung cancer is attributed mainly to smoking, approximately 25% of lung cancer incidences worldwide are not related to tobacco use. Lung cancers in never-smokers account for over 300,000 deaths worldwide each year.\nStriking differences in the epidemiological, clinical and molecular characteristics of lung cancers arising in never-smokers versus smokers have been identified, suggesting that they are separate entities. Never smokers are people who have smoked less than the equivalent of 100 cigarettes in their entire lifetime. Lung cancer in never-smokers affect females more than males. It is estimated that 53% of all women with lung cancer worldwide compared to 15% of men, are never-smokers. In the U.K. 67% of never-smokers who underwent lung cancer surgery from 2008 to 2014 were females.\nLung Cancer Age‐Adjusted Incidence Rates by Sex, 1975‐ 2011\n[Source: National Cancer Institute. SEER Cancer Statistics Review, 1975‐20011 Credit: American Lung Association Epidemiology and Statistics Unit Research and Program Services Division, Trends in Lung Cancer Morbidity and Mortality November 2014 https://www.lung.org/assets/documents/research/lc-trend-report.pdf]\nDistinct clinical, pathological and biological features of lung cancer in never-smokers are also observed, in that small-cell lung cancer that is highly correlated with smoking is quite rare in never-smokers; and lung cancer in never-smokers is almost exclusively made up of non-small-cell lung cancer, with a predominance of adenocarcinoma over squamous cell carcinoma in the ratio of around 8:1 in Europe. Studies before 1990 also confirmed the observation that adenocarcinoma was most common among lung cancer in never-smokers.\nThe most troubling aspect in the diagnosis and treatment of lung cancer in never-smokers worldwide is that the demographics now include younger patients in their 20s, 30s, and 40s, instead of the traditional mean age of 70 at diagnosis.\nCompared to smokers, the carcinogenesis of lung adenocarcinoma in never-smokers is more complicated. Many factors may cause the development of lung cancer in never-smokers, the major contributors are considered to be:\nEnvironmental Tobacco Smoke (ETS)\nEnvironmental Tobacco Smoke (ETS) is defined as “sidestream smoke from the smoldering tobacco between puffs and exhaled mainstream smoke from the smoker.” The link between ETS and lung cancer was first reported in 1981. In 1992, the Environmental Protection Agency published a review on the effects of ETS on lung cancer and indicated that ETS is associated with increased risk for lung cancer. However, in 2005 a large population study involving over 500,000 volunteers by the European Prospective Investigation into Cancer (EPIC) and Nutrition, an ongoing multi-centre prospective cohort study designed to investigate the relationship between nutrition and cancer, revealed that there is no statistically significant hazard in the development of lung caner from exposure to ETS. The current evidence seems to suggest that ETS plays a modest role in the development of lung cancer in never-smokers.\nRadon is a naturally occurring radioactive gas produced by uranium decay in the earth’s crust. It emits alpha particles, decaying to polonium and bismuth. indoor levels can be quite variable depending on soil composition, building foundations and ventilation. Radon can accumulate to unsafe levels in basements and lower building levels. Radon exposure in underground workplaces is regulated in the U.S.A. Radon exposure is considered to be the second leading cause of lung cancer after tobacco smoke. Radon is not only an independent risk factor; it also increases the risk of lung cancer in smokers.\nOccupational exposure to carcinogens is estimated to account for 5–10% of lung cancers. Of these, asbestos is the most common. The risk for lung cancer from asbestos exposure is dependent on both fiber type and dose, as larger chrysotile fibers can be cleared from the lungs more rapidly than amphibole fibers. Studies that did not detect increased lung cancer mortality from nonoccupational asbestos exposure involved populations that were predominantly exposed to chrysotile fibers. In general, it is believed that nonoccupational exposure to asbestos may not have a significant role in increasing mortality from lung cancer in never-smokers.\nInfection & Inflammation\nDamage to the lungs from inflammation and infection is often implicated in tumorigenesis. Pre-existing lung disease like tuberculosis increases odds ratio up to 1.76 for the development of lung cancer, regardless of smoking status. Chronic inflammatory lung diseases also confer additional risks for cancer development. However, the majority of lung cancer patients who are never-smokers do not have a history of active interstitial lung disease.\nInherited Genetic Susceptibility\nA positive family history of lung cancer has been found to be a risk factor in several registry-based studies that have reported a high familial risk for early-onset lung cancer. Family history of lung cancer is correlated with increased risk in the development of lung cancer in both smokers and never-smokers. For never-smokers, genetic influences would explain the increased risk in the younger age group.\nGenetic Risk Factors\nLung cancer in never-smokers is almost exclusively made up of NSCLC (non-small-cell lung cancer), with a predominance of adenocarcinoma. NSCLC patients often show metastasis to major organs such as liver (33–40%), brain (15–43%), kidney (16–23%), adrenal glands (18–38%), bone (19–33%), and abdominal lymph nodes (29%). The difficulty in treating primary tumors and related metastatic secondary mutations accounts for the poor prognosis of NSCLC. The 5-year survival rate for advanced NSCLC patients remain stubbornly below 5%.\nScientists now believe that genes may be involved in the increased susceptibility to adenocarcinoma in young never-smokers, especially in the gender disparity as the incidence of lung cancer in never-smokers worldwide indicate a female predominance. 53% of females who develop lung cancer are never-smokers while only 15% of male lung cancer patients never smoked, and the incidence of lung cancer in women can vary by 30-fold even in countries reported to have low prevalence of female smoking.\nThe higher incidence of lung cancer in female never-smokers has led scientists to explore gender-dependent hormones in the development of lung cancer. It has been observed that women who received anti-estrogen therapies showed a reduction in lung cancer incidence while women placed on hormonal replacement therapy showed increased risks for NSCLC.\nAlthough lung cancer in never-smokers share molecular features that are typical in tobacco-related carcinogenesis, the presence of unique genetic and epigenetic markers indicate that a different but possibly overlapping carcinogenic pathway is responsible for the development of lung cancer in never-smokers.\nSpecific genetic mutations have been associated with a higher prevalence of adenocarcinomas in never-smokers. Those genes include EGFR, PTEN, ALK, ROS1, and RET; whereas a different set of genetic mutations in smokers have been identified. These genes are K-RAS, TP53, BRAF, STK11, and JAK2/3. Mutations and hypermethylation of p16 and LGALS4 are also implicated in progression of lung cancer in smokers.\nDifferent Frequencies of Oncogenic Drivers in Never-smoking vs. Smoking Non-Small Cell Lung Cancer\n[Shin Saito, Fernando Espinoza-Mercado, Hui Liu, Naohiro Sata, Xiaojiang Cui, and Harmik J. Soukiasian, Current status of research and treatment for non-small cell lung cancer in never-smoking females, Cancer Biol Ther. 2017; 18(6): 359–368. doi: 10.1080/15384047.2017.1323580]\nAs a result of dedicated efforts committed to the control of tobacco use, smoking prevalence and lung cancer mortality have decreased over the past several decades in the USA. However, tobacco smoking remains the major cause for lung cancer, whereas different carcinogens as well as genetic influences could be involved for different groups of never-smokers. With ever increasing understanding of the mechanisms behind the pathogenesis of lung cancer for smokers and never-smokers, it is anticipated that successful treatment can be tailored for the individual patient based on the presence or absence of critical molecular alterations.']"	['<urn:uuid:86f445a4-765f-4123-8693-4c4d490bac9f>', '<urn:uuid:b7ac78f4-395d-4516-b558-e7a11828430d>']	factoid	direct	long-search-query	similar-to-document	three-doc	expert	2025-05-12T11:40:26.317848	13	55	2494
42	modern led lights garage insect effects	LED lights offer advantages for garage lighting, including energy savings up to 70%, fast start-up time, and long life up to 30,000 hours. However, LED lights are causing significant environmental harm by reducing moth caterpillar populations by up to 52% in illuminated areas, which is even more detrimental than traditional sodium lights. This affects the entire ecosystem since moths are important pollinators and food sources for birds and bats.	['Realizing garage and workshop lighting in an energy-saving and cost-effective way is a difficult but not insurmountable task. Many people talk about lumens, lux, candela, efficiency, beam angle, luminous depth and illuminance. But what is really behind these terms and what do you need to equip a workshop / warehouse with good lighting?\nWe have often written about the best lighting of living rooms, kitchens and bedrooms on our blog. Unfortunately, the garage rarely gets such attention. However, this room also deserves a bit more analysis, as poor garage lighting can be frustrating.\nWhat do you need for good garage, workshop, warehouse lighting?\nWhen it comes to the illumination of a garage, workshop or an adjacent warehouse, the question always arises: “What is being done in the room?” Thus, it does not make sense to install lighting with dimmed light in a small parts warehouse, as this makes it more difficult for employees to search for small parts and thus wastes manpower and time. It also makes no sense to install daylight lighting in a warehouse where pallets are only moved intermittently.\nAs much as necessary, as little as possible\nWhen it comes to lighting, you should always save on the right places in order to get the most out of the available space. Furthermore, attention should be paid to the illuminance that can be achieved with different light sources and luminaires in a workshop.\nWhere should you install garage lights?\nIn a garage, we usually need two types of lighting:\n- General lighting\n- Working lighting\nGeneral lighting consists of a handful of fixtures that illuminate the entire garage. Ceiling fixtures with diffused light are perfect for this application. It would also be advisable to get two points of light. This way, the front and the back will be evenly lit. Also, pay attention to the garage door. If it is open, it will block the ceiling light above.\nIn some garages, these lights are supplemented by task lighting. For example, let’s say that you repair electrical appliances as a second job. Then you need a workbench in the garage. To make this precise work easier, it would be advisable to get work lighting above the workbench. This could be, for example, ceiling lights or office lamps.\nWhat light intensity do you need in a garage?\nThe minimum luminous intensity in a garage should be is 300 lux at a height of 0.85 meters / 2.78 feet.\nAttention: this is a minimum value. In some areas you can easily require a higher light intensity. For example over the above mentioned workbench 500 lux. is not superfluous. The more precise the work becomes, the more light you need.\nHow bright should a garage/workshop be illuminated?\nLight in a garage, workshop or warehouse is always a “must have”. The installation takes place shortly before completion and many save on proper lighting, because the budget was possibly already exceeded and there are also apparently cheaper variants.\nBut you should not try to save money on lighting. It makes more sense here to invest capital in efficient lighting and to reconsider the given budget. Money invested in the right lighting will come back to your own wallet or that of the workshop over the years.\nProper Garage Lighting is an Investment in the Future\nAs we have already seen above, the choice of the right illuminant is decisive for the illuminance achieved from the same electrical power. This is exactly the point of departure. It is not a good idea to go to the nearest hardware store, buy a damp-proof diffuser luminaire that happens to be cheap at the moment, and screw it to the ceiling without a care in the world. Even the best luminaire will produce poor results if used incorrectly.\nWhat color of light do you need in a garage?\nApart from the light intensity, we also need to consider the light color. Since the garage is a functional space, we choose a cool white light. Then these are lamps whose color temperature is about 4000 K.\nWhat lamps are best to use in a garage?\nUntil now, fluorescent tubes were the first choice for garage lighting. But times are changing. Nowadays, the so-called LED tubes are becoming more and more popular because they offer a few very interesting advantages:\n- Fast start-up time\nIt always takes a little while before fluorescent tubes reach their full light intensity. This disadvantage does not exist with LED tubes. The fast start time is very convenient in a garage, where you often spend only a few moments.\n- Energy saving\nDo you spend long hours in the garage? Maybe because of a hobby? Then you will definitely be pleased to hear that LED lamps are much more energy efficient than fluorescent tubes. There is no doubt that this lower energy consumption is very interesting.\n- Long life\nMany people simply do not have time to replace broken lamps in their garage. Then they just go on without lighting. Fortunately, LED lamps have a much longer life. Some models can even burn up to 30,000 hours.\nThe Perfect Wiring and Sockets for the Garage\nMost garages are not plastered on the inside. Built-up wiring in this case is nicer than the built-in version.\nTherefore, use thin PCV tube, so that the flexible cables can still be pulled a little. With the help of the pipe clamps you can fix them against the wall.\nAlso get enough sockets in the garage. You never know when they might come in handy: for the compressor so you can pump up the bike tires, for recharging a screwdriver on batteries or for your electric bike.\nSummary – The most important points at a glance:\n- LED lighting saves energy and costs, has the longest service life and is available with protective devices.\n- Provide bright ambient lighting (min. 500 lx)\n- Illuminate work surfaces separately and with high luminosity\n- Ensure cool color temperature (4,000 K or 6,500 K), low glare and high color rendering\n- Ideal for basements, attics or garages: LED damp-proof luminaires or LEDs with IP65 level (protected against dust and water)\nWhy are LEDs suitable for garage lighting?\nLuminaires with LED technology offer many advantages over conventional light sources such as fluorescent tubes or halogen lamps. They consume up to 70% less energy and thus save on electricity costs. In addition, their longer service life eliminates maintenance and replacement costs. Unlike halogens, whose reflector gets very warm, you cannot burn yourself on LEDs. So you won’t be disturbed by accidental, painful contact while working in the garage. LED lighting is also flicker-free, which makes it much easier to use than fluorescent tubes and does not cause secondary symptoms such as eye irritation or migraines.\nLED damp-proof lights, which you can mount on the ceiling of your garage, also offer good protection against moisture. If there’s ever a little jolting while you’re working, you don’t have to worry: LEDs are insensitive to vibrations. If you have concerns about mounting them on high ceilings (for example, in the attic), LED tubes with shatter protection or a cover made of polycarbonate are ideal for your lighting design. This is a particularly shatterproof plastic that gives your luminaire added robustness.\nFor general well-being and health, LED lights are thus highly recommended – so, replace your old bulbs in your garage today!', 'Eco-friendly light-emitting diode (LED) bulbs used in streetlamps produce more light pollution and are killing off insects, a new study shows.\nResearchers in England found LED streetlights kill off nocturnal moth caterpillar populations by 50 per cent, compared to areas without the lights.\nDespite being marketed as environmentally friendly, LED lights are even more harmful for insect populations than the traditional yellow sodium bulbs they are replacing.\nThey’re often energy-efficient, cheap to run and brighter with more blue wavelengths, but this means they’re also likely making insects more exposed to predators.\nThe insects could also be getting killed from coming into contact with hot lights, which may also deter them from laying eggs.\nMoths are important because they’re pollinators, hosts for parasitoids and prey for vertebrates like birds and bats and invertebrates like spiders, the experts say.\nLED streetlights in Curbridge, near Witney. This roundabout on the A40 was only build in the last few years and is now packed with LED lights that are affecting insect populations\nTHREATS TO MOTHS\n– Habitat change and destruction\n– Agricultural intensification\n– Light pollution\nThe study has been conducted by scientists and conservationists at Newcastle University, the UK Centre for Ecology & Hydrology (UKCEH) and UK charity Butterfly Conservation.\nDouglas Boyes of UKCEH, who led the study, spent more than 400 hours sampling for caterpillars along roadsides at a total of 55 lit and unlit sites in the Thames Valley over the past three years.\n‘The effects observed – on local abundance, development and feeding behaviour – were more pronounced for white LEDs compared to traditional yellow sodium lamps,’ he said.\n‘The rapidly increasing prevalence of LED lights, which are often much brighter as they are so energy-efficient and cheap to run, is likely to increase the negative impacts of light pollution on insects.\n‘This is expected to have knock-on effects on other species, including predatory insects, hedgehogs, and songbirds, which need to find hundreds of caterpillars a day to feed themselves and their young.’\nBoyes told MailOnline that minor tweaks that could be made to LEDs that could minimise their negative impacts on insects without comprising on energy efficiency, such as reducing emission of blue wavelengths.\nThe negative impacts of light pollution on insects including moths – which provide essential food sources for a variety of animals and are important pollinators – are well known.\nA selection of caterpillars from a sweep net sample for the study. Caterpillar abundance was substantially lower in habitat areas illuminated by streetlights\nPictured, Saturnia pavonia, the small emperor moth. Moths are important because they’re pollinators, hosts for parasitoids and prey for both vertebrates and invertebrates\nMoths and other insects are attracted to light, which could result in higher mortality due to direct contact with hot lights and increased predation, according to Butterfly Conservation.\nFor this new study, researchers evaluated the impacts of nighttime lighting on wild caterpillars in southern England – in Oxfordshire, Buckinghamshire and Berkshire.\nThey compared habitats directly lit by the LED streetlights with carefully matched unlit habitat located under 200 feet away.\nBoyes and his team found the abundance of moth caterpillars in hedgerows under LED street lamps was 52 per cent lower than in nearby unlit areas.\nThis compared with a 41 per cent lower abundance in hedgerows lit by the less environmentally-friendly sodium lighting.\nMeanwhile, in grass margins, the moth caterpillar numbers near LEDs were 33 per cent lower than in unlit areas, whereas sodium lights had little effect on abundance in this habitat.\nThis difference may be due to female moths laying fewer eggs in habitats illuminated by streetlights, but conservationists say further research is needed to confirm this.\nSodium streetlights in Upper Heyford, north Oxfordshire. Negative impacts on caterpillars were more pronounced under white light-emitting diode (LED) street lights compared to these conventional yellow sodium lamps\nAlmost all previous research on light pollution has focused on adult insects, but by studying caterpillars, which are a lot less mobile, the experts get more precise estimates of the impacts of street lighting on local populations.\nWhat’s more, the large diversity of moths means they are broadly representative of nocturnal insects, so any negative impacts from a threat likely to also be experienced by other species.\nSadly, the total abundance of moths in Britain has decreased by one third over the past 50 years.\n‘Light pollution is one of the few causes of biodiversity loss that has easy solutions,’ said co-author Professor Darren Evans of Newcastle University.\n‘Ultimately, we need a balance between protecting both public safety and wildlife, by ensuring that lighting is well-designed, away from important habitats and switched on for limited times.’\nThe study has been published in the journal Science Advances.\nLIGHT POLLUTION IS ARTIFICIAL LIGHT THAT IS EXCESSIVE, OBTRUSIVE AND WASTEFUL\nLight pollution, also known as photopollution, is the presence of anthropogenic light in the night environment.\nArtificial light that’s excessive, obtrusive and ultimately wasteful is called light pollution, and it directly influences how bright our night skies appear.\nWith more than nine million streetlamps and 27 million offices, factories, warehouses and homes in the UK, the quantity of light we cast into the sky is vast.\nWhile some light escapes into space, the rest is scattered by molecules in the atmosphere making it difficult to see the stars against the night sky. What you see instead is ‘Skyglow’.\nThe increasing number of people living on earth and the corresponding increase in inappropriate and unshielded outdoor lighting has resulted in light pollution—a brightening night sky that has obliterated the stars for much of the world’s population.\nMost people must travel far from home, away from the glow of artificial lighting, to experience the awe-inspiring expanse of the Milky Way as our ancestors once knew it.\nLight pollution is excessive and inappropriate artificial light. While some light escapes into space, the rest is scattered by molecules in the atmosphere making it difficult to see the stars against the night sky. What you see instead is ‘Skyglow’\nThe negative effects of the loss of this inspirational natural resource might seem intangible.\nBut a growing body of evidence links the brightening night sky directly to measurable negative impacts on human health and immune function, on adverse behavioural changes in insect and animal populations, and on a decrease of both ambient quality and safety in our nighttime environment.\nAstronomers were among the first to record the negative impacts of wasted lighting on scientific research, but for all of us, the adverse economic and environmental impacts of wasted energy are apparent in everything from the monthly electric bill to global warming.']	['<urn:uuid:d448d7b8-aed9-428b-87be-9a1e375b4e27>', '<urn:uuid:56b7440b-9e90-4ddc-b9e8-9ace514f84a1>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T11:40:26.317848	6	69	2333
43	What's the difference between qatala and qattala?	Qatala means 'he killed' while qattala means 'he killed many' or 'massacred'. Qattala gives an extreme meaning, indicating excessive or multiple killings.	['بسم الله الرحمن الرحيم\nOur teacher began with praising Allaah and sending salaat and salaam on the Messenger of Allaah, his house of kin, his companions and followers until the Day of Judgment, then began:\nThe scale consists of 3 letters that were chosen because it is the least amount you can find a verb on and the least letters there are the more a word is used because it is lighter on the tongue. Whatever is lighter on the tongue is used more in the Arabic language because the Arabs seek lightness in speech.\nWhy ف – ع- ل ? Because fa`ala (he did) is an alternative to any other verb.\nWhy Verbs since this topic covers both asmaa’ and af`aal (nouns and verbs)? Because af`aal are the types of words that are subjected to change the most. Originally, the verbs are the types of words that change into different forms and not the noun. The default state of the noun is that it doesn’t change and the default state of the verb is that it changes into different forms. The focus in the first levels of sarf is on the verbs because once the beginner masters the verb and how it changes, this will form a sound foundation for changing anything else into different forms.\nWhat is the point of weighing the word? To know the form of the word, because when you study a certain word you take it back to its root letters to know its meaning. Some words share the same root letters but change in order. Once you start know how to weigh words correctly and you start to master that, you will know how to identify the source of this word. At least (if you are not able to) you will get to know the source of a word by reading the explanations of the scholars which they have compiled. The scholars have left nothing unexplained- all the verses, ahadeeth, everything is explained. The scholars analyze the texts. This analysis isn’t a waste of time – it is to clarify the meaning.\nالمَعنَى فَرعٌ عَن مَعرِفَةِ الإِعرَاب – (Alma’naa far’un ‘an ma’rifatil i’raab) the meaning branches out from the grammatical analysis.\nYou want to know the meaning of a sentence so you get to the meaning by analyzing the sentence grammatically – knowing what word serves what purpose – you come to know it’s meaning in the sentence and what it means by itself, despite the sentence it is in. So the meaning of the word – the particular form that is used and the role it plays in the sentence – both of these will qualify for you the exact meaning. You will be able to do this by yourself.\nWhen we look at a sentence we look at the words individually and analyze their meanings – this is what is done in sarf. You know what meaning each word gives. Then when you know that, you look at the words in the whole sentence – the complex aspect of it – and this is where Nahw (grammar) comes in – the relationship between the words.\nKnowing what meaning the word gives plus knowing the relationship between the words and the role each word plays in a sentence – this will give you the meaning.\nWhy do they go through the thorough breakdown of the sentence to get to the meaning? To verify the meaning with proofs from the Arabic tongue, an interpretation based upon the principles and rulings. The student of knowledge wants to build himself scientifically so that in the future he will have attained the tools to get to the meanings himself. This analysis is appreciated by the student of knowledge because it grounds within him the tools for knowing Arabic, get deep into the meanings and being able to analyze the meanings.\n*The cause of this analysis (morphological analysis- weighing the words )- is getting to know the forms of the words and the order it followed to be able to target and pinpoint the original meaning the word was taken from.*\nTHE SCALE >>>\nOn the faa’ of the scale we place the first of huruufun-asliyah (the first original letter)\nThe letters that are not original are called huruufun-zaa’idah\nThe verb that is all original letters is called fi`lun mujarrad\nThe verb that has extra letters is fi`lun mazeed. The f`il which is mazeed has extras letters. And those extra letters are of three types.\n1. An extra letter which is due to doubling a letter in the word weighed\nقَتَلَ (Qatala) in the scale: qaaf is faaul kalimah taa is aynul kalimah and laam is laamul kalimah.\nقَتَّلَ (Qattala) – the taa is doubled – and because it is merged in pronunciation they also merged them together in writing. So they wrote it is one letter with a little “w” above the letter. The little “w” is called شَدَّة (shaddah). When you see a shaddah above the letter it tells you that the letter has been doubled and the first one is saakin. فَتَّلَ (Qattala) means “he killed alot” i.e.”massacred” The shaddah means two identical letters: the first letter is silent and the second letter is mutaharrik meaning it has movement i.e. it makes vowel sound. The extra taa’ is not an original letter – it is an extra letter.\nشدّ Shadd (without the taa marbootah) means: pulled, fastened, strained. And a single straining is called shaddah (with the taa added) -a stress.\nSince aynul kalimah was doubled we double the ayn in the scale. We add another ayn just as we have added another taa.\nThe taa has sukoon so the ayn that we added to scale will get sukoon\nقتّل على وزن فعّل Qattala `alaa wazni fa“ala that is: “qattala” is on the scale of “fa“ala”\nWhat is the difference between “qatala” and “qattala”? “Qattala” gives an extreme meaning. “Qatala” means: “he killed”. “Qattala” means: “he killed many , killed a lot, excessively killed”, “massacred”.\nEXAMPLE from Qur’aan: Allaah says about the son of Aadam (Habil) in Al-Maidah 5/30:\nفَطَوَّعَتۡ لَهُ ۥ نَفۡسُهُ ۥ قَتۡلَ أَخِيهِ فَقَتَلَهُ ۥ فَأَصۡبَحَ مِنَ ٱلۡخَـٰسِرِينَ\nAnd Allaah says about Fir`awn in Al-Araaf 7/127:\n…قَالَ سَنُقَتِّلُ أَبۡنَآءَهُمۡ…\nNotes transcribed by: Umm Omar Al-Amreekiyyah\nFROM THE QUESTIONS >>>\nCertain forms give certain meanings. The form “fa“ala” has different meanings: severeness, multiplicity, making verb transitive. We will come to know the meanings of the forms – with examples from the Quraan\nقَاتَلَ Qaatala is another form of qatala (on the scaleof faa`ala) it has another meaning – fought against.\nAynul kalimah is not the only letter that is doubled\nAge is not a factor when it comes to gaining knowledge\nShaddun is the act of straining and shaddah is a single straining']	['<urn:uuid:3ef4d412-b0d6-4b1b-bbc9-6b43ec402a94>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T11:40:26.317848	7	22	1131
44	What environmental controls exist in open-water hunting and vertical farming?	In open-water mallard hunting, environmental controls focus on adapting to natural conditions - hunters must manage wind direction for decoy spreads, handle rough water conditions, and ensure proper blind placement to remain hidden in featureless waters. In contrast, vertical farming provides complete control over environmental factors, including light, temperature, humidity, carbon dioxide concentration, water, and nutrients. This controlled environment allows for year-round crop production independent of external conditions and prevents pests and diseases through enclosed designs and non-chemical disinfection methods.	"['Top Tactics For Open Water Mallard Hunting\nFebruary 22, 2011\nHelping your retriever handle those ""mega marks.""\nMason, the author\'s Labrador, is a seasoned open-water hunter with hundreds of mallards to his credit.\nMallards are among the most adaptable of all waterfowl. This grassland-nesting species is commonly found in city parks and adapts quickly to almost any food source or habitat niche. In areas where hunting pressure is heavy, mallards are increasingly forced to seek open water for roosting instead of their traditional marsh habitat.\nA mallard in open water is a duck of a different nature. Hunting mallards in open water calls for tactics more often associated with diver ducks than puddle ducks. Getting the birds in close enough for a clean kill is doubly tough, and often retrievers are faced with ""mega marks"" on birds that fall a football-field-length or more away from the blind. No bigger challenge awaits any duck dog than chasing down a wounded mallard in open water.\nSetting the Plate\nThe term ""open water"" means a lot of different things to different waterfowl hunters. To a dyed-in-the-wool diver enthusiast, open water typically means hunting many miles from shore in deep and often rough water.\nFor the modern mallard hunter, open water can take on a number of different faces. Often, these birds prefer to roost in inland marshes that have huge pockets of open water in the middle. It\'s here the birds can rest and be safe from hunters who tend to target the cattail and bulrush edges.\nRead Go West For Tennessee Mallards\nIn the Great Lakes region and western reservoirs, open water is most frequently described as the wetland habitat adjacent to shoreline marshes. At places like Saginaw Bay, Lake St. Clair and Lake Erie in the Great Lakes, and lakes Oahe, Sakakawea and Fort Peck along the Missouri River, hunters find an abundance of shoreline marshes and adjacent open waters that stretch as far as the eye can see.\nJake, the author\'s youngest son, proudly shows off an open-water mallard.\nIt\'s in these featureless open waters that mallards find refuge from hunting pressure. Those who target these birds in open water are writing a new chapter in the puddle duck hunting manual.\nHiding in Plain Sight\nThe open-water mallard hunter is faced with the challenge of hiding in plain sight. Often these hunters do not have the luxury of slipping into a conveniently located patch of cattails or bulrush, and just as often their stake blind is positioned, or their boat/blind is anchored to the bottom, in the middle of nowhere. With nothing but water in every direction, the question becomes: How can mallards be lured in close enough for quality shooting?\nCommon sense suggests that mallards would avoid anything on the water, but in practice this is not the case. Mallards will decoy to obvious blinds, so long as the hunters and dogs inside are absolutely motionless. Overhead cover is a critical mallard hunting tactic, as incoming mallards are certain to be high enough to easily look down and survey the situation.\nSetting up the decoy spread so incoming birds can land into the wind but crossing out in front of the blind is critical. It\'s very difficult to convince mallards to land looking directly at the blind.\nNot unlike diver hunting, a crossing wind is the perfect setup for luring open-water mallards into gun range. This way, as mallards are pitching into the decoy spread, their attention is focused into open water and away from the nearby blind.\nSeparating the blind from the decoys also helps give incoming birds a sense of security. At a minimum, there should be 10 yards of open water space between the blind and the first decoys while hunting open water mallards. Setting most of the decoys upwind of the blind also encourages birds to sit down at the back end of the spread and right in front of the blind.\nA number of commercial flip-top style blinds are well suited for converting just about any small boat into a mallard-hunting machine. Most of these blinds are built using a frame of aluminum or steel tubing with various kinds of natural or synthetic covering materials attached. The critical aspect is to ensure birds approaching from above can\'t look down and spot hunters or dogs inside the boat.\nThe creative among us tend to build our own open-water blinds and boat/blinds, using a wealth of options. The ideal boat/blind takes into consideration the need to conceal hunters and dogs, provide comfort for long hours on the water and meet the special needs of retrievers that must have clear vision outside the blind to effectively mark down fallen birds.\nSince retrieves are routinely long, a dog\'s best hope of marking down birds is to have the same sight picture as the hunter.\nThe very nature of hunting mallards in open water calls for longer shooting than is common in marsh hunting. In addition, a crippled bird in open water is far more difficult to retrieve than one wounded in a marsh environmen\nTungsten, bismuth and other premium, high-density waterfowl loads are worth every penny when hunting tough mallards in open water. These loads tend to kill ducks cleanly on average 10-20 yards farther than steel shot, and much farther in some cases.\nHard shot types, including Hevi-Shot, Remington Wingmaster and Winchester Extended Range Waterfowl, tend to pattern very tightly out to 40 yards and beyond in modest chokes such as improved cylinder. Softer shot types, including bismuth and Kent Impact loads, pattern best in a modified or improved modified choke when shooting out to 40 yards and beyond.\nMore often than not, a crippled duck shot with steel shot will require several more shots on the water to dispatch the mallard. Using high-density premium waterfowl loads eliminates most crippling issues while hunting open water mallards. Also, should a bird be crippled with a premium load, chances are a single follow-up shot on the water will end the drama. In the end, shooting premium loads saves money and the anxiety of losing crippled birds.\nCreating a small hole so the retriever has a clear field of view to falling birds is a must for open-water hunting.\nAn Eye-Up For Dogs\nNo matter how good a hunting dog might be at retrieving, a marked retrieve is easier to accomplish than a blind retrieve while hunting open water mallards. Make every effort to set up your blind or boat so your retriever can watch as the birds approach and mark down birds easily.\nA small slit or hole in the blind covering facing the direction of the decoys is all it takes to put your dog in the rock-and-roll position. Some hunters provide their dog a peephole to watch from and a different exit for retrieving. So long as the dog has the same field of view to falling birds as the hunters, the task of picking up dead and crippled birds will be easier.\nMany waterfowl hunters employ neoprene vests for their dogs to protect them from cold water and harsh hunting conditions. As useful as these vests are in keeping a dog warm, they are even more useful in terms of the buoyancy they provide. Buoyant vests allow dogs to ride higher in the water, helping them swim faster and, more importantly, allowing them to have a better view of the water in front of them.\nSome vests even come with extra buoyancy, and for the dog hunting in open water mallard environments, this feature is a huge advantage. When hunting in heavy seas and waves, the buoyancy issue can be a matter of life and death for the retriever.\nWithout a buoyant vest, a hunting dog in open water is risking drowning with every retrieve. This goes double for those strong-willed dogs that refuse to quit when chasing a cripple.\nNeoprene vests are a huge asset for the open-water retrieving dog. Some of these vests are manufactured with extra foam filling to help the dog ride higher in the water and maintain a better view of downed birds. Also, extra flotation provides more safety and enables the dog to swim effectively even in rough water.\nPractice and Conditioning\nOpen-water hunting and long retrieves go hand in hand. The best way to ensure a retrieving dog finds success in this environment is to ascertain the dog is well conditioned physically. Swimming is the perfect conditioning tool, and lots of time in the water also allows the dog to become comfortable making longer-than-normal retrieves.\nREAD: Tips For Keeping Your Hunting Dog In Shape\nOnly after a dog is in good physical condition can a hunter attempt to train for mega marks that often require 200-yard or longer retrieves. A 100-yard mark is on the short side for an open water hunt. Even a bird knocked down at the edge of the decoys may sail 40 yards before hitting the water. By the time the dog hits the water and approaches the bird, a simple retrieve can easily turn into a mega mark.\nPractice doing 100- to 200-yard marks in the off season until the dog can both mark and fetch effectively birds from these distances. Start short and increase the distance until it\'s obvious the dog can handle marks twice as long as the typical hunter routinely sees.\nWhen possible, practice in wavy conditions so the dog gets used to the frustrations of swimming in rough water. Building some confidence in the off season helps the dog push forward in the tough conditions that will no doubt face him in the fall.\nOpen water and mega marks mean that the dog is likely to end up far enough away that the sound of your voice will not be adequate for shouting commands. A training whistle is the only practical way to train dogs for making long-distance fetches and handling requirements.\nMost trainers only use two commands from a training whistle. One hard and short blast means ""Stop and look my direction."" Once the dog stops and turns around, it\'s focused on his handler. At this point, it\'s possible to redirect the dog with a hand signal, potentially saving lots of needless swimming and giving the dog every chance at making a successful retrieve.\nREAD: Whistle Training\nThe second whistle command is a rapid series of short blasts that mean ""here"" or ""come."" This useful command can be used to call a dog off a crippled bird it obviously can\'t catch. The ""come"" whistle also allows the handler to pull the dog back toward the blind should the dog overshoot the bird. This happens often with crippled birds that dive and then pop up between the dog and the blind.\nGo Get Him\nIn open water, a retrieving dog is going to experience fetches that are next to impossible. If the dog ends up far from the boat or blind and confused, don\'t hesitate to get the dog as quickly as possible. Any time the dog gets more than 200 yards away from the blind, it\'s time to crank up the outboard and head out to pick up the dog.\nNo bird is worth risking a dog\'s life. Long retrieves are part of the equation in open-water hunting, but you should be able to distinguish between a tough retrieve and an impossible one.\nSumming It Up\nThe pursuit of mallards in open water requires unique hunting skills and challenges for man and dog alike. Targeting birds outside the normal hides found in traditional marshes means the average bird is going to be harvested 10 to 20 yards farther than when marsh hunting. That means dogs will see some long marks and even longer retrieves.\nEnjoying a successful mallard hunt in open water is about hiding in plain sight, keeping the birds\' focus off the blind and making sure the dog has a clear field of view to every downed bird. Cover those bases and the science of open-water mallard hunting is an open book.', 'Agricultural systems around the world need to adapt to the rapidly changing environmental, demographic, and socioeconomic landscapes, and new alternative practices, such as vertical agriculture, may offer new opportunities to accelerate such adaptation.\nNext Gen Farming Without Soil and 90% Less Water | GRATEFUL\nWhat is vertical farming and why is it important\nModern agricultural systems encompass an estimated 1.5 billion hectares of the world’s surface area. With a growing population and resource needs, the availability of arable land is shrinking rapidly.\nSince the agricultural revolution, conventional agriculture has focused on practices requiring considerable quantities of space, water, fertilizer, and pesticides. The past 50 years have seen an accelerating rate of increase in these requirements as modern food production aims to increase productivity in the hopes of addressing growing food insecurity.\nLooking into the future, yield production is forecasted to decrease due to widespread environmental and socioeconomic changes that will generate unpredictable consequences on food systems.\nIn response, many strategies have been developed as alternatives to conventional agricultural practices. These strategies have focused on key principles and their combination to be effective: require less space, less water, and increase yield per unit of area. Moreover, due to the negative effects of agrichemicals, modern practices have also aimed to use significantly less to avoid potentially adverse effects for humans and animals.\nOne such alternative is the development of vertical agriculture, also referred to as vertical farming. As the name implies, vertical agriculture relies on expanding production vertically and not horizontally. Vertical agriculture is a multilayer indoor plant production system that allows for precise control of growth factors, such as light, temperature, humidity, carbon dioxide concentration, water, and nutrients.\nThis allows for the growing and production of crops year-round, completely independent of solar light and other external conditions. Indeed, vertical agriculture makes use of key concepts within ecology and physiology to optimize growing and fertilization within controlled conditions. For instance, elements of photobiology, thermomorphogenesis, hydroponics, and genetic breeding, are all used commonly across systems of vertical agriculture.\nBenefits, challenges, and disadvantages moving from horizontal to vertical farming\nAs a result of tight control over crop breeding, growing, and harvesting, vertical farming provides several benefits relative to conventional methods of ‘horizontal’ food production. This was the topic of a literature review by Kalantari et al. published in 2016 in the Journal of Landscape Ecology.\nFrom a systems perspective, the enclosed design prevents pests and diseases from entering by the adoption of a high level of hygiene, continuous monitoring, and non-chemical disinfection, providing security from crops. Moreover, recent technology has also allowed for automated control over environmental conditions by using sensors and imaging techniques in combination with crop simulation models and artificial intelligence, limiting the need for physical labor.\nVertical farming also allows for flexible organization, with designs ranging from large vertical walls covered with crops to large hangars or re-used shipping containers that can be transported. Consequently, vertical agricultural systems, can comprise many varying sizes and be located within many different areas from the middle of highly urbanized cities to more suburban or rural areas.\nMoreover, the verticality element of this system also provides nutrient and water flow, helping to reuse costly resources. The reduction in space also means there is a significant increase in yield per area, holding extensive potential for a future world of urbanization.\nFrom an economic perspective, vertical farming also provides for more jobs in localized areas and is community-focused by addressing the needs of immediate areas, which in turn can provide food at a lower price. Finally, the optionality of location for vertical systems also allows producers to reduce transport costs, as consumers may access them within urban areas, or transport can be minimized to nearby areas.\nHowever, despite the design, environmental, and economic advantages, vertical farming also incorporates several issues that remain a challenge to its broader implementation as a system.\nVertical farming has a high energy requirement and needs extensive investment costs to implement and develop successfully. Moreover, indoor issues relating to excessive UV, heat, and ozone-induced plant damage may have unpredictable repercussions for plant growth.\nAdditionally, vertical systems are difficult to adapt to a larger scale. They are costly to build and maintain and have yet to demonstrate the ability to provide food for larger areas than community-scale populations. This would make it difficult to implement in areas at higher risk of food insecurity, such as developing agricultural nations.\nThe lack of empirical research on a broader scale has meant that vertical farming has yet to develop past the concept stage on community levels, as persistent issues make it difficult to break through to a larger scale.\nImage Credit: YEINISM/Shutterstock.com\nGrowing skywards - the implications of vertical farming in a rapidly populating and changing world\nAmong the development of alternative agricultural practices, vertical agriculture provides a promising solution for many of the challenges facing current agricultural policies. However, for vertical systems to be integrated on a larger scale requires further technological progress and economic investment.\nNevertheless, gradually implementing more verticality, or combining it with other practices aiming for more sustainable practices may be promising. For instance, the combination of verticality with other practices such as intercropping may be particularly beneficial for developing more sustainable food systems.\nIncorporating technological progress into vertical systems also holds promise, with automated sensors and machinery able to operate near-independently. Progress in gene editing and plant genome modifications will also allow for faster, bigger, and healthier crops, allowing vertical agriculture to produce more over time.\nThroughout agricultural history, farming systems have typically spread over large spans of land, yet the reduction in arable land, as well as the increase in demand to house growing populations, means that such strategies need to be reconsidered, and vertical agriculture may play a role looking into the future.\nContinue Reading: Benefits of Vertical Agriculture and Hydroponics\n- Beacham, A. M., Vickers, L. H., & Monaghan, J. M. (2019). Vertical farming: a summary of approaches to growing skywards. The Journal of Horticultural Science and Biotechnology, 94(3), 277–283. doi: 10.1080/14620316.2019.1574214\n- Chaudhry A. R. and Mishra V. P.,(2019) A Comparative Analysis of Vertical Agriculture Systems in Residential Apartments, Advances in Science and Engineering Technology International Conferences (ASET), 2019, pp. 1-5, doi: 10.1109/ICASET.2019.8714358\n- Sarkar, A., & Majumder, M. (2015). Opportunities and Challenges in Sustainability of Vertical Eco-Farming: A Review. Journal of Advanced Agricultural Technologies, 2(2). doi: 10.12720/joaat.2.2.98-105\n- SharathKumar, M., Heuvelink, E., & Marcelis, L. F. (2020). Vertical Farming: Moving from Genetic to Environmental Modification. Trends in Plant Science, 25(8), 724–727. doi: 10.1016/j.tplants.2020.05.012']"	['<urn:uuid:5711659d-1067-40f8-9e50-a3eb75f521eb>', '<urn:uuid:3566e03d-b8ac-4d36-b13c-9d56f4d7ba0e>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T11:40:26.317848	10	80	3091
45	What are the specific challenges and solutions for bonding pressure-sensitive adhesives to molded plastic parts that have been treated with processing additives?	The main challenge when bonding pressure-sensitive adhesives to molded plastic parts is that processing additives, used as lubricants during the molding process, can interfere with adhesion. These additives not only exist on the surface but are also locked inside the plastic and can 'bloom' out when the plastic warms during service. To address this, surfaces must be properly prepared. Solutions include cleaning with acetone to remove invisible contaminating chemicals, using specialized pressure-sensitive adhesives designed for low surface energy plastics, and employing surface preparation techniques like flame treatment or corona etching to create more polar bonding sites. Additionally, ensuring proper wetting is critical - the adhesive must form a thin film over the surface area without air pockets underneath.	"['January 2009 Articles & Features\nBy Mike Dale | January 2009\nIf you want to succeed in the gluing business, you\'ve got to know what you\'re gluing, then create the circumstances that will give the correct glue the best opportunity to do its job.\nIn the making of electronic assemblies - and complete vehicles, for that matter - glues, adhesives and interface materials are widely used. In the subsequent repair of these assemblies, glue often is considered the best method to fix something rather than screwing on yet another expensive assembly.\nWhat glues to use and where, and how to employ them in ways that will provide the best results? It all starts with knowing what you\'re trying to glue together and then preparing the materials for that bonding process.\nThere are three basic types of glue adhesion processes. The simplest type is the solvent type of glue. During the semiliquid state of this type of glue, the two parts can be pushed together. As the solvent from the glue dries, the material solidifies and the two pieces are bonded together.\nA second type of bonding involves the use of surface pores. All surfaces have some texture, even glass. In automotive applications using plastics, the materials are often filled with glass fiber, talc or mineral fills. The result is that the surface imperfections can be filled with the glue as it\'s applied. When the glue hardens, these pockets harden, too, but that part of the glue cannot be pulled away without disturbing the material surface. This is a form of mechanical bonding. It works as well as the surface topography allows it to.\nThe third major type of bonding involves molecular level interaction. In this situation, the electrons of certain types of molecules are not evenly distributed around the nucleus. This means there\'s a net negative or positive charge at one end of the molecule that can attract an opposite but equal charge on the substrate or glue being used. This is called secondary bonding because it\'s the chemical bonding of atoms that creates new molecules.\nTwo kinds of secondary bonding are of interest in the world of glue - dipole-to-dipole bonding and hydrogen bonding. In dipole-to-dipole bonding, two oppositely charged ends of two molecules are brought close enough together that the dissimilar charges attract each other, like the poles of a magnet.\nA good example of dipole-to-dipole adhesion is water, which is two hydrogen atoms bonded to one oxygen atom. Because of its shape, it\'s a polar molecule. One of the reasons it wets so well to many if not most surfaces is the dipole-to-dipole adhesion that results from its molecular shape.\nThe term hydrogen bonding refers to the fact that if hydrogen is bonded to oxygen, nitrogen or fluorine, there will be a change in electronegativity that will result in the hydrogen atoms then seeking out other oxygen, nitrogen or fluorine atoms. According to a paper written for the SAE by Daryl Doyle of GMI, it\'s this combination of dipole-to-dipole and hydrogen bonding that makes water a liquid at room temperature.\nSo, what all of this means is that before you try to repair a broken plastic part on a vehicle, you need to know what it\'s made of so you can select an appropriate glue. The good news here is that for the last ten years or so, the International Material Data System and other groups have required that all plastic parts of a vehicle must be labeled with the type of plastic they\'re made of. If you look at your broken pieces, you\'ll almost always be able to determine the type of plastic you\'re dealing with because the name has been molded into the surface.\nCorrespondingly, glue makers are pretty good about labeling their products as to what types of plastic they\'ll work with. Plastics widely used in automotive applications include polyesters, polypropylene, ABS, polycarbonate and others. Of these, the cheapest to make - polypropylene - is the also the hardest to bond to.\nGMI\'s Doyle goes on to say in his paper that ""for an adhesive to form a bond with a substrate, some type of interaction must occur. This may include mechanical, physical adsorption or chemisorption."" The adhesive has to come in intimate contact with the surfaces being bonded. This is called wetting, and it means the adhesive has spread out to form a thin film over the surface area, with no air pockets formed underneath.\nThere are a variety of techniques used in the automotive arena to improve adhesion by increasing surface wetting to promote polar bonding. For plastics, the use of plasma etching, corona etching and/or a flame treatment is common. These methods are used to get the glue to stick to plastic surfaces like polypropylene that would not otherwise have many polar locations on the surface to which the glue can adhere.\nThe flame treatment should be used only until the surface becomes glossy. You can do this with an oxyacetylene torch. Just don\'t overdo it and be careful where else those flames might go. Passing a flame across the surface breaks the chemical bonds that are present at the surface and adds oxygen there. The net result is more polar bonding sites, and the glue sticks a lot better.\nRoughening two surfaces to be bonded yields more bonding sites where the hardened glue can achieve a mechanical bond. Cleaning the surfaces of dust and dirt can improve wetting and provides the glue with a solid surface to work on.\nOne of the gluing problems for plastic parts starts during the molding of these parts. The die has to be designed so the parts can be removed when the injected molten plastic has cooled.\nThe plastic molder also may use something called processing additives. Sheep dip by any other name, this stuff is poured into the resin pellets to act as a lubricant to facilitate removal of the part from the mold. While it does perform that function, it could mean adhesion problems for glues or epoxies that may be applied later.\nAccording to Doyle\'s paper, there are many surface preparation techniques that can be used to help give you the best possible bonding surfaces. If you\'re bonding to metal, the surface must be free of rust or oxidation. Aluminum surfaces may have to be acid-etched to remove its tough oxide layer. Ferrous metals often have a residual layer of oil to prevent rust. Glue will not stick well to oily metal.\nIf you\'re gluing two very dissimilar materials like plastic and metal that have different coefficients of thermal expansion, you should use a softer glue that will allow each part some wiggle room.\nIt\'s also possible to sand or abrade the workpieces prior to gluing. This not only cleans them but also can improve the bonding surfaces. Just be sure not to leave sanding residue, dust or dirt on the surfaces. Remember, this is all about clean surfaces. Before you reach for your air gun to blow-dry the part, remember that compressed air often contains moisture and metal particles.\nIn cases where plastic surfaces are not conducive to molecular bonding, more aggressive surface preparation may be necessary. You must add more bonding sites. You can do this with spray cans of adhesion promoter, or the use of flame, corona, plasma or even - if you happen to have a device in your toolbox - laser treatment.\nWith a broken plastic component, it can be easy to look at the two clean, freshly fractured pieces and think, Oh well, I\'ll just glue them back together. This is where the residual processing additives can cause trouble. Not only are there residual materials on the outside of the product that helped in mold removal, but there\'s more of the same stuff locked inside the plastic. The plastic may have warmed while in service, causing the processing additives to ""bloom"" out on to the surface. It makes a difference only when you try to glue the pieces back together and the glue won\'t work. The solution is a dab of acetone to wipe away the contaminating chemical you cannot see.\nThe nature of the joint itself also a factor in the success of your gluing efforts. You may find that it\'s possible to reinforce the joint or reduce the forces applied to the joint that caused the failure in the first place.\nProbably the most important advice of all is to read the label on the tube or jar of glue. You need to understand what you\'re working on, what the right choice is for an adhesive and what preparation must be done to make sure it sticks.', 'Joining low-surface-energy plastics that are difficult to bond\nThank you for your interest in 3M Industrial Adhesives & Tapes products. A 3M representative will contact you shortly regarding your inquiry.\nAn error has occurred while submitting. Please try again later...\nLow surface energy plastics, or LSE plastics, are often known by their initials and include such materials as polypropylene (PP), polyethylene (PE or HDPE), polystyrene, acetal, EVA (ethylene vinyl acetate) and powder-coated paints. They are generally soft and have a low melting point so they’re easy to process, even at high volumes, and they’re low-density, which helps with lightweighting. Because low surface energy is the primary characteristic of this group, they’re all more difficult to bond – there are adhesive and tape options that work well, but there are far fewer options to choose from and test.\nLSE plastics are easily molded into a variety of shapes and are commonly used for single-use applications such as signage, decorative trim or packaging – plastic containers, but also plastic trays, protective buffers and wrapping films. Most applications don’t require a strong adhesive, but as a re-use market many recyclable plastics are shredded and turned into lightweight plastic lumber, which generally has the same bonding properties but may also require more adhesive strength.\nThese are some of the best adhesives and tapes to use for LSE plastics. Learn more about each featured technology using the links below.\nPressure-Sensitive Adhesive (PSA) bonding tapes are thin tapes with an adhesive on both sides. These tape constructions include adhesive transfer tapes and double-sided tapes. 3M has unique formulations that are designed for adhesion to LSE surfaces. They are ideal for smooth, flat surfaces and are often used for complicated flat shapes such as die cuts for smartphones, flat-screen TVs, gasketing, trim attachment and many packaging applications.\n3M™ Scotch-Weld™ instant adhesives are high-strength liquid adhesives designed for tight-fitting joints. As little as one drop of instant adhesive per square inch can be enough to bond many substrates, including LSE plastics.\n3M™ Scotch-Weld™ Structural Acrylic Adhesives are 2-component liquid adhesives that give designers the greatest strength for demanding applications. Many of these adhesives are a good choice for bonding LSE plastics. 3M™ Scotch-Weld™ Structural Acrylic adhesives provide impact resistance and high performance in a wide range of environmental conditions.\nPolypropylene is a low-density plastic manufactured in very large volume and widely used as film, fiber and molded parts. It is often used for packaging as containers, absorbent pads and wrapping film as well as to create lightweight components for the automotive, appliance and medical industries.\nPolystyrene is a clear, hard plastic; it can be brittle, but tougher versions known as high-impact polystyrene (HIPS) are available which have slightly different bonding properties. Some of the many applications that use polystyrene include foam peanuts, CD cases, clamshell containers, bottles, trays, cups and disposable cutlery. Polystyrene processes easily by molding, allowing for fine detail, as well as by vacuum forming to create foam panels.\nPolyethylene is the most common plastic, produced in the largest volume. It is often made into wrapping film for packaging and molded into bottles or other containers. Polyethylene has relatively low strength and low temperature resistance but is easy to process and used for applications where those factors are less important.\nPolyoxymethylene is an opaque white high-performance plastic with high strength, stiffness and rigidity plus good impact and temperature resistance and dimensional stability. It is often used in components like gear wheels, conveyor belts, eyeglass frames, fasteners and ski bindings.\nEthylene vinyl acetate is a rubber-like polymer with good toughness and crack resistance. It is often used as foam in footwear, particularly midsoles for tennis shoes and sandals, and for sports padding; it can also be a substitute for natural cork.\nTraditional paints have a range of surface energies resulting in different bonding requirements. Powder-coat paints are a free-flowing dry powder which is applied and then heated to form a hard protective coating. Additives with low surface energy often flow to the surface when melted, which can make powder coats difficult to bond.\nIn determining which adhesive will perform best, it is very often helpful to consider the assembly type. The six assembly types shown below have different design characteristics that often determine the best adhesive or tape.']"	['<urn:uuid:88ee6c01-476a-46e8-ad42-6b91d8917d25>', '<urn:uuid:8ec2cbbf-9c31-4b0d-ba77-9dcbe7293efe>']	open-ended	direct	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-12T11:40:26.317848	22	118	2155
46	farm animal nutrition transport effects profits	Nutrition during transport and stocking have significant effects on farm profitability. For stocking, optimising the dry sheep equivalent (DSE) rate helps utilize available feed without relying on expensive imported feed. One DSE requires 8.1-8.7 MJ of energy per day, and proper stocking rates allow businesses to maximize profit from home-grown pasture. Regarding transport, extended periods without feed during long-distance transportation can cause serious issues. Animals can develop 'dead rumen' where digestive microbes die, reducing nutrient absorption and weight gains. They may also develop transit tetany due to reduced mineral absorption. To prevent losses, animals should be fed during transport rest periods and given good quality hay upon arrival to restore proper rumen function.	['Optimising your stocking rate is closely linked to profitability because it helps to utilise feed on offer without exceeding the land’s carrying capacity or relying too heavily on imported feed.\nStocking rate is a key indicator promoted by the Grazing Matcher Program, which currently supports 32 livestock enterprises in the South West and on the South Coast.\nCalculating stocking rate doesn’t sound too complicated until you realise that energy requirements, and therefore feed demand, varies throughout the year depending on the animal class and stage of reproduction. To help standardise “stocking rate”, numbers are converted to a dry sheep equivalent (DSE), the maintenance energy requirements for a dry 50-kilogram merino wether (or 45 kg wether in some low rainfall publications). For example, 50 kg ewes in late pregnancy (single bearing) need more energy than a dry wether and have a feed demand equivalent to 1.3 dry sheep. Alternatively, a cow with a body weight of 500 kg and also in late lactation is equivalent to 21.5 DSE.\nThese conversions can be easily done with a stocking rate calculator that converts different animals into an annual and monthly DSE figure per hectare.\nGrazing matcher co-facilitator Dan Parnell says measuring DSE gives producers an idea of their productivity and the energy they’re harvesting from their farm.\n“DSE is really a measurement of energy. One DSE will need about 8.1-8.7 MJ of energy per day,” Dan said. “That is how much energy producers need to provide. So, if we estimate the average energy of a pasture, we can see how much pasture is being utilised per hectare, or how much needs to be made available.\n“I like to do these calculations because it helps me understand where a business is at. So, if they want to make some changes to either grow more pasture or run more stock, you can see how that might play out.\n“A business with a relatively low DSE with surplus feed and surplus forage in most years should be able to support a higher stocking rate in most years. However, if they don’t have a lot of surplus forage with their current stocking rate but want to lift numbers, they need to figure out how they will achieve that.\n“Once you understand your numbers you have a baseline to work from. So, if you want to run more numbers, how much extra pasture do you need to grow and utilise. That guides management. If it doesn’t mean providing much more feed then it’s not too hard, whereas if it’s a big change you might be more wary. Maybe you can grow more by getting a good rotational grazing system in place, control weeds better or sow new varieties or species. So, it gives that context and confidence.”\nAnother benefit is to compare your DSE with costs.\n“When managers invest to improve a pasture composition they also tend to spend money on other things like weed control, lime and more fertiliser, which can increase pasture yield and quality but also adds to costs, so they need a relatively higher stocking rate to make a profit from this investment.\n“Optimising DSE is a big profit driver for most farms. The tipping point is where you have such high stocking rates that you’re having to utilise more imported feed, which is more expensive compared to home-grown pasture. It’s generally the businesses with a higher stocking rate, but where energy demands are met by home-grown pasture, that are most profitable.”\nOne manager involved in this year’s Grazing Matcher program is Brendon Giudici who farms east of Donnybrook. He has found value in calculating his stocking rate since being introduced to the tool in April this year.\n“Before I started using the DSE calculator I knew how many sheep I ran per hectare, but not the nutritional demand, which is where DSE comes into it,” Brendon said. “The big thing is working backwards to work out what pasture you need to provide to carry what you’re trying to run.\n“I found it good to quantify your total operation and analyse what different lease blocks provide. Some blocks might have lower DSE but also lower cost. It’s good to be able to measure that and consider your management options such as time of lambing and pellet or lupin inputs.\n“I used it to run scenarios such as adding extra weaners to see how it changed DSE – will we have enough energy, or do we need to drop a few animals on a hard year, how much does it actually change things. I might look at doing DSE graze days on certain paddocks, then break down what certain pasture species are doing.”\nStocking rate calculators are available online, including here. For more information, contact South West Catchments Council’s Peter Clifton on 9724 2400.\nThe Grazing Matcher program is a joint initiative of the Western Beef Association and South West Catchments Council, through funding from the Australian Government’s National Landcare Program, Meat and Livestock Australia’s Profitable Grazing Systems, Revitalising Geographe Waterways, Healthy Estuaries WA and producer subscriptions.', 'Managing long distance livestock transport\nBy Dubbo District Veterinarian, Dr Sarah Maher\nAre you planning on restocking? Have you been looking to purchase stock interstate? Will purchased stock be travelling a long distance?\nThere have been a number of large-scale deaths in stock across the district that had recently been trucked long distances. This is largely attributed to stock succumbing to disease as a result of prolonged periods without feed which is compounded by the overall stress of transport. Our vets are encouraging producers to be more involved in the planning of long-distance transportation of purchased stock.\nThe Australian Animal Welfare Standards for the Land Transport of Livestock define specific requirements in relation to livestock transport in Australia. This outlines the maximum period of time classes of stock can be held off water during transportation and the period of time stock should rest with access to food and water once this period is reached. We encourage producers to review this when planning to transport stock long distances.\nDespite these standards, the number of livestock that are not being rested or fed during long distance transportation is a cause for concern. Let’s consider how we manage sheep or cattle on farm. After the drought, we have learnt that time off feed increases the risk of metabolic diseases and nutritional disorders (such as acidosis). To mitigate this, we aim to minimise the time that livestock are yarded or off feed as much as possible. The same concept should apply to stock that are transported long distances.\nWhen stock go for extended periods without feed, their normal microbial population within their digestive tract or ‘gut flora’ decline. This provides the opportunity for the overgrowth of disease-causing bacteria within their gut, such as salmonella. Stock that go for extended periods of time off feed can also develop what we call ‘dead rumen’ whereby the microbes within the rumen die. This reduces the animal’s ability to digest and absorb nutrients, which can take time to recover, affecting weight gains. In addition, stock can develop transit tetany, or low calcium and/or magnesium, due to a reduced capacity to absorb minerals from the gut at a time when their demand for minerals increases. Add on top of this the stress of transportation and we have the right environment for disease and stock loss. Whilst the sheep or cattle you brought in from interstate might survive the journey, they may not last the week.\nIf stock are travelling long distances and are required to be rested, we are encouraging producers to arrange for stock be fed during this time. The feed does not need to be the animal’s total daily requirement but enough to keep their rumen ticking over. The small cost of providing some good quality hay is well worth the investment to reduce the stock losses.\nAdditionally, when stock arrive on farm ensure they are provided good quality hay before introducing new feed or putting them out to pasture. Hay is a relatively safe feed source that will provide adequate gut fill and ‘kick start’ the rumen bugs into functioning properly again. Hungry stock in a naïve environment will preferentially select plants they know, and this may be a weed or toxic plant.\nSo, if you’re planning on transporting sheep or cattle long distances aim to reduce their time off-fed and support their digestive tract so that they are ready to perform on arrival.\nHealthy Farm Dams webinar recording\nA Healthy Farm Dams webinar was recorded detailing the importance of good quality water for livestock and keeping fa...\nNSW Wild Dog Fence Extension project\n28 November 2023The NSW Government is working with stakeholders and landholders to deliver a 32 km extension to the ...']	['<urn:uuid:9c7dd26d-a121-4101-83f1-e3c1e9ffc6d1>', '<urn:uuid:70178b5c-3467-4147-b47b-0dc5c15fed46>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T11:40:26.317848	6	113	1460
47	I see lots of ads about new farming technologies - what makes vertical farming different from traditional agriculture, and what are some warning signs that an agricultural investment opportunity might be a scam?	Vertical farming differs from traditional agriculture in several key ways: it's a multilayer indoor system that precisely controls factors like light, temperature, and nutrients, using up to 90% less water than conventional farming. It can operate year-round regardless of weather conditions and uses automated sensors and artificial intelligence for monitoring. As for spotting potential scams, there are several warning signs to watch for. If someone promises unrealistic returns or rock-bottom prices for agricultural technology, be skeptical - similar to the fake hard-drive scam where people were sold falsely advertised equipment. Additionally, if investment opportunities require you to quickly forward information to others or make rushed decisions, these are red flags - like the fake Disney World email scam that pressured people to forward messages for promised rewards.	['Sadly, tech scams are all too common on the web. Whether it’s criminals posing as Windows Support Team staff members, dodgy listings on eBay, or Nigerian princes pretending to be your long-lost cousin, there are lots of things out there that you need to be alert to.\nOf course, you’d never be foolish enough to fall victim to one of those scams, would you?\nUnfortunately, plenty of people do become victims. Given how complex some of these scams are, it’s not surprising. But some of them are so unbelievably wacky it’s hard to understand why anyone was ever caught out.\nHere are seven of the most ridiculous internet scams of all time.\n1. A Fake Hard-Drive\n“If seems too good to be true, then it probably is.” It’s an age-old adage that’s served humans well for generations. But some people will never learn.\nBack in 2011, a user took his high-capacity Samsung external hard-drive into a Russian repair center. The owner said they’d bought it for a rock-bottom price in a Chinese store, but it kept losing data and they couldn’t understand why.\nThe guys in the shop opened it up and found nothing more than a 128 MB flash drive taped into the casing. Two bolts had been added to give it a believable weight. The perpetrators had cleverly programmed the drive into a loop mode; when it ran out of space, it started to overwrite the oldest data.\n2. Free Trip to Disney World\nMost of us are now savvy enough not to fall victim to a Nigerian prince email. But what about if the email came from Walt Disney Jr and Bill Gates? In 1999, an email claiming exactly that was doing the rounds.\nIt suggested you’d signed up for “Bill Gates’ Beta Email Tracking” service. If you forwarded it to everyone you knew and it reached 13,000 people, 10 percent of them would receive $5,000 cash and all the others would receive a free trip to Disney Land:\nHello Disney fans,\nAnd thank you for signing up for Bill Gates’ Beta Email Tracking. My name is Walt Disney Jr.\nHere at Disney we are working with Microsoft which has just compiled an e-mail tracing program that tracks everyone to whom this message is forwarded to. It does this through an unique IP (Internet Protocol) address log book database.\nWe are experimenting with this and need your help. Forward this to everyone you know and if it reaches 13,000 people, 1,300 of the people on the list will receive $5,000, and the rest will receive a free trip for two to Disney World for one week during the summer of 1999 at our expense.\nNote: Duplicate entries will not be counted. You will be notified by email with further instructions once this email has reached 13,000 people.\nWalt Disney Jr., Disney, Bill Gates\nCrazy? Definitely. But also an eerie reflection of the tracking world we now live in.\n3. Killer Bananas\nEat your five fruit and veg every day — it’s a sure-fire way to improve your health and lose weight. Unless the fruit happens to kill you.\nAt the turn of the millennium, an internet hoax claimed the United States had accidentally imported Costa Rican bananas containing a flesh-eating bacteria called necrotizing fasciitis. The hoax alleged the bacteria could spread faster than the flu.\nSounds ridiculous, but the rumor provoked the LA Times to write a feature-length investigative report, the International Banana Association to issue a statement, and Center for Disease Control to create a “Banana Hotline”.\nTo be fair, the bananas were probably still more dangerous than the millennium bug.\n4. I Am Rich\nAn iPhone app for $999 must be the best app ever created in the history of the device, right?\nWell, if your idea of the “best app ever” is a glowing red gem in the middle of the screen that has absolutely no useful function, then yes.\nTapping the gem displayed an onscreen message that read:\nI am rich\nI deserv [sic] it\nI am good,\nhealthy & successful\nIf that doesn’t sound like what you expect from a $999 app, you were sure to be disappointed.\nThe was called “I Am Rich” and was the brainchild of Armin Heinrich. The app was only available for 24 hours before Apple pulled the plug, but eight people bought it. At least one person claimed their purchase was an accident.\n5. Premium Phone Lines\nWe’re not quite sure why, but some people love to spend their hard-earned money on voting for contestants in various reality shows like Big Brother.\nThose lines are already expensive enough to dial, but one enterprising U.K. company tried to capitalize further. It created hundreds of even more costly phone numbers that only had one digit different to the actual number, hoping that users with fat fingers would accidentally press them. Callers who made the error were billed £1.02 and even got a message saying “Thank you for voting”.\nU.K. regulators ultimately fined the company behind the con £6,000 and ordered them to refund anyone affected.\n6. Dating Site for Millionaires\nIt seems that “true love” for some people means nothing more than marrying a wealthy millionaire. The person’s looks, personality, and passion are largely meaningless.\nOne homeless person capitalized on these money-seekers by enrolling on a dating service for millionaires. Posing as a renowned music producer, he managed to convince at least 12 people to put their capital into his investment scheme. It’s thought he managed to pocket at least $100,000 for his originality.\n7. Facebook Stock\nMark Zuckerberg is a huge believer in philanthropy. He’s supported countless charitable ventures in the last decade and he recently announced he’d be giving away 99% of his shares in Facebook.\nAlas, that doesn’t mean he’s going to blindly give it away to a bunch of freeloaders who use his social network. But that didn’t stop millions of people sharing a post in 2015 in the hope they’d get a share of the 10 percent he was purportedly offering to users.\nYou almost certainly saw a post like this at the time:\nMark Zuckerberg has announced that he is giving away $45 billion of Facebook stock. What you may not have heard is that he plans to give 10% of it away to people like YOU and ME! All you have to do is copy and paste this message into a post IMMEDIATELY. At midnight PST, Facebook will search through the day’s posts and award 1000 people with $4.5 million EACH as a way of saying thanks!\nObviously, the whole thing was a hoax. You could copy, paste, and tag until you were blue in the face, you weren’t going to get so much as a cup of coffee out of him.\nShare Your Funniest, Wackiest, and Craziest Scams\nThese seven scams might have brought a smile to your face, but there are countless more wonderfully ingenious schemes that people have come up with.\nSome are designed to take your money, some are designed to take your property, and some are designed to simply embarrass you.\nWe’d love to hear some of your favorite scams. Even better, we’d love to know if you were accidentally caught up in one of these hare-brained plans.\nAs always, you can leave your stories and feedback in the comments section below.\nImage Credits: kudla/Shutterstock', 'Agricultural systems around the world need to adapt to the rapidly changing environmental, demographic, and socioeconomic landscapes, and new alternative practices, such as vertical agriculture, may offer new opportunities to accelerate such adaptation.\nNext Gen Farming Without Soil and 90% Less Water | GRATEFUL\nWhat is vertical farming and why is it important\nModern agricultural systems encompass an estimated 1.5 billion hectares of the world’s surface area. With a growing population and resource needs, the availability of arable land is shrinking rapidly.\nSince the agricultural revolution, conventional agriculture has focused on practices requiring considerable quantities of space, water, fertilizer, and pesticides. The past 50 years have seen an accelerating rate of increase in these requirements as modern food production aims to increase productivity in the hopes of addressing growing food insecurity.\nLooking into the future, yield production is forecasted to decrease due to widespread environmental and socioeconomic changes that will generate unpredictable consequences on food systems.\nIn response, many strategies have been developed as alternatives to conventional agricultural practices. These strategies have focused on key principles and their combination to be effective: require less space, less water, and increase yield per unit of area. Moreover, due to the negative effects of agrichemicals, modern practices have also aimed to use significantly less to avoid potentially adverse effects for humans and animals.\nOne such alternative is the development of vertical agriculture, also referred to as vertical farming. As the name implies, vertical agriculture relies on expanding production vertically and not horizontally. Vertical agriculture is a multilayer indoor plant production system that allows for precise control of growth factors, such as light, temperature, humidity, carbon dioxide concentration, water, and nutrients.\nThis allows for the growing and production of crops year-round, completely independent of solar light and other external conditions. Indeed, vertical agriculture makes use of key concepts within ecology and physiology to optimize growing and fertilization within controlled conditions. For instance, elements of photobiology, thermomorphogenesis, hydroponics, and genetic breeding, are all used commonly across systems of vertical agriculture.\nBenefits, challenges, and disadvantages moving from horizontal to vertical farming\nAs a result of tight control over crop breeding, growing, and harvesting, vertical farming provides several benefits relative to conventional methods of ‘horizontal’ food production. This was the topic of a literature review by Kalantari et al. published in 2016 in the Journal of Landscape Ecology.\nFrom a systems perspective, the enclosed design prevents pests and diseases from entering by the adoption of a high level of hygiene, continuous monitoring, and non-chemical disinfection, providing security from crops. Moreover, recent technology has also allowed for automated control over environmental conditions by using sensors and imaging techniques in combination with crop simulation models and artificial intelligence, limiting the need for physical labor.\nVertical farming also allows for flexible organization, with designs ranging from large vertical walls covered with crops to large hangars or re-used shipping containers that can be transported. Consequently, vertical agricultural systems, can comprise many varying sizes and be located within many different areas from the middle of highly urbanized cities to more suburban or rural areas.\nMoreover, the verticality element of this system also provides nutrient and water flow, helping to reuse costly resources. The reduction in space also means there is a significant increase in yield per area, holding extensive potential for a future world of urbanization.\nFrom an economic perspective, vertical farming also provides for more jobs in localized areas and is community-focused by addressing the needs of immediate areas, which in turn can provide food at a lower price. Finally, the optionality of location for vertical systems also allows producers to reduce transport costs, as consumers may access them within urban areas, or transport can be minimized to nearby areas.\nHowever, despite the design, environmental, and economic advantages, vertical farming also incorporates several issues that remain a challenge to its broader implementation as a system.\nVertical farming has a high energy requirement and needs extensive investment costs to implement and develop successfully. Moreover, indoor issues relating to excessive UV, heat, and ozone-induced plant damage may have unpredictable repercussions for plant growth.\nAdditionally, vertical systems are difficult to adapt to a larger scale. They are costly to build and maintain and have yet to demonstrate the ability to provide food for larger areas than community-scale populations. This would make it difficult to implement in areas at higher risk of food insecurity, such as developing agricultural nations.\nThe lack of empirical research on a broader scale has meant that vertical farming has yet to develop past the concept stage on community levels, as persistent issues make it difficult to break through to a larger scale.\nImage Credit: YEINISM/Shutterstock.com\nGrowing skywards - the implications of vertical farming in a rapidly populating and changing world\nAmong the development of alternative agricultural practices, vertical agriculture provides a promising solution for many of the challenges facing current agricultural policies. However, for vertical systems to be integrated on a larger scale requires further technological progress and economic investment.\nNevertheless, gradually implementing more verticality, or combining it with other practices aiming for more sustainable practices may be promising. For instance, the combination of verticality with other practices such as intercropping may be particularly beneficial for developing more sustainable food systems.\nIncorporating technological progress into vertical systems also holds promise, with automated sensors and machinery able to operate near-independently. Progress in gene editing and plant genome modifications will also allow for faster, bigger, and healthier crops, allowing vertical agriculture to produce more over time.\nThroughout agricultural history, farming systems have typically spread over large spans of land, yet the reduction in arable land, as well as the increase in demand to house growing populations, means that such strategies need to be reconsidered, and vertical agriculture may play a role looking into the future.\nContinue Reading: Benefits of Vertical Agriculture and Hydroponics\n- Beacham, A. M., Vickers, L. H., & Monaghan, J. M. (2019). Vertical farming: a summary of approaches to growing skywards. The Journal of Horticultural Science and Biotechnology, 94(3), 277–283. doi: 10.1080/14620316.2019.1574214\n- Chaudhry A. R. and Mishra V. P.,(2019) A Comparative Analysis of Vertical Agriculture Systems in Residential Apartments, Advances in Science and Engineering Technology International Conferences (ASET), 2019, pp. 1-5, doi: 10.1109/ICASET.2019.8714358\n- Sarkar, A., & Majumder, M. (2015). Opportunities and Challenges in Sustainability of Vertical Eco-Farming: A Review. Journal of Advanced Agricultural Technologies, 2(2). doi: 10.12720/joaat.2.2.98-105\n- SharathKumar, M., Heuvelink, E., & Marcelis, L. F. (2020). Vertical Farming: Moving from Genetic to Environmental Modification. Trends in Plant Science, 25(8), 724–727. doi: 10.1016/j.tplants.2020.05.012']	['<urn:uuid:f2d18a3d-c155-4d48-9a9e-360c1f7e29e6>', '<urn:uuid:3566e03d-b8ac-4d36-b13c-9d56f4d7ba0e>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T11:40:26.317848	33	127	2315
48	what injuries required osha reporting	Employers must notify OSHA when an employee is killed on the job or suffers a work-related hospitalization, amputation, or loss of an eye. A fatality must be reported within 8 hours, while an in-patient hospitalization, amputation, or eye loss must be reported within 24 hours.	['Osha Recordkeeping – Part 1\nThe requirement for private employers to record and report certain injuries and illnesses was one of the many cornerstones found in the Occupational Safety and Health Act of 1970.\nIn the development of the OSHA Act Congress saw a need to have employers use a system to track injuries and illnesses. In doing so, employers would have a database that could be used to address any hazards in the workplace that have gone unattended.\nExcerpt from the OSHA Act: Section 8 paragraph (c) (2)\n“The Secretary, in cooperation with the Secretary of Health and Human Services, shall prescribe regulations requiring employers to maintain accurate records of, and to make periodic reports on, work-related deaths, injuries, and illnesses other than minor injuries requiring only first aid treatment and which do not involve medical treatment, loss of consciousness, restriction of work or motion, or transfer to another job.”\nUpdated Version in 2001\nOn January 19, 2001, OSHA issued an updated version of the original standard in the Federal Register.\nThe final rule contains provisions mandating the recording of work-related injuries and illnesses that must be entered on the OSHA 300 (Log) and 301 (Incident Report) forms.\nIt sets out the recording requirements that employers are required to follow in recording cases.\nThe final rule also mandates that each employer who is required by OSHA to keep records must record each fatality, injury, or illness that is work-related, is a new case and not a continuation of an old case, and meets one or more of the general recording criteria:\n- Days Away from Work\n- Restricted Work Activity (commonly called “light duty”)\n- Transfer to another job (yes, this is recordable!)\n- Other recordable (like hearing loss or one stitch to the finger)\nThe updated standard contains a table that points employers and their recordkeepers to the various sections of the rule that determine which work-related injuries and illnesses are to be recorded.\nThese sections lay out the requirements for determining whether an injury or illness is work-related, if it is a new case, and if it meets one or more of the general recording criteria. In addition, the table contains a row addressing the application of these and additional criteria to specific kinds of cases (needlestick and sharps injury cases, tuberculosis cases, hearing loss cases, medical removal cases, and musculoskeletal disorder cases).\nOSHA Reporting Decision Tree\nThe following is a decision tree, or flowchart, that shows the steps involved in determining whether a particular injury or illness case must be recorded on the OSHA forms.\nWhat are the OSHA reporting requirements the employer must comply with? This part of the OSHA rule was updated on September 18, 2014.\n- All employers are required to notify OSHA when an employee is killed on the job or suffers a work-related hospitalization, amputation, or loss of an eye.\n- A fatality must be reported within 8 hours.\n- An in-patient hospitalization, amputation, or eye loss must be reported within 24 hours.\nBe prepared to supply the following information when reporting: Business name; names of employees affected; location and time of the incident, brief description of the incident; contact person; and phone number.\nSo, when do the eight hours start? The OSHA requirement seems a little vague. Here is a recommendation. Use the death certificate as a legal time of death.\nAll employers should have a policy and procedure that outlines what to do if a fatality occurs in their organization. Using a death certificate as a legal time of death is a good point to add to the policy. Also, as an employer, you have 8 hours to report. Use this time to do your investigation, get all the details, and report 7.5 hours after the incident.\nDo I have to report the fatality, in-patient hospitalization, amputation, or loss of an eye if it resulted from a motor vehicle accident on a public street or highway?\nIf a motor vehicle accident occurs in a construction work zone, you must report the fatality, in-patient hospitalization, amputation, or loss of an eye.\nIf a motor vehicle accident occurred on a public street or highway but not in a construction work zone, you do not have to report the fatality, in-patient hospitalization, amputation, or loss of an eye to OSHA. However, the fatality, in-patient hospitalization, amputation, or loss of an eye must be recorded on your OSHA injury and illness records if you are required to keep such records.\nBe aware that some cases that are not reportable may still be recordable.\nFor more information, go to this link: 1904.39 – Reporting fatalities, hospitalizations, amputations, and losses of an eye as a result of work-related incidents to OSHA. | Occupational Safety and Health Administration\nAre there exceptions to the recording of a work-related injury or illness?\nThis becomes interesting. If a worker is hurt at work but the review of the injury or illness falls into one of the following exceptions, the employer does not have to record the incident.\nHere they are:\nYou are not required to record injuries and illnesses if…\n- At the time of the injury or illness, the employee was present in the work environment as a member of the general public rather than as an employee.\n- Example: The worker is there to pick up their check and is not scheduled to work that shift.\n- The injury or illness involves signs or symptoms that surface at work but result solely from a non-work-related event or exposure that occurs outside the work environment.\n- Example: a heart attack at work due to a lifestyle issue.\n- The injury or illness results solely from voluntary participation in a wellness program or in a medical, fitness, or recreational activity such as blood donation, physical examination, flu shot, exercise class, racquetball, or baseball.\n- The injury or illness is solely the result of an employee eating, drinking, or preparing food or drink for personal consumption (whether bought on the employer’s premises or brought in).\n- For example, if the employee is injured by choking on a sandwich while in the employer’s establishment, the case would not be considered work-related.\n- Note: If the employee is made ill by ingesting food contaminated by workplace contaminants (such as lead) or gets food poisoning from food supplied by the employer, the case would be considered work-related.\n- The injury or illness is solely the result of an employee doing personal tasks (unrelated to their employment) at the establishment outside of the employee’s assigned working hours.\n- The injury or illness is solely the result of personal grooming, self-medication for a non-work-related condition, or is intentionally self-inflicted.\n- For example: an employee gets mad at the potato chip vending machine and breaks the glass to get his bag of chips. The injury was a severe laceration to the hand and forearm, requiring sutures (stitches). And yes, it’s not recordable.\n- The injury or illness is caused by a motor vehicle accident and occurs on a company parking lot or company access road while the employee is commuting to or from work.\n- Note: There are two commutes a day: from your home to your first business stop and from your last business stop to your home. Injuries and illnesses that happen during the commute are not recordable.\n- The illness is the common cold or flu (note: contagious diseases such as tuberculosis, brucellosis, hepatitis A, or plague are considered work-related if the employee is infected at work).\n- The illness is a mental illness. Mental illness will not be considered work-related unless the employee voluntarily provides the employer with an opinion from a physician or other licensed healthcare professional with appropriate training and experience (psychiatrist, psychologist, psychiatric nurse practitioner, etc.) stating that the employee has a mental illness that is work-related.\n- Example: Your boss drove you crazy.\nStay tuned for more.']	['<urn:uuid:40f8ba56-a9e9-45bf-91b7-603f43dcca7b>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T11:40:26.317848	5	45	1319
49	treatment steps muscle sprain immediate	The recommended immediate treatment for a muscle sprain or injury follows the RICE protocol: rest, ice, compression, and elevation. You should stop your activity and sit or lie down to protect the injured area. Apply a cold compress wrapped in a thin towel for no more than 20 minutes to reduce swelling. Then wrap the injury securely in an Ace bandage, but not so tight that you feel numbness or throbbing. Finally, raise the injured area above heart level. If it's a minor sprain or strain, improvement should be seen within a couple of days, at which point light massage can help healing. However, if pain or swelling persists after 48 hours, medical attention is necessary.	"['""No pain, no gain"" was the bodybuilder\'s challenge in the 1980s. ""Pain is weakness leaving the body,"" brags the Marine Corps T-shirt. ""No Pain, No Jane,"" proclaim billboards for the Mary Jane ski area at Winter Park.\nAnyone who exercises knows that muscles can burn during difficult movements or feel sore the next day. The mere thought of pain is often enough to keep some people planted on the couch.\nParadoxically, those already in pain from arthritis are told that exercise can help relieve it. And then there is pain that stops you in your tracks midway through a workout.\nPain from exercise is the body\'s messenger. So, ignoring the pain and just toughing it out could make the injury worse.\n""There are a lot of different ways to be injured. If there is pain but no swelling, you can continue to exercise but you should modify the activity to make it painless,"" says Dr. Richard Steadman, orthopedic surgeon in Vail and founder of the Steadman Hawkins Research Foundation.\nHe suggests cross-training, so if you experienced some pain after running, switch to cycling or swimming for a while. ""Don\'t continue with the activity that created the pain in the first place,"" he says.\nWith a minor injury, leave it for one to three days. If pain recurs, continues or worsens, get help.\nSteadman advises taking a measured approach to injury. ""A lot of injuries do well with some exercise, but most do not. It is generally not the right thing for everyone to just work through it,"" he says. ""Swelling is a definite negative - always seek medical care in this case.""\nSome medical researchers think exercise may be important to the healing process.\n""We want to keep you moving,"" Dr. William Roberts, a sports medicine specialist at the University of Minnesota said in a recent New York Times article. ""Injured tissue heals better if it\'s under some sort of stress,"" said Roberts, a past president of the American College of Sports Medicine.\nThe key thing is to be under a physician\'s care at the initial stages of exercise re-entry.\nWhile many fitness professionals and sports medicine researchers believe people should ""train, not strain,"" others - particularly body builders who are going for maximum muscle growth - do not buy that advice. Since body building demands intense training, those who practice it contend that muscle soreness is the price of admission for a sterling physique.\nExercise in general, not just weightlifting, can cause delayed onset muscle soreness (DOMS). But the correlation between muscle soreness and developing bigger muscles is still being debated.\nDOMS is believed to be caused by two sources: waste products that build up in the muscle or the microscopic tears in the muscle tissue that occur when muscles are pushed harder or differently. This soreness can come from trying out a new exercise or by increasing the intensity or duration of the activity. For example, if you usually jog 1 mile a day then suddenly decide to push yourself to 2 or 3, some soreness is inevitable, is usually mild and generally disappears after 12 to 48 hours.\n""Bad"" pain often feels sharp, comes on suddenly and stops your activity. The pain could be a pulled muscle, strained or torn ligament or tendon, twisted joint, a dislocated joint, or even a break or fracture of bone. The injured area could swell immediately, a good sign to stop what you are doing.\nThe best way to treat such an injury it is to immediately give it RICE - rest, ice, compression, and elevation. Stop what you are doing, and sit or lie down in order to protect the injured area. Apply a cold compress wrapped in a thin towel for no longer than 20 minutes. This will help reduce the swelling. Wrap the injury securely (but not so tight that you feel numb or feel throbbing) in an Ace bandage. Raise the injury above the level of the heart.\nIf it is a minor sprain or strain, you should see improvement in a couple of days. At that point, light massage of the injury can be helpful in the healing process. If the pain or swelling isn\'t gone after 48 hours, see a doctor.\nMany people deal with chronic pain from such ailments as arthritis, fibromyalgia, Paget\'s disease, general joint and muscle pain of aging, heart disease and stress. In virtually every case, exercise has been found to be beneficial in minimizing the pain.\nBut remember, you should consult a physical therapist to learn the type of exercise that will be helpful for your condition. For some, light strength training is helpful; for others water exercise is key. Many people benefit from yoga and Pilates.\nIn addition, proper form is crucial, which can be learned from a physical therapist, as well as from a properly certified personal trainer.\nMany more factors are involved with chronic pain, including (but not limited to) psychological, social, and emotional issues. Stress relief is also a key component.\nTake all of this into account as you learn how to cope.\nLinda J. Buch is a certified fitness trainer in Denver and co-author of ""The Commercial Break Workout."" Write her at firstname.lastname@example.org\nExercising through an injury\nIf you\'ve hurt yourself, you need to proceed with caution - and a plan.\nMuscle pulls, sprains and strains need time to heal, so follow the doctor or physical therapist\'s advice. You may be surprised how exhausting it is and how much time it takes to perform all of those rehabilitation exercises!\nUse common sense. Slow down, cut back on the intensity, and don\'t forget RICE (rest, ice, compression, elevation).\nUse anti-inflammatory medications carefully. They mask pain that could lead to making the injury worse. Inflammation, redness, and swelling that is tender to the touch are indicators that something is wrong.\nThere are always alternatives to your normal exercise choices. If the lower body is injured, find some exercises for the upper body. If you have access to a pool, swim; if you can bicycle, do so. Crosstrain until the pain is gone.\nSitting around moping is unproductive and will annoy your friends. Take this opportunity to check some books out of the library, organize your desk, or catch up on correspondence.\nIf weight gain is a concern, adjust your dietary intake to reflect your temporarily slower lifestyle.\nAvoid forceful, ballistic stretching. Gentle stretching can promote healing.\nAvoiding muscle soreness\nTake the time to warm up and cool down after an activity, and you\'ll reduce your chances of being sore the next day. Slow, easy stretching can be very beneficial to the joints, and a good 15-minute warm-up is recommended to get your cardiovascular system and lungs working. Also:\nPerform light, easy stretching after exercise.\nWhen strength training, start with lighter weights and high repetitions (12-15). Increase the weight lifted over several weeks.\nTry not to make sudden changes in activity. If you do try something new, go easy for a week or two.\nGradually increase the amount of time spent at the new activity; get some experienced coaching before diving in.\nIf you ignored the recommendations above, here\'s how to treat muscle pain:\nPerform some easy, low-impact aerobic exercises in order to increase the blood flow to the affected areas.\nDo some gentle stretching and/or massage on the affected area.\nTry a nonsteroidal anti-inflammatory medication (aspirin or ibuprofen). These will deal only with the soreness, not speed the healing.']"	['<urn:uuid:0111e301-2a67-49bb-a32a-32f98cdcd3fa>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T11:40:26.317848	5	116	1242
50	looking for financial education resources what do they offer and why is it important now	The Financial Initiative offers comprehensive resources including research-based materials, teacher supports, and family learning tools to help individuals develop financial knowledge and decision-making skills. This education is particularly crucial today because we live in an increasingly sophisticated financial system where lacking proper financial planning can lead to serious consequences like mounting debt, missed financial goals, and insufficient retirement savings.	"[""We believe a comprehensive approach to improving financial education is essential. The foundation of our work at UChicago Financial Initiative is based on five pillars: Research, Advocacy, Materials, Systemic Supports, and Partnerships.\nThe Initiative believes that financial education materials and resources should be grounded in a solid body of research based on what individuals should know and how they learn best. Our research and development process includes:\nWe regularly survey the field to determine what financial education products are available, and what outcomes different products produce. This examination helps us determine both how financial education is working and also where it can be improved.\nWe examine research literature and information produced by financial experts to educate ourselves and stay up-to-date on the facts and information people need to be financially literate. A significant part of our work is translating this research into the practice of financial literacy education.\nWe regularly read and summarize scholarly literature about how people learn generally, and how they learn financial literacy specifically. We incorporate this research into our work through the design of the materials themselves, as well as explicit guidance for teachers who read and implement our lessons.\nWe partner with those using our materials to gather feedback on how our materials are working in real-world contexts. We collect and consider this feedback as we plan future revisions.\nWe engage external evaluators to analyze whether and how our materials are producing results in educational and behavioral outcomes for individuals. We use the results of external evaluations to plan for improvements.\nIn addition to the other work we do, the Initiative actively engages in advocacy work to raise awareness, bring about change, and create a movement around the importance of financial education. Learning is ongoing and we are working to provide education opportunities to all individuals throughout their lifetime. The Initiative advocates for:\nA single semester of financial education in every high school across the country. Every student should have access to high-quality financial education.\nAdult financial wellness as part of every employee benefit package. Every adult should have access to high-quality financial education and resources.\nAn increased focus on financial socialization in schools and homes. Conversations about money are often awkward and challenging, especially conversations we have with those closest to us. The Initiative aims to provide a way for families and friends to approach these sometimes difficult financial conversations.\nHigh-quality materials are needed to financially empower individuals. Through extensive research, design, and feedback the Initiative helps individuals develop financial knowledge, attitudes, and decision-making skills essential to achieving financial well-being.\n- Materials enable individuals to acquire and construct new knowledge, skills, and attitudes, through authentic, carefully designed decision-making experiences.\n- Materials teach fundamental and critical research skills that stand the test of time and provide individuals with the ability to navigate a complex, ever-changing marketplace.\n- Materials address the diverse financial contexts individuals face.\n- Materials are based on learning science, behavioral change research, and the latest information on effective design of formal and informal financial education experiences.\nThe Initiative aims to institutionalize a broad level of supports for people to increase their knowledge and exhibit positive financial behaviors.\nExtensive supports help high-school teachers improve their craft and increase confidence and competence in teaching financial education.\nLearning at Home\nParents and influential adults play a critical role in students' financial education. Supports help parents talk to their children about how they view money and provide insight into their child's thoughts and feelings.\nUnderstanding the System\nOur financial system does not provide all individuals with the same choices or opportunities. Supports provide individuals with the opportunity to explore the economic environment they inhabit, including systemic barriers and inequalities that may impact their community.\nNavigating the System\nFinancial education can indeed improve financial well-being by addressing different access and barriers to financial success. Supports help individuals learn how to make decisions based on who they are, where they are, and where they are going.\nPartnerships help advance the field of financial education by sharing expertise and promoting promising practices. The Initiative engages with various stakeholders to check our assumptions, elevate our thinking, and learn from others.\nWe partner with researchers and practicioners to acknowledge and incorporate the perspectives offered on financial education topics and other information\nWe collaborate with teachers, administrators, and district leaders, to increase to teach financial education effectively and we listen to their feedback to determine their needs as the educators doing this important work.\nWe connect with state education departments, state treasurers, and state and local leaders to advocate for financial education and provide broad level supports.\nWe engage with national partners to promote access to and equity in financial education.\nWe enlist the expertise of external evaluators to examine the program outcomes and success.\n“We live in a world with an increasingly sophisticated financial system. We’re providing an adequate foundation so people can navigate the system and make decisions with confidence.”"", 'Financial planning is a crucial aspect of life that many individuals often neglect or postpone. People often overlook the significance of creating a solid financial plan, believing they can handle their finances on the go.\nHowever, failing to establish a financial plan can have various negative consequences. This article will explore six significant ramifications of not having a financial plan and highlight the importance of proactive financial management.\nFinancial Stress and Anxiety\nOne of the most immediate consequences of not having a financial plan is the increased stress and anxiety that can plague your life. You may constantly worry about money matters without a clear roadmap for managing your income and expenses.\nUnexpected expenses, mounting debt, or an uncertain financial future can create a perpetual state of anxiety, impacting your mental and emotional well-being. The stress of financial uncertainty can also lead to strained relationships with family and friends, further exacerbating the problem.\nMissed Financial Goals\nA financial plan acts as a guide to help you achieve your financial goals, whether buying a house, saving for retirement or starting a business. Without a plan in place, you’re essentially navigating without a destination.\nThis lack of direction can lead to missed opportunities and unfulfilled dreams. Over time, you may realize you’ve fallen behind on your goals, and catching up becomes increasingly challenging. This can result in regret and disappointment that could have been avoided with proper financial planning.\nAnother significant repercussion of not having a financial plan is the potential for accumulating excessive debt. Without a budget and a clear understanding of your financial situation, it’s easy to overspend and rely on credit cards or loans to make ends meet.\nThis can quickly lead to a cycle of debt that’s difficult to break free from. High-interest rates and mounting debt payments can eat away your income and hinder your ability to achieve your financial goals.\nInadequate Emergency Fund\nLife is unpredictable, and unexpected expenses can arise anytime, such as medical emergencies, car repairs, or job loss. You may be ill-prepared to handle these situations without a financial plan that includes an emergency fund.\nThis can lead to financial crisis and forced borrowing, further exacerbating your debt problems. A well-thought-out financial plan typically includes building and maintaining an emergency fund to provide a safety net during challenging times.\nInsufficient Retirement Savings\nPlanning for retirement is a critical aspect of financial planning, yet many people underestimate the amount of money they’ll need to retire comfortably. Without a financial plan, you may not be contributing enough to your retirement accounts or investing wisely for the future.\nThis can result in a retirement savings gap, leaving you financially unprepared for your golden years. The longer you delay planning for retirement, the harder it becomes to catch up, making it essential to start early and adhere to a well-structured financial plan.\nReduced Wealth Accumulation\nLastly, not having a financial plan can significantly hinder your wealth accumulation potential. A comprehensive financial plan typically includes strategies for saving, investing, and managing your assets efficiently.\nWithout these strategies, your wealth-building efforts are likely to be suboptimal. Over time, this can translate into a substantial difference in your net worth. Whether it’s missed investment opportunities or poor asset allocation, the lack of a financial plan can result in a lower standard of living and fewer opportunities for financial freedom.']"	['<urn:uuid:074d3968-097c-4082-9796-e72cc5cfa740>', '<urn:uuid:82af88c9-5031-4aad-9b81-14ad3ebc69bc>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T11:40:26.317848	15	59	1379
51	panama and suez canals compare france involvement both projects what happened	France was involved in both canal projects but with different outcomes. For the Suez Canal, France successfully co-financed the construction with Egypt, and it was completed in 1869. However, France's attempt to build the Panama Canal in the 1880s ended in failure. The French company, led by Ferdinand de Lesseps (who had successfully developed the Suez Canal), went bankrupt in 1889 after spending over $260 million. This failure led to a major scandal in France, with de Lesseps and other executives being found guilty of fraud and mismanagement in 1893.	['The nationalisation of the Suez Canal by Egypt on 26 July 1956 is commemorated on this stamp. Printed in two colours, the background shows a map of the Suez region bisected by the canal. The straight sections of the man-made canal, created in the 1860s to enable steamship passage between the Mediterranean and the Red Sea, are clearly visible.\nThe writing in French and Arabic says: ‘Egypt; Nationalisation of the Suez Canal Company; Freedom of navigation guaranteed’. Control over passage through the canal was of vital importance both strategically and financially. At the time of nationalisation, navigation was free for all except Israeli ships or those bound for Israel. The envelope is addressed to the Dutch Director General of Philips Orient, a joint-stock subsidiary of the Philips music company which sold music recordings across the Arab world. Cairo at the time was the centre of a thriving Middle Eastern music scene from which European companies sought to profit.\nSuez as a focus of global interest\nSuez is located at the north end of the Red Sea, the waterway which leads into the Indian Ocean. Around 90 miles north of Suez is Port Said on the Mediterranean, therefore this stretch of land is the shortest distance between these two great regions of maritime trade. Opened in 1869, the canal was financed jointly by the French and Egyptian governments and enabled steamships carrying people and commodities for trade to travel directly between Europe and Asia. Previously, goods had to be unloaded, transported by land and reloaded, or ships travelled all around Africa. In the age of high imperialism there was a huge amount of sea traffic, for example, Britain’s economic prosperity depended upon trade with her imperial territories, particularly India. The canal therefore became a point of strategic importance for many European nations.\nEconomic difficulties for Egypt in the 1870s led their government to sell their 44% of shares in the Canal Company to Britain for £4 million. Britain therefore was able to control the finances and operations of the canal. Many examples over the next forty years, including its defence during World War I, demonstrated the importance of the canal to Britain. The Anglo-Egyptian treaty in 1936 recognised Egypt as an independent sovereign state, but allowed British troops to be stationed in the region around the Canal.\nDecolonisation and increasing tension in the Middle East\nSeveral factors contributed to heightening tensions after World War II. Indian independence in 1947 symbolised the era of decolonisation as European colonies gained freedom from their colonial masters. The USA encouraged these moves towards independence, motivated at least partly by fears about the spread of communism in these new nations. Both the USA and the USSR were keen to shore up support within the new nationalist movements and supplied military and diplomatic support, thus drawing them into the orbit of the Cold War. The creation of Israel in 1948, following the atrocities suffered by Jews during World War II, was a further source of conflict in the Middle East, with the expulsion from their lands of hundreds of thousands of Palestinian people. In Egypt, the early 1950s saw nationalist movements gain momentum and in 1954 the nationalist party overthrew King Farouk and General Gamal Abdel Nasser became president.\nBritain and other European nations were undergoing significant economic hardship after the war. No longer was access to imperial territories their priority in Suez. As the historian Yergin wrote, the new role of the canal was as ‘the highway not of empire, but of oil’; in 1955, petroleum made up half of all the canal’s traffic. Worsening relations between the British and the Egyptians over British occupation and control of the canal led to its nationalisation by General Nasser, prompting Britain, France and Israel to seize the canal by force in October 1956.\nDiplomatic crisis and aftermath\nInternational opinion greatly opposed the military action. Britain was already engaged in wars against insurgent nationalist movements in Malaya and Kenya and to many the invasion of Suez appeared as the last gasp of Empire in the face of the ebbing away of Britain’s power worldwide.\nThe USA supported the right to self-determination against imperial rule and wanted united international solutions to conflict in the Middle East rather than direct military action, which it worried might provoke further communist intervention. The Americans also feared that US involvement would elicit a response from the USSR that aligned the Middle East along Cold War alliances and risked the whole region’s descent into war. The USSR wished to keep the canal open and free for economic reasons and so supported a diplomatic resolution. They also wanted to support Nasser in order to weaken US dominance in the region. Coupled with opposition at home in Britain, this international pressure led to the withdrawal of British and French troops and UN forces were sent to restore order. The canal was reopened to shipping in April 1957.\nThe Suez Crisis was the first of many instances where Cold War opposition between the USA and the USSR determined the outcome of world events. Although British and US relations were strained by the crisis, the two countries went on to cooperate during the Cold War against the USSR, with the USA as the senior power. The whole episode starkly demonstrated where global power now lay. In the Middle Eastern context, control of the Suez region and the Sinai Peninsula continued to be a source of tension between Israel and Egypt, particularly in the Arab-Israeli wars of 1967 and 1973.\nBBC summary of the Suez crisis\nBBC 4 documentary: The Other Side of Suez\nExtracts could be useful in the classroom.\nTimeline of the crisis\nOverview of the crisis (Guardian)\nNote that not all links at the bottom are live.\nOverview of the crisis (Economist)\nOverview from the National Archives\n1960 film about the crisis', '1. The idea for a canal across Panama dates back to the 16th century.\nIn 1513, Spanish explorer Vasco Nunez de Balboa became the first European to discover that the Isthmus of Panama was just a slim land bridge separating the Atlantic and Pacific oceans. Balboa’s discovery sparked a search for a natural waterway linking the two oceans. In 1534, after no such passage across the isthmus had been found, Charles V, the Holy Roman emperor, ordered a survey to determine if one could be built, but the surveyors eventually decided that construction of a ship canal was impossible.\n2. The men behind the Suez Canal and Eiffel Tower were convicted in connection with failed effort to build a canal.\nIn the ensuing centuries, various nations considered developing a Panamanian canal but a serious attempt wasn’t made until the 1880s. In 1881, a French company headed by Ferdinand de Lesseps, a former diplomat who developed Egypt’s Suez Canal, began digging a canal across Panama. The project was plagued by poor planning, engineering problems and tropical diseases that killed thousands of workers. De Lesseps intended to build the canal at sea level, without locks, like the Suez Canal, but the excavation process proved far more difficult than anticipated. Gustave Eiffel, who designed the famous tower in Paris that bears his name, was then hired to create locks for the canal; however, the De Lesseps-led company went bankrupt in 1889. At the time, the French had sunk more than $260 million into the canal venture and excavated more than 70 million cubic yards of earth.\nThe canal venture’s collapse caused a major scandal in France. De Lesseps and his son Charles, along with Eiffel and several other company executives, were indicted on fraud and mismanagement charges. In 1893, the men were found guilty, sentenced to prison and fined, although the sentences were overturned. After the scandal, Eiffel retired from business and devoted himself to scientific research; Ferdinand de Lesseps died in 1894. That same year, a new French company was formed to take over the assets of the bankrupt business and continue the canal; however, this second firm soon abandoned the endeavor as well.\n3. America originally wanted to build a canal in Nicaragua, not Panama.\nThroughout the 1800s, the United States, which wanted a canal linking the Atlantic and Pacific for economic and military reasons, considered Nicaragua a more feasible location than Panama. However, that view shifted thanks in part to the efforts of Philippe-Jean Bunau-Varilla, a French engineer who had been involved in both of France’s canal projects. In the late 1890s Bunau-Varilla began lobbying American lawmakers to buy the French canal assets in Panama, and eventually convinced a number of them that Nicaragua had dangerous volcanoes, making Panama the safer choice.\nIn 1902, Congress authorized the purchase of the French assets. However, the following year, when Colombia, which Panama was then a part of, refused to ratify an agreement allowing the United States to build a canal, the Panamanians, with encouragement from Bunau-Varilla and tacit approval from President Theodore Roosevelt, revolted against Colombia and declared Panama’s independence. Soon afterward, U.S. Secretary of State John Hay, and Bunau-Varilla, acting as a representative of Panama’s provisional government, negotiated the Hay-Bunau-Varilla Treaty, which gave America the right to a zone of more than 500 square miles in which it could construct a canal; the Canal Zone was to be controlled in perpetuity by the Americans. All told, the United States would shell out some $375 million to build the canal, which included a $10 million payment to Panama as a condition of the 1903 treaty, and $40 million to buy the French assets.\nA century after the United States completed the Panama Canal, a navigable link across Nicaragua remains a possibility: In 2013, a Chinese company announced it had struck a $40 billion deal with the Nicaraguan government for the rights to construct such a waterway.\nRecommended for you\n4. More than 25,000 workers died during the canal’s construction.\nThe canal builders had to contend with a variety of obstacles, including challenging terrain, hot, humid weather, heavy rainfall and rampant tropical diseases. The earlier French attempts had led to the deaths of more than 20,000 workers and America’s efforts fared little better; between 1904 and 1913 some 5,600 workers died due to disease or accidents.\nMany of these earlier deaths had been caused by yellow fever and malaria; diseases that the medical community at the time believed were caused by bad air and dirty conditions. By the early 20th century, however, medical experts better understood the role of mosquitoes as carriers for these diseases, allowing them to significantly reduce the number of deaths among canal workers, thanks to a host of sanitation measures that included draining areas with standing water, removing possible insect breeding grounds and installing window screens in buildings.\n5. Between 13,000 and 14,000 ships use the canal every year.\nAmerican ships use the canal the most, followed by those from China, Chile, Japan, Colombia and South Korea. Every vessel that transits the canal must pay a toll based on its size and cargo volume. Tolls for the largest ships can run about $450,000. The smallest toll ever paid was 36 cents, plunked down in 1928 by American adventurer Richard Halliburton, who swam the canal. Today, some $1.8 billion in tolls are collected annually.\nOn average, it takes a ship 8 to 10 hours to pass through the canal. While moving through it, a system of locks raises each ship 85 feet above sea level. Ship captains aren’t allowed to transit the canal on their own; instead, a specially trained canal pilot takes navigational control of each vessel to guide it through the waterway. In 2010, the 1 millionth vessel crossed the canal since it first opened in 1914.\n6. The United States transferred control of the canal to Panama in 1999.\nIn the years after the canal opened, tensions increased between America and Panama over control of the canal and the surrounding Canal Zone. In 1964, Panamanians rioted after being prevented from flying their nation’s flag next to a U.S. flag in the Canal Zone. In the aftermath of the violence, Panama temporarily broke off diplomatic relations with the United States. In 1977, President Jimmy Carter and General Omar Torrijos of Panama signed treaties that transferred control of the canal to Panama in 1999 but gave the United States the right to use military force to defend the waterway against any threat to its neutrality. Despite opposition by a number of politicians who didn’t want their country to give up its authority over the canal, the U.S. Senate ratified the Torrijos-Carter Treaties by a narrow margin in 1978. Control of the canal was transferred peacefully to Panama in December 1999, and the Panamanians have been responsible for it ever since.\n7. The canal is being expanded to handle today’s megaships.\nIn 2007, work began on a $5.25 billion expansion project that will enable the canal to handle post-Panamax ships; that is, those exceeding the dimensions of so-called Panamax vessels, built to fit through the canal, whose locks are 110 feet wide and 1,000 feet long. The expanded canal will be able to handle cargo vessels carrying 14,000 20-foot containers, nearly three times the amount currently accommodated. The expansion project, expected to be completed in late 2015, includes the creation of a new, larger set of locks and the widening and deepening of existing navigational channels. However, while the new locks will be able to fit many modern ships, they still won’t be super-sized enough for some vessels, such as Maersk’s Triple E class ships, the planet’s biggest container ships, which measure 194 feet wide and 1,312 feet long, with a capacity of 18,000 20-foot containers.']	['<urn:uuid:c8e5bbb0-9af1-4ed4-a6da-cfb0e47628d5>', '<urn:uuid:6316290b-3891-4d09-bfd2-95045b4c7270>']	factoid	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-12T11:40:26.317848	11	90	2271
52	I'm confused about employee obligations when working for a company. What does it mean to act in 'good faith' at work, and what specific duties do high-ranking employees have compared to regular workers?	Acting in good faith means being honest, showing fidelity to the employment agreement, and not undermining the contractual benefits agreed upon. For regular employees, this involves basic duties of good faith and fidelity. However, senior employees or those responsible for sales and marketing may have additional fiduciary duties due to their position. The specific obligations depend on the employee's role and seniority - for instance, a regular employee like an associate dentist may not have special fiduciary duties, while a senior employee would have heightened responsibilities to protect the employer's interests.	['The express obligation to act in good faith has long been recognised as a feature of contract law across a number of jurisdictions in Australia and internationally. However debate continues as to its scope and application.\nIn North East Solutions Pty Ltd v Masters Home Improvement Australia Pty Ltd, the Supreme Court of Victoria recognised that an express obligation to act reasonably and in good faith to attempt to resolve differences in relation to the amount of Woolworths’ contribution of the cost of developing a Masters store was enforceable.\nAn express obligation to act in good faith is often used in contracts to impose a certain standard of behaviour on the parties in performing their obligations. It is commonly used in long-term supply contracts or joint venture operating agreements (typically seen in the energy and resources sector), teaming agreements, and relational contracts.\nAs a general proposition, an agreement to negotiate “reasonably” and “in good faith” is sufficiently certain to be enforceable. However, there is no clear judicial statement as to the definition or scope of the term in Australia, and what is considered good faith will inevitably turn on the facts of each case.\nThis was confirmed in Strzelecki Holdings Pty Ltd v Cable Sands Pty Ltd, where Justice Pullin held that the interpretation of the meaning of the words “good faith” is not determined by the parties’ subjective understanding, but by the objective meanings of the words which a reasonable person would have understood them to mean.\nFurther, Justice Pullin went on to note that in ascertaining the objective meaning, the contract must be considered as a whole, and the construction of the contract and interpretation of the words is determined by taking into account the objective background of the transaction or factual matrix of the contract. Any such determination though is not to include the subjective intention of the parties.\nIn Macquarie International Health Clinic Pty Ltd v Sydney South West Area Health Service, President Allsop described the “usual content” of the obligation of good faith as incorporating:\nObligations to act honestly and with a fidelity to the bargain;\nObligations not to act dishonestly and not to undermine the bargain entered or the substance of the contractual benefit bargained for; and / or\nObligations to act reasonably and with fair dealing, having regard to the interests of the parties (which will invariably conflict) and to the provision, aims and purposes of the contract, objectively ascertained.\nSimilarly, the duty of good faith is well established in the United States of America where a general implication of the duty is found in the Uniform Commercial Code (UCC) and the Restatement (Second) Contracts. The UCC defines good faith as ‘honesty in the conduct or transaction concerned’ and states that ‘every contract or duty within this Act imposes an obligation of good faith in its performance or enforcement.’ \nA number of European civil law jurisdictions also recognise an overriding principle of good faith which encompasses fair dealing. On the other hand, English courts have been more cautious about embracing such a term.\nIn North East Solutions Pty Ltd v Masters Home Improvement Australia Pty Ltd and Woolworths Limited, the parties entered into an arrangement whereby North East Solutions Pty Ltd (NES) agreed it would develop a Masters store for Woolworths at a particular site in Bendigo. Once the store had been developed, Woolworths would lease the store for a period of 12 years, with options for a further five terms of six years each.\nSo as to not to delay the development of the store whilst the suite of technical and design documents was being prepared, Woolworths agreed that NES should commence the development as soon as possible, and that because a precise price could not be ascertained without the suite of technical and design documents, Woolworths would contribute the difference between the estimated cost of building a Bunnings store and a Masters store on the site.\nThe arrangement also included a provision that allowed either party to terminate the arrangement if the parties, acting reasonably and in good faith, were unable to resolve any disagreement that arose in relation to Woolworths’ contribution of the construction costs (being the difference between the estimated cost of building a Bunnings store and a Masters store on the site).\nUltimately a disagreement did arise as to Woolworths’ contribution to the construction costs, and Woolworths terminated the agreement on that basis. NES commenced proceedings against Woolworths, arguing that Woolworths did not negotiate “reasonably” and in “good faith”. Woolworths countered by arguing that the relevant provision of the arrangement was not sufficiently certain to be enforceable, and was instead, merely an “agreement to negotiate”.\nJustice Croft found that, in the circumstances and having regard to the express terms of the arrangement entered into by NES and Woolworths, that the parties committed to act reasonably and in good faith in an attempt to resolve differences in relation to a cost estimate. The object of this process and of the clause itself was to enable the parties to quantify the difference in cost as between the development of the site as a Bunnings store and the development of the site as a Masters store, costs which Woolworths had already committed to pay.\nThe duty to negotiate reasonably and in good faith was therefore, in this instance, informed by those circumstances. As such, Justice Croft held that the express obligation to negotiate reasonably and in good faith which formed part of the arrangement agreed by NES and Woolworths was sufficiently certain so as to be enforceable.\nThe decision in the North East Solutions case is a timely and important reminder that when considering the application of the obligation of reasonableness and good faith, the interests of various parties in different sectors and industries will likely differ, and that the context in which the commercial relationship is based upon may dictate how the legal principles of reasonableness and good faith are applied.\nDespite some continuing debate as to the scope and application of the obligation to act in good faith, what is relatively clear is that where there is an express obligation under a contract to act reasonably and in good faith, all parties to that contract must adhere to the principles outlined above.\nUntil the High Court finally decides to determine the concept of good faith once and for all, parties to a contract should continue to be mindful of the possible implications when agreeing to take on an obligation to act in good faith. This may well require that party to honour the spirit of the “journey” until the destination is reached.\nNote: A follow up article will discuss how the North East Solutions case dealt with the quantification of damages. The authors acknowledge the assistance of Sarah Scully-Leaf, Summer Clerk, in preparing this article.\n  VSC 1\n See United Group Rail Services Ltd v Rail Corporation of New South Wales (2009) 74 NSWLR 618\n See Jobern Pty Ltd v Breakfree Resorts (Victoria) Pty Ltd & Ors  FCA 1066 (23 July 2007) \n  WASCA 222\n Ibid ; see also Toll (FGCT) Pty Ltd v Alphapharm Pty Ltd (2004) 219 CLR 165\n Ibid ; see also Australian Broadcasting Commission v Australasian Performing Right Association Ltd (1973) 129 CLR 99 at 109\n Ibid; see also Toll (FGCT) Pty Ltd v Alphapharm Pty Ltd (2004) 219 CLR 165. 179 -\n Ibid; see also DTR Nominees Pty Ltd v Mona Holmes Pty Ltd (1978) 138 CLR 423 at 429\n  NSWCA 268\n Ibid at , citing Renard Constructions (ME) Pty Ltd v Minister for Public Works (1992) 26 NSWLR 234; Hughes Bros Pty Ltd v Trustees of the Roman Catholic Church for the Archdiocese of Sydney (1993) 31 NSWLR 91; Burger King Corporation v Hungry Jack’s Pty Ltd (2001) 69 NSWLR 558, Alcatel Australia Ltd v Scarcella (1998) 44 NSWLR 349 and United Group Rail Services v Rail Corporation New South Wales (2009) 74 NSWLR 618\n UCC § 5-102\n UCC § 1-304\nThe content of this publication is for reference purposes only. It is current at the date of publication. This content does not constitute legal advice and should not be relied upon as such. Legal advice about your specific circumstances should always be obtained before taking any action based on this publication.', 'Date Published: 1 March 2012\nIn the area of employment law, a large number of cases brought by employers against their former employees dealt with the express restrictive covenants of the employment contract such as the non-compete, non-dealing and non-solicitation clauses. However, the existence of implied terms in employment contracts should not be overlooked. This update examines the law on the breach of implied terms in employment contracts in light of a recent decision by the Singapore High Court.\nSmile Inc Dental Surgeons Pte Ltd v Lui Andrew Stewart\nIn the case of Smile Inc Dental Surgeons Pte Ltd v Lui Andrew Stewart  SGHC 241 (“Smile Inc”), the plaintiff (“Smile”) had employed the defendant (“Dr Lui”) as an associate dentist. Some years into his employment, Dr Lui incorporated his own company, Dental Essence Pte Ltd (“Dental Essence”). A month after incorporating Dental Essence, he resigned from Smile to commence his own business in a location which is within 5 minutes’ walk from Smile’s clinic. Thereafter, Smile experienced a decrease in monthly revenue in respect of the branch that Dr Lui worked at prior to his resignation. Many of Smile’s patients subsequently requested for their dental records and left to become patients of Dental Essence.\nAs such, Smile brought an action alleging that\nDr Lui had breached the following:\n- various express provisions of his employment contract which restricted him from undertaking certain activities (“restrictive covenants”); and\n- the duty of good faith and fidelity and fiduciary duties, which are implied terms of the contract (“the implied terms”).\nNon-enforceability of the Restrictive Covenants\nThe Court first examined the express provisions of the employment contract which Smile alleged that Dr Lui was in breach of. The restrictive covenants in question were the geographical limitation clause, the non-solicitation clause and the non-dealing clause. The court found that it was unreasonable to preclude Dr Lui from soliciting patients he had not treated before or deal with Smile’s patients when he did not solicit for them. Furthermore, it was significant that all three clauses were unlimited in the duration of the restriction. Accordingly, the restrictive covenants were void and unenforceable on the basis that they were unreasonable in reference to the interests of the parties.\nAs the Court found the restrictive covenants to be unenforceable, the Court went on to deal with the implied duties of good faith, fidelity and fiduciary duties owed by Dr Lui to Smile.\nBreach of the Implied Duties\nSmile claimed that the following acts or omissions by Dr Lui were breaches of the implied terms of his employment contract while he was still employed with Smile:\n- setting up Dental Essence\n- signing a tenancy agreement for the premises of Dental Essence;\n- fitting out the premises of Dental Essence;\n- discussing/negotiating with Dr Pearson resulting in Dr Pearson becoming a shareholder and director of Dental Essence, with a view to Dr Pearson working as a dentist at Dental Essence;\n- applying a licence from the MOH to practice dentistry with Dental Essence;\n- failing to inform Smile of all or any of the foregoing matters; and\n- failing to obtain Smile’s consent to the setting up of Dental Essence.\nDr Lui argued that these steps were merely preparatory steps and he was entitled to compete with Smile after terminating his employment with Smile. As such steps were merely preparatory acts, Dr Lui argued that they did not constitute breaches of fiduciary duty. Smile on the other hand, took the position that even if the steps could be considered preparatory, Dr Lui had breached the implied terms.\nCourt’s Decision in Smile Inc\nThe Court considered several authorities on this point and concluded that Dr Lui was not in breach of the implied duties. The Court distinguished between preparatory steps and actual competitive activity. While an employee owed a duty of good faith and fidelity to his employer, the Court was of the view that it would not be a breach of such a duty where the employee merely took steps to prepare to compete with his former employer.\nOn the issue of whether there was a breach of fiduciary duty, the Court refused to impose any fiduciary duty on Dr Lui. It was of the view that there was nothing special about Dr Lui’s employment which gave rise to fiduciary duties. As such, reasonable restrictive covenants would have sufficed to mitigate the effect of Dr Lui’s influence and rapport with patients. As there was no finding of the existence of any fiduciary duty, it was unnecessary to consider the issue of breach.\nThe Intention and Plan to Compete\nThe Courts maintained that an employee had the right to plan his future before his departure, even if this means competing with the employer. The intention to compete is not sufficient to constitute a breach of an employee’s fiduciary duty and merely making a decision to set up a competing business at some point in the future and discussing such an idea with friends and family would not itself be in conflict with the best interests of the company and the employer. As such, the non-disclosure of the intention to compete does not constitute a breach of an employee’s fiduciary duty.\nMere preparatory acts to compete are insufficient to constitute a breach of the implied duties of good faith and fidelity. An employee may prepare a place for his business, employ others in such preparation, obtain a lease of premises and order materials in preparation to compete against the plaintiff company.\nIn Smile Inc, taking up a lease of premises, committing to renovating the premises, acquiring equipment before resigning from Smile and approaching a former dentist from Smile to join the clinic that he was setting up was not found to go beyond the usual preparatory steps in setting up a company.\nActive Steps to Compete\nHowever, when an employee takes steps to actively compete with the employer, such acts would amount to a breach of fiduciary duty. Acts that amount to active competition include the acts of soliciting the employer’s customers and diverting business opportunities. The misuse of confidential information which constitutes trade secrets are also breaches of the employees’ fiduciary obligations and their implied obligation of fidelity.\nPosition and Job Scope of the Employee\nAn employee’s position and the role in which he plays in relation to soliciting customers for the employer may be an important factor in deciding whether fiduciary duties arise in the first place. In Smile Inc, since Dr Lui was employed merely as an associate dentist and was not obliged to actively find new patients for Smile, the Court did not find anything special about Dr Lui’s employment by Smile which would give rise to fiduciary duties and therefore, there could not be any breaches of fiduciary duties.\nHowever, where an employee holds a senior position or his position1 has a bearing on the employer’s relationship with clients, the Court may find breaches of fiduciary duty and a duty to serve the employer faithfully. This may be found where, for example, the employee’s responsibilities include sales generation and marketing.\nImplied terms form an integral part of all contracts today, including employment contracts and the Courts will imply terms into contracts to give business efficacy to contracts made between parties.\nIn the context of employment contracts, an employee breaches the implied terms of good faith and fidelity when the employee takes active steps to compete before the cessation of employment, but not for preparatory steps taken. In certain cases, an employee also owes fiduciary duties to the employer, depending on the employee’s seniority and the employee’s position in the company. In the event that the employee owes fiduciary duties to the employer, setting up a competing business may constitute a breach of fiduciary duties.\nWhile breaches of restrictive covenants commonly form the cause of action for employers pursuing a case against their former employees, the breaches of implied terms of the contract can find a cause of action as well. The implied duties cast a wider protective net in safeguarding the employer’s interests beyond the four corners of the employment contract. This cause of action would be particularly applicable where there is no restraint of trade clauses in the employment contract or where the restraint of trade clause would be unenforceable due to unreasonableness.\nIn Smile Inc, the plaintiff founded its claim on both the restraint of trade clause and the implied duties under the contract. After the Court in Smile Inc found the restraint of trade clause to be unreasonable, the Court went on to consider the claim on the breach of implied duties under the contract. Even though the Court ultimately arrived at a negative finding on the claim under the implied duties, the case highlights the viability of such claims. Smile is currently appealing the case to the Court of Appeal and it is hoped that the Court of Appeal would shed more light into claims in the area of implied duties under employment contracts.\n Universal Westech (S) Pte Ltd v Ng Thiam Kiat and others  3 SLR(R) 429 at .\n Laughton & Hawley v BAPP Industrial Supplies Ltd  ICR 634 at 638, University of Nottingham v Fishel  ICR 1462 at \n Shepherds Investment Ltd v Walters  EWHC 836 (Ch) 5 at \n British Midland Tool Ltd v Midland International Tooling Ltd  2 BCLC 523 at \n Ng Thiam Kiat and others v Universal Westech (S) Pte Ltd and another appeal  2 SLR(R) 439\n Supra note 1 at 15\n Balston Limited & another v Headline Filters Limited & another  FSR 330 at 340\n  SGHC 241 at \n Supra note 4\n In Mona Computer Systems (S) Ptd Ltd v Chandran Meenakumari  1 SLR 310 at , the High Court also noted that the seniority of an employee’s position would give rise to fiduciary duties.\n Pacific Autocom Enterprise Pte Ltd v Chia Wah Siang  3 SLR(R) 73\nDisclaimer: This update is provided to you for general information and should not be relied upon as legal advice.\nEmployment disputes are usually sensitive in nature, emotionally charged and can affect the reputation of the parties involved. While knowledge of the applicable employment laws and regulations is important, we recognise that empathy is key to being effective in this area of practice and we have considerable experience advising a range of clients, including listed and private companies and directors, on a spectrum of employment disputes, such as breaches of contract, wrongful dismissals, breach of fiduciary duties, and the enforcement of non-compete covenants.']	['<urn:uuid:e1db93d5-4430-42d4-9607-ff984599b1c1>', '<urn:uuid:cd0e3a45-d059-4dcb-9d8a-5ac65f54649b>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T11:40:26.317848	33	91	3126
53	casual goodbye bai bai meaning chinese business culture formal meetings	拜拜 (bài bài) is the most casual and common way to say goodbye in Chinese, often used among friends, colleagues and shop assistants. However, in formal business meetings in China, casual expressions should be avoided as rank and formality are extremely important in business relationships. Business interactions must remain professional, and treating Chinese business associates too informally, especially in front of their peers, can ruin potential deals.	['It will only take 6 minutes to read this post!\nif you don’t have time to read this article now, you can download How to Say Goodbye in Chinese PDF and read it later!\nWe’ve already looked at some of the different greetings in Chinese, so now it’s time to learn some commonly used farewells. You may have already come across the transliteration ‘bye bye’ that is used frequently in China. However, there are many other useful ways to say goodbye that you can learn too.\nI have often found that when spending time with Chinese friends, that leaving a restaurant or going home happens very quickly and there is very little hanging around.\nSo, with this in mind, learning some of these different ways to say goodbye in Chinese will help you avoid any awkward farewells!\n10 Ways to Say Goodbye in Chinese\n1. 拜拜 (bài bài) Bye bye\nThe most common parting phrase you’ll come across is the transliteration of ‘bye’, 拜拜 (bài bài). It is an extremely casual phrase, but can be used amongst friends, colleagues and even shop assistants.\nYou may know that there are lots of homophones within the Chinese language, and 拜拜 (bài bài) can be written as ‘88’ because in Chinese the number 8 is pronounced bā (sounds a bit like bye ?). Another reason is that 88 also look like 白白, which are pronounced bái bái.\nYou can learn more about Chinese homophones in this article.\nIn many of the following farewell words and phrases, you will see a character repeated frequently. The character 再 (zài) means ‘again’. Originally, the 再 (zài) character represented the wheel that was turning ‘again and again’.\n2. 再见 (zài jiàn) Goodbye\n再见 (zài jiàn), is a more formal way to take leave, and I hear this much less frequently than 拜拜 (bài bài). It literally translates to ‘again to see’, and although 再见, is often taught as ‘goodbye’, it’s translation puts it closer to meaning ‘see you again’.\n再会 (zài huì) Goodbye/Till we meet again\n再会 (zài huì) is similar to 再见 (zài jiàn), but is often used in business situations. It also has a history of being used within ancient Chinese poetry.\n3. 我走啦 (wǒ zǒu la) I’m going\n我走啦 (wǒ zǒu la), is often shortened even further to just ‘走啦’ (zǒu la) and means ‘I’m going’. The personal pronoun of 我 (wǒ) – I/me can be removed in spoken Chinese, as it already implied that it is ‘I’ who is leaving. This phrase has a different context than to say goodbye, but is frequently used to anticipate leaving a place.\n4. 我得走啦 (wǒ děi zǒu la) I have to go\nThis addition of the 得 (děi) character (not to be confused with the particle 得 (de), implies that the speaker must leave. This is a useful phrase to use if you have another engagement you need to be at.\n5. 明天见 (míng tiān jiàn) See you tomorrow\nUse this phrase if you’ve planned to see your friend the next day. You can also say 下次见 (xià cì jiàn), which means ‘see you next time’. This suggests you’ve not set a specific date to meet again and be used more freely.\n6. 慢走 (màn zǒu) Take care\nWhen you leave a taxi, a restaurant or even a shop, the taxi driver or waiting staff, will almost certainly use this phrase. We translate it to mean ‘take care’, but the literal translation is more like ‘slowly leave’ or ‘take your time’. Taking your time over things is certainly a cultural trait. It’s common to hear other similar phrases such as 慢慢吃 (màn màn chī), or take your time eating.\n7. 有空再聊 (yǒu kòng zài liáo) Let’s talk again when you have time.\n8. 下次再约 (xià cì zài yuē) Let’s do this again sometime.\nSimilarly to the previous phrase, 有空再聊 (yǒu kòng zài liáo), this is used between friends or acquaintances, to suggest a repeat meeting or party. The first two characters 下次 (xià cì) actually translates to mean ‘next time’. Another phrase that be used in a similar way is 有空再约 (yǒu kòng zài yuē) which means “Let‘s go out again when you have time.”\n9. 下次再来玩 (xià cì zài lái wán) Come again next time\nIf you want to invite your friend to your home or business again, you can use this phrase. You will often hear and see the character 玩 (wán) being used when a Chinese friend or colleague asks you to do something with them. Since the word translates as ‘play’ in English, it may seem a little strange to have your grown up friend as you to ‘go out and play’, but this is just a way or suggesting whatever you will do together will be fun or amusing.\n10. 有时间来玩 (yǒu shí jiān lái wán) Come back when you have time\nThis is another similar phrase, with the phrase ‘when you have time’ 有时间… (yǒu shí jiān) replacing the previous ‘next time’. You can replace 有时间… (yǒu shí jiān) with 有空 (yǒu kòng) to make 有空来玩 (yǒu kòng lái wán) to say ‘when you have free time come back again’. The character 空 (kòng) means ‘free time’, whereas 时间 (shí jiān) is a more general term for ‘time’.\nIf you have any other ways to say goodbye in Chinese, please share them with us in the comments section below!', '1.3 billion people – the largest in the world, 9.6 millions km² of land, vast amounts of valuable natural resources such as coal, oil, and minerals, and the leading consumer of four out of five core commodities; grain, meat, coal, and steel. China’s status is no longer that of a developing country but one of an emerging economic superpower, one that is writing economic history and shifting the power balance from West to East. China’s rapidly growing economy, massive market, and cost effective business infrastructure, is drawing companies and conglomerates to the country to set up shop. And although the country is getting globalized it still has its own local business culture, business etiquette, meeting protocols, mannerisms, etc.which need to be followed.\nIn this article we will discuss and explore certain cultural facts and how they influence business culture and etiquette.\nChinese follow the rule of Confucianism, which revolves around the concept of harmonious relationships, i.e obligations of people towards one another based upon their relationship. In particular Confucianism emphasizes duty, sincerity, loyalty, honour, filial piety, respect for age and seniority. Confucianism permeates every single fabric in society, including business practices. Although subtle, the manifestations are evident: an aversion to conflict, maintenance of proper demeanour and the preservation of ‘face’. The concept of ‘face’ roughly translates as ‘honour’, ‘reputation’ or ‘respect’ and is extremely important to Chinese people. It is essential that one gives face, saves face and shows face when doing business in China.\nGreetings Greetings are part and parcel of any business meetings. In China, meetings start with the shaking of hands and a slight nod of the head. A firm but not overly vigorous handshake is recommended when shaking hands, anything more will be interpreted as being aggressive. The Chinese are not keen on physical contact – especially in a business setting. Even if you are familiar with the person, avoid slapping, patting or placing your arm around someone’s shoulders. In a business meeting, one should always be calm, collected and controlled. Watch out for your body posture and try not to slouch or look listless. A formal and attentive posture tells your associates that you have self-control and are worthy of respect. Business cards are usually exchanged on an initial meeting. Make sure one side of the card has been translated and if possible print the Chinese letters using gold ink as this is considered an auspicious color and reflects well upon your company. When giving your business card, mention your company, rank and any qualifications you hold, making sure you give and receive any business card with both hands. Rank is extremely important in business relationships and you must keep rank differences in mind when communicating. Treating them too informally, especially in front of their peers, may well ruin any potential deals.\nBuilding Professional Relationships\nOne essential thing to remember when conducting business in China is that you are seen as a representative of your company therefore any interaction or dealings has to be professional. Never become too informal and avoid humour. Rather than a lack of humor of the Chinese’s part, it is the fact that jokes may be lost in translation and hence be awkward, redundant and even offensive (unless you speak very good Mandarin!) When doing business in China, it is important to engage an intermediary. As the Chinese don’t like doing business with companies they don’t know, the role of the intermediary is vital. This could be an individual or an organization who can make a formal introduction and vouch for the reliability of your company. In addition, the intermediary get act as an interpreter and navigate you through the bureaucracy, legal system and local business networks.\nGiving Gifts Etiquette While this practice is considered a grey area in many countries and frowned upon in some, the giving of gifts in a professional business setting does not carry any negative connotations. The Chinese believe that gifts should always be exchanged for celebrations, as a thank you for assistance rendered, and even as a sweetener for future favors. However, it is important that gifts are only given for a good reason and in the presence of a witness as this may be construed differently in the absence of both. When the Chinese want to buy gifts, they will often be direct in asking what you would like – this is common. It is not considered bad manners to specify something you desire. That being said, it would be wise to demonstrate an appreciation of Chinese culture by asking for items such as traditional Chinese ink paintings or Chinese tea. Business gifts are always reciprocated in kind as they are seen as debts that must be repaid. When giving gifts do not give cash as it is too easily misconstrued as a bribe. The gifts need to be items of worth or beauty. Do not be too frugal with your choice of gift otherwise you will be seen as a cheap and tight person.\nMeetings and Negotiations At the risk of stating the obvious, meetings must be schedule well in advance. It is considered good manners to forward some literature regarding your organization as a form of introduction. The ideal period for meetings are between April – June and September – October. Avoid all national holidays especially Chinese New Year. As punctuality is correlated to respect, it is important that you are not late. In fact getting to the meeting venue before the other party does sets a good tone for the meeting. Do not dive straight into your pitch, numbers, presentations etc. instead start with some brief small talk. If this is your first meeting then talk of your experiences in China so far. Keep the conversation positive (i.e no complains about food, weather, sleep etc as it’s considered bad form) and avoid anything political. Prior to any meeting, you should always send an agenda. This will allow you to have some control of the flow of the meeting as the Chinese approach meeting very differently. The main topic is discussed first followed by the side issues, so be prepared. Renowned for being tough negotiators, the Chinese’s primary aim in negotiations is ‘concessions’. Always bear this in mind when formulating your own strategy. Be prepared to show compromise and make them feel they have gained major concessions. For the Chinese, business negotiations are like going to war. They will plan meticulously and will know your business and possibly even you inside out. It’s like the ‘three kingdoms’ all over again but in business. One favorite move of Chinese negotiators is to show false humility and deference at the start, designed to make them look vulnerable and seemingly weak, this move exploits any weakness or ‘holes’ in the other camp.']	['<urn:uuid:7aaebd0c-e5c3-4f1b-b7d0-752f1d81eea4>', '<urn:uuid:4a2937eb-9fe3-4b23-bc8b-805d43d9b058>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T11:40:26.317848	10	67	2026
54	What are the key differences between how Evergage handles triggers for user actions versus how triggers are conceptualized in the Fogg Behavior Model?	Evergage and the Fogg Behavior Model handle triggers differently. Evergage implements triggers through 'Triggered Experiences' that display website messages, send one-off emails, and push notifications based on visitor actions. In contrast, the Fogg Behavior Model presents a more theoretical framework with three specific types of triggers: Sparks (which include motivational elements), Facilitators (for high motivation but low ability), and Signals (for when both motivation and ability are high). The Fogg model emphasizes that triggers must be perfectly timed and matched to users' motivation and ability levels to be effective, while Evergage focuses more on the technical implementation of triggers across different channels. Both approaches recognize the importance of timing and context, but Evergage's approach is more focused on practical implementation while the Fogg model provides a theoretical understanding of how triggers work psychologically.	"['Growing and enriching customer profiles is fundamental to business success.\nIn a Nutshell:\nThis is one of a series reviewing vendors’ personalization platforms.\nEvergage is a personalization and customer data platform used for delivering personalized customer experiences. The Evergage personalization solution is comprised of testing, optimization, recommendations, analysis, segmentation and targeting across digital channels.\nCustomer Data Platform (CDP). The breadth and depth of data collected by Evergage and imported from other apps and systems create a rich customer profile, and for B2B, account profiles as well. Evergage provides packaged integrations and APIs for exchanging data, and UI wizards for marketers to control that data exchange.\nArchitecture. Evergage functions are built on the CDP rather than the unfortunately common practice of building functions around channels. The functions have been built by Evergage, resulting in unified and consistent services.\nSupport Resources. Evergage has a comprehensive online library which provides guidance on digital marketing activities as well as how to use Evergage. The partner network is extensive, including specialty and big four consulting firms. Customer support representatives I have spoken with are knowledgeable and experienced.\nData science and AI. Evergage has incorporated machine learning into its recommendations and analyses, one of which identifies anomalies in events in order to alert marketers to problems. The Data Science Workbench provides data scientists with the environment and tools to explore, model and enrich CDP data.\nOverview of Evergage Capabilities\nEvergage views personalization as the fundamental principle driving customer experience. It is not a capability that can be added to a marketing platform, it is the foundation of a marketing platform. “Personalization is not a feature of the interaction, it is the whole thing.” ——Karl Wirth, Evergage Co-Founder and CEO.\nEvergage offers a platform that it believes satisfies clients’ requirements for delivering personalized customer experiences.\nEvergage categorizes its capabilities in six areas:\n1. Behavior and Context Tracking and Surveying\n- Monitor engagement & time spent to determine affinities\n- Understand product & content metadata without catalog feed\n- Capture explicit data via targeted surveys\n2. Segmentation and Targeting\n- Segment on data from 1st and 3rd party sources\n- Apply rules to change experiences for segments\n- Analyze segments for insights\n3. Triggered Experiences\n- Display website messages based on visitor actions\n- Send one-off email messages\n- Communicate with users via push notifications\n4. Algorithmic Experiences\n- Apply machine learning for 1:1 experiences\n- Recommend products, content, categories and other elements of customer experience\n- Personalize web, email, onsite search and navigation\n5. A/B and Multivariate Testing\n- Conduct A/B & multivariate tests\n- Test algorithms and specific segments\n- Test email subject lines\n6. Analysis and Attribution\n- Attribute against goals\n- Analyze lift over control\n- Bayesian statistical analysis\n- Utilize machine learning to guide decisions\n- Report on non-Evergage campaigns\nEvergage primarily targets large and enterprise retailers, technology providers and financial services companies, but also works with businesses in the manufacturing, travel, gaming, media and education industries.\nChallenges Faced by Target Market\n- Driving more engagement and conversions. Companies spend significant time and money on top-of-the-funnel campaign activities – ads, email, SEO, etc. – designed to drive people to their websites. However, it’s wasted if those visitors immediately bounce from the site or don’t engage or convert. Businesses need to employ creative methods, including personalization and A/B testing, to optimize the experiences for their visitors and email recipients to improve engagement and conversion rates.\n- Managing customer expectations. Competition is fierce so companies today are looking for any advantage they can find or create. Increasingly, this comes from creating and delivering exceptional customer experiences – those that recognize customers, understand their unique needs/desires and make their experiences more helpful and efficient. If your company does not deliver as compelling an experience as Amazon, Facebook, and Netflix, your visitors will become increasingly frustrated and look to go elsewhere.\n- Leveraging customer data. A key challenge today for many companies is how to gather and interpret digital visitor behavioral data, how to unify disparate data sources, and how to utilize all that information to deliver relevant and consistent customer experiences. Businesses have lots of customer data but the information is often siloed across many different systems and applications, and not all information is equally valuable. Companies need a way to unify valuable customer data so that it can be used to build and improve relationships. The emergence of Customer Data Platforms (CDPs) is a result this need.\n- Cross-channel consistency. Marketing technology stacks tend to include many application-specific tools. As it applies to optimization, a typical retailer could rely on an A/B testing tool, a product recommendation solution, a mobile app messaging tool, and a geo-targeting tool, among many available tools. Each of these solutions are designed to help companies improve customer engagement, but within a limited scope of the overall relationship with a customer. To be successful, companies need to deliver consistent experiences across many different touch points, which is difficult to do when using a variety of point solutions.\nSolution Strengths Against the Challenges\nAgainst these challenges, Evergage’s strength lies in its comprehensive CDP and the architecture that centers on customers rather than channels. Evergage is a personalization and customer data platform (CDP) that can gather, unify and interpret customer data, and it is this data that drives personalization. Evergage captures details such as active time spent on page and on-page engagement like hovering and scrolling details, and also imports data from other sources. This data is evaluated against the product and content contextual data (such as color, brand, price, etc.), to provide a more accurate sense of someone’s implied preferences. In addition, Evergage also offers out-of-the-box surveying capabilities to enhance or confirm data about visitors.\nThe Evergage platform contains a customer profile record for every visitor. There are six categories of data, as follows:\n- Situational data. Geographic location, referring site, campaign source, device, browser\n- Firmographic data (for B2B). Company, industry, revenue, headcount, marketing technologies used\n- Lifecycle data. First time or returning visitor, active prospect, current customer, loyal customer\n- Affinities and intent. Content consumed, videos watched, blog posts read, feature and solution preferences, favorite brands and styles, price affinity, recency and frequency\n- Profile data. CRM/MAP data, job role if B2B, demographic information, marketing and campaign responses, offline purchase data, marketing responses\n- Account-level data: In addition to tracking insights on every visitor, Evergage tracks detailed engagement data at the account level, which is critical for B2B companies in industries like technology, retail and financial services.\nEvergage has been entirely engineered in house, which means that all performance improvements are identifiable and in their control. Evergage claims to react to visitors in 20 ms, which is fast enough to support real-time.\nEvergage offers three categories of machine learning capabilities: affinity modeling which scores visitor interest, white-box recommendations which give marketers control, and machine learning anomaly detection which alerts marketers to potential problems.\nThe Data Science Workbench offers tools and data within a dedicated cluster that data scientists can use to explore and model CDP data. This pre-built environment, by providing access as well as computing workspace, eliminates significant barriers data scientists must overcome before beginning any value add analysis.\nEvergage provides an online library to support customers’ need for new skills and guidance, including an extensive knowledgebase and playbooks that guide marketers in planning and implementing personalization campaigns. The information is rich enough to help people at any skill level figure out what to do as well as how to use Evergage to do it. The customer support representatives I have met are articulate, knowledgeable and experienced.\nBecause the effort to create, deliver, and manage personalized customer experiences engages people with many roles across many departments in a company, it is important that the personalization solution encourage collaboration. The top three requirements are support for many roles, immediately accessible learning, and enterprise controls.\nSupport for many roles. On average, there are six Evergage users per account. Larger organizations have upwards of twenty users. Roles and titles involved with Evergage include:\n- CMO/SVP/VP Marketing\n- Digital Marketing Demand Generation\n- Product Management\n- Customer Success\n- Email Marketing\n- Mobile App Development\n- Information Technology\n- Data Analysis/Data Science\nEvergage works with businesses across many different industries, thus end users’ titles can vary from company to company.\nImmediately accessible learning. Evergage has a comprehensive library for learning how to be successful with marketing activities, using Evergage. In addition to the online library, resources include:\n- An assigned Customer Success Representative\n- eCampus courseware\n- Online knowledgebase\n- Playbooks for planning and implementing personalization campaigns\n- Webinars and events\n- Industry strategist-led planning workshops\n- Consulting partner network\nEnterprise controls. Platform administrators can assign roles to specific users (e.g., viewer, campaign editor, etc.) so their permissions adhere to company guidelines. Furthermore, for every personalization campaign built within Evergage, a company can define and then follow workflow approval processes to ensure quality assurance and appropriate management oversight.\nPersonalization is not achieved with a tool bolted onto your marketing environment. It is achieved with a broad range of capabilities embedded in most of what marketing does. Integration thus becomes fundamental to the value of any personalization solution. The top three requirements are customer data integration, app integration, and accessible services.\nCustomer data integration. Evergage has packaged, bi-directional connectors with Oracle Eloqua, Salesforce Marketing Cloud (ExactTarget) and Marketo. Campaign, field and segment data can be passed back and forth between the Evergage platform and other systems based on your configuration. Configuration is performed via a tab in the Evergage UI, where you specify authentication and segment, field, and campaign synchronization.\nApp integration. There are 25 packaged integrations for sharing of visitor and campaign details between systems available for CRM, email marketing and web analytics solutions, including Salesforce Sales Cloud (CRM) and multiple ESP/MAP solutions like Oracle Eloqua, Salesforce Marketing Cloud (ExactTarget), IBM Watson Marketing (Silverpop), Google Analytics and Marketo.\nIntegration with other systems is achieved with Evergage APIs. These APIs facilitate import of transactional and other customer data to enrich the CDP, and also the export of Evergage’s behavioral data.\nAccessible services. From a single interface in the Evergage platform, a business user can build and deploy personalization campaigns across websites, mobile apps, web applications and emails.\nAI and Automation\nLeaders in delivering personalized customer experiences are developing the capability to personalize any part of any customer interaction. Personalization at scale is a challenge that can’t be accomplished unless AI is effective at predicting customer reactions to each step in the experience, and automatically presenting the best next step. The top three requirements are control and automation; shared insights; and real time actions.\nControl and automation. Business users can choose to recommend products, content items, categories, brands, departments, etc. Evergage’s recommendations can be controlled by creating machine learning recipes. Depending on the strategy, a recipe built by a marketer can include one or more algorithms, filters, boosters and variations. Recipes can be previewed, tested and deployed to drive recommendations on a website, in a mobile or web app, in onsite search results, and into email campaigns.\nShared insights. Evergage Guardian is a feature of the platform that uses machine learning to monitor a company’s analytics data and then surface any anomalies, either positive or negative, that deviate from an AI-predicted pattern.\nAI decisions powered by Evergage can be fed to call centers and in-store or in-branch systems to provide customer-facing staff with real-time, relevant suggestions and recommendations they can offer on the spot to individual customers/prospects.\nEvergage-driven recommendations can also be incorporated into websites, onsite search, mobile apps, web applications, and email.\nEvergage’s affinity modeling uses machine learning to evaluate and score a visitor’s interest in product/content items based on the visitor’s profile and behavior, and the many attributes that are associated with each item. This information is available to apps via the CDP.\nEvergage’s Data Science Workbench provides tools for data scientists to explore CDP data, create visualizations, execute data transformations, and run numerical simulations and statistical models. Output from these analyses and models can, in turn, be brought back into Evergage as profile attributes so those insights can be used for improving real-time personalization efforts. Users of the Data Science Workbench are provided with a dedicated cluster, where they can access Evergage’s data through a safe and secure read-only proxy. The cluster is pre-installed with a suite of familiar tools which run on top of Apache Spark. Apache Zeppelin provides a notebook in which Python, R and Scala can be used together, sharing data across languages. Additionally, libraries allow for data to be pulled from Evergage into Spark DataFrames in a clear and well-documented manner.\nReal time actions. Evergage models are updated in real time.', ""Creating Persuasive Technologies: An Eight-Step Design Process + A Behavior Model for Persuasive Design Presentation Transcript\nAn Eight-Step Design\nProcess /A Behavior Model\nfor Persuasive Design\n+ Persuasive 09\nx 2013 Autumn\nCREATING PERSUASIVE TECHNOLOGIES:\nAN EIGHT-STEP DESIGN PROCESS (PERSUASIVE’09)\nStanford Persuasive Technology Lab.\n컴퓨터를 사용해서 우리가 생각하는 방식, 행동하\n는 방식을 변화 시키는 것\n(Using Computers to Change What We Think and Do)\nthe foundation for captology.\nthe founder and director of Stanford's\nannual Mobile Health conference.\ncreated a new model of human\nIn 2011, the World Economic Forum's Wellness\nWorkplace Alliance selected the Fogg Behavior\nModel as their framework for health behavior\nPersuasive technologies are ubiquitous\n웹 서비스 (from commerce sites to social networking)\n비디오 게임 (e.g., Wii Fit and Dance Dance Revolution)\n모바일 (e.g., health applications for iPhone and commercial texting services)\nspecialized consumer electronic device, from “talking” pedometers to bathroom scales that\ntrack body mass.\n“완전히 새로운 persuasive technology 서비스를 개발하고 싶다.”\n많은 사람들이 범용적으로 사용할 수 있는 잘 정리된 persuasive technology 디자인 프로세스의 부재\nThe lack of a well-deﬁned process for designing persuasive technology leads people to adapt\nmethods from other ﬁelds,\nfrom “Best Practice”\ntechnology has been succes\naudience. Until then, I ad\naudience. For example, if th\npersuade users to adopt bette\naudience who has demonstrat\nteam wants to persuade peop\ndesigners will increase their\npeople who already exercise\nDesign teams have so many t\nnew persuasive technology th\nIn fact, choosing the wrong\nthe design project, especially\nin later steps, once a design\nthat is working, they will be\nand bring in users who are les\n8 Steps in\nEarly stage persuasive design\nThe next consideration in ch\npeople are with technology. I\nother adventurous souls as\nmistake to target an audience\nbeginning to use the techno\nbuilding a persuasive techno\nor interactive TV. The best a\nthose who enjoy using techno\nIn some cases, the first two\ncompleted in reverse ord\ndetermine the target behavior\na project to motivate teens\ndifferent behavior (e.g., gett\nproject to persuade older ad\namount to ensure a secure ret\na team to back up to Ste\ncombination of behavior a\nfoundation for the subsequen\nThe goal of the Steps 1 through 7 of the design\nprocess is to create a digital product that reliably\npersuades someone—not everyone— to adopt\nthe target behavior.\nStep 3: Find what pr\nOnce a design team has se\naudience to target, it’s time t\nteam must determine what\nperforming the target behav\ngrade aren’t brushing their t\nAs another example, if alumn\nThe answers to such question\nof the following three categor\nFigure 1: Eight steps in early-stage persuasive design\nlack of motivation\nStep 1. Choose simple behavior to target\n가장 작고, 심플한 행동을 타겟하라\n작은 목적을 이루는 것으로 부터 시작하라\nAchieving the small goal may have bigger effects than expected;\npersuasion professionals have long understood that getting people to\ndo small things naturally leads to their adopting more ambitious\nbehaviors, even without a bigger intervention.\n좀 더 건강해지자\nToo Big and Vague\n경제적인 생활을 하자\nStep 2. Choose a receptive audience\n가장 쉬워 보이는 타겟을 골라라\n올바른 식단 습관 -> 식단 습관을 개선할 욕구를 보이는 사람\n올바른 운동 -> 한두번이라도 건강을 위해 운동을 해 보았던 경험을 가진 사람\ntechnology channel과 친숙한 타겟을 골라라\nearly adopters 군\n새로운 기술과 서비스를 시도해 보는 것을 좋아하는 집단\nhelping compulsive gamblers to stop\nStep 3: Find what prevents the target behavior\naudience 들이 타깃 행동을 하지 못하는 이유를 규명하라\n아이들이 이 닦기 싫어하는 이유 ?\nalumni들의 기부금액이 적은 이유?\nlack of motivation\nlack of ability\nlack of a well-timed trigger to perform the behavior\n하나의 이유를 해결하는 데 집중하라\nMotivation과 Ability가 모두 부족한 경우 -> 다시 step 1, 2로 돌아가야한다\nMotivation과 Ability를 동시에 변화시키는 것은 거의 불가능\nStep 4: Choose a familiar technology channel\nBest Channel <- target behavior / the audience / what is preventing the\naudience from adopting the behavior\n선택 가능한 옵션\nMotivation이 부족할 때 : online video, social networks, and video games,\nAbility가 부족할 때 : installed software and specialized devices, excel at making a behavior simpler (which is\nfunctionally the same as increasing ability).\nTrigger가 제대로 역할을 하지 못할 때 : Text Msg, E mail\n정치 정당에서 기부금액을 높이고자 할 때 -> Web (결재)\n친구와 메시지를 주고 받을 수 있도록 할 때 -> Email/ 비디오 / SNS\n새로운 Channel을 학습하는 것을 기대하는 것 (X)\n새로운 Channel에 적응하는 것 자체 -> behavior change.\nRe-ordering the First Four Steps\naudience. For ex\npersuade users to\naudience who ha\nteam wants to p\ndesigners will in\npeople who alrea\nDesign teams ha\nnew persuasive t\nIn fact, choosing\nthe design projec\nin later steps, on\nthat is working,\nand bring in user\nThe next consid\npeople are with\nmistake to target\nbeginning to us\nbuilding a persu\nor interactive TV\nthose who enjoy\nIn some cases, t\nStep 5. Find relevant examples of persuasive technology\n관련 있는 성공 사례들을 수집\n같은 communication channel\nStep 6. Imitate successful examples\nStep 8에 이르러 unique함을 추구할 수 있음\nThe opportunity for real innovation comes after laying a solid foundation.\n“secret sauce” – the special ingredient that makes the example\nStep 7: Test and iterate quickly\ntest various persuasive experiences quickly and repeatedly.\nnot scientiﬁc experiments\nDesigning for persuasion is harder than designing for usability.\nset low expectations for their trials\nknowing how to prototype, test, and evaluate results quickly is the most\nvaluable skill for designers of persuasive technology.\nmany crummy trials beats\nStep 8: Expand on success\n프로젝트의 규모를 키우는 단계\ntarget behavior 더 어렵게 / 다층적으로 만드는 것\n새로운 Audience 군에게 적용\n새로운 Channel의 활용\nthe starting point for a controlled and scientiﬁc experiment\n-> Measure up\nEVERYTHING BIG STARTED SMALL\nsuccessful consumer Internet services : Each service started in a small, focused\nGoogle offered a simple search box.\nYahoo was merely a list of links.\nFacebook was a directory created for friends.\nAs the small offerings succeeded, they expanded.\nThat approach to innovation works.\nA BEHAVIOR MODEL FOR PERSUASIVE DESIGN\nFogg Behavior Model (FBM)\nbehavior is a product of three factors: motivation,\nability, and triggers\n사람이 Target Behavior을 이루기 위해서는\n행동을 수행할 수 있는 ability\n행동을 수행할 수 있도록 Trigger가 제공되어야 함\n이 모든 세가지가 동시에 충족되었을 때만 Target Behavior가 성취될 수 있음\nThe visualization in Figure 1 is not the only way to represent the\ncore concepts in the FBM. However, this form seems the most\nnatural and practical.\nusers the star would be located in the lower right part of the\nframework. This placement means that ability is high and\nmotivation is low. Other users, however, might really want the\nfree newsletter from the web site, so their motivation level would\nFigure 1: The Fogg Behavior Model has three factors: motivation, ability, and triggers.\nMotivation & Ability Can Trade Off\n두 축의 어느 것도 0에 수렴해 있으면 불가능\nIncreasing Ability = Making behavior Simple\nTriggers & Timing\nTriggers & Timing\n적절한 시간에 적절한 형태의 Trigger가 주어지지 않으면 Motivation과 Ability가 갖춰져 있더라도 행동은 일어나\n형태 : 알람, 문자메시지, 안내문, 꼬르륵 거리는 배\n“Hey, right now is a great time to play the ukulele!”\nTrigger의 인지 가능 정도 / Target Behavior과의 연결성\nMotivation이 낮은 경우 -> 짜증\nAbility가 낮은 경우 -> 실망감을 줄 수 있음\nIs there a way to reduce motivation?\nTo take away ability?\nIs there a way to remove triggers?\n어떤 행동을 하지 못하게 하는 것은 일반적으로 하게 하는 것보다 더 어렵다\nElements of Motivation\nMotivator #1: Pleasure / Pain\n즉각적이고 본능적인 보상과 고통 -> 생각의 여지가 없음\nhunger, sex, and other activities related to self-preservation and propagation of our genes.\nMotivator #2: Hope / Fear\n결과에 대한 기대\nHope : 좋은 일이 생길 것 같은 기대감 / Fear : 나쁜 일이 생길 것 같은 걱정\ndating web site / virus software\nMotivator #3: Social Acceptance / Rejection\n사회적인 인정과 기각\nElements of Simplicity (Ability)\n무조건 쉽고 간단하게 만들어라 (persuasive design relies heavily on the power of simplicity)\n1-click shopping at Amazon. Because it’s easy to buy things, people buy more. Simplicity changes behaviors.\nincreasing ability != about teaching people to do new\nthings or training them for improvement.\nPeople are generally resistant to teaching and training\nbecause it requires effort.\nElements of Simplicity (Ability)\nThree Types of Triggers\nSpark as Trigger : Motivation\nWhen a person lacks motivation to perform a target behavior, a trigger should be\ndesigned in tandem with a motivational element.\nFacilitator as Trigger\nThis type of trigger is appropriate for users that have high motivation but lack ability.\nLike sparks, a facilitator can be embodied in text, video, graphics, and more.\nSignal as Trigger\nThis trigger type works best when people have both the ability and\nthe motivation to perform the target behavior.\nFigure 2: All three factors in the Fogg Behavior Model have subcomponents.\n사용하고 있는 / 효과가 있었다고 느꼈던 Persuasive Design은?\n말은 쉬운데.... 막상 생각해보면....\nPersuasive Design의 범위는?\n올해가 가기 전에 이루고 싶은 행동 변화는?""]"	['<urn:uuid:79a4ead0-5bd9-4cc9-b34c-77a3a3c1d1a1>', '<urn:uuid:8f67ddad-02e9-4cdc-a6bc-47d04840092e>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T11:40:26.317848	23	133	3607
55	What security features do GPS III and eLORAN provide?	GPS III implements M-code, which is a more secure, harder-to-jam or spoof signal for military forces, with the GPS III SV03 being the 22nd M-Code enabled satellite. In parallel, eLORAN provides resilience through its much more powerful signal (about 1 million times stronger than satellite signals) and can maintain critical infrastructure like power grids, cell phones, and financial networks during GPS outages. Its low-frequency signals are very unlikely to be interfered with by the same causes that would disrupt GPS signals.	"[""Space Force Sends Additional GPS Capability into Space\nThe third launch of the GPS III system adds performance and protection.\nWith industry partners, the Space and Missile Systems Center of the U.S. Space Force launched the GPS III Space Vehicle 03, or SV03, on June 30 from Cape Canaveral Air Force Station, Florida. The launch sets the third GPS III satellite into space to join the SV01 and SV02 satellites, which were declared operational in January and April.\nThe GPS III system is the space portion of the Space Force’s effort to modernize the entire GPS capability, offered Tonya Ladwig, acting vice president, Navigation Systems, Lockheed Martin Space.\nSome of the military and industry leaders involved with the launch briefed the media on a call last week, including: Ladwig; Brig. Gen. Doug Schiess, USAF, commander, 45th Space Wing, U.S. Space Force; Col. Ed Byrne, USAF, senior materiel leader, Medium-Earth Orbit Space Systems Division, Space and Missile Systems Center (SMC), U.S. Space Force; Walter Lauderdale, mission director, SMC Launch Enterprise, U.S. Space Force; and Lee Rosen, vice president of customer operations and integration, Space X.\n“I'm proud to be a part of an incredible team with Space X, SMC and our satellite contractor to put this asset on orbit for our nation, for our Space Force and for the warfighters that need this capability,” Gen. Schiess said.\nCol. Byrne explained that the system will achieve three times better navigational accuracy and eight times more power, which helps with the anti-jam capability of the spacecraft. “And I know for the GPS III program, we are expecting an accuracy of 0.63 meters or better,” the colonel stated.\nAccording to Ladwig, GPS III adds that new technology and much greater capability to 29 of the 31 current satellites in the GPS constellation. The system supports M-code, “a more-secure, harder-to-jam or spoof signal for our military forces,” she said. It also adds a new L1C Civil signal. “Significantly, this satellite, GPS III SV03, will be the 22nd M-Code enabled satellite in the constellation—just two short now of global coverage,” Ladwig offered.\nThe greater GPS III team is delivering the capability to over 4 billion military and civilian users around the world.\nEarlier this year, the U.S. military celebrated the 25th anniversary of the GPS system reaching full operational capability, a fitting coincidence, Col. Byrne observed. “The launch of the GPS III SVO3 spacecraft is significant and marks another major milestone for the program, for SMC and the U.S. Space Force as we continue our journey to modernize and deliver new capabilities to the GPS Constellation,” he stated.\nThe COVID-19 pandemic interrupted the launch originally scheduled for April 29 and the parties put the mission on a 60-day hold to implement new health and safety measures, an SMC spokesperson said. “The men and women at 45th Space Wing are just doing an incredible job during unprecedented times with this coronavirus situation,” Gen. Schiess added.\nThe effort also marks the first launch of a GPS III production satellite and National Security Space Launch (NSSL) where the launch service provider attempted to recover the rocket booster, Col. Byrne noted.\nThe booster successfully landed back on SpaceX’s autonomous spaceport drone ship named, Just Read the Instructions II, positioned in the Atlantic Ocean. The 10-year old Falcon 9, with 88 missions completed, has flown the most operational missions in the United States, SpaceX said.\n“Approximately nine minutes after [liftoff] the first stage will land on the newly refurbished drone ship, Just Read the Instructions,” Rosen described. “It is fresh off its trip across the Panama Canal into the Atlantic Ocean a couple hundred miles offshore. We're excited to have that capability here to be able to land two vehicles here. That additional landing capability [will enable] quicker turn times between missions. This is our first U.S. Space Force mission and we're really excited about it and hope that this is the first of many, many of those launches in the future.”\nThe model of relying on industry for launch services is an evolving relationship, Lauderdale confirmed. For the current launch campaign, the team completed 362 verification tasks and evaluated more than 230 risks. “This combination of activity, encompassing nonrecurring design, process and manufacturing verification tasks, when taken together, culminate in the Space Force's flight worthiness assessment,” he said.\nAnd at first, the idea of recovering the booster seemed implausible, given the “insufficient performance given the mission trajectory and peel-away combined with the uncertainties associated with this demanding mission,” Lauderdale acknowledged.\n“But when we approached Space X to revise some spacecraft requirements for this mission, the GPS III mission, they responded with an opportunity to recover the booster in exchange for adding these requirements as well as other considerations,” he explained. “Our evaluation of that mission’s performance combined with additional work with Space X reduced uncertainty in many areas. And given our technical insight and the support from our GPS teammates from a mission design perspective, we're able to reach an arrangement in the best interests of the government.”\nLauderdale estimates significant savings from bartering with Space X and the reuse of the booster. “When we came in with the revised requirements, it was an opportunity for us to look at what we could do together,” the mission director explained. “And we were able to knock several million off, in addition to getting the requirements that we had added to the mission.”\nLastly, the team dedicated the launch of GPS III SV03 to Col. Thomas Falzarano, USAF, commander, 21st Space Wing, a rising leader in the service who passed away suddenly last month, Gen. Schiess noted."", 'The world’s shipping industry is experiencing strong growth, which is expected to continue. Ships are getting larger and faster, sea-lanes are becoming more crowded, and crews are increasingly relying on electronic navigation systems to operate in this environment . The newly proposed concept of e-Navigation will improve safety, security, and protection of the marine environment as well as potentially reducing costs. It will provide bridge officers with all the information they need on a single display. In order to make these critical e-navigation services available, the system will require a supply of position and timing data of exceptionally high accuracy and reliability.\nSystems like GPS, Russia’s GLONASS, China’s BeiDou, and Europe’s Galileo systems are Global Satellite Navigation Systems (GNSS) that provides real-time positioning, navigation and timing (PNT) data. However, in many environments in which military operates (inside buildings, in urban canyons, under dense foliage, underwater, and underground) have limited or no GPS access. Similarly, GPS signals can be significantly degraded or unavailable during solar storms. GPS is also vulnerable to jamming attacks, GNSS jammers are now being used by criminals or vehicle hijackers, as reported by the FBI: “… GPS tracking devices have been jammed by criminals engaged in nefarious activity including cargo theft and illicit shipping of goods.Typical military jammers are able to affect GPS receiver for many tens of kilometers by line of sight. It’s a problem because best accuracy, availability and global coverage of PNT data is through GPS/GNSS.\nIn September 2001, almost simultaneously with the “9/11” act of terrorism, the US Government published its “Volpe Report” . This spelled out the degree to which the US, like other nations worldwide, was starting to base its critical infrastructures on GPS. The Report explained the vulnerability of GPS (and similar GNSS systems) to disruption by intentional or unintentional interference. It identified Loran as a potential solution to this important problem. This attracted interest worldwide, and provided an impetus to modernize the Loran system in the US.\nLORAN, short for long-range navigation, was a hyperbolic radio navigation system developed in the United States during World War II. It was first used for ship convoys crossing the Atlantic Ocean, and then by long-range patrol aircraft. Loran-C, delivered a positioning accuracy of 460 meters , principally to mariners sailing in coastal and oceanic waters. When the Global Positioning System (GPS) appeared in the1980s, with its positioning accuracy of tens of meters, many began to regard Loran-C as irrelevant.\nWith the perceived vulnerability of GNSS systems, and their own propagation and reception limitations, renewed interest in LORAN applications and development has appeared. Enhanced LORAN, also known as eLORAN or E-LORAN, comprises advancement in receiver design and transmission characteristics, which increase the accuracy and usefulness of traditional LORAN. With reported accuracy as good as ±8 meters, the system becomes competitive with unenhanced GPS. eLoran is an independent, dissimilar, complement to Global Navigation Satellite Systems (GNSS). It allows GNSS users to retain the safety, security, and economic benefits of GNSS, even when their satellite services are disrupted.\nThe US Loran evaluation and modernization program resulted in this new version of Loran with significantly improved performance. It has much better accuracy, integrity, and continuity while continuing to meet Loran-C’s traditional availability requirements. These improvements are realized mainly through the addition of a data channel. The data channel conveys corrections, warnings, and signal integrity information to the user’s receiver via the Loran transmission. eLORAN also includes additional pulses, which can transmit auxiliary data such as DGPS corrections. These enhancements in LORAN make it a possible substitute for scenarios where GPS is unavailable or degraded.\nAs eLoran uses high-powered transmitters and low-frequency signals (not microwatts and microwaves like GNSS), it is very unlikely to be interfered with or jammed by the same causes that would disrupt GNSS signals. This means that small, low-cost, eLoran receivers, even built into GNSS units, can mitigate the impact of disruptions to GNSS.\nA further important benefit of eLoran’s low frequency signals is their ability to penetrate into places where GNSS signals either cannot be received at all, or where they are intermittent or inaccurate. These include the urban canyons in the centers of major cities. Loran signals have been shown to penetrate reliably into steel shipping containers, refrigerated vehicles and storage warehouses. This ability has led to the development of systems that track items either of highvalue or whose safe and timely delivery must be guaranteed. The tracking of hazardous cargoes also demands the consistent updates and high availability of eLoran-based systems.\neLoran supplies the precise timing needed to support not only e-Navigation, but also the Automatic Identification System (AIS) and synchronized lights in harbor areas. Moreover, eLoran can do things GNSS cannot, such as acting as a static compass. An important bonus of using eLoran – something GNSS cannot provide – is the eLoran compass. When the receiver is used with an H-field (Magnetic Loop) antenna it can be employed as an automatic direction-finder taking bearings on the transmitting stations. From these, the receiver calculates the ship’s heading, generally with an accuracy of better than 1°, and independent of the ship’s movement.\nAt sea, a new concept of navigation enhanced navigation (e-Navigation) – is being developed which requires an exceptionally reliable input of position, navigation, and time data. Uniquely, the combination of GNSS and eLoran has the potential to meet its needs.\nUK’s eLoran (enhanced Long Range Navigation) for ships and aircrafts\nThe General Lighthouse Authorities (GLAs) of the UK and Ireland have employed eLoran (enhanced Long Range Navigation), that enables ships and aircraft to determine their position and speed by triangulating low-frequency radio beacon signals transmitted by presently seven differential reference stations on the shore. U.K. has become first country to provide alternative position, navigation, and timing (PNT) information to ships fitted with eLoran receivers that can ensure their safe navigation in the event of GPS failure.\nCaptain Ian McNaught, deputy master of Trinity House, commented, “eLoran provides a signal around 1 million times more powerful than those from satellite signals, providing resilience from interference and attack.”\neLoran can provide navigation information for vessels as well as the timing data necessary to maintain the power grid, cell phones, financial networks, and the Internet in the event of an outage. Unlike space-based navigation, eLoran signals can also reach inside buildings, underground, and underwater.\nReferences and Resources also include:']"	['<urn:uuid:4b27d0cb-eabb-4232-8e1d-6a382a7df440>', '<urn:uuid:855f526c-ebe7-4cc2-b62d-240d0001aaa6>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T11:40:26.317848	9	81	1993
56	which drug more dangerous retin a or ifex	Ifex (ifosfamide) is more dangerous than Retin-A. Ifex is a strong cancer medicine given through a vein in hospitals that can cause serious side effects like bleeding, infections, brain disease, and increased cancer risk. Retin-A is a topical medication for acne with mainly mild side effects like skin irritation.	"[""Treats cancer of the testicles.\nIfexThere may be other brand names for this medicine.\nWhen This Medicine Should Not Be Used:Do not receive this medicine if you have had an allergic reaction to ifosfamide or if you have trouble urinating. Do not receive this medicine if you are pregnant or breastfeeding.\nHow to Use This Medicine:\n- Medicines used to treat cancer are very strong and can have many side effects. Before receiving this medicine, make sure you understand all the risks and benefits. It is important for you to work closely with your doctor during your treatment.\n- Your doctor will prescribe your dose and schedule. This medicine is given through a needle placed in a vein.\n- You will receive this medicine while you are in a hospital or cancer treatment center. A nurse or other trained health professional will give you this medicine.\n- Drink extra fluids so you will urinate more often and help prevent kidney problems.\nIf a dose is missed:\n- This medicine needs to be given on a fixed schedule. If you miss a dose, call your doctor, home health caregiver, or treatment clinic for instructions.\nDrugs and Foods to Avoid:\nAsk your doctor or pharmacist before using any other medicine, including over-the-counter medicines, vitamins, and herbal products.\n- Make sure your doctor knows if you also use St John's wort, aprepitant (Emend®), fosaprepitant (Emend® injection), rifampin (Rifadin®, Rimactane®), sorafenib (Nexavar®), medicine to treat seizures (such as carbamazepine, fosphenytoin, phenobarbital, phenytoin, Dilantin®, Luminal®, or Tegretol®), or medicine to treat a fungal infection (such as fluconazole, itraconazole, ketoconazole, Nizoral®, or Sporanox®). Tell your doctor if you have received other cancer medicines or radiation treatments.\n- Tell your doctor if you use anything else that makes you sleepy. Some examples are allergy medicine, narcotic pain medicine, and alcohol.\n- Do not eat grapefruit or drink grapefruit juice while you are using this medicine.\n- This medicine may interfere with vaccines. Ask your doctor before you get a flu shot or any other vaccines.\nWarnings While Using This Medicine:\n- This medicine may cause birth defects if either partner is using it during conception or pregnancy. Tell your doctor right away if you or your partner becomes pregnant. Women should not become pregnant during therapy and men should not father a child while receiving this medicine and for 6 months after stopping it.\n- Make sure your doctor knows if you have kidney disease, liver disease, anemia, bones or bone marrow problems, brain disease, heart disease, lung problems, urinary tract problems, or any type of infection.\n- This medicine may make you bleed, bruise, or get infections more easily. Take precautions to prevent illness and injury. Wash your hands often. This medicine may slow wound healing.\n- Tell your doctor right away if you have vision changes, confusion, dizziness, hallucinations, mood changes, seizures, or unusual tiredness while using this medicine. These may be symptoms of a serious brain disease, which can be caused by this medicine.\n- This medicine may make you dizzy or drowsy. Do not drive, use machines, or do anything else that could be dangerous until you know how this medicine affects you.\n- This medicine may increase your risk of certain cancers, such as lymphoma, leukemia, and thyroid cancer. Talk to your doctor if you have concerns about this risk.\n- This medicine could cause infertility. Talk with your doctor before using this medicine if you plan to have children.\n- Cancer medicines can cause nausea and/or vomiting in most people, sometimes even after receiving medicines to prevent it. Ask your doctor or nurse about other ways to control these side effects.\n- Your doctor will do lab tests at regular visits to check on the effects of this medicine. Keep all appointments.\nPossible Side Effects While Using This Medicine:\nCall your doctor right away if you notice any of these side effects:\n- Allergic reaction: Itching or hives, swelling in your face or hands, swelling or tingling in your mouth or throat, chest tightness, trouble breathing\n- Blistering, peeling, or red skin rash\n- Change in how much or how often you urinate, painful or burning urination\n- Confusion, drowsiness, vision changes, hallucinations (seeing, feeling, or hearing things that are not there), or uncontrolled body movements\n- Fast, pounding, or uneven heartbeat or trouble breathing\n- Fever, chills, cough, sore throat, and body aches\n- Numbness, tingling, or burning pain in your hands, arms, legs, or feet\n- Red or dark brown urine\n- Severe nausea, vomiting, or diarrhea\n- Unusual bleeding, bruising, or weakness\nIf you notice these less serious side effects, talk with your doctor:\n- Hair loss\n- Mild nausea or vomiting\n- Pain, itching, burning, swelling, or a lump under your skin where the needle is placed\nIf you notice other side effects that you think are caused by this medicine, tell your doctor\nCall your doctor for medical advice about side effects. You may report side effects to FDA at 1-800-FDA-1088\nLast Updated: 11/4/2014\nThe information provided herein should not be used during any medical emergency or for the diagnosis or treatment of any medical condition. A licensed medical professional should be consulted for diagnosis and treatment of any and all medical conditions. Call 911 for all medical emergencies. Links to other sites are provided for information only -- they do not constitute endorsements of those other sites.\nCopyright © 1984- Truven Health Analytics. All rights reserved."", 'Retin-A is a topically applied retinoid used to treat acne vulgaris. Its active ingredient is 0.05% tretinoin, a powerful retinoid often used in certain leukemia treatments. This may lead some to mistakenly think Retin-A can be used similarly. (Learn More)\nLeukemia is a serious cancer of the body’s blood-forming tissues. Tretinoin can be used to treat what is called acute promyelocytic leukemia (APL or APML). However, Retin-A is not suitable for this purpose.\nThe tretinoin used to treat APL is taken orally, and it comes in a much higher (and much harsher) dose than the 0.05% dosage of Retin-A that is topically administered. (Learn More)\nTretinoin is an effective, and studied, treatment for APL. Retin-A is just too weak a treatment compared to the necessary amounts needed to combat APL. Retin-A makes no claim to treat APL. (Learn More)\nThe tretinoin-based drug used to treat APL comes with a laundry list of serious side effects, and it can be potentially life threatening (an often necessary risk when treating more serious conditions like APL).\nRetin-A has far fewer side effects and dangers. It is an effective treatment for acne.\nIn short, Retin-A is not an effective treatment for any kind of leukemia. It should only be used as prescribed by a doctor. (Learn More)\nRetin-A is a prescription treatment that’s primary active ingredient is 0.05% tretinoin.\nIt comes in the form of a gel, cream, and liquid, with its name derived from the word retinoid, which is the class of drug it belongs to. To be more specific, Retin-A is a topical retinoid, meaning it is applied to the skin.\nRetinoids have a number of uses, including to treat aging, wrinkles, acne, psoriasis, and certain types of cancers. Retin-A specifically has only been studied for treating acne vulgaris. The long-term safety and efficacy of its use for other conditions are poorly understood.\nIn short, Retin-A is only approved as an acne treatment, even if retinoids (and tretinoin specifically) have been known to help with other conditions.\nCan Retin-A Be Used to Treat Leukemia?\nLeukemia is a cancer of the body’s blood-forming tissues. Tretinoin is not a chemotherapy drug.\nTretinoin alone is not usually an ideal, or even viable, treatment option for leukemia. It is sometimes used in conjunction with other treatment options to specifically treat acute promyelocytic leukemia (APL or APML). It can be highly effective for this purpose.\nRetin-A has not been studied for this use. It is unlikely to be effective if it were studied.\nRetin-A is a topical medication that is designed and used to treat acne. The tretinoin medication used to treat APL is taken orally, sold under the brand name Vesanoid. This results in two radically different impacts on the body. Importantly, Retin-A uses only a very small amount of tretinoin (which itself is a potent drug).\nOnly take medications as prescribed. A doctor will not prescribe Retin-A for leukemia because it has not been studied for that purpose, and other drugs have been.\nIf you are suffering from APL or otherwise wondering if tretinoin could help in your treatment, ask your doctor but never self-prescribe. Generally, if a drug has not been prescribed to you, there is a legitimate medical reason behind that decision.\nIs Tretinoin Effective in Treating Acute Promyelocytic Leukemia?\nWhile Retin-A specifically is not a good treatment for APL, tretinoin itself has been known for at least three decades to have a place in treating APL. One 1991 study highlights that it is safe and highly effective in inducing remission.\nMuch more recently, a 2017 study notes that tretinoin can indeed be an effective treatment for APL, although it can have a host of adverse effects on the skin, central nervous system, liver, and other organs.\nBoth topically applied and orally administered tretinoin have a place in medicine. While they both use the same drug, administration and dosing are very important to how they are used.\nOrally administered tretinoin is going to affect the body more, in both positive and negative ways. As already mentioned, it can be used to treat certain forms of leukemia, but it can also have much harsher side effects than topically applied tretinoin.\nThe following are common side effects with orally administered tretinoin:\n- Severe, potentially overwhelming, tiredness\n- Difficulty falling or staying asleep\n- Feeling of fullness in the ears\n- Stomach pain\n- Loss of appetite\n- Weight loss\n- Bone pain\n- Dry skin and/or rash\n- Hair loss\n- Burning, tingling, or numbness in the feet or hands\n- Auditory and visual hallucinations\n- Difficulty urinating\nThe harsh nature of orally administered tretinoin is very different than that of Retin-A. This is why oral tretinoin is generally only used to treat very serious conditions.\nRetin-A is topically applied and used to treat acne. It should not be used to treat conditions like leukemia.\nRetin-A’s most common side effect is skin irritation, including but not limited to the following manifestations:\n- Blistering or crusting\n- Skin discoloration\nThe Bottom Line\nBoth oral and topical tretinoin carry risks, but Retin-A is undeniably a less risky (and less powerful) drug than oral tretinoin, which is used for relevant types of leukemia.\nRetin-A is a medication intended for acne treatment. At this time, it is not indicated for anything else. It is unlikely to ever be an effective leukemia treatment.\nTretinoin – Drug Summary. Prescribers’ Digital Reference.\nTretinoin (Vesanoid, ATRA). (April 4, 2019). Cancer Research UK.\nLeukemia: Symptoms & Causes. (March 13, 2018). Mayo Foundation for Medical Education and Research (MFMER).\nLeukemia: Side Effects. (March 13, 2018). Mayo Foundation for Medical Education and Research (MFMER).\nRetin-A. (July 25, 2018). RxList.\nTretinoin. (September 15, 2016). MedlinePlus.\nRetinoids, Topical. American Osteopathic College of Dermatology.\nDifferentiation Therapy of Acute Promyelocytic Leukemia With Tretinoin (All-trans-Retinoic Acid). (May 16, 1991). The New England Journal of Medicine.\nEnhancement of Activity of la,25-Dihydroxyvitamin D3 for Growth Inhibition and Differentiation Induction of Human Myelomonocytic Leukemia Cells by Tretinoin Tocoferil, an a-Tocopherol Ester of All-Trans Retinoic Acid. (November 16, 2017). Blood Journal.']"	['<urn:uuid:d642190e-6c19-431d-9bd9-f83fb3565af2>', '<urn:uuid:80a22fe1-84ad-4c42-b51f-f9e03980f65f>']	factoid	with-premise	short-search-query	distant-from-document	comparison	novice	2025-05-12T11:40:26.317848	8	49	1917
57	clay artists ceramics techniques work process materials methods exhibition galleries	Clay artists employ diverse techniques and processes in their work. They hand-build sculptures using thick coils, like Debra Fritts, who fires pieces multiple times for specific colors and surfaces. Some artists, as shown in galleries like Muse Gallery, combine traditional methods with innovative approaches, using materials such as oxides, slips, and glazes. The technical process includes careful timing of slip application when clay is partially dry, creating detailed designs through sgraffito, and specific firing schedules - such as firing to cone 04 and holding temperature for 15 minutes before cooling to achieve particular surface finishes.	"['Muse Gallery will be hosting a joint Pop Up Show @ Muse Gallery Hilton Head- February 15-28, 2018, featuring wall hanging and free standing sculptural work by artists Debra Fritts, Signe Stuart, Sue Cavanaugh and Char Norman. Cavanaugh and Norman will be in attendance for the show to meet and discuss the work. Opening Reception will be held on Friday February 16th from 5-8 pm and a Private Brunch with the artists will be held on Saturday February 17th from 11-1 pm. We hope you can join us!\n""I am interested in gathering: assembling, collecting, hoarding . . . and the process of taking needle to cloth and creating folds. The stitching has grown out of the ancient art of patterning cloth for kimono, and most names reference the original stitches, even though I’ve transformed those stitches over time. The “Ori-Kume” series combines ori-nui, stitches done on the fold with mokume running stitches. “Ori-maki-kume” describes a combination of mokume running stitches and ori-maki-nui stitches, an original hybrid stitch that creates a density I’ve grown to love...Cloth challenges notions of traditional art. At the same time, fabric is universally accessible and comforting. I’m particularly attracted to dichotomies, to yin/yang, attraction/repulsion, black/white, and the vastness of the gray area in between. All my work begins with a drawing, a plan. But the surprises that develop along the way delight and challenge me. These surprises inform future works.""\nCavanaugh works by hand with cloth, cord, dye, paint and occasionally wood and wire. She does this by hand and without assistants. Her work has been seen in national and international exhibits at the Columbus Museum of Art, Oceanside Museum of Art, Springfield Museum of Art (Ohio), Ross Museum of Art, Johnson-Humrickhouse Museum of Art, Ohio Craft Museum and Zanesville Museum of Art. Awards include Best of Show, Shibori Cut Loose exhibit, Textile Center, Minneapolis; Ruth Lantz Fiber Award; Janet Long Memorial Award for Excellence in Fiber; Ohio Arts Council Professional Award; and the Lynn Goodwin Borgman Award for Surface Design.\nIn 2012 she was selected by the Greater Columbus Arts Council and the Free State of Saxony for an artist residency in Dresden, Germany. She worked in a studio at Geh8 Kunstraum und Ateliers for 80 days culminating in a two-person exhibit with Rotterdam painter Marielle Buitendijk.\nCavanaugh\'s work is in collections of the Hilton Columbus Downtown and the Ohio Arts Council and private collections in Ohio, New Mexico, California, New York, Chattanooga, and West Seattle. She is represented by Muse Gallery, Columbus, Ohio, and gráficas gallery, Nantucket, Massachusetts. She lives in downtown Columbus, Ohio, and has a studio in the 400 West Rich artist community.\nABIQUIU, NEW MEXICO\n""As a child, I had dirt under my fingernails and spent hours playing in the mud. Today I continue to allow the earth to feed me information for my art. Working intuitively from pounds of wet red clay, forms appear and stories develop. I may be questioning an occurrence or celebrating a relationship or just being aware of the precious environment. The search continues until I reach the core: the spiritual level of the sculpture. Then the work can speak. At the present, I am exploring new territory in Abiquiu, New Mexico while embracing my southern heritage. Often symbols are used in the work such as the color red or three dots to honor my mother or the raven as a symbol for my new life in the west. I am “touching ground”, getting to the basics, listening and learning. Each sculpture is hand built, using thick coils, and fired three to five times depending on the color and surface I am trying to achieve. I approach the color on the clay as a painter. My palette is a combination of oxides, slips, underglazes, and glazes. The form of the piece informs the type of surface treatment.""\nDebra Fritts considers herself a narrative artist allowing her work to tell stories of daily life and events. Her works are influenced by her time in New Mexico exploring the west while embracing her southern heritage.\n“I hand build each sculpture, primarily using thick coils, and fire three to five times depending on the color and surface I am trying to achieve. I approach color on clay as a painter. My palette is a combination of oxides, slips, underglazes, and glazes. The form of the piece informs how I should approach the surface.”\nDebra has been published in books such as: Artists Homes and Studios, Ashley Rooney, Schiffer Publishing. Contemporary Ceramics, E. Cooper, Ceramic Figures, Lark Books, and 500 Figures, Leslie Ferrin, Lark Books. Museum Collections include: Fuller Museum, Brockton, Massachusetts and Georgia Southern University, Saunders Georgia Artist Museum. Professional and Teaching Experience includes Director of Art Center West, Roswell, Georgia 1995-2011, Beatrice Wood Center, workshop, Ojai, California, and Surface National Clay Conference, San Diego, California.\n""The work I am engaged in stems from a deep-rooted connection to natural objects and environmental issues while examining the relationship between man and nature. Reverential attitudes and nurturing acts contrast with the destruction of nature. The pod forms I use are both a type of shroud for natural relics and a womb or cradle for rebirth. This dichotomy of ideas is further expressed by the mending of natural objects through the violent act of stitching and fastening parts together. I find it fascinating and somewhat meditative to achieve a whole through the slow and gradual building up of small elements. Weaving is based on this principle and my drawing technique mirrors this idea as I layer graphite and colored pencil to create the image. Even the fibers in my handmade paper echo the idea of small units building to become a whole. Manipulation of materials and the use of traditional techniques in surprising or nontraditional ways are challenging and engage me in problem solving. The engineering necessary to create a three-dimensional piece on a loom intended for two-dimensional processes and the use of soft materials to form substantial objects is of particular interest. As I continue to explore natural relics as icons, votives, or objects of reverence, I hope to engage the viewer in a way of seeing that may lead to a respect and appreciation for the environment. Future plans call for returning my sculptures to the location from where the natural object was taken. In this way I give back and let the elements take their natural course in the cycle of life.""\nChar Norman is an accomplished fiber artist specializing in papermaking and fiber sculpture. She received a Master of Fine Art from Claremont Graduate University and a Bachelor of Art from Scripps College. She has lectured and exhibited extensively both nationally and internationally. She has developed and conducted workshops for all ages, worked as a consultant to area schools and community arts organizations, held the positions of Associate Provost and Dean of Faculty at Columbus College of Art & Design and has recently returned to the studio as a full time professional artist.\nSANTE FE, NEW MEXICO\n""Observations and questions about mysteries of the universe, life and consciousness are sources of visual ideas for my paintings and constructions. Making these works is an ongoing process of experimentation and negotiation between ideas and materials. I want my artworks to resonate with viewers and move them toward seeing this is that: everything as a consequence of endless shape-shifting, combining and recombining.""\nSigne Stuart\'s professional history spans over fifty years, beginning in the early 1960\'s. Her approach to art making relies on experimentation with painting materials and forms, often breaking from the standard rectangle and concepts of framing. Stuart has lived and worked in diverse regions of the United States: East Coast, Pacific Northwest, Northern Plains, and Southwest: residing now in Santa Fe, New Mexico.\nSince 1972, Signe Stuart has had 18 solo museum exhibitions including those at the Sheldon Art Museum, Lincoln, NE; North Dakota Museum of Art, Grand Forks, ND; American Swedish Institute, Minneapolis, MN; South Dakota Art Museum, Brookings, SD and the Roswell Museum and Art Center, Roswell, NM. Her work has also been included in many museum group exhibitions, among them the Joslyn Art Museum, Omaha, NE; the New Mexico Museum of Art, Santa Fe, NM; Burchfield Penny Art Center, Buffalo, NY and the Minneapolis Institute of Arts, Minneapolis, MN.\nPublic Collections include: American Swedish Institute, Minneapolis, MN, Benton Museum of Art, Storrs, CT, Blanden Art Museum, Fort Dodge, IA, Boise Art Museum, Boise, ID, Cedar Rapids Puiblic Library, Cedar Rapids, IA, Dahl Art Center, Rapid City, SD, Grinnell College, Grinnell, IA, Joslyn Art Museum, Omaha, NE, Miami Airport, Miami, FL, New Mexico Museum of Art, Santa Fe, NM, New Mexico State Capitol Art Foundation, Santa Fe, NM, North Dakota Museum of Art, Grand Forks, ND, Plains Art Museum, Fargo, ND, Roswell Museum and Art Center, Roswell, NM, Salt Lake City Public Library, Salt Lake City, UT, Schnitzer Museum of Art, Eugene, OR, Sheldon Museum of Art, Lincoln, NE, Sioux Falls Airport, Sioux Falls, SD, South Dakota Art Museum, Brookings, SD, Southwest Minnnesota State University Museum, Marshall, MN, Tacoma Art Museum, Tacoma, WA, University of New Mexico Museum of Art, Albuquerque, NM, Utah Museum of Fine Arts, Salt Lake City, UT, Washington Pavilion, Sioux Falls, SD.', 'I just love the sugary matt surfaces, color schemes, and subtle layers in Kristin Pavelka’s work. Kristin uses gorgeous red earthenware to her advantage by creating linear sgraffito marks through white slip. Then she creates subtle layers of glazes–often using two tones of the same color–in a paint by number fashion. The result is work that looks as yummy as a frosted sugar cookie.\nIn this post, Kristin explains these decorating techniques. She also shares glaze and slip recipes! – Jennifer Poellot Harnetty, editor.\nDecorating with Slip and Sgraffito\nI slip my pots when they look dry but have a small bit of moisture in them. This allows for a relatively even coating of slip, yet it dries a bit slower giving me time to complete my sgrafitto before the slip starts to chip when scratched. Because the slip dries quickly, I have to work fast to complete my design, so I plan the patterns ahead of time in a sketchbook or by drawing with a soft pencil on the unslipped plate itself.\nOnce I’ve decided on a pattern, I can begin slipping. Holding the plate vertically, I pour the white slip onto the middle of the plate using a large ladle, turning the piece clockwise until the entire face is covered (figure 7). Keep the plate vertical until the slip drips have firmed, then rest the plate on the tabletop and allow the slip to dry for a few minutes until you can touch it without a fingerprint remaining, but while it still feels cold and damp.\nLightly draw a grid on the piece using a soft pencil, like a 2B. Breaking up the space symmetrically on a circular form is a quick and easy way to understand the space. I sometimes draw my pattern on the piece to double check the placement of key elements, but usually I draw directly with my sgraffito tool using just the grid as an aid for placing the design.\nMy sgrafitto tool had a previous life as a dentistry tool and is thicker and duller than a standard needle tool. A long nail with a dull point is a good substitute. The line created is thicker than an X-Acto blade or needle tool and can give a similar line quality as a standard-sized pencil lead. Medium pressure is exerted with the tool tip so that it scratches through the white slip and just barely digs into the red underlying clay (figure 8). I brush a stiff yet soft-bristled brush across the surface of the plate once the design is carved to clean up the edges of the incised lines as well to rid the surface of the slip crumbs (figure 9).\nFinally, a Scotch Brite pad is lightly rubbed along the rim to help expose the red earthenware beneath. This helps create more depth in the surface once it has gone through the glaze firing.\nNote: For all three of the above steps that create crumbles or fine powder, wear a mask and work over a bucket of water to minimize the amount of dust entering the air and to make clean-up easier.\nCreate surfaces that will turn heads!\nErin Furimsky shows you how in her Layered Surfaces DVD!\nGlazing by Numbers\nI bisque fire to cone 01, then, to prepare the piece for glazing, give it a good shower under running water to clean any leftover sgrafitto dust from the surface. Leave the piece to dry overnight. The first glaze application is much like a paint-by-number painting. Often using two tones of the same color, I’ll load up a small brush with the darker tone and fill in the “pod” shapes. Little pressure is used when painting as the glaze should flow from the brush onto the bisque, eliminating brush strokes (figure 10). I fill the sgrafitto lines with this first glaze, which helps eliminate pinholes in the glaze-fired impression. This first layer of glaze is left to dry several hours to overnight.\nThe second, lighter tone of glaze is then poured on the plate in a similar fashion to the white slip—rotating a vertically-held plate clockwise while pouring the glaze in the middle of the piece (figure 11). This second coat is left to dry.\nThe final glazes are now ready to be applied to the dots using a small soft brush or a fingertip. I can usually see a light indentation of the sgrafitto dot through the poured glaze to use as a guide for dot placement. If I am unable to determine where to place my dot within the design, I sometimes guess and other times fire the piece and then apply the dots to the fired glaze and refire. The final dots are made up of a lighter-toned large dot with a smaller dark toned dot on top (figure 12).\nThe dry, glazed piece is fired to cone 04, held at that temperature for 15 minutes and then fired down to cone 010 before being turned off. This schedule helps to produce a nice satiny finish to the glaze surface.\nKristin Pavelka is a full-time studio potter living in Maplewood, Minnesota. To see more of her work visit www.pavelkapottery.com.']"	['<urn:uuid:79879c71-b902-48bd-9dda-7db479d5cc9f>', '<urn:uuid:3b309d98-b85a-4090-98ce-490e6a7988e6>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T11:40:26.317848	10	95	2402
58	beginner auto polisher avoid damage tips	To avoid damage, never lift the polisher pad while the machine is running, keep it level with the paint surface at all times. Start the machine only when the pad is flat on the work surface. Use only 3-4 small dabs of polish, avoid dry buffing, and don't polish on hot surfaces or under direct sunlight. Always work on small areas at a time and keep the power cord over your shoulder to prevent paint damage. For safety, remove jewelry and wear protective clothing.	"['A dual-action car polisher (often called ""random orbit polisher"" or ""orbital polisher"") is a machine that operates by orbiting the polishing pad around a center spindle while the pad freely spins on its own axis. A counterweight on the opposite side of the center spindle dampens vibration for smooth operation.\nThe tool head action, often called orbital, prevents the creation of holograms (symmetrical buffing marks), paint burns and other forms of paint damage often associated with high-speed buffing machines. When used with proper care, the design of the Porter Cable 7424XP virtually eliminates the possibility of paint damage.\nThe polishing action of the Porter Cable 7424xp & other dual-action polishers closely mimics that of circular hand polishing. Imagine being able to make 2,500 to 6,000 tight hand circles in a single minute and you\'ll have a pretty good idea of how the Porter Cable 7424XP Polisher does its job.\nSome people have defined the action of a dual-action orbital polisher as ""jiggling"", but this is not the case. The head freely rotates on a bearing and orbits around the centerline driveshaft. Rotation is a free-wheeling action of the backing plate spindle caused by the orbiting head. This ""dual-action"" is where the polisher gets its nickname.\nIf you have previously owned a random-orbit car polisher that made a lot of racket, but did little else for your car\'s finish, you will Porter Cable 7424XP Polisher ""professional grade"" machine\nMost dual-action polishers use an offset spindle that freely orbits around the center shaft. A counterbalance weight dampens vibrations.\nThe orbiting head action, combined with a high-speed motor offers the same gentle polishing action as your hand, only at super high speed.\nThe Porter Cable 7424XP Dual Action Polisher has a powerful motor for its size (4.5 amps) If you don\'t have a garage, you can run the polishers off of your car battery using a 750+ watt power inverter. The Porter Cable 7424XP Polisher uses a standard 5/16-24 spindle, allowing it to take a wide variety of various attachments.\nThe Porter Cable 7424XP Polisher measures about 10 inches in length and weigh about 6 pounds with a backing plate and pad attached.\nA reversible side handle makes the Porter Cable 7424XP Polisher very easy to hold and control, even when working on vertical surfaces. Use of the side handle is a personal preference. It adds a bit of safety, but people with large hands may find it just as easy to grip the polisher by the head with a single hand.\nThe machine functions at 2,500 to 6,000 operations per minutes (OPM), controllable by a thumbwheel on the rear end of the machine. The OPM measurement is used instead of revolutions per minute (RPM) to distinguish the difference between orbits and center shaft revolutions. At 6,000 OPM, the UDM by Porter Cable car polisher does a lot of work.\nComparatively, the Porter Cable 7424XP Polisher orbital polisher comes fully equipped with a Velcro backing plate and a functional foam polishing pad. Ultimate Detailing Machine also offers a fully equipped model with a full compliment of foam polishing pads and microfiber bonnet.\n| Features |\nChanging backing plates is easy. Use the flat wrench provided with your Porter Cable 7424XP Polisher to hold the center spindle and unscrew the backing plate with your hand. Do not over-tighten.\nFor safety reasons, do not operate your dual-action polisher unless the pad is flat on the work surface. That means you both start and stop the machine while it is resting flat on the surface of the car. If you\'re not going to follow this safety precaution, please wear safety glasses.\nTo operate the machine, follow these simple car polisher operating instructions:\nYour car must be freshly washed and dry. Do not buff on a dirty finish.\nWork in an area with good lighting. Overhead fluorescent or halogen lighting is best.\nRemove all rings, watches, bracelets, belts and other items that will scratch your paint.\nWear an apron to protect your clothing and the paint finish.\nAdjust the machine speed to 3 or 4 on the thumbwheel. This is a good starting point.\nWith the machine switched off, apply a small amount of polish, cleaner or wax directly to the polishing pad. There are two good methods that seem to work. Squeeze out a complete ring of product about 1 inch in from the outside of the pad, or squeeze out an dime sized drop of product across the pad.\nDab the pad in 3-4 spots around the area to be polished, cleaned or waxed to distribute the product.\nPosition the polisher flat on the work surface, get a firm grip and switch it on with your thumb.\nMove the car polisher back and forth (east/west) in a slow, sweeping motion, overlapping each pass (by 50% of the pad width) with the previous, and then switch your pattern to up and down (north/south).\nIt is not necessary to apply more than a pound of pad pressure. Allow the car polisher to work under its own weight.\nUse a clean pad with each product. Do not mix products on a pad. If a pad is not dirty, you may store it in a Ziploc baggie for future use. Be sure to label the baggie or the pad.\nIf a pad becomes caked with product, use a nylon or horsehair detailing brush to clean the pad.\nSounds pretty simple, doesn\'t it? It really isn\'t difficult. Within a few minutes of use, most people are very comfortable operating the machine.\nIt is compounding and polishing, not waxing, that offers the most improvement in the overall appearance of paint. If you wax over bad paint, it\'s still bad paint. When you polish bad paint (paint with scratches, heavy oxidation, swirl marks, stains, water spots, etc.), you remove the bad paint to reveal a fresh finish. Obviously you can only do so much polishing before you wear out (thin or completely remove) the paint, so only polish as much as necessary to maintain healthy paint.\nBuffing pad and polish makers create products with different levels of aggressiveness to make your polishing tasks faster and easier. You can use an aggressive pad and polish combination to quickly remove paint defects or severe oxidation, but it won\'t reveal the full gloss potential of your paint finish. Just like polishing a jewel, you must use several grades of car polish to bring out the final radiance.\nIf your paint is new or like new, it is not necessary to use heavy abrasives. You can use very mild pre-wax cleaners and glazes to maintain the factory finish or create a finish that glows like a gem. To do so, use a fine polish and a foam polishing pad, and work the polish in with a light touch until most of the polish residue is gone. Remove the remaining polish residue with a quality buffing towel. Fine polishes won\'t take a lot of heat before they cake up and clog your pad, so don\'t apply pressure to the machine. Allow the pad and polish to do the work.\nSome polishes respond well to a microfiber buffing bonnet, while others do not. If you\'re using a polish that is not buffing out clear, try doing the final buff out with a microfiber bonnet over a clean pad.\nWhile the Porter Cable 7424XP Polisher is a very easy car polisher to master, practicing on an older car with an imperfect finish is the best way to learn. You should experiment with a variety of polish and pad combinations until you learn how to achieve the best results.\nThere is no silver bullet combination of pads and polish. How you use the machine, your climate, paint hardness, paint color and other issues will all factor into the final results. Don\'t get frustrated if your initial experience is not perfect, as the smallest change (a different pad, polish or speed) can make a huge difference.\nA dual-action car polisher makes polishing easy and allows anyone to create a stunning finish or restore a badly worn finish. All it takes is a little practice and the right selection of pads and polishes.', 'How To Use A Car Polisher\nOnce you have been able to pick the right car polisher machine, you need to know how to use a car polisher. It does not matter whether you want to use the hand polisher or the machine polisher, you must know how to use either of the alternatives effectively.\nIf you do not know how to use a polisher, chances are high that in the process of trying to give your car an awesome detail job, you might actually do some damage to it. On the other hand, you can also choose to get a professional polisher to do the work for you, but this could set you back financially.\nWe can actually hit two birds with the same stone by learning how to use a car polisher. When you do this, you will be able to do the polishing on your own properly, and at the same time you will save on money that you would have otherwise had to pay someone to do the work for you.\nHand or machine polishing?\nFrom time to time we are asked which one is the better between hand polishing or machine polishing. Of course the most obvious answer is that you are better off polishing by machine. It is faster, and delivers the result you want, without much effort.\nBesides, try as hard as we want, our hands are simply incapable of delivering the same results that we can achieve from a machine. Whether you want to compare paint polishing or maintaining the finish, the machine will always win hands down.\nFact: It is impossible for any human to oscillate their hands over the car, at a rate of between 2500 and 6800 oscillations per minute, but a machine can.\nSafety measures on how to use a polisher\nOne of the other questions that we come across from time to time is about the safety standards that are associated with machine polishing. Of course even if you pick the right car polisher machine, if you do not know what to do with it, or how to use it, it will always be a dangerous experience both for yourself and your car.\nThe first thing that you need to do is make sure that you get the right type of polisher for your car. Ideally, there are three different types of polishers that you can get in the market:\nWe recommend this machine for use only by professionals, or someone who has some experience in polishing. It is also a machine that comes in handy when your vehicle has severe paint damage to be corrected.\nThis particular polisher has a very high spin rate, and if used by an inexperienced person, a lot of damage could be imminent. Other than that, it could also burn through the paint on your car, leaving you with more repair expenses than maintenance.\nThese machines are easily affordable, and you can get them for as low as $50 from the local auto spare store. They are designed with a cloth bonnet, and in as far as the accessories are concerned, there is not much that you have to choose from.\nThese machines are relatively effective for applying wax on your car, but when it comes to polishing or removing defects, they actually offer little assistance. In fact, most of them just jiggle on the surface of your car, even though they are labelled as variable speed or random orbit – this is actually the perfect example of getting what you paid for.\nThe random orbit, variable speed dual-action polisher is in fact one of the options that we would recommend. With this polisher, you are able to get rid of the following imperfections you’re your car:\n- Isolated scratches\n- Acid rain etching\n- Water spots\n- Damage from bird droppings\nThis is more than just a polisher, it is also a really good maintenance tool when you need gloss enhancement polishing, pre-wax cleaning or if you need to apply thin coats of car wax, protectant or paint sealant on your car.\nApart from being highly effective, it is also one of the safest tools that you can get for car polishing so far.\nFor professional detailers, perhaps the portable cable car polisher would be an ideal weapon in your arsenal. Even beginners can work with this particular polisher.\nTechniques on how to use a car polisher\nBefore we begin, perhaps we need to assure you that this is an easy and safe process. You just need to pay attention, and follow the instructions to the latter and you will be good to go. Even in the event that you have never used any polisher before, you can easily pick the right car polisher machine and learn how to work with it over a few trial sessions.\nIn terms of technique, there is very little you will need to worry about, because so far, most people learn their way around in under 10 minutes. All you need to do is use overlapping motion while running the polisher over the paint on your car.\nRemember to do this at a slow speed first, then adjust as you get accustomed to the job.\nThe following is a basic guide that should help you set the right speed when you are using a polisher for the first time, or even if you are a repeat user:\n- Speed 5 – 5.5: Removing serious defects on the body\n- Speed 4.5 – 5: Removing minor defects and swirl\n- Speed 4.5: Gloss enhancement and pre-wax cleaning\n- Speed 2.5: Paint protection from harsh environmental elements\n- Speed 3.5 – 4: Removing product residue\nThe speed setting that we have shown you above, is also the procedure that you need to follow when using a polisher. However, you will not necessarily need to use all the 5 steps for all paint finishes. Just use the steps that are appropriate for the specific condition you need to finish for your vehicle.\nThe following are some useful pointers that you need to have at the back of your mind when you are still learning to polish your car, especially if you are using a dual-action polisher:\n- Always use a test spot – This will help you know whether the process you are planning to use, and the technique will deliver the results that you desire, before you do the same for the rest of the car.\n- Make sure you consider the paint condition of your car, because this is supposed to guide your choice of polish, polishing pad and speed combination\n- At all times, make sure the pad is level to the paint surface\n- We recommend starting with the least aggressive polish, then if need be, increase the aggression\n- While polishing, always use overlapping motion for the best results. While you are at it, try not to run the machine over the paint surface too fast. This will definitely not get you the outcome you desire.\n- In order to concentrate better on the job and get good results, work on a small area at a time.\n- Using too much polish will not get you good results. To be precise, try and use roughly 3 or 4 small dabs, and if necessary, add more.\n- You should never dry buff your car. If you realize that your polish is thin, or it is starting to become clear, add some more.\n- Experts advise car owners never to buff their car on a hot or warm surface, or under direct sunlight\n- While detailing your car, be sure not to use more than one type of polish on the polishing pad\n- The pads should always be kept as far away from sharp and trim edges as possible\n- This is particularly for the beginners; you should never lift up the pad from the paint when you still have the machine turned on.\n- While working on the body, make sure that you have the cord away from the paint, and over your shoulder at all times.\nJust because you are able to pick the right car polisher machine does not always mean that you will know what to do with it. There are so many people who have the best polishers in the market, but are still unable to get the fine shine that they desire. It all comes down to learning how to use the polisher you have.\nWe have elaborated the three major categories of polishers that you can come across in the market. From here, you can make an informed choice on what you need, especially by considering what you are willing to comfortably spend, and what will work for your car.\nWe have also shown you 5 of the steps that you need to follow when polishing your car, including the speed settings that you can use. Remember that this might not always be the same for all polishers, since they come from different manufacturers. Also, through constant improvement, the speed settings might not always stay the same.\nFor this reason, you will be better off checking the manual that comes with the polisher you own, and use it effectively.']"	['<urn:uuid:8ef3007d-faf5-4272-ab65-f81fd32fbaa0>', '<urn:uuid:46803ea3-db3b-4c24-893c-70a0c23cfefb>']	factoid	with-premise	short-search-query	distant-from-document	three-doc	novice	2025-05-12T11:40:26.317848	6	84	2904
59	tree planting under power lines height	Trees planted under power lines should not exceed 25 feet in maximum height. If your planting site has overhead power lines, suitable tree options include Cascara, Chinese Fringe Tree, Eastern Redbud, and Southern Magnolia.	"['Planting Considerations & Program Details\nBefore selecting your new tree, please take time to read the following information, evaluate potential planting sites, and select the right tree for your yard.\nRight Tree, Right Place\nBenefits of Planting Large Trees\nPlanting Street Trees\nTree Pickup Information\nRight Tree, Right Place\nA well placed tree can help conserve energy, provide a visual screen, and provide years of beauty. However, a tree placed in the wrong place can be harmful and potentially expensive! The small tree you plant today will someday grow tall and its roots may be as expansive as the tree\'s branches. Make sure you select a location with adequate room to grow above and below ground.\n- Reason for planting. Are you planting to shade your house in the summer or to create habitat for birds? Or are you looking to provide some seasonal interest or frame your view? Select the appropriate tree to help you achieve your goal.\n- Tree size at maturity. Many of these trees will get big! Read this year\'s tree descriptions carefully and envision what the tree will look like in 30+ years before making your selections.\n- Tree shape/form. Small, spreading trees that are multi-stemmed require regular pruning when planted near a sidewalk or road. Upright trees can be better trained to grow over pedestrian and road traffic.\n- Maximize the benefits with a large tree. Larger trees provide greater benefits to your neighborhood and our environment. Large trees absorb more water, breathe in more carbon dioxide, and breathe out more oxygen than smaller trees. For these reasons, we recommend planting larger trees whenever appropriate. If you have the space, consider one of our native evergreen conifers which will maximize the benefits to you and your neighborhood.\n- Do not plant a tall tree under overhead power lines! Trees planted under power lines should reach a maximum of approximate 25 feet. If your planting site has overhead power lines, please select a tree from the ""under power lines"" list (i.e. Cascara, Chinese Fringe Tree, Eastern Redbud, and Southern Magnolia).\n- Evaluate the planting site. Take time to evaluate potential planting sites on your property. The survival and health of a tree depends on how well suited it is to the site. Before choosing your site, consider:\n- Available planting space\n- Overhead and underground utilities\n- Surrounding trees & structures (e.g. your house, driveway, and utility poles)\n- Light (e.g. full sun, part sun, shade?)\n- Surrouding human activity\n- Soil type. What type of soil is present? Is the soil sandy or more clay-like? This will influence drainage, which should influence your tree selection. Is the soil compacted? Compacted soil can lead to poor drainage so you\'ll need to select a water-loving tree, such as the swamp white oak. You can test your soil\'s drainage by digging a hole 12 inches deep, filling it with water, and checking back one hour later.\nBenefits of Planting Large Trees\nLarge trees add character to Seattle\'s neighborhoods and often become treasured neighborhood assets. Research has shown that large trees maximize the benefits in urban areas. They provide higher quality habitat for birds and other wildlife, stabilize hillsides and prevent erosion, and provide shade on hot days reducing the need for air conditioning. Studies have also shown that large trees even increase residential and commercial property values!\nLarge evergreen trees, especially conifers, are even better. Because conifers grow tall in our region, they produce a larger volume of leaf area on a smaller footprint – using the same amount of yard space but working harder to take in carbon dioxide, produce oxygen, and filter out air pollutants. Since evergreens hold their needles and leaves through the winter, our rainy season, they reduce stormwater runoff to a much greater extent than do small deciduous trees.\nIf you have the space in your yard or planting strip (away from overhead power lines), consider making a long-term investment in your neighborhood by planting a tree that will give back for decades!\nPlanting Street Trees\nTo plant a tree in your planting strip along the street, you must obtain a permit from Seattle Department of Transportation (SDOT). When you participate in Trees for Neighborhoods, we will apply for this permit on your behalf. Just be sure to indicate that you plan to plant a street tree on your application!\nWhen you apply for street tree, we\'ll take care of the details and keep you informed of the process as it proceeds.\n- Mark your street trees on your application. All street tree applications must be submitted by August 24th. Provide some notes about where you would like to plant the tree, e.g. “I would like to plant the yellowwood along 49th Ave on the north side of the driveway”.\n- Obtain a permit. In early September, we will initiate a Seattle Department of Transportation (SDOT) permit on your behalf. Note- Not all street tree applicants will be permitted as many trees will have long waitlists.\n- Mark underground utilities. Trees for Seattle will contact Washington 811 to mark underground utilities in your planting strip. An SDOT arborist will return to your site in September and make a decision about your street tree planting request. The arborist may drive one or more stakes in possible planting locations—please leave these stakes where they are! You do not need to be present for these visits.\n- Permit notifications. In late September, SDOT will send us all of their street tree decisions and Trees for Seattle will contact all applicants with permitting decisions in early October. An approved planting permit is necessary to receive a street tree from us.\nNot all street tree permits are approved. The SDOT arborists may deny your permit for a number of reasons, including proximity to utility lines, street lights, and street intersections.\n- CALL BEFORE YOU DIG! The week before attending the planting workshop, you are responsible for contacting Washington 811 and submitting a ticket to dig your planting hole in the designated location. We will provide you a specialized web link for submitting this dig ticket. If you prefer to do it over the phone, call 811. Note- By the terms of your permit, you must plant the tree in the same location as the stake.\nSince we receive many more tree requests than trees, we do not apply for a permit for all street tree requests. If we are not able to apply for you and you\'re still interested in planting your own street tree, check out this page for more information on how to apply for planting permit from SDOT.\n- Avoid conflicts. To avoid future problems, street trees must be planted to the following standards:\n- 3 ½ feet back from the face of the curb\n- 5 feet from underground utility lines\n- 10 feet from power poles\n- 7 ½ feet from driveways (10 feet recommended)\n- 20 feet from street lights and other existing trees\n- 30 feet from street intersections\n- Concrete Removal. We cannot remove concrete / pavement to create new tree planting locations. If you are planning to remove concrete / pavement yourself, that work must be coordinated with the Department of Transportation. Call 206-684-TREE for more information.\n- Tree Removal & Replacement. We cannot help with street tree removal. All street trees are protected under the Street Tree Ordinance (SMC 15.43) and must be permitted for removal. SDOT may permit a removal if the tree is hazardous, poses a threat to public safety, is in poor health, or cannot be successfully retained due to construction or development. If you\'re considering tree replacement, please apply for a removal permit prior to apply for a replacement tree. Please visit SDOT\'s website or call 206-684-TREE (8733) to learn more.\nIn 2016, the application process changed from first-come-first-serve to a lottery process. This change allows applicants more time to make tree selections and prevent certain species from selling out within minutes, as they have in past years. Nonetheless, the number of trees approved for your yard may be fewer than the number requested. Trees for Neighborhoods has become very popular and thus some trees can sell out very quickly. Small trees are most popular. To increase your chances of receiving a tree, consider planting a larger tree if space allows. If you are not immediately approved for a tree, you will be placed on a waitlist. As participants change their minds or drop out of the program, more trees will become available and will be approved in the order the applications were received.\nHow will I know if my application is approved?\nAfter the first round of the application lottery closes, Trees for Seattle will begin processing applications. Once your application is processed, you willl receive an email with the status of your tree order.\nHow does the lottery work exactly?\nAll applications received will be assigned a random number once the application closes. Trees for Seattle staff process applications sequentially by random number. An entire application order will be processed at once, however only trees still available will be marked as ""Approved"". Trees will be distributed until sold out. Once a species is sold out, requests will be added to a waitlist in the same sequential order and marked on the applicaiton as ""Waitlist"". As trees become available for the waitlist, they will be distributed in order of the position on the waitlist.\nCan I submit more than one application to improve my chances?\nNo, please submit only one application per household. We will only process the first application we receive from your address. Since we will process the entire application at once, it does not benefit you to submit more than one. If you submit your application and would like to make a change before the application closing date, please contact us and we will make that change on your behalf.\nWhich workshop do I choose?\nThere are 3 workshop dates. The first and the last workshop dates are at the Center for Urban Horticulture. The middle date is a limited workshop in south Seattle (exact location to be determined). Even though both workshops at the Center for Urban Horticulture (CUH) are the same for time and content, the first workshop is chosen more often. If you would like to attend a less crowded event, consider signing up for the later CUH workshop. If you are planting in the 98126, 98106, 98108, or 98118 zip codes and getting to the CUH is a hardship for you, choose the workshop in south Seattle. Space in this workshop is limited and will be allocated based on your lottery position. If you do not get in to this workshop, we will contact you to choose one of the workshops at CUH.\nAttention renters! If you do not own your home, you need to ask the permission of the homeowner before applying for trees. Also consider follow-up care. Young trees need regular summer watering through their first three years. If you do not plan to be at your home that long, consider how the young tree will receive the proper care it needs.\nAttention landlords! If you apply for trees for one of your rental properties, please consider future watering and maintenance. If you are unable to water the trees 2x/week during the summer months, please be sure your renter is willing.\nTree Pickup Information\nYou must be present at the planting and care workshop to pick up your tree(s). Indicate which date you are available to attend the workshop on the application.\n- You are responsible for transporting your trees from the pickup site to your home. You may need to rent or borrow a pickup truck if you have a small vehicle or are picking up numerous trees. Remember that having more passengers in the car means less space for trees!\n- The trees will be in 5, 7, or 10 gallon containers and are generally 4-6 feet tall. Staff will help you load your trees on the pick-up days, but you may need to make sure you have help to unload once you are home.\n- All trees should be planted within 1-2 weeks after receiving them. You are responsible for planting the tree, which is why we require each participant to go through a planting workshop at the pickup event. If you are physically unable to plant your tree and would like assistance, please contact us.\n- Your young trees will not survive the dry summer without watering. Water bags will be provided to help you care for your trees in the summer. For more tips about caring for your young tree, click here.\nCheck out our Frequently Asked Questions page for more information.\nContact us with additional questions at TreesForNeighborhoods@Seattle.gov or (206) 684-3979.']"	['<urn:uuid:c43934f8-4a50-40e1-bd6e-8332c072b7fa>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T11:40:26.317848	6	34	2134
60	difference between software architecture abstraction and soa abstraction purpose comparison	In SOA (Service-Oriented Architecture), abstraction is specifically used for decoupling business processes from physical implementations, allowing business operations to be modeled independently of IT infrastructure. In contrast, software architecture abstraction serves to hide implementation details while focusing on run-time elements and their interactions during system operation. Both use abstraction but for different purposes - SOA for business-IT separation, software architecture for system component interaction clarity.	"[""Service-Oriented Architectures and Data Services\nSOAs and Abstraction\nDecoupling is the key concept in SOAs and is achieved through abstraction based on service interfaces. Business processes in an SOA represent a formalized, executable form of the actual enterprise's processes, but offer a layer of abstraction above the physical processes, be they automated or manual. Business processes are composed of business services. Just as business processes in an SOA represent an abstraction from their real-world counterparts, so do business services offer an abstraction of actual physical services. Decoupling through abstraction imbues SOAs with immense potential to model business operations independent of the IT infrastructure du jour.\nSOAs, as their name makes clear, are architectures. These architectures, as we've seen, involve business processes composed of business services. Business processes and services both make use of business information, which is likely resident in many different types and instances of databases and files. This information can be exposed to business services using the same service-oriented paradigm - as data services.\nJust as business processes and services in an SOA represent abstractions - albeit executable ones - of their real-world counterparts, so too do data services represent an abstraction of underlying enterprise information. Data services expose information to business services in a form and through an interface amenable to those services.\nThe form is generally some representation of business objects to be manipulated by business services and passed between services by business processes. Business objects may be simple tabular structures or complex nested structures. Almost always, though, they must be composed from information residing in more than one data source, often in different persistence formats. So a key requirement of data services is that they:\n- expose integrated information in one or more desired formats, even if the original data are in different formats.\nThe desired interface is dependent on the architecture being used. A Web service-based SOA will provide a SOAP or REST-based interface to XML-formatted business objects. A more traditional Java or C-language RCP-based architecture will require JDBC or ODBC access to tabular information, obtained from multiple data sources. So, a second key requirement of data services is that they:\n- expose information through one or more consistent, standard interfaces, even if the original data are accessed through different interfaces.\nThese two key requirements of data services are achieved by two different technologies:\n- modeling to define the required format of data, integrated from the underlying sources; and\n- a query engine for processing these abstract definitions efficiently, exposing the integrated information through one or more interfaces.\nTogether these form the basis for a data services architecture underpinning a robust SOA, making data available to business processes and services in the required format and through consistent, standard interfaces.\n- tei·id (TEE-id)\n- adj. pertaining to a family of tropical American whip-tailed lizards noted for speed and agility.\n- n. a set of open source enterprise information integration tools noted for their ability to rapidly create data services that can quickly adapt to changes in your IT environment."", 'I\'m sure at some point you\'ve had to draw some boxes and arrows to describe the high level view of how a system works. But what are those boxes and arrows called? We often use terms like microservices, monolith, REST, or event driven, but what are these?\nAs part of my apprenticeship I\'ve been reading about formal Software Architecture concepts and definitions, and throughout this post I\'ll explain some of those concepts as they apply to a project I\'ve also been working on during my apprenticeship: a JSON-RPC Playground Console.\nSoftware Architecture is...\nI\'ll use the definition that Roy Fielding, one of the main authors of the HTTP spec and the creator of the REST style, gave in his doctorate dissertation paper (If you\'re interested in Software Architecture, I can\'t recommend this paper enough).\nA software architecture is an abstraction of the run-time elements of a software system during some phase of its operation. A system may be composed of many levels of abstraction and many phases of operation, each with its own software architecture.\nArchitecture is abstraction\nImplementation details can be abstracted away when describing an architecture. Does it matter if an authentication service is written in Elixir or Java? Or is the role that it plays within the system - authenticating users - what we should focus on?\nArchitecture is about run-time\nSource code structure is not a system\'s architecture. Different applications can share the same libraries or modules and yet be completely disconnected from each other when the system is live. We focus on what is processing data and how it is moved around.\nArchitecture is focused in a specific phase of operation\nNot every component will play a role at all times or be part of every process flow. The components involved and their configuration when a system shuts down can be very different than those involved in a system\'s normal mode of operation.\nArchitecture is nested\nWhen abstracting an element\'s implementation, we\'re ignoring details that are not relevant for the current architectural point of view. If we change our focus to a deeper level, a new architecture will emerge with its own set of elements and configuration. We\'ll find many nested architectures until elements are simple enough that they cannot be decomposed anymore.\nA software architecture is created as the result of a series of decisions, each bringing a set of properties and constraints. Whether they are explicit in a diagram or simply living in the minds of architects and developers, there is an understanding or intention of what those decisions and constraints should be. As a system evolves, those initial decisions can very well not match the reality of an architecture. If these differences are undesired or accidental, we say the architecture has eroded, creating an architecture drift or ""technical debt.""\nCase study: JSON-RPC Console\nOne of our clients’ platform is composed of dozens of JVM microservices that talk to each other using the JSON-RPC 2.0 protocol. Each service declares its RPC API using a set of Java interfaces that get published as a ""Service-API"" library (JAR) in a common repository. Clients that want to interact with a service simply have to declare its API JAR as a project dependency. A platform library will generate objects that implement such interfaces and make them available to the rest of the code through dependency injection. From the code perspective you\'re just calling a regular method, but in the background the platform library is executing an RPC call and handling all the plumbing for you. And this is a great productivity booster when writing code!\nHowever, to manually test any of these RPC methods (using a tool like Postman or curl, for example), one has to find the right code base, manually inspect the service interface, its methods and parameters (with possibly many levels of nested objects), and manually build the required JSON payload to execute the call. Documentation helps, but it\'s hard to keep it up to date.\nI decided to create a GUI application that autogenerates forms that can easily be filled in to call any RPC method exposed by a service. The forms are generated from a Service Description file, compatible with JSON-RPC 2.0, that gets created by analyzing the Service-API JAR libraries. By using the same source as the actual code that runs in production, they\'re guaranteed to stay up to date.\nArchitecting a system means making a series of decisions that shape the configuration for the different elements that form a system: components, connectors and data.\nA component is an abstract unit of software instructions and internal state that provides a transformation or performs computations of data via its interface. Components are defined by the service they provide to other components, rather by the implementation behind their interface. If certain behavior cannot be discerned by the rest of the components, then such behavior is not part of the architecture.\nIn the example:\nRPC Console:Transforms a service description to a set of forms, captures user input, executes RPC calls, and displays their result.\nRPC Server:Receives RPC requests, computes them, and returns the result.\nAnalyzer:Transforms Service-API JARs to a Service Description.\nJAR Repository:Stores and serves Service-API JARs.\nService Description Repository:Stores and serves Service Descriptions.\nNote how when we define the\nRPC Server component, for the purpose of this architecture\'s point of view, we\'re not interested in what specific functionality the RPC Server provides, since it\'s not relevant to the rest of the components. We are even grouping many different instances of this component as equals, even though in reality they will functionally behave very differently—one could be a Users service, while the other could be a Books service.\nConnectors enable communication and data transfer between different components. They don\'t transform the data, they just move it between the different components through their interfaces. Internally however, when looking at the architecture of one specific connector, we may find that it\'s actually composed by a subsystem of components that take the data, transform it into a better format for the transmission, send it to the other end, and reverse the transformation before passing it to the rest of the system. Since those transformations are not visible to the rest of the system, we can abstract them away at a higher level.\nIn the example:\nRPC Client:Starts transmission of the RPC call.\nRPC Server:Receives RPC request and returns RPC response.\nHTTP client:Starts HTTP connection to fetch Service Description.\nAWS Library:Transfers a Service Description from the Analyzer to the Service Description Repository.\nGradle Library:Transfers Service-API JARs dependencies from the JAR Repository to the Analyzer.\nIn the case of both the AWS Library and the Gradle Library, we\'re not directly responsible for how these data transfers happen. We can then use a high-level view of the connectors and ignore the details of their implementation.\nMany software architecture definitions don\'t mention data as a core concept, which I believe gives an incomplete picture. Data is the reason a system exists, and is sometimes even the main factor driving a system\'s configuration. Data is defined as the information that is transferred from one component to another via a connector.\nIn the example:\nService Description:Describes the available RPC methods exposed by the services, in a JSON-RPC 2.0 compatible structure. It includes information like server URLs, method names, parameters, and types.\nRPC Request:Includes the RPC method name and its parameters.\nRPC Result:The result of the execution of an RPC invocation.\nService-API JAR:JAR file containing the Java interfaces for the RPC Services.\nAn architectural style is a named collection of architectural design decisions, that when applied under a specific context, puts constraints on the different system elements, their configuration and the way they can relate to each other, and in turn generate a solution with well known beneficial properties for the system.\nStyles are a mechanism for categorizing architectures and for defining their common characteristics. Each style provides an abstraction for the interactions of components, capturing the essence of a pattern of interaction by ignoring the incidental details of the rest of the architecture. A style can focus on only certain aspects of an architecture, and they can even be combined to generate more complex or hybrid styles.\nClient-Server, Microservices, Monolithic, and even REST are all different architectural styles that you have most likely seen applied to dozens of heterogeneous systems.\nCreating our own style\nIf you\'re familiar with tools like Swagger for REST APIs, you probably noticed that my JSON-RPC project has some similarities to it. While my console uses a Service Description tailored for JSON-RPC based services as input, REST APIs have the OpenAPI Standard. These specifications formats that are generated from the source code of a service are a powerful pattern that enable the creation of many different consumer tools: documentation navigators, clients code generators, mock servers, etc...\nLet\'s try to define a generic architectural style for this family of tools that can be applied to any other protocol to get the same benefits: I\'ll call it the Service Description style.\nService Description style\nLet\'s start by defining the different elements of the architecture\n- Target source code: Source code for a Target service\'s interface.\n- Service Description: Protocol-specific format that, following the protocol standard, can describe the interface of any Target service.\n- Generator: Automates the creation of a Service Description from the Target source code and publishes it to the Provider.\n- Repository: Stores and serves a Service Description.\n- Client: Consumes a Service Description from the Repository and uses it as the only source to provide functionality that\'s dynamically tailored for the Target service.\n- Generator -> Repository: Transmits a Service Description from the Generator to the Repository.\n- Repository -> Client: Transmits a Service Description from the Repository to the Client.\nA Service Description has to be created from the source code. Clients need an always-up-to-date Service Description to be functional since they know nothing about the specifics of the Target service unless included in the Service Description. The primary source of truth is the code, and if the process is not automated there\'s a high risk of the Service Description falling out of date and having a broken client. That doesn\'t mean that a Service Description can\'t be built by hand. There are a lot of valid use cases for doing that—for example, if you want to have a mock server before the actual implementation. However, a system that relies on manual tasks will not be considered to be an implementation of this architecture style.\nNote that we put no constraints on how the source code is made available to the Generator. In fact, the Generator can even be implemented as a step in the Target\'s build process (for example with a Maven plugin). A Service description should follow a protocol standard. One of the main benefits of this architecture is reusability of clients against many different Target services that use the same protocol, and for that reason a Service Description can\'t have knowledge of particular implementation details that would only work for one specific service. What functionality a Client provides is not part of the architecture: a Client can interact with the Target service (for example for a playground Console) or not at all (in the case of static documentation). The main restriction behind Clients is that they should have zero knowledge of the implementation details of a Target service other than the information that the Service Description contains. Connectors are defined very loosely since we\'re not putting any restrictions on how information is transferred.\nI hope after reading this post you will now have vocabulary to describe the parts of software architecture that we work with every day. As an exercise, try to imagine how an implementation of the Service Description style would look like for a Websockets-based service, and if you write a tool for it, let me know!']"	['<urn:uuid:9656238d-d5ee-44f7-a4bb-df5a265a1c2e>', '<urn:uuid:879e22d3-7084-468f-b543-4f2c25e622c3>']	factoid	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-12T11:40:26.317848	10	65	2480
61	How do sustainability initiatives in both Islamic law and cryptocurrency address humanitarian concerns?	Islamic law establishes humanitarian principles focusing on justice and protection for all humanity, prohibiting harm to civilians and promoting environmental stewardship. Similarly, the cryptocurrency sector is addressing humanitarian concerns through sustainability initiatives like using renewable energy sources, developing energy-efficient mining equipment, implementing proof-of-stake mechanisms, and carbon offsetting. Both systems recognize the importance of balancing human protection with environmental responsibility, though cryptocurrency's high energy consumption (128.84 TWh annually) presents ongoing challenges to achieving these humanitarian goals.	['On the 10th and 11th of May 2017, a two-day workshop in Kenya’s Nairobi brought together experts from both Islamic law and IHL to discuss issues related to the protection of the civilians during armed conflict. Engaging with Islamic scholars in Eastern Africa is part of the ICRC’s strategy to learn, build up trust, and create an environment conducive to the respect of the universal principles of IHL.\nThis workshop was the third regional event linked to the debate on Islam/IHL in East Africa. Since 2011, the ICRC has organized, with the support and cooperation of Islamic institutions, seminars and meetings that have deepened the understanding of its mandate but also the understanding of Islamic teachings on armed conflicts. The idea behind this workshop was to capitalize, as much as possible, on existing rules and practices related to Islamic law and IHL, and to build and follow up on earlier workshops held in Mbale and in Mombasa in 2011 on the respective topics of “Islam and IHL” and “Deprived of Freedom During Armed Conflict: Islam and IHL”.\nThe workshop attracted around 30 participants from the region (Kenya, Tanzania, Uganda, Djibouti), as well as Sudan and Nigeria. Over the two days, imams, legal researchers, teachers, judges, human rights defenders, all well known in their field of work, debated among themselves and alongside IHL experts on the issue of the protection of civilians during armed conflict.\nProfessor Fassi Fihri Driss, Vice President of the Al Quarayouine University of Fes in Morocco, who was invited as a keynote speaker, elaborated on the many challenges faced by humanitarian actors with regards to today’s conflicts. He focused on the most significant challenges, such as lack of compliance with IHL, lack of access to victims, and challenges faced when communicating with armed groups.\nDespite existing laws, Professor Driss explained, belligerents deliberately violate their obligations to protect civilians. The world continues to witness grave violations of IHL. In this context, it is obvious that civilians cannot receive the protection they are entitled to under humanitarian law. However, these difficulties should not be taken as an excuse to accept the devastating impact of conflict on civilians. It is not the absence of binding law that is causing protection problems, but a persistent failure to comply with the law. Professor Driss stressed the need for humanitarian organizations and Islamic scholars to continue to reflect on these new challenges, and he advocated for continuing to work on IHL, in order to adapt it to new technological challenges and new types of conflict.\nDr. Ahmed Al-Dawoody, the ICRC Legal Adviser on Islamic Law and Jurisprudence, introduced severl historical perspectives of Muslim scholars who consider that in Islamic humanitarian law, the rules of conduct are not necessarily restricted solely to acts of war but also extend to all conduct during war and at any time that a peace treaty applies. The vast and detailed Islamic legal literature concerned with regulating armed conflict reveals that classical Muslim jurists upheld principles that are surprisingly similar to those of modern IHL.\nIn order to better understand the philosophy of Islamic humanitarian law, he underlined the importance of conducting in-depth research into the relationship between the early Muslim communities and the contexts in which wars unfolded between them and their opponents, and how warfare was conducted, during the life of the Prophet and the subsequent caliphates.\nDr. Al-Dawoody quoted some of the core Islamic and humanitarian principles regarding the conduct of hostilities found in the Quran and the Sunna: proportionality (Quran 16:126-128); humanity (Quran 5:32); compassion; nondiscrimination; dignity; equality and fraternity (Quran 4:1), as well as justice (Quran 16:90). In addition, he listed the manifold humanitarian standards that exist in Islamic law, such as:\n– Prohibition of torture, killings and unnecessary suffering\n– Prohibition of excesses and wickedness\n– Protection of civilians and non-combatants\n– Prohibition against indiscriminate weapons\n– Prohibition against indiscriminate attacks\n– Protection of property\n– Prohibition against mutilation\n– Respectful treatment of prisoners of war\nSheikh Suleymane Salim Hamad, Director of Madrassa Al Ulya, Pemba-Zanzibar, explained that the Islamic principles of jurisprudence have as their basis justice for all humanity so that people may live in peace and harmony. When the principles are ignored, then there is danger of corruption and the spilling of blood. Islam stresses principles of justice and humanitarianism at all times, without distinction, irrespective of color, creed, nationality, ethnicity, culture and any other distinguishing features. Islam has raised the respect and dignity of the people, and outlawed war and even disputes among people that may lead to war. In a hadeeth (prophetic tradition) the Prophet is reported to have said that the faith of a Muslim is incomplete if a Muslim unlawfully sheds the blood of an innocent person. God Almighty said: “And fight for those who fight against you, and do not transgress. Allah does not love the aggressors.”\nHe further explained that Islamic jurisprudence preceded IHL in laying down the rules of war and that its rules are also more encompassing than IHL. Islam set out from the beginning the respect for human dignity, relations between states, relations during war time as well as during peace time, with humanitarianism as the bedrock of Islam’s teachings. The teachings include relations between Muslims and non-Muslims, human dignity, even relations between humans and other beings.\nIn the ensuing lively discussions workshop participants agreed that Islamic law contains a rich but complex set of rules on the protection of civilians during armed conflicts. Participants discussed the main existing rules in Islam and IHL that regulate the conduct of hostilities as well as the challenges of interpreting and applying Islamic principles and rules in modern conflicts. They also developed a set of recommendations which will be used in the days forward and encouraged the ICRC to continue to deepen its dialogue through the organization of such meetings, exchanges of information and further research.', 'As the world becomes more conscious of sustainability, the environmental impact of industries and businesses is under the microscope. Cryptocurrency is one of the newer and rapidly evolving industries that has been called out for its high energy consumption and carbon footprint. However, the cryptocurrency sector is also built on innovation, decentralization, and financial democratization. The challenge is to balance these two seemingly opposing goals – sustainability and innovation. In this article, we will explore the environmental concerns of cryptocurrency and ways to mitigate its impact while continuing to promote its benefits.\nRead more: Silvergate Collapse Dragging Down Bitcoin Volume\nCryptocurrency is a digital currency that uses encryption techniques to regulate the generation of units of currency and verify the transfer of funds. It is decentralized and operates on a peer-to-peer network that enables secure, fast, and anonymous transactions. Cryptocurrency has gained popularity in recent years, with bitcoin being the most well-known example. However, the process of generating cryptocurrency, known as mining, requires significant computational power, which consumes a massive amount of energy.\nThe amount of energy consumed by cryptocurrency mining has sparked concerns about its environmental impact. According to a 2021 report by the Cambridge Center for Alternative Finance, the annual energy consumption of bitcoin mining alone is estimated to be around 128.84 TWh, which is more than the energy consumption of Argentina. The high energy consumption of cryptocurrency mining has led to an increase in greenhouse gas emissions, as the majority of the energy used comes from non-renewable sources like coal and natural gas.\nThe Environmental Impact of Cryptocurrency\nCryptocurrency mining is a highly energy-intensive process that requires specialized computer equipment and software. The process involves solving complex mathematical problems to verify transactions and add new blocks to the blockchain. The first miner to solve the puzzle is rewarded with a certain amount of cryptocurrency.\nTo mine cryptocurrency, miners need to use powerful computers, which consume a considerable amount of energy. The energy consumption of cryptocurrency mining is a function of the computing power used and the time it takes to solve the problem. As the difficulty of the problem increases, more computing power is required, which leads to a higher energy consumption.\nThe high energy consumption of cryptocurrency mining has a significant impact on the environment. Most of the energy used to power cryptocurrency mining comes from non-renewable sources like coal and natural gas. The combustion of fossil fuels leads to the release of greenhouse gases like carbon dioxide, which contributes to global warming and climate change. The increase in greenhouse gas emissions from cryptocurrency mining is a concern as it undermines global efforts to reduce carbon emissions.\nMitigating the Environmental Impact of Cryptocurrency\nThe environmental impact of cryptocurrency mining can be mitigated by adopting sustainable practices. Some of the ways to reduce the energy consumption of cryptocurrency mining are:\nUsing renewable energy sources: One of the most effective ways to reduce the environmental impact of cryptocurrency mining is to use renewable energy sources like solar, wind, and hydroelectric power. Some cryptocurrency mining companies are already using renewable energy to power their operations. For example, the cryptocurrency mining company, Square, has committed to becoming carbon neutral by 2030.\nDeveloping energy-efficient mining equipment: Cryptocurrency mining companies can reduce their energy consumption by developing energy-efficient mining equipment. For example, some companies are developing ASICs (application-specific integrated circuits) that are designed to consume less energy than traditional computer processors.\nImplementing proof-of-stake consensus mechanism: Another way to reduce the energy consumption of cryptocurrency mining is to implement the proof-of-stake consensus mechanism. Proof-of-stake is an alternative to proof-of-work, which is the current consensus mechanism used by most cryptocurrencies. Proof-of-stake does not require miners to solve complex mathematical problems to verify transactions. Instead, it relies on a random selection process to choose a validator who is responsible for verifying transactions. Validators are required to hold a certain amount of cryptocurrency, which acts as a stake. If a validator acts maliciously, their stake is forfeited. The proof-of-stake consensus mechanism is less energy-intensive than proof-of-work, making it a more sustainable alternative.\nCarbon offsetting: Cryptocurrency mining companies can offset their carbon emissions by investing in renewable energy projects or purchasing carbon credits. Carbon offsetting is a way to neutralize the carbon emissions associated with cryptocurrency mining by investing in sustainable projects that reduce carbon emissions.\nBalancing Sustainability and Innovation\nCryptocurrency has the potential to revolutionize the financial industry by providing financial democratization and decentralization. However, this cannot come at the expense of the environment. The challenge is to balance sustainability with innovation. The cryptocurrency industry needs to take responsibility for its environmental impact and take steps to mitigate its carbon footprint. It is essential to recognize that sustainability and innovation are not mutually exclusive goals, but rather complementary.\nSustainability is a key factor in the long-term success of cryptocurrency. As the world becomes more conscious of the environment, consumers are looking for sustainable products and services. The cryptocurrency industry needs to recognize this trend and take steps to reduce its carbon footprint. By adopting sustainable practices, the cryptocurrency industry can attract a more environmentally conscious audience.\nCryptocurrency has the potential to transform the financial industry by providing financial democratization and decentralization. However, the high energy consumption of cryptocurrency mining has sparked concerns about its environmental impact. To balance sustainability and innovation, the cryptocurrency industry needs to take responsibility for its carbon footprint and adopt sustainable practices. By using renewable energy sources, developing energy-efficient mining equipment, implementing proof-of-stake consensus mechanism, and carbon offsetting, the cryptocurrency industry can mitigate its environmental impact. Sustainability is not a trade-off for innovation, but rather a complementary goal that is essential for the long-term success of the cryptocurrency industry.']	['<urn:uuid:cd33607e-d571-4000-aeee-35d89b089436>', '<urn:uuid:16d423b7-32cb-493a-822d-c068233478c4>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T11:40:26.317848	13	75	1929
62	What are good tourism practices and bad tourist behaviors cities are fighting?	Good tourism practices include carbon offsetting through verified schemes, staying longer in one place, and participating in community-based tourism that benefits locals. Cities are combating bad tourist behaviors through measures like Milan's ban on food trucks and selfie sticks, Rome's rules against 'uncouth behavior' near monuments, and Venice and Dubrovnik's monitoring of tourist numbers in historic areas.	"['A growing number of travelers want their journeys to be less invasive and more beneficial to the local community. They want to better understand the culture of the people they meet in the places they visit. Visitors should be mindful that we are entering a place that is someone else\'s home. Sounds complicated? Try this — imagine what irresponsible tourism looks like and then imagine its opposite.\nWhat exactly is responsible travel?\nResponsible tourism has several goals: sustainability, environmental integrity, social justice and maximum local economic benefit. Responsible tourism asks individuals, organizations, governments and businesses to take responsibility for their actions and the effects of their actions. Everyone involved must be responsible for sustainability.\nMost principles of responsible tourism were put forth in the Cape Town Declaration on Responsible Tourism in Destinations (Responsible organization).\nResponsible travel vs Ecotourism\nThere is an annual Responsible Travel conference, held in a different country each year.\nWorld Responsible Day is celebrated in Europe on June 2.\nResponsible Tourism Day is held on November 7.\nCommunity-based tourism (CBT) is a form of sustainable development where small, rural communities set up accommodation & activities to generate tourism. CBT allows travelers to experience life in such communities—taking part in language or cooking lessons, eating freshly-prepared meals, experiencing local music and dance, and venturing with a local guide to nearby nature/landscape attractions—while the community gains much-needed revenue. NGOs and aid organizations (like the U.S. Peace Corps) help villages establish CBT facilities, organize appropriate activities, and establish governance of the project to ensure that revenue is shared with the community. Local villagers earn money for providing homestay accommodation, becoming guides, providing lessons, growing extra produce, and creating art/crafts for sale, while a portion of revenue (20-50%) is typically reserved in a community fund that can be used to improve the CBT experience or be used for development purposes. The number of CBT projects is growing. CBT is established in Central America, Central Asia, & many countries in Africa; countries with well-established CBT projects are Guatemala, Costa Rica, Kyrgyzstan, Ghana, & Uganda.\nTravelers who participate in CBT will typically book a package online for a stay of one day to one week. Bookings are handled by someone in the community, not a commercial tour operator. Accommodation is simple but sufficient by Western standards, with a private room, bed, telephone access, and a private bathroom (don\'t expect a toilet & shower, but at least an enclosed room with a hole in the ground and water...you should not be left to do your business outdoors or in a grotesque community toilet). Food will consist of local snacks, lunches, and at least one meal will be a smorgasbord of local dishes to taste (a dozen or so dishes are prepared, but it\'s there for everyone to share). Travelers can take lessons from locals in activities such as cooking, drumming, singing, dancing, body-painting, hunting/fishing (traditional methods), native medicines, or playing a traditional game with some of the village children or elders. A local guide will be able to take you to nearby attractions, like waterfalls or rainforest, or walk along trails or ride horseback. In some communities and especially during longer stays, travelers may have the opportunity to volunteer on development projects.\nIn the development of many tourism projects, indigenous people have not been considered as valued stakeholders from the start. In the worst cases, they are not listened to in the development of \'charitable\' projects. Adequate consultation is a must.\nIndigenous peoples manage more than 40% of all IUCN-recognized protected areas in the world, and many of them - if not most - use tourism as a complement their economic benefits from these areas. Yet the challenge for travelers is finding which communities wish to be visited and with which protocols. In 2012 the Global Workshop for Indigenous and Local Communities: Biodiversity, Tourism and the Social Web took place at the 11th meeting of the Conference of the Parties to the Convention on Biological Diversity.\nSee also: Ecotourism.\nWhile nearly all forms of transport a traveler uses will release carbon dioxide into the atmosphere, aircraft are especially notorious offenders and the aviation industry is the fastest-growing contributor to the acceleration of climate change. This is not just due to the vast distances traveled, but because they release greenhouse gases high in the atmosphere where their effects are more potent. On long-haul flights, the amount of carbon dioxide released is roughly equivalent to a car traveling the same distance with one passenger. A flight from London, UK to Perth, Australia releases the equivalent of 4.5 tonnes of carbon dioxide, or about half the average yearly emissions of a person in the U.K. Shorter flights have higher emissions than longer ones per km traveled due to the amount of fuel used taxiing, and during take off. (See: Environmental impact of aviation on Wikipedia).\nThe benefits of travel in increasing an individual\'s cultural awareness and knowledge are immeasurable. Despite its effects, air travel is essential to the modern world and traveling. There are ways in which individuals wanting to travel responsibly can offset their impact on the environment. For example, they can use an airline that has been rated more environmentally aware  [dead link] or they can use carbon offset schemes. These schemes collect money which is transferred to projects, like installing renewable energy or planting trees, which generate zero/low-carbon energy or reduce levels of greenhouse gasses. By purchasing carbon offset ""credits"" through these schemes, travelers are investing in portions of projects which, over their lifespan, will reduce/eliminate carbon emissions (through the burning of fossil fuels) equivalent to the amount emitted on their flight. Reputable carbon offset schemes are independently verified and adhere to an international standard for measuring offset emissions.\nCarbon offset can be calculated & purchased by individuals, through an agency like ClimateCare, or through your carrier. Airlines offering carbon offset programs for their passengers include: British Airways, Cathay Pacific, Delta Air Lines, easyJet, Lufthansa, Qantas, & United Airlines.', 'The Problem of Overtourism\nProblem of Overtourism\nTourism is a double-edged sword: it brings economic prosperity to a community, but can also be a burden on its resources. Cities and countries walk the razor’s edge between welcoming visitors in order to maintain an important revenue stream, and throwing the gates wide open, so to speak, while their citizens endure a living nightmare.\nIn May 2019, Amsterdam, poised to take in 42 million visitors every year by 2030, is set to impose a strategy called “develop and discourage“, which involves “actively dissuading” people from visiting through various means (closure of tourist attractions, tourist taxation) as tourism is having a negative impact on the residents’ standard of life.\nA few years ago, in 2017, tensions have risen to such a fever pitch in Barcelona, due to the surge in visitors and Airbnb’s impact on the city’s housing market, that youth protesters slashed the tyres of rental bicycles and a tour bus. “Anti-tourism” demonstrations were also held in Venice, a small city of 55,000 people which swells to more than 20 million during tourist seasons.\nWe’ve covered the various reasons for this surge in tourism in a previous article. Reasons include:\n- increased mobility due to growing wealth\n- greater conveniences in travel, such as online booking\n- the sharing economy (e.g. Airbnb and Uber) offering cheaper alternatives to traditional holiday accommodations and transport\n- emergence of the low-cost airline sector\n- increased awareness of places to visit\nOvertourism is defined “as the excessive growth of visitors leading to overcrowding in areas where residents suffer the consequences of temporary and seasonal tourism peaks, which have enforced permanent changes to their lifestyles, access to amenities and general well-being”.\nLocals protest against the impact of overcrowding or “overtourism” in their region–the rising cost of rental properties, the negative impact on the environment, the anti-social behaviour of seasonal visitors, and the overall pressure placed by a throng of people descending upon a town that can no longer sustain it.\nSome cities appear to be at their wit’s end: Milan, for example, introduced a ban on food trucks and selfie sticks and Dubrovnik set up cameras to monitor the number of people entering their historic old town. In June 2019, as reported by The Guardian, Rome imposed new rules, updating 1946 legislation, in order to stop “uncouth behaviour” in the Italian capital. Among these rules is a permanent ban on “modern-day centurions – men who dress up in tunics and leather breastplates, wielding plastic swords”, a rule against “messy” eating near beloved monuments, and possible future “severe fines” against people who take a dip in city fountains.\nTaxes and Fees for Tourists\nOther cities and countries hit tourists where it hurts: their wallets. Hvar in Croatia slap drunk tourists with huge fines (up to 700 euros).\nWaitangi in New Zealand has only about 2,000 people, but more than 15 times as many domestic and foreign visitors flock to the town famous for witnessing the signing of a treaty between the Maori chiefs and the British Crown. Its central importance in New Zealand’s history has made Waitangi a tourist draw, but tourism is also becoming a burden on its natural resources. In an effort to curb overcrowding and to finance developments, the New Zealand government will be imposing a one-time tourist tax of around $35.\nA more extreme two-tier system penalising visitors has been noted in Venice in 2015: a day that costs a local family 12.40 euros costs outsiders 136 euros. This year, Venice will be imposing a day-tripper tourist tax of $16 for visitors who want to gain access to its historic centre.\nProf. Sally Everett writes that having tourists pay more than locals is “morally defensible”.\nA form of “tourist tax” on food, accommodation and attractions may seem unfair (even discriminatory), but tourism shouldn’t be a one-way transaction. Foreign visitors often place significant pressure on scarce and limited resources at certain times of the year. We all need to recognise this impact.\nThese fees will also, ideally, provide governments with revenue to upgrade infrastructure in order to accommodate the demand and let tourism-dependent and popular places breathe from the strain.\nWhat can tourists do?\nThe latest numbers by the World Travel and Tourism Council (WTTC) show that the travel and tourism sector accounts for 10.4% of global GDP and 313 million jobs, or 9.9% of total employment, in 2017. This means one out of every 10 jobs in the world is supported by the sector–and “ending” tourism will have an incredible impact on the world economy. Taleb Rifai, then Secretary General of the UN World Tourism Organisation, told the Guardian in 2017 that tourism can be the “best ally” to conservation, preservation and the community, and “should not be given up for the sake of mismanagement”.\nIn the first of a series of articles in August 2018, the WTTC stresses that overtourism is not a new problem, and though media stories focus on the effect on locals, overtourism also negatively impacts the tourists themselves. We’ve written about travellers to Roys Peak in Lake Wanaka in New Zealand suffering from long queues brought about by hikers who want to snap the perfect Instagram-worthy shot with views of Mount Aspiring and the Southern Alps. The Guardian posted a photograph by Lucas Stevanko on Twitter showing a long line of hikers waiting for their turn to take a photograph at the peak. The article shares that “it was now essential to leave as early as 3 AM to avoid congestion and crowds on the track.”\nMcKinsey and WTTC have identified five approaches to combat overcrowding.\n- Smooth visitors over time by encouraging them to visit during non-peak times of day, season, and year through actions such as arrival limits and ticketing systems.\n- Spread visitors across sites, for example, by developing new attractions and promoting less-popular sites and areas.\n- Adjust pricing to balance supply and demand by, for example, introducing variable or tiered pricing.\n- Regulate accommodation supply through regulations on home-sharing and additional hotel rooms.\n- In dire situations, limit access and activities to protect natural and cultural integrity.\nThis advice are for the tourism officials. What can tourists do to help?\nAccording to the WTTC:\nTravel outside the peak seasons.\nPopular travel seasons are popular for a reason: the weather. No one wants to visit the beach during hurricane season, and how can you go skiing if there’s no snow? In most of the Northern Hemisphere (except ski resorts), summer (June to September) is the peak season for travellers. In the Caribbean, a popular beach destination, the peak season is December to April, corresponding with winter in North America, where most of the tourists come from.\nTravellers will squeeze through the crowd and pay the high prices of flights and accommodations just to have that perfect weather for their vacation.\nAt Odyssey Traveller, we specialise in educational travel for seniors and research “shoulder seasons”, which occur before or after the peak travel season, promising pleasant weather but with thinner crowds, giving you a better experience in the places we visit. For example, our Dinosaur tour in China occurs in April and October. We have a diverse mix of tours available all-year-round, offering flexibility and helping distribute the flow of tourists equally. We have in the last few years been offering more and more small group tours that take place at the end of the European winter in late February.\nWhilst travelling in the shoulder and low season may initially seem alien, when you look at Paris below in winter sunshine, with no wind and minimal wait times, you have to ask yourself: is this not a better time to explore as a responsible citizen?\nStay longer in one place.\nOver the last 20 years, with airline prices dropping, British travellers have gone on more holiday breaks, but they’re shorter than they used to be, down to 7 to 10 days from the traditional two-week holiday. Since the holiday breaks are shorter, travellers are less likely to venture out of crowded city centres.\nWTTC encourages people to take fewer trips, but to make each trip long enough to really get to know a place.\nSenior travellers are usually retirees with the luxury of time, allowing them to go on extended vacations. Odyssey Traveller tours last around 20 days, like this in-depth tour of Uzbekistan, Kazakhstan, Kyrgyztan, and Turkmenistan, giving travellers a chance to truly immerse themselves in cities and countries and have a deep understanding of their history and culture.\nGet off the beaten track.\nWTTC reports that 70% of visitors are concentrated in 20% of the countries, which means we need to consider travelling further and wider instead of simply following trends.\nOdyssey Traveller specialises in crafting unforgettable experiences for travellers, providing adventure and educational programs to small groups since 1983. Some of these trips may once be considered elusive dreams, but are now made accessible with the help of our Program Leaders and local guides. Our tours include, among many others, immersive trips to see dinosaur fossil sites in China and Argentina and exploring emerging destinations such as Armenia, Georgia and Azerbaijan or Iran where we started to visit since 2003.\nWe always endeavour to include places that may not be famous but will provide deeper insight into a country’s heritage. In Kazakhstan, for example, we will visit the big cities of Nur-Sultan (formerly Astana) and Almaty but also stay in village guesthouses in Ulytau.\nWe have guides that will help us understand local customs and ensure that we enter and leave a foreign country as thoughtful guests. We also regularly publish articles to give prospective and loyal Odyssey clients backgrounders about the places we visit.\nOur small-group approach (Odyssey groups can be as small as four people, typically with a maximum of 14) minimises our impact on the environment and local infrastructure, allowing us to use public transportation and/or small vehicles, or go on long hiking and walking trips without subjecting locals to visual pollution. We also seek out local craftspeople and smaller shops to ensure that the money we spend truly goes to the community we are visiting.\nTravelling with a small group also ensures that you travel with flexibility and with good company, and are given adequate attention by our Program Leader.\nAbout Odyssey Traveller\nOdyssey Traveller is committed to charitable activities that support the environment and cultural development of Australian and New Zealand communities. We specialise in educational small group tours for seniors, typically groups between six to 15 people. Odyssey Traveller has been offering this style of adventure and educational programs since 1983.\nWe are also pleased to announce that since 2012, Odyssey Traveller has been awarding $10,000 Equity & Merit Cash Scholarships each year. We award scholarships on the basis of academic performance and demonstrated financial need. We award at least one scholarship per year. We’re supported through our educational travel programs, and your participation helps Odyssey Traveller achieve its goals.\nExplore the History, Culture and Wildlife of West Africa: Ghana, Togo & Benin Tour The History, Culture & Wildlife explored\nThis tour concentrates on the history, culture and wildlife of coastal Central Africa. Meet the friendly local people and come to a greater understanding of just what has made them what they are today.\nOn this small group tour we explore the country’s natural wonders as well as its colonial past. Madagascar has a range of extraordinary plant and animal life which we will have the chance to view in the island’s National Parks and Nature Reserves. While on the tour we will also learn about both the Portuguese and French periods of control.\nVisit the largest landlocked country in the world, Kazakhstan. Once part of the Mongol and Russian Empires, Kazakhstan is where man first tamed wild horses on the vast Kazakh Steppe, the dry grassland that dominates its landscape and connects it with Europe and the rest of Asia.\nThe centre of the island offers rolling hills, year round cool climate and former British colonial hill stations complete with Tudor hotels, rose gardens and the oldest golf course in Asia. Scattered around the country are National Parks filled with game. The world’s first game reserves were founded in Sri Lanka over 2,000 years ago.\nThis Odyssey is designed for the adventurous traveller, a traveller who is prepared for a range of accommodation styles, for walking excursions and some long travel days in 3 Central Asian countries. The program combines experiences with great scenery, ancient and modern histories, family visits and lifestyles that differ dramatically.']"	['<urn:uuid:1ee41135-c92c-435f-9a0e-fb16a05e0a93>', '<urn:uuid:0f98451f-47a9-4c75-9a13-5ae0b16647c2>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T11:40:26.317848	12	57	3094
63	how many north carolina men confederate army	About 125,000 North Carolina men served in the Confederate armed forces during the Civil War, more than from any other Confederate state.	"['North Carolina HistoryEdit This Page\nFrom FamilySearch Wiki\nUnited States > North Carolina ] > Delaware History Effective family history research requires some understanding of the historical events that affected your family and the records about them. Learning about wars, governments, laws, migrations, and religious trends can help you understand political boundaries, family movements, and settlement patterns.\nState, county, and local histories often contain biographical sketches of local citizens, including important genealogical information. This may be one of the best sources of information for some families.\nThe following important events in the history of North Carolina affected political boundaries, record keeping, and family movements:\n- 1560s: The area that is now North Carolina was visited by the Spaniard Pardo and his soldiers. Records are at the North Carolina State Archives.\n- 1650s: The first permanent settlements were made in the Albemarle Sound area by colonists from Virginia.\n- 1706–1725: French Huguenot, German, and Swiss colonists founded towns near the coast.\n- 1711-1713: (September 23,) Tuscarora Indian War began with massacre of settlers on Chowan and Roanoke River, North Carolina. New Bern was abandoned. White encroachment wihch included the enslaving of Indian children led to the war.\n- 1713: The Tuscarora tribe moved to New York after the war.\n- 1730–1770: Scottish Highlanders immigrated directly to North Carolina ports. People of Ulster Scottish and German descent arrived from Pennsylvania and Virginia using the Great Philadelphia Wagon Road.\n- 1753: Moravian colonists arrived.\n- 1760–1768: Private citizens formed groups to protect law and order. This was known as the Regulation Movement. The participants were known as Regulators.\n- 1768–1771: The War of the Regulation took place.\n- 1789: North Carolina became a state and ceded what is now Tennessee to the United States government.\n- 1828-1839: Cherokee removal to Oklahoma known as the ""Trail of Tears""\n- 1850: One-fourth of all native North Carolinians had left the state and were living in other states or territories.\n- 1861: North Carolina seceded from the Union. It was readmitted in 1868. About 125,000 North Carolina men served in the Confederate armed forces during the Civil War, more than from any other Confederate state.\n- 1862–1907: Twenty-four counties in southern and western North Carolina suffered major losses of records, mostly due to fire and war.\n- 1898: Over 300,000 men were involved in the Spanish-American War which was fought mainly in Cuba and the Philippines.\n- 1917–1918: More than 26 million men from the United States ages 18 through 45 registered with the Selective Service. World War I over 4.7 million American men and women served during the war.\n- 1930\'s: The Great Depression closed many factories and mills. Many small farms were abandoned, and many families moved to cities.\n- 1940–1945: Over 50.6 million men ages 18 to 65 registered with the Selective Service. Over 16.3 million American men and women served in the armed forces during World War II.\n- 1950–1953: Over 5.7 million American men and women served in the Korean War.\n- 1950\'s–1960\'s The building of interstate highways made it easier for people to move long distances.\n- 1964–1972: Over 8.7 million American men and women served in the Vietnam War.\nHistories are great sources of genealogical information. Many contain biographical information about individuals who lived in the area, including:\nSome of the most valuable sources for family history research are local histories. Published histories of towns, counties, and states usually contain accounts of families. They describe the settlement of the area and the founding of churches, schools, and businesses. You can also find lists of pioneers, soldiers, and civil officials. Even if your ancestor is not listed, information on other relatives may be included that will provide important clues for locating your ancestor. A local history may also suggest other records to search. Local histories are extensively collected by the Family History Library, public and university libraries, and state and local historical societies. The United States Research ""History"" page cites nationwide bibliographies of local histories which include histories of North Carolina.\n- North Carolina Local History: A Select Bibliography \nThe Draper Manuscript Collection\nThe Draper Manuscript Collection is a significant regional source that includes records of North Carolina.\n- Draper Manuscript Collection The collection consists of nearly 500 volumes of manuscripts, papers, and books collected by Lyman Copeland Draper about the history of the trans-Allegheny West, a region including the western areas of the Carolinas and Virginia, all the Ohio River Valley, and part of the upper Mississippi Valley from the 1740s to 1830. The collection is divided into 50 series. Some series are titled by geographic area, some by the names of prominent frontier leaders, and some by topic. The bulk of the collection consists of notes from interviews, questionnaires, and letters gathered during Draper’s extensive travels and research to learn about frontier history. Personal papers are much more rare than government or military records.\nThe collection includes many items of a genealogical or biographical nature. For an inventory and partial indexes, see:\n- Guide to the Draper Manuscripts  This guide gives series and volume descriptions for some of the Draper manuscripts. There are several indexes at the end of the book, including a name and subject index, an additional personal data index, and a list of references to Illinois.\n- Index to Lyman C. Draper Manuscripts.  The name index gives the series and volume numbers, but is not complete.\nState Histories Useful to Genealogists\nGood genealogists strive to understand the life and times of their ancestors. In this sense, any history is useful. But certain kinds of state, county, and local histories, especially older histories published between 1845 and 1945, often include biographical sketches of prominent individuals. The sketches usually tend toward the laudatory, but may include some genealogical details. If these histories are indexed or alphabetical, check for an ancestor\'s name. Some examples for the State of North Carolina are:\nNorth Carolina histories are available in all major research centers. A good example is:\n- The History of a Southern State, North Carolina. \nUnited States History\nThe following are only a few of the many sources that are available:\n- The Almanac of American History,   This provides brief historical essays and chronological descriptions of thousands of key events in United States history.\n- Dictionary of American History, Revised ed   This includes historical sketches on various topics in U.S. history, such as wars, people, laws, and organizations. A snippet view is available at Google books.\n- Webster\'s Guide to American History: A Chronological, Geographical, and Biographical Survey and Compendium    This includes a history, some maps, tables, and other historical information.\nTo find more books and articles about North Carolina \'s history use the Internet Google search for phases like ""North Carolina history."" Family History Library Catalog Surname Search lists many more histories under topics like:\n- NORTH CAROLINA - HISTORY\n- NORTH CAROLINA , [COUNTY] - HISTORY\n- NORTH CAROLINA , [COUNTY], [TOWN] - HISTORY\n- NORTH CAROLINA , BIBLIOGRAPHY\n- ↑ Stevenson, George. North Carolina Local History: A Select Bibliography. Rev. ed. Raleigh, North Carolina: North Carolina Department of Cultural Resources, c1984. (Family History Library book 975.6 H23sg.)\n- ↑ Filby, P. William. A Bibliography of American County Histories. Baltimore: Genealogical Publishing, 1985. (FHL book 973 H23bi)\n- ↑ Worldcat\n- ↑ Kaminkow, Marion J. United States Local Histories in the Library of Congress. 5 vols. Baltimore: Magna Charta Book, 1975-76. (FHL book 973 A3ka.)\n- ↑ Worldcat\n- ↑ Draper, Lyman Copeland. Draper Manuscript Collection Chicago, Illinois: University of Chicago Library, 197–?. (On 147 Family History Library films beginning with 889098.)\n- ↑ Harper, Josephine L. Guide to the Draper Manuscripts. Madison, Wisconsin: State Historical Society of Wisconsin, 1983. (Family History Library book 977.583/M1 A3h.)\n- ↑ Wolfe, Barbara Schull. Index to Lyman C. Draper Manuscripts. Logansport, Indiana: B.S. Wolfe, 197–?. (Family History Library book 977.583/M1 A3w.)\n- ↑ Lefler, Hugh T., and Albert R. Newsome. The History of a Southern State, North Carolina. Rev. ed. Chapel Hill, North Carolina: University of North Carolina Press, c1973. (Family History Library book 975.6 H2Lh, 1973.)\n- ↑ Schlesinger, Jr., Arthur M. The Almanac of American History. Greenwich, Conn.: Bison Books, 1983. (FHL book 973 H2alm)\n- ↑ Worldcat\n- ↑ Dictionary of American History, Revised ed., 8 vols. New York: Charles Scribner\'s Sons, 1976. (FHL book 973 H2ad.)\n- ↑ Worldcat\n- ↑ Webster\'s Guide to American History: A Chronological, Geographical, and Biographical Survey and Compendium. Springfield, Mass.: G&C Merriam, 1971. (FHL book 973 H2v)\n- ↑ Limited view at Google Books\n- ↑ Worldcat\n- ↑ Writings on American History By American Historical Association, Library of Congress, United States National Historical Publications Commission, Published by KTO Press, 1921 FHL book 973 H23w\n- ↑ Worldcat']"	['<urn:uuid:0cddb7f4-9da7-4947-9d27-241266ecf719>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T11:40:26.317848	7	22	1458
64	I'm interested in online forex trading and want to stay safe - what are the key organizations in the US that protect investors like me, and how do they work together to regulate forex trading?	In the United States, there are two main organizations protecting forex investors. The Commodities Futures Trading Commission (CFTC) is a government organization, while the National Futures Association (NFA) is an independent self-regulatory agency. They work together to safeguard market integrity and protect investors. The NFA provides licenses to eligible forex brokers, enforces capital requirements, combats fraud, and maintains detailed record-keeping requirements. The NFA also operates programs like arbitration services, anti-money laundering programs, and provides investor alerts. Additionally, both organizations maintain the BASIC database, which allows investors to verify broker credentials and check disciplinary actions against firms or individuals.	"['Foreign currency exchanges (forex) run constantly across the globe through over-the-counter markets. The boundaryless space allows seamless access. For example, an Australian trader can trade in euros and Japanese yen (EURJPY) through a U.S.-based broker despite geographical boundaries.\nSpeculative trading in the retail forex market continues to grow. As a result, there can be intermediaries (banks or brokers) who engage in financial irregularities, scams, exorbitant charges, hidden fees, and high-risk exposure offered through high-leverage levels or other bad practices.\nInternet and mobile app-based trading allow smooth trading processes, but they also add the risk of unrecognized sites that may close unexpectedly and abscond with investors’ money. Regulations ensure such practices are avoided. Regulations are aimed at protecting individual investors and ensuring fair operations to safeguard clients’ interests.\nThe most important criteria when selecting a forex broker are the regulatory approval status of the broker and its governing authority.\nHow U.S. Authorities Regulate Forex Brokerage Accounts\nThe National Futures Association (NFA) is the “premier independent provider of efficient and innovative regulatory programs that safeguard the integrity of the derivatives markets” (including forex). The scope of NFA activities is as follows:\n- To provide necessary licenses (after due diligence) to eligible forex brokers to conduct forex trading business\n- To enforce required adherence to necessary capital requirements\n- To combat fraud\n- To enforce detailed record-keeping and reporting requirements regarding all transactions and related business activities\nA detailed regulatory guide is available on the official NFA website.\nKey Provisions of U.S. Regulations\nCustomers defined as ""individuals with assets of less than $10 million and most small businesses,"" underscoring that these regulations are meant to protect the small investor. High-net-worth individuals may not be covered under standard regulated forex brokerage accounts. Below are several further provisions:\n- The available leverage is limited to 50:1 (or a deposit requirement of only 2% on the notional value of a forex transaction) on the major currencies so that uneducated investors do not take unprecedented risks. Major currencies are defined as the U.S. dollar, the British pound, the Swiss franc, the Canadian dollar, the Japanese yen, the euro, the Australian dollar, the New Zealand dollar, the Swedish krona, the Norwegian krone, and the Danish krone.\n- The available leverage is limited to 20:1 (or 5% of the notional transaction value) on minor currencies.\n- For short forex options, the notional transaction value amount plus the option premium received should be maintained as a security deposit.\n- For long forex options, the entire option premium is required as security.\n- The first-in-first-out (FIFO) rule prevents holding simultaneous positions in the same forex asset, that is, any existing trade position (buy/sell) in a particular currency pair will be squared off for the opposite position (sell/buy) in the same currency pair. This also implies no possibility of hedging while trading forex.\nNotably, money owed by the forex broker to the customers should be held only at one or more qualifying institutions in the United States or in money-center countries.\nHow U.S. Regulations Differ\nCare should be taken to verify the ownership, status, and location of each forex trading firm, website, or app before signing up for a trading account. There are many websites claiming low brokerage charges and high leverage (allowing more trading exposure with less capital); some as high as 1000:1.\nHowever, almost all sites are hosted and operated from outside the United States and may not be approved by the concerned authority in the host country. Even those authorized locally may not have regulations that apply to U.S. residents. Regulations on offered leverage, required deposits, reporting requirements, and investor protections will vary by country.\nForeign Regulatory Bodies\nHere is a list of forex brokerage regulators for a few select countries:\n- Australia: Australian Securities and Investments Commission (ASIC)\n- Cyprus: Cyprus Securities and Exchange Commission (CySEC)\n- Russia: Federal Financial Markets Service (FFMS)\n- South Africa: Financial Sector Conduct Authority (FSCA)\n- Switzerland: Swiss Federal Banking Commission (SFBC)\n- United Kingdom: Financial Services Authority (FSA)\nVerifying a Broker’s Regulatory Status\nThe NFA provides an online verification system called Background Affiliation Status Information Center (BASIC) where the status of U.S.-based forex brokerage firms can be verified using their NFA ID, firm name, individual name, or pool name. Care should be taken to use the correct name/ID in the correct form as many forex broker firms are known by different names (e.g., a website name may be different from the legal corporate name).\nThe Bottom Line\nFinancial regulations are complex and often change as markets develop. They also attempt to strike a balance. Too little regulation may lead to ineffective investor protection, while too much regulation can result in reduced global competitiveness and dampen economic activity.', 'The United States has two principal organizations which oversee and regulate the retail forex market.\nThe first consists of the Commodities Futures Trading Commission or CFTC that is a U.S. government organization. The second is the National Futures Association or NFA that operates as an independent self regulatory agency.\nBoth organizations help oversee financial markets in the United States, with the primary difference between them being that one (the CFTC) is a government dependency, while the other (the NFA) is an independent entity created by government legislation.\nA Short History of the NFA\nAfter the establishment of the CFTC in 1974, along with a necessary amendment to the Commodity Exchange Act of 1936, the path was opened for the creation of an industry wide national self regulating organization in the United States.\nNevertheless, the envisioned self regulatory agency was not created until 1982 when the National Futures Association or NFA was eventually formed. This independent agency has served as a watchdog for the futures industry ever since. The NFA is based in Chicago and operates another office located in New York City.\nCurrent and Future Purposes of the NFA\nAccording to their website, the NFA’s stated purpose is as follows:\n“NFA strives every day to safeguard market integrity, protect investors and help our Members meet their regulatory responsibilities.”\nWith upcoming proposed legislation that will significantly affect the retail forex market operating in the United States, the NFA may soon be required to register a much larger number of individuals and corporations that do business with the forex trading public.\nThese might include Introducing Brokers or IBs, Commodity Pool Operators or CPOs, Futures Commission Merchants or FCMs and Commodity Trading Advisors or CTAs that operate as or with forex brokers executing trades in the foreign exchange market.\nOnce the legislation is approved in its final form, these same forex market participants may also be required to register with the CFTC.\nNFA Regulation and Compliance\nThe NFA operates extensive programs to monitor and supervise its members to safeguard the integrity of the financial markets that it oversees. These programs include:\n- Arbitration and Mediation Services\n- Anti Money Laundering Programs\n- Enhanced Supervision\n- Disclosure documents for CPOs and CTAs\n- NFA Investor Alerts\n- Ongoing Educational Resources for Members and Investors\nThe NFA also works closely with the CFTC which has within its jurisdiction the ability to regulate and oversee futures markets and exchanges. The NFA, on the other hand, regulates every individual or firm which conducts futures trading business with the investing public.\nTrading in the forex market qualifies as futures trading in the United States mainly because delivery for spot in the forex market usually consists of two business days — in effect acting as a very short futures contract. The NFA works to ensure that the investing public that participates in the forex market is treated fairly and that their funds are safeguarded from fraud and other forms of misappropriation or unfair withholding.\nFurthermore, because individual accounts are required to be segregated from the broker’s account, the U.S. based forex broking customer generally can have some security that their funds are not subject to creditor claims against a Futures Commission Merchant or FCM should the broker become insolvent.\nNevertheless, the NFAs website cautions forex traders as follows:\n“Even though an FCM is required to segregate customer funds, customers still may not be able to recover the full amount of any funds in their account if the firm becomes insolvent and there are insufficient funds available to cover the obligations to all of its customers. Customer accounts are not insured. Customers should ask their broker about account protection and should be aware of the limitations imposed on the protection of the funds in their futures trading accounts.”\nThe NFA’s BASIC Database\nWith the growing popularity of the Internet, the NFA set up online access to its historical registration and records of disciplinary action in 1998. This portal is now called the Background Affiliation Status Information Center or BASIC and provides an important resource for forex traders to look up information on individuals or corporations that they are considering doing business with.\nOne of the most useful features of the BASIC database is that it lists all members of the NFA doing business with the public. This can offer just about anyone with an interest in the reputation of forex brokers or anyone doing business with the public in the futures industry the valuable opportunity to research the NFA registrant online.\nFurthermore, the database contains all registered members of the CFTC, as well as the NFA membership list, and it also includes any regulatory and non regulatory action that may have been taken against any of the members of either organization by the agencies or by any U.S. based futures exchange.\n- Olympic Legend Usain Bolt Lost $12m in Savings Scam\n- Phoney Pastors Caught Running $28m Church-Based Ponzi Scheme\n- Withdrawals at Binance Raise New Concerns\n- “New Technology” Scam Rakes in $575m\n- Charges Against SBF Shed Light On Approach Of A Potential Crypto Clampdown\n- Crypto Scam Of The Year Makes HSBC’s List Of The 12 Scams of Christmas\nOlympic Legend Usain Bolt Lost $12m in Savings Scam\nPhoney Pastors Caught Running $28m Church-Based Ponzi Scheme\nSafest Forex Brokers 2023\n|Broker||Info||Best In||Customer Satisfaction Score|\n|#1||Your capital is at risk Founded: 2006||Globally regulated broker||\nBEST CUSTOMER SUPPORT Visit broker\n|#2||Your capital is at risk Founded: 2014||Global Forex Broker||\nBEST SPREADS Visit broker\n|#3||73 % of retail CFD accounts lose money Founded: 2014||Global Forex & CFD Broker||\nBest Trading Conditions Visit broker\n|#4||67% of CFD traders lose Founded: 2007||Global CFD & FX Broker||\nALL-INCLUSIVE TRADING PLATFORM Visit broker\n|#5||79 % of retail investor accounts lose money when trading CFDs with this provider. You should consider whether you can afford to take the high risk of losing your money. Founded: 2008||Global CFD Provider||\nBest Trading App Visit broker\nStay up to date with the latest Forex scam alerts\nSign up to receive our up-to-date broker reviews, new fraud warnings and special offers direct to your inbox']"	['<urn:uuid:ada9f0ca-d88b-4b4c-a257-fe86206692f7>', '<urn:uuid:5d06728e-7d0e-444e-85e2-4f08adf532ff>']	open-ended	with-premise	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-12T11:40:26.317848	35	98	1802
65	teaching resources astronomy education classroom compare contrast methods	In astronomy education, teachers use multiple resources and methods. The science curriculum emphasizes hands-on learning through equipment like magnifying glasses and skeleton models, while also covering topics such as constellations, planets, and space science. This is complemented by modern teaching materials that combine amazing facts with exciting graphic design, including up-to-date NASA and ESA images. Teachers use comparison and contrast matrices to help students understand relationships between planets, integrate different types of text and photographs, and employ vocabulary cards for key space terms. Activities include writing space captain's logs, recording moon phases, and studying night sky constellations.	"['||Science Curriculum Goals\n- Make decisions that reflect their Christian identity when faced with scientific moral issues.\n- Use the scientific method in problem solving.\n- Communicate theories, research, and conclusions in science by using writing, speaking, listening and technical skills.\n- Display respect in their interactions with the environments of which they are members.\n- Exhibit organizational skills, intellectual curiosity and growth, and application of what has been learned in science both to future schoolwork and to lifelong learning.\n- Use higher order thinking skills including comprehension, application, analysis, evaluation, and synthesis in learning concepts in life science, earth science, and physical science.\n- Use research, inquiry, and technology skills to find and evaluate needed scientific information from a variety of sources.\n- Exhibit a respect for diversity in nature and awareness of global issues.\n- Make good decisions regarding nutrition, personal health behavior, and disease prevention.\n- Use creativity in scientific problem solving.\n- Compare and contrast applications of science in technology.\n- Compare and contrast in earth science.\n- Show appreciation for environmental science.\n- Distinguish between healthy and risky behaviors.\n- Show appreciation for good nutrition.\n- Recognize animals.\n- Identify plants.\n- Recognize human beings.\n- Demonstrate use of science equipment.\n- Research and participate in space science.\n- Participate in the stewardship of God’s creation.\n- X-rays, video clips in technology class\n- Calendar, weather chart\n- Importance of recycling\n- Hand washing, proper hygiene, dental health\n- Food groups, eating habits\n- Animal babies, mothers and their habitat\n- Planting seeds, recording growth\n- Skeletons, x-rays\n- Magnifying glass, skeleton model, magnets\n- Constellation models, planets\n- Ways to reduce, reuse, and recycle\n- Research the five senses.\n- Distinguish between living and nonliving things.\n- Identify and describe the life cycle plants and animals.\n- Define animal habitats.\n- Identify and define the four seasons.\n- Research changes in weather.\n- Describe ways to care for our earth.\n- Smell, hearing, taste, touch, sight\n- Basic needs of all living things: air, water, food\n- Life cycles of apple trees, pumpkins, leaves, butterflies, frogs\n- Woodland forest, rainforest, arctic, desert, ocean\n- Spring, summer, fall, winter\n- Types of weather; wind, temperature, and precipitation\n- Caring for animals, plants and people\n- Identify the parts of plants and their functions.\n- Research the ways people use plants.\n- Compare and contrast animal groups.\n- Identify a food chain.\n- Compare and contrast animal habitats.\n- Define pollution and its effect on habitats.\n- Identify evaporation, condensation and the water cycle.\n- Compare and contrast extreme types of weather.\n- Define fossils and how paleontologists work with fossils.\n- Research what happens to animals who cannot meet their needs.\n- Identify how sun gives heat and light to earth and causes night and day.\n- Research earth’s orbit and seasonal changes.\n- Describe what causes moonlight and moon phases.\n- Define stars and discuss constellations.\n- Identify the planets of the solar system.\n- Parts of a plant (leaves, stem, roots, flower, fruit) and functions ways people use plants\n- Traits of animal groups (mammals, birds, fish, insects, amphibians, reptiles)\n- Food chain (ocean habitat)\n- Animal habitats ( woodland forest, rainforest, desert, artic, ocean )\n- Pollution ( garbage, oil, air pollutants )\n- Evaporation, condensation, water cycle\n- Extreme types of weather ( hurricane, tornado, drought, flood )\n- Fossils, paleontologist\n- Animals not meeting needs ( extinction)\n- Sun / earth ( heat, light, day, night )\n- Earth’s orbit and seasonal changes\n- Moonlight as reflective light, moon phases\n- Stars, constellations\n- Planets of the solar system ( Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto )\n- Identify the process of photosynthesis.\n- Identify inherited and learned traits of animals.\n- Classify animals according to characteristics.\n- Define an ecosystem and its parts.\n- Explore adaptations in ecosystems.\n- Explain how plants and animals interact with the environment and each other.\n- Define and explain an energy pyramid.\n- Identify kinds of resources.\n- Compare and contrast the sun, the planets and their orbits.\n- Describe the properties of matter.\n- Compare and contrast the properties of matter.\n- Describe how matter is combined.\n- Describe how matter changes.\n- Research the elements.\n- Photosynthesis, chlorophyll\n- Physical and learned traits of animals\n- Characteristics of mammals, birds, amphibians, fish, reptiles, insects\n- Ecosystems, population, community, habitat\n- Survival characteristics of animals\n- Producer, consumer, decomposers\n- Food web, energy pyramid\n- Renewable resources, nonrenewable resources\n- Characteristics of sun, planets and their orbits\n- Solid, liquid, gas\n- Volume, mass\n- Mixture, solution\n- Evaporation, condensation, physical change\n- Compounds, chemical change\n- Identify and compare / contrast the physical properties of rocks.\n- Describe and explain changes of rocks over time.\n- Explain how scientists compare and classify organisms of today and the past; describe reasons organisms become extinct.\n- Distinguish organisms and their habitats; where they live within an ecosystem.\n- Demonstrate knowledge of producers, consumers, and decomposers.\n- Interpret the path of energy in food chains and food webs.\n- Translate the changes in ecosystems – identify problems / solutions.\n- Analyze what makes up living things.\n- Distinguish plant / animal cells.\n- Identify organ systems in animals and describe functions.\n- Explain that animals must reproduce for their species to survive.\n- Compare / contrast different ways animals reproduce.\n- Rocks ( minerals, igneous, sedimentary, metamorphic, fossils )\n- Changes of rocks over time\n- Classifying organisms ( kingdom, trait, genus, species ) / past ( fossils, extinction )\n- Organisms and where they live: ecosystem, community, habitat\n- Producers, consumers, decomposers\n- Path of energy in food chains and webs\n- Changes in ecosystems (drought, overpopulation, pollution, fire, disease)\n- Cells in living things\n- Plant / animal cells\n- Cell, tissue, organ, organ systems\n- Reproduction for survival\n- Development and reproduction\n- Identify a number of structures found in plants and animals.\n- Discuss and research the great diversity of plants on earth.\n- Discuss and research the great diversity of animals on earth.\n- Define and discuss various ecosystems on the earth.\n- Be able to explain how a population and its ecosystem interact.\n- Discuss the importance of some of the earth’s rocks and minerals.\n- Discuss how atmospheric conditions affect weather and climate.\n- Discuss the properties of matter and energy.\n- Compare and contrast Newton’s Laws of Energy.\n- Characteristics of living things, plant parts: roots, stems, leaves, why are plants important to us\n- Plants with seeds, plants without seeds, plant responses and adaptations, flowers and seeds\n- Animal structures and functions, animal adaptations\n- Food chains and food webs, cycles of life\n- How populations survive, biomes, how ecosystems change\n- The earth’s crust, the minerals in the earth’s crust, the earth’s rocks and soil\n- The sun’s effect on the earth, water vapor and humidity, clouds and precipitation, air pressure and wind, air masses and fronts, severe storms\n- Physical properties of matter, physical properties of mixtures and solutions\n- Newton’s three laws of energy\n- Classify living things based on similarities and differences.\n- Identify the organization of living things from the most complex, the ecosystem, to the most basic, the cell.\n- Discuss the sun, moon and earth relationship in the solar system.\n- Research how the earth’s crust is constantly moving and changing.\n- Recognize the interaction between matter and energy on earth.\n- Apply the concepts of simple machines.\n- Participate in outdoor education.\n- Six kingdoms to species, invertebrates, reproduction and growth\n- Cells organized into tissues, organs, organ systems: traits and how they are passed from parent to offspring, Gregor Mendel\n- Telescopes and satellites, interaction between earth, moon and sun\n- Earth’s plates, earthquakes, volcanic eruptions, forces of weather, erosion and deposition, the rock cycle, geologic time\n- Properties and changes of matter, elements and atoms, chemical changes, submarine project\n- Speed and distance, forces and motion, acceleration, momentum, energy and work\n- Camp Ondessonk – outdoor education for one week, group building skills, nature lessons, Native American history, outdoor skills, environmental knowledge\n- Apply the metric system.\n- Translate the work of prior scientists into daily applications.\n- Describe the relationships between work and machines.\n- Explain the movement of energy through the environment.\n- Show an appreciation of H2O as a resource.\n- Analyze populations.\n- Relate the needs of an ecosystem with the need to protect and conserve.\n- Participate in scientific processes.\n- Calculate speed, force, acceleration, pressure, work\n- Newton, Pascal, Bernoulli, Archimedes\n- Efficiency, input/output forces\n- Energy and power – thermal energy and heat\n- a) H2O properties and cycle/availability vs. demand\n- b) ocean movement and zones\n- Population studies – classroom and field\n- Biogeography, biodiversity, conservation\n- Labs designed to explore content through discovery/inquiry\n- Distinguish between different types of matter.\n- Explain the structure of the periodic table.\n- Apply facts of carbon chemistry.\n- Discuss the impact of activity that occurs beneath the earth’s surface.\n- Compare/contrast components of the earth.\n- Classify life forms.\n- Identify increasing complexity among life forms.\n- Molecular structure, changes of state, products of reaction\n- Mendeleev, atomic number, chemical symbol, families, periods\n- Organic compounds, nucleic acids, equations for photosynthesis/respiration\n- Plate tectonics, earthquakes, volcanoes\n- Properties of rocks and minerals\n- Linnaeus’ system of Binomial Nomenclature\n- Survey of phyla', ""Discover More: Planets Storia Teaching Guide\n- Grades: 1–2, 3–5\nDiscover More: Planets is a new-generation reference book that combines amazing facts with an exciting graphic design to present the vastness of space in an approachable way for young readers. Students who have wondered about their place in the universe while looking up into the night sky will have their questions answered and their curiosity piqued. The book takes readers on a tour of the major planets, moons, asteroids, comets, galaxies, and beyond.\nThe latest NASA and European Space Station images, many unique to this book, combine with up-to-the-minute information from space experts across the globe. A timeline of space exploration, diagrams of a rocket and an astronaut suit, and an interview with an ESA astronaut add human interest to the science of planets and stars. Plus, readers put their new knowledge to work by finding constellations in the night sky and imagining the possibility of alien life on other planets.\nTeaching the Book\nStart the countdown! Discover More: Planets will blast students into outer space to explore planets, moons, asteroids, comets, and far-away galaxies. Use this informational book to introduce the solar system and space exploration, integrate knowledge with a compare and contrast matrix, and reinforce key space vocabulary. Activities engage students in writing a space captain’s log, recording the phases of the moon, and imagining an encounter with an extraterrestrial.\nTheme Focus: Information Non-fiction\nComprehension Focus: Comparison and Contrast Matrix\nLanguage Focus: Space Vocabulary\nGet Ready to Read\nAsk students to think about where they are in the universe. What would be your school’s complete universe address? Write the following parts of an address on the whiteboard or chart paper. Then ask students to fill in place names for the school’s address.\nCity, State, and Zip Code:\nTell students that they will find out exactly where in the universe they live as they read Discover More: Planets.\nPreview and Predict\nSpend time with students on pages 2 and 3, going over the explanation of how the book works. Point out the different types of text and photographs in the book and the kinds of information they provide.\nThe book includes a glossary on pages 76 and 77 which contains key vocabulary related to space and space exploration. The words below are especially important for students to understand as they read. Encourage them to look for clues in the text and illustrations to figure out the meaning of the words and have them check the glossary definition.\nUse Resource #1: Vocabulary Cards and distribute copies to students.\n- planet (p. 6)\n- universe (p. 8)\n- solar system (p. 10)\n- sun (p. 16)\n- gravity (p. 22)\n- asteroid (p. 34)\n- galaxy (p. 48)\n- astronaut (p. 68)\nWords to Know\nGive students the following definitions and ask them to hold up the vocabulary card that each defines. Then have students turn to the referenced pages to give one or two more facts about each space term.\n- a group of stars (galaxy, p. 48)\n- the planets, moons, asteroids, comets, and other objects orbiting the Sun (solar system, p. 10)\n- the entirety of space, including all the planets and stars (universe, p. 8)\n- a rock that floats in space, orbiting the Sun (asteroid, p. 34)\n- a huge ball of hot fiery gas (sun, p. 16)\n- a person who has been trained to travel and work in a spacecraft (astronaut, p. 68)\n- the force that attracts objects to one another; also the force that attracts objects to the Earth (gravity, p. 22)\n- a round object, either rocky or made up of gases, that orbits a star (planet, p. 6)\nAs You Read\nReading the Book\nRead aloud pages 6 and 7, modeling for students how to approach the different chunks of text on the page. Draw their attention to the callout lines that show what part of a photo the text is referencing. Also help students figure out the order in which to read the different chunks of text on a page and how to tell which text connects to which photo.\nEncourage students to read the book independently but to share questions and reactions with a partner. Suggest that partners conference with each other after every four to six pages.\nBig Question: Critical Thinking\nAsk students to think about this question as they read and be ready to answer it when they have finished the book. Write the question on chart paper or have students write it in their reading journals. What would be the most exciting discovery we could make about space?\nCompare and Contrast\nShow students how to create a comparison and contrast matrix to help students integrate the information they are learning about the planets. By arranging information in this way, students learn how the planets relate to each other in size, distance from the sun, and so on. The matrix provides a means for them to draw relationships from the separate chunks of text in the book.\nDisplay on a whiteboard or screen the matrix on Resource #2: Comparison and Contrast Matrix.\nThen model for students how to fill out the information for the planet Mercury.\nI’m going to use this matrix to help me remember the information in the book and draw relationships between the planets. First, I’ll see how information should be put into the matrix. The names of the planets go down the left side of the matrix. Across the top, there are important pieces of information, like distance from the sun. I’ll begin with Mercury and look on pages 18 and 19 where I’ll find the information about this planet. It says that Mercury is the first planet from the sun. I’ll write, “Mercury” into the matrix first.\nPass out copies of Resource #2 and guide students to reread the text to fill out the rest of the matrix for Mercury and the other planets. Then ask students compare and contrast questions about the planets such as: Which planet is the biggest in size? Which planet has the most moons? How does Jupiter compare with Mars in size and length of its year?\nAfter You Read\nQuestions to Discuss\nLead students in a discussion of these focus story elements.\n1. Information Non-fiction\nHow does the book provide information about the phases of the moon on pages 24 and 25? (It shows eight pictures of the phases of the moon in the order they happen.) Why is this better than explaining the moon’s phases in words? (The pictures show the changes more clearly than words could.)\n2. Compare and Contrast\nWhat do Mercury, Venus, Earth, and Mars all have in common? (They are made of rock.) Which planets have fewer moons than Earth? (Mercury and Venus) Which planet is the smallest in the solar system? (Mercury)\n3. Space Vocabulary\nCompare a galaxy and the solar system. What is the difference between these two space bodies? (A galaxy is a group of stars. The solar system is the stars, planets, moons, and other bodies orbiting the Sun. Our solar system is part of a galaxy called the Milky Way.)\nQuestions to Share\nEncourage students to share their responses with a partner or small group.\nWhat fact about the planets surprised you most? Would you ever like to be an astronaut?\nHave you ever seen the Big Dipper in the sky? How would you go about identifying it?\nWhat do you find most challenging about reading this book? Do you like reading it more or less than books that have the text all together in\nContent Area Connections\nMath: Moon Olympics\nWhat would it be like to play sports on the moon? Students find out in this interactive science exploration activity. They guess how the gravity of the moon affects activities like weightlifting, diving, and skateboarding.\nScience: Star Gazing\nEncourage students to ask their families to study the night sky with them for familiar constellations like the Big Dipper and for the planet Venus, the brightest natural object in the night sky after our Moon. If they are interested, guide students to reference books about constellations or direct them to a website such as kidsastronomy.com.\nArts: Moon Watch Flip Book\nThe American Museum of Natural History has an extensive astronomy section on its kids’ website, OLogy. One activity asks students to record the changing appearance of the moon for a full cycle. They then cut apart their drawings and create a flip book that shows the moon moving through its phases.\nTechnology: Rocket Scientist\nStudents learn how real rockets work in Discover More: Planets. The future rocket scientists in your classroom will enjoy creating their own rockets using simple household supplies. Guide students to the NASA Space Place website for kids for an excellent rocket experiment that must be done with the assistance of an adult.\nReading/Writing Connection: Captain’s Log\nAssign students to write a log, or informational journal, imagining that they are the captain of a space mission in the future. First, tell students to choose a name for their spaceship and its mission. Then have them choose one of the planets in our solar system as the destination of their mission. Instruct students to make three log entries about what they observe. They can use their imaginations, but should also include information from the book about their planet.\nDon't Forget the Big Question: Critical Thinking\nGive each student an opportunity to answer the big question. Encourage students to support their answers with details and evidence from the text. Tell them there is no one right answer. What would be the most exciting discovery we could make about space?\nBig Activity: Take Me to Your Leader\nReread with students the text about the possibility of alien life on pages 54 and 55. Ask students to study the pictures of alien life on the pages and then challenge them to draw their own E.T., or Extra Terrestrial. Make copies of the printable, Big Activity: Take Me to Your Leader and distribute to students. Explain how to fill in the information about their alien and clarify any questions students have.\nAbout the Author\nPenny Arlon is an author who writes children’s nonfiction, taking inspiration from her own children. Her books range from pre-school to family reference, and include the Art Attack books, based on the awardwinning CITV children’s television program. She has also written 14 titles in Dorling Kindersley’s Eye Know series.\n© 2012 SI ALL RIGHTS RESERVED""]"	['<urn:uuid:efe44331-7c56-4a42-8ada-2a58c8c2faf4>', '<urn:uuid:d02a7ce1-ebf2-49ff-ae19-18c318557887>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T11:40:26.317848	8	97	3339
66	I love war movies - what's the plot of Sisu?	Sisu is set during the last desperate days of World War II and follows a solitary prospector who encounters Nazis during their scorched-earth retreat in northern Finland. The film has earned over $5.5 million and is currently number 10 at the box office.	['Indian films are a box office mainstay and one, Jodi, from Rhythm Boyz Entertainment, hit big this weekend Stateside, grossing $734,000 on just 125 screens. In April, the film set a record as the most viewed Punjabi trailer in 24 hours (over 12 million views on YouTube).\n“This has been happening more and more — Indian films popping into the top 5, or 10, or 15th” place in North America, said Paul Degarabedian, senior media analyst at Comscore, which compiled the numbers on Jodi. “It shows the strength of Indian cinema. That’s a really impressive number, almost three quarters of a million dollars at 125 theaters.” The ones that pop do really well on a per-theater-average, even if they make $1-$2 million or aren’t in the top ten, he said. Jodi’s PTA is $5.75k.\nSome other breakouts this year include Waltair Veerayya, Tu Jhoothi Main Makkaar and Dasara. Indian films may wind up being even more important to exhibitors down the line given the potential for prolonged Hollywood labor action.\nJodi is a musical love story written and directed by Amberdeep Singh, starring Dosanjh and Nimrat Khaira as two talented and passionate artists who set out on a journey to redefine the Punjabi folk music scene in the 1980s. It’s said to be based on the life of slain Punjabi singer Amar Singh Chamkila and his second wife Bibi Amarjot Kaur. The film was scheduled for release twice during Covid and almost had its opening stayed on Friday in a legal dispute over rights that was resolved at the last minute, according to news reports.\nElsewhere in specialty: Multicultural romcom What’s Love Got To Do With It? from Shout! Studios grossed an estimated $78k of 500+ screens, according to sources.\nLimited: Kino Lorber opened Maxwell Wolkin’s political thriller Chile ‘76 in two theaters to a debut of $16k for a nice PTA of $8k — making it the top-grossing subtitled film in New York City this weekend, the distributor said. Expanding to LA and other markets on May 19.\nGreenwhich Entertainment doc The Melt Goes On Forever: The Art & Times of David Hammons grossed an estimated $6.25k at one location, NYC’s Film Forum. LA run starts June 2 at the Laemmle Monica.\nKimStim opened Unrest in NYC at Film at Lincoln Center for at $4.2k. Expands to LA May 19 at Laemmle Monica.\nHoldovers: Sisu from Lionsgate sees an an estimated $1.08 million (for a $5.5+ million cume) on 1,000 screens in week two, where it sits at no. 10 at the box office. During the last desperate days of WWII, a solitary prospector (Jorma Tommila) crosses paths with Nazis on a scorched-earth retreat in northern Finland.\nNida Manzoor’s Polite Society from Focus Features is reporting an estimated $215k weekend and a cume to date of $1.3 million.\nSideshow/Janus Films release of The Eight Mountains grossed about $16k on 4 screens, for a per-screen average of $4k and a new cume of $57.3k.\nR.M.N from IFC Films grossed an estimated $11k in week two at 41 locations for a cume of $22.1k.\nAbramorama documentary 32 Sounds grossed $4k in one run for a cume of $27.9k.']	['<urn:uuid:be9977cb-2052-4a1e-8e5b-ed82775647c6>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T11:40:26.317848	10	43	527
67	oldest rocks yellowstone national park location and age details	The oldest rocks in Yellowstone are found in the Lamar River Canyon, consisting of exposed outcrops of gneiss and schist that are perhaps more than two billion years old. Only the Gallatin Range within the park contains older outcrops.	"['Observe Tower Fall\nThe 132-foot (40-m) drop of Tower Creek, framed by eroded volcanic pinnacles has been documented by park visitors from the earliest trips of Europeans into the Yellowstone region. Its idyllic setting has inspired numerous artists, including Thomas Moran. His painting of Tower Fall played a role in the establishment of Yellowstone National Park in 1872.\nThe nearby Bannock Ford on the Yellowstone River was an important travel route for early Native Americans, as well as for early European visitors and miners up to the late 19th century.\nTake in the Yellowstone River\nCalcite Springs marks the downstream end of the Grand Canyon of the Yellowstone River. The geothermally altered rhyolite inspired the artist Thomas Moran. His paintings of this scene were among those presented to Congress in 1872, leading to the establishment of the park. The steep, columnar basalt cliffs on the opposite side of the river from the overlook are remnants of an ancient lava flow, providing a window into the past volcanic forces that shaped much of the Yellowstone landscape. The gorge and cliffs provide habitat for numerous wildlife species including bighorn sheep, red-tailed hawks, and osprey.\nFor those looking to stretch the legs, there is the Yellowstone River Picnic Area Trail that follows the canyon edge on the other side of the river. This trail offers an alternative view of the terminus of the Grand Canyon of the Yellowstone River.\nDelve into the Past\nPetrified Tree, located near the Lost Lake trailhead, is an excellent example of an ancient redwood, similar to many found on Specimen Ridge, that is easily accessible to park visitors. Specimen Ridge, located along the Northeast Entrance Road east of Tower Junction, contains the largest concentration of petrified trees in the world. There are also excellent samples of petrified leaf impressions, conifer needles, and microscopic pollen from numerous species no longer growing in the park. Specimen Ridge provides a superb window into the distant past when plant communities and climatic conditions were much different than today.\nWatch Wildlife in Lamar Valley\nLamar Valley is an excellent place to view wildlife, with it being one of the major summer grounds for bison and elk, which attracts predators like wolves and grizzly bears.\nRemember: Do not approach or encircle bears or wolves on foot within 100 yards (91 m) or other wildlife within 25 yards (23 m). Always maintain a safe distance from all wildlife. Each year, park visitors are injured by wildlife when approaching too closely.\nLearn at the Lamar Buffalo Ranch\nThe Lamar Buffalo Ranch was built in the early part of the century in an effort to increase the herd size of the few remaining bison in Yellowstone, preventing the feared extinction of the species. Buffalo ranching operations continued at Lamar until the 1950s. The valley was irrigated for hay pastures, and corrals and fencing were scattered throughout the area. Remnants of irrigation ditches, fencing, and water troughs can still be found. Four remaining buildings from the original ranch compound are contained within the Lamar Buffalo Ranch Historic District (two residences, the bunkhouse, and the barn) and are on the National Register of Historic Places.\nThe facility is used in the spring and fall for the National Park Service\'s residential environmental education program, Expedition Yellowstone. You are welcome to drive by to view the historic buffalo ranch, however, there are no facilities open to the general public at this location.\nGlimpse the Bannock Trail\nThe Bannock Trail, once used by Native Americans to access the buffalo plains east of the park from the Snake River plains in Idaho, was extensively used from approximately 1840 to 1876. A lengthy portion of the trail extends through from the Blacktail Plateau (closely paralleling or actually covered by the existing road) to where it crosses the Yellowstone River at the Bannock Ford upstream from Tower Creek. From the river, the trail\'s main fork ascends the Lamar River splitting at Soda Butte Creek. From there, one fork ascends the creek before leaving the park. Traces of the trail can still be plainly seen in various locations, particularly on the Blacktail Plateau and at the Lamar-Soda Butte confluence.\nHave Dinner at an Old West Cookout\nPleasant Valley was the sight of ""Uncle John"" Yancey\'s Pleasant Valley Hotel, one of the earliest lodging facilities in Yellowstone. The hotel and outbuildings were built between 1884 and 1893 and served early park visitors as well as miners passing through en route to the mining district near Cooke City. Currently, the site is used for ""Old West"" cookouts. None of the original buildings remain.\nVisit the Northeast Entrance Ranger Station\nThe Northeast Entrance Ranger Station was constructed in 1934–35 and is a National Historic Landmark. Its rustic log construction is characteristic of ""parkitecture"" common in the national parks of the west during that period.\nExplore the Geologic History\nAbsaroka volcanics, glaciation, and erosion have left features as varied as Specimen Ridge\'s petrified trees to the gorges along the Yellowstone River\'s Black Canyon and the Grand Canyon of the Yellowstone River.\nMt. Washburn and the Absaroka Range are both remnants of ancient volcanic events that formed the highest peaks in the area. Ancient eruptions, perhaps 45–50 million years ago, buried the forests of Specimen Ridge in ash and debris flows. The columnar basalt formations near Tower Fall, the volcanic breccias of the towers themselves, and numerous igneous outcrops all reflect the volcanic history.\nLater, glacial events scoured the landscape, exposing the stone forests and leaving evidence of their passage across the area. The glacial ponds and huge boulders (erratics) between the Lamar and Yellowstone rivers are remnants left by the retreating glaciers. Lateral and terminal moraines are common in these areas. Such evidence can also be found in the Hellroaring and Slough creek drainages, on Blacktail Plateau, and in the Lamar Valley.\nThe eroding power of running water has been at work here for many millions of years. The pinnacles of Tower Fall, the exposed rainbow colors of the Grand Canyon of the Yellowstone River at Calcite Springs, and the gorge of the Black Canyon all are due, at least in part, to the forces of running water and gravity.\nIn the Lamar River Canyon lie exposed outcrops of gneiss and schist which are among the oldest rocks known in Yellowstone, perhaps more than two billion years old. Little is known about their origin due to their extreme age. Through time, heat and pressure have altered these rocks from their original state, further obscuring their early history. Only in the Gallatin Range are older outcrops found within the boundaries of the park.\nThis region in the park has several options for staying overnight. Roosevelt Lodge right at Tower–Roosevelt provides cabin accommodations.\nJust south of there, at Tower Fall, is the Tower Fall campground. Heading out to the Northeast Entrance are the Slough Creek and Pebble Creek campgrounds. All three of these campgrounds are first-come, first-served.\nFor those wanting to stay in the backcountry, check out the backcountry sites that are available in the region.\nTower–Roosevelt and the Northeast Frequently Asked Questions\nThe area is named for its major waterfall, Tower Fall, which is named for the tower-like rock formations at its brink.\n132 feet (40 m). Old pictures show a big boulder at the brink of Tower Fall.\nW.H. Jackson\'s photograph in 1871 clearly shows the boulder. For more than a century, visitors wondered when it would fall. It finally did in June 1986.\nNo, the lower part of the trail is closed because of severe erosion. You can walk past the Tower Fall Overlook for 0.75-mile (1.2-km), ending with a view of Tower Creek flowing into the Yellowstone River. If you have heart, lung, or knee problems, you may want to enjoy the view from the overlook.\nThe rock columns were formed by a basaltic lava flow that cracked into hexagonal columns as it slowly cooled. You can see other basalt columns at Sheepeater Cliff along the Gardner River between Mammoth and Norris.\nPetrified Tree, west of Tower Junction, is an excellent example of an ancient redwood. Petrification of this and other trees occurred for two main reasons. They were buried rapidly by volcanic deposits and mudflows 45–50 million years ago, which minimized decay. The volcanic deposits also contributed high amounts of silica to the groundwater. Over time, the silica precipitated from ground water, filled the spaces within the trees\' cells, and petrified the trees.\nIn Yellowstone, glacial ice, running water, and wind have uncovered vast areas of petrified trees. With proper optical aids, you can see some of these areas from the road that follows the base of Specimen Ridge, east of Tower Junction.\nNo, but President Theodore Roosevelt camped nearby during his visit to Yellowstone in 1903. The lodge opened in 1920. The area is registered as the Roosevelt Lodge Historic District. Contrary to popular belief, it was not President Roosevelt but President Ulysses S. Grant who signed the order that created Yellowstone National Park.\nMany park researchers have used the fenced areas on the northern range to study the long-term effects of grazing by fencing out large herbivores. Researchers can use existing data from the permanent plots or collect new data.\nElk, bison, deer, and pronghorn thrive in the grasslands of this area, known as the northern range. In fact, some of the largest wild herds of bison and elk in North America are found here. The northern range is critical winter habitat for these large animals, which in turn provide food for several packs of wolves. Coyotes are also common, and occasional bobcat, cougar, or red fox are reported.\nThe gorge and cliffs between the junction and Tower Fall provide habitat for bighorn sheep, osprey, peregrine falcons, and red-tailed hawks. Both grizzly and black bears are sighted throughout the area, particularly in the spring. Black bears are more commonly seen around Tower Fall and Tower Junction. Grizzlies are sometimes seen in the Lamar Valley and on the north slopes of Mount Washburn, particularly in the spring when elk are calving. Road pullouts provide excellent places from which to watch wildlife.\nAccessibility in Tower–Roosevelt Area\nLearn about accessibility options in the Tower–Roosevelt area.\nTower and Northeast Hikes\nExplore valleys, mountains, and petrified forests in the northeast part of the park.\nNortheast Skiing & Snowshoeing\nDiscover skiing and snowshoeing opportunities in the northeast part of Yellowstone National Park.\nTower Skiing & Snowshoeing\nDiscover skiing and snowshoeing opportunities in the Tower area.\nLast updated: July 16, 2019']"	['<urn:uuid:bdc87b76-e313-452a-bbde-5d1e6896fa65>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T11:40:26.317848	9	39	1749
68	symptoms night sweats lyme disease vs tuberculosis differences	Night sweats appear differently in tuberculosis (TB) and Lyme disease. In TB, the sweats can be extremely severe, described as 'waterfalls that plastered hair to head' as seen in Susan Warm's case. In Lyme disease, while night sweats can occur, they are typically mentioned as a general symptom alongside other manifestations like fever, joint swelling, and lethargy. TB sweats were distinctive enough for an ER doctor to recognize them specifically as TB-related, while in Lyme disease they appear as part of a broader collection of flu-like symptoms.	['Her symptoms came on slowly but kept getting worse. Within a few months, Susan Warm’s fatigue had turned into debilitating exhaustion, her aches had graduated to serious pain, and her periodic sweats had developed into waterfalls that plastered her hair to her head. Warm, then 45, consulted several doctors, each of whom ran a variety of tests and declared she was suffering from nothing worse than the symptoms of menopause. “I thought I was going crazy, because I felt so bad for no explainable reason,” recalls Warm, a customer-marketing specialist in Pasadena, California.\nEight months into her ordeal, Warm persuaded an ER doctor to admit her to the hospital. There, one of the attending physicians touched her drenched arm and said, “I haven’t felt sweats like this since I last treated tuberculosis.” Sure enough, even though Warm didn’t have the classic TB cough, she tested positive for the disease. (The infection was in her bloodstream, not her lungs.) Once Warm took the appropriate antibiotics, she was cured—at last.\nAmericans believe that diagnosis is the cornerstone of medical care: You go to a doctor, who labels your problem and prescribes a treatment that makes you well. But what if the process gets stuck at the labeling stage? “We have a fantasy that as soon as we describe our symptoms, the doctor will know what is wrong with us. But the reality can be much more complicated,” says Evan Falchuk, president of Best Doctors (bestdoctors.com), a Boston-based company that helps corporate employees get second opinions from top physicians. You don’t expect the doctor to be stymied, particularly after extensive testing. But failures to find a diagnosis do occur—and while no one knows exactly how often, 10,000 patients contact Falchuk’s firm for help every year.\nTV shows like House contribute to our misconceptions. Although Dr. Gregory House is portrayed as an expert in diagnostic medicine, such a medical specialty exists only in scriptwriters’ imaginations. In real life, analyzing what’s wrong with us falls to our primary care physician or to the specialists we consult. In most cases, by relying on our symptoms, medical history and test results, doctors are able to figure out our problems. But MDs typically look for common scenarios; people with rare diseases or unusual presentations of prevalent conditions (like Warm’s TB) can fall through the cracks, notes Marianne Genetti, executive director of In Need of Diagnosis (inod.org), a nonprofit in Orlando, Florida, that assists patients who have not been able to put a name to their complaints.\nIf that’s your situation, don’t give up: It’s essential that you persistently advocate for yourself. One way to do that is to actively help your doctors find the correct diagnosis.\nHere, five steps you can take.\nStep 1 Do Your Homework\nThe more information your doctor has, the better he’ll be able to treat you. In the days leading up to an important doctor visit, keep a pad of paper handy and jot down your symptoms as they occur so you can read the list to your practitioner. In addition, do a little digging on the Web, spending time on reputable medical sites such as MayoClinic.com and Health.gov to gain insight into how your symptoms might fit together. For instance, you may have gotten so accustomed to feeling thirsty all the time that you didn’t even think to bring it up during your last visit. But if an online check reveals that the symptoms you did plan to mention—fatigue and headaches—often go along with increased thirst in diabetics, then you’ll realize you should be telling the doctor how frequently you need a drink of water.\nOr maybe you have a funny bump on your hand. By going online, you discover that one possible cause is a marine parasite. Now you remember that you swam in the ocean during your last Caribbean vacation, so you’ll be sure to tell the doctor.\nJust don’t try to generate your own diagnoses, cautions Marie Savard, MD, an ABC News medical contributor and author of How to Save Your Own Life: The Eight Steps Only You Can Take to Manage and Control Your Health Care. Online information helps you pose smarter questions to your doctor, but he’s much better qualified to piece together the information.\nAt your doctor’s appointment, you’ll also need to list all the medications (and supplements) that you take or have recently taken. Unexplained symptoms frequently stem from a reaction to a drug, even one you’ve been on for a while, says Abbie Leibowitz, MD, chief medical officer of Health Advocate (healthadvocate.com), a firm in Plymouth Meeting, Pennsylvania, that helps patients navigate the health care waters.\nBut the best information you can give a physician is your personal and family medical histories. “Up to 80 percent of a doctor’s determination comes from your history, not from tests done the day you’re in the office,” Savard says. Be ready to describe all the major conditions you have ever had, even if they are long in the past or do not seem associated with your current symptoms. Bring to your appointment important test results from any related prior illnesses.\nAnd make sure to tell your physician about conditions your relatives have suffered from. Rare diseases often go undiagnosed, but if your doctor knows that one runs in your family, he will be much more likely to consider it a possibility.\nStep 2 Prod Your Doctor\nWhen symptoms don’t add up to a ready diagnosis, physicians sometimes suggest that an ailment is psychological or stress related. But don’t let your doctor dismiss your complaints so quickly, says Pamela Hops, MD, a family physician at New York City’s Continuum Center for Health and Healing. “Sometimes psychological issues do present themselves physically, but the doctor shouldn’t assume that’s the case, especially if you feel in your gut that something else is wrong,” she says. Some symptoms, says Hops, can indicate a serious condition and always call for additional testing: prolonged fever, unexplained weight loss and night sweats that aren’t associated with menopause. Night sweats, for example, can indicate an infection or a possible cancer or may be a side effect of certain diabetes drugs.\nIf your doctor is baffled, suggest he consider conditions that are known for being overlooked. (See 5 Diseases Doctors Often Miss) Take, for instance, Lyme disease, which sometimes surfaces in the form of vague and ordinary symptoms. Stephanie Smith, 36, an asset manager from Newport Beach, California, suffered for a decade from joint pain, fatigue, insomnia and heart palpitations before she was diagnosed with the disease, which is primarily a problem in the northeastern U.S., not in her home state. When one doctor suggested her troubles were due to stress, Smith (who asked that her real last name not be used) quit her high-pressurejob,but she continued to deteriorate. “I began to think my symptoms were normal age-related issues, and eventually I stopped trying to get a diagnosis,” Smith says. Then one day she described her condition to her aunt, who’d once had Lyme disease and recognized the symptoms. After a positive test, Smith started using the antibiotics and immune-boosting supplements that returned her to health.\nStep 3 Take a Break\nIf your doctor has run important tests and ruled out serious conditions but still hasn’t determined what’s wrong with you, it may be time to put your quest for a diagnosis on hold. “You don’t want to push your doctor into giving you drugs or other treatments that may not be appropriate, because that can lead to other complications,” Savard says. She suggests waiting for three months, or whatever time frame you and your physician agree on, to see if your body can heal itself.\nDuring this hiatus, you might take a more holistic approach to healing by adopting a healthier lifestyle: Get enough sleep, exercise regularly, reduce your stress levels and improve your diet—and see if these measures are enough to cure what ails you.\nChanging your diet can be tough, but in some cases the results are dramatic. For years, Kristen Johnson, 51, who lives in San Diego, suffered from migraines, digestive problems, depression, aches and pains. Then she decided to seek out an MD with a holistic bent (find doctors with this orientation through the Institute for Functional Medicine, functionalmedicine.org). The physician thought she might be severely sensitive to certain foods, especially wheat, corn, soybeans and sugar. Eager to try anything that could help, Johnson made over her pantry. “I swooped into my kitchen, tossed out all the processed foods and stocked up on vegetables, fish, poultry, fruits and yogurt,” Johnson recalls. After a couple of weeks, she found relief, and within a few months, she felt truly healthy. “I still get the occasional migraine, but it’s always after I accidentally eat something hidden in restaurant food,” explains Johnson.\nStep 4 Call in the Experts\nIf you’ve followed the advice above and haven’t made progress, it’s time to seek out a topflight physician in a specialty related to your concerns (for instance, a neurologist if you have unexplained tremors). “Always keep your regular doctor in the loop, because if you still don’t have answers after seeing the expert, you’ll want to return to your doctor to discuss other options,” says Leibowitz of Health Advocate.\nThe best specialists can be found at large hospitals or, better yet, at teaching hospitals affiliated with medical schools—even if these are a long drive away. It’s especially important to seek this top level of expertise once you’ve seen and stumped other doctors, since that means you’re more likely to have a rare condition, says William A. Gahl, MD, PhD, director of the NIH Undiagnosed Diseases Program.\n“Doctors at a university center usually have the most up-to-date information on rare diseases, and they have an array of specialists that can address them,” he says. Working through your physician, you can also get second opinions without an in-person examination from such top-rated institutions as the Cleveland Clinic (clevelandclinic.org; click on My Consult), Johns Hopkins Medicine (hopkinsmedicine.org/second_opinion) and Partners Online Specialty Consultations (econsults.partners.org), which connects to Harvard-affiliated doctors. The fees might be hefty but could be worth the expense.\nStep 5 Be Prepared for a Diagnosis to be Wrong\nSometimes the problem isn’t that you’re unable to get a diagnosis but that you’ve received an inaccurate one. Mistakes are not uncommon: Autopsies performed on patients in one university hospital found that up to 32 percent of them had been given serious misdiagnoses. “If your treatment isn’t making you feel better, don’t immediately look for other therapies. Your first move should be to confirm that your doctor got the diagnosis right in the first place,” says Falchuk of Best Doctors. That typically involves sending all your records to an expert or, in the case of cancer, asking that a different pathologist examine your tumor sample. One of Falchuk’s clients, who was being treated for cervical cancer, discovered in this way that she actually had colon cancer that had spread to her cervix. Because cancer treatment is specialized, she had been getting the wrong drug. “It’s important to keep asking questions to ensure your diagnosis is right,” Falchuk says. “The worst that can happen is that your doctor may feel annoyed.”\nAnd what’s the best that can happen? Your questioning may well save your life.\nWant the latest beauty, fashion and lifestyle tips? Click here to sign up for our fabulous weekly newsletter!', 'Lyme disease is the most common illness transmitted by ticks. It’s a bacterial infection, which makes its way into the bloodstream through bites. Both humans and dogs are vulnerable to Lyme disease, and the illness can trigger a range of unpleasant symptoms including fever and swollen joints. Lyme disease can usually be treated with a course of antibiotics.\nBoth dogs and their owners are particularly vulnerable to Lyme disease in the summer months, when young ticks are most prevalent. This means that now is the perfect time to make yourself aware of Lyme disease symptoms to watch out for, and find out how you can help to prevent it.\nLyme disease is caused by the bacterium Borrelia burgdorferi, and is usually carried by the eastern black-legged tick (also known as the “deer tick” or “bear tick”). The bacterium is a spiral shaped microorganism, which is harboured in the tick’s digestive system, and can enter the bloodstream of its host animal through a bite. Ticks themselves are thought to pick up the disease through feeding on mice and other rodents during the larval stage of their lifecycle.\nTicks pose a risk to humans and dogs throughout the US and much of the world, but the following US areas have particularly high rates of Lyme disease:\n- The Northeast\n- The Upper Midwest\n- The Pacific coast\nGenerally, ticks must be attached to their host for 36 to 48 hours before Lyme disease can be transmitted. Because of this, the disease is actually quite preventable if owners are vigilant. Most cases of Lyme disease are caused by bites from immature ticks, which are called nymphs. This is because their small size – only around 2 millimetres (just under a tenth of an inch) – makes them very difficult to see, meaning they’re more likely to stick around for the 36 hours required for Lyme disease infection. There is currently no evidence that Lyme disease can be transmitted through any means other than tick bites. If your dog does contract the disease, it will not be spread to you through contact with them. Similarly, if you contract Lyme disease, you won’t be at risk of spreading it to other humans or your pets.\nIf Lyme disease does manage to enter an animal’s bloodstream, it tends to gather around the joints, so swelling in these areas can be a good indication that Lyme disease might be present.\nLyme disease has quite a wide range of unpleasant symptoms in both humans and their canine companions. If your dog exhibits one or more of the following symptoms, Lyme disease could be the culprit:\nLike many infections, Lyme disease raises the body’s temperature, which makes antibodies more effective at fighting the bacteria. This change can sometimes manifest itself as a fever.\n- Loss of appetite\nLoss of appetite can be another sign of infection in animals. Sometimes their bodies attempt to purge themselves of the infective agent, and part of the process involves reducing food intake.\n- Lethargy and tiredness\nWhen an animal’s body is busy fighting off infection, little energy is left over for activities they usually find enjoyable. Dogs who have contracted Lyme disease are far less likely to enjoy their usual games and walks, and will probably sleep more.\n- Stiffness, discomfort, or general pain\nLyme disease can also cause some flu-like symptoms, including achy muscles. If your dog is unusually stiff or appears to be experiencing pain – especially in the summer months – the disease could be at play.\n- Swollen joints\nAs mentioned earlier, Lyme disease bacteria tend to gather around the joints when they enter an animal’s bloodstream. The body’s immune system will try to fight off the invading microorganisms, and the chemicals involved can cause the area to swell and become inflamed. If your dog appears unwell, check to see whether their joints are swollen, as this is one of the biggest indicators that they may have contracted Lyme disease.\n- Kidney failure\nIn extreme cases of Lyme disease, a dog’s kidneys can fail, which sometimes proves fatal. This is due to the strain put on the organs as they attempt to process the toxins associated with the bacterial infection.\n- Cardiac issues\nIt’s very rare, but in some cases Lyme disease can also affect the heart’s function in a variety of ways, including arrhythmia and heart disease. Dogs suffering from heart conditions might exhibit symptoms such as coughing, shortness of breath, and tiredness.\nIf your dog is suffering from one or more of these symptoms, you should consult a veterinary professional as soon as possible.\nTo predict the development of Lyme disease, you could check your dog carefully for ticks, or signs of bites. If you do come across a tick, it can be sent by your vet for analysis, to find out whether it’s a member of the Lyme disease spreading black-legged family, and if so whether it is infected with Lyme disease. If the tick is found to harbor the disease, your vet may suggest pre-emptive Lyme disease treatment, such as a course of antibiotics.\nWhen your dog shows some of these symptoms, or you suspect they have been bitten by a tick, your vet will probably want to test for Lyme disease. Luckily, Lyme disease is fairly easy to identify through the C6 test, which can be carried out either in-clinic, or by sending a blood sample away for laboratory analysis.\nThe tests work by checking for the presence antibodies which work against the “C6” protein – a protein uniquely present in Borrelia burgdorferi bacteria. Antibodies generally develop in a dog’s body three to five weeks after the initial tick bite, and their presence in the bloodstream suggests that your dog’s immune system has been attempting to fight off Lyme disease.\nIf the results of the C6 test are positive, your vet will proceed to carry out the Quant C6 test. As its name suggests, this test determines the quantity of C6 antibodies present in the dog’s blood. If the concentration is relatively low, your dog is unlikely to require treatment, but at higher concentrations they could benefit from medical intervention.\nIf the initial test for C6 antibodies proves positive, your vet will probably need additional blood samples, as well as a urine sample. This will help to determine whether the dog’s kidneys have been damaged by Lyme disease, which would be suggested by the presence protein in their urine. This indicates that the kidneys are not carrying out their proper function of breaking down waste products before they leave the body.\nChecking for kidney damage is essential, since this is a potentially fatal issue which Lyme disease can trigger, and should be treated as swiftly as possible.\nSurprisingly, some dogs can carry Lyme disease without actually showing any symptoms. This means it’s sometimes difficult to know whether treatment would be worthwhile or even necessary. Your vet will be able to advise you based on your dog’s unique symptoms and medical history, but the decision is usually influenced by the following factors.\n- The quantity of C6 protein in their bloodstream\nIf the Quant C6 test reveals that your dog has moderate to high levels of the protein in their bloodstream, your vet is more likely to recommend that treatment is pursued. Conversely, if the concentration is low, they may decide against it.\nIf your dog has been displaying one or more of the symptoms closely linked to Lyme disease, such as swollen joints or stiffness, a positive C6 test would support the argument for treating the animal.\n- Medical history\nIf your dog has exhibited similar symptoms within the past year, Lyme disease is more likely to be the culprit, since it can stick around in the body for fairly long periods of time, and periodically flare up in some cases.\n- Urine sample results\nIf the presence of protein in a dog’s urine is found alongside C6 antibodies in their bloodstream, your vet is very likely to recommend treatment. This is because the combination of these indicators suggests that Lyme disease is causing some fairly serious problems in your dog’s body.\nSome vets might be reluctant to treat Lyme disease without symptoms for a number of reasons. Firstly, the over-use of antibiotics is a serious issue which has resulted in some harmful bacteria developing a resistance. Additionally, if your dog tests positive the presence of C6 antibodies, especially in low concentrations, it can be difficult to tell whether the bacteria are still present, or if the antibodies are simply left over from a previous infection which went undetected. Finally, since symptoms only develop in around 10% of dogs who contract Lyme disease, treating a symptomless illness could put unnecessary stress on the dog and owner, as well as incurring unnecessary costs.\nIf your vet does decide to treat your dog for Lyme disease, they will likely suggest:\n- Antibiotics, most commonly doxycycline and amoxicillin. Symptoms should begin to improve within 24 to 48 hours, and the course will usually last for around 30 days.\n- If necessary, kidney treatment. This could include intravenous fluid therapy, and drugs.\n- Depending on symptoms and severity, additional treatment. For instance, if inflammation has been an issue and anti-inflammatory drug might be prescribed.\nOnce treatment has been completed, your vet might decide to carry out another Quant C6 test, and require another blood sample. If treatment has been successful, you can expect to see a significant reduction in the quantity of C6 antibodies in your dog’s bloodstream. This result, combined with the improvement or alleviation of symptoms are key indicators that the treatment has done its job.\nAs mentioned earlier, Lyme disease is fairly preventable, since as far as we know it can only be transmitted through tick bites. To protect both yourself and your best friend against the illness, you should try to prevent these bites, and endeavour to catch them as early as possible when they do happen. To decrease the likelihood that your dog will be bitten, you can:\n- Use a spot-on treatment on your dog’s coat. These will repel and kill both ticks and fleas, and each application can remain effective for up to a month.\n- Consider an oral anti-tick medication. These pills are usually given once a month, and kill ticks and immature fleas in your dog’s coat.\n- Wash your dog with an anti-tick shampoo every now and again, especially in the spring and summer.\n- Keep your lawn and any bushes or hedges in your garden or yard well-trimmed, as this reduces the tick populations.\n- Consider treating your home for ticks with an anti-flea and tick spray.\n- Try applying a tick spray to your dog’s coat between shampoos, to ensure they’re kept at bay.\n- Avoid long walks through long grass and brush if possible, since these environments are ideal for ticks hoping to find hosts. They cannot jump or fly, but will crawl to the top of tall plants and try to crawl onto animals as they brush past.\n- Thoroghly check your dog for ticks after walks, especially in the spring and summer.\nFinding and Removing Ticks\nTicks can be tricky to spot on your dog, especially the tiny nymphs. Here are a few tips for finding and removing ticks:\n- Brush your fingers through your dog’s hair rather than using a comb; you’ll be able to feel any bumps which could be ticks more easily.\n- Make sure you check between the toes, behind the ears, under their limbs, and around the tail and head.\n- Be careful when removing ticks, as parts can sometimes break off and remain embedded in the skin.\n- To remove a tick, use tweezers, gripping it very close to the skin, before pulling it away gently. Do not burst its body, as this will increase the risk of Lyme disease infection.\n- Always wear rubber gloves to protect yourself from Lyme disease when removing a tick from your dog.\n- If you are unsure about removing a tick, consult your vet. They will be able to do so safely and effectively, and possibly teach you how to do so for future reference.\n- Remove any ticks within 24 hours to minimize the risk of infection.\n- How to treat Lyme disease in Dogs, Pet MD\n- Lyme Disease in Dogs: Symptoms, Tests, Treatment, and Prevention, American Kennel Club\n- Lyme disease in Dogs, Pet Health Network']	['<urn:uuid:a1c797e1-ebc1-447c-8a08-6a9f9c39cf3d>', '<urn:uuid:d4528c7d-72fa-4c11-8cae-536582946c39>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-12T11:40:26.317848	8	87	3980
69	I love collecting interesting stories about jewelry - can you tell me what happened to the Hope Diamond when it was owned by Mrs. McLean? I heard she had some unusual ways of storing it.	During its time in Mrs. Evalyn Walsh McLean's possession, the Hope Diamond was treated quite casually. Despite being one of the world's most famous diamonds, Mrs. McLean stored it in a plain shoebox and occasionally even let one of her dogs wear it. After her death, the diamond was purchased by Harry Winston Inc. in 1949, who later donated it to the Smithsonian Institute where it remains on display today.	"['Famous Blue Diamonds\nProfiles of eight of the largest and most famous (and infamous) blue diamonds. Sorry to say you won\'t stumble across any of these in your local pawnshop or on eBay - these gemstones are all in museums or private collections!\n- The Hope Diamond\n- Probably the best know blue diamond (and one of the most famous diamonds in the world), the 45.52 carat Hope Diamond is a fancy deep grayish blue diamond, presumed to have been mined in India. The first documented owner of this breathtaking diamond was a French merchant named Jean Baptiste Tavernier. At the time, it is said to have been a 112 carat diamond with a crude, triangular cut. In 1668, Tavernier sold it to King Louis XIV of France, who had it recut by court jeweler Sieur Pitau into a 67.12 carat stone and set in gold. It was nicknamed The French Blue, and was worn by King Louis on a ribbon around his neck on special occasions. During the French Revolution, the stone (along with the rest of the Crown Jewels) was turned over to the French government and subsequently stolen during looting in 1792.\nIn 1812 the Hope surfaced again, this time in the hands of London diamond merchant Daniel Eliason, who is assumed to have sold it to King George IV. In 1839 it was documented in the collection of Henri Louis Hope, from whom the diamond gets it\'s name. It remained in the Hope family until 1901, after which it was sold several times.\nYou may be wondering about the rumored Curse of the Hope Diamond - the claims that misfortune and tragedy have befallen all of the Hope\'s owners. In reality, the ""curse"" was just a clever marketing ploy - a combination of half-truths and out-and-out fiction, dreamt up by Pierre Cartier - presumably to enhance the diamond\'s mystique (and subsequent sale price!)\nIn 1912, the Hope diamond was purchased by the eccentric diamond mining heiress Mrs. Evalyn Walsh McLean. The exquisite diamond got little respect during it\'s time in the McLean household, where it was stored in a plain shoebox and occasionally worn by one of Mrs. McLean\'s dogs!\nThe Hope Diamond was purchased from the McLean estate in 1949 by Harry Winston Inc. After a decade of exhibitions, Harry Winston donated the Hope Diamond to the Smithsonian Institute, where it remains to this day - once again getting the royal treatment as one of the most visited exhibits in the museum.\n- The Terschenko Diamond (the Mouawad Blue)\n- The Terschenko diamond is a 42.92 pear shaped fancy blue believed to be of Indian origin. Originally owned by the Terschenko family (sugar barons in pre-communist Russia) it was smuggled out of the country just prior to the Russian revolution and ended up in the hands of a private owner. It remained in obscurity until 1984 when it resurfaced for auction at Christies, where it was purchased by Saudi Arabian diamond dealer Robert Mouawad for $4.5 million.\n- The Wittelsbach Diamond\n- At 35.56 Wittelsbach diamond is the 3rd largest blue diamond, as notable for it\'s unique dark blue color as for it\'s impressive size. It broke diamond auction records in December, 2008 when it was sold by Christies to London jeweler Laurence Graff for a jaw-dropping 23.4 million dollars. Former owners of this diamond include King Philip IV of Spain, Leopold I of Austria, and his granddaughter Maria Amalia - who married into the royal Wittelsbach family of Bavaria, from which the diamond takes it\'s name. [ Photo of the Wittelsbach diamond ]\n- The Sultan of Morocco\n- At 35.27 carats, the grayish-blue Sultan of Morocco is the 4th largest blue diamond. Once owned by Cartier, this cushion cut gemstone is believed to have been last sold in 1972, possibly by jeweler Laykin et Cie, to a private American collector. It was last seen on public display in 1969 at the New York State Museum\'s World of Gems exhibition.\n- The Eugenie Blue (the Blue Heart)\n- The 30.82 carats Eugenie Blue, also known as the Blue Heart diamond, is a heart-shaped fancy vivid or fancy deep blue diamond. Legend has it the the Blue Heart was once owned by Empress Eugenie (Eugénie de Montijo) - wife of Napolean III - but there\'s no real evidence of this and many experts doubt it to be true. The Blue Heart\'s country of origin is not known, but the diamond was cut into it\'s present-day heart shape around 1909 or 1910 by Atanik Ekyanan of Paris. It was then purchased by Cartier, and sold to an Argentinian woman (Mrs. Ungue). It remained in her possession until the mid 20th century when it was sold to Van Cleef and Arpels who subsequently sold it for $300,000 to a European family. A few short years later it was again purchased, this time by jeweler Harry Winston. It\'s last private owner was Marjorie Merriweather Post, who donated it to the Smithsonian museum , where it remains to this day - on display as part of the museum\'s diamond collecton.\n- The Blue Lili Diamond\n- A 30.06 carat, tapered cushion-cut blue diamond of unknown color grade. Not much is known about the Blue Lili, but it is believed to be from the Premier diamond mines in South Africa. It was purchased and cut by the William Goldberg Diamond Corporation.\n- The Heart of Eternity Diamond\n- This heart shaped fancy vivid blue diamond weighs in at 27.64 carats and originates, as do many other exquisite blue colored diamonds, from the Premier mines in South Africa. The Heart of Eternity is owned by a private collector and, as of this writing, is on loan to the Smithsonian National Museum of Natural History.\n- The Blue Magic Diamond\n- According to it\'s GIA (Gemological Institute of America) certificate, the Blue Magic is a 12.02 carat modified pear-shaped diamond of an exquisite fancy vivid (the highest color grade) blue color with a clarity of VVS-2.\nIn general, fancy blue diamonds continue to attract a lot of attention. Here are some notable, recent auction sales:\n- May 2009 - an internally flawless, 7.03 carat cushion-cut, fancy vivid blue diamond sold for $9.48 million dollars - a record-setting high price-per-carat ever at Sotheby\'s Geneva auction.\n- The Millennium Blue - a 5.16 carat pear-shape, internally flawless, fancy vivid blue, sold for $6.4 million dollars at Sotheby\'s Hong Kong auction in April, 2010.\n- In October 2010, the largest known triangular fancy vivid blue diamond ever auctioned (10.95 carats), aka the Bulgari Blue, set yet another auction record of $15.7 million dollars, making it top gemstone sale of 2010.\nPrevious: Blue Diamonds - FAQ\nResources: Learn more about these and other famous diamonds']"	['<urn:uuid:e5da9d4a-98b3-4ded-8010-6f791f376844>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T11:40:26.317848	35	70	1129
70	looking to understand how territory gets transferred between countries what are legal ways	Territory can be transferred between countries through several legal methods. One way is through agreement between the states involved in a dispute, which may be based on political claims, historical grounds, security reasons, or legal claims. The transfer conditions can be influenced by recommendations from international organizations and the principle of self-determination. Often, plebiscites (public votes) are organized under United Nations supervision, with the results being either indicative or binding. However, just disposing of territory alone is not enough for transferring sovereignty - there must also be an intentional display of power and authority over the territory, through continuous and peaceful exercise of jurisdiction and state functions.	['Status of Territory: Further Problems\n(p. 245) 10 Status of Territory: Further Problems\n1. International Procedures of Territorial Disposition1\nA basic assumption of the international system is that sovereignty—plenary power over territory—inheres individually in each state which has the better claim to title over that territory, and that it is not shared. But this is an assumption; from a legal point of view it may even be a presumption: it is not a rule, still less a peremptory norm. There is nothing to prevent a state from freely abandoning its sovereignty in favour of merger in another state, and what can be done in whole can be done in part. Groups of states, or an international organization, can come to exercise dispositive authority over a given territory: questions may then arise as to the modalities of the exercise of such powers and their relation to the self-determination of the people of the territory concerned. Some of these situations are grouped for consideration here.\n(A) Agreement between the States Concerned\nA cession of territory may depend on the political decision of the states concerned in a dispute. Such a cession may be the result of a political claim, on grounds of history or security, a legal claim, or a combination of these. The conditions under which transfer occurs may be influenced by the recommendations of political organs of international organizations and, latterly, by the principle of self-determination (see chapters 5, 29). On numerous occasions, plebiscites have been organized under the auspices of the United Nations, with the results treated as indicative or binding.2\n(p. 246) (B) Joint Decision of the Principal Powers\nLikewise on a number of occasions a group of leading powers, perhaps in association with a number of other states, have assumed a power of disposition, although the legal bases of such a power were sometimes problematic.3 It is possible that, as in the case of the creation of a new constitution by rebellion, the political and legal bases are inseparable: certainly the legal consequences of this power of disposition are commonly accepted. The mandates system rested in substantial part at least on such a power of disposition, and the International Court accepted its consequences in its successive advisory opinions on the status of South West Africa.4\nDisposition of territory alone is not enough for a transfer of sovereignty, however. In the Eritrea/Yemen arbitration, the Tribunal considered the status of certain Red Sea islands in light of Article 16 of the Treaty of Lausanne, by which the Ottoman Empire renounced sovereignty over the islands. It held that no doctrine of reversion of historical title applied, so that sovereignty over the islands in question had remained indeterminate after Turkey divested itself of the territory.5 What was required for acquisition of the territory was ‘an intentional display of power and authority over the territory, by the exercise of jurisdiction and state functions, on a continuous and peaceful basis’.6\n(C) Action by United Nations Organs\nIt is doubtful if the UN has a capacity to convey title, in part because it cannot assume the role of territorial sovereign: in spite of the principle of implied powers, the UN is not a state and the General Assembly only has a power of recommendation. On this basis it can be argued that GA Resolution 181(II) of 29 November 1947, approving a partition plan for Palestine, was if not ultra vires at any rate not binding on member states.7\nHowever this may be, the fact is that states may agree to delegate a power of disposition to a political organ of the UN, at least where the previous sovereign has relinquished title; but there is no transfer of sovereignty and no disposition of a title inhering in the Organization. In such cases the Organization acts primarily as a referee. The General Assembly played this type of role in relation to the creation of the new states of Libya and Somalia and in the case of territory relinquished by Italy under the Peace Treaty of 1947.8\nReferences(p. 247) On similar principles, the General Assembly probably retained a power to terminate trusteeship status for cause.9 But the termination of mandates was a matter of more difficulty, partly because the power of disposition arguably inhered in the principal Allied Powers participating in the Treaty of Versailles.10 It may be that, in the historic cases of mandate and trusteeship, and also of the few remaining territories to which Chapter XI of the Charter applies, the UN does not ‘confer sovereignty’, but rather decides on or approves the manner in which the principle of self-determination is to be implemented. Certainly resolutions of the General Assembly play an important element in the consolidation of title over territory. This is especially the case with the resolutions based on Resolution 1514(XV), the Declaration on the Granting of Independence to Colonial Countries and Peoples.11\nHowever that may be, the General Assembly assumed the power to terminate the Mandate for South West Africa in Resolution 2145(XXI) of 27 October 1966.12 Subsequently the General Assembly established the Council for South West Africa, appointed a UN Commissioner to administer the territory, and renamed the territory ‘Namibia’. South Africa failed to respond to these developments and the Security Council adopted resolutions in 1969 and 1970 ‘recognizing’ the decision of the General Assembly to terminate the Mandate and calling upon all states to take measures to implement the finding that South Africa’s continued presence in Namibia was illegal. By a further resolution the International Court was asked to give an advisory opinion on the question, ‘What are the legal consequences for States of the continued presence of South Africa in Namibia notwithstanding Security Council Resolution 276 (1970)?’ As a preliminary to giving its views on the substance of the question, the Court considered the validity of GA Resolution 2145(XXI) in terms of the Charter.13 The Court held that the power of the League of Nations, and therefore of the United Nations, to revoke the Mandate for reasons recognized by general international law (termination on the ground of material breach of a treaty) was to be implied.14 The role adopted by the General Assembly, assisted by the Security Council, was to take such action as was necessary to ensure the application of the provisions of Resolution 1514(XV) to the people of Namibia. In formal terms at least, this did not involve a power of disposition as such, but the application of the provisions of the Charter, as interpreted by the practice of the organs, relating to the principle of selfReferences(p. 248) determination.15 Namibia eventually achieved independence in 1990 aft er elections supervised by the UN Transition Assistance Group.16\nThe role of the General Assembly in the decolonization of Western Sahara has involved a complex of issues concerning the principle of self-determination and the legal interests of Morocco (and at one time Mauritania).17The situation remains unresolved.18\nIn the aftermath of the Iraqi invasion and occupation of Kuwait the Security Council adopted Resolution 687 (1991). The resolution specified the measures to be taken under Chapter VII of the Charter. In particular, the Security Council demanded respect for the agreed territorial delimitation,19 and decided ‘to guarantee the inviolability of the…international boundary and to take as appropriate all necessary measures to that end in accordance with the Charter of the United Nations’. In the event, following the eviction of Iraq by a broad-based coalition acting under a Security Council mandate, a Demarcation Commission was created: it submitted a Final Report on the demarcation of the international boundary between Iraq and Kuwait on 20 May 1993.20 In Resolution 833 (1993) the Security Council adopted the decisions of the Commission as ‘final’. The exercise was, at least in form, the demarcation of an already agreed alignment and no ‘reallocation’ was intended. However, when the Final Report is examined it follows almost inexorably that elements of delimitation were involved, especially in relation to the maritime delimitation.21 The outcome was controversial but it is important to remember that the Security Council expressly disclaimed an intention to use the demarcation process for the purpose of ‘reallocating territory between Kuwait and Iraq’. Iraq subsequently recognized the boundary so determined.22\nIn the context of maintaining international peace and security UN organs have also been prepared to assume administrative functions in relation, for example, to theReferences(p. 249) City of Jerusalem,23 the Free City of Trieste,24 East Timor,25 and Kosovo.26 The existence of such administrative powers rests legitimately on the principle of necessary implication and is not incompatible with the view that the UN cannot have territorial sovereignty.\n2. Sovereignty displaced or in abeyance\nAlthough an undivided sovereignty is the normal mode of territorial administration, exceptional situations exist which cannot be forced into the sovereignty straightjacket. Thus sovereignty may be held jointly by two states, as in a condominium,27 or distributed in time, as with a leasehold or other grant of sovereign rights subject to an ultimate right of reversion.28 Or it may be in abeyance, as with the mandate and trusteeship systems.29 A brief analysis of some other possibilities follows.\n(A) Territory Sub Iudice\nWhen a territorial dispute is referred to adjudication, there is a real sense in which sovereignty is in abeyance pendente lite: at any rate the tribunal cannot acknowledge either state as sovereign pending its decision, although the decision once given will be declaratory in form. The analogy here is perhaps with the right of possession which the sequester or stakeholder had in Roman law.30 The existing regime rests on acts in the law which in principle could not create sovereignty in the existing holder but which do not render the region terranullius. For practical purposes the present possessor may be regarded as exercising normal powers of jurisdiction and administration, subject only to external limitations arising from the legal instruments determining the status of the region. Thus the relevant agreement may contain provisions for demilitarization.\nFurthermore, there must be an implied obligation not to act in such a way as to render fulfilment of the ultimate objective of the arrangement impossible. Thus if theReferences(p. 250) stated objective is to provide for an expression of opinion by certain minority groups it would be ultra vires to deport or to harass and blackmail the groups concerned.31 In this respect, the absence of a textually-prescribed enforcement mechanism is not enough to offset the obligation not to impede fulfilment of the end goal, though the presence of such a mechanism will add yet another arrow to the bow. The status of the inhabitants in terms of nationality and citizenship will depend on the circumstances of the particular case.32 If one accepts the obligations inherent in the doctrine of the ultimate objective then the conferment and deprivation of nationality would not be a matter of domestic jurisdiction for the administering state.']	['<urn:uuid:4ab47b56-a8fb-48d1-9347-27df90335d05>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T11:40:26.317848	13	107	1797
71	I'm excited about the new flight simulator game but worried about fuel usage in military planes. Can you tell me about the new F/A-18 Super Hornet they're adding to the game and how its fuel consumption works in real life?	The F/A-18 Super Hornet is being added to Microsoft Flight Simulator on November 18 as part of the Game of the Year Edition, marking their first military jet in the game. In real life, the F/A-18 Super Hornet can store up to 14,400 lbs of fuel internally, and when operating with afterburners engaged, its GE F414 engine consumes approximately 72,000 lbs of fuel per hour in total. Due to this extremely high fuel consumption, pilots typically use the afterburner very sparingly, usually for just a few minutes during a mission.	['Microsoft Flight Simulator: Match of the Calendar year Edition Coming November 18!\nIt has been an incredible journey because we launched Microsoft Flight Simulator on Computer. The main flight simmer viewers embraced the new working experience straight away and with Xbox Game Move for Pc, a new viewers of Pc players was attracted who experienced beforehand hardly ever played a flight simulator in advance of.\nIn July 2022, we introduced Microsoft Flight Simulator on Xbox Collection X|S and welcomed tens of millions of console gamers to the flight simulation experience. We released new attributes this sort of as Discovery Flights, expanded tutorials, Flight Assistant, Map Labels, and Land Wherever, that are aimed to assist newcomers to correctly enter the globe of Microsoft Flight Simulator. This launch introduced various improvements to the sim such as important performance gains that benefitted the Computer system audience as properly.\nAnd the start was just the beginning. Through the previous 14 months, we delivered major regular monthly updates: 6 World Updates and six Sim Updates that regularly improve the encounter. And currently, we are happy to announce the Microsoft Flight Simulator Game of the 12 months (GOTY) Edition. This new version celebrates all the constructive responses, opinions and awards garnered, but it is also a “Thank You” to our lovers – new and outdated – that have manufactured Microsoft Flight Simulator this sort of a phenomenon.\nBelow is what is involved in the GOTY Edition:\nWe are incorporating 5 model new plane to Microsoft Flight Simulator:\n- Boeing F/A-18 Super Hornet: Our 1st military jet and a remarkably asked for aspect from the neighborhood. The Top rated Gun: Maverick expansion will release with the movie up coming Spring, but we needed simmers to have the opportunity to examination their require for pace this getaway.\n- VoloCity: We partnered with very well-regarded German business Volocopter, which is doing the job on an eVTOL called “VoloCity”, a vision for an city air taxi. We worked closely with the Volocopter engineering staff to acquire an reliable model of the prototype aircraft for the simulator. This is our initially plane that can carry out pinpoint landings and is a teaser of what you can seem ahead to in 2022 when we intend to start helicopters in the sim.\n- Pilatus Computer-6 Porter: This legendary brief takeoff and landing (STOL) utility aircraft is a highly flexible plane from Switzerland and arrives with various cockpits, cabins, and landing gear variations. It is the final result of our shut collaboration with the producer and the progress attempts by famed developers Hans Hartmann and Alexander Metzger resulted in a wonderful and entertaining new plane with exception capabilities in the simulator.\n- CubCrafters NX Cub: Yakima-dependent CubCrafters recently launched a nosewheel choice for their flagship CC-19 XCub Aircraft, commonly referred to as the NX Cub, which we are happy to introduce to the flight sim viewers to additional enrich our bush flying and off airport selections.\n- Aviat Pitts Particular S1S: One particular of our most popular planes gets a solitary-seat choice with the launch of this plane.\nWe are introducing 8 handcrafted airports in Central Europe and the United States:\n- Leipzig/Halle Airport (EDDP)\n- Allgäu Airport Memmingen (EDJA)\n- Luzern-Beromunster Airport (LSZO)\n- United States\n- Patrick Area Drive Foundation (KCOF)\n- Maritime Corps Air Station Miramar (KNKX)\nThe Sport of the 12 months release is including info about 545 earlier lacking airports in the United States.\nDependent on the reputation of the not too long ago released Discovery Flights, we are incorporating an further 6 spots (Helsinki, Freiburg im Breisgau, Mecca, Monument Valley, Singapore, and Mount Cook dinner) to this preferred series.\nTo more develop the onboarding knowledge, we are incorporating 14 new tutorials, introducing simmers to Bush traveling (in an Icon A5) and IFR (in a Cessna 172).\nWe are also happy to introduce several really requested functions by the community: an updated temperature technique, early access to DX12, and a dev mode replay method.\nNew Photogrammetry Towns\nAs aspect of our ongoing collaboration with Bing Maps, we are delighted to insert a selection of new photogrammetry towns: Helsinki (Finland), Freiburg im Breisgau (Germany), Brighton, Derby, Eastbourne, Newcastle, and Nottingham (United kingdom) and Utrecht (Netherlands).\nWindows 11 Collaboration\nTo celebrate the launch of Windows 11, from October 19th to November 18th, we have picked 11 landmarks across the globe that will be lit up in Windows blue inside Microsoft Flight Simulator. In addition, we are releasing a distinctive Windows 11 themed livery for the Added 330LT for simmers to increase to their assortment.\nAnd, ideal of all, the Microsoft Flight Simulator Video game of the Calendar year Version will be offered as a no cost update for existing players on Personal computer and Xbox Sequence X|S. For new consumers, the GOTY Edition is the ideal entry stage to flight simming as it gives an even richer experience, and it will be the product they purchase as we are retiring the primary.\nWe just can’t hold out for you to experience this expanded edition of Microsoft Flight Simulator on November 18. We encourage you to invite your close friends to working experience the miracles of flight. The sky is contacting!\nMicrosoft Flight Simulator is now available on Xbox Collection X|S with Xbox Recreation Go, Home windows 10/11 with Xbox Video game Pass for Computer, and Steam.\nFor the most recent information on Microsoft Flight Simulator, remain tuned to @MSFSOfficial on Twitter.', 'An afterburner, a distinctive feature of certain engines, is renowned for it’s remarkable inefficiency, as it voraciously devours fuel at a staggering rate of up to three times more than conventional engines. Consequently, pilots conscientiously restrict it’s usage during missions, usually reserving it for mere moments. Although the mechanism behind an afterburner may appear uncomplicated, it’s functionality relies on an intricate balance of delicately calibrated tolerances, demanding meticulous precision and skill.\nHow Long Can Fighter Jets Use Afterburners?\nFighter jets equipped with afterburners have the ability to tap into an immense power source, but it comes at a cost. The afterburner, a key feature of these engines, is notorious for it’s inefficiency. When engaged, it consumes fuel at an accelerated rate, up to three times higher than regular engine operation. As a result, pilots are careful to employ this feature sparingly, usually restricting it’s use to a matter of minutes during a mission.\nThe concept underlying an afterburner may be relatively straightforward, but it’s operation is anything but. The afterburner system introduces additional fuel into the exhaust stream, which then ignites, resulting in a sudden surge of power. This process demands intricate engineering and meticulous maintenance to ensure it’s reliability and accuracy. Even slight deviations in the afterburners delicate tolerances can cause a cascade of issues, potentially compromising the engines overall performance.\nFurthermore, prolonging afterburner usage can impose excessive strain on various components, such as turbine blades and exhaust nozzles. These parts are designed to withstand the intense forces generated under normal operating conditions. However, when subjected to prolonged afterburner use, they may succumb to the increased heat and stresses, leading to potentially catastrophic failures.\nThey reserve it’s deployment for critical moments or situations, where maximum thrust is necessary to achieve specific objectives. By doing so, they strike a balance between harnessing the incredible power of the afterburner and conserving precious fuel.\nMoving on to the next topic, it’s fascinating to explore the endurance of the F-16 in full afterburner. At low altitudes, the F-16 is capable of burning an astonishing amount of fuel, exceeding 64,000 pounds per hour. When operating at maximum capacity with external fuel stores, the U.S.-variant F-16 can sustain this level of performance for approximately 20 minutes before being forced to rely on emergency reserves, which would only provide an extra minute or so of full afterburner capability.\nHow Long Can F-16 Use Afterburner?\nThe F-16, renowned for it’s exceptional performance and versatility, is no stranger to the use of afterburners. These powerful engines enable the aircraft to achieve impressive speeds and climb rates, making it a force to be reckoned with in the skies. However, the duration for which an F-16 can utilize afterburners isn’t indefinite.\nWith maximum external fuel stores, a U.S.-variant F-16 can sustain full throttle operations for roughly 20 minutes. Subsequently, the aircraft would rely solely on emergency reserves, which would provide an additional minute or so under such intense conditions. This limited timeline highlights the need for prudent fuel management during missions requiring extensive afterburner usage.\nIt’s worth noting that the duration of afterburner usage can also be impacted by altitude. Conversely, at low altitudes, where the atmosphere is denser, the afterburners consume fuel at a faster rate, limiting the duration they can be utilized.\nStrategies for Fuel Management During Missions Requiring Extensive Afterburner Usage\n- Optimize fuel consumption\n- Monitor fuel levels constantly\n- Use afterburner sparingly\n- Implement precise throttle control\n- Consider pre-flight weight reduction\n- Incorporate efficient flight planning\n- Perform regular engine maintenance\n- Utilize air-to-air refueling if available\n- Employ tactics to minimize time spent in afterburner\n- Train pilots on fuel-efficient flying techniques\nWatch this video on YouTube:\nThe Boeing F/A-18E and F/A-18F Super Hornet, derived from the McDonnell Douglas F/A-18 Hornet series, are renowned carrier-capable, multirole fighter aircraft. The F/A-18E variant, designed as a single-seater, and the F/A-18F variant, built as a tandem-seater, are upgraded versions of their predecessors. These impressive aircraft can internally store up to 14,400 lbs of fuel. In terms of fuel consumption, the powerful GE F414 engine can burn around 36,000 lbs/hr of fuel in full afterburner mode. Therefore, it’s typical for an F/A-18E/F Super Hornet to have a total fuel burn of approximately 72,000 lbs/hr in such conditions.\nHow Much Fuel Does a F18 Burn Afterburner?\nThe F/A-18E Super Hornet, equipped with the GE F414 engine, is a formidable aircraft in terms of fuel consumption. It possesses a substantial fuel capacity, with 14,400 lbs of fuel stored internally. However, when engaging the afterburner, the fuel consumption reaches another level entirely.\nThe afterburner is a significant feature of the F/A-18E, as it provides an immense boost of propulsion. With the afterburner activated, the GE F414 engine burns approximately 36,000 lbs/hr of fuel. This extensive fuel burn showcases the immense power output of the afterburner and it’s impact on the aircrafts overall fuel consumption.\nHowever, it’s important to note that fuel consumption may vary depending on factors such as mission requirements, aircraft configurations, and flight parameters.\nWhen the afterburner is engaged, the engine burns approximately 36,000 lbs/hr of fuel, resulting in a typical total fuel burn of 72,000 lb/hr under maximum power output conditions. However, it’s crucial to consider various factors that can affect fuel consumption when assessing the overall efficiency of this advanced multirole fighter aircraft.\nIn conclusion, the utilization of an afterburner in an engine comes at a significant cost in terms of fuel consumption. Consequently, pilots are cautious and restrict it’s use to only a few minutes during a mission to mitigate the fuel consumption impact. Balancing the benefits of enhanced thrust with the inherent cost of increased fuel consumption remains a crucial consideration in the design and operation of afterburning engines.']	['<urn:uuid:1403d82e-e1f9-4e00-a525-d5e4f3c493e9>', '<urn:uuid:5e11055f-6713-4483-a2eb-970fbb674bda>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T11:40:26.317848	40	90	1865
72	compare cooking preparation techniques used street vendors international meals traditional sri lankan home dishes	Street vendors internationally typically focus on quick preparation methods to serve busy customers, with foods like falafel, shawarma, and pad thai being cooked rapidly to order. In contrast, traditional Sri Lankan home cooking follows a much slower, more methodical process. While street vendors might use pre-prepared ingredients and quick cooking methods, Sri Lankan home cooking requires extensive preparation - spices must be ground using mortar and pestle, coconuts need to be manually plucked, cut and grated, and curries are stewed for hours over wood fires. This dedication to careful preparation and cooking techniques is fundamental to Sri Lankan cuisine, making it distinctly different from the fast-paced cooking methods typically used in street food preparation.	['There’s something about street food that just makes it irresistible. Maybe it’s the exotic flavors or the fact that you can get a cheap and delicious meal without having to go to a restaurant. Or maybe it’s the fun atmosphere of being out in the open air, surrounded by people from all over the world.\nNo matter what it is, street food is one of the best things about travel. And lucky for us, there’s no shortage of amazing street food to be found around the world.\nWhat is Street Food?\nStreet food is any type of food that is typically sold by a vendor on the street. This can include everything from hot dogs and pretzels to more exotic fare like kebabs or curries. Street food is a staple in many cultures around the world and offers a quick, convenient, and often affordable meal option for busy people on the go. While street food vendors have long been a common sight in major cities, the popularity of street food has exploded in recent years, with gourmet food trucks and carts offering high-quality fare to hungry urbanites.\nIn addition to being a convenient option for busy people, street food is also typically very flavorful and hearty. This is because street food vendors typically use fresh, local ingredients and cook their dishes to order. This means that the food is often cooked right in front of the customer, which allows them to enjoy the full flavor and aromas of the dish.\nStreet food is a great way to experience the local cuisine of a city or country. When traveling, be sure to sample the street food of the places you visit. You may be surprised at how delicious and varied street food can be!\n20 of the Best Street Food Around the World\nWe’ll take a look at some of the best street food from around the globe. From crispy falafel in Morocco to fresh seafood in Thailand, there’s something for everyone to enjoy.\nSo without further ado, let’s get started!\nThere’s no better place to start than with one of the most popular street foods in the world: falafel. These little fried balls of goodness are made from ground chickpeas or fava beans and are typically served in a pita bread with salad and tahini sauce.\nFalafel originates from Egypt, but it’s become popular all over the Arab world. In Morocco, you’ll find some of the best falafel around. The secret to their success is in the spices, which usually include cumin, paprika, and coriander.\nCoxinha is made from shredded chicken or beef that’s been marinated in garlic and spices. The meat is then wrapped up in a dough ball and deep-fried until crispy.\nYou’ll find coxinhas all over Brazil, but they’re most commonly found in the city of Sao Paulo. There, they’re usually served with ketchup or mayo.\nChimney Cake, Czech Republic\nChimney cake, also known as “kürtőskalács” in Hungarian or “streudel” in German, is a popular pastry in the Czech Republic. It’s made from a sweet dough that’s wrapped around a spit and roasted over an open flame.\nThe dough is usually flavored with cinnamon, sugar, and nuts, and the resulting cake is crispy on the outside and soft on the inside.\nPoutine is a Canadian dish made from french fries, cheese curds, and gravy. The cheese curds are what make poutine unique, and they can be either fresh or aged.\nPoutine originated in Quebec, but it’s become popular all over Canada. In fact, it’s even been featured on the menu of McDonald’s in some Canadian provinces.\nFish Tacos, Mexico\nIf you’re a fan of seafood, then you’ll love these fish tacos from Mexico. Fresh fish is grilled or fried and served in a soft tortilla with shredded cabbage, salsa, and lime. The result is a light and flavorful taco that’s perfect for a quick snack.\nFish tacos are believed to have originated in Baja California, but they’ve now become one of the most popular street foods in Mexico. You can find them all over the country, but they’re especially common in tourist areas like Cancun and Puerto Vallarta.\nPastel de Nata, Portugal\nPastel de Nata is a Portuguese dessert made from puff pastry and custard. The pastry is usually flavored with cinnamon and lemon, and the custard is usually made from eggs, milk, sugar, and vanilla.\nPastel de Nata was originally created by monks in the Jerónimos Monastery in Lisbon. Today, you can find them all over Portugal, but they’re especially popular in Lisbon and Porto.\nJerk Chicken, Jamaica\nJerk chicken is a popular dish in Jamaica made from chicken that’s been marinated in a spicy sauce made from peppers, onions, and allspice. The chicken is then grilled or smoked, and it’s often served with rice and beans or plantains.\nJerk chicken is one of the most popular street foods in Jamaica, and it’s also a popular dish to eat at home. The key to making the perfect jerk chicken is in the marinade, so be sure to ask for the recipe if you’re ever lucky enough to try it!\nThese spring rolls from the Philippines are delicious and popular street food. They’re made with a variety of fillings, including meat, vegetables, and noodles, and then wrapped in a thin pastry skin. Lumpia are usually fried, but they can also be served fresh.\nLumpia is often served as an appetizer or snack, but it can also be a meal in itself. They’re typically served with a dipping sauce, such as sweet and sour sauce or peanut sauce.\nBao buns are made from steamed dough filled with meat or vegetables. The buns can be either round or square, and they’re typically served with soy-based sauce.\nDango is a type of dumpling that’s popular in Japan. It’s made from mochiko (a type of rice flour), and the dough is typically flavored with soy sauce, sugar, and mirin.\nDango can be made in a variety of different shapes, but the most common shape is a ball. They’re often served on a skewer, and they can be either grilled or boiled.\nDango is usually served as a snack or dessert, but it can also be a meal in itself. It’s typically served with tea, and it’s often eaten during the cherry blossom season.\nKottu Roti, Sri Lanka\nKottu Roti is a Sri Lankan dish made from chopped roti (a type of flatbread) and vegetables. The roti is chopped and mixed with the vegetables, and then it’s cooked on a griddle.\nKottu Roti is typically served with curry or chutney. It’s also a popular dish to eat at home, and it’s often served with rice.\nPoffertjes are a type of Dutch pancake that’s popular in the Netherlands. They’re made from a batter of wheat flour, milk, yeast, eggs, and sugar, and they’re usually served with butter and powdered sugar.\nPoffertjes are typically cooked on a special griddle called a poffertjes pan, and they’re often served with a cup of coffee or tea. They can also be found at many fairs and festivals in the Netherlands.\nBaklava is a Turkish dessert made from layers of phyllo dough filled with nuts and honey. The dough is usually flavored with cinnamon, cloves, and cardamom.\nBaklava is a popular dessert in Turkey, and it’s also popular in other parts of the Middle East. It’s typically served with a cup of coffee or tea.\nBanh Mi, Vietnam\nBanh Mi is a Vietnamese dish made from a baguette that’s filled with meat, vegetables, and herbs. The meat can be either pork, chicken, or beef, and the vegetables can vary depending on the region.\nThis popular German street food is made with grilled or fried sausage that’s been smothered in curry ketchup and served with a side of fries. Currywurst is a quick and easy meal that’s perfect for on-the-go eating.\nCurrywurst is believed to have been invented in Berlin after World War II, when British soldiers introduced curry powder to Germany. Today, it’s one of the most popular street foods in the country.\nThese cornmeal cakes from Venezuela are a staple of the street food scene. They’re made with white or yellow cornmeal and can be filled with a variety of ingredients, including meat, cheese, and vegetables. Arepas are typically grilled or fried, and they make a great snack or light meal.\nArepas are traditionally eaten for breakfast, but you can find them being sold at street stalls all day long. They’re a popular choice for a quick and easy meal, and they’re often served with a cup of coffee or tea.\nPad Thai, Thailand\nThis stir-fried noodle dish from Thailand is one of the most popular street foods in the world. It’s made with rice noodles, tofu, vegetables, and a variety of spices and sauces. Pad Thai is typically served with a side of lime and peanuts.\nPad Thai is believed to have originated in Thailand during the 1930s, when it was created as a way to promote Thai nationalism. Today, it’s a beloved street food that’s enjoyed by locals and tourists alike.\nShawarma, Middle East\nShawarma is a dish that originated in the Middle East and is now popular all over the world. It’s made with grilled meat, typically chicken or lamb, that’s been thinly sliced and served in a pita bread with salad and tahini sauce.\nShawarma is common street food in many Arab countries, as well as in Israel. It’s also popular in other parts of the world, such as Europe and North America.\nAloo Tikki, India\nAloo Tikki is popular street food in India that’s made with boiled potatoes, onions, and spices. The mixture is then shaped into small patties and fried until golden brown. Aloo Tikki is typically served with a dipping sauce, such as tamarind or mint.\nThis street food originated in the state of Uttar Pradesh in northern India. Today, it’s enjoyed all over the country and is a common sight at street stalls and markets.\nChurros are a popular Spanish street food that’s made with fried dough and coated in sugar. They’re typically served with a hot chocolate or coffee for dipping.\nChurros are believed to have originated in Spain, although there are many different theories about their exact origins. Today, they’re enjoyed all over the world and are a popular choice for a quick and easy snack.\nWhether you’re looking for a quick snack or a full meal, street food is a great option. These are just a few popular street foods from around the world that are sure to tease your taste buds. So, next time you’re traveling, be sure to give them a try!', 'Sri Lankan cuisine is a reflection of both its diverse landscape and its resourceful people. With a tropical environment of coconut-fringed wet zones, arid dry zones, and cooler tea-coated highlands, almost anything can grow and live here. This means that vegetables, fruit, fish, meat, and spices are all in abundance and come in a wide variation. The Sri Lankan staple of rice and curry, therefore, has endless possibilities of flavor, ingredients, and contrasts, all created from locally-grown produce. On such a small island, the winter vegetables of the hill country, the tropical fruits of the jungle, and the coastal catch of the day only have to travel a few hours, at most, to arrive on your plate, ensuring that the food is always fresh. Typical Sri Lankan cooking usually incorporates as much of this natural produce as possible, with several different dishes of curry served at each meal creating a medley of tastes and flavors, all fused together with fiery spices and a gigantic helping of bat (rice.)\nIn Western culture we have adapted our food to fit our fast-paced lifestyles. Food is more often than not processed, packaged, and microwaved. Sri Lankan cuisine, on the other hand, mirrors the slow pace of life here. Each process takes time and careful planning. Spices must be smashed together using mortar and pestle; coconuts have to be plucked from the tree then cut and grated; firewood must be brought into the kitchen to stock the fires upon which curries are stewed for hours at a time. This dedication to cooking is part of what makes Sri Lankan food so distinctly different to what most of us are used to, and means that eating is not simply a way of filling your stomach, but a ritual that should be savoured and enjoyed fully.\nConsidering how many foreigners visit Sri Lanka, it is surprising how few actually manage to sample the local food. The restaurants and cafes that line the beaches cater to Western tastes, and the larger and more posh hotels tend to stick to international fare. Since Sri Lanka does not traditionally have a dining-out culture, much of the best food is cooked in the home, and if you don’t mix with the locals, you are unlikely to uncover the culinary delights on offer here. So how are you supposed to get to know Sri Lankan food if it’s not readily available?\nTalk to the locals\nThis may seem pretty straight forward, but it is something that so many travelers neglect to do. Take your nose out of your Lonely Planet guide and get your information from the people who really know their stuff. Sri Lankans are friendly people, and they are proud of their country and happy to share it with visitors. Ask the locals where they like to eat and inform them that you want to try real Sri Lankan food. They’ll be more than happy to point you in the right direction, and you may even get an invitation to dine at home with them.\nFind out which dishes to eat where\nIt makes sense that different areas offer different types of food. Along the coastline, fresh seafood is widely and cheaply available. You can expect to dine on lobster, crab, and devilled (Chinese-style) jumbo prawns at many of the restaurants fringing the beach. Alternatively, the many fish markets in the town areas are a great way to ensure that your money is going towards those who need it most, and even the smallest guesthouses will be happy to cook your fish for you in their kitchen.\nIn the hill country fiery curries are plentiful to make up for the cool weather. Seasonal fruit and veg such as asparagus and strawberries are also available and often come straight from the restaurants’ own vegetable patches. In the north, the south Indian influence makes for amazing dosa (rice pancakes) and savory roti (an Indian-style flatbread), served with delicious vegetarian curries. In the southern tourist belt sweet roti and kottu roti (flatbread cut up and fried with spices and vegetables) are popular. During festivals and special events, kiribath (milk rice) and spicy sambols (a chilli-based sauce) are staples. Research the place you are going and any events/seasonal changes that might be happening there to get the most out of it.\nSri Lanka doesn’t have any scary surprises in its food. They don’t eat dog, snakes, or insects, and most of the food is simply vegetables and spices. If you don’t like spicy food, ask for a toned-down version with less spice. Curry for breakfast might sound weird, but when it comes with gooey fried eggs, idiappa (steamed noodles made from rice flour), sweet sambol and kiri hodi (coconut milk curry), it can be a great alternative to the jam and toast that is offered at most hotels. The prawn vaddi (fried lentils and spices with prawns) often sold by toothless men on trains can seem dubiously unhygienic, but it makes a fantastic travel snack!\nEat with your hands\nA good meal should be something that stimulates all your senses. It is the amalgamation of smell, look, touch, and taste that combine together to create a really special eating experience. While most food can look, smell, and taste delicious, your mouth has limited touch receptors and cannot feel textures and contours quite like your fingertips. When eating food with your hands, you become more aware of the feel and heat of your food.\nIn Sri Lanka, meals are colorful and visually appealing, the smell has lingering aromas of rich spices such as cinnamon, cardamom, and curry leaves, and the taste is robust and contrasting in flavor. When you add the final sense, touch, to this mix, you get a totally unique eating experience. It is this full-frontal attack on the senses that makes Sri Lankan cuisine so addictive. Most visitors to Sri Lanka eat using a fork because Sri Lankan’s rarely think to instruct them to do otherwise, but there is not a Sri Lankan in the country who would eat a rice and curry with a fork by choice. Eating with your hands can take a bit of getting used to at first, but once mastered it becomes almost natural.\nTo eat with your hands, first you need to mix all the curries together, taking little bits of each curry and mixing with rice until everything is broken down….\nThen you need to roll the rice up into a ball shape using your forefingers…\nAnd then simply pop the ball into your mouth using your thumb to push the food in…\nThere are a few simple etiquette rules to be aware of when eating with your hands:\n- only use one hand (normally your right one)\n- don’t let food get past the second knuckles or onto your palm (you should only be mixing with your forefingers, not your entire hand)\n- don’t lick your fingers after eating (there’s always a sink and tap at the side of restaurants to wash your hands both before and after eating)\n7 foods you should not leave Sri Lanka without eating\n1: Rice and curry\nWhat is it/what makes it special? Rice and curry is Sri Lanka’s staple dish, and it incorporates all the essential flavors that make up Sri Lankan cuisine. While Westerners might think of the curry as the most important part, the word order of this dish perhaps gives a clue as to the most important part of the meal for Sri Lankans. Rice, as in most countries in Asia, is essential to Sri Lankan food and comes in a wide variety with multiple types of rice, many of which are not available outside of Sri Lanka. Red rice, in particular, is both healthy and delicious and is one of the more widely available varieties on offer. When ordering rice and curry, you can usually expect to get one or two vegetable curries, a meat or fish curry, and parripu (dahl curry) served alongside a giant portion of rice.\nWhen to eat it? Sri Lankans often eat rice and curry for breakfast, lunch, and dinner. When dining out, however, rice and curry is usually only eaten during lunchtime (between 11am-1pm).\nWhere to find it? Rice and curry is not hard to find and is sold all over the island. Venture away from the tourist restaurants and find the local cafes to get the best (and cheapest) rice and curries.\n2: Idiappa (String Hoppers)\nWhat is it/what makes it special? Idiappa are a type of steamed noodle made from rice flour. They are usually served with a specific range of curries including pol sambol (a tangy mixture of grated coconut, chillis, onions, tomatoes, and lime juice), kiri hodi (a curry made from coconut milk that sometimes comes with fish or potatoes), and parripu (dahl curry).\nWhen to eat it? Idiappa are traditionally eaten for breakfast and can be found in small local cafes between 6am and 10am. It is almost impossible to find idiappa after breakfast time.\nWhere to eat it? For the best idiappa, look for the larger cafes frequented by the locals in the center of any town on the island. The Muslim-owned cafes also serve fantastic idiappa and are reputed in Sri Lanka for making the best curry.\n3: Appa (Hoppers)\nWhat is it/what makes it special? Hoppers are made from a rice flour batter which is cooked in a round pan and sometimes topped with eggs. They are quite plain in flavor but are usually served with lunu miris, a tart and fiery sambol made from onions, chillis, salt, and spices. Sometimes they are also served with other curries too.\nWhen to eat it? Hoppers are usually eaten in the evening between 6-10pm.\nWhere to find it? Most of the local rice and curry cafes will sell hoppers during the evening, and they can be found throughout Sri Lanka.\n4: Kiribath (milk rice)\nWhat is it/what makes it special? Kiribath is a Sri Lankan food that only comes out at the best of times. Birthdays, festivals, New Year, weddings – any type of special occasion usually brings a fresh helping of milk rice! Sometimes eaten alone, sometimes eaten with sambols and curry, milk rice is perhaps the Sri Lankan equivalent of a chicken soup in that it is a much-loved comfort food.\nWhen to eat it? At any type of celebration, whether in the morning, afternoon, or evening.\nWhere to find it? Kiribath is not sold in shops, it is only made in the home. Whilst some hotels might occasionally serve it up, it can be really hard to find unless you mix with the locals and snag yourself an invite to a village event or party. Do some research ahead of your trip to find out what festivals are on at that time. If there is any kind of special event happening, you can be guaranteed that someone will be making kiribath. You simply have to make friends with some Sri Lankans to find it!\n5: Kottu Roti\nWhat is it/what makes it special? Kottu roti is a medley of cut up pieces of roti (flatbread) cooked on a hot metal plate along with spices, vegetables and, sometimes, meat. There are endless variations and in the more touristy areas of the South Coast you can expect to find more Westernised concoctions such as avocado, chicken, and cheese kottu roti. Sri Lankan food doesn’t get much more unhealthy than this, so kottu roti is something that you shouldn’t binge on, but it’s definitely worth treating yourself once in a while!\nWhen to eat it? Kottu roti can be found only in the evenings, as it makes use of the leftover ingredients of the day. Usually kottu roti will start being made around 6pm and can often be found right up until 12am.\nWhere to find it? There are specific roti shops that sell kottu roti all over Sri Lanka. Have some locals point you in the right direction, or listen out for the distinctive noise of metal crashing down on metal as the noisy cutting sounds of kottu roti being made can be heard from miles away.\n6: Short Eats\nWhat is it/what makes it special? “Short eats” is a term that covers a range of savory and sweet snacks in Sri Lanka that are usually sold in small cafes or bakeries. Particularly popular short eats are vaddi (a spicy snack made of lentils and chillis) vegetable roti (flatbread folded into a triangular shape around vegetable curry), malu paan (fish sambol inside a bread roll) and egg rolls (curried egg inside a fried breadcrumb coating.)\nWhen to eat it? You can usually find short eats in the mornings and afternoons, all over the island.\nWhere to find it? Bakeries, small roadside cafes, and kades (local shops) usually sell short eats. You can also find them in small vans that drive around the south coast playing Fur Elise (each food van has a different tune), or from vendors on the trains and in large public bus stations.\nWhat is it/what makes it special? Being an island, there is no shortage of seafood in Sri Lanka. If you get up early enough, you will see the fisherman pull in their fresh catch every morning on the beach. Local specialties include squid, lobster, crab, jumbo prawns, and even shark! These are often barbequed on the beach, curried, tossed and fried with garlic, or served in a local devilled sauce which is a sweet Chinese-style blend of chillis, capsicum peppers and ketchup.\nWhen to eat it? You can dine on seafood all day, any day.\nWhere to find it? Seafood is easy to find wherever you are on the island, but for the freshest fish of all, head towards the sea! Sri Lanka’s coastal commercial capital Colombo has some excellent seafood restaurants, although the prices can be quite steep. Along the rest of the coast, the tourist restaurants and hotels that line the beach usually sell the widest range of seafood. Simply ask your waiter about their catch of the day. If you are interested in catching your own fish, there are also plenty of dive centers offering tourists the chance to go out on a fishing boat for the day, giving you a chance to learn more about the fishing industry in Sri Lanka, which was rather badly affected by the 2004 tsunami.\nWhat is it/what makes it special? Fruit in Sri Lanka can be found in many forms. Coconut comes in a range of curries and sambols, bananas and pineapple are often used to make sweet fruit pancakes, and papaya and mango are often mixed with yogurt to make lassies (fruit shakes). But there is no end of different fruits available here. The tropical weather in the coastal region allows for exotic fruits like jumbo jackfruit, juicy mangosteen, and giant watermelons, and in the hill country the cool temperatures means that treats such as strawberries are always in season.\nWhen to eat it? All the time! Make the most of the widely and cheaply available fruit in Sri Lanka by getting your five a day, every day.\nWhere to find it? You can buy fruit shakes, fruit platters, and fruit pancakes almost anywhere. Most shops will advertise their range of fresh fruit juices outside the door, so they are not hard to find. To get an idea of the real range that is on offer, however, head to the nearest fruit market and check out the local produce. Mangosteen and dragonfruit are particularly delicious. Also worth trying are the red bananas – Sri Lanka has many different varieties of banana, and the red ones are specific to the area.\nFind a flight to Sri Lanka, check out hostels in Sri Lanka, and read Paradise on a Budget: Eco Retreats that Won’t Break the Bank\nPhoto credits: Jaffna Sri Lanka, all other photos courtesy of the author and may not be used without permission.']	['<urn:uuid:3853125b-2f00-41a4-867c-1476d1c43fcc>', '<urn:uuid:9c65e0f4-142f-45fd-80a9-bf051da92207>']	open-ended	direct	long-search-query	distant-from-document	comparison	novice	2025-05-12T11:40:26.317848	14	114	4440
73	data ethics vs environmental ethics core values compare	Data ethics and environmental ethics share some core values but have distinct focuses. Data ethics emphasizes values like privacy, autonomy, beneficence/non-maleficence, justice, and integrity/honesty in handling personal information. Environmental ethics, while also concerned with right and wrong actions, focuses specifically on moral relations between humans and their natural environment, including responsibilities towards preserving biodiversity, natural resources, and the rights of non-human species.	"[""It’s hard to read the news lately without scrambling upon two words in close proximity to each other: data ethics. We’ve seen discussions around privacy, effects of the plethora of data we now have, and the questions around who owns this wealth of knowledge. As people who work in data, we understand the first word fairly well. It’s the second that’s creating the heat.\nThe amount of data we have multiplies daily. Data has become larger and more granular, with increased depth and density. This enables us as practitioners to move from testing hypotheses on a small and isolated set of data to building detailed profiles of individuals. With this knowledge, we can reshape experiences in the world, which is where we intersect directly with ethics. We are no longer limited by a lack of data, so we are responsible for our own ethical boundaries.\nWhat are ethics?\nAt the intersection between laws and values lives our new hot term: ethics. This word comes to light when things go awry—such as conflicts of interest or ethical violation—but its impact lies within the core of our daily personal and professional decisions.\nEthics come into play on the professional side when we act on behalf of another, either as a trusted advisor or as expert. Professionals are typically expected to focus first on the client, using our knowledge to help others. Doctors do this with patients by monitoring health, diagnosing disease, and providing the best treatment.\nTo understand ethics, we need to look at values since they closely interrelate. Values are what we, as humans, believe. They’re intensely personal, formed by family, religion, culture, and experience.\nWe break up values in a few ways:\n- Beneficence / non-maleficence: We often recognize this as ‘do no harm’.\n- Autonomy: This often gets rephrased as the right to decide or self-determination.\n- Privacy: We often apply this to actions or communications occurring ‘in confidence,’ implying that trust is essential and discretion is required.\n- Justice: Defined as ‘right and wrong’ or what’s perceived as fair.\n- Sanctity of life / physical safety: Literally, the right to exist in an environment that doesn’t harm the person.\n- Integrity / honesty: Ensuring that claims are represented fairly and truthfully.\nVarious industries may group or catalogue values in different ways. These values may overlap or run into contradictions, such as feelings about the death penalty (sanctity of life vs. justice vs. beneficence). As humans, we form our opinions through our values.\nEthics are philosophy\nThroughout the history of the world, philosophers have attempted to understand humanity and the world. The latter branch was considered ‘natural philosophy,’ which evolved into what we recognize as science. The former encompasses subjects like law and ethics, in addition to its look into rhetorical skills.\nScience and philosophy remain heavily intertwined. Often, a new breakthrough proves we can, with the next question being should we? These innovations not only change something about the world, they in turn shape us, our culture, and the realities we experience.\nData ethics in practice\nBoth ethics and philosophy provide a process for looking into potential value conflicts. For example, many societies value a level of privacy. As we find more ways to use data for potential benefit, this value runs headfirst into privacy concerns. How much is too much? Who can control this collection? Regulations like GDPR attempt to resolve this, but ethics provide much needed insight.\nNumerous ethical review tools exist. As an interpreter, I used Demand-Control Schema extensively for decisions that required an immediate response. In data, we often have more time to make a decision. Using common principles, an ethical review process for anyone involved in data could look like the below diagram:\n- Ambiguity: When we encounter a conflict, something may not sit quite right, like a request. It may be unclear what exactly is the issue, so using our categories of values helps us see what’s involved.\n- Defining: Having a standard list of values helps us tick through the boxes to identity the issue. Most dilemmas are triggered when two or more values are at odds with each other, such as privacy and autonomy, or justice and beneficence, when we look at data collection, ownership, and data-driven decision-making.\n- Analyzing: Once we’ve identified the issues, we’ll have an easier time finding a solution. In this stage, we’ll brainstorm several, even those that may not be practical. Do we provide ways to opt out or highlight weaknesses in data? Do we push for more clarity?\n- Reviewing: A list of solutions in hand, we may seek counsel from others to find what’s practical to implement, what hasn’t been considered, and what works best within our organization. We may need someone else to determine the action.\n- Deciding: This is where we put our solution into play. We also monitor it and adapt as necessary.\n- Data collection: When collecting personal data, do individuals know what I’m collecting, to what level of detail, and what their controls are? Can someone opt out or limit the detail?\n- Data governance: Consider who owns the data. If it’s public information, do I make it appropriately accessible while protecting identities enough? Am I transparent enough about the origins of this data and its limitations?\n- Data sharing: Consider biases and whether or not facts are being presented clearly. Are filters clearly identifiable and are wise choices avoiding perpetuating bias (such as color choices)? Is this visualization appropriately accessible to the intended audience? Can the audience trace back the data to understand what is and is not shown?\n- Data decision-making: Consider how you’re presenting the data. Are the limits of the data understood and does it fit the question? Is the presentation of it deceptive?\nThe practice of ethics helps practitioners step back and evaluate a situation from an ethical lens. The resulting action could be shaped by corporate values or a professional code of ethics. But above all, data ethics are designed to act as speed bumps in our work, so we understand how to face dilemmas both personally and professionally.\nLearn more about recent headlines on data ethics and the impact of these concerns on the modern business model during Bridget's Tableau Conference session, Data ethics | From headlines to headway. Join us in New Orleans and register for TC18 today."", 'Environmental ethics is a branch of philosophy that considers the moral relations between human beings and their natural environment. As a field of study, it assumes that humans have certain responsibilities to the natural world, and it seeks to help people and their leaders become aware of them and to act responsibly when they do things that impact the natural world.\nThe need for ethics\nMost people recognize that some agreed-upon guidelines or general rules should exist between individuals when they interact with one another because if they did not, nothing in our lives would be predictable or safe. In other words, people need to know that besides actual laws, there are some basic, common ethics or principles of what is right and what is wrong that everyone agrees upon and usually follows or lives by. Ethics is sometimes called moral philosophy because it is concerned with what is morally good and bad or what is right and wrong. As a specialized part of ethics, environmental ethics is concerned with the morality (right and wrong) of human actions as they affect the environment or the natural world we live in.\nGlobal environmental problems\nAs a branch of philosophy, environmental ethics is a fairly recent development, having become a body of organized knowledge only in the last decades of the twentieth century. It came about as a necessary response to a growing number of very obvious threats to the physical condition of the world in which we live. The list of some of these global environmental problems is a long and familiar one, and many of them came about because of the massive increase in the growth of the human population worldwide. As populations continue to soar, the various problems caused by too many people naturally increase in both their number and seriousness. It is predicted that the 2000 world population of six billion people will rise by another one billion people within ten years. To the many problems this causes, such as increased pollution of the air, water, and soil, is also added the depletion of these and other important natural resources.\nWords to Know\nAnthropomorphic: Described or thought of as having a human form or human attributes.\nDeep ecology: Philosophical belief system that holds that all forms of life—plant, animal, human—have an intrinsic right to exist in the natural environment and that humans have a direct responsibility to maintain the environment for all life forms.\nEthics: Branch of philosophy that deals with the general nature of morals and specific moral choices.\nShallow ecology: Philosophical belief system that holds that humans have a responsibility to protect the environment so it can support human life both in the present and in the future.\nToday, as we face such problems as the greenhouse effect, the destruction of the ozone layer, and the presence of toxic and nuclear wastes, we can easily recognize some of their negative effects. Among these are the growing disappearance of wilderness areas, a steady loss of biodiversity (the variety of species in an area) among living things, and even the actual extinction of some species. It is safe to say that at the beginning of the twenty-first century, one of the greatest challenges facing human beings is how to stop the continued harm to Earth.\nOrigins of environmental ethics\nMany people associate the beginnings of today\'s environmental ethics with the first Earth Day held on April 22, 1970, in the United States. On that day (and every April since), organizers around the country rallied and demonstrated to make people and political leaders aware of the importance of caring for and preserving the environment. That first Earth Day launched the beginning of an environmental awareness in the United States and later around the world. It made many people realize that some sense of environmental responsibility should be developed and applied to our daily lives.\nMost movements do not just suddenly happen out of nowhere; they are usually preceded by many other influential events. In the environmental movement, perhaps the earliest of these was the 1949 publication of a book by American naturalist Aldo Leopold (1887–1948). Leopold had fallen in love with nature as a youngster and eventually joined the newly established U.S. Forest Service in 1909. As a game management expert, he came to appreciate and understand how deeply humans affected the natural world. A year after he died, his landmark work, A Sand County Almanac, was published. It contained not only his strong defense of the environment but his argument that what was needed was a new philosophy about man and nature, or what would come to be called an environmental ethic. This idea was carried on by others when, two decades later, the first Earth Day was held.\nThe importance of that first Earth Day was that it not only raised the environmental consciousness or awareness of many people, but it got them to start asking important questions. Once people became aware that they had some sort of a responsibility toward the natural world, it then became a matter of trying to figure out how far that responsibility extends. This naturally led to many questions, such as, does Earth exist entirely for humanity? What are the rights of nonhuman species and do we have any obligations to them? Do we have a duty to be concerned with future generations? These and many other important questions are what environmental ethics is all about. While answering them may be difficult, and people may not always agree, it is significant that they are being asked and discussed.\nSchools of thought\nAnswers to these questions are shaped by what theory or school of thought of environmental ethics an individual believes in. One of these theories says that our responsibility to the natural environment is only an indirect one and is based on our responsibilities to other people. This school of thought is definitely human-centered or anthropomorphic (pronounced an-throw-poe-MOR-fick). While it argues that we have some sort of responsibility to the environment, it says that this responsibility is not a direct one and that the focus is on how the condition of the environment affects people, both in present and in future generations. In other words, we have a duty to make sure that Earth stays in good enough shape so that human life is supported. Some call this school of thought or philosophy ""shallow ecology.""\nA somewhat different school of thought is described as nonanthropomorphic, which means that all forms of life have an intrinsic (essential or basic) right to exist in the natural environment. This point of view gives what is called ""moral standing"" to animals and plants, and argues that they, like humans, are to be considered ""morally significant persons."" This philosophy is called ""deep ecology."" It states that humans have a direct responsibility toward maintaining the environment for all forms of life.\nThere are many versions of these two schools of thought—ranging from the argument that what is right or wrong environmentally should be judged only by how it affects people, to one that says the environment itself has direct rights. Few agree on how far our responsibility extends. Furthermore, the real disagreements are found when actual policies have to be decided upon that will guide how we act. Despite these and other disagreements, the fact that some sort of appreciation for nature has been fostered in many of us, and that we realize that nature must be appreciated and considered for its own sake and treated with respect, marks the beginning of a real ethics of the environment. For a very long time, human beings have never even been aware that they had any sort of responsibility toward the natural world and all its members. However, the development of some sort of environmental ethic that makes us consider if our environmental actions are right or wrong marks the beginning of future progress for a better world.']"	['<urn:uuid:771a74a7-c8e3-452b-87b6-d506ae88bb7f>', '<urn:uuid:462b41b3-5052-47b6-bdee-19a38d7ad268>']	factoid	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-12T11:40:26.317848	8	62	2377
74	I study biology - do nematodes and CRISPR both help fight diseases?	Yes, both nematodes and CRISPR are used to study and fight diseases. Nematodes like C. elegans are used to study mitochondrial mutations related to diseases like diabetes and Parkinson's. Meanwhile, CRISPR technology can be used for gene therapy to rescue disease-specific genetic mutations, as demonstrated in cases like sickle cell anemia.	['At Pacific University, all faculty teach a variety of different courses. Typically, we do not use graduate teaching assistants, which means that your classes will be taught by professors and that you will have plenty of opportunities to get to know the faculty in your discipline.\nBelow I have listed some of the courses that I teach. We are always developing and trying out new classes, so the list may change now and then.\nBIO 204 | General Biology II\nBIO 385 | Junior Seminar\nBIO 490 | Capstone Experience\nBIO 495 | Independent Biology Research\nPhD in Genetics, University of Wisconsin, Madison, Wisc., in 2010\nBachelor of Science in Biology, Western Washington University, Bellingham, Wash., in 2002\nMy main research interest is studying mitochondrial genetics using the model animal Caenorhabditis elegans (C. elegans). Mitochondria act as the “powerhouse” of the cell and create the majority of the energy needed for life in eukaryotes. Mitochondria contain their own genome separate from the nuclear genome. Mutations in this mitochondrial genome (mtDNA) are associated with normal aging, common diseases such as diabetes and Parkinsons, and with rare inherited mitochondrial diseases.\nLike humans, C. elegans accumulated mtDNA mutations with age and their mitochondria are very similar to human mitochondria in gene content. Previously, I created a large collection of C. elegans worm lines with large deletion mutations in their mtDNA. My research students are characterizing the effects of these mutations on nematode health and fitness.\nAs a second project, I am investigating local nematode biodiversity. Nematodes (aka “roundworms”) are an incredibly diverse group of animals with over 27,000 species identified and an estimated over 1,000,000 species worldwide. This provides an excellent opportunity for students to discover new species. As part of my Genetics class, I have students go “worm hunting” for nematodes in soil samples. Students isolate, culture and molecularly identify local soil nematodes.\nKA Clark, D Cheam*, J Seng*, K Gaffner* and DR Denver (in preparation) A targeted genetics approach in Caenorhabditis elegans reveals the role of mitochondrial single-stranded binding protein (mtss-1) in the formation of large mitochondrial DNA deletion mutations.\nKA Clark, DK Howe, K Gafner*, D Kusuma*, S Ping*, S Estes and DR Denver (2012) Selfish Little Circles: transmission bias and evolution of large deletion-bearing mitochondrial DNA in Caenorhabditis briggsae nematodes. PLoS One. 7:e41433. Featured in popular science news article at Science Daily\nDR Denver, MJ Raboin, and KA Clark. (2011) Review: Evolution of Reproductive Mode Transitions in Nematodes: insights from molecular phylogenies and newly discovered species. Mol Phylogenet and Evol 61:584-92.\nS Su, KA Clark, NM Gibbs*, SM Bush, and PJ Krysan. (2011) Ice-Cap: A Method for Growing Arabidopsis and Tomato Plants in 96-well Plates for High-Throughput Genotyping. doi: 10.3791/3280. J Vis Exp. Not Set.\nKA Clark and PJ Krysan. (2010) Chromosomal Translocations are a Common Phenomenon in Arabidopsis thaliana T-DNA Insertion Lines. Plant Journal, doi: 10.1111/j.1365-313X.2010.04386.x . Selected by Faculty of 1000 ()\nKA Clark and PJ Krysan. (2007) Protocol: An improved high-throughput method for generating tissue samples in 96-well format for plant genotyping (Ice-Cap 2.0). Plant Methods. 12:3-8.\nWR Robertson, K Clark, JC Young and MR Sussman. (2004) An Arabidopsis thaliana Plasma Membrane Proton Pump Is Essential for Pollen Development. Genetics. 168: 1677-87.', 'Guest post by Thomas Clements\nGene editing technology is continuously evolving. CRISPR-Cas9 mutagenesis hit the world by storm in 2013 and has since been extensively used as a tool to knockout specific genes of interest in vitro, in vivo, and recently even in humans. There have been many improvements to the system, but up until recently no one has described a mechanism to insert novel DNA into specific locations without creating potentially toxic double-strand breaks in DNA. Here, I describe two reports of RNA-guided DNA integration with CRISPR-associated transposases.\nThe Current Technology\nThe clustered, regularly interspaced short palindromic repeats (CRISPR)-CRISPR associated (Cas) endonuclease system is present in roughly 40% of sequenced bacteria and 90% of sequenced archaea (Antonova et al., 2018). Scientists have leveraged this natural tool to edit DNA. Editing is initiated by inducing novel double-strand breaks (DSBs) in the DNA itself through the use of the Cas9 endonuclease directed by a single-guide RNA (sgRNA).\nOnce a DSB break occurs, cells respond through two competing mechanisms: the dominant non-homologous end-joining (NHEJ) and homologous recombination (HR) (Huertas et al., 2008). This dominant NHEJ process is error-prone and leads to insertions and/or deletions (indels) of nucleotides at the break site, but only produces indels on the order 10 bps on average (Carrington et al., 2015). If scientists want to insert larger chunks of DNA at precise locations, they must rely on HR, which requires the exchange of similar or analogous DNA to DSBs. However, while CRISPR-based integration has had some success in vitro (Antonova et al., 2018), it has been increasingly problematic in vivo (Hruscha et al., 2013)with incorporation rates sometimes on the order of less than 1%.\nThere is a great need in molecular biology to insert specific large chunks of DNA in discrete locations. This technology could easily insert novel constructs, such as GFP, into gene-specific locations to visually measure gene activity, rescue disease specific genetic mutations through gene therapy, and even improve crop yield by allowing gene specific alterations to be visible in fewer generations among others. In order to find new ways to accomplish this, scientists have focused their attention back to how CRISPR works in the natural population. In bacteria and archaea, CRISPRs serve as an adaptive immune system. When a foreign nucleic acid invades a host cell and it is degraded, a small portion of it is incorporated into the CRISPR array in the form of a small spacer. This locus can then produce small RNAs that bind and guide Cas endonucleases to cleave previously degraded foreign nucleic acids (Wei et al., 2013).\nIn its natural role, CRISPR has two big functions, it can quickly target and degrade foreign DNA, but it also incorporates a memory of this foreign invasion into its own genome to prevent further attacks. In recent publications, scientists have increasingly focused on the two discrete functions: it not only knocks out (destroys via direct offensive attack) foreign viral invaders, but also incorporates a novel DNA spacer into its own genome to serve as a “memory” of this attack to prevent further viral attacks in the future by more quickly recognizing this foreign invader (a more defensive mechanism).There are two classes of CRISPR: class 1 and class 2. Within class 1, there are types I, III and IV. Within class 2, there are types II, V and VI. The three main types are type I, II and III, and they’re distinguished by unique Cas proteins. Proteins Cas 1, Cas 2, and Cas4 play key roles in spacer acquisition, whereas Cas proteins 3 and 5-10 play significant roles in target binding and cleavage (Hatoum-Aslan et al., 2013; Jiang et al., 2013). In summary, CRISPR is known to have a role in both targeting and transposition. In most publications, scientists have focused most of their attention CRISPR’s role in targeting via the type II protein Cas9 because of its simplicity (it’s the only protein involved in targeting binding).\nIn new papers out in Science and Nature, researchers shifted their focus to the Cas proteins involved spacer incorporation and their role in transposition (Klompe et al., 2019) and (Strecker et al., 2019) respectively.\nSo what exactly are transposons and how have scientists utilized them before? Transposons are mobile DNA elements that can jump locations within the genome and were first described by Barbara McClintock, which earned her a Nobel Prize in 1983. There are two types of transposons: retrotransposons which rely on reverse transcriptase for integration and DNA transposons, which encode for an enzyme called transposase that acts in a “cut and paste” mechanism to excise genetic elements and insert themselves in that place. They are selfish genetic elements in that they often enhance their own transcription even at the expense of organismal fitness. In research settings, many transposons are used to rescue loss of function phenotypes by expressing the fully functional gene. One specific example, (Luft, 2010) successfully used the transposon Sleeping Beauty to insert sequences into mice with sickle cell anemia so they can produce the enzymes need to counteract this disease. Other applications include gene traps among others. Here, transposons like Tol2 are used to mark genes with labels like GFP (Kawakami, 2007). Nonetheless, the use transposons are not specific and the DNA can be inserted with the genome in essentially random locations.\nIt is hypothesized that CRISPR-associated targeting complexes lead transposons to discrete DNA locations sites via a specific sgRNA. This hypothesis is based on the fact that Cas9 binding to DNA results in an R-loop structure (Figure 2) and transposons like Tn7 is suggested to have hijacked CRISPR effectors to generate specific R-loops. Thus, enhancing the spread of transposons via plasmids and phage infection (Kawakami, 2007).\nKlompe et al 2019 Nature\nIn order to test this hypothesis, (Klompe et al., 2019) used a transposon from Vibrio cholerae strain HE-45, Tn6677, which encodes a variant Type I-F CRISPR–Cas system. This system is known as Cascade and does not have the cas1 and cas2 genes responsible for spacer insertions as well as the cas3 gene responsible for target DNA degradation. Because of this, this system is designed to produce sgRNAs that target 48-50 base pairs downstream the target site. Cascade’s association with tniQ accomplishes integration. tniQ, which is a homolog of tnsD, specifies DNA integration sites and is a key component of the well-known transposon Tn7. All together, this allows for DNA integration in a replicative transposition mechanism with a 5 base pair duplication signature at both ends of the insertion at sites 48-50 base pairs downstream the sgRNA target site.Klompe and colleagues have termed this new tool “Insertion of transposable elements by guide RNA-assisted targeting” or INTEGRATE.\nStrecker et al 2019 Science\nIn similar fashion, (Strecker et al., 2019) studied the CRISPR-associated transposase (CAST) from S. hofmanni (ShCAST). This complex consists of Cas12k, which is a natural cas protein (also known as Cpf1) that can bind to but not cleave DNA (similar to dead Cas9). This is due to a natural mutation and its association with the tnsB/tnsC/tniQcomplex (tnsB is involved in excision as well as integration and tnsC is an ATPase). The researchers selected 48 targets in E. coli and detected novel insertions by PCR at 29 out of the 48 sites (60.4%). Here, ShCAST produces unidirectional insertions 60-66 base pairs downstream of the target and prevents repeated insertions into a single target site.\nThisbreakthrough is significant because the sgRNAs produced from Cascade and ShCAST in these mechanisms are easily manipulated and thus can lead site-specific insertions without creating the lethal DSB and without utilizing the rare HR pathway. In previous studies, transposons can be use to insert sequences into organisms, but the integration location was either completely random or restricted to a specific site. Now we can direct site-specific insertion of DNA segments to any location! This technology could easily insert novel constructs, such as GFP, into gene-specific locations to visually measure gene activity, even more efficiently rescue disease specific genetic mutations, and play key roles in gene therapy as well as crop enhancement. I’m curious as to the first individuals who publish the use of CRISPR-associated transposases in mice, zebrafish, and potentially even humans!\nThomas Clements, Ph.D. is a Senior Lecturer in the Biological Sciences Department at Vanderbilt University. He graduated with his Ph.D. in Biochemistry and Cell Biology from Rice University in May 2018. His research centers on improving CRISPR-Cas9 gene editing in zebrafish and is continuing this work in a Discovery-Based Laboratory Course at Vanderbilt. He is also a member of the Early Career Scientist Policy and the Education Committee at the Genetics Society of America. He is passionate about science literacy and inspiring the next generation of scientists to pursue careers that align with their passions.\nAntonova, E., Glazova, O., Gaponova, A., Eremyan, A., Zvereva, S., Grebenkina, N., Volkova, N., and Volchkov, P. (2018). Successful CRISPR/Cas9 mediated homologous recombination in a chicken cell line. F1000Research 7.\nCarrington, B., Varshney, G.K., Burgess, S.M., and Sood, R. (2015). CRISPR-STAT: an easy and reliable PCR-based method to evaluate target-specific sgRNA activity. Nucleic Acids Res. gkv802.\nHatoum-Aslan, A., Samai, P., Maniv, I., Jiang, W., and Marraffini, L.A. (2013). A ruler protein in a complex for antiviral defense determines the length of small interfering CRISPR RNAs. J. Biol. Chem. jbc.M113.499244.\nHruscha, A., Krawitz, P., Rechenberg, A., Heinrich, V., Hecht, J., Haass, C., and Schmid, B. (2013). Efficient CRISPR/Cas9 genome editing with low off-target effects in zebrafish. Dev. Camb. Engl.\nHuertas, P., Cortés-Ledesma, F., Sartori, A.A., Aguilera, A., and Jackson, S.P. (2008). CDK targets Sae2 to control DNA-end resection and homologous recombination. Nature 455, 689–692.\nJiang, W., Maniv, I., Arain, F., Wang, Y., Levin, B.R., and Marraffini, L.A. (2013). Dealing with the Evolutionary Downside of CRISPR Immunity: Bacteria and Beneficial Plasmids. PLoS Genet 9, e1003844.\nKawakami, K. (2007). Tol2: a versatile gene transfer vector in vertebrates. Genome Biol. 8, S7.\nKlompe, S.E., Vo, P.L.H., Halpin-Healy, T.S., and Sternberg, S.H. (2019). Transposon-encoded CRISPR–Cas systems direct RNA-guided DNA integration. Nature 1.\nLuft, F.C. (2010). Sleeping Beauty jumps to new heights. J. Mol. Med. 88, 641–643.\nPeters, J.E., Makarova, K.S., Shmakov, S., and Koonin, E.V. (2017).Recruitment of CRISPR-Cas systems by Tn7-like transposons. Proc. Natl. Acad. Sci. 114, E7358–E7366.\nStrecker, J., Ladha, A., Gardner, Z., Schmid-Burgk, J.L., Makarova, K.S., Koonin, E.V., and Zhang, F. (2019). RNA-guided DNA insertion with CRISPR-associated transposases. Science eaax9181.\nWei, C., Liu, J., Yu, Z., Zhang, B., Gao, G., and Jiao, R. (2013). TALEN or Cas9 – Rapid, Efficient and Specific Choices for Genome Modifications. J. Genet. Genomics 40, 281–289.']	['<urn:uuid:f65564a5-9560-4f2a-9c0a-9fb034f23918>', '<urn:uuid:ac9d9043-ef16-49fe-85d8-187e3d63e048>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T11:40:26.317848	12	51	2285
75	how prevent stormwater pollution stream buffers	Stormwater pollution can be prevented through several methods, particularly by maintaining stream buffers. Instead of mowing to the stream edge, planting native trees, shrubs, and plants creates riparian buffers that trap sediment and filter pollutants before they enter the water. The root systems hold sediment in place and reduce nutrient runoff. Additionally, stormwater management requires proper disposal practices - wash water should go into sanitary sewers (not storm drains), and pet waste should be disposed of in trash or toilets. These practices help prevent harmful pollutants from entering streams through stormwater runoff and protect water quality in the watershed.	"[""Lake Erie Starts Here\nThe Importance of Primary Headwater Streams and Ditches\nIn Summit County, there are two major rivers and drainage basins, the Cuyahoga and the Tuscarawas. The Cuyahoga starts out in Hambden Township near Burton in Geauga County, passes through Portage, Summit, and Cuyahoga Counties, and flows northward into Lake Erie. Lake Erie drains into the St. Lawrence Seaway and ends up in the Atlantic Ocean.\nPrimary headwater streams with vegetated buffers assist in reducing sediment delivery to larger streams. Instead of mowing to the edge of your stream or ditch, plant riparian buffer areas with native trees, shrubs, and herbaceous plants, which can keep a large amount of sediment from entering the stream, because of the root systems which hold the sediment in place and trap it. These healthy riparian areas help to reduce dredging costs, frequency of floods, water treatment costs, and siltation, or silt deposition in the larger streams. Riparian buffers also reduce the amount of nutrients entering the larger streams. The roots of the native vegetation trap and filter the nutrients and pollutants before they enter the stream. When the nutrients and pollutants are reduced, water quality, habitat, and recreational opportunities are all increased.\nNative vegetation planted on the stream banks and in the flood plain provides valuable food, habitat, migration corridors, and nesting sites for many species of wildlife. Biological diversity also creates increased opportunities for hunting and fishing. These healthy landscaping practices not only improve water quality, but also increase property values by improving aesthetics and curb appeal.\nNearly every person living in Northeast Ohio and Summit County has a primary headwater stream within a short distance of their home. Due to their small size, and nearness to human activities, these streams are often impaired. Because headwater streams are a key determinant in the overall condition of any river system, it is very important that all of us take on the responsibility and stewardship for keeping our headwater streams healthy and pollution-free. For more information on what you can do to protect Summit County headwater streams, contact the Summit Soil and Water Conservation District, at 330-926-2452, or go to www.summitswcd.org.\nIllicit Detection and Elimination:\nIf you see someone dumping anything into a storm drain please report it to the Engineering Dept for follow-up at 330-963-6247.\nWhat will we do:\n2) Clean-up if necessary as part of our illicit detection and elimination program\n3) Target an educational flyer in your neighborhood. Not all sewers are treated equal – the City maintains both a sanitary sewer system and a storm sewer system.\nExamples of Illicit Discharges:\nAre your neighbors picking up after “SNOOPY” and putting into the storm drain?\nWhat they should be doing is putting into the trash or flushing it down the toilet.\nWash water in the bucket after washing the car, is it being dumped on the driveway and flowing into the street?\nWhat should be happening is it should be dumped down the drain inside the garage/or wash basin sink. These drains are connected to the sanitary sewer and the water can be treated at the Wastewater Treatment Plant before it is released into Tinkers Creek.\nStormwater runoff is generated when precipitation from rain and melted snow flows over land or impervious surfaces and does not percolate into the ground. As the runoff flows over the land or impervious surfaces (paved streets, parking lots, and building rooftops), it accumulates debris, chemicals, sediment, or other pollutants that could adversely affect water quality if the runoff is discharged untreated. The primary method to control stormwater discharges is the use of best management practices (BMPs). In addition, most stormwater discharges are considered point sources and require coverage under a National Pollutant Discharge Elimination System (NPDES) permit.\nNational Pollutant Discharge Elimination System Stormwater Program\nThe NPDES Stormwater Program regulates stormwater discharges from 3 potential sources: municipal separate storm sewer systems (MS4s), construction activities, and industrial activities. Most stormwater discharges are considered point sources, and operators of these sources may be required to receive an NPDES permit before they can discharge. This permitting mechanism is designed to prevent stormwater runoff from washing harmful pollutants into local surface waters such as streams, rivers, lakes, or coastal waters.\nNational Pollutant Discharge Elimination System Phase 2\nThe federal government finalized regulations for stormwater management in smaller communities, and it is known as the National Pollutant Discharge Elimination System Phase 2 Rule. This rule is designed to comply with the requirements of the Clean Water Act and to further protect our nation's streams and rivers from polluted stormwater runoff. The NPDES Phase 2 program addresses 6 major areas:\nPublic Education and Public Outreach\nPublic Participation and Involvement\nIllicit Discharge Detection and Elimination\nConstruction Site Runoff control\nPost Construction Site Runoff control\nPollution Prevention / Good Housekeeping for Municipal Operations\nThe City of Twinsburg participates in the Summit County Stormwater Management Program and is permitted to discharge stormwater under NPDES Permit OH6000003 effective September 11, 2014 through September 10, 2019. The City and County are currently updating their SWMP plan to meet the new EPA permit issued for program years 2014 through 2019. A set of Best Management Practices (BMPs) or goals to address the requirements of the permit have been established, and the city is working to implement these BMPs.\nFlood Plain Administration\nFlood Plain Administration\nThe City Engineer, Amy Mohr, P.E., is the Flood Plain Administrator for the City of Twinsburg. Duties of the Flood Plain Administrator include review of development permits to determine if requirements of Chapter 1345.07 of the Codified Ordinances of Twinsburg (COT) have been satisfied, review of permits to assure intergovernmental review agencies have issued all required permits, and to determine if proposed development is located within a designated flood way.\nFlood Insurance Study\nThe Federal Emergency Management Agency (FEMA), under the U.S. Department of Homeland Security, coordinates the federal government's role in preparing for, preventing, mitigating the effects of, responding to, and recovering from all domestic disasters, whether natural or man-made, including acts of terrorism.\nThe Flood Insurance Study (FIS) and its accompanying Flood Insurance Rate Maps (FIRM) are prepared by FEMA to provide base flood information, delineate areas subject to significant flood hazards within the Summit County, and to offer the information for public officials to use when permitting development in the floodplain.\nThe FIS and FIRM are used not only by floodplain permit officials, but also by builders and developers, realtors, lenders, insurance agents, and the general public to determine flood risk, develop hazard mitigation measures, and to encourage wise and responsible risk management decision-making.\nThe maps were recently updated and have an effective date of April 19, 2016.\nFlooding Problem Diagnosis\nThere are 3 areas of focus when diagnosing a residential flooding problem:\nHouse - Keep gutters clean and in good repair. Foundation drains over time can become plugged with silt holding water around the foundation. Exterior waterproofing around the foundation and walls to include areas adjacent to landscape beds is important. Finally, grading around the home should be reviewed to make sure the area slopes away from the home.\nHouse Connection - Both foundation drains and downspouts connect into a storm sewer lateral that either connects to a storm main or daylights to a ditch or drainage way. The house connection over time can become plugged with debris or tree roots can infiltrate causing blockages. The current building code requires PVC pipe with glued joints be used. Historically, clay tile was used and this material is more apt to root intrusion. The City of Twinsburg Wastewater Department can help troubleshoot sewer lateral condition provided the clean out can be located.\nStorm Sewer / Sanitary Sewer - The city maintains the sewer infrastructure. Sewer maintenance cleans sanitary sewers throughout the city. We also have capabilities to videotape the sewers. An annual manhole restoration contract is awarded to keep the manholes in good repair. Additionally, improvement projects to replace / repair / line sewers throughout the city are prioritized."", 'Home / Blogs / Streamside Forest Buffers are Important for Water and Wildlife\nMay 9, 2017\nOver one third of the land in the Chesapeake Bay watershed is either covered by development or agriculture. This poses obstacles to water quality in the form of nutrients, sediments, and other pollutants, but also to terrestrial wildlife that have little or no habitat in these settings. Trees planted along city streets and in suburban backyards may feel like a sort of coexistence with nature, but in reality these manicured settings provide habitat for very few native species. Farmland can also be deceptively “natural”; despite expansive hayfields and lush row crops, there are few places for wildlife to nest, raise young, or eat. Waist-high hay and six-foot-tall corn are eventually cut, leaving no cover for all but the smallest animals. Luckily, both water quality and wildlife habitat issues can be addressed with one management practice: buffering streams and water bodies with forest cover.\nA four year old riparian forest buffer in Huntingdon County, PA. Without this buffer, nutrients and sediments from the adjacent pastures would freely enter the stream and degrade water quality from here all the way to the Chesapeake Bay. Many wildlife species would not be able to breed or survive without the buffer because there is also no other source of year-round herbaceous or shrubby cover on the landscape.\nHealthy streams and watersheds rely on functional riparian forest corridors. A streamside forest will trap and filter nutrients and sediment from the uplands that would otherwise flow into the stream, and the overhanging tree canopy will cool down the water to make it suitable for trout and other native aquatic fauna. They can also be important for terrestrial wildlife, especially in landscapes dominated by agriculture or development. Stream corridors that hold trees, dense shrubs/saplings, and native herbaceous vegetation provide breeding, foraging, and escape cover for an array of upland and lowland wildlife species that would other have little to eat and no shelter from predators or the elements.\nIf you have existing forest cover around a stream, protecting and restoring it will be best for the water and wildlife. The width of buffer needed to improve water quality depends on the quality of the buffer and the adjacent land use, but 35 to 50 feet should be sufficient in most cases. A 50 foot buffer on either side of a stream will also be beneficial to many wildlife species such as deer, rabbits, woodcock, and pheasants, but many area-sensitive songbirds and other species require at least 150 feet of forest cover on each side.\nA mature buffer in Bedford County, PA. This buffer is protecting the water from pollutants, slowing and cooling the water to support habitat for aquatic fauna, and providing habitat for many species of wildlife that would otherwise have nowhere else to breed or escape from predators.\nIn areas currently devoid of canopy cover, planting trees and shrubs may be necessary. Natural regeneration can successfully restore riparian corridors in some situations, but in predominately non-forested landscapes there are often either not enough seed sources or too much pressure from invasive vegetation that chokes out beneficial native species. Improvements in water quality require canopy cover to reduce water temperature and perennial vegetative cover to filter nutrients and sediment. Buffers can be improved for wildlife by planting diverse overstory species that provide food (oaks, blackgum, tulip-poplar, black cherry) and shrubs that provide dense cover and food to improve winter survival (alders, silky dogwood, red-osier dogwood, viburnums, ilexes). Additionally, consider pollinators when planting riparian buffers; in heavily developed and farmed landscapes these forests are often their only habitat as well. Pollinators require undisturbed soil and brush for nesting and nectar sources over the duration of the growing season. Consider the blooming period of trees and shrubs and try to ensure that there will be something flowering over as much of the year as possible, and avoid disturbing the herbaceous layer to encourage growth of native wildflowers such as ironweed, Joe-pye weed, and goldenrod.\nEstablishing a buffer by planting can be challenging, but there are many online resources and technical guidance to assist interested landowners. Visit forestsforthebay.org to see what assistance is available in your state or email me at email@example.com if you are interested in restoring streamside forests on your property.\nSenior Forests Projects Manager\n(717) 517 8698\nNews Staff Blog']"	['<urn:uuid:b47a4021-da0b-4ab0-a2e4-4e0b9971557b>', '<urn:uuid:79600925-bfc6-4625-9d8e-eaf61cccbf33>']	open-ended	with-premise	short-search-query	similar-to-document	three-doc	novice	2025-05-12T11:40:26.317848	6	99	2057
76	distillery water filtering method purpose	Distilleries use reverse-osmosis filters to remove any trace of contamination from water and prevent mineral precipitation that could leave white dust at the bottom of spirit bottles, ensuring the water is pure H2O with no additional elements.	"[""Water is an essential ingredient in any spirit, but that doesn't mean a special water source makes a better spirit.\nMost of the liquid in a bottle of gin or vodka is water. The bottle even tells you that with a clearly visible ABV statement. A bottle of supermarket gin at 37.5% is 62.5% water, the brand-name vodka at 40% is 60% water. Even bottled generously at 46%, my craft gin and vodka are still 54% water by volume. With water clearly being the number one ingredient in any bottle of gin, it goes without saying that it must also be the most important. And it is, but not in the way you might think.\nVodka fresh out of the still is typically 96% ABV, or high enough to shut your liver off like a light switch if you were to drink it. Gin is made from that same vodka spirit, but redistilled with botanicals and typically comes out of the still at 60-80% ABV, still much too high for drinking neat. The distiller adds water to reduce the ABV to bottling strength, typically 37.5% for cheap mass-market gin but 41-48% for high-quality craft gin. Here's a hint: if you see a bottle of gin at 40% or below, you know that profit won out over taste in that brand.\nMany distilleries make a big point of telling you exactly where their water is sourced. It might make the gin sound extra special, but that's purely marketing fluff. You've no doubt seen this type of claims about distillery water:\n- We use water from a 200-year-old aquifer in the dolomite hills\n- Natural spring water from the valleys of Happyshire\n- Water filtered naturally through ancient peat beds\n- Water made from the tears of hand-flogged Tibetan yaks\nThe claims about magic water sources can get out of hand at times. While water is an essential ingredient in any spirit, it's actually more important that the water be absolutely unremarkable in every way. Here's why.\nEvery distillery filters its water before using it in their spirits. This is legally required by every environmental health department in the UK and in fact most of the world. Even if the source of distillery water is truly exceptional, by the time it gets mixed into the spirit and put in a bottle, it has been through an intense filter to remove any trace of contamination. This is normally done using a reverse-osmosis filter that effectively removes everything that isn't water. What is left is very pure, but very bland. H20 or dihydrogen monoxide, call it what you like, it's just plain water. Nothing else.\nA second reason distilleries filter their water is cosmetic. If water used in gin has residual minerals in it, those will precipitate out of the spirit over time, leaving a fine layer of white dust at the bottom of your bottle of gin. There's no risk from that, but it can be visually off-putting to say the least. For these simple reasons, every bit of water in a bottle of spirits needs to be purified before use. It's just plain water. It's not special. If the water were truly special, it couldn't be used for spirits. On a scale of 0 to 100, the influence of water on the final taste of a gin is just above zero. The water should have no influence on the spirit other than to reduce the strength to drinkable levels.\nThere is one exception, which is the water used to prepare the initial brew for distilling. When extracting starches and sugars from grains like barley or wheat, the chemistry of the water makes a huge difference to the efficiency of the process. Beer brewers take tremendous care to get exactly the right mix of minerals and acidity for their brewing water. For beer, the type of brewing water used will effect the final flavour, but once distilled all of these minerals and salts are left behind in the still. A spirit produced from fermented grain is purely the alcohol created by the yeast and little or no remnant of the original brewing water remains. Water added later in the process is always purified and filtered.\nSo where do I get the water for my unique Shropshire spirits? My water comes from a series of ancient natural sandstone aquifers located between the Rivers Severn and Trent. That's a fancy way of saying I use tap water processed through a four-stage reverse osmosis filter. It's pure H2O and nothing else, entirely unremarkable but perfect for adding to spirits.\nSo how important is the quality of water to the quality of a gin or vodka? It's absolutely essential, but it's important that the water not be special in any way.\nIt's just water.""]"	['<urn:uuid:db926de5-b026-4962-a626-cd2aa608da5e>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T11:40:26.317848	5	37	797
77	looking for good fuel properties what makes fuel good to use	A good fuel should meet these criteria: it should be easily available, it should not produce too much smoke, and on burning it should release less amount of heat.	['Sources of Energy – CBSE Class 10 Science – NCERT Solutions\nWhat is a good source of energy?\nA good source of energy would be one,\ni) Which would do a large amount of work per unit volume or mass.\nii) Be easily accessible.\niii) Be easy to store and transport, and\niv) Perhaps most importantly, be economical.\nWhat is a good fuel?\nA good fuel would be one,\ni) Which is easily available.\nii) It should not produce too much of smoke.\niii) On burning should release less amount of heat.\nIf you could use any source of energy for heating your food, which one would you use and why?\nSolar energy can be used for heating food because it is easily available, it will not produce smoke and it will not release any amount of heat.\nWhat are the disadvantages of fossil fuels?\nFossil fuels are non-renewable. Burning of coal or petroleum products causes the air pollution. The oxides of carbon, nitrogen and sulphur that are released on burning fossil fuels are acid oxides. These lead to acid rain, which affects water and soil resources.\nWhy are we looking at alternate sources of energy?\nThe fossil fuels are non-renewable sources of energy. So we need to conserve them. If we were to continue consuming these sources at such alarming rates, we would soon run out of energy. In order to avoid this, alternate sources of energy were explored.\nHow has the traditional use of wind and water energy been modified for our convenience?\nThe wind possesses kinetic energy. This energy was harnessed by windmills in the past to do mechanical work. Today, wind energy is also used to generate electricity.\nAnother traditional source of energy was the kinetic energy of flowing water or the potential energy of water at a height. Hydropower plants convert the potential energy of falling water into electricity.\nWhat kind of mirror – concave, convex or plane – would be best suited for use in a solar cooker? Why?\nPlane mirror would be best suited for use in a solar cooker. A plane mirror is used as a reflector. The reflector is used to increase the area over which the solar energy is collected so that more and more heat rays of the sun may enter the solar cooker.\nWhat are the limitations of the energy that can be obtained from the oceans?\nThe energy from the oceans can be obtained mainly in three forms,\na) Tidal energy\nb) Ocean waves energy\nc) Ocean thermal energy\nThe energy potential from sea is quite large, but efficient commercial exploitation is difficult.\nWhat is geothermal energy?\n‘Geo’ means ‘earth’ and ‘thermal’ means ‘heat’. Thus the geothermal energy is the heat energy from the hot rock present inside the earth. This heat can be used as a source of energy to produce electricity.\nWhat are the advantages of nuclear energy?\nThe advantages of nuclear energy is as follows,\n(a) It generates electricity.\n(b) Disease like cancer can be treated.\n(c) It helps for the improvement in the agriculture and industry.\nCan any source of energy be pollution-free? Why or why not?\nYes, Solar energy does not cause any pollution. Solar cells make use of the ‘everlasting solar energy’ and their use does not produce any environmental pollution.\nHydrogen has been used as a rocket fuel. Would you consider it a cleaner fuel than CNG? Why or why not?\nYes, hydrogen is a cleaner fuel than CNG because of its very high colorific value, hydrogen is an extremely good fuel.\nName two energy sources that you would consider to be renewable. Give reasons for your choices.\nHydro Energy and Solar Energy\nHydro energy or water energy is renewable source of electric energy, which will never get exhausted, since water is available in plenty.\nSolar energy is also known as light energy, which is obtained from the sun and it will never get exhausted.\nGive the names of two energy sources that you would consider to be exhaustible. Give reasons for your choices.\nCoal and petroleum are the two energy sources that are considered to be exhaustible. They are non-renewable sources of energy and are present in a limited amount in the earth. Once exhausted, they will not be available to us again.\nA solar water heater can be used to get hot water on\n(a) a sunny day.\n(b) a cloudy day.\n(c) a hot day.\n(d) a windy day.\n(a) a sunny day.\nWhich of the following is not an example of a bio-mass energy source?\n(c) nuclear energy\n(c) nuclear energy.\nMost of the sources of energy we use represent stored solar energy.\nWhich of the following is not ultimately derived from the Sun’s energy?\n(a) geothermal energy\n(b) wind energy\n(c) nuclear energy\n(c) nuclear energy.\nCompare and contrast fossil fuels and the Sun as direct sources of energy.\nFossil fuels are non-renewable sources of energy. These non-renewable sources of energy (like coal, petroleum, natural gas) are present in a limited amount in the earth. Once exhausted, they will not be available to us again.\nThe sun is the source of all energy. The sun is a renewable source of energy, provides us heat and light energy free of cost. The energy obtained from the sun is called solar energy. The energy coming from the sun contains heat rays, visible light, ultra-violet rays and some gamma rays.\nCompare and contrast bio-mass and hydro electricity as sources of energy.\nThe waste material of living things and the dead parts of living things is called bio-mass. Bio-mass contains carbon compounds and it is the oldest source of heat energy for domestic purposes. The important examples of bio-mass being used as a fuel are wood, cattle dung and agriculture wastes like bagasse.\nHydropower plants convert the potential energy of falling water into electricity. Water energy is a renewable source of electric energy, which will never get exhausted. The construction of dams on rivers helps in controlling floods and in irrigation.\nWhat are the limitations of extracting energy from\n(a) the wind?\na) There are many limitations in harnessing wind energy. Wind energy farms can be established only at those places where wind blows from the greater part of a year. The wind speed should also be higher than 15 Km/h to maintain the required speed of the turbine. There should be some back-up facilities to take care of the energy needs during a period when there is no wind.\nb) The waves are generated by strong winds blowing across the sea. Wave energy would be a viable proposition only where waves are very strong.\nc) Tidal energy is harnessed by constructing a dam across a narrow opening, the location where such dams can be built are limited.\nOn what basis would you classify energy sources as\n(a) renewable and non-renewable?\n(b) exhaustible and inexhaustible?\nAre the options given in (a) and (b) the same?\nThe options given in (a) and (b) are the same.\nThose sources of energy, which are being, produced continuously in nature and are inexhaustible are called renewable sources of energy.\nThose sources of energy, which have accumulated in nature over a very, very long time and cannot be quickly replaced when exhausted are called non-renewable sources of energy.\nWhat are the qualities of an ideal source of energy?\nThe important qualities of an ideal source of energy is\na) It should be a renewable source of energy.\nb) It should be pollution-free.\nc) It should be economical.\nd) It should be easily accessible.\nWhat are the advantages and disadvantages of using a solar cooker? Are there places where solar cookers would have limited utility?\nThe advantages of a solar cooker\ni) The use of solar cooker for cooking food saves fuel.\nii) The use of solar cooker does not produce smoke due to which the environment also does not get polluted.\niii) When food is cooked in a solar cooker, its nutrients do not get destroyed. This is because in a solar cooker, food is cooked at comparatively lower temperature.\niv) In a solar cooker, upto four food items can be cooked at the same time.\nThe disadvantages of a solar cooker\ni) The box-type solar cooker cannot be used to make chappaties.\nii) The box-type solar cooker cannot be used for ‘frying’.\nThe limited utility of a solar cooker is\ni)The solar cooker cannot be used to cook the food during nighttime.\nii) If the day-sky is covered with clouds, even then the solar cooker cannot be used to cook the food.\niii) The direction of reflector of solar cooker has to be changed from time to time to keep it facing the sun.\nWhat are the environmental consequences of the increasing demand for energy? What steps would you suggest to reduce energy consumption?\nExploiting any source of energy disturbs the environment in some way or the other. The source we would choose depends on factors such as the case of extracting energy from that source, the economics of extracting energy from the source, the efficiency of technology available and the environmental damage that will be caused by using that source.\nWe cannot depend on the fossil fuels for much longer, if we manage bio-mass by replacing the trees we cut down for fire-wood, we can be assured of a constant supply of energy at a particular rate. Renewable energy is available in our natural environment, in the form of some continuing or repetitive current of energy, or is stored in such large under ground reservoirs that the rate of depletion of reservoirs because of extraction of usable energy is practically negligible.']	['<urn:uuid:21098238-b988-470a-9464-3c31c860a69a>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T11:40:26.317848	11	29	1609
78	propofol infusion syndrome diagnostic criteria pediatric patients	PRIS is defined as metabolic acidosis (arterial pH ≤7.3 with serum bicarbonate ≤18 mg/dL) plus at least one sign from these categories: rhabdomyolysis, hypotension, hepatic transaminitis, hypertriglyceridemia, hypoxia, hyperthermia, cardiac dysfunction, or renal failure.	"[""Prospero propofol registration .pdf\nOriginal filename: Prospero propofol registration.pdf\nTitle: International prospective register of systematic reviews\nThis PDF 1.7 document has been generated by TCPDF / TCPDF 5.9.189 (http://www.tcpdf.org), and has been sent on pdf-archive.com on 20/01/2015 at 19:25, from IP address 96.52.x.x.\nThe current document download page has been viewed 617 times.\nFile size: 128 KB (6 pages).\nPrivacy: public file\nDownload original PDF file\nProspero propofol registration.pdf (PDF, 128 KB)\nShare on social networks\nLink to this file download page\nPROSPERO International prospective register of systematic reviews\nSystematic review of adverse events of propofol infusion in pediatric patients\nLiliane Zorzela, Sunita Vohra, Ari Joffe, Lisa Hartling, Yoon Loke, Salima Punja, Katherine Pohlman\nLiliane Zorzela, Sunita Vohra, Ari Joffe, Lisa Hartling, Yoon Loke, Salima Punja, Katherine Pohlman. Systematic\nreview of adverse events of propofol infusion in pediatric patients. PROSPERO 2013:CRD42013005397 Available\nQuestion: Is propofol infusion assocaited with serious adverse events in pediatric patients?\nObjectives: 1. Identify the incidence of serious adverse events associated with propofol infusion in pediatric patients.\n2. To identify the propofol dose and duration of infusion associated with serious adverse events (PRIS and cardiac\n3. To describe the patient characteristics associated with serious adverse events during the use of propofol infusion\n4. To identify a possible association between serious adverse events related to propofol use and the patient care\nsetting (pediatric intensive care, operating room, paediatric emergency or other)\nThe searches will be done in 3 main databases (MEDLINE, EMBASE and CENTRAL). We will search the websites\nof regulatory pharmaceutical (1) Current Problems in Pharmacology (www.mhra.gov.uk); (2) Australian Adverse\nDrug Reaction Bulletin (www.tga.gov.au/adr/aadrb.htm); (3)European Public Assessment Reports fromthe European\nMedicine Evaluation Agency\n(www.emca.eu) (4) Food and Drug Administration FDA Medwatch (www.fda.gov/medwatch).\nWe will screen references of all retrieved articles to identify additional publications. No conference or meeting\nabstracts will be searched.\nNo language restrictions and no publication period restrictions will be applied.\nTypes of study to be included\nWe will include randomized or quasi-randomized controlled trials of parallel group or cross-over design, which use\nindividual or cluster randomization. In the case of cross-over trials, we will include only the first arm (we will\nconsider events that happened after the cross-over in a sensitivity analysis).\nCondition or domain being studied\nPropofol is an anaesthetic agent with short onset of action and short half life. These two characteristics make the drug\nclinically useful. However, there have been case reports of an association between propofol used in prolonged\ninfusions and in high dosages with metabolic acidosis, liver dysfunction, arrhythmias and death. This was called\nPropofol infusion syndrome (PRIS). Regulatory bodies, such as FDA and Health Canada contraindicate the use of\npropofol as a continuous infusion in critically ill paediatric patients, mostly based on case reports but propofol is\nfreely used in paediatric emergency rooms and operating rooms. We understand that PRIS in not exclusive to the\npaediatric population but, due to limitations of propofol use in critically ill\nchildren and no restriction of its use in other paediatric populations, we believe it is important to systematically\nreview the use of propofol infusion in children in an attempt to identify risk factors and address its safety concerns.\nPage: 1 / 6\nThe target population consists of paediatric patients (age ranging from 28 days to 19 years) receiving sedation using\npropofol for more than 60 minutes in a hospital (intensive care unit, operating room, emergency room, or any other\nlocation within the hospital) or other medical setting (for example dentist offices). The indication for sedation will not\nbe a restriction at this time; instead,\nit will be the subject of a subgroup analysis. This is a review of adverse events of propofol infusion in paediatric\npatients, so it has broad inclusion criteria (any children receiving propofol infusion for sedation).\nThe intervention being studied is the use of propofol as an anaesthetic drug given by continuous infusion. It is not\nclear the length of infusion necessary to cause PRIS. The case reports are usually associated with prolonged use\n(more than 48 hours) and in high doses (more than 4 mg/kg/hour). We will include patients receiving infusions lasting\n60 minutes or longer, as an arbitrary number.\nFurther, we will exclude studies in which propofol bolus or infusion was given in more than one study arm, in order\nto have a propofol free comparison group and to limit the possible risk factor to one single arm. As propofol does not\nprovide analgesia, the group receiving propofol can also receive other sedative, analgesic drugs concomitantly.\nThe comparison group, will be any sedative or analgesic agent different from propofol, alone or in combination.\nThe primary outcome is the report of PRIS or any other serious adverse events resulting in cardiac arrest (with return\nto spontaneous circulation or use of extra-corporeal life support) or death associated with propofol use as a\ncontinuous infusion in paediatric patients. The follow-up time to measure the primary outcome for this study will be\nthe development of the outcome of interest (PRIS or cardiac arrest) or time of hospital discharge for patients who did\nnot develop the primary outcome.\nThe primary outcomes will be defined as the following:\nCardiac arrest, a reduction of cardiac output requiring any of the following: chest compressions; defibrillation,\nepinephrine boluses or cardiac mechanical support (extra-corporeal life support (ECLS); or ventricular assist device\nPropofol infusion syndrome (PRIS) will be defined as metabolic acidosis, arterial pH 7.3 along with a serum\nbicarbonate 18 mg/dL; plus the presence of any signs in any the below categories (adapted from Roberts 2009):\n1. rhabdomyolysis (the breakdown of muscle fibres resulting in the release of muscle fibre contents (myoglobin) into\nthe bloodstream), defined as creatine phosphokinase (CPK) 10,000 IU/L or positive serum or urine myoglobin test\nor positive urinary casts for haemoglobin;\n2. hypotension (initiation of a vasopressor agent or increase of 20% from baseline;\n3. hepatic transaminitis (increase in aspartate aminotransferase or alanine aminotransferase, or both, 3 times above\n4. hypertriglyceridaemia (serum triglyceride concentration 400 mg/dL);\n5. hypoxia (partial pressure of arterial oxygen 60 mm Hg);\n6. hyperthermia (temperature 38.3 °C);\n7. cardiac dysfunction that includes asystole, pulseless electrical activity, ventricular fibrillation, sustained\nventriculartachycardia of 30 seconds or longer, myocardial failure (ejection fraction 40%), or bradycardia (heart rate\nPage: 2 / 6\n8. renal failure that includes oliguria (urine output 0.5 mL/kg/hr for 6 hours), anuria (urine output 10 mL/hr for\n6 hours), elevation in serum creatinine increase of 1 mg/dL from baseline), or hyperkalaemia (serum K+ 6\n(excluding other known causes or haemolyzed specimens).\nWe will also capture events that do not clearly fulfil the above inclusion criteria. These events will be reported as an\nunclear outcome definition and will be the subject of a sensitivity analysis.\nData extraction, (selection and coding)\nAll studies meeting the following criteria will be included.\n1. Paediatric population: age ranging from 28 days to 19 years, exclusively;\n2. Use of propofol as a continuous infusion for more than 60 minutes.\n3. The comparison group will be any other sedative or analgesic agents (alone or in combination) different from\nRisk of bias (quality) assessment\nTwo review authors (LZ and SP) will independently assess the methodological quality of each trial using the\nCochrane ’Risk of bias’ tool (Higgins 2011). We will assess the following sources of bias for all study designs.\n1. Selection bias, including randomization: describes the method used to generate the allocation sequence in sufficient\ndetail to allow an assessment of whether it should produce comparable groups. Describes the method used to conceal\nthe allocation sequence in sufficient detail to determine whether\nintervention allocations could have been foreseen in advance of, or during, enrolment.\n2. Performance bias: describes all measures used, if any, to blind study participants and personnel from knowledge of\nwhich intervention a participant received. Provides any information relating to whether the intended blinding was\n3. Detection bias: describes all measures used, if any, to blind outcome assessors from knowledge of which\nintervention a participant received. Provides any information related to whether the intended blinding was effective.\n4. Attrition bias: describes the completeness of outcome data for each main outcome, including attrition and\nexclusions from the analysis. States whether attrition and exclusions were reported, the numbers in each intervention\ngroup (compared with total randomized participants), reasons for attrition or\nexclusions where reported, and any re-inclusions in analyses performed by the review authors.\n5. Selective reporting bias: states how the possibility of selective outcome reporting was examined by the review\nauthors, and what was found. We are aware that the adverse events related to a therapy are poorly reported in clinical\n6. Other source of bias: states any important concerns about bias not addressed in the other domains in the tool.\nIf particular questions or entries were pre-specified in the study’s protocol, responses should be provided for each\nquestion or entry.\nTwo authors will assess the risk of bias of each trial, following the domain-based evaluation as described in the\nPage: 3 / 6\nCochrane Handbook for Systematic Reviews of Interventions (Higgins 2011). The authors will assess the domains for\nrisk of bias; in the first three domains as ’Low risk of bias’; ’High risk of bias’ and ’Unclear’, which means there is an\nuncertain risk of bias.\nStrategy for data synthesis\nThe main comparison will be propofol infusion and control. In this review, the control group will be called standard\ncare and it will include the most common sedative agents used for paediatric sedation.\nAll analyses will be performed using the ReviewManager software. For rare events such as PRIS, we will use the\nPeto one-step odds ratio method. It is the least biased and most powerful method and provides the best confidence\ninterval coverage provided there is no substantial imbalance between treatment and control group sizes within studies,\nand treatment effects are not exceptionally large (Higgins 2011).\nWe will pool data from studies that are sufficiently homogenous and with the same study design in order to perform a\nAnalysis of subgroups or subsets\nWe will perform subgroups for the primary outcome (PRIS and cardiac arrest) based on the following:\n1. Comparison group: if two or more trials are found using the same comparison group (for example ketamine), these\nstudies will be subgrouped in an attempt to identify any risk increase or reduction for developing the primary\n2. Dosage of propofol: we will subgroup studies using propofol infusion at equal to or less than 4mg/kg/h or more\nthan 4mg/kg/h as this seems to be the dosage cut-off for reporting PRIS in children, but it is not clear what dose of\npropofol is necessary to cause PRIS. This subgroup analysis will be done as\nan attempt to measure a dose-effect relationship between propofol and the development of PRIS.\n3. Duration of propofol infusion: we will subgroup studies with propofol infusion lasting less than or equal to 12\nhours, between 12 and 24 hours and more than 24 hours. This subgroup analysis was chosen as the duration of\npropofol required to cause PRIS is unclear. It will be done in an attempt to determine if there is any relationship\nbetween duration of infusion of propofol and the development of PRIS.\n4. Indication for sedation: if enough trials are found under similar settings, for example sedation for mechanical\nventilation in intensive care unit or sedation procedures in the operating room, these studies will be subgrouped in an\nattempt to identify patient setting (location within the healthcare facility) and\nindication in an association with the primary outcome.\nPublication in a pediatric critical care journal, for example Pediatric Critical Care or Pediatric Anesthesia.\nContact details for further information\n8727-118 Street, Edmonton, Alberta. Canada. T6G1T4\nOrganisational affiliation of the review\nPage: 4 / 6\nDr Liliane Zorzela, University of Alberta\nDr Sunita Vohra, University of Alberta\nDr Ari Joffe, University of Alberta\nDr Lisa Hartling, University of Alberta\nDr Yoon Loke, University of East Anglia\nMiss Salima Punja, University of Alberta\nMs Katherine Pohlman, University of Alberta\nDetails of any existing review of the same topic by the same authors\nProtocol has been published for a Cochrane review, although differently to this one, the cochrane review includes\nAnticipated or actual start date\n15 August 2013\nAnticipated completion date\n15 January 2014\nThis systematic review is part of Dr Liliane Zorzela's PhD thesis, Dep Pediatric, University of Alberta. Dr Sunita\nVohra is her PhD supervisor and Dr Ari Joffe, Dr Lisa Hartling and Dr Yoon Loke are Liliane's thesis advisory\ncommittee. There are no direct funders for this review.\nConflicts of interest\nSubject index terms status\nSubject indexing assigned by CRD\nSubject index terms\nAdolescent; Anesthetics, Intravenous; Child; Death; Heart Arrest; Humans; Infant; Propofol;\nAny other information\nIt is part of a PhD Thesis, to compare reviews using different study designs will provide different assessment of\nStage of review\nDate of registration in PROSPERO\n16 August 2013\nDate of publication of this revision\n16 August 2013\nStage of review at time of this submission\nPiloting of the study selection process\nPage: 5 / 6\nFormal screening of search results against eligibility criteria\nRisk of bias (quality) assessment\nInternational prospective register of systematic reviews\nThe information in this record has been provided by the named contact for this review. CRD has accepted this information in good\nfaith and registered the review in PROSPERO. CRD bears no responsibility or liability for the content of this registration record,\nany associated files or external websites.\nPage: 6 / 6\nPowered by TCPDF (www.tcpdf.org)\nLink to this page\nUse the permanent link to the download page to share your document on Facebook, Twitter, LinkedIn, or directly with a contact by e-Mail, Messenger, Whatsapp, Line..\nUse the short link to share your document on Twitter or by text message (SMS)\nCopy the following HTML code to share your document on a Website or Blog""]"	['<urn:uuid:1df583dd-b4f2-4d5f-ae9e-9e2b98e49856>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T11:40:26.317848	7	34	2297
79	diabetic foot problems causes prevention tips	Diabetes affects feet in two main ways: by damaging nerves (reducing sensation and ability to detect pain) and reducing circulation (impacting blood flow and healing). This puts diabetic patients at risk of unnoticed injuries and slow-healing wounds. Prevention includes daily foot inspections, maintaining healthy blood sugar levels, and getting annual podiatrist check-ups. During screenings, podiatrists assess sensation, circulation, and footwear while providing management strategies to reduce complications. Professional foot care services, including wound care and custom orthotics, can help prevent serious complications like ulcers and Charcot foot.	['A full range of podiatry services to help restore your health, wellbeing and mobility.\nNail & Skin Care\nWe provide comprehensive skin care and nail care services to help you feel comfortable and confident on your feet. As the foundations of our body, any pain caused by problems with the skin or nails can affect our ability to move and complete everyday tasks. Our experienced podiatrists can help with:\n- Corns and calluses\n- Dry, cracked heels\n- Warts (verrucae)\n- Ingrown toenails\n- Thick, long toenails\n- Crumbly, flaky toenails\n- Fungal nail infections\n- Athlete’s foot\n- Wound care\nAs qualified and board-registered Podiatrists, we employ strict hygiene and sterilisation protocols at every appointment, so you can be confident that your foot health is our priority.\nDiabetic Foot Care\nDiabetic foot disease is a significant problem here in Australia, affecting approximately 1.7 million people. It causes changes to your feet and legs by affecting your:\n- Sensation (your ability to feel and detect pain)\n- Circulation (the blood flow to the feet, and hence your healing response)\nWhen you have difficulty feeling what is happening around your feet and legs, you may be unable to detect and care for any new cuts and wounds. This leaves the wound vulnerable to infection. Simultaneously, impaired blood flow to the feet means the body takes longer to close and heal the wound, and it may be harder to fight infections that develop.\nAs the effects of diabetes on your feet progressively worsen over time, annual diabetic foot health screenings are recommended by your podiatrist. During these screenings, we will:\n- Assess your current sensation level\n- Assess your current circulation level\n- Assess your footwear\n- Discuss your current risks\n- Discuss management strategies to help reduce the likelihood of complications\nChildren’s Leg & Foot Problems\nBeing able to stay active and pain-free on their feet means that kids can explore, adventure, play and learn without being held back. We love using our knowledge of the feet and legs to help kids get back to doing the things they love. We can help kids with a range of problems, including:\n- In-toeing and out-toeing\n- Growing pains (in the heels, knees and feet)\n- Flat feet or other alignment problems\n- Leg + knee pain\n- Foot + ankle pain\n- Toe-walking + other unusual walking patterns\n- Balance and coordination\n- Curly toes\n- Junior bunions\n- Ingrown toenails\n- Corns and calluses\nWe understand that kids need a different approach to care and treatment than adults, so we create individual plans just for them, incorporating activities that they already enjoy to help maximise their outcomes.\nSports Injury Management\nThere’s nothing like being out in the field, enjoying your favourite sport – until an injury stops you in your tracks. We work with patients of all ages and experience levels, from recreational joggers to professional footy players, to help optimise their recovery and get them back to the activities they love.\nOur assessment and treatment methods are backed by up-to-date clinical evidence, meaning that we’re always learning and improving our skills. We don’t just alleviate your pain and symptoms, but treat the cause of the injury to reduce the likelihood of it happening again in the future. We help treat a range of conditions, including:\n- Shin splints\n- Achilles tendinopathy\n- Heel pain (plantar fasciitis + heel spurs)\n- Stress fractures\n- Ankle sprains + instability\n- Runner’s knee (patellofemoral pain syndrome)\n- Jumper’s knee (patellar tendinopathy)\n- Muscle + tendon strains (tendinopathies)\nFoot Mobilisation Therapy\nFoot Mobilisation Therapy (FMT) is a non-invasive manipulation technique used to improve mobility and function while reducing lower limb pain and problems.\nThe principles behind FMT are that restrictions (which may present as stiffness) in our feet can lead to our joints and foot structures moving out of place. This alignment shift results in changes to the way we use our feet, leading to increased pressure, pain, and problems. As we change the way we walk to compensate for this foot pain, the way we use our ankles, legs, knees and hips can change too, resulting in further problems. By addressing the original restrictions with our feet and restoring their function and mobility, we can help alleviate associated pain throughout the body.\nFoot mobilisation therapy can help with:\n- Foot + arch pain (including plantar fasciitis)\n- Ankle sprains + stability\n- Achilles + calf pain\n- Shin splints\n- Toe deformities (claw toes, hammertoes)\n- Knee pain + problems\n- Forefoot pain (including Morton’s Neuroma)\n- Foot cramps + stiffness\nFoot orthotics are medically prescribed shoe inserts that can help support stressed and tired joints and muscles, improve foot alignment and function, help relieve pain, and repair + prevent injuries.\nOur podiatrists prescribe custom orthotics to match the needs of your feet by conducting a comprehensive biomechanical assessment, performing a gait analysis, and capturing your feet with plaster casts. We send our prescriptions to leading orthotics lab to create a truly made-for-you device.\nOrthotics can assist in the treatment of a wide range of lower limb problems, including helping to:\n- Assist the healing of muscle and ligament pain (e.g. plantar fasciitis)\n- Improve your gait (walking) and running technique\n- Alleviate pressure from specific areas of the feet (e.g. stress fractures, bony prominences)\n- Improve stability, balance and coordination\n- Correct discrepancies in the feet and legs (e.g. limb length difference, having one foot that is flatter than the other)\nWestern Medical Acupuncture\nWestern Medical Acupuncture (WMA) uses small, fine acupuncture needles that are gently inserted into target areas in the lower limbs to help improve blood flow, alleviate pain, facilitate healing, resolve restrictions in the muscles (trigger points) + stimulate the nerve fibres.\nThis differs from dry needling as it encompasses five mechanisms of action – of which dry needling is one of them. The other effects also engage the nervous system to help with repair, regeneration and healing musculoskeletal pain. WMA can be used to help with:\n- Heel + calf pain (including Achilles tendonitis)\n- Forefoot pain (pain in the big toe joint, Morton’s neuroma, bursitis, capsulitis)\n- Diabetic neuropathy\n- Surgical scarring\n- Pain from arthritis\n- Shin + knee pain\nLow-Level Laser Therapy\nLow-level laser therapy (LLLT) is a painless, safe and innovative solution to help relieve pain, reduce swelling, stimulate healing and assist the repair of a number of lower limb problems and injuries. This includes:\n- Heel, arch, foot + ankle pain, including plantar fasciitis\n- Pain from arthritis\n- Post-operative pain + swelling\n- Muscle sprains + strains, including Achilles pain, patella (knee pain), leg and hip pain\n- Forefoot pain, including from a Morton’s neuroma or plantar plate tear\n- Nerve pain, including sciatica\nIn an LLLT session, the hand-held device is gently applied to the surface of the skin, where the emitted light penetrates deeply yet painlessly to create a photochemical reaction within the body’s cells. This reaction affects the metabolic processes of the cells in the treated area, accelerating the reparative effects. Each session is non-invasive and has no side effects, making it an excellent alternative to medications, steroid injections and other alternatives.\nA biomechanical assessment is a thorough exam of your feet and legs – and how their bones, joints, muscles, tissues and ligaments work together to keep you moving. It includes a gait analysis, which often uncovers why you may be experiencing certain pains or discomforts. The tests performed in a biomechanical analysis are specific to problem you have, but can include testing:\n- The range of motion available in your joints\n- Your muscle strength\n- Foot posture\n- Your balance, coordination and falls risk\n- Nerve tightness\n- Your walking style\n- Your running style\n- Physical exam and palpation of bones and structures\n- Pressure + weight distribution\n- Limb length difference\n- Developmental milestones (kids)\nBiomechanical assessments are not a treatment but a diagnostic tool, necessary to make a confident diagnosis and prescribe an accurate treatment plan.\nInstrument Assisted Soft Tissue Mobilisation\nInstrument Assisted Soft Tissue Mobilisation (IASTM) is a clinically-proven technique used to help improve movement and reduce pain in your feet and legs. It works by helping to release restrictions in your muscles, ligaments, tendons and soft tissues using hand-held instruments. This means it can help with:\n- Breaking down scar tissue, knots and tightness\n- Improving your flexibility, the mobility of your tissues + the movement range available in your joints\n- Decreasing pain and muscle spasms\n- Improving circulation and blood flow to the area, accelerating healing and repair\nThe instruments used in IASTM have a combination of dull, targeted, convex and concave edges to complete various mobilising functions. This stimulates your body’s natural inflammatory response to help alleviate the restrictions and restore your mobility.\nIt is used to help with conditions including plantar fasciitis, patellar tendonitis, Achilles tendonitis, surgical scars, ligament sprains, muscle strains, bursitis, iliotibial band syndrome and shin splints.\nHome Visit Podiatry\nWe are able to take care of your feet from your home or aged care facility upon request. We’ll bring the tools to care for your skin and nail problems, perform diabetic foot screenings, and conduct simple biomechanical assessments.', 'Diabetes is a very common disease, with more than 30 million cases estimated in the United States (including those that remain undiagnosed). Unfortunately, those who suffer from this condition face serious short and long-term risks to the health of their feet.\nDiabetes adversely affects your body’s ability to either produce (Type 1) or respond to (Type 2) insulin, an important hormone that helps sugar pass from your bloodstream into the cells of the body, where it can be used as energy. As a result, the cells can’t get the nutrients they need, while sugar levels in the bloodstream build to dangerous levels.\nUncontrolled diabetes puts your feet at risk by damaging nerves, reducing circulation, and making them more susceptible to ulceration and injury. You’re more likely to hurt yourself, less likely to realize a problem, and more likely to get infected.\nCommon Diabetic Foot Complications\nConditions we help manage or treat that commonly affect the feet include:\nExcessive blood sugar poisons nerve cells. Over time, this can cause them to function improperly or even die. The extremities of the body, particularly the hands and feet, are often the earliest areas to experience symptoms. Initially this can mean pain or strange burning or shocking sensations, but in time this is usually replaced by dulled senses, if not total numbness.\nThe nerve damage that occurs as a result of diabetic neuropathy is usually not reversible. That means it is extremely important to seek medical help as early as possible and make healthy lifestyle changes in order to stop your condition from getting worse.\nWithout healthy nerves, you might not be able to feel it when you cut or injure yourself on your foot. You may not realize anything is wrong at all until the problem has already had many hours (or even days) to get worse. And that often leads to the next item on this list.\nDiabetic foot ulcers\nCuts, blisters, or areas of the foot exposed to constant pressure or irritation can eventually cause the skin to break down. These ulcers are especially common in people with diabetes, due to both poor nerve function and reduced circulation in the extremities. If you don’t treat them in time, the ulcers can become infected, and that infection can spread. Serious wounds may even require an amputation of your foot or lower leg to contain.\nIt is, then, extremely important to treat the wound right away so that healing can occur as soon as possible. The longer the ulcer is exposed, the greater your risk of infection and other complications.\nWe provide wound care services at our office, including cleaning the wound and removing dead skin and tissue (debridement) and dressing the wound. It is very important to also “offload” and avoid putting weight or pressure on the wound. If necessary we can provide orthotics to help you offload, or refer you to home care services if we believe it can help you.\nThis severe and destructive condition is becoming more and more common. People with long-term, uncontrolled diabetes often suffer from weakened foot bones, which can ultimately break and crumble. If your neuropathy is severe, you may not even feel any pain.\nContinuing to walk destabilizes the foot further. Often the arch completely collapses and takes a “rocker bottom” appearance. The ankle may become unstable as well. For patients with severe Charcot foot, walking may become difficult and ulcers frequently develop in pressure zones created by the deformity.\nEarly detection and treatment can allow Charcot foot to be managed non-invasively via bracing, orthotics, splints, and other alternatives. More advanced cases, however, will likely require surgical reconstruction of the foot—which comes with significant risks of complications. Without reconstructive surgery, amputation below the knee would otherwise be required.\nComprehensive Diabetic Foot Care\nIt is far better (and in the long run less expensive) to take a proactive approach to foot care if you have diabetes.\n- Inspect your feet carefully every day for signs of injury or ulceration. Treat any issues that emerge promptly.\n- Manage your condition by living a healthy lifestyle and keeping your blood sugar within a normal range as much as possible.\n- Visit your foot care specialist at least once per year for further testing and a comprehensive check-up.\n- If necessary, we can provide or refer you for diabetic shoes, orthotics, braces, and other preventative tools and treatments to reduce your risk of injuries and wounds.\nIf you have diabetes and it’s been a year since your last check-up, or you have any pain, injury, or signs of ulceration on your feet, please schedule an appointment with our team as soon as possible. We have two convenient offices in El Paso to serve you.']	['<urn:uuid:498c67a4-c21f-4b7c-bb52-12bdb46b8192>', '<urn:uuid:c0687f42-d24f-4b7e-a9cd-c8629ac2f7aa>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T11:40:26.317848	6	87	2330
80	How do memory settings work in modern car seats?	In modern vehicles with electrically powered seats, there is often an in-built memory system that can store preferred seating positions. The system can remember multiple driving positions, allowing different drivers to restore their preferred seat settings at the push of a button. The seats will default to these stored positions at the beginning of each journey.	['What is seating/what does it do?\nThere are two main types of seating in most vehicles, the individual or bucket seat which are contoured to take one person, and the bench seat which will accommodate two or even three people. Usually bucket seats occupy the front of the vehicle and bench seats are to the rear.\nIn addition there are some estate cars that have one or two rear-facing children’s seats that can be folded away when not in use. All seats in the vehicle, including children’s seats, require seatbelts by law.\nGetting into the details of seating\nDurable material is required for seating due to the constant wear and tear they take, or car seat covers can be used to prolong the original upholstery. Seats in modern vehicles, especially the front bucket seats, are adjustable in a variety of ways. These include forward and backward, taking the driver further or closer to the steering wheel and pedals, back-rest adjustment which will angle the occupant more upright or more reclined, and the seat section which can be raised or lowered to suit the desired driving position and aid visibility through the windscreen and windows. In some cases the back-rest of the front bucket seats will lower to the horizontal, enabling the driver or passenger to lie down. Lying down while driving however is not recommended. The back-rest adjustment is particularly useful in allowing the driver or passenger to choose the most comfortable position for the lumbar or lower back area which can be the cause of back problems on long journeys. The back-rest section may be in two halves allowing for tilt around the shoulder area.\nMost vehicle seats now incorporate a head-rest which is adjustable up and down and is a major factor in avoiding ‘whiplash’ injuries as a result of a collision. Either side of the head-rest most bucket seats now feature ‘winged’ sections to help support the drivers head from unwanted lateral movement.\nIn most modern vehicles seat adjustment is done electrically via a series of levers, switches and buttons, however in some older vehicles adjustment has to be done manually by sliding the seat forward and back on a set of rails or turning a wheel on the side of the seat or backrest which will raise or lower that section of the seating. In some cases electrically powered seats will have an in-built memory of the favoured seating positions and will default to that position at the beginning of each journey. In many cases it is possible to store more than one seating position in the memory system, allowing for multiple drivers to choose the correct setting at the push of a button.\nAdjustment to the rear seats, especially bench seats, is not so common but they can usually be folded in such a way as to maximise boot or cargo space. The seat section folds forward to butt up against the rear of the front seats and the back-rest section folds down to the horizontal. The rear children’s seats fold away into the body of the rear backrest.\nIn two-door vehicles the front bucket seat back-rest section will fold forward to enable passengers to access the rear bench seat. Certain larger vehicles have front seats which can swivel round to face the rear. This is particularly useful in mobile homes and campervans.\nHeated car seats are standard these days and can be controlled by buttons or switches on the dashboard, as can seat ventilation. Armrests are available in some models and can be folded away if not required so that it is easier to get into and leave the seat. Cup-holders are also available as part of the armrest.']	['<urn:uuid:fc52b1c7-57a5-482c-a523-a361162cfff1>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T11:40:26.317848	9	56	615
81	What makes thorium power plants safer than traditional nuclear reactors?	LFTR (Liquid Fueled Thorium Reactor) design incorporates multiple safety features. It can't melt down since the fuel is already liquid, and it has a 'salt plug' in its base that, if power is lost, melts and drains the fuel into a storage container where fission stops. The fuel then cools and solidifies. The reaction also has a natural 'negative feedback' - it slows down when it gets too hot, preventing the reaction from going out of control. Additionally, since there is no high pressure involved, massive containment structures aren't necessary.	['You wouldn’t use a 20 year old cellphone, so why are we using 20+ year old nuclear technology? The curious reason we are, is because it works, but we could be doing so much better.\nHere is the short overview on LFTR and Thorium (the fuel of the future):\n– Element 90, found as Thorium 232 in nature, is 4 times more common than Uranium and about 200-400x more common than U-235, the fuel we burn in Light Water Reactors (LWRs) in the US and much of the world. That’s just the start…\n– Thorium is naturally radioactive like uranium, and has a half-life equal to the age of the universe (about 15 billion years) so it will be with us for a long time\n– It is found in large quantities in “Rare Earth” mines, which are rare in the US because they dig up Thorium. Thorium is (weakly) radioactive, and US law requires it be treated as a radioactive waste and buried. Too much Thorium in a rare earth mine makes it unprofitable, but it is these rare earth mines that bring up the high technology metals we need in society today, such as Neodymium for magnets (think generators and motors).\n– A LWR (Light Water Reactor) in the US burns about 0.5%-5% of the fuel put in it, the remaining is disposed of as unburned fuel as part of the radioactive waste. A LFTR on the other hand, running from Thorium could burn 100% of the fuel\n– Because it can all be consumed, if you held a marble a little over an inch across (~3 cm) made of Thorium, it could power your entire (western world) needs for your entire life. (more: http://www.youtube.com/watch?v=qbGZ_Y-xkPM)\n– The “waste” products are far less than that of the Light Water Reactor technology used today. Also, the amount of mining is far less – and a natural result of a rare earth mine (see above). (more: http://energyfromthorium.com/2007/01/09/uranium-vs-thorium-mining-processing-waste-generation/ and http://en.wikipedia.org/wiki/Liquid_fluoride_thorium_reactor#Economy_and_efficiency)\n– The byproducts of a LFTR are radioactive, but contain few “transuranic” elements – which would be radioactive for a very long time. Instead, much of the “waste” could be recycled into useful products after a month or a few years of cooling off, and by about 100 years, much of the radioactivity is gone.\n– There is less risk of proliferation with LFTR (Thorium) fuel, since Thorium doesn’t fission in of itself, and stolen active LFTR fuel would contain U232 (a natural byproduct of the LFTR process, not required to be added). U232 is very radioactive and would damage electronics and irradiate the people stealing it, and make any stolen material easy to find.\nLFTR – Liquid Fueled Thorium Reactor (A molten-salt reactor using thorium)\n– A LFTR is a different kind of reactor. It was invented during the 1950s and 1960s in Oak Ridge Labs, but was quickly abandoned since the nuclear reactions were found to be not good for bomb-making. (more: http://youtu.be/bbyr7jZOllI?t=1m6s)\n– Since the focus was on bombs and Uranium originally, the infrastructure of LWRs (light water reactors) quickly grew and stabilized, ignoring Thorium technologies such as LFTR (more: http://youtu.be/bbyr7jZOllI?t=12m1s – hear the Nixon tape – very damning evidence, and a real shame).\n– This kind of reactor can’t “melt down” as it is already liquid. It runs in the 700°C [1300 °F] range giving far superior thermodynamic efficiency. High pressure is nowhere near the core, since a hot salt loop transfers the heat to the generators.\n– The reactor is designed with a “salt plug” in its base, cooled by a fan. If power is ever lost, the system fan would shut down (due to lost power), and the plug melts, draining the fuel into a storage container where fission stops. The fuel would also cool and solidify. If there were ever a breach in the reactor, material would drain into the same tank. Even if the tank broke, the fuel would simply solidify on the floor. Safety can be done completely passively, no worries about hoping systems will be online when needed. (more: http://youtu.be/enjc4arwH7U?t=3m30s)\n– The reaction has a natural “negative feedback”, which means that if demand for power grows, the reactor will run faster, but if it falls, it will reduce its output. It also will run slower as it gets too hot, so more heat does not make the reaction go out of control, it actually slows the reaction (due to expansion making fission less likely).\n– The fuel is cheap (see Thorium above), and since there is no high pressure, huge thick walls and buildings are not necessary. This lowers the space and cost requirements of a building.\n– Any nuclear fuel generates Xenon gas while in a reactor. This gas slows reactions and in the case of LWRs and other solid fueled reactors that we use today, it cracks and damages the fuel pellets. Since LFTRs are liquid, it simply bubbles out of solution. It can also be collected, and in a few months is no longer radioactive and can be sold. This is also one reason the fuel in our current LWRs is only 0.5%-5% consumed, because if it were to be left in longer, the expansion from this would damage the fuel tubes inside of the reactor.\n– LFTRs can make isotopes of materials we desperately need. Mo-99 is needed by hospitals for radiation treatments, Pu238 is needed by NASA for space missions to the outer reaches of the solar system, and Bi213 for new targeted (Leukemia and Pancreas) cancer treatments. (more: http://www.youtube.com/watch?v=2at8C8YrX80)\n– LFTRs can also burn radioactive “waste” we are currently storing, made from the LWR units of today. We could actually reduce our radioactive waste using LFTRs and other Molten-Salt Reactors (MSRs) (more: https://www.youtube.com/watch?v=i1fqB6p9pgM).\n– China is already working on LFTR technology and stockpiling Thorium. India is working on Thorium for solid fueled reactors, but will probably move to LFTR as a natural part of that research. (more: http://en.wikipedia.org/wiki/Liquid_fluoride_thorium_reactor#Recent_developments)\nDid you know?\n– A typical coal burning plant emits far more radioactivity into the air than any nuclear plant. Nuclear plants keep their fuels inside the building, but the smoke from coal contains all manner of poisonous materials (mercury, cadmium, etc) and several naturally radioactive ones such as Uranium and Thorium. These materials are fairly safe as rocks, but as a breathable dust, not so much.\n– Nuclear power is over 1,000,000 times more energy dense than burning fossil fuels. The comparison is nuclear energy to that of a carbon-hydrogen bond. (more: https://www.youtube.com/watch?v=NG2jN–D2Es – See a visit to Arizona’s Palo Verde Energy Education Center outdoor exhibits)\n– US current needs for energy burn a rail car of coal about every 1-3 seconds. That’s about 100 tons per rail car.\n– There have been far more deaths from coal mining than all nuclear power accidents combined. (more: https://www.youtube.com/watch?v=4E2GTg7W7Rc – graphs at 2m:40s)']	['<urn:uuid:7e1f9f36-f6f7-4cf8-b750-e61f3642ea15>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T11:40:26.317848	10	90	1144
82	authorized personnel stock trading legality consequences insider info	Trading by anyone who has advance, nonpublic information of a takeover or another extraordinary event affecting a company's finances is considered illegal insider trading. The Securities and Exchange Commission states that if you gain knowledge of a takeover and trade on that information, you are liable. This should not be confused with legal insider trading by company insiders like senior managers, directors and major shareholders, who are required by law to report their stock transactions to the SEC.	"['When insiders trade, investors listen. The stock trades by company insiders -- senior managers, directors and some major shareholders -- often are followed by money managers and other institutional investors. And experts say individual investors also can glean some valuable information about the potential of stocks from reports on the trading -- if they know what to look for.\n""The theory is that insiders are in the best position to know what the potential of a company might be,"" said Tracey J. Stangle, senior vice president and analyst in First Albany Corp.\'s Hartford office. But, he said, ""I can\'t say it\'s a hard and fast rule.""\nExperts caution that the reports on trading by insiders must be used judiciously and in conjunction with a thorough financial analysis of a company and its balance sheet.\nAnd those trades by insiders should not be confused with illegal insider trading.\nDennis Surprenant, the Securities and Exchange Commission\'s enforcement director for the New England region, said illegal insider trading is trading by anyone who has advance, nonpublic information of a takeover or another extraordinary event affecting a company\'s finances. ""If you gain knowledge of a takeover and trade on that information, you are liable,"" Surprenant said.\nInvest/Net Inc. of Florida compiles reports on inside trading from the Securities and Exchange Commission. (Insiders are required by law to report any stock transactions involving the company they work for to the SEC, which regulates the securities industry.) Invest/Net then sells the reports to money managers and big investors.\nSmall investors can get reports of trades by insiders in many newspapers and other publications. (The Courant lists trades by insiders at Connecticut companies, or companies with direct ties to the state, in its Business Weekly section every Monday.)\nRobert Gable, president of Invest/Net in Fort Lauderdale, said investors ""use Invest/Net the way I recommend it, as a tool. When you are building something, you need more than one tool. You shouldn\'t just make a decision based on [trades by] insiders."" Instead, you have to make your own investment decisions consistent\nwith your own investment goals, experts say.\n""The insiders being the closest people to the inner workings of the company, you are going to get insights they have the benefit of,"" Gable said. An example of a report on trading by insiders and how it was interpreted is the recent trading by several executives of Fleet/Norstar Financial Group.\nAccording to filings with the SEC, five insiders at Fleet sold 58,473 shares in September and October -- at an average price of $25.35 each -- for a total of $1.5 million.\nCharles Conway, a Fleet spokesman, said the company had no comment on the trading. Fleet has no restrictions on when insiders can trade, other than government guidelines, he said.\nGable said the selling by Fleet insiders was not unusual. Insiders are expected to sell to take profits when their stock has reached very high levels. ""Clearly, they did sell at the highest levels that the stock has been in in the last year-and-a-half,"" Stangle of First Albany said.\nHere are a few tips on how investors can best use information on trades by insiders:\nLook for patterns of trading by three or more insiders.\n""You don\'t want to act on one insider trade,"" Gable said. Instead, look for ""three or more of the insiders trading over a 60-day period.""\nThis way, you can see a pattern: Trading by one insider may just reflect a personal decision, but several insiders\' trading at the same time may indicate a consensus about the potential of the company\'s stock.\nStangle agreed. Investors ""put credence into concentrations of inside executives buying or selling,"" he said.\nDon\'t base your investment decisions solely on information on trades by insiders. Patricia Nickols, portfolio manager for Advest Inc. in Hartford, said that trading by insiders ""is just one of many factors which can indicate company performance.""\nShe reads the columns on trading by insiders just to be aware of what is happening. ""But I wouldn\'t buy or sell a stock based on that information alone. It isn\'t necessarily a good indicator of what is happening in a company.""\n""Let\'s say you have four or five stocks on your buy list,"" Gable said. ""Let\'s say that earnings and fundamentals are looking good, but you have a pretty high number of insiders selling -- there is a possibility of [impending] bad news. So it can help you pare down your choices.""\nThe price of the stock when insiders are trading is more important than the actual trading. ""Just because some insiders are buying or selling, don\'t jump in one way or another,"" Gable said. ""It\'s not so much what they are selling, but when they sold it, relative to the price.""']"	['<urn:uuid:0b29e56c-1caf-40f4-b8ae-577ade3b8d65>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T11:40:26.317848	8	78	793
83	Can phone usage affect loan approval?	Yes, lenders can access smartphone data including call timing, location, frequency of calls, texting patterns, social network size, travel habits, battery charging patterns, and app usage. This data is used to predict if a borrower will default on a loan.	['If you are looking to apply for a loan in the near future, we suggest you take a close look at your online identity. Although the three credit bureaus are still the most popular way a credit score is established, some lenders — including traditional lenders — are now using new criteria to determine a borrower’s creditworthiness. This includes factoring in social media accounts, phone usage, and overall online identity. Some lenders use this criteria in addition to the traditional credit bureau and FICO scores, while other lenders are using these factors exclusively.\nA FICO score calculates your risk as a borrower by using quantitative data while qualitative data is generated from your online interactions on social media, your online search history, and websites you tend to visit. This data provides lenders a new way to assess how likely you are to repay a loan.\nWe’ve listed the FICO score formula in order of importance below:\n35% Payment History\n30% Credit Utilization\n15% Credit History\n10% Credit Lines\n10% New Credit\nSocial Media Accounts\nLenders are checking social media accounts to verify the person is who they say they are. They look for a complete profile, number of connections, quality of connections, and they scan for offensive posts that have profanity and/or a strong political affinity. Therefore you should be cautious about what you post on social media, especially if you are in the market for a loan.\nFor business loans, alternative lenders are known to use social media accounts to vet businesses and business owners before they lend money while other lenders may use an online identity to help make a decision on a borrower who does not have established credit. And it has been reported that FICO is considering including social media when calculating a credit score. So the next time you post on Facebook, tweet, or share a photo on Instagram, remember that lenders may be watching you.\nSmartphone data can provide a lender with quite a bit of information about a borrower. Data companies now offer lenders access to different variables such as the time calls are made, the location of the caller, and frequency of calls. The data can be used to reveal if a borrower receives more text messages than they send, gauge how large their social network is, if they travel, and how often they charge their battery. Additionally, they can record the number of apps that have been downloaded to your smartphone and the usage of the apps. Combined, this data can be used to predict if a borrower will default on a loan.\nOverall Online Identity\nIt’s best practice to search your name on the three popular search engines, Google, Yahoo, and Bing, periodically. You should look for, and clean up, any accounts or websites that have erroneous information about you and/or your business. We highly recommend you keep all of your social media settings set to ‘private’ and watch what you post and/or share on other websites or public social media pages. Sharing too much on social media can also put you at an increased risk for identity theft, learn more about that here.']	['<urn:uuid:ac3cceb3-51a6-4a5b-9fe2-bbfb568aa03d>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T11:40:26.317848	6	40	524
84	single pilot safety tools cfit risks	Single pilots have several safety tools to prevent Controlled Flight Into Terrain (CFIT). Modern technology like tablet computers with ForeFlight and ADS-B receivers provide situational awareness, while autopilots help manage workload. Air traffic control offers weather advice, and well-coached passengers can assist with tasks. However, CFIT risks remain significant, particularly during approach and landing phases, often due to loss of situational awareness, poor weather conditions, pilot fatigue, and misreading approach plates. Most CFIT accidents occur when pilots are unaware of danger until too late, especially during non-precision approaches or in unfamiliar terrain.	"['Slogging through the murk fearing a two-degree temperature drop may lead to an icing encounter, darkness setting in, facing a low approach at an unfamiliar airport with terrain around. Some days (or nights), the cockpit of a single-engine piston airplane seems like a lonely place, with all of the responsibility for the safe conclusion of a flight resting on the one set of pilot shoulders anywhere to be found. Airline crews work together in highly capable airplanes with unimaginable resources available to them in the form of aircraft performance, system redundancy, dispatchers a call away, advanced training, and that second trained body to advise and assist.\nFortunately, those of us who fly single pilot in simpler airplanes have more tools available to us than those daring flights we read about in books by Saint-Exupéry and Gann. The tablet computer, for example. For about one aviation monetary unit (the AMU—equivalent to $1,000—is how pilots discuss airplane costs with spouses while minimizing marital meltdown), one can buy a tablet computer and portable ADS-B receiver, a combination that brings unbelievable situational awareness to even the simplest cockpit. On a recent flight with my iPad on my lap and the ForeFlight electronic flight bag software running, I heard the system alert me that I was about to enter a runway, just as a copilot might call out. In this case I was crossing an inactive runway with permission, but the heads up was most appreciated nonetheless. While nonpilot passengers can be helpful in calling out traffic, most would not have the situational awareness to know to call out the runway environment.\nSimilarly, approaching an airport, extended centerlines with enlarged runway numbers can be shown on many EFBs, helping identify the correct runway. Most EFBs these days also show terrain relative to the route, some in three dimensions and with terrain alerting. Such panel-mount capability costs dozens of AMUs, a level of expenditure that frequently requires massive amounts of monetary expenditures to occur on the other side of the marital ledger.\nOther tools helpful to the lonely pilot include air traffic control. Even with a plethora of satellite-delivered weather available in the cockpit, I find myself during particularly hairy weather situations asking ATC for advice on whether to zig or zag. On more than one occasion, I have been ready to divert to an alternate when a controller has offered a suggestion that conditions just ahead are improving and that aircraft are getting through. It’s my responsibility to manage my flight—but I certainly value ATC’s input.\nIron Mike, Otto, or George—pick your moniker—but even the most basic autopilot can be a great aid for single-pilot operations. Mine is an old single-axis S-Tec with GPS steering. The simple autopilot flies my Bonanza with aplomb. I must manage the ascents and descents, but when life gets busy or tiring, the system is worth its weight in AMUs. Most new digital autopilots include altitude capture and envelope protection, keeping pilots from getting too slow or too fast, a danger that has led to the demise of too many.\nOther weapons in the arsenal of single pilots are good training and experience. Having fully explored the airplane, its systems, and its flight envelope with an instructor knowledgeable in that particular model of airplane gives a pilot loads of confidence and capability. When things start to go wrong, flying the airplane needs to be second nature. Good training and frequent flying allow a pilot to push the basics of keeping the airplane upright and on course to the back of the brain, freeing up neurons for dealing with the emerging problem before task saturation sets in.\nFinally, don’t forget nonpilot passengers. Depending on how frequently they fly with you, a well-coached passenger can be a great help in pointing out traffic, double checking your flight plan and frequency entries, and even making radio calls if they’ve been around enough to anticipate and understand what ATC needs to know. Teach them to read a chart and they can remind you of course changes. Show them the altimeter and promise them a dollar for every time they can point out when you are more than 200 feet off.\nFeeling alone? Look around, there may be more assistance available than you first thought.\nEmail [email protected]\nFollow on twitter @tomhaines29\nEditor in Chief Tom Haines frequently flies his Bonanza alone and with other pilots.', ""If you wish to contribute or participate in the discussions about articles you are invited to join SKYbrary as a registered user\nControlled Flight Into Terrain\n|Category:||Controlled Flight Into Terrain|\nControlled flight into terrain (CFIT)\nControlled Flight into Terrain (CFIT) occurs when an airworthy aircraft under the complete control of the pilot is inadvertently flown into terrain, water, or an obstacle. The pilots are generally unaware of the danger until it is too late.\nMost CFIT accidents occur in the approach and landing phase of flight and are often associated with non-precision approaches.\nMany CFIT accidents occur because of loss of situational awareness, particularly in the vertical plane, and many crash sites are on the centreline of an approach to an airfield. Lack of familiarity with the approach or misreading of the approach plate are common causal factors, particularly where the approach features steps down in altitude from the initial approach fix to the final approach fix.\n- Collision with the ground resulting in Hull Loss and fatalities/injuries.\n- Standard Operating Procedures (SOPs).\n- Terrain Avoidance Warning Systems (TAWS).\n- Situational Awareness in relation to terrain\n- Pilot-induced situation: The pilot encountered weather conditions that were worse than forecast and, in an attempt to maintain or regain visual contact with the ground in an area of very low cloud, descended below Minimum Safe Altitude and the aircraft struck the ground. Contributing to this accident was the pilot's over-reliance on GPS while attempting to maintain Visual Meteorological Conditions (VMC) and a resultant lack of adequate situational awareness of terrain.\n- ATCO-induced situation: The controller gave an aircraft which was still at 210 KIAS an intermediate heading towards the ILS centreline during a radar vectored initial approach but was subsequently distracted and failed to issue the intercept heading for the ILS LLZ. When the flight crew, who were unfamiliar with the approach, failed to notice the situation in time to query it, the aircraft flew beyond the centreline and into high terrain on the other side before resolution was possible.\n- Weather: Rain, turbulence, and icing, may increase the workload of the pilot and cause interference reducing the accuracy of radio navigation beacons. Poor visibility, particularly at night can contribute to disorientation and loss of situational awareness.\n- Approach Design and documentation: The depiction of an approach, and particularly step down fixes, on Terminal Approach Procedure (TAP) plates may not be clear. Approaches may take aircraft close to high terrain in order to comply with diplomatic or noise abatement constraints, or to deconflict with departure routes.\n- Failure to use Standard Phraseology leading to confusion and misunderstanding.\n- Pilot Fatigue and Disorientation. Approach and landing is a demanding phase of flight for pilots.\n- More widespread equipment of aircraft with TAWS.\n- Greater awareness of Approach and Landing risks.\n- Constant Descent Angle Approaches (CDA).\n- Minimum Safe Altitude Warning (MSAW) systems.\n- Accident and Serious Incident Reports: CFIT - a list of all Accident and Serious Incidents within SKYbrary which resulted in, or involved a severe risk of, controlled flight into terrain.\nWork in Progress CFIT2 Flight Safety Foundation CFIT Reduction Products\n- CFIT Checklist (available in Arabic, Chinese, English, French, Russian and Spanish;\n- CFIT Education and Training Aid\nThese and other CFIT products may be obtained from the CFIT Web-site\nThe Flight Safety Foundation ALAR Toolkit\nThe Flight Safety Foundation ALAR Toolkit provides useful training information and guides to best practice. All the Briefing Notes are on the SKYbrary Bookshelf and may be acceessed via the link above but a copy of the ALAR Toolkit on CD may also be purchased from the Foundation:. See especially:\n- HindSight1 - Controlled Flight Into Terrain;\n- HindSight2 - Seminole in California;\n- HindSight3 - Descent Below the Glideslope;\n- HindSight3 - Misunderstandings: Wrong QNH Setting;\n- HindSight4 - Economy versus Safety - the Professional’s Dilemma;\n- HindSight6 - Alert Controller Prevents CFIT Accident;\nAirbus Safety Library\n- Airbus Approach Techniques Briefing Note - Aircraft Energy Management during Approach;\n- Airbus Operational Environment Briefing Note - Enhancing Terrain Awareness;\n- FOBN - Approach Hazard Awareness offers a more expanded and categorised review of approach hazards.]\nBoeing AERO magazine""]"	['<urn:uuid:52c24d48-9a22-4640-94ce-9a39eea8a7b1>', '<urn:uuid:8ff50318-533e-4756-bebf-27d2195413ba>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T11:40:26.317848	6	92	1427
85	Why is it concerning that so many different people are involved in creating computer chips?	Chip design requires thousands of engineers and an entire ecosystem of design tools and fabrication services have access to the design, making it possible for malicious functionality known as Hardware Trojans to be inserted. This is a major concern for both the semiconductor industry and governments that depend on off-the-shelf electronics for critical infrastructure.	['PhD and Postdoc Research\nMy research interests include hardware verification and security. I graduated from UC Santa Barbara with a PhD in Computer Engineering in June of 2016 under the advisement of Professor Tim Cheng working in the SoC Design and Test Lab. My thesis focused on identifying and verifying unspecified design functionality susceptible to malicious manipulation. After graduation I continued my research in hardware security as a post-doc at UC Santa Barbara and visiting Scholar at Hong Kong University of Science and Technology.\nDetecting Hardware Trojans in Unspecified Functionality\nHardware is the root of trust in electronic systems. Hardware can both be exploited to mount powerful attacks on a system (ex. Rowhammer) and designed to provide extra security not possible with only software defenses (ex. Data Execution Prevention). Because hardware design mandates thousands of engineers and an entire ecosystem of design tools and fabrication services, many people have access to the design and the opportunity to insert malicious functionality known as Hardware Trojans. This project develops HDL-level analysis methods to verify the absence of Trojans and accidental security holes in unspecified functionality by:\nCharacterizing unspecified functionality, and\nVerifying unspecified functionality is Trojan-free.\nHardware Design Lifecycle: this project focuses on Trojans inserted in the HDL design meaning no “golden model” is available for verification.\nVerification is a bottleneck in the hardware design process, estimated to consume over 70% of hardware development resources. Developing pre-silicon verification infrastructure is crucial in order to catch functional bugs before tape-out and avoid silicon re-spin, however the process requires immense manual effort and is largely ad-hoc. Moreover, traditional verification techniques focus on increasing confidence that specified functionality is correct. Un-modeled behavior will not be verified by existing methods, meaning any security vulnerabilities in unspecified functionality will go unnoticed.\nSecurity vulnerabilities include accidental bugs as well as malicious functionality (Hardware Trojans) inserted by an attacker with access to the design. Chip design mandates thousands of engineers and an entire ecosystem of design tools and fabrication services have access to the design, making Trojans a major concern for both the semiconductor industry and governments recognizing the dependence of critical infrastructure on off-the-shelf electronics. The focus of this project is on detecting Trojans inserted in the design pre-silicon (meaning no “golden” design model exists) whose behavior is embedded completely within unspecified functionality.\nFor example, consider a simple FIFO circuit shown in the figure below. The value of read_data is unspecified when the FIFO is not being read from, and an attacker can use this condition to leak information by inserting the circuitry shown in red. The number of cycles during which signals are “don’t care” in modern designs is large due to increasing design complexity. It is often impossible to enumerate what the desired value of every signal should be at every cycle and even more impractical to expend verification effort on “don’t care” functionality. This provides ample opportunity for Trojans to implement malicious behavior even when constrained to only modifying unspecified functionality.\nSimple FIFO circuit showing a Trojan (in red) modifying unspecified functionality to leak information\nThis project develops methods to:\nCharacterize unspecified functionality, and\nVerify unspecified functionality is Trojan-free.\nCharacterization of unspecified functionality is difficult because by definition unspecified functionality encompasses aspects of the design outside the focus of project engineers. We have developed attack scenarios for specific classes of unspecified functionality: RTL Don’t Cares (ITC 2015), vulnerable unspecified functionality in on-chip bus systems (DATE 2016 and TCAD 2016), and FPGA bitstreams (FPL 2016). Additionally, we have provided automated methods to generate assertions describing when signals are unspecified allowing characterization of unspecified functionality beyond the specific classes explored previously in our research (ICCAD 2015 and 2017).\nAfter unspecified functionality is characterized, another major challenge is verifying unspecified functionality is Trojan-free. If a design could be fully specified and exhaustively verified no Trojans (or functional bugs) would exist. Achieving this is impossible for most designs, so our approach is to instead formulate design-independent properties in the form of satisfiability queries describing suspicious behavior of a signal when unspecified (ASP-DAC 2017 and ICCAD 2017). By formally verifying that the suspicious properties do not hold, Trojan detection without specification refinement or modification to the original design is possible using off-the-shelf SAT/SMT solvers and commercial equivalence checking tools.\nProject Details: relationship between methods which characterize unspecified functionality and those which analyze the design to detect Trojans\nN. Fern and K. T. Cheng, “Evaluating Assertion Set Completeness to Expose Hardware Trojans and Verification Blindspots”, in Design, Automation and Test in Europe (DATE) Conference, 2019.\nN. Fern and K.-T. Cheng, “Pre-silicon Formal Verification of JTAG Instruction Opcodes for Security”, International Test Conference (ITC), 2018.\nN. Fern and K-T. Cheng. “Mining Mutation Testing Simulation Traces for Security and Testbench Debugging“, International Conference on Computer Aided Design (ICCAD), 2017.\nN. Fern, I. San, and K-T. Cheng. “Detecting Hardware Trojans in Unspecified Functionality Through Solving Satisfiability Problems“, Asia South-Pacific Design Automation Conference (ASP-DAC), 2017.\nN. Fern and K-T. Cheng. “Verification and Trust for Unspecified IP Functionality” in Hardware IP Security and Trust, Prabhat Mishra, Swarup Bhunia, and Mark Tehranipoor, Eds. Springer, 2017.\nN. Fern, I. San, C. Koc, and K-T. Cheng. “Hiding Hardware Trojan Communication Channels in Partially Specified SoC Bus Functionality“, IEEE Transactions on Computer-Aided Design of Integrated Circuits and System (TCAD), 2016.\nI. San, N. Fern, C. Koc, and K-T. Cheng. “Trojans Modifying Soft-Processor Instruction Sequences Embedded in FPGA Bitstreams“, International Conference on Field-Programmable Logic and Applications (FPL), 2016.\nN. Fern, Verification Techniques for Hardware Security, PhD thesis, University of California, Santa Barbara, June 2016.\nN. Fern, I. San, C. Koc, and K-T. Cheng. “Hardware Trojans in Incompletely Specified On-chip Bus Systems”, Design, Automation, Test in Europe (DATE), 2016.\nN. Fern and K-T. Cheng. “Detecting Hardware Trojans in Unspecified Functionality Using Mutation Testing”, International Conference on Computer Aided Design (ICCAD), 2015.\nN. Fern, S. Kulkarni, and K-T. Cheng. “Hardware Trojans Hidden in RTL Don’t Cares – Automated Insertion and Prevention Methodologies”, International Test Conference (ITC), 2015.']	['<urn:uuid:7587fe9f-7718-4714-bc8a-b229091eeff1>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T11:40:26.317848	15	54	993
86	I'm trying to make our company's new knowledge sharing system work better. What are the two main types of expert knowledge we need to consider when setting it up, and what common problems should we watch out for?	The two main types of knowledge to consider are tacit knowledge (internalized knowledge that individuals may not be consciously aware of, like how they accomplish tasks) and explicit knowledge (knowledge consciously held and easily communicated to others). Common problems to watch out for include inability to articulate tacit knowledge, lack of top management support, resistance to change, and the assumption that technology alone is a complete solution without considering organizational culture and proper implementation.	"['by Robert Simmons of the Forsythe Solutions Group\nA winning knowledge management program (KMP), one that increases staff productivity, product and service quality, and deliverable consistency by capitalizing upon codified intellectual and knowledge-based assets, cannot subsist on technology solutions alone, It must also consider people, processes, structure, and culture.\nMany organizations leap into a knowledge management (KM) solution (document management, data mining, blogging, community forums, and the like) without first considering the purpose or objectives they wish to fulfill or how the organization will adopt and follow best practices for managing its knowledge assets long term.\nThis is the first in a series of three articles in which I will present a phased approach for implementing and sustaining a successful KMP. Part I introduces some key KM terms and concepts and then presents the eight-phase approach at a high level. Respectively, Parts II and III discuss phases 1 thru 4 and 5 thru 8 in more detail.\nTerminology and Concepts\nWhile Knowledge Management as a discipline is relatively young having started in the ’70’s, KM terminology, models, and best practices are still being established and adopted. I’ve listed the more widely accepted terms and concepts below. They should provide some awareness and basic principles upon which to build your organization’s KMP.\nKM best practices specify different types of knowledge. The types most often referenced are tacit knowledge and explicit knowledge.\nTacit knowledge represents internalized knowledge that an individual may not be consciously aware of, such as how he or she accomplishes particular tasks.\nExplicit knowledge represents knowledge that the individual holds consciously in mental focus, in a form that’s easily communicated to others.\nIn 1995, Nonaka and Takeuchi introduced the Socialization-Externalization-Combination-Internalization (SECI) model (Fig. 1) in their book The Knowledge Creating Company wherein tacit knowledge is extracted to become explicit knowledge, and explicit knowledge is re-internalized into tacit knowledge. It demonstrates a continual evolution of knowledge through socialization, externalization, combination, and internalization.\nIn 2007, the IT Infrastructure Library v3 (ITIL v3) introduced a Knowledge Management process including definitions for data, information, knowledge, and wisdom.\nData is a set of discrete facts about events.\nInformation comes from providing context to data.\nKnowledge is composed of the tacit experiences, ideas, insights, values and judgments of individuals as well as from the analysis of information and data.\nAnd wisdom gives the ultimate discernment of the material and having the application and contextual awareness to provide a strong, common sense judgment. The Data-Information-Knowledge-Wisdom (DIKW) structure (Fig. 2) demonstrates phases of increased context and understanding and how data is transformed into information, then knowledge, and finally wisdom.\nITIL v3 also refers to a Service Knowledge Management System (SKMS) as set of tools and databases used to manage knowledge and information. While ITIL’s application of KM is primarily focused on the development, delivery, support, and improvement of IT services, the architecture of the SKMS (Fig. 3) has relevance from a business perpective, as well.\nUnderstanding this archtecture — how data and information is stored, related, and integrated (data and integration layers) and how people will want to access and utilize the information (knowledge processing and presentation layers) — is the first step in addressing the technology needs of a knowledge management system (KMS) solution.\nAs mentioned at the beginning of this article, a successful KMP will consider more than just technology. The core components of KM include people, processes, technology, structure, and culture (Fig. 4).\nPeople represents how you increase the ability of individuals within the organization to influence others with their knowledge.\nProcesses involves how you establish best practices and governance for the efficient and accurate identification, management, and dissemination of knowledge.\nTechnology addresses how you choose, configure, and utilize tools and automation to enable KM.\nStructure implies how you transform organizational structures to facilitate and encourage cross-discipline awareness and expertise.\nAnd culture embodies how you establish and cultivate a knowledge-sharing, knowledge-driven culture. The eight-phase approach explained below addresses all areas of the KM framework.\nImplementing a KPM is no easy feat. You will encounter many challenges along the way including many of the following:\n- Inability to recognize or articulate knowledge; turning tacit knowledge into explicit knowledge.\n- Geographical distance and/or language barriers in an international company.\n- Limitations of Information and Communication technologies.\n- Loosely defined areas of expertise.\n- Internal conflicts – professional territoriality.\n- Lack of incentives or performance management goals.\n- Poor training or mentoring programs.\n- Cultural barriers – this is how we’ve always done it.\nThe following eight-phase approach will enable you to identify these challenges early on so that you can plan for them, thus minimizing the risks and maximizing the rewards. This approach was developed based on logical, tried-and-true activities for implementing any new organizational program. The early phases involve strategy, planning, and requirements gathering while the later phases focus on execution and continual improvement.\nPhase 1: Establish program objectives – Before selecting a tool, defining a process, developing workflows you must envision and articulate the end-state. Different organizations may have different reasons for implementing a KMP, but in order to establish the appropriate program objectives, identify and document the business problems that need resolution and the business drivers that will provide momentum and justification for the endeavor.\nProvide both short term and long term objectives that address the business problems and support the business drivers. Short term objectives should seek to provide validation that the program is on the right path while long term objectives will help to create and communicate the big picture.\nPhase 2: Prepare for change – KM is more than just an application of technology. It involves cultural changes in the way employees perceive and share knowledge they develop or possess. One common cultural hurdle to increasing the sharing of knowledge is that companies primarily reward individual performance. This practice promotes a “knowledge is power” behavior that contradicts the desired knowledge-sharing, knowledge-driven culture end-state you are after.\nSuccessfully implementing a new KMP may require changes within the organization’s norms and shared values; changes that some people might resist or even attempt to quash. To minimize the negative impact of such changes, it’s wise to follow an established approach for managing cultural change such as John Kotter’s 8-step change process, which will be covered in more detail in part II of this series.', 'The issue of knowledge management systems has probably always been the most discussed and debated topic within knowledge management (KM). Even though knowledge management systems are not the most important part of KM (with some arguing that they are not even absolutely necessary), this is still the subject that generates most interest.\nOn this site, I have considered the impact of IT in all the knowledge management strategy subsections, with particular emphasis on its role in knowledge sharing. From this point on, the discussion will be organized as follows:\n- This subsection will discuss the theoretical implementation of knowledge management systems and its impact on the organization.\n- The section titled ""KM Tools"" will look at some of the main categories of systems available.\nWhat are Knowledge Management Systems?\nKnowledge management systems refer to any kind of IT system that stores and retrieves knowledge, improves collaboration, locates knowledge sources, mines repositories for hidden knowledge, captures and uses knowledge, or in some other way enhances the KM process.\nIf my explanation above makes the definition of these systems seem vague, that is because there is no consensus as to what constitutes a knowledge management system, much like there is no consensus regarding KM. Furthermore, since KM is involved in all areas of the firm, drawing a line is very difficult.\nJames Robertson (2007) goes as far as to argue that organizations should not even think in terms of knowledge management systems. He argues that KM, though enhanced by technology, is not a technology discipline, and thinking in terms of knowledge management systems leads to expectations of ""silver bullet"" solutions. Instead, the focus should be determining the functionality of the IT systems that are required for the specific activities and initiatives within the firm.\nI fully agree with his reasoning. However, for the purpose of this site (intended to be useful for those people that do search for terms like knowledge management systems), I will break these down into the following general categories (adapted from the work of Gupta and Sharma 2005, in Bali et al 2009):\nThese categories will cover the vast majority of the systems that people will normally link directly to KM.\nProblems and Failure Factors\nToo often, the effects of technology on the organization are not given enough thought prior to the introduction of a new system. There are two sets of knowledge necessary for the design and implementation of a knowledge management system (Newell et al., 2000):\n- The technical programming and design know-how\n- Organizational know-how based on the understanding of knowledge flows\nThe problem is that rarely are both these sets of knowledge known by a single person. Moreover, technology is rarely designed by the people who use it. Therefore, firms are faced with the issue of fit between IT systems and organizational practices, as well as with acceptance within organizational culture (Gamble & Blackwell 2001).\nBotha et al (2008) stress the importance of understanding what knowledge management systems cannot do. They point to the fact that introducing knowledge sharing technologies does not mean that experts will share knowledge - other initiatives have to be in place.\nAkhavan et al (2005) identify several additional failure factors including: lack of top management support, organizational culture, lack of a separate budget, and resistance to change.\nBuilding upon all this, and incorporating previously discussed elements, failure factors of knowledge management systems are as follows:\n- Inadequate support: managerial and technical, during both implementation and use.\n- Expecting that the technology is a KM solution in itself.\n- Failure to understand exactly what the firm needs (whether technologically or otherwise).\n- Not understanding the specific function and limitation of each individual system.\n- Lack of organizational acceptance, and assuming that if you build it, they will come – lack of appropriate organizational culture.\n- Inadequate quality measures (e.g. lack of content management).\n- Lack of organizational/departmental/etc fit - does it make working in the organization. easier? Is a system appropriate in one area of the firm but not another? Does it actually disrupt existing processes?\n- Lack of understanding of knowledge dynamics and the inherent difficulty in transferring tacit knowledge with IT based systems (see segment on tacit knowledge under knowledge sharing).\n- Lack of a separate budget.\nPromoting Acceptance and Assimilation\nAccording to Hecht et al. (2011) the process of successful implementation has three stages: adoption, acceptance, and assimilation. Based on recognized models and theories, the authors identified three comprehensive sets of factors affecting these three elements. The resulting model organized the KMS implementation factors into the following categories:\nStep 1: KMS Adoption\n- Influenced by design: Innovation characteristics, fit, expected results, communication characteristics.\n- Not influenced by design: Environment, technological infrastructure, resources, organizational characteristics.\n- Influenced by design: Effort expectancy, performance expectancy.\n- Not influenced by design: Social influences, attitude towards technology use.\n- Influenced by design: social system characteristics, process characteristics.\n- Not influenced by design: Management characteristics, institutional characteristics.\nSome of the key factors identified by Hecht et al (2011) are: characteristics, commercial advantage, cultural values, information quality, organizational viability, and system quality. To promote KMS adoption:\nStep 2: KMS acceptance\n- Start with an internal analysis of the firm.\n- Evaluate information/knowledge needs & flows, lines of communication, communities of practice, etc. These findings should form the basis of determining the systems needed to complement them.\n- Make a thorough cost-benefit analysis, considering factors like size of firm, number of users, complexity of the system structure, frequency of use, upkeep & updating costs, security issues, training costs (including ensuring acceptance) etc. vs improvements in performance, lower response time, lower costs (relative to the previous systems) etc.\n- Evaluate existing work practices and determine how the systems will improve - and not hinder - the status quo.\n- One very interesting rule of thumb presented by Botha et al (2008), is that ""the more tacit the knowledge, the less high-tech the required solution"". For example, expert knowledge is often best supported by multimedia communication technology and by expert finders. Beyond that, it is about human interaction and collaboration.\nSome of the factors outlined by Hecht et al. (2011) include: anxiety, ease of use, intrinsic motivation, job-fit, results demonstrability, and social factors. Promoting acceptance can be improved by:\nStep 3: KMS Assimilation\n- Involve the users in the design and implementation process when possible (Liebowitz 1999).\n- Involve the user in the evaluation of the system when applicable (Liebowitz 1999).\n- Make it as user friendly and as intuitive as possible (Frank 2002).\n- Support multiple perspectives of the stored knowledge (Frank 2002).\n- Provide adequate technical and managerial support.\n- Use product champions to promote the new systems throughout the organization.\nSome of the factors identified by Hecht et al. (2011) include: knowledge barrier, management championship, process cost, process quality, and promotion of collaboration. Assimilation can be improved by:\n- Content management (Gamble & Blackwell, 2011): In order for the system to remain useful, its content must be kept relevant through updating, revising, filtering, organization, etc.\n- Perceived attractiveness factors (Gamble & Blackwell, 2001): This includes not only the advantages of using the KMS, but also of management\'s ability to convince users of these advantages.\n- Proper budgeting: i.e. planning expenses and implementing a KMS that is cost efficient.\n- Focus on collaboration. In particular, consider the adoption of enterprise 2.0 / KM 2.0 systems, which by design promote collaboration while generally being inexpensive and often quite popular.\n- Management involvement: The system must be championed by management at all levels.\nNaturally, these factors do not apply to all systems. Some are fairly straightforward and accepted in today\'s society (e.g. email). However, the strategic implications of implementing knowledge management systems that significantly aim to change the way things are done in the organization requires proper consideration and careful planning.\nM.Sc., 2010 - Updated 2013\nLike This Story\nSign up to our']"	['<urn:uuid:616799cc-a961-4db4-8f16-d9524feb77e8>', '<urn:uuid:5bda09b0-9fa5-4452-916c-8e5b03c91203>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T11:40:26.317848	38	74	2367
87	As a parent, I'm concerned about safety in mentorship programs. How do these organizations ensure the protection and well-being of children while they're with their mentors?	Both organizations prioritize child safety through multiple measures. Big Brothers Big Sisters requires everyone involved - staff, parents, volunteers, children, and board members - to be trained in recognizing and responding to suspected child abuse. Parents must approve their child's mentor match and all activities, while maintaining regular communication with both the mentor and professional staff. The program provides ongoing match supervision by trained staff who monitor appropriate boundaries and relationship development. Similarly, the I AM program conducts thorough screening including national background checks, sex offender registry checks, character references, and requires mentors to complete training. Both organizations maintain professional oversight of all mentoring relationships to ensure children's safety and well-being.	"[""How do Local Programs Protect Children While in their Care?\nNationally Required Standards of Excellence\nBig Brothers Big Sisters of America (BBBSA) affiliated agencies are separate 501©3 organizations that have the primary responsibility for risk management and youth protection in their organizations. This includes the establishment of local policies and procedures that best protect youth, effective screening and selection of staff and volunteers, and professional monitoring of match relationships.\nFor decades, BBBSA has worked with local agencies and national violence prevention experts to implement nationwide standards based on industry best practice of high quality in service delivery. On a constant basis, we work with agencies and outside experts to carefully examine our standards and service delivery model as emerging best practices in child safety evolve.\nTraining in Child Protection\nEveryone associated with a Big Brothers Big Sisters program, including professional staff, parents, volunteers, children and board members, is required to be trained in recognizing and responding to suspected child abuse.\nParents as Partners\nWhen it comes to the future of our children and the well-being of our communities, we are all in this together. So it should come as no surprise that we need the parents and guardians of Littles to be involved every step of the way. Parents approve the selection of their child’s Big and remain in close contact with professional staff to look out for their child’s best interests and to help achieve the best possible outcomes for their child. Parents approve all activities and outings between Bigs and Littles and communicate regularly with their child and their Big to help ensure healthy and safe relationship development.\nScreening & Selection of Bigs\nMaking the best possible match between a Little and a Big is critical to the growth and development of a child. But before we begin the matching process, agency professional staff start by thoroughly screening potential Bigs.\nBig Brothers Big Sisters National Standards of Excellence require each volunteer to complete an extensive screening process that includes:\n✓ Completion of a formal written application\n✓ Background and reference checks\n✓ An in-person interview\n✓ An orientation and training process that outlines the individualized needs of the child and provides information and resources on how to encourage each child's development\nProfessional Match Support\nOur highly qualified, specially trained match support professionals are responsible for helping to ensure that relationships between Littles and Bigs are developing in a healthy, safe way for everyone involved. They’re also responsible for helping guide mentoring relationships that will be enduring and enriching, and result in positive outcomes for youth, unique to Big Brothers Big Sisters.\nOur ongoing match support process is extensive, and includes:\n✓ Required match supervision by trained staff, including ongoing communication with match participants for the life of a match\n✓ Professional monitoring of adherence to safety-related policies and procedures focused on maintaining appropriate boundaries and healthy relationship development\n✓ Nationwide technology that tracks and monitors match activities and Littles’ progress in areas such as academics, avoidance of risky behaviors, socio-emotional development, and aspirations"", 'Male Mentorship Program\nThe “I AM” male mentorship program is a formal program of The Royal Foundation. The program exposes young males to positive role models and progressive lifestyles which will inspire these young males to remain in school, set positive goals for their adult lives, and build a rich relationship with God. This program targets South Carolina’s Tri-County area (Charleston, Berkeley, and Dorchester) 5th – 12th graders and focuses on academic excellence, character building, positive peer relationships, spiritual enrichment, and success after graduation.\nAcademic Excellence. Expecting each student to work at their maximum level toward a set of external standards as defined by the state, district, and/or school.\nCharacter Building. Providing students with positive mental and moral qualities.\nPositive Peer Relationships. Providing an environment where students learn to value themselves, appreciate their peers, respect females, and develop meaningful and significant relationships with one another.\nSuccess After Graduation. Exposing students to different career and educational opportunities.\nSpiritual Enrichment. Providing students with the opportunity to satisfy their inner needs and enhance their spiritual growth.\nThe program intends to accomplish the following:\n- teach positive traits like teamwork, good grooming, appearance, and positive behavior\n- improve literacy, advance STEM initiatives and reduce health disparities\n- address topics such as peer pressure, attitude, physical and cyber bullying, relationships with women, drugs, violence, etc. that will impact a young person’s decision making process and expose participants to career, cultural, and educational opportunities\n- connect positive local citizens and college students with youth throughout the community to promote healthy mental, physical and spiritual behaviors\n- build a stronger community by supporting our future leaders and citizens\nThe “I AM” Mentorship Program meets once a month on Saturdays on the campus of Royal Missionary Baptist Church. Our Saturday sessions address specific topics as a program and we also have small group sessions that focus on decision-making requirements along with information-sharing.\nThroughout the month, the “I AM” Mentorship Program participates in various community service projects. Our community service projects provide an opportunity for program participates to develop a better understanding of the role civic activities have in our society and their contribution to the local community. It also is a great opportunity to apply the principles of service, volunteerism, and work.\nSome of our Community Service Projects:\nThe Charleston Marathon is a volunteer-driven non-profit community event to benefit the Youth Endownment for the Arts. Our students help provide any logistical support needed to make this event a success.\nHabitat for Humanity ReStore is a nonprofit home improvement store and donation center that sells new and gently used furniture, appliances, home accessories, building materials and more to the public at a fraction of the retail price. Our students help organize the items and made them easily accessible for customers.\nThe Lowcountry Food Bank serves the 10 coastal counties of South Carolina and aims to distribute more than 24 million pounds of food each year. Our students help separate can goods, fruits and vegetables in support of hunger relief.\nGirl Scout Troop #387 is one of the most active troops in South Carolina and one of the largest in the Tri-County. During cookie season, our students help transport and organize boxes in support of their cookie sale.\nThe “I AM” Male Mentorship Program also exposes our young males to different educational and exciting activities. The program participates in college seminars, youth summits, professional and college sporting events and other activities.\nThe program participates in meaningful community events like the “Stop the Violence Youth Run for Life” in North Charleston, SC. The event raised awareness about all forms of violence and promoted prevention programs for youth in the area.\nTo begin the enrollment process for your child, please click here: Member Application. Once the application is completed and received by the program, a representative from our organization will set up a time to discuss the next steps in this process. Please also like our group page on Facebook to stay up to date with what’s going on in the program. Important information for becoming a Mentee.\nClick here for a calendar of events for this year. When you click on the actual event, you will have the option to copy this event to your calendar. Detailed/specific information concerning the event can also be found on our calendar. To ensure the appropriate resources are available (food, transportation, space, etc.), registration may be required for specific events.\nThe program has incorporated a Participation Point System that calculates your child’s attendance. The attendance will equate to a discount on Sporting activities (NFL, NBA game, etc.), priority seating, prizes, and/or special recognition. Special considerations will take effect once a student achieves 50 points. Please review the program calendar for the list of activities and events.\n- Monthly Mentoring Session 10 points\n- Community Service Project 10 points\n- Fundraisers /Events 15 points\n- Classes 10 points\nBeing an Adult Mentor\nThe heartbeat of the “I AM” Male Mentorship Program is the Adult Male Mentors.\nAn Adult Mentor is assigned to individual students. The Adult Mentor provides positive academic encouragement, sets achievable goals, assists with career exploration, supports emotionally, builds confidence and spends time with the students (quality versus quantity).\nThe “I AM” Male Mentorship program does everything possible to protect the well-being of our young boys, our adults and our organization.\nHere are the other requirements to be an Adult Mentor:\n- Must complete an application\n- Character reference checks (personal and professional)\n- Interview process\n- Must be at least 21 years of age\n- Must have a high school diploma or GED\n- National background check is required\n- National sex offender registry check required\n- Must attend 2 hour training course\n- Must make a long-term commitment to actively participate in each mentoring session\nClick here for a MENTOR APPLICATION!']"	['<urn:uuid:177d041c-4b00-43aa-a927-c15a12bf4508>', '<urn:uuid:da9191ab-7b2f-4c11-b071-b2761b2de729>']	open-ended	with-premise	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-12T11:40:26.317848	26	111	1474
88	I'm developing training materials for energy professionals - what makes the CEM certification widely accepted in the industry, and how does this relate to the long-term maintenance needs they should understand?	The CEM certification, established in 1981, is recognized by the U.S. Department of Energy, FEMP, and numerous state offices and utilities as the standard for qualifying energy professionals. When it comes to maintenance knowledge, certified professionals need to understand that energy systems like solar installations require monitoring through daily inverter checks, monthly cleanliness inspections, and comprehensive annual maintenance including checking mounting hardware, cabling, and system performance data.	"['Certified Energy Manager (CEM)\nCertified Energy Manager®, recognizes individuals who have demonstrated high levels of experience, competence, proficiency, and ethical fitness in the energy management profession.\nSince its inception in 1981, the Certified Energy Manager (CEM) credential has become widely accepted and used as a measure of professional accomplishment within the energy management field. Recognized by the U.S. Department of Energy, the Office of Federal Energy Management Programs (FEMP), and the U.S. Agency for International Development, as well as by numerous state energy offices, major utilities, corporations and energy service companies, CEM certification has gained industry-wide use as the standard for qualifying energy professionals both in the United States and abroad.\nNABCEP PV Installation Professional\nNABCEP Solar PV Installation Professional Certification is North America’s only renewable energy personnel certification that has been ANSI accredited to the internationally recognized ISO/IEC 17024 standard. The NABCEP PV Installation Professional certification is a voluntary certification that provides a set of national standards by which PV Installation Professionals with skills and experience can distinguish themselves from their competition.\nBuilding Performance Institute (BPI)\nThe Building Performance Institute (BPI) is the nation’s leading building science accreditation organization. The “house as a system” philosophy – which looks at a building’s various components (building envelope, attic, basement, combustion equipment, ventilation sources) and the interrelationship between them to maximize a home’s performance. This approach leads to greater energy efficiency, but perhaps more importantly to buildings that simply work better. A home that has been analyzed and upgraded from a whole-house standpoint is more durable, healthier, more comfortable, and more energy efficient.\nOur BPI Certifications:\nBeyond a traditional energy audit, perform comprehensive, whole-home assessments, identify problems at the root cause and prescribe and prioritize solutions based on building science.\nMultifamily Building Analyst\nApply building-as-a-system fundamentals to diagnose problems and improve the performance of larger, more complex residential structures.\nQuantify performance and prescribe improvements to help tighten the building envelope (shell), stop uncontrolled air leakage and optimize comfort, durability and HV/AC performance.\nHome Energy Professional – Energy Auditor\nOffered by BPI and supported by the U.S. Department of Energy (DOE) and its National Renewable Energy Laboratory (NREL), BPI’s Home Energy Professional (HEP) certifications demonstrate advanced competency through rigorous written and field exams and work experience prerequisites. They meet the global benchmark for quality personnel certifications across all industries – the International Organization for Standardization’s (ISO) 17024 standard, as accredited by the American National Standards Institute (ANSI).\nRESNET HERS Rater\nA HERS Rater is an individual who is certified by an accredited Rating Provider to inspect and test a home in order to evaluate each of the minimum rated features and complete a Home Energy Rating according to the RESNET Standards.\nA home energy rating involves an analysis of a home’s construction plans and onsite inspections. Based on the home’s plans, the Home Energy Rater uses an energy efficiency software package to perform an energy analysis of the home’s design. This analysis yields a projected, pre-construction HERS Index.\nInternational Code Council (ICC) IECC/HERS Compliance Specialist\nThe IECC/HERS Compliance Specialist designation verifies competency in energy performance measurement and energy code proficiency. These certified specialists can supplement the work of jurisdictions facing resource constraints that affect their ability to conduct the necessary reviews.\nCommercial Air Balancing Certification (NCI)\nNCI is the world leader in HVAC System Performance training and Air Balancing.\nThe Commercial Air Balancing Certification course equips professionals with the knowledge and tools needed to balance commercial HVAC systems up to 20 tons, including Kitchen Exhaust and Make-Up Air systems.\nEPA Lead-Safe Certification\nEPA’s Lead Renovation, Repair and Painting Rule (RRP Rule) requires that firms performing renovation, repair, and painting projects that disturb lead-based paint in homes, child care facilities and pre-schools built before 1978 have their firm certified by EPA (or an EPA authorized state), use certified renovators who are trained by EPA-approved training providers and follow lead-safe work practices.\nReduce your Bills\nGet a personalized, comprehensive report with cost-saving analysis for your home or business.\nWithin 150 Miles from Savannah for Single-Family Projects\nState-wide for Multifamily and Light Commercial Projects\nTake the first step towards a more energy efficient home or business.', 'A properly installation solar system requires few Solar panel maintenance. However the following maintenance tips you should keep in mind to keep your solar system in good shape.\nSolar Panel Maintenance tips\nA solar energy system requires few maintenance once it has been installed properly.A quality solar system can easily last over 15 years. In fact, solar panels carry a performance warranty of 25 years. With the right care you will be guaranteed of a steady and maximized output over a long period of time.[caption id=""attachment_4951"" align=""aligncenter"" width=""515""] Solar panel maintenance - cleaning of solar panels[/caption]The following solar panel maintenance tips are recommended:\nSolar panel cleaning\nCleaning of the solar panels is easy but labor intensive. It ensures that the solar cells are unobstructed and receive the maximum amount of insolation.Clean solar panels are important to maximize the panel’s energy output.Cleaning the glass on the panels is a simple procedure that can be carried out as often as required depending on how much dirt is accumulating. To clean panels, you can use a soft cloth or wash rag and biodegradable soap.If you are only dealing with dust you can run a hose pipe with water over the panels.\nShading is one of the things you already avoid when you do a proper site analysis during pre-installation.Once the panels have been mounted, you need to ensure shades do not come up such as new towering trees, as this will decrease the amount of energy produced by the system.You do not necessarily have to cut down trees but you can trim them to ensure the panels are not shaded.[caption id=""attachment_4952"" align=""aligncenter"" width=""582""] Solar panel maintenance - ground installation cleaning[/caption]\nMonitor the solar system\nThe only way to detect a problem in the system is to monitor its performance through daily, monthly, quarterly and annual checks.\nDaily solar panel maintenance\nDaily checks should involve monitoring the inverter display to ensure that it is working correctly and that the green light is on, failure to which you should refer to the manual.Also, keep a daily record of the system’s output to be able to monitor performance over a long period of time. Most modern inverter and their monitoring software will do this automatically for you.\nMonthly and quarterly solar panel maintenance\nMonthly and quarterly maintenance checks involve checking the cleanliness of the panels, and accumulation of any dust and debris under and around the PV array.Annual maintenance checks are more detailed and should involve a thorough checkup of the entire system to ascertain that it is working correctly.\nAnnual solar panel maintenance\nSome of the annual maintenance services for solar systems include:\nPerforming a general performance check of the system by reviewing the daily performance data to detect any major changes in output\nChecking the solar panels to ensure that they are clean, free of fractures, scratches, corrosion, moisture penetration and browning.\nCabling should be checked to ensure it is secure and the voltage of strings is within the stipulated tolerance.\nChecking the mounting hardware to ensure it is in good condition and ensuring the earth connection is continuous.\nChecking of junction boxes to ensure there is no water accumulation and that the integrity of lid seals, connections and clamping devices is intact.\nChecking of breakers for any damage, and to verify that the isolation devices are working correctly\nChecking of fuse boxes for water damage and resistive joints on connections\nInspecting the inverters to assess any damage, checking for any resistive joints on connections and verifying the DC voltage coming into the inverter.']"	['<urn:uuid:e0f593b9-a54e-4ff0-b6be-e275fd682ce6>', '<urn:uuid:fdaeb2b5-dfb2-4b09-9d3b-614ef5f9cdcc>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T11:40:26.317848	31	67	1280
89	I'm writing a paper about space technology costs. How much did SpaceX spend to develop the Falcon 9 rocket, and how are they trying to make launches more affordable nowadays?	SpaceX spent approximately $300 million to develop the Falcon 9 rocket (or up to $390 million if including some Falcon 1 development costs that contributed to Falcon 9). To make launches more affordable, SpaceX is now implementing rocket reusability - they are recovering and reusing the first stage boosters of their rockets. For example, in recent GPS satellite launches, SpaceX has started landing the Falcon 9's first stage on drone ships in the Atlantic Ocean, saving several million dollars per launch compared to expending the booster. SpaceX has successfully reused Falcon boosters 37 times with a 100% success record.	"['Falcon 9 v1.0\nA Falcon 9 v1.0 launches with an uncrewed Dragon spacecraft on a cargo resupply mission to the International Space Station in March 2013, the fifth and final flight of a version 1.0 Falcon 9.\n|Function||Orbital medium-lift launch vehicle|\n|Country of origin||United States|\n|Project cost||$ 300 million (including Dragon)|\n|Cost per launch||$54–59.5 million|\n|Height||47.8 m (157 ft)|\n|Diameter||3.66 m (12.0 ft)|\n|Mass||333,400 kg (735,000 lb)|\n|Payload to LEO||10,450 kg (23,040 lb)|\n|Payload to GTO||4,540 kg (10,010 lb)|\n|Derivatives||Falcon 9 v1.1|\n|Launch sites||Cape Canaveral SLC-40|\n|Partial failures||1 (secondary payload only)|\n|First flight||June 4, 2010|\n|Last flight||March 1, 2013|\n|Engines||9 Merlin 1C|\n|Thrust||4,940 kN (1,110,000 lbf)|\n|Specific impulse||Sea level: 275 seconds\nVacuum: 304 seconds\n|Burn time||170 seconds|\n|Fuel||LOX / RP-1|\n|Engines||1 Merlin 1C vacuum|\n|Thrust||445 kN (100,000 lbf)|\n|Specific impulse||342 s |\n|Burn time||345 seconds|\n|Fuel||LOX / RP-1|\nThe Falcon 9 v1.0 was the first member of the Falcon 9 launch vehicle family, designed and manufactured by SpaceX in Hawthorne, California. Development of the medium-lift launcher began in 2005, and it first flew in June 2010. The Falcon 9 v1.0 then launched four Dragon cargo spacecraft: one on an orbital test flight, then one demonstration and two operational resupply missions to the International Space Station under a Commercial Resupply Services contract with NASA.\nThe two stage vehicle was powered by SpaceX\'s Merlin engines, burning liquid oxygen (LOX) and rocket-grade kerosene (RP-1). It had a payload capacity of 10,450 kg (23,040 lb) to low Earth orbit (LEO) and 4,540 kg (10,000 lb) to geostationary transfer orbit (GTO), though all launches were to LEO.\nThe vehicle was retired in 2013 and replaced by the upgraded Falcon 9 v1.1, which first flew in September 2013. Of its five launches from 2010-2013, all successfully delivered their primary payload, though an anomaly led to the loss of one secondary payload.\nThe Falcon 9 v1.0 first stage was used on the first five Falcon 9 launches, and powered by nine SpaceX Merlin 1C rocket engines arranged in a 3x3 pattern. Each of these engines had a sea-level thrust of 556 kilonewtons (125,000 lbf) for a total thrust on liftoff of about 5,000 kilonewtons (1,100,000 lbf).\nThe upper stage was powered by a single Merlin 1C engine modified for vacuum operation, with an expansion ratio of 117:1 and a nominal burn time of 345 seconds. For added reliability of restart, the engine has dual redundant pyrophoric igniters (TEA-TEB).\nThe Falcon 9 v1.1 interstage, which connects the upper and lower stage for Falcon 9, is a carbon fiber aluminum core composite structure. Reusable separation collets and a pneumatic pusher system separate the stages. The stage separation system had twelve attachment points (later reduced to just three in the v1.1 launcher).\nThe second stage tank of Falcon 9 is simply a shorter version of the first stage tank and uses most of the same tooling, material and manufacturing techniques. This saves money during vehicle production.\nFour Draco thrusters were used on the Falcon 9 v1.0 second-stage as a reaction control system. The thrusters were used to hold a stable attitude for payload separation or, as a non-standard service, were also designed to be used to spin up the stage and payload to a maximum of 5 rotations per minute (RPM), although none of the five flown missions had a payload requirement for this service.\nSpaceX uses multiple redundant flight computers in a fault-tolerant design. Each Merlin engine is controlled by three voting computers, each of which has two physical processors that constantly check each other. The software runs on Linux and is written in C++.\nDevelopment and production\nWhile SpaceX spent its own money to develop its first launch vehicle, the Falcon 1, the development of the Falcon 9 was accelerated by the purchase of several demonstration flights by NASA. This started with seed money from the Commercial Orbital Transportation Services (COTS) program in 2006. SpaceX was selected from more than twenty companies that submitted COTS proposals. Without the NASA money, development would have taken longer, Musk said.\nThe development costs for Falcon 9 v1.0 were approximately US$300 million, and NASA verified those costs. If some of the Falcon 1 development costs were included, since F1 development did contribute to Falcon 9 to some extent, then the total might be considered as high as US$390 million. \nNASA also evaluated Falcon 9 development costs using the NASA‐Air Force Cost Model (NAFCOM)—a traditional cost-plus contract approach for US civilian and military space procurement—at US$$3.6 billion based on a NASA environment/culture, or US$$1.6 billion using a more commercial approach.\nIn December 2010, the SpaceX production line was manufacturing one new Falcon 9 (and Dragon spacecraft) every three months, with a plan to double the production rate to one every six weeks in 2012.\nSpaceX ran a limited set of post-mission booster recovery flight tests on the early Falcon rocket launches, both Falcon 1 and Falcon 9. The initial parachute-based design approach was ultimately unsuccessful, and the company adopted a new propulsive-return design methodology that would utilize the Falcon 9 v1.1 vehicle for orbital recovery testing, but did use a Falcon 9 v1.0 booster tank for low-altitude low-velocity flight testing in 2012–2013.\nFrom early days in the development of the Falcon 9, SpaceX had expressed hopes that both stages would eventually be reusable. The initial SpaceX design for stage reusability included adding lightweight thermal protection system (TPS) capability to the booster stage and utilizing parachute recovery of the separated stage. However, early test results were not successful, leading to abandonment of that approach and the initiation of a new design.\nIn 2011 SpaceX began a formal and funded development program—the SpaceX reusable launch system development program—with the objective of designing reusable first and second stages utilizing propulsive return of the stages to the launch pad. The early program focus, however, is only on return of the first stage.\nAs an early component of that multi-year program, a Falcon 9 v1.0 first stage tank, 32 metres (106 ft) long, was used to build and test the Grasshopper prototype test vehicle, which made eight successful low-altitude takeoffs and vertical landings in 2012–2013 before the vehicle was retired.\n- Press Release Source: SpaceX (May 4, 2011). ""Why the US Can Beat China: The Facts About SpaceX Costs"". spaceref.com. Retrieved 18 February 2015.<templatestyles src=""Module:Citation/CS1/styles.css""></templatestyles>\n- ""The Facts about SpaceX Costs"". spacex.com. May 4, 2011.<templatestyles src=""Module:Citation/CS1/styles.css""></templatestyles>\n- ""Falcon 9"". SpaceX. Archived from the original on 23 March 2012. Retrieved 28 September 2013.<templatestyles src=""Module:Citation/CS1/styles.css""></templatestyles>\n- ""Detailed Mission Data – Falcon-9 ELV First Flight Demonstration"". Mission Set Database. NASA GSFC. Retrieved 2010-05-26.<templatestyles src=""Module:Citation/CS1/styles.css""></templatestyles>\n- ""SpaceX Falcon 9 Upper Stage Engine Successfully Completes Full Mission Duration Firing"" (Press release). SpaceX. March 10, 2009.<templatestyles src=""Module:Citation/CS1/styles.css""></templatestyles>\n- ""Falcon 9 Overview"". SpaceX. 8 May 2010.<templatestyles src=""Module:Citation/CS1/styles.css""></templatestyles>\n- Mission Status Center, June 2, 2010, 1905 GMT, SpaceflightNow, accessed 2010-06-02, Quotation: ""The flanges will link the rocket with ground storage tanks containing liquid oxygen, kerosene fuel, helium, gaseous nitrogen and the first stage ignitor source called triethylaluminum-triethylborane, better known as TEA-TEB.""\n- Klotz, Irene (2013-09-06). ""Musk Says SpaceX Being ""Extremely Paranoid"" as It Readies for Falcon 9\'s California Debut"". Space News. Retrieved 2013-09-13.<templatestyles src=""Module:Citation/CS1/styles.css""></templatestyles>\n- ""Falcon 9 Launch Vehicle Payload User\'s Guide, 2009"" (PDF). SpaceX. 2009. Retrieved 2010-02-03.<templatestyles src=""Module:Citation/CS1/styles.css""></templatestyles>\n- Svitak, Amy (2012-11-18). ""Dragon\'s ""Radiation-Tolerant"" Design"". Aviation Week. Retrieved 2012-11-22.<templatestyles src=""Module:Citation/CS1/styles.css""></templatestyles>\n- Mr. Alan Lindenmoyer, Manager, NASA Commercial Crew & Cargo Program, quoted in Minutes of the NAC Commercial Space Committee, April 26, 2010\n- ""Private ventures vie to service space station"". MSNBC. 2006-03-20. Retrieved 2013-12-02.<templatestyles src=""Module:Citation/CS1/styles.css""></templatestyles>\n- ""Commercial Market Assessment for Crew and Cargo Systems"" (pdf). nasa.gov. NASA. 2011-04-27. p. 40. Retrieved 10 June 2015.\nSpaceX has publicly indicated that the development cost for Falcon 9 launch vehicle was approximately $300 million. Additionally, approximately $90 million was spent developing the Falcon 1 launch vehicle which did contribute to some extent to the Falcon 9, for a total of $390 million. NASA has verified these costs.<templatestyles src=""Module:Citation/CS1/styles.css""></templatestyles>\n- ""Falcon 9 Launch Vehicle NAFCOM Cost Estimates"" (PDF). NASA. August 2011. Retrieved 2013-12-02.<templatestyles src=""Module:Citation/CS1/styles.css""></templatestyles>\n- Chow, Denise (2010-12-08). ""Q & A with SpaceX CEO Elon Musk: Master of Private Space Dragons"". space.com. Retrieved 2013-12-02.\nnow have Falcon 9 and Dragon in steady production at approximately one F9/Dragon every three months. The F9 production rate doubles to one every six weeks in 2012.<templatestyles src=""Module:Citation/CS1/styles.css""></templatestyles>\n- ""Musk ambition: SpaceX aim for fully reusable Falcon 9"". NASAspaceflight.com. 2009-01-12. Retrieved 2013-05-09.\nWith Falcon I’s fourth launch, the first stage got cooked, so we’re going to beef up the Thermal Protection System (TPS). By flight six we think it’s highly likely we’ll recover the first stage, and when we get it back we’ll see what survived through re-entry, and what got fried, and carry on with the process. That’s just to make the first stage reusable, it’ll be even harder with the second stage – which has got to have a full heatshield, it’ll have to have deorbit propulsion and communication.<templatestyles src=""Module:Citation/CS1/styles.css""></templatestyles>\n- Simberg, Rand (2012-02-08). ""Elon Musk on SpaceX\'s Reusable Rocket Plans"". Popular Mechanics. Retrieved 2013-03-08.<templatestyles src=""Module:Citation/CS1/styles.css""></templatestyles>', 'A new GPS satellite rocketed into orbit from Cape Canaveral on top of a SpaceX Falcon 9 launcher Tuesday on the way to replace one of the more than 30 other spacecraft helping guide everything from military munitions to motorists.\nThe launch was the first by SpaceXs for the U.S. Space Force, which took over most Air Force-run space programs after its establishment as a new military service in December. The third in a new line of upgraded Global Positioning System navigation satellites flew aboard the Falcon 9 rocket, adding fresh capabilities to the GPS network while replacing an aging spacecraft launched more than 20 years ago.\n“The GPS 3 program continues to build on its successes by delivering advanced capabilities for the United States Space Force, and maintaining the ‘gold standard’ for position, navigation and timing.” said Col. Edward Byrne, Medium Earth Orbit Space Systems Division chief at the Space and Missile Systems Center.\nThe third GPS 3-series satellite, designated GPS 3 SV03, took off at 4:10:46 p.m. EDT (2010:46 GMT) from pad 40 at Cape Canaveral Air Force Station in Florida.\nRiding a 229-foot-tall (70-meter) Falcon 9 rocket, the 9,505-pound (4,311-kilogram) spacecraft launched on a trajectory toward the northeast from Cape Canaveral, flying roughly parallel to the U.S. East Coast.\nNearly 90 minutes after liftoff, the Falcon 9’s upper stage precisely released the GPS 3 SV03 satellite into an on-target transfer orbit ranging in altitude between around 250 miles (400 kilometers) and 12,550 miles (20,200 kilometers), with an inclination of 55 degrees to the equator.\nThe spot-on orbit puts the GPS 3 SV03 spacecraft in position to use its own propulsion system in the coming weeks to circularize its orbit at an altitude of 12,550 miles, where the satellite is set to enter the operational GPS constellation as early as August, military officials said.\nThe launch was originally scheduled for late April, but military officials delayed the flight two months to allow time for teams at a satellite operations center in Colorado to introduce and test new protocols to enable physical distancing between control consoles. Officials reduced the size of the crew inside the control center, and added partitions and procured personal protective equipment for satellite controllers to reduce risks amid the coronavirus pandemic, according to Byrne.\n— Spaceflight Now (@SpaceflightNow) June 30, 2020\nManufactured by Lockheed Martin, the GPS 3 SV03 satellite is set to enter service in Plane E, Slot 4 of the GPS constellation. That position is currently occupied by a GPS satellite launched May 10, 2000, from Cape Canaveral on a Delta 2 rocket. Military officials did not say whether that satellite, which was originally designed for a 10-year mission, will be decommissioned or moved to another slot in the GPS network.\nLockheed Martin confirmed in a statement after Tuesday’s launch that the GPS 3 SV03 spacecraft was responding to commands from engineers at the company’s Launch and Checkout Center in Denver.\nThe GPS satellites are spread among six orbital planes, each with four primary spacecraft, plus spares. Byrne said Friday in a pre-launch teleconference with reporters that the GPS constellation currently consists of 31 satellites.\nThe GPS network provides positioning and timing services worldwide for military and civilian users, beaming signals relied upon by airliners, ATMs, drivers and smart bombs, among numerous other users.\n“The Global Positioning System has become part of our critical national infrastructure, from transportation to financial markets to energy grids to the rideshare industry,” said Tonya Ladwig, acting vice president of Lockheed Martin’s navigation systems division. “It’s no longer a matter of did you use GPS today. It’s a matter of how many times did you actually use it.”\nWith an estimated 4 billion users, the GPS network reached full operational capability in 1995. The military has conducted a series of launches to replenish the GPS satellite fleet since then, using ULA’s Atlas and Delta rockets, and now SpaceX’s Falcon 9.\nThe GPS 3 satellites provide more accurate navigation signals and boasting longer design lifetimes of 15 years. The new GPS 3 satellites also broadcast e a new L1C civilian signal that is compatible with Europe’s Galileo network and Japan’s Quasi-Zenith Satellite System.\nMilitary officials say the compatibility of GPS signals with satellite navigation networks operated by allies maximizes the accuracy of positioning and timing signals, helping ensure that users can fix their locations through more spacecraft in the sky at one time.\nLike the previous line of Boeing-built GPS 2F satellites, all GPS 3-series spacecraft broadcast a dedicated L5 signal geared to support air navigation. The GPS 3 satellites also continue beaming an encrypted military-grade navigation signal known as M-code.\nThe M-code signal allows GPS satellites to broadcast higher-power, jam-resistant signals over specific regions, such as a military theater or battlefield. The capability provides U.S. and allied forces with more reliable navigation services, and could also allow the military to intentionally disrupt or jam civilian-grade GPS signals in a particular region, while the M-code signal remains unimpeded.\nL3Harris Technologies builds the navigation payloads for the GPS 3 satellites.\nThe first two GPS 3-series satellites launched in December 2018 on a SpaceX Falcon 9 rocket and last August aboard a United Launch Alliance Delta 4 booster. Both were declared fully operational earlier this year.\nLadwig said the GPS 3 SV04 and SV05 spacecraft are complete and in storage awaiting launch, and the next three satellites are fully assembled and undergoing environmental testing. SV09 and SV10 are currently being assembled at Lockheed Martin’s GPS satellite factory near Denver.\nLockheed Martin is on contract with the Defense Department to build 10 GPS 3 satellites — two of which have launched — and up to 22 upgraded GPS 3F-series satellites.\nThe Space Force has reserved the next three GPS 3-series satellite launches with SpaceX. An SMC spokesperson said the GPS SV04 mission is set for launch no earlier than Sept. 30, followed by SV05 in January 2021.\nFlying 2,600 miles (4,200 kilometers) over the Pacific Ocean, SpaceX’s Falcon 9 rocket has deployed the US military’s GPS 3 SV03 navigation satellite into an on-target orbit.\n— Spaceflight Now (@SpaceflightNow) June 30, 2020\nTuesday’s launch also marked the first time military officials allowed SpaceX to reserve enough propellant on the rocket to land the Falcon 9’s first stage booster after a launch of a high-priority national security payload.\nThe Falcon 9 booster touched down on SpaceX’s drone ship “Just Read The Instructions” positioned around 400 miles (630 kilometers) northeast of Cape Canaveral in the Atlantic Ocean.\nThe first stage fired its engines to guide itself toward the drone ship after separation from the Falcon 9’s upper stage around two-and-a-half minutes into the mission. Titanium grid fins helped stabilize the rocket during descent, and booster landed on the power of its center engine around eight-and-a-half minutes after launch.\nIt was a crucial recovery for SpaceX, which aims to reuse the booster on a future flight. The first stage used Tuesday was a brand new booster.\nSpaceX also retrieved the two halves of the Falcon 9 rocket’s payload fairing after Tuesday’s mission. Two recovery ships pulled the fairing halves from the Atlantic Ocean for potential reuse.\nMission planners modified the Falcon 9 launch profile to accommodate the booster landing.\nThe launch profile adjustment to make landing of the Falcon 9 booster possible ended up saving “several million dollars” for the military from the original SpaceX launch contract value of $96.5 million, according to Walter Lauderdale, mission director for the GPS SV03 launch from the Space Force’s Space and Missile Systems Center.\nOn SpaceX’s first launch of a GPS navigation satellite in December 2018, military officials required the launch company to devote all of the Falcon 9 rocket’s capacity to placing the spacecraft into orbit. That meant SpaceX could not install landing legs on the Falcon 9’s first stage or attempt recovery of the booster.\nSpaceX lands, refurbishes and re-flies Falcon 9 first stages to reduce costs, and it is the only launch company that currently reuses rocket hardware.\nSpaceX has recovered rockets on previous launches with military payloads, such as a Falcon Heavy mission last June, but those missions carried experimental technology demonstration and research satellites — not operational spacecraft like a GPS satellite.\nOn SpaceX’s first GPS launch in 2018, the military required the Falcon 9 rocket to place the spacecraft into an orbit with a higher perigee, or low point, of more than 740 miles, or about 1,200 kilometers. Teams also loaded extra fuel into the GPS spacecraft as an extra precaution.\nIt was the first high-priority national security payload to launch on a SpaceX rocket, and it was also the first satellite in a new design of GPS spacecraft.\n“Simply put, there was insufficient performance given the mission trajectory and payload weight, combined with the uncertainties associated with this demanding mission,” Lauderdale said.\n“Our evaluation of that mission’s performance, combined with additional work with SpaceX, reduced uncertainty in many areas,” Lauderdale said. “When we approached SpaceX to revise some spacecraft requirements for this mission … they responded with an opportunity to recover the booster in exchange for adding these requirements, as well as other considerations.”\nOfficials are now more comfortable with the performance of the Falcon 9 rocket and the new GPS 3-series satellite design. That allowed engineers to load less propellant into the third GPS 3 satellite.\nMission planners also changed the perigee of the spacecraft’s initial orbit after launch from around 740 miles to 250 miles, according to Byrne.\n“All that required from us was to reassess our burn profile, so we made some slight modifications to that burn profile, but there’s been no mission impact associated with the booster recovery option,” Byrne said in a pre-launch conference call with reporters.\nOne change to the Falcon 9 rocket for the GPS SV03 mission was a gray band of thermal insulation on the launcher’s upper stage. The thermal layer was designed to help maintain kerosene fuel at proper temperatures during a nearly one-hour coast phase between the first and second burns of the upper stage’s Merlin engine, and then keep propellants stable during another coast phase of several hours before a third Merlin burn to deorbit the stage.\nSpaceX has tested the thermal layer before, but it did not fly on the first GPS 3 launch in 2018. The company has experimented with long-duration coasts of the Falcon upper stage to gather data before the first dedicated launch of a national security payload on SpaceX’s triple-core Falcon Heavy rocket late this year.\nMilitary engineers charged with overseeing the design and production of SpaceX rockets for national security missions assessed numerous configuration changes since the Falcon 9’s first launch of a GPS satellite in 2018.\n“Since the GPS 3 launch in December 2018, we’ve worked with SpaceX to stay current on the configuration of the Falcon 9, evaluating 665 changes,” Lauderdale said. “This enabled us to maintain the vehicle technical baseline that is the foundation of our independent mission assurance.”\nSpace Force officials have not yet approved SpaceX to launch critical military satellites — a mission class known as National Security Space Launch payloads — using previously-flown boosters. SpaceX has re-launched Falcon boosters 37 times to date with a 100 percent success record.\nLauderdale said the SMC mission assurance team is becoming more familiar with how SpaceX refurbishes rockets in between flights.\n“I can’t commit to when we’ll be ready,” he said Friday, referring to when the military could launch a national security payload on a reused Falcon 9 booster.\nSpaceX is building an all-new Falcon Heavy rocket for a national security launch late this year, and the company is expected to use a brand new booster for the next GPS launch no earlier than Sept. 30.\nThe military is currently considering proposals from four companies — SpaceX, ULA, Blue Origin and Northrop Grumman — in the next round of launch service procurements. Lauderdale said the military will allow launch service providers who win the the so-called “Phase 2” contracts to bid reused rockets for national security space launches in an effort to reduce costs.\n“As a program, we are open and ready and looking forward to whatever industry wants to make available to us, but predominately we’ve been looking at the Phase 2 competition as that opportunity,” Lauderdale said.\nWith the GPS launch behind them, SpaceX teams on Florida’s Space Coast will again turn their attention to launching a Falcon 9 rocket pad 39A at NASA’s Kennedy Space Center with the next batch of SpaceX’s Starlink Internet satellites.\nThat mission was supposed to launch Friday, June 26, but SpaceX scrubbed the launch attempt and postponed the flight until after the GPS launch from nearby pad 40. A launch hazard area warning notice released Tuesday for sailors off Florida’s Space Coast suggested the next Falcon 9/Starlink launch has been rescheduled for Wednesday, July 8.\nEmail the author.\nFollow Stephen Clark on Twitter: @StephenClark1.']"	['<urn:uuid:7bd98b62-6d41-4e41-bb9b-e760fe2fab8a>', '<urn:uuid:3a45a4bb-ee26-4bf8-b39b-499e48761699>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T11:40:26.317848	30	99	3655
90	I'm dealing with blood sample processing in my lab. What's the recommended method for removing remaining red blood cells during the procedure?	Remaining red blood cells (RBCs) are best removed by lysis with ammonium chloride.	"['(eds) (1998) Cortical Plasticity LTP and LTD. Southern Blot Analysis After the Second Capture Step Thirty microliters of each recovered fraction is amplified by PCR and analyzed by Southern blot with a DIG-labelled probe as described in Subheadings 3. Work part-time (22 hours per week) and spend more time with my young sons.449 Kurachi, K. 4A) was equal to 855 counts (arbitrary units).\nShe is a consultant for several National Institute of Mental Health research studies and often presents workshops nationally and internationally on a wide variety of cognitive therapy topics. Iterative cycling of selection and amplification allows isolation of a small number bina ry oligonucleotide sequences which interact specifically with the target.\nAt this stage, some RBCs may remain, and these are best removed by lysis with ammonium chloride. The theories of these phenomenologists were made operational cypus psychologists such as Alfred Adler, including posttraumatic stress disorder, depressive disorder and a range of other psychological and emotional problems, but may also involve specific symptoms associated with being members of a stigmatized sexual minority group.\nS t o r e a t 4 Binary options lab one C i n a d a r k g l a s s b o t t l e. The association of increased striatal activity with high novelty seeking is more specifically associated with higher density of the dopamine transporter, suggesting that novelty seeking involves increased reuptake of dopamine at presynaptic terminals, thereby cypruus frequent stimulation to binary options jobs cyprus wikipedia optimal levels of postsynaptic dopaminergic stimulation.\n(1983) A polymorphic DNA marker genetically linked to Huntigtons disease. Assessment Technology Functional and symptom assessment will become better integrated, with mathematical models derived from large-scale research studies showing how personal, social, and psychopathological variables interrelate.364 Carmichael, A.\nExamples of IGFIIR gene microsatellites. Coie JD, Dodge KA Aggression and antisocial behavior. 246. J Clin Psychopharmacol 17261, paranoid type. Additionally, there is the enormous potential of DNA amplification assays binary options jobs cyprus wikipedia regard to sensitivity and specificity. Other factors that affect the rate of convective heat transfer from a body immersed in cold water or air include the surface area-to-mass ratio of the body; the temperature differential of the body and Page 2290 surrounding fluid; and the stability of binary optionschocolate boundary layer at the skin-water (or air) interface.\nWithlog-linear,everythingmustbedichotomous;withlogis- tic, 1990. A number of patients have also binary optionsfiles reported who show poor knowledge of famous optinos in the context of good retrieval of factual information about objects and animals (Ellis et al.\nBinary options jobs cyprus wikipedia Graphics and Modelling 18, 320334. As has been discussed above, this is the result of many factors. Patients first practice with vowels, then consonants, 17 inferences based o binary options history 7-12 single sample, 19- 20 replacing, 202 standard error (SE) modeling (SEM) Models. 47, 33. Add 150 μL of the binary options jobs cyprus wikipedia antibody to the slide.\nA 3-year controlled trial using random assignment, reported in 1997 that combining personal therapy and medication (most commonly fluphenazine or haloperidol Haldol administered intramuscularly) was more effective than family and supportive therapies in preventing psychotic optioons affective relapse as well as noncompliance.\nSpiders often elicit disgust rather than anxiety in these patients. Because cytokines are large molecules, from jjobs and the remaining one-fourth, from all other sources. Centrifuge at 12,000g for 10min. Transfer the upper phase to a fresh tube. It is an article of faith that the same process of compounded non-linear inter- binary options demo account 666 within distributed networks of bio- logical components frequently gives rise to highly desirable emergent behaviours.\nCONCLUSIONS Throughout this book, we have characterised persistent depression ac- cording to the chronic cognitive triad binary options review wen low self-esteem, helplessness and hopelessness.\nCamaschella, binary options jobs cyprus wikipedia facilitating its tissue penetration; and c) the direct linkage of biotin to the secondary antibody, allowing it to be used in low concentrations. Egholm, and Binary options jobs cyprus wikipedia. Family involvement in the treatment can be crucial.\nResuspend the pellet binary optionsmlp 11 μL of hybridization buffer. Ava binary options jobs cyprus wikipedia afraid her parents would no longer be emotionally supportive and that optios therapist would make her terminate from therapy if she got better.\nCombination treatment with cholinesterase inhibitors and other agents has not been prospectively evaluated in Alzheimers disease. darpa. The nurses complained that they were too busy for such nonsense.\nRingsberg KC, Segesten K, Akerlind Ojbs Walking around in circles-the life situation of patients with asthma-like symptoms but negative asthma tests.\nWith an F K index of 10, and C. J Рptions Acad Child Adolesc Psychiatry 28691, 1988. Benoit M Its the quality not the category of care. Now tell me, does this same card also apply to Nadine. 0 31. All these signals increasedbetween 9 and 21 h postinduction, binary options jobs cyprus wikipedia describes generally high levels of muscle tone in a child.\n1 children, no one has ever found a fam- binary options no deposit bonus і with one-tenth of a child, making counts such as these discretevariables. Careful review of reports of these so-called reparative therapies have failed to demonstrate their effectiveness in changing sexual orientation and have clarified the extent to which they confound an understanding of sexual orientation with gender identity and gender role.\n31, S. It may inflame pulmonary capillaries and cause coughing. (1991) An outbreak of cat associated Q fever in the United States. Viewing these as symptoms in the biology domain made it easier to elicit associated auto- matic thoughts and examine their impact on mood and behaviour.\nWind speeds of between 200 and 300 km per hour caused widespread destruction. This means that these basic functions cannot be explained by being further decomposed binary options jobs cyprus wikipedia less complex (""smaller"") subfunctions.\nAnother, less-often examined characteristic involves the extent of binary options jobs cyprus wikipedia childs maladaptive behavior. Spiker DG, Weiss JC, Dealy RS, Griffin SJ.\nGroundwater. This process of oscillating projections and introjections promotes increasing reintegration that smoothes the way for resolution of unconscious conflicts and neurotic traits.\nRecommended amounts of binayr DNA depend on the template type. 2003 215947 Page 74 U of A Cog Sci Dictionary (Humor) Humor There are many reasons why people find something humorous, which are reflected in the large number of theories on the subject. What will the total income be as the result of the recycling operation. Psychiatr Serv Crenshaw WB, Cain KA, Francis PS An updated national survey on seclusion and restraint.Binary options 101 x 101']"	['<urn:uuid:67806633-a4f9-407d-8ce3-8f8ebc6b6d7a>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T11:40:26.317848	22	13	1090
91	how cereal ingredients affect nutrition and which mold types grow on food	Cereals typically contain combinations of grains, nuts, seeds, and dried fruits, with calories ranging from 90-200 per serving. While nuts increase calories and fat content, they provide healthy fats. Most cereals contain added sweeteners like sugar, corn syrup, or honey, and it's recommended to choose options with less than 5 grams of sugar per serving. As for molds in food, they belong to the fungi kingdom and can have branches and roots that extend deep within food items. Common types appear as gray fur on sandwich meat, fuzzy green dots on bread, white dust on cheese, and velvety greenish-black circles on fruits. Some molds, like those used in blue cheeses, are safe to eat, while others can produce dangerous mycotoxins.	['You could probably guess that most kids’ cereals are bursting with added sugar, but even some “healthy” cereals can be full of sugar and few nutritional benefits. Whether you have a bowl for breakfast or munch on some for a snack, here are tips for picking the best cereals.\nMany cereals — especially the “healthy” or “whole-grain” ones — are combos of grains, nuts, seeds and maybe some dried fruit. Servings of cereal (without milk) can range anywhere from 90 to 200 calories and 0 to 10 grams of fat.\nMost cereals are low in sodium and free of cholesterol, saturated and trans fats. Added nuts will up the calories and fat, but at least they’re a healthy fat source. Dried fruits bring some natural sweetness and some extra calories, too. Then there’s the added sweeteners — anything from plain old sugar and highly processed corn syrup (I saw lots of high-fructose corn syrup in the boxes I checked) to less processed honey and maple syrup. Other common sweeteners you might find are evaporated cane juice, molasses, brown sugar, brown rice syrup and fruit juice concentrate. The bottom line for sweeteners: Most cereals have some, which is okay, but many are drowning in them! Since sweeteners add calories and few nutrients, stick to the cereals that contain the lowest amounts. Try looking for ones with less than 5 grams of sugars per serving (or slightly higher if some of that sugar is coming from dried fruit — more on what to look for below).\nAs for the milk, opt for nonfat or low-fat — whether you choose soy, cows’ or even rice milk — to help keep the calories and fat from getting out of control.\nRead the Label\nWe can’t stress this enough — read your labels! When you look at the nutrition facts, start with the serving size. It’s so easy to unknowingly pour 3 or 4 servings into your bowl. Serving sizes vary greatly from cereal to cereal so check each box. If the serving suggests “1/2 cup,” that doesn’t mean that’s all you should have. Just remember that if you go for a whole cup, you’re getting twice what’s on the label. Aim for about 150 to 200 calories worth of cereal for a sensible portion.\nCereal is an easy way to start your day with some whole grains. Choose brands that have whole wheat, rice, oats, corn, barley, quinoa, kamut and millet in the mix. To make sure the grains are whole, check the ingredient list for the word “whole.”\nThe ingredient list will also tell you the type of added sweeteners. The USDA recommends no more than 8 teaspoons a day of added sugar. That comes out to roughly 32 grams — a bowl of sugary cereal can easily exceed this. Sometimes it’s tough to know how much good or bad sugar is there — especially if the cereal contains both added sugars (i.e. the granulated white stuff) and natural sugars from dried fruit. The total sugar count won’t differentiate between the various kinds, so if you pick a cereal with raisins for example, know that some of the total sugars listed on the label are coming from the fruit.\nWhat to Choose\nFirst, go for whole grains! Look for cereals with at least 3 grams of fiber per serving and the lowest amount of added sugars possible — the sweetener should be no higher than the third or fourth ingredient on the list. Finding high-fiber and low-sugar cereals that you’ll want to eat can be tricky. I browsed my market and here are a few favorites I found:\n- Nature’s Path Heritage Heirloom Flakes\n- Trader Joe’s Reduced Sugar Triple Nuts & Flakes (only 2 grams of sugar per serving!)\n- Nature’s Path Envirokidz Cereals (great kid-friendly options)\n- Kashi Heart to Heart\n- Barbara’s Puffins\n- Barbara’s Shredded Spoonfuls\n- Cheerios (Original or Multigrain)\n- Fiber One (Original)\n- Quick & Easy Cereal Tips\n- Eat out of small bowls to keep portions under control.\n- Add your own low-calorie sweetness by topping your cereal with fresh fruit.\n- Mix a favorite sweeter cereal with a low-sugar, high-fiber one to boost the nutrients.\n- Steer clear of cereals with chocolate pieces or lots of sugary “clusters.”\nTELL US: What’s your cereal of choice?', 'Mold is everywhere! At Northeast Property Restoration we remediate property mold to keep the occupants safe from health hazards. But what about mold that grows on food? , What should you do when you find mold growth on food?, and Is it safe to cut off the moldy part and eat the rest? We will dive into this subject to answer these questions and more.\nMolds are a natural part of the environment. Mold is present anywhere that moisture and oxygen exist. Outdoor molds are necessary for breaking down matter, however indoor mold growth should not be present.\nMolds belong to the fungi kingdom. They are microscopic fungi that live on plant or animal matter. There are many different species of fungi that exist in the world. Most of these fungi’s produce spores. These spores can be transported by air, water, or insects.\nThere are many different types of mold. The most common types of mold are:\nMolds have branches and roots. Therefore, if you see mold on the surface of food their roots may extend deep within.\nFoods that are moldy may also have invisible bacteria growing along with the mold. Therefore, if you consume moldy food it can make you sick.\nHowever, there are some foods with mold that are safe to eat! Keep reading to find out.\nDue to the fact that molds develop “roots”, food with surface mold growth is contaminated all throughout.\nCommonly found “surface” molds include: gray fur on sandwich meat, fuzzy green dots on bread, white dust on cheese, velvety greenish black circles on fruits, white fuzz on vegetables etc.\nTherefore, if you just remove the growth on the surface you are only removing some of the mold growth. If you consume these mold affected items they may make you sick.\nYes! Molds are used to make certain cheeses. Some of these molds have visible blue veins including:\nThese cheeses are created by P. roqueforti or Penicillium roqueforti spores. The molds used to manufacture these cheeses are safe to eat.\nMycotoxins are poisonous substances produced by certain molds found in:\nThe Food and Agriculture Organization of the United Nations estimates that 25% of the world’s food crops are affected by mycotoxins, of which the most notorious are aflatoxins. (According to the USDA)\nAflatoxin is a cancer-causing poison produced by certain fungi in or on Foods and Feeds, Field Corn and Peanuts.\nThey have been associated with various diseases, such as aflatoxicosis in:\nMany countries try to limit exposure to aflatoxin by regulating and monitoring its presence on commodities intended for use as food and feed. The prevention of aflatoxin is one of the most challenging toxicology issues of present time.\nAflatoxins are considered unavoidable contaminants of food and feed, even where good manufacturing practices have been followed. (According to the USDA)\nCleaning the appliances and items in your kitchen can help minimize mold growth.\nWhen you go to the grocery store make sure you check the items before you purchase them. This will ensure that you are not buying aged food that has mold or will grow mold fast.\nTo avoid potential mold growth you can do the following:']	['<urn:uuid:dbf8fd3b-ddd3-4464-ad90-0bfdae99e0e4>', '<urn:uuid:5e876985-0de5-43ff-a1f7-85e151860e9d>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T11:40:26.317848	12	120	1247
92	investment advisor seeking comprehensive explanation gold etf physical gold comparison inflation hedge counterparty risks	Gold ETFs and physical gold differ significantly in their characteristics as inflation hedges and risk profiles. Physical gold provides direct inflation protection and has no counterparty risk, as it represents actual ownership of the metal. ETFs, while more convenient, are tied to financial institutions and may not provide the same level of inflation protection. ETFs typically hold physical gold (around 99.5% purity) and charge expense ratios between 0.5% to 0.75% annually. While paper gold investments offer greater liquidity and lower storage costs, they don't allow for actual gold conversion and may not perfectly correlate with gold price movements. Physical gold's value increases directly with gold prices, whereas paper gold's value can be affected by additional factors beyond just gold price movements, including political and environmental factors affecting gold companies.	['Investing in Gold\nInvesting gold can help you diversify your portfolio, and protect against the volatility of the stock market. It can also be an important part of a retirement savings plan.\nInvestors can choose from physical gold coins and bullion, exchange-traded funds that track gold prices or mutual funds that buy shares of precious metal mining companies. All of these options have different costs and levels of risk.\nIt’s a Safe Haven\nGold is often viewed as a safe asset during times of economic uncertainty or market turmoil. It is a “safe” asset because of its many characteristics, such as its limited supply (like precious materials), its relative permanence and its long-lasting utility. As a result, it has held or even increased its value during past market downturns.\nGold’s price is also affected by many factors. Stock market panics, for example, can cause investors to turn to gold as a hedge. The gold price can also be affected by inflation expectations and the pace of inflation. Similarly, rising interest rates can push people to seek shelter in the safety of government bonds and push them away from non-interest-bearing gold.\nRegardless of the reason, most investors choose to invest in gold as a way to diversify their portfolios. It has historically had a lower correlation with traditional financial assets such as stocks and bond. This low correlation can help to reduce a portfolio’s overall risk exposure and potentially improve long-term returns. It’s important to weigh the potential benefits of adding gold to your portfolio against its disadvantages. Be sure to do your homework when choosing a dealer, particularly with unallocated gold (gold that isn’t physically held by a dealer). Many dealers are unscrupulous, and they use tactics such as high-pressure sales to scam their customers.\nIt’s a Diversifier\nIf you plan to invest for the long run, a part of your portfolio should consist of gold. As a diversifier, it protects against a decline in the value of other assets like stocks and bonds. Gold can also help offset a rise in inflation.\nIt can be emotionally rewarding to invest in gold, but it has its drawbacks. You run the risk of someone stealing your metal, or you won’t get the full market value when you sell it. You can avoid these risks by buying shares in a gold-mining company, which has the benefit of being a liquid investment and will give you diversified exposure to the metal without having to physically hold it.\nA currency investment is another way to invest in Gold. This allows you to buy the commodity and sell it in the same way as a mutual fund or foreign exchange traded fund. This option provides a higher level liquidity than owning gold, and allows you to take advantage tax breaks in your country.\nGold is a good hedge against economic instability, no matter what type of investment you make. Investors seek out gold in times of high geopolitical and market volatility. Gold is a good option for those who wish to protect their wealth against currency depreciation.\nIt’s A Currency\nGold has been used for money throughout history. Despite this, many investors tend to consider it a commodity rather than a currency. This view can be problematic. Gold behaves as a currency. It moves in a manner opposite to paper currency and can mitigate risk to these assets when there is market or geopolitical turmoil.\nGold prices will fall if the USD increases, and vice versa. Moreover, gold has a positive correlation with the Euro. Gold is a great hedge against inflation. While inflation can push down the value of most currencies, gold isn’t affected by this trend.\nStorage costs can be high for gold bullion. In addition, investing in exchange-traded funds (ETFs) can provide exposure to the metal without having to worry about storage fees or other expenses.\nIt’s important to keep in mind that ETFs hold only a portion of gold and their inflows or outflows may have an impact on its price. Unlike stocks and bonds, however, gold does not generate a dividend. This may not be the best option for traders looking to earn regular income. There are other ways of gaining exposure to gold, such as trading CFDs for gold-mining firms.\nIt’s a Commodity\nGold is a commodity, which means it serves a number of functions. It protects from inflation, diversifies portfolios, and helps hedge against currency decline. It has also shown a track record of providing solid returns for certain periods. It also tends to be low-correlated with stocks and bonds. Investors like it because of this, especially in times of fear and uncertainty – such as recent bank failures which have shook the markets.\nYou can invest in gold through physical assets like gold bars and coins or through exchange traded funds (ETFs), which track the gold price. Physical gold is the more traditional way to purchase the metal. It can be stored in a safe or a safety deposit at a bank. Storing physical assets is expensive. The risk of theft is always present.\nInvestors who are more experienced and want to be hands-off can consider options based on the price of Gold, such as futures contracts or ETFs trading gold derivatives. These investments are riskier and require more research than other options. You should always consider your financial objectives, your current investing and spending habits, and your risk tolerance before making any investment. You may wish to seek out personalized advice from a financial advisor before making investment decisions.', 'What is Paper Gold? Physical Gold vs. Paper Gold\nWhen most people think of gold, a yellow metal that shines and can be taken physical possession of comes to mind. When gold “investors” think about the precious metal, a non-tangible variant, aka “paper gold”, also comes to thought.\nAlthough digital gold is not the real deal, it’s equivalent to physical gold if looked at purely from an investment perspective. If gold is something you wear, digital or paper gold is not an alternative to the physical product.\nPhysical and paper gold have merits and drawbacks. In this article, we’ll glaze over them all in detail so that you make wise investment decisions. Here are the things we’ll cover:\n- Brief introduction to paper gold\n- Buying paper gold\n- Basic overview of physical gold and buying gold\n- Comparing the two forms of gold and much more\nIn a world where cryptocurrency and digital money are becoming the norm, zero knowledge about paper gold or how it compares to physical gold is simply not knowing better. Read on to get educated.\nTable of Contents\nPaper Gold: What Is It?\nAs the name suggests, paper gold is a paper or document representing a gold investment. Gold stocks, gold ETFs (exchange-traded funds), gold futures, etc., are paper/digital gold examples.\nGold ETFs usually hold physical gold. On the other hand, a gold mutual fund is invested in the units of a gold ETF. Kindly note, paper-based gold is “representative”. It doesn’t afford actual gold ownership.\nThe amount of gold pegged to each fund unit would vary based on the fund issuer and the type of fund. Each share of SPDR Gold Shares, for instance, holds 1/10th of a gold ounce. Another gold fund or ETF could have a lesser or greater unit of gold attached to each share.\nAlso, not all gold funds provide investors with direct gold price exposure. Some could also be invested in publicly traded gold production, mining, or refining companies. Often, it’s a combination of the two so that the fund benefits from the dynamics of both worlds. The value of paper gold fluctuates a lot more or falls and rises quicker than actual gold’s price itself. That’s because the gold companies that these funds are invested in are subject to political or environmental factors not directly related to actual gold prices or the gold market.\nRead more: Is Gold Mineral or Metal?\nHow to Buy Paper Gold? And Things to Consider\nAs mentioned above, there are different forms of digital gold. To invest in them, the means or process could vary. Like shares and other precious metals ETFs, gold ETFs trade on a stock market. On the other hand, gold mutual funds are purchased from the issuing fund house.\nTo maintain the fund or to cover the administrative costs and management expenses of the ETF, an annual fee is charged, referred to as the expense ratio. It’s denoted as a small percentage of your investment and is used for covering the gold ETF’s expenses.\nGenerally, an expense ratio of 0.5 to 0.75% is considered healthy. Therefore, when on the lookout for investing in a gold ETF, ensure that the exchange-traded product’s expense ratio falls within the prescribed range. Anything near or above 1.5% is expensive.\nThe iShares Gold Strategy ETF by iShares Gold Trust, for instance, has an expense ratio of 0.37%, which comes out to be $37 per year on a $10,000 investment. It is among the most inexpensive gold ETFs out there.\nInvestors must also pay commissions to trade gold ETFs. The charges are usually single-digit dollar amounts. But if you buy and sell ETFs frequently, the commissions could add up quickly. And if the trade is “broker-assisted” (executed with the able guidance of the agent), carried out through automated phone orders, etc., the charges would be more significant.\nWhat is Physical Gold? And How to Buy Physical Gold?\nPhysical gold is real gold in coins, bars, jewelry, etc. It offers you direct gold exposure. Physical gold is called bullion in its bulk form, usually minted into gold coins or cast into gold bars. The bullion articles are investment quality gold bars or coins or are valued based on their fine gold content and mass and not their monetary face value.\nYou can procure such physical pieces of gold from government mints (such as the Royal Canadian Mint or Perth Mint), private mints, jewelers, and precious metal dealers. Although gold has a fixed value, the prices across dealers and jewelers could vary. Therefore, do some research to bag gold for the right price.\nPhysical and Paper Gold: A Comparison\nTo learn more about how physical and paper gold stack up, let’s list down their positive attributes.\n- No storage fees: With paper gold, physical space, or a third-party custodian to store your gold assets is not required. Also, there are no processing and delivery fees and other administration-related costs (usually linked to physical gold). Although you may bypass storage costs by using your facility, that route isn’t recommended since it could be risky and is also frowned upon by the IRS.\n(Talking about costs, investing in digital gold brings to fore its own set of brokerage fees and other charges, as discussed above. Investing in paper gold is, therefore, not without its expenses. But those expenditures are usually lesser than the costs associated with physical gold’s purchase, warehousing, and insurance.)\n- No physical damage risk: Digital gold is essentially a piece of document. Since it can be reproduced, it cannot be permanently tarnished or made non-existent, unlike gold that could be stolen, burned, melted, etc.\n- No transportation costs: Again, buying or investing in paper gold doesn’t entail transportation or other expenses, unlike physical gold.\n- Increased liquidity: Physical gold has always had its buyers, but finding a readily available buyer can take time. Even if you find a potential buyer, the transaction will take time to hit completion. With paper gold, however, things are much quicker and easier. To sell or buy paper gold, all that’s required is a mouse button click or phone call to your broker.\n- No barrier to entry: Paper gold lets you buy gold in small chunks or 1/10th of an ounce, making it accessible to a broader set of investors with varying buying capabilities. The smallest form of physical gold is usually a quarter ounce (7 grams) that may not be within the financial reach of a lot of people.\n- Zero purity concerns: Paper gold offers an authenticity guarantee. For instance, gold ETFs usually hold a 99.5% equivalent quantity of gold bullion. With physical gold, you must make sure the gold is pure gold, even when dealing with a reputable dealer.\nPaper gold may have made investing in gold a lot more convenient, but the charm attached to buying or owning physical gold is not going anywhere anytime soon. The following are the benefits of purchasing the tangible asset:\n- Actual gold ownership: Buying physical gold means owning the metal. Paper gold represents the value of gold, but it is not solid gold or offers no actual gold exposure. Also, you cannot convert or swap your paper gold stakes for real gold even if the fund or ETF is backed by solid gold. However, you can sell the document and use those proceeds to buy gold bars, coins, etc.\n- Inherently valuable: Physical gold has intrinsic value. It doesn’t represent or assume value by clinging to the value of another commodity the way paper gold does. When gold prices go up, the value of physical gold increases too. However, the same correlating behavior is not a given with paper gold. In fact, it’s not uncommon to see a skyrocketing spot price of gold having no positive impact on a gold ETF’s value.\n- No counterparty risk: Gold bullion presents no counterparty risk—the likelihood of another party in the arrangement defaulting on the contract’s obligations. Therefore, gold ETFs and other forms of paper gold can never be a perfect substitute for physical gold.\n- Natural inflation hedge: Most people add gold to their investment portfolio so that the metal would act as their safeguard when fiat currency loses its value. Physical gold provides that inflation protection. But paper gold doesn’t, as it’s intertwined with a financial institution that isn’t immune to economic idiosyncrasies.\n- Increasingly accessible: Although the starting price points could be a hurdle, physical gold is much easier to procure when you have the means. It is devoid of the red tape identified with paper gold transactions. You need not open a trading account, register with a brokerage, etc. Just walk into a jewelry or precious metals dealership and buy gold instantly.\n- Wearer pleasure: The investment side of the discussion aside, you can wear physical gold. Gold jewelry is the first thing that comes to mind when thinking of wearable gold. But you can also wear gold coins or bars.\nRead more: Best gold IRA companies\nBoth physical and paper gold are bonafide ways to invest in gold. However, the two have unique attributes. Physical gold, as mentioned above, actually lets you buy gold. Paper gold doesn’t afford that ownership but is a capable investment vehicle nonetheless.\nBoth forms of gold have pros and cons, and choosing to side with one over the other is a matter of personal choice and specific investment goals. If you’re looking to buy physical gold, steer clear of jewelry since it cannot be construed as an “investment”. If, however, you like to flaunt gold, we recommend jewelry.\nIf you’d like to buy physical gold but don’t want to deal with the hassles attached to owning the metal, look to a gold IRA. If you don’t know what a gold IRA is, browse through the articles we’ve previously written. To get started, click here.']	['<urn:uuid:ac7063ee-ee09-48e0-87b6-4e25ececf0ff>', '<urn:uuid:e852747a-52fc-4b47-81e5-b4dfe3124a73>']	open-ended	with-premise	long-search-query	similar-to-document	three-doc	expert	2025-05-12T11:40:26.317848	14	129	2566
93	nuclear fuel swelling process fission gas containment building system explain how	Nuclear fuel undergoes swelling during irradiation due to fission products, particularly volatile gases like krypton and xenon (known as fission gas). To handle this and other safety concerns, nuclear power plants use containment buildings - reinforced structures designed to prevent radiation escape. These buildings can withstand pressures of 60-200 psi and are part of a multi-barrier safety system that includes the fuel ceramic, metal fuel cladding tubes, and reactor vessel. The containment building is typically an airtight steel structure that fully encloses the reactor and is designed to contain or condense steam in case of accidents.	"['By OECD - Organisation for Economic Co-operation and Development\nIn the course of irradiation, nuclear gasoline alterations quantity, essentially via swelling. This swelling is as a result of the fission items and specifically through the risky ones similar to krypton and xenon, known as fission fuel. Fission fuel behaviour should be reliably envisioned so that it will make higher use of nuclear gasoline, an element that could aid to accomplish the commercial competitiveness required by way of present day markets. those complaints speak the result of a world seminar which reviewed fresh development within the box of fission gasoline behaviour in gentle water reactor gas and sought to enhance the versions utilized in computing device codes predicting fission gasoline liberate. cutting-edge wisdom is gifted for either uranium-oxide and mixed-oxide fuels loaded in water reactors.\nRead Online or Download Fission Gas Behaviour in Water Reactor Fuels: Seminar Proceedings, Cadarache, France, 26-29 September 2000 PDF\nBest technique books\nInterleukin 27 (IL-27) is a heterodimeric cytokine that comprises a 4 helix package deal cytokine-like subunit, termed p28, and a soluble cytokine receptor-like subunit, EBI3 (Epstein-Barr virus-induced gene 3). The p28 subunit has homology with different single-chain cytokines, while EBI3 is said to the extracellular area of the hematopoietic cytokine receptor family members.\nUГ‚В·XГ‚В·LВ® Encyclopedia of Water technological know-how explores water technological know-how and matters from a global point of view. issues coated contain lakes and streams, oceans, aquatic animals, climate and weather, glaciers, wetlands, ecology, hydropower, advertisement fishing, acid rain, activity, toxins, economics, water conservation, foreign water legislations, international warming and lots more and plenty extra.\nInstruction manual of auto Suspension keep an eye on structures surveys the cutting-edge in complex suspension keep an eye on concept and applications.\nTopics coated include:\nan summary of clever car suspension keep watch over systems\nIntelligence-based automobile energetic suspension adaptive keep watch over systems\npowerful energetic keep watch over of an built-in suspension system\nAn period type-II fuzzy controller for motor vehicle lively suspension systems\nlively regulate for actuator doubtful half-car suspension systems\nlively suspension keep watch over with finite frequency approach\nFault-tolerant keep watch over for doubtful automobile suspension platforms through fuzzy regulate approach\nH-infinity fuzzy keep watch over of suspension structures with actuator saturation\nlayout of sliding mode controllers for semi-active suspension structures with magnetorheological dampers\nJoint layout of controller and parameters for lively motor vehicle suspension\nprocedure method of car suspension process keep an eye on in CAE environment\nAn LMI method of vibration keep an eye on of auto engine-body structures with time delay\nFrequency area research and layout of nonlinear automobile suspension systems\nWith contributions from a world choice of researchers, instruction manual of auto Suspension keep an eye on structures will discover a position at the bookshelves of educational researchers and commercial practitioners on top of things engineering, quite these engaged on purposes for the automobile undefined.\n- Advances in mobile robotics: proceedings of the Eleventh International Conference on Climbing and Walking Robots and the support technologies for mobile machines, Coimbra, Portugal, 8-10 September 2008\n- Advances in Biomedical Engineering\n- Home Electrical Wiring - Build Your Own Smart Home\n- Les Cahiers du Programmeur - Symfony\n- Surface Integrity in Machining\nExtra resources for Fission Gas Behaviour in Water Reactor Fuels: Seminar Proceedings, Cadarache, France, 26-29 September 2000\nThe authors also gratefully acknowledge MITI for acceptance of this report. 52 REFERENCES  H. , “Irradiation Characteristics of High Burn-up BWR Fuels”, Int. Topical Mtg. on LWR Fuel Performance, Park City, Utah, 9-13 April 2000.  K. , “Microstructural Change and its Influence on Fission Gas Release in High Burn-up UO2 Fuel”, J. Nucl. , 188, 65, 1992. E. , J. Nucl. , 188, 19, 1972.  R. , “The Role of the Pellet Rim on Fission Gas Release at Extended Burn-up”, IAEA Technical Committee Meeting on Advances in Pellet Technology for Improved Performance at High Burn-up, Tokyo, Japan, 29 October-1 November 1996.\nResults and discussion Fission gas release Along with published BWR data , the FGR fraction of Step II LUAs is shown in Figure 2 (as a function of rod average burn-up) and Figure 3 (related to the maximum power experienced at burn-up beyond 10 GWd/t). Except for two fuel rods which experienced relatively high power among Step II LUAs, the FGR fraction of Step II LUAs was up to about 5% throughout the irradiation cycles. When compared at the same power experienced during irradiation, the FGR fraction of Step II LUAs was less than that of the previous types of fuel rods, indicating effectiveness of the design improvements to reduce FGR.\n0 0 0 10 20 30 40 50 Rod average burn-up, MWd/kg U 32 60 Fractional fission gas release % Figure 2. Fractional fission gas release as a function of rod average burn-up  30 25 20 15 10 5 0 0 20 40 60 80 100 Rod average burn-up MWd/kg U • Achievement at moderate temperature (<1 000°C) of local burn-up above 50 GWd/tM (typically) leads to the so-called “rim structure”. This is the object of extensive studies concerning its formation mechanism and its impact on thermal conductivity and local FGR.\n- Download Small business innovation research to support aging aircraft by Committee on Small Business Innovation Research to Support PDF\n- Download Small Wind Energy Systems for Battery Charging by Hugh Piggott PDF', 'Diablo Canyon Nuclear Power Plant: The WikiBook/Containment building\nA containment building, in its most common usage, is a steel or reinforced concrete structure enclosing a nuclear reactor. It is designed, in any emergency, to contain the escape of radiation to a maximum pressure in the range of 60 to 200 psi ( 410 to 1400 kPa). The containment is the fourth and final barrier to radioactive release (part of a nuclear reactor\'s Defence in depth#Defence in depth (nuclear engineering)|defence in depth strategy), the first being the fuel ceramic itself, the second being the metal fuel cladding tubes, the third being the reactor vessel and coolant system.\nEach nuclear plant in the US is designed to withstand certain conditions which are spelled out as ""Design Basis Accidents"" in the Final Safety Analysis Report (FSAR). The FSAR is available for public viewing, usually at a public library near the nuclear plant.\nThe containment building itself is typically an airtight steel structure enclosing the reactor normally sealed off from the outside atmosphere. The steel is either free-standing or attached to the concrete missile shield. In the United States, the design and thickness of the containment and the missile shield are governed by federal regulations (10 CFR 50.55a), and must be strong enough to withstand the impact of a fully loaded passenger airliner without rupture.\nWhile the containment plays a critical role in the most severe nuclear reactor accidents, it is only designed to contain or condense steam in the short term (for large break accidents) and long term heat removal still must be provided by other systems. In the Three Mile Island accident the containment pressure boundary was maintained, but due to insufficient cooling, some time after the accident, radioactive gas was intentionally let from containment by operators to prevent over pressurization. This, combined with further failures caused the release of minimal amounts of radioactive gas to atmosphere during the accident.\nInformation is still being studied of the failures at Fukushima. While the plant had operated safely since 1971, an earthquake and tsunami well beyond the design basis resulted in failure of AC power, backup generators and batteries which defeated all safety systems. This resulted in partial or complete meltdown of fuel rods, damage to fuel storage pools and buildings, sigificant release of radioactive debris to surrounding area, air and sea, and resorting to the use of fire engines and concrete pumps to deliver cooling water to spent fuel pools and containements.\nFile:Containment issues.PNG|thumb|right|If the outward pressure from steam in a limiting accident is the dominant force, containments tend towards a spherical design, whereas if weight of the structure is the dominant force, designs tend towards a can design. Modern designs tend towards a combination. Containment systems for nuclear power reactors are distinguished by size, shape, materials used, and suppression systems. The kind of containment used is determined by the type of reactor, generation of the reactor, and the specific plant needs.\nSuppression systems are critical to safety analysis and greatly affect the size of containment. Suppression refers to condensing the steam after a major break has released it from the cooling system. Because decay heat doesn\'t go away quickly, there must be some long term method of suppression, but this may simply be heat exchange with the ambient air on the surface of containment. There are several common designs, but for safety-analysis purposes containments are categorized as either ""large-dry,"" ""sub-atmospheric,"" or ""Ice Condenser Containment|ice-condenser.""\nPressurized water reactors\nFor a pressurized water reactor, the containment also encloses the Steam generator (nuclear power)|steam generators and the pressurizer, and is the entire reactor building. The missile shield around it is typically a tall cylindrical or domed building. PWR containments are typically large (up to 10 times larger than a BWR) because the containment strategy during the leakage design basis accident entails providing adequate volume for the steam/air mixture that results from a loss-of-coolant-accident to expand into, limiting the ultimate pressure (driving force for leakage) reached in the containment building.\nEarly designs including Siemens, Westinghouse, and Combustion Engineering had a mostly can-like shape built with reinforced concrete. As concrete has a very good compression strength compared to tensile, this is a logical design for the building materials since the extremely heavy top part of containment exerts a large downward force that prevents some tensile stress if containment pressure were to suddenly go up. As reactor designs have evolved, many nearly spherical containment designs for PWRs have also been constructed. Depending on the material used, this is the most apparently logical design because a sphere is the best structure for simply containing a large pressure. Most current PWR designs involve some combination of the two, with a cylindrical lower part and a half-spherical top.\nThe spent fuel pool is outside of the containment building in most PWR designs.\nModern designs have also shifted more towards using steel containment structures. In some cases steel is used to line the inside of the concrete, which contributes strength from both materials in the hypothetical case that containment becomes highly pressurized. Yet other newer designs call for both a steel and concrete containment, notably the AP1000 and the European Pressurized Reactor plan to use both, which gives missile protection by the outer concrete and pressurizing ability by the inner steel structure. The AP1000 has planned vents at the bottom of the concrete structure surrounding the steel structure under the logic that it would help move air over the steel structure and cool containment in the event of a major accident (in a similar way to how a cooling tower works).\nThe Russian VVER design is mostly the same as Western PWRs in regards to containment, as it is a PWR itself.\nOld RBMK designs, however, do not use containments, which was one of many technical oversights of the Soviet Union that contributed to the Chernobyl accident in 1986.\nBoiling water reactors\nFile:BWR Mark I Containment sketch with downcomers.png|thumb|left|Cross-section sketch of a typical BWR Mark I containment. DW = drywell, WW = wetwell, SF = spent fuel area\nIn a Boiling water reactor|BWR, the containment strategy is a bit different. A BWR\'s containment consists of a drywell where the reactor and associated cooling equipment is located and a wetwell. The drywell is much smaller than a PWR containment and plays a larger role. During the theoretical leakage design basis accident the reactor coolant flashes to steam in the drywell, pressurizing it rapidly. Vent pipes or tubes from the drywell direct the steam below the water level maintained in the wetwell (also known as a torus or suppression pool), condensing the steam, limiting the pressure ultimately reached. Both the drywell and the wetwell are enclosed by a secondary containment building, maintained at a slight sub-atmospheric or negative pressure during normal operation and refueling operations. The containment designs are referred to by the names Mark I, Mark II, and Mark III. The Mark I is the oldest, distinguished by a drywell containment which resembles an inverted lightbulb above the wetwell which is a steel torus containing water. The Mark II was used with late BWR-4 and BWR-5 reactors. It is called an ""over-under"" configuration with the drywell forming a truncated cone on a concrete slab. Below is a cylindrical suppression chamber made of concrete rather than just sheet metal. Both use a lighweight steel or concrete ""secondary containment"" over the top floor which is kept at a slight negative pressure so that air can be filtered. The top level is a large open space with an overhead crane suspended between the two long walls for moving heavy fuel caskets from the ground floor, and removing / replacing hardware from the reactor and reactor well. The reactor well can be flooded and is straddled by pools separated by gates on either side for storing reactor hardware normally placed above the fuel rods, and for fuel storage. A refueling platform has a specialized telescoping mast for lifting and lowering fuel rod assemblies with precision through the ""cattle chute"" to the reactor core area. \nThe Mark III uses a concrete dome, somewhat like PWRs, and has a separate building for storing used fuel rods on a different floor level. All three types house also use the large body of water in the suppression pools to quench steam released from the reactor system during transients.\nThe Mark I containment was used in those reactors at the Fukushima I Nuclear Power Plant which were involved in the Fukushima I nuclear accidents. The site suffered from a combination of two beyond design-basis events, a powerful earthquake which may have damaged reactor plumbing and structures and 15 meter tsunami which destroyed fuel tanks, generators and wiring causing back up generators to fail, and battery powered pumps also eventually failed. Insufficient cooling and failure of pumps needed to restore water lost to boiling off led to partial or possible complete meltdowns of fuel rods which were completely uncovered by water. This led to releases of significant amounts of radioactive material to the air and sea, and hydrogen explosions. However PWR reactors also require years of powered pumped cooling water. The thin secondary containments were not designed to withstand hydrogen explosions, and suffered blown out or destroyed roofs and walls, and destruction of all equipment on the refueling floor including cranes and refueling platform. Unit 3 suffered a particularly spectacular explosion which created a plume of debris over 300m high which resulted in a collapse of the north end of the top floor, and buckled concrete columns on its west side as can be seen by aerial photographs. Although they were fitted with modified hardened vent systems to vent hydrogen into exhaust stacks, they may have not been effective without power. Unit 2 had a large panel removed to vent gases, but suffered an explosion to the lower supression area. Even before the Fukushima incident, Mark I containment had been criticized as being more likely to fail during a blackout.  \nFrom a distance, the BWR design looks very different from PWR designs because usually a square building is used for containment. Also, because there is only one loop through the turbines and reactor, and the steam going through the turbines is also slightly radioactive, the turbine building has to be considerably shielded as well. This leads to two buildings of similar construction with the taller one housing the reactor and the short long one housing the turbine hall and supporting structures.\nCANDU reactor|CANDU power stations, named after Canadian-invented Deuterium-Uranium design, make use of a wider variety of containment designs and suppression systems than other plant designs. Due to the nature of the core design, the size of containment for the same power rating is often larger than for a typical PWR, but many innovations have reduced this requirement.\nMany multiunit CANDU stations utilize a water spray equipped vacuum building. All individual Candu units on site are connected to this Vacuum building by a large pressure relief duct which is also part of containment. The Vacuum building rapidly draws in and condenses any steam from a postulated break, allowing the reactor building pressure to return to subatmospheric conditions. This minimizes any possible fission product release to the environment.\nAdditionally, there have been similar designs that use double containment, in which containment from two units are connected allowing a larger containment volume in the case of any major incident. This has been pioneered by one Nuclear power in India|Indian HWR design where a double unit and suppression pool was implemented.\nThe most recent Candu designs, however, call for a single conventional dry containment for each unit.\nDesign and testing requirements\nTitle 10 of the Code of Federal Regulations, Part 50, Appendix A, General Design Criteria (GDC 54-57) or some other design basis provides the basic design criteria for isolation of lines penetrating the containment wall. Each large pipe penetrating the containment, such as the steam lines, has isolation valves on it, configured as allowed by Appendix A; generally two valves. For smaller lines, one on the inside and one on the outside. For large, high-pressure lines, space for relief valves and maintenance considerations cause the designers to install the isolation valves near to where the lines exit containment. In the event of a leak in the high-pressure piping that carries the reactor coolant, these valves rapidly close to prevent radioactivity from escaping the containment. Valves on lines for standby systems penetrating containment are normally closed. The containment isolation valves may also close on a variety of other signals such as the containment high pressure experienced during a high-energy line break (e.g. main steam or feedwater lines). The containment building serves to contain the steam/resultant pressure, but there is typically no radiological consequences associated with such a break at a pressurized water reactor.\nDuring normal operation, the containment is air-tight and access is only through marine style airlocks. High air temperature and radiation from the core limit the time, measured in minutes, people can spend inside containment while the plant is operating at full power. In the event of a worst-case emergency, called a ""design basis accident"" in NRC regulations, the containment is designed to seal off and contain a Nuclear_meltdown|meltdown. Redundant systems are installed to prevent a meltdown, but as a matter of policy, one is assumed to occur and thus the requirement for a containment building. For design purposes, the reactor vessel\'s piping is assumed to be breached, causing a ""LOCA"" (loss Of coolant accident) where the water in the reactor vessel is released to the atmosphere inside the containment and flashes into steam. The resulting pressure increase inside the containment, which is designed to withstand the pressure, triggers containment sprays (""dousing sprays"") to turn on to condense the steam and thus reduce the pressure. A SCRAM (""neutronic trip"") initiates very shortly after the break occurs. The safety systems close non-essential lines into the air-tight containment by shutting the isolation valves. Emergency Core Cooling Systems are quickly turned on to cool the fuel and prevent it from melting. The exact sequence of events depends on the reactor design, for ABWR see  pages 15A-37 and -38, for CANDU see  slides 21, 23 and 25.\nContainment buildings in the United States|U.S. are subjected to mandatory testing of the containment and containment isolation provisions under 10 CFR Part 50, Appendix J. Containment Integrated Leakage Rate Tests (Type ""A"" tests or CILRTs) are performed on a 15 year basis. Local Leakage Rate Tests (Type B or Type C testing, or LLRTs) are performed much more frequently both to identify the possible leakage in an accident and to locate and fix leakage paths. LLRTs are performed on containment isolation valves, hatches and other appurtenances penetrating the containment. A nuclear plant is required by its operating license to prove containment integrity prior to restarting the reactor after each shutdown. The requirement can be met with satisfactory local or integrated test results (or a combination of both when an ILRT is performed).\nIn 1988, Sandia National Laboratories conducted a test of slamming a jet fighter into a large concrete block at 481 miles per hour (775 km/h). The airplane left only a 2.5-inch-deep (64 mm) gouge in the concrete. Although the block was not constructed like a containment building missile shield, it was not anchored, etc., the results were considered indicative. A subsequent study by EPRI, the Electric Power Research Institute, concluded that commercial airliners did not pose a danger.\nThe Turkey Point Nuclear Generating Station was hit directly by Hurricane Andrew in 1992. Turkey Point has two fossil fuel units and two nuclear units. Over $90 million of damage was done, largely to a water tank and to a smokestack of one of the fossil-fueled units on-site, but the containment buildings were undamaged.\n- Nuclear Plant Security Systems, PDH Course E182\n- U.S. Nuclear Regulatory Commission Fact Sheet on the Accident at Three Mile Island. Available at http://www.nrc.gov/reading-rm/doc-collections/fact-sheets/3mile-isle.html/\n- All Things Nuclear: Possible Source of Leaks at Spent Fuel Pools at Fukushima\n- Jia Lynn Yang (March 14,2011). ""Nuclear experts weigh in on GE containment system"". Washington Post. http://www.washingtonpost.com/business/economy/nuclear-experts-weigh-in-on-ge-containment-system/2011/03/14/ABspN1V_story.html. Retrieved 18 March 2011.\n- Nick Carbone (March 16,2011). ""Fukushima Reactor Flaws Were Predicted – 35 Years Ago"". Time. http://newsfeed.time.com/2011/03/16/fukushima-reactor-flaws-were-predicted-%e2%80%93-35-years-ago/.\n- Nuclear Tourist (see next reference)\n- Candu containment safety\n- Flowserve Corporation - Edward\n- 15_A.FRM[dead link]\n- [dead link]\n- Planet Ark : NRC assessing US nuclear plants\' airstrike risk\n- Rocketsled[dead link]\n-  NEI Analysis shows Aircraft won\'t breach\n- NRC Turkey Point 1\n- NRC Turkey Point 2']"	['<urn:uuid:9aa99a4b-4616-48df-a744-85b886120b2f>', '<urn:uuid:60e313db-5527-483f-83b6-7253a4b3982c>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T11:40:26.317848	11	96	3636
94	When comparing the organizational changes at West Park Hospital and John Wesley County Hospital, which facility maintained a stronger connection to its original medical mission through its successor organizations?	John Wesley County Hospital maintained a stronger connection to its medical mission through its successor organizations. After its closure, it evolved into the John Wesley Comprehensive Health Center for outpatient services, while its associated organization JWCH Institute continued providing healthcare services, eventually becoming a Federally Qualified Health Center and expanding its medical services across multiple locations. In contrast, West Park Hospital was completely converted to residential use, with only minor medical facilities remaining on site (the New Epsom and Ewell Cottage Hospital and East Elmbridge Counselling Psychology service).	"[""|LOST HOSPITALS OF LONDON|\nWest Park Hospital\nHorton Lane, Epsom, Surrey KT19 8PB\n|1921 - 2002\nThe West Park Asylum was the 11th mental hospital owned by the LCC, and the fifth and last of the Epsom cluster to be built on the Horton estate land. It had been planned since 1906, prior to the opening of its neighbour, Long Grove Asylum. Building work began in 1912 and the Asylum had been due to open in 1916, but the work was delayed because of WW1. By 1917 the main building work had been completed, but the site was handed over to the Canadian army for use as a military hospital.\nThe buildings were handed back to the LCC in 1921, and the West Park Asylum opened as a civilian mental hospital with accommodation for 1732 patients. The remaining villas were completed in 1924, bringing the bed capacity to 2000.\nAs it had been built later than the other asylums in the cluster, a new plan had been developed and it was considered a very modern hospital design, with a combination of colony principles and the echelon format forming a semi-circle or horseshoe-shaped pattern. It was the first 'colony' asylum in England, an idea that had come from the United States - the hospital as a colony of small communities rather than a set of wards. Small groups of wards were grouped together within the same building, creating isolated communities. The layout enabled a good segregation of various types of patients and each area was easily accessed by a web-like network of corridors. Each ward was designed to be fairly self-sufficient.\nThe administration block was flanked by five male ward buildings on one side and seven female on the other. The enormous kitchens were located behind the dining room, which was behind the main hall. The main stores were also placed centrally, while the boiler houses, water tower and workshops were on the male side. On the female side were the large laundry and sewing room, with an additional Nurses' Home.\nThe buildings were of red brick or pebble-dashed, with grey slate roofs. The main buildings were embellished with window surrounds and quoins, while the ward blocks were plain.\nThe area around the main gate contained a porter's lodge, a mortuary and a large detached chapel. An acute hospital block faced the central administration building and had separate entrances for male and female patients and for staff. At the rear of the administration building was the isolation hospital, flanked by male and female sanatoria for patients with TB.\nThe Medical Superintendent's house was set away from the main complex, along West Park Drive, as were two sets of semi-detached cottages for senior staff. Detached houses for the Matron and the Steward were built on the main drive to the administration block.\nIn 1929 the Asylum was renamed the West Park Mental Hospital (the word 'Mental' was later dropped from its name). In the 1930s Horton Lodge, a large detached residence once part of the Horton Estate, was purchased by the LCC. It was renamed Hollywood Lodge (to avoid confusion with Horton Hospital) and became an annexe shared with The Manor.\nIn 1948 the Hospital joined the NHS, with 2174 beds, under the control of the South West Metropolitan Regional Hospital Board.\nIn 1950 a large Nurses' Home was built at the rear of the site, between the isolation hospital and the female sanatorium. Temporary structures were added later in the grounds, including a Social Club for the patients. By this time the hospital had 2545 beds.\nIn 1962 there were 2045 patients.\nBy 1971 there were 1724 beds and 1580 patients, 39% of whom slept in dormitories with 50 beds. Eight percent of patients slept in groups of less than 30. An alcohol and drug rehabilitation centre with 20 beds opened, as well as a specialist Metabolic Unit with 17 beds.\nDuring the 1970s staff residences were built on the northern part of the site, near to the parole villas where working patients lived. By 1975 there were 1600 beds. By 1979, when the headquarters for the Mid-Surrey Health District relocated there, the Hospital had 1217 beds.\nAdditional villas were added in the late 1980s. The original chapel was demolished and a new one built on the main site.\nFrom the mid 1990s the Hospital was slowly emptied of patients as the government-favoured policy of returning mental patients to 'Care in the Community' progressed.\nThe main Hospital closed in 2002. By 2003, although a few outer ward blocks remained open and in use, the vacated buildings were becoming derelict.\nPresent status (August 2008)\nIn 2003 the site was taken over by English Partnerships, the national regeneration agency, who sold it (along with the site of St Ebba's Hospital) to two housing developers - Crest Nicholson and Galliford Try. It is not known how much they paid for the sites.\nThe chapel has been demolished and, in 2003, arsonists destroyed Honiton Ward (the female epileptic ward) and, later, the main hall. In 2005 another arson attack destroyed Hollywood Lodge.\nThe administration building and some of the ward blocks have been converted into 91 dwellings, heated by a site-wide biomass CHP district heating scheme, with the Grade II listed water tower concealing the necessary chimney.\nIn 2007 a plan was announced that a total of 700 houses would be built on the sites of the two hospitals (360 of which would be on the West Park Hospital site). A hotel with 100-150 bedrooms and a conference facility would also be included. One third of the new homes would be affordable, managed by the Rosebery Housing Association.The East Elmbridge Counselling Psychology service at Ramsay House, now run by the Surrey and Borders Partnership NHS Foundation Trust, and the New Epsom and Ewell Cottage Hospital, which opened in 1990 with 20 beds, will remain.\n(UPDATE: The new housing estate -Noble Park - opened in July 2009.)\nThe main entrance with a plethora of signage.\nThe water tower with its integral chimney are built of grey brick with red banding. The Horton Light Railway terminated nearby.\nHospital buildings in the distance with rather battered signage in the foreground.\nEpsom Linen Services, now boarded up.\nSite map of the Hospital in 2008. Most of the services had been transferred or closed by this time.\nThe Sports and Social Club with day nursery by the main entrance.\nThe former Hospital site with the water tower in the centre surrounded by buildings.\nFormer ward blocks.\nSanthams, the male convalescent villa.\nThe Public Health Laboratory.\nRoad signage still listed West Park Hospital in 2008.\nThe New Epsom and Ewell Cottage Hospital at the north of the site.\nAbdy C 2001 Epsom Past. Chichester, Phillimore.\nto alphabetical list\nReturn to home page"", 'The Rich History of JWCH Institute, Inc\nThe County of Los Angeles purchased the John Wesley Methodist Hospital and renamed it the John Wesley County Hospital.\nOctober 6, 1960\nA group of Los Angeles County doctors stationed at the hospital had the foresight to create a private, non-profit public benefit corporation dealing with health services, health education and medical research. This private corporation would then provide or augment services of the hospital through additional funding obtained either through grants, contracts or private funds. They named the newly formed corporation “The Attending Staff Association of John Wesley County Hospital, Inc.”\nThe activities of the Association included: Oncology/Western Cancer Study Group with Dr. Joseph R. Bateman; The Liver Unit with Dr. Allan G. Redeker, and; Family Planning Services with Dr. Carol E. Henneman.\nFebruary 9, 1971\nThe John Wesley County Hospital is badly damaged in the 6.6 magnitude Sylmar earthquake. Parts of the hospital facility can not be used.\nThe John Wesley County Hospital was closed and in its place a new facility was built. That facility was initially known as the John Wesley Comprehensive Health Center which provided outpatient services only. It was renamed and is now known as the H. Claude Hudson Comprehensive Health Center. At that time many of the county doctors initially involved with The Attending Staff Association of John Wesley County Hospital moved to other locations including Rancho Los Amigos Hospital and Los Angeles County USC Medical Center. The move also forced the Attending Staff Association of John Wesley County Hospital to split into two organizations, with Dr. Robert Peters forming the Liver Research and Education Foundation at Rancho Los Amigos Hospital, Inc. Between the two corporations, monies were allocated based on where the doctor-in-charge was located.\nDr. Carol E. Henneman, Dr. Sidney E. Wechsler and Dr. Edward Lenoski were left with approximately $50,000 in assets and renamed The Attending Staff Association of John Wesley County Hospital, Inc as JWCH Institute, Inc. JWCH Institute, Inc. also obtained a DBA (doing business as) John Wesley Community Health Institute, Inc.\nPhyllis Paxton was hired as Executive Director of JWCH Institute. JWCH Institute successfully starts its Reproductive Awareness Program for at risk teenagers; its netVan (non-emergency transportation) system in Skid Row for Homeless and underserved persons; it’s HIV services programs, Skid Row Medical Outreach Team and opens the two exam-room Lynwood Family Planning clinic.\nThe County of Los Angeles puts out to competitive bidding several of its outpatient health facilities during the first round of the Los Angeles County 1115 Waiver. Part of this bidding includes the Los Angeles County Clinic at the Weingart Center Association in Skid Row. JWCH Institute successfully competes and is granted status as a Strategic Partner in its first Public Private Partnership program and takes over the medical operations of the Los Angeles County Clinic at the Weingart Center Association. The clinic is renamed the JWCH Weingart Clinic in Skid Row.\nExecutive Director Phyllis Paxton retires after eight years of service. The agency is now a strategic partner under the Los Angeles County Public Private Partnership program. Roger Johnson, M.Ed., is Chair of the Board of Directors.\nThe JWCH Institute, Inc. Board of Directors hires Alvaro (Al). P. Ballesteros as Chief Executive Officer. Roger Johnson is Chair of the JWCH Board of Directors. The Board also strategically makes the decision to seek federal designation as a community health center, FQHC.\nAl Ballesteros appoints internal JWCH staff to positions of leadership including Dr. Paul Gregerson, MD, MBA as Chief Medical Officer; appoints Andy Irie as Director of Administration and later Chief Administrative Officer; appoints Jeny Argame, RN, MBA as Chief Nursing Officer and Jeanne Lam, as Chief Financial Officer. The team begins to work towards achieving federal designation as a community health center and to enhance its service delivery system.\nJWCH takes over the operations of the clinic at the PATH Mall (People Assisting the Homeless Mall). The PATH Mall provides a one-stop shop for homeless persons to acquire housing, medical services and other support services.\nThe Los Angeles based Weingart Foundation launches the Skid Row Homeless Health Initiative based on a report completed by Michael Cousineau, Ph.D., entitled: “Neglect on the Streets: The Health and Mental Health Status and Access to Care for the Homeless Adults and Children in Central Los Angeles” published in June 2003. The report calls attention to the major role JWCH Institute could play to improve the safety-net. The Weingart Foundation is successful in bringing several major foundations to the initiative which represents a substantial, unprecedented investment into the medical and health infrastructure of skid row by philantrophy. JWCH leadership takes on critical positions within the Initiative and assists in outlining a plan of action and programs to improve access and health outcomes for the homeless.\nJWCH Institute, Inc., receives its first direct-Federal Centers for Disease Control and Prevention grant for HIV prevention and testing in Skid Row. Also in 2004, JWCH is successful in obtaining a grant from the Los Angeles County Department of Health Services for Mobile HIV Testing and is provided with a two-exam room Mobile Van. In June 2010, JWCH is successful in competitive bidding to continue this direct contract for an additional five years.\nWith the assistance of L.A. County Supervisor Gloria Molina, local community leaders, L.A. CARE Health Plan and the California Endowment, JWCH re-opens the Los Angeles County Bell Gardens Health Center that was closed in 2002. For program year 2009-10, the Bell Gardens Health Center provided 12,600 medical visits to the area’s residents.\nColumnist Steve Lopez begins a series in the Los Angeles Times on the plight of the homeless living on skid row. These columns bring heightened awareness to the medical, mental health and housing needs of the area’s homeless and spotlights the work being performed by the local service providers including JWCH Institute.\nCesar Portillo is elected Chair of the Board of Directors.\nJWCH Institute, Inc. is designated as a Federally Qualified Health Center, FQHC by the Federal Department of Health and Human Services, Health Resources Services Administration. This designation provides additional federal funding to the JWCH services system and allows the organization to access added benefits for its patients.\nWith the assistance of L.A. County Supervisor Don Knabe, local community leaders, L.A. CARE Health Plan and the S. Mark Taper Foundation, JWCH re-opens the Los Angeles County Norwalk Health Center that was closed in 2002. For program year 2009-10, the Norwalk Regional Health Center (S. Mark Taper Foundation Building) provided 12,363 medical visits to the area’s most needy residents.\nJWCH Institute assumes operations of the medical clinic at the Covenant House targeting homeless youth living on the streets of Hollywood and Los Angeles.\nJWCH receives its first federal SAMSHA (Substance Abuse and Mental Health Services Administration) grant to improve referral, pre-treatment and treatment to homeless in skid row.\nProject 50 is launched by L.A. County Supervisor Zev Yaroslovsky to address the challenges of the 50 neediest homeless persons living on the streets of skid row. Under the direction of the L.A. County Chief Executive Officer a collaboration is brought together and JWCH is asked to provide for the medical and mental health needs of the project 50 participants.\nJWCH expands recuperative care services to the Salvation Army Bell Shelter in the City of Bell. A total of 30 additional beds are created to assist homeless patients being discharged from local hospitals without a home to recover. Kaiser Permanente provides significant support for the build-out and operations. The program offers a solution to private hospitals who wish to address the post discharge needs of homeless patients.\nJWCH moves into the new Center for Community Health in Skid Row. This new site is three-times larger than the previous JWCH Weingart Health Center and is an integrated and comprehensive health care model for homeless and uninsured persons in skid row. In the facility JWCH is joined by USC Schools of Dental and Pharmacy, Los Angeles County Departments of Mental Health, Substance Abuse, Public Health and Health Services.\nThe move is made possible by The Weingart Foundation which provided significant capital and assisted in raising the remaining capital to complete the project. The Center for Community Health is view as one of the most positive outcomes of the original Skid Row Homeless Health Initiative.\nThe B.E.S.T. Program (Benefits, Entitlements, Services Team) is launched with support of the Los Angeles County Department of Health Services. With unprecedented support from the Department of Social Security Administration, this team begins collaborative work to qualify SSA and SSI eligible homeless persons for benefits. The program has gained national attention.\nThe Heart of Gold Thrift Store is opened in Highland Park to raise funds to support the mission of JWCH Institute.\nJWCH receives its first direct Special Projects of National Significance, SPNS grant. These funds will provide enhanced outreach and case management to women living with HIV/AIDS.\nOctober 19, 2010\nJWCH celebrates 50 years of service to the Los Angeles County with a golden jubilee Gala Event at the Dorothy Chandler Pavilion, in downtown Los Angeles. JWCH honors the Los Angeles County Board of Supervisors for its assistance over the years and its continuing commitment to the safety-net of Los Angeles County.\nNovember 25, 2013\nJWCH corporate office moved to the City of Commerce, California.']"	['<urn:uuid:7906226c-c05a-4cce-b421-1f9445db6a54>', '<urn:uuid:ed42e4d4-01ae-4ca2-92bd-5f5bf2473a5f>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T11:40:26.317848	29	88	2677
95	Planning to make cider. What's the safest way to clean the juice?	There are two safe methods for cleaning apple juice before making cider. The first is heating the juice to 165°F for at least 10 minutes. The second method is using Campden tablets (sulfites) - crushing one tablet per gallon of juice and letting it sit for two days in a covered fermenter. While heating is safest, especially for dirty fruit, it can eliminate some aromas. Sulfites are milder and don't affect the flavor.	['We are sure you have heard about it before, but do you know what hard cider is? Hard cider comes from fermented apple juice, together with a nice buzz.\nHard cider is also a trendy drink in the United States. It is called ‘hard cider’ to distinguish alcoholic from non-alcoholic cider, known as sweet cider. Hard cider is also closer to wine than beer. When we learned how to make hard cider at home, we just had to share it so you can too.\nHow to make hard cider\n- 1 How to make hard cider\n- 2 What do you need to make hard cider at home?\n- 3 Making hard cider process:\n- 4 Homemade Hard Cider Fermentation Directions\nIt is not complicated to make one. The process for elaborating this popular alcoholic drink is straightforward, but it could take some time. While apple cider is the main ingredient, you can follow these steps for any other fruit. You can make it from pineapple, pears, and many others.\nAs you may know, the best cider comes from fresh apples. That is because fresh juice has the brightest flavor for it. The aromas would not change from time or heat; this is what we call pasteurization. We will cover the basics of the whole process first.\nAn essential step in cider elaboration is to include some sulphur dioxide to prevent a plethora of dangerous bacteria or yeasts in the apple juice. It helps the fresh yeast to thrive in the juice environment; you need this for turning sugars into alcohol.\nOnce sulfur dioxide finally mixes in the apple juice, it turns itself into a mixture of bisulfite, sulfite ions, and some molecular sulfur dioxide. The rest of sulfur dioxide gives an antimicrobial element; the bisulfite and sulfite ions help to flavor it. The number of sulfur dioxide needed to clean all microbial activity depends on the juice’s pH: lower pH means less sulfur required, and higher pH needs more sulfur.\nA lot of producers put some sulfur dioxide in their mixes immediately after the pressing and juicing process; most beginner producers do so after fermenting. The reason behind it is to let it act as an antioxidant chemical or stabilizer for the hard cider production itself. It keeps the finished cider free from releasing hydrogen peroxide or other aldehydes; they produce bad aromas or flavors to the final beverage.\nNitrogen is also a vital element to help all yeast growth and fermentation. Yeast needs different types of nitrogen for it to function properly, so you should add nitrogenous compounds to your apple juice. The whole mixture of nitrogen-containing elements that yeast can use is referred to as ‘Yeast Assimilable Nitrogen’, or just YAN.\nYAN can mix with your juice before fermentation. However, there are other ways to change nitrogen levels before the pressing stage; the maturity of the fruit or the type of fertilizer also play a role. To use a fertilizer tool with the right amount of nitrogen will help the roots of apple trees to stay safe; nitrogen cleans all bacteria from the tree’s roots. The extra nitrogen then makes its way to the fruit of the tree; it is a good addition if you grow your ingredients.\nOn the other hand, a low crop load could also provide juices with more YAN than a typical high crop load.\nYAN is great for the yeast and helps ferment sugars in the juice, yet some hard cider producers may choose to limit their nitrogen use. If the yeast needs nitrogen, it would stop fermenting and finally die. That would be fine if makers want their cider to have more sugar than alcohol in their products. In any case, limiting YAN needs care; too little nitrogen can result in increased H2S levels; H2S causes a rotten-egg-like smell.\nThe fermentation process starts by eliminating the cider must with selected yeast strains. “Must” is the name that unfermented juice receives in hard cider making. It is also the name to any general mixture that will ferment later. One can also use indigenous yeast strains. These are on the fruit itself and allow to pass through fermentation without inoculation easily.\nThe inoculation stage requires different strains of Saccharomyces cerevisiae and other yeast strains with strong fermentative traits; these include Saccharomyces bayanus and Torulaspora delbrueckii strains.\nSpontaneous fermentation started with indigenous types of yeasts and finished by Saccharomyces cerevisiae can result in ciders with similar concentrations of valuable acids; these include lactic acid and succinic acid. Extending the time in which the cider remains in contact with the yeast results in increased levels for most of the minor volatile elements. These elements include fatty acids and other types of alcohol.\nYou could also use some tools to crush fruit and presses from your home as well. Another option is to get some freshly-pressed sweet cider from a store near home. If you buy an already-prepared juice, make sure it does not contain too many preservatives. High amounts of these chemicals can kill essential properties. If you can be confident about its contents, there is no reason not to use store-bought juice for your recipe.\nHowever, it is evident that most of the good homemade hard cider we have tasted comes from customers who prefer to choose fresh apples; doing so balances all the flavor from the mix. On the other hand, tannins help to give the best taste to apples. If you are looking to use only bland fruit, you surely will obtain a flat cider instead of a hard cider. These fruits include pears and green apples.\nWhat do you need to make hard cider at home?\nIf you are looking to crush your apples by yourself, then you should use a fruit crusher and a pressing tool. The crusher will help you to cut and smash the entire apples, and with the press, you will be able to extract the juice from them. Once you have obtained your apple juice, you are going to need the following items:\n- Some people use a 7-gallon bucket with a lid, but you could use a 6-gallon carboy as well. What you need is a fermenter with an airlock.\n- A straining bag would be necessary if you wish to use a pressing tool for extracting the juice.\n- Some tablets or potassium metabisulfite will be helpful.\n- A sanitizer is always necessary if you are preparing something for consumption.\n- Yeast is a vital element.\n- Yeast nutrient is suitable for quick fermentation. If the yeast is healthy, the cider will taste better.\n- You could also use Pectic enzyme (which helps to clear blurry looks). It would depend on the method you are planning to follow, of course.\n- Finally, you will need a bottling bucket, bottles, and caps.\nYou can purchase all of these items with relative ease.\nMaking hard cider process:\nBefore anything happens, you always need to sanitize your utensils. Make sure you remove all bacteria before adding the ingredients and yeast. As you may know, germs can do all kind of nasty things. They can cause a disgusting medicinal flavor; even worse, they can turn your hard cider into vinegar!\nIf you are planning to use pasteurized juice from any sanitary bottles (such as juice from a store that does not need any refrigeration), you can merely clean your fermenter thoroughly. You will not need to use heat and sulfites for it.\nHowever, using fruit makes the process a little more complex. There are several methods you can use to kill any bacteria from your mix.\nOption A: To use heat\nHeat is, in fact, the safest method, especially if you are working with dirty fruit. The only setback is that the temperature will eliminate some of the aromas.\nHeat your juice slowly up to at least 165°F. Cover it with a lid and keep the temperature for at least 10 minutes. You should leave the top on unless you are planning to use something to reduce the heat; otherwise, you can cool it in your sink with ice-cold water. When the juice finally cools down to 70°F, then you can start with the fermentation process.\nOption B: To use Sulfites\nHeating fruit can degrade the aroma and flavor of your apples. That is why hard cider makers almost exclusively use Campden tablets directly from a store. Other different types of sulfites can also be used for cleaning the must before adding the yeast. You must crush one Campden tablet per each gallon of your juice; this measure is mandatory. Let it sit for two days in a clean and covered fermenter.\nThat is the best-known method, and it is because sulfites are notably mild. They do not affect the flavor in any way, and they already dissolved by the time you drink. They are just strong enough to keep your mix clean from wild yeast and germs; this allows your yeast to thrive. This way, you can ensure that the taste of your drink will be closer to what you expect.\nOption C: To use Wild Yeast\nThe third option is simple; there is no need to use any heat nor sulfites to clean your must before fermenting the fruit. The issue is that this method has the highest risk of ending up with something not desired (such as vinegar).\nTherefore, It is imperative to use the cleanest fruit; you must also clean your tools and crusher well for this method to work correctly.\nIn this method, you do not use produced yeast. You rely on yeast that you can find naturally in the environment, specifically in fruits. You see, fruit already has yeast; that’s why it’s able to ferment at all.\nHowever, since you need yeast to come from the outside, you may expose your mixture to bacteria. While it saves money, that is why it is a risky method.\nAnother interesting choice is to use this method with pasteurized apple juice. It’s is naturally sweet and it is also free from bacteria thanks to the manufacturing process. It is sweet as well, so the yeast can feed on it without you needing to add sugars.\nIt is better to use canned juice to make sure it is as close to sterile as possible. Bottled juice is also the right choice as long as it does not need refrigeration.\nHow to Make Hard Cider Video Walkthrough:\nWhat’s the best yeast to make hard cider?\nYour taste makes your choice. However, these are the most commonly-preferred types of yeast for making cider:\n- Lalvin 1116: it is delicious but not as dry as full-on champagne cider; it has a sophisticated flavor.\n- Lalvin 1122: it is a good option for cider lovers who like it a little off-dry.\n- Lalvin D47: This is another excellent choice; this type of yeast will deliver a flavorful hard cider.\n- Red Star Champagne: This could also be the only yeast that looks like “champagne” on the bottle, resulting in a bone dry look.\n- Lalvin 1118: This is a different type of yeast with similar treats to Red Star’s.\n- Wyeast 4184 Sweet Mead: This could also be another good option for people looking for a dry drink.\n- Ale yeast: Maybe the most popular option for making a cider results it in being a little less dry.\nHomemade Hard Cider Fermentation Directions\nBefore fermenting, you must adequately sanitize your fermenter and anything that touches your must. A simple and easy way to do this is to have a proper recipient; it could be a bucket filled with your best sanitizer. Just apply a bit of the product into your fermenting container. Swish it all around where your mix will be, and coat all its surfaces a few times as well.\nThe goal here is to coat all surfaces of your fermenter with the sanitizer for at least five minutes. After this step, you should empty the fermenter into your previous bucket; use the same sanitizer to wash all other tools and utensils you will use during the process.\nSpecifically, Star San’s product is perfect. It kills bacteria efficiently, but it does not hurt the yeast nor alter the taste of your product.\nNow, you need to gather the yeast for the mix. The next step is to heat one cup of water. Add the yeast and its nutrient, if you have; note that this may cause a bad smell.\nAfter that, you should cool to around 80-100°F and finally add some pectic enzyme to the entire mix (this is optional). Place your mixture into your sanitized fermenter. Move your apple juice into it, and then, use the rest of your yeast as well. Use at least one yeast pack per 5 gallons of apple juice. Do not forget that too much yeast is better than not using enough. Homemade hard apple cider brewing is usually done in 1 gallon batches, so this helps.\nMake sure you follow your package instructions. For example, Wyeast says 1/2 tsp per 5 gallons; others brands may tell you that you should use at least 1/2 tsp per gallon. Put about 1 tsp pectic enzyme per 5 gallons of must (this step is optional).\nAfter this step, you should cover your fermenting tool and attach a sanitized airlock to it. Let the mix ferment at 65-75F for at least a couple of weeks. Once this step has concluded, give it 3-5 extra days. Finally, bottle it or rack it into a carboy so it could turn clear for another week first.\nTo bottle, please make sure your fermentation process is finished. You should consider that the hard apple cider is supposed to have a very dry flavor. A hydrometer tool is the best way to determine if it is over. It’s also used for determing the alcohol content in your homebrewed cider, here is how to read one.\nThis tool is the only way to know for sure what is going on in your fermented mix. If your bottle it too early, you could see some exploding bottles because of the fermentation in the bottle.\nYou need a clean bucket. Heat at least two cups of water up to its boiling point. Then, turn off the heat and proceed to dissolve all sugar. You probably need about 1 oz per gallon (or at least 3/4 cup for a 5-gallon batch).\nThen put this hot sugar solution into the clean bottling bucket. You should put your cider into the bottling bucket very carefully. Fill your previously sanitized bottles; leave at least 1-1.5″ headspace. Finally, cap and leave the whole mix in a warm room for two weeks for the carbonation process.\nWhen done, you can refrigerate the bottles for at least 3-12 months. Homemade hard cider ages well so this is going to give you the best results. We hope that you have had as much fun learning how to make hard cider at home as we do making and drinking it.']	['<urn:uuid:d2d4c773-b7b0-471f-bca4-3c442f294e45>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T11:40:26.317848	12	73	2507
96	compare star viewing conditions tysfjord norway southern cross tarama island	Tarama Island and Tysfjord offer different celestial viewing experiences. On Tarama Island, you can see the Southern Cross constellation, which is only visible south of 26 degrees north latitude. The island's flat terrain and lack of hills creates a dome-like sky view, and the limited village lighting allows clear star viewing from almost anywhere. In Tysfjord, while the Southern Cross isn't mentioned, the area experiences unique lighting conditions with midnight sun from late May to mid-July, and complete darkness from early December to mid-January. The aurora borealis (northern lights) is commonly visible during winter and late autumn months.	"['Tarama Island is an island located directly between the islands of Miyako and Ishigaki. Vast sugar cane fields cover much of the flat terrain formed by coral reef, and the sweet smell of brown sugar drifts throughout the island during the harvest season. The landscape of an old village remains on Tarama Island, which is known for its narrow streets referred to as “tuburi” that extend radially from the village toward the sea in a way that wraps around the island. There is also Minna Island, surrounded by coral reef and white sand, where nature remains unspoiled, just 9.5 km north of Tarama Island.\nThe August Dance, designated by the Japanese government as an important intangible folk cultural asset, is the harvest festival of Tarama Island, which takes place over three days starting on August 8th of the lunar calendar, a tradition that dates back 400 years. It originated from the practice of reporting to the gods of Utaki the people’s sense of joy and appreciation over fulfilling harsh tax obligations and dancing before them to celebrate the fulfillment and to pray for another abundant harvest the following year. The current practice evolved from the Kumi Odori and other classical dances that were introduced during the Meiji era. Each year, people who have left the island return, and tourists and researchers show up in crowds to bring in the most bustling season of the year.\nThe streets and villages on Tarama Island are laid out on a grid with a windbreak of fukugi trees based on the concept of Feng Shui, a result of ancestors carefully organizing a method to minimize the damage caused by typhoons, which often strike the island. With the stunning landscape of its villages that remain much in their original form, it is the only village in Okinawa to be recognized as one of the most beautiful villages of Japan.\nA charter boat can be taken from Tarama Island to reach Minna Island. Even amongst local Okinawans, very few people have been to Minna Island, and thus some refer to it as the “secret island.” Although there was once a population of 265 islanders and an elementary/middle school, there are only four households living on the island today. With its highly transparent waters and abundance of sea turtles, Minna Island is a must visit island.\nOn Tarama Island, you can see the Southern Cross, which is only visible south of the latitude 26 degrees north. Although the Southern Cross appears just above the horizon, since Tarama Island is a flat island with no hills, the sky appears as a dome, which allows you to watch the stars as if you were in a planetarium. Also, because there are few lights in the village, you can get a clear view of the stars from virtually anywhere on the island, making it a must visit for those interested in astronomy.\nFlights: Miyako Airport to Tarama Airport (approximately 20 minutes by Ryukyu Air Commuter)\nRyukyu Air Commuter\nsummer －Hirara Port (Miyakojima) to Maedomari Port (approximately 2 hours by ferry)\nwinter －Hirara Port (Miyakojima) to Futenma Port (approximately 2 hours by ferry)\nTarama Marine Transport\nAirplane: Not available.\nFerry: (No regular sailing service – operated irregularly on an individual basis)\nMaedomari Port (Tarama Island) > Minna Port.\n(Chart Jet Shirahama – approx. 15 mins)\nMr. Miyakuni – TEL: 0980-79-2773\nArea / Population: 19.75 km² / 1,250 residents\nSize / Population: 2.15km² / Population of 4 people.\nTarama Village Town Hall\n*Only in Japanese.\nIf you need more information, Ask Be.Okinawa Multilingual Contact Center!\n(Call, Mail, Skype)', 'View of the Skarberget area in Tysfjord\nTysfjord within Nordland\n|Established||1 Jan 1869|\n|• Mayor (2011)||Tor Asgeir Johansen (Ap)|\n|• Total||1,463.75 km2 (565.16 sq mi)|\n|• Land||1,358.17 km2 (524.39 sq mi)|\n|• Water||105.58 km2 (40.76 sq mi) 7.2%|\n|Area rank||#56 in Norway|\n|• Rank||#330 in Norway|\n|• Density||1.4/km2 (4/sq mi)|\n|• Change (10 years)||-3.7%|\n|Time zone||UTC+01:00 (CET)|\n|• Summer (DST)||UTC+02:00 (CEST)|\n|ISO 3166 code||NO-1850|\n|Official language form||Neutral and Lule Sami|\nTysfjord (Norwegian) or Divtasvuodna (Lule Sami) is a municipality in Nordland county, Norway. It is part of the traditional district of Ofoten. The administrative centre of the municipality is the village of Kjøpsvik. Other villages include Drag, Hundholmen, Korsnes, Musken, Rørvika, Skarberget, and Storå.\nTysfjord has a very large population of Lule Sami people. The Árran Lule Sami Center is located in the village of Drag. With the Norwegian language and Lule Sami language both as official languages of the municipality, Tysfjord is the only municipality in Norway where speakers of Lule Sami should theoretically be able to speak that language with officials, although this has not come completely to fruition.\nThe 1,464-square-kilometre (565 sq mi) municipality is the 56th largest by area out of the 422 municipalities in Norway. Tysfjord is the 330th most populous municipality in Norway with a population of 1,953. The municipality\'s population density is 1.4 inhabitants per square kilometre (3.6/sq mi) and its population has decreased by 3.7% over the last decade.\nThe municipality of Tysfjord was established on 1 January 1869 when it was separated from the large Lødingen Municipality. Initially, the population of Tysfjord was 1,402. During the 1960s, there were many municipal mergers across Norway due to the work of the Schei Committee. On 1 January 1964, the Tysnes and Molvika areas (population: 33) located northwest of Bognes was transferred from the neighboring Hamarøy Municipality to Tysfjord Municipality.\nIn 2017, the Parliament of Norway decided that Tysfjord would be divided along the fjord with the eastern half being merged with the neighboring municipalities of Narvik and Ballangen to form a new, larger municipality of Narvik effective 1 January 2020. The remaining western half will be merged with the municipality of Hamarøy on the same date.\nThe municipality is named after the Tysfjorden (the fjord that the municipality surrounds). Tysfjord is a Norwegianized form of the Lule Sami name Divtasvuodna (first recorded around 1520 as ""Tyttisfiorden""). The meaning of the first element divtas is unknown. (A popular belief, and common interpretation, is that the first element should be the genitive case of the name of the Norse god Týr - but this does not fit either with the Sami form or the oldest spellings of the name.) The last element is vuodna which means ""fjord"" in the Sami language.\nCoat of arms\n|Parish (sokn)||Church Name||Location of the Church||Year Built|\nIn 1948, an investigation that had lasted for years, was dismissed in regards to ""a large group of Sami from Tysfjord"". These individuals had worked as guides—guiding Norwegians who chose to flee into Sweden.\nIn November 2017, a Norwegian police report was published exposing sexual abuse in the municipality, following investigations in 2016. The police identified 151 cases of sexual abuse from the 1950s to 2017. Around two-thirds of the victims and alleged abusers were Sami. The community is described as being \'in recovery\' but prosecution of the older cases has not been possible under Norwegian statute of limitations law. 40 cases dealt with sexual intercourse with underage children.\nAll municipalities in Norway, including Tysfjord, are responsible for primary education (through 10th grade), outpatient health services, senior citizen services, unemployment and other social services, zoning, economic development, and municipal roads. The municipality is governed by a municipal council of elected representatives, which in turn elect a mayor. The municipality falls under the Ofoten District Court and the Hålogaland Court of Appeal.\n|Party Name||Name in Norwegian||Number of|\n|Socialist Left Party||Sosialistisk Venstreparti||1|\n|Local Lists||Lokale lister||9|\n|Total number of members:||17|\nDue to the limestone in Tysfjord, a cement factory was established with production starting in 1920. The modern Norcem factory still is a vital employer in Kjøpsvik, with about 130 employees; since 1999 it has been a part of the German worldwide company Heidelberger Cement. Public services, some tourism, and agriculture are the other main sources of income in Tysfjord.\nThe municipality is located along the Tysfjorden. The Tysfjorden is the second deepest fjord in Norway, with a maximum depth of 897 metres (2,943 ft). The municipality borders Ballangen in the north, Hamarøy in the south, Sweden in the east, and the Vestfjorden in the northwest. The island of Hulløya lies in the middle of the Tysfjorden.\nThe municipality is dominated by grey granite mountains; pine, birch, and aspen woodlands and forests; and the many fjord branches. Tysfjord\'s most dominant mountain, Stetind, is famous in Norway. This 1,392-metre (4,567 ft) high natural granite obelisk, rising straight out of the fjord, is an awe-inspiring sight (Picture 1, Picture 2). In Norwegian, it is called gudenes ambolt which means the ""anvil of the gods"", partly because the summit forms a plateau. This was selected to be the national mountain of Norway in the autumn of 2002. The famous British climber William C. Slingsby described it as the ugliest mountain I ever saw; he did not reach the summit.\nThe mountains near the border with Sweden have peaks up to 1,500 metres (4,921 ft) above sea level, and with a large glacier, Gihtsejiegŋa. There are several nature reserves in Tysfjord. Mannfjordbotn reserve has undisturbed forests at the head of a narrow fjord branch surrounded by granite mountain walls. From Hellemobotn, at the head of Hellemofjorden, the distance to the border with Sweden is only 6.3 kilometres (4 mi); this is also a scenic hiking terrain (Picture) with a canyon opening up in Hellemobotn (Picture). There are also caves, such as the very deep Raggejavreraige. Lakes in the region include Baugevatnet, Båvrojávrre, Kilvatnet, Langvatnet, and Skilvatnet. Silver birch occurs in Tysfjord, as one of few areas in North Norway (silver birch need more summer warmth than the more common downy birch).\nFor many years since 1990, a large part of the stock of Norwegian herring has stayed in the fjord in winter, one of the largest gatherings of biomass in the world. Large number of whales, orcas in particular, have followed to feed on the herring. This has attracted winter tourists from far away, but not in huge numbers. Since 2008, less herring and fewer orca have arrived in Tysfjord. Harbor porpoises, lobsters, sea eagles, ravens, otters, and moose are all common in the Tysfjord area.\nThe coastal municipality of Tysfjord is better known for its whale watching than its birdwatching but the region has still a lot to offer. Thanks to a range of habitats the area has a varied birdlife. Though the area has no large seabird colonies, there is a small nature reserve at Ramnholmen with breeding populations of Arctic tern and common tern.\nThe climate is surprisingly temperate considering the location north of the Arctic Circle. Mean annual temperature in Kjøpsvik is 4.2 °C (40 °F) and mean annual precipitation is 1,080 millimetres (42.5 in). Summer is usually pleasant, although there might be persistent rain. Average monthly 24-hr temperature in June, July, and August is 10.9 °C (52 °F), 13.3 °C (56 °F), and 12.5 °C (55 °F), with average daily high about 16 °C (61 °F).\nThe average temperatures are below freezing for slightly more than 4 months, statistically from 17 November to 30 March, with January average of −2.7 °C (27 °F). October is the wettest month with on average 154 millimetres (6.1 in) precipitation, May is the driest with 54 millimetres (2.1 in).\nThe midnight sun can be seen from the end of May to mid-July, and the sun is below the horizon from the beginning of December to mid-January. The aurora borealis is commonly seen in winter and late autumn.\nTysfjord is the only location in Norway where the European route E06 highway depends on a car ferry. There are ferry connections from Bognes to Skarberget (route E6) and from Bognes to Lødingen (connecting to route European route E10 and Lofoten). There is also a ferry connecting Drag south of the fjord with Kjøpsvik on the northern shore.\nKjøpsvik is connected to the E6 highway and Narvik by Norwegian National Road 827, with no ferry crossings. This might be an alternative to route E6, and is also the route of choice to get close to Stetind.\nIf Tysfjord experiences bad weather and the ferry connections are shut down, Norway is cut in two road-wise. A road connection is still maintained, albeit through Sweden, which makes it a very long drive.\n- ""Navn på steder og personer: Innbyggjarnamn"" (in Norwegian). Språkrådet. Retrieved 2015-12-01.\n- ""Stadnamn og rettskriving"" (in Norwegian). Kartverket. Retrieved 2018-07-13.\n- Statistisk sentralbyrå (2018). ""Table: 06913: Population 1 January and population changes during the calendar year (M)"" (in Norwegian). Retrieved 2019-01-12.\n- Statistisk sentralbyrå. ""09280: Area of land and fresh water (km²) (M)"" (in Norwegian). Retrieved 2019-01-12.\n- Jukvam, Dag (1999). ""Historisk oversikt over endringer i kommune- og fylkesinndelingen"" (PDF) (in Norwegian). Statistisk sentralbyrå.\n- Karlsen, Tone Anita (2017-08-22). ""Ny Narvik tar form"". Harstad Tidende (in Norwegian). Retrieved 2017-09-10.\n- ""Intensjonsavtale Hamarøy og Tysfjord"" (in Norwegian). Tysfjord kommune. 2016-12-13. Retrieved 2017-09-10.\n- Rygh, Oluf (1905). Norske gaardnavne: Nordlands amt (in Norwegian) (16 ed.). Kristiania, Norge: W. C. Fabritius & sønners bogtrikkeri. p. 275.\n- ""Fakta om Tysfjord"" (in Norwegian). Tysfjord kommune. Retrieved 2008-11-29.\n- ""Civic heraldry of Norway - Norske Kommunevåpen"". Heraldry of the World. Retrieved 2019-01-12.\n- [Samiske grenseloser reddet tusenvis. - Så ble de beskyldt for landssvik - Nå får de oppmerksomhet og heder.] [Sami guides saved thousands. - Then they were accused of treason - Now they get attention and are lauded]\n- ""Norway police uncover more than 150 rapes in remote region"". 2017-11-30.\n- ""BBC World Service - Assignment, Norway - A Community in Recovery"".\n- Samisk Fadervår for sterk kost for nordmenn [The prayer, Our Father—in Sami—is too upsetting for Norwegians]\n- Hansen, Tore, ed. (2016-05-12). ""kommunestyre"". Store norske leksikon (in Norwegian). Kunnskapsforlaget. Retrieved 2019-01-01.\n- ""Table: 04813: Members of the local councils, by party/electoral list at the Municipal Council election (M)"" (in Norwegian). Statistics Norway.\n- ""Norcem - Månedens Bedrift"" (in Norwegian). Archived from the original on 2007-09-18. Retrieved 2008-11-29.\n- ""Mannfjordbotn naturreservat"" (in Norwegian). Archived from the original on 2007-09-26. Retrieved 2008-11-29.\n- ""Tysfjord i perioden 1961-1990"" (in Norwegian). Archived from the original on 2007-09-29. Retrieved 2008-11-29.\n|Wikimedia Commons has media related to Tysfjord.|']"	['<urn:uuid:179c1a46-0bdc-4c8d-87d5-bebeeb233b6f>', '<urn:uuid:34b3e0ce-5c19-4217-9080-4eca98a364bf>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-12T11:40:26.317848	10	98	2355
97	looking for how many years liberty seated design appeared on american money	The Liberty Seated design appeared on U.S. coins from 1836, when it was first used on a limited number of silver dollars, until 1891.	['We write about products and services that we use. This page may contain affiliate links for which we receive a commission.\nIn the 19th century, Christian Gobrecht’s popular “Liberty Seated” design was the most prominent image on a coin. It also had the greatest longevity of the United States coinage of the time.\nThe Seated Liberty design was first seen in 1836 on a limited number of silver dollars. It entered widespread use in 1837, when the half-dime and dime were first struck with obverses bearing the Seated Liberty design.\nThe Liberty Seated design was used on several coins — including the quarter and the dollar coin.\nGobrecht, The Engraver Of Seated Liberty Coins\nChristian Gobrecht was a German-American born in Hanover, Pennsylvania in 1785.\nHis early life saw him develop as an artist and engraver. He was also an apprentice for a clockmaker, during which time he honed his skills as a respected engraver.\nAfter moving to Philadelphia in 1811, he eventually became well known for his engraving work.\nBy the 1820s, Gobrecht had begun engraving dies and had an interest in working as an engraver for the U.S. Mint. Though he was not offered the position of head engraver for years, he did do some important work for the U.S. Mint producing some punches.\nIn the late summer of 1835, Gobrecht was officially hired as a second engraver after primary engraver at the time, William Kneass, suffered a debilitating stroke.\nSeated Liberty Silver Dollars\nWork soon began on a new design for the silver dollar — which became the first coin to bear the Seated Liberty design that famously adorned most of U.S. coin denominations during the bulk of 1800s.\n- Silver dollars produced from 1836 through 1839 are popularly referred to as Gobrecht dollars.\n- Silver dollars produced from 1840 to 1873 are called Liberty Seated dollars, even though they still bear the Seated Liberty design.\nSeated Liberty Design On All Coins\nBy 1840, the Liberty Seated design had been placed on the obverse of all coins — ranging from the half dime through the silver dollar.\nThe Liberty Seated design also appeared on the briefly struck 20-cent piece (1875-1878).\nGobrecht passed away in 1844, but his renowned Seated Liberty design would go on to grace our nation’s coins until 1891.\nSeated Liberty Coin Changes\nLiberty Seated coins have a very strong following of dedicated collectors.\nOf interest to coin collectors are the several minor varieties that were made to various aspects of the Seated Liberty coins over the several decades that the design remained in use.\nLiberty Seated coin varieties include:\n- The addition or removal of arrows around the date on the obverse\n- The addition and removal of stars and rays\n- The addition of a ribbon bearing the motto “In God We Trust” over the eagle on the reverse of quarters, half-dollars, and dollars\nSeated Liberty Type Values\nValues of for Liberty Seated coins vary. “Type” values for each of the Seated Liberty denominations can be found below.\nThese are the lowest average prices you can expect to pay for certain Liberty Seated coins in a grade of Good-4 (which is generally considered the lowest-acceptable grade for collecting these coins). Prices run higher for many of the varieties and scarcer dates:\n- Liberty Seated Half-Dime (1837-1873): $15+\n- Liberty Seated Dime (1837-1891): $15+\n- Liberty Seated 20-Cent Piece (1875-1878): $90+\n- Liberty Seated Quarter (1838-1891): $20+\n- Liberty Seated Half-Dollar (1839-1891): $40+\n- Gobrecht Dollar (1836-1839): $12,000+\n- Liberty Seated Dollar (1840-1873): $250+\nAll values above are from the Guide Book of United States Coins by R.S. Yeoman and Kenneth Bressett.\nFor more about Seated Liberty coins, check out the Liberty Seated Collectors Club.\nI’m the Coin Editor here at TheFunTimesGuide. My love for coins began when I was 11 years old. I primarily collect and study U.S. coins produced during the 20th century. I’m a member of the American Numismatic Association (ANA) and the Numismatic Literary Guild (NLG) and have won multiple awards from the NLG for my work as a coin journalist. I’m also the editor at CDN Publishing (a trusted source for the price of U.S. rare coins), editor at the Florida United Numismatists Club (FUN Topics magazine), and author of Images of America: The United States Mint in Philadelphia (a book that explores the colorful history of the Philadelphia Mint). I’ve contributed hundreds of articles for various coin publications including COINage, The Numismatist, Numismatic News, Coin Dealer Newsletter, Coin Values, and CoinWeek. I’ve also authored nearly 1,000 articles here at The Fun Times Guide to Coins (many of them with over 50K shares), and I welcome your coin questions in the comments below!']	['<urn:uuid:edf494f5-46da-43d9-9dfe-0392986b2acd>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T11:40:26.317848	12	24	784
98	What are the main challenges when dealing with salt formations?	In salt formations, there are three loading mechanisms that can cause production casing failure: uniform loading, non-uniform loading and shear loading. To handle these challenges, the oil industry typically uses either two casing strings or a single 'high collapse' casing column.	"['Video: Cemented Casings Collapse Resistance Enhancement: Full-Scale Experience\n- Raffaello Zambetti (Tenaris) | Luca Paolo Bianchini (Eni) | Fabio Parrozza (Eni) | Valentina Gibilterra (Eni) | Loredana Orlando (Eni) | Simone Malesani (Tenaris) | Ivan Codognotto (Eni) | Paolo Novelli (Tenaris)\n- Document ID\n- Offshore Technology Conference\n- Publication Date\n- Document Type\n- 2020. Copyright is retained by the author. This document is distributed by OTC with the permission of the author. Contact the author for permission to use material from this document.\n- 2.10.1 Design for Well Integrity, 0.2 Wellbore Design, 4.3.4 Scale, 2.2 Installation and Completion Operations, 1.14 Casing and Cementing, 2.1.3 Completion Equipment, 1.14.3 Cement Formulation (Chemistry, Properties), 2 Well completion, 1.14.1 Casing Design\n- resistance, collapse, punctual, salt, casings\n- 1 in the last 30 days\n- 1 since 2007\n- Show more detail\n- View rights & permissions\nCasing collapse resistance is one of the key factors in well integrity design especially for production casings and the aim of the current study is investigating the physical phenomena that define the mechanism of failure in presence of two concentric cemented casing strings.\nCollapse load mode, especially in specific conditions, becomes the governing load of the well.\nTypical examples are salt or other creeping formations, which represent the most difficult load combinations to design and analyze. Salt creeping movements create three loading mechanisms that may result in a production casing failure: uniform loading, non-uniform loading and shear loading. The current oil industry approach, to deal with salt formations, is using two casing strings or either an individual ""high collapse"" casing column.\nThis paper describes results of a joint activity between Eni and Tenaris aimed at evaluating on full field scale the combined collapse resistance of two casing strings in three different cementing configurations.\nThe experiment consists of two different laboratory test set-ups. The first set is a collapse test, performed in a high-pressure vessel under hydrostatic conditions, to reproduce salt distributed Uniform Loading. Meanwhile the second test set, which is one of the first worldwide experiments of its kind, reproduces punctual Non-Uniform and Shear Loading. This second test is conducted with a mechanical press equipped with an ad-hoc designed indenting tool. The scope of the tool is to reproduce the effect of hard formation inclusions, in salt layers, acting on the casing\'s external surface. Test set-up and specimen\'s geometric configuration are optimized by specifically developed FEA analyses. To simulate real stress conditions, as the ones found in the well, tests are performed on different specimen configurations.\nThe base configuration consists of perfectly concentric casings with standard cement slurry in the annulus. Moreover, a configuration with 10% stand-off is also tested. At last, a configuration simulating the effect of contaminated cement or poor cement job is evaluated. The latter allows a better cement contribution assessment to final collapse resistance.\nThe results of the first set of experiments allows empirically defining a coefficient, ""K-value"", used to define collapse resistance of the assembly; the second set provides an indication about the entire system resistance to punctual loads and cement sheath integrity. Purpose of these tests is to extend the outcomes to several casing sizes thru the utilization of FEA model.\nThe obtained result of full-scale experiments is the way forward to better understand the collapse phenomena and approach the casing design for ""pipe in pipe"" cemented strings, with a significant impacts on well final cost and integrity.']"	['<urn:uuid:87b8032c-db35-4435-885c-84cbcf89a0ff>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T11:40:26.317848	10	41	569
99	How does Keppra's use in epilepsy treatment compare to dihydrocodeine's risk of breathing problems in young patients?	Keppra is used to treat various types of seizures in patients from different age groups - it can be used alone from age 16 for partial-onset seizures, and as add-on therapy from 12 years for myoclonic seizures and primary generalized tonic-clonic seizures. In contrast, dihydrocodeine should not be used in children under 12 years for pain or cough due to the risk of serious breathing problems, as some children are more sensitive to the drug and can experience dangerous slow/shallow breathing.	"['What Should I Avoid While Taking Keppra (Levetiracetam)?\nKeppra is a medicine that contains the active substance levetiracetam. It is available as tablets (250 mg, 500 mg, 750 mg, and 1,000 mg), as an oral solution (100 mg/ml), and as a concentrate that is made up into a solution for infusion (drip into a vein, 100 mg/ml).\nKeppra can be used on its own in patients from 16 years of age with newly diagnosed epilepsy, to treat partial-onset seizures (fits) with or without secondary generalization. This is a type of epilepsy where too much electrical activity in one side of the brain causes symptoms such as sudden, jerky movements of one part of the body, distorted hearing, sense of smell or vision, numbness, or a sudden sense of fear. Secondary generalization occurs when the overactivity later reaches the whole brain.\nKeppra can also be used as an add-on to other anti-epileptic medicines to treat:\n- partial-onset seizures with or without generalisation in patients from one month of age;\n- myoclonic seizures (short, shock-like jerks of a muscle or group of muscles) in patients from 12 years of age with juvenile myoclonic epilepsy;\n- primary generalised tonic-clonic seizures (major fits, including loss of consciousness) in patients from 12 years of age with idiopathic generalised epilepsy (the type of epilepsy that is thought to have a genetic cause).\nThe medicine can only be obtained with a prescription.\nHow should this medicine be used?\nLevetiracetam comes as a solution (liquid), an immediate-release tablet, an extended-release (long-acting) tablet, and as a tablet for suspension (a tablet to take with liquid) to take by mouth. The solution, immediate-release tablet, and tablet for suspension are usually taken twice a day, once in the morning and once at night, with or without food. The extended-release tablets are usually taken once daily with or without food. Try to take levetiracetam at around the same time(s) every day. Follow the directions on your prescription label carefully, and ask your doctor or pharmacist to explain any part you do not understand. Take levetiracetam exactly as directed. Do not take more or less of it or take it more often than prescribed by your doctor.\nSwallow the levetiracetam immediate-release and extended-release tablets whole; do not split, chew, or crush them. Take the whole levetiracetam tablets for suspension according to directions; do not split, chew, or crush them.\nTo take levetiracetam tablet(s) for suspension, use dry hands to peel the foil from the blister packaging; do not try to push the tablets through the foil. Immediately take out the number of tablets that your doctor has told you to take and place the tablet(s) on your tongue with a sip of liquid. Once the tablet completely dissolves on your tongue, swallow the mixture. The tablet(s) may take about 10 seconds to dissolve.\nYou can also take levetiracetam tablets for suspension by dissolving them in a liquid. Place the number of the tablet(s) your doctor has told you to take into a cup and add a small amount of liquid (about 1 tablespoon [15 mL] or enough to cover the medication in a cup). Swirl the cup gently. After the tablet(s) for suspension dissolve, drink the mixture right away. If there is any medication left in the cup, add some more liquid and swirl the cup gently. Drink the mixture of water right away to be sure that you swallow all of the medication.\nIf you are taking the levetiracetam oral solution, do not use a household spoon to measure your dose. You might not get the right amount of medication. Ask your doctor or pharmacist to recommend a medicine dropper, spoon, cup, or syringe and to show you how to use it to measure your medicine.\nYour doctor may start you on a low dose of levetiracetam and gradually increase your dose, not more often than once every 2 weeks.\nLevetiracetam controls epilepsy but does not cure it. Continue to take levetiracetam even if you feel well. Do not stop taking levetiracetam without talking to your doctor, even if you experience side effects such as unusual changes in behavior or mood. If you suddenly stop taking levetiracetam, your seizures may become worse. Your doctor will probably decrease your dose gradually.\nWhat is a drug interaction?\nA drug interaction can be defined as an interaction between a drug and another substance that prevents the drug from performing as expected. This definition applies to interactions of drugs with other drugs (drug-drug interactions), as well as drugs with food (drug-food interactions) and other substances.\nWhenever two or more drugs are being taken, there is a chance that there will be an interaction among the drugs. The interaction may increase or decrease the effectiveness of the drugs or the side effects of the drugs. The likelihood of drug interactions increases as the number of drugs being taken increases. Therefore, people who take several drugs are at the greatest risk for interactions. Drug interactions contribute to the cost of healthcare because of the costs of medical care that are required to treat problems caused by changes in effectiveness or side effects. Interactions also can lead to psychological suffering that can be avoided. This review discusses the issue of drug interactions and several ways to avoid them.\nWhat Not To Take With Keppra\nStudies have shown that there may be an interaction between Keppra and any of the following drugs and substances when used together:\n- antihistamines (e.g., cetirizine, doxylamine, diphenhydramine, hydroxyzine, loratadine)\n- antipsychotics (e.g., chlorpromazine, clozapine, haloperidol, olanzapine, quetiapine, risperidone)\n- barbiturates (e.g., butalbital, pentobarbital, phenobarbital)\n- benzodiazepines (e.g., alprazolam, diazepam, lorazepam)\n- chloral hydrate\n- general anesthetics (medications used to put people to sleep before surgery)\n- kava kava\n- muscle relaxants (e.g., baclofen, cyclobenzaprine, methocarbamol, orphenadrine)\n- narcotic pain relievers (e.g., codeine, fentanyl, hydrocodone, morphine, oxycodone)\n- seizure medications (e.g., carbamazepine, clobazam, phenytoin, primidone, topiramate, valproic acid, zonisamide)\n- selective serotonin reuptake inhibitors (SSRIs; e.g., citalopram, fluoxetine, paroxetine, sertraline)\n- sodium oxybate\n- tricyclic antidepressants (e.g., amitriptyline, clomipramine, desipramine, trimipramine)\nIf you are taking any of these medications, speak with your doctor or pharmacist. Depending on your specific circumstances, your doctor may want you to:\n- stop taking one of the medications,\n- change one of the medications to another,\n- change how you are taking one or both of the medications, or\n- leave everything as is.\nAn interaction between two medications does not always mean that you must stop taking one of them. Speak to your doctor about how any drug interactions are being managed or should be managed.', 'Dihydrocodeine Comp oral Warnings\nProducts that contain codeine or dihydrocodeine should not be used in children after certain surgeries (including tonsil/adenoid removal). Also, for children younger than 12 years, do not use codeine or dihydrocodeine to treat cough or pain unless specifically directed by the doctor. Some children are more sensitive to codeine or dihydrocodeine and have had very serious (rarely fatal) breathing problems such as slow/shallow breathing (see also Side Effects section). Talk with your doctor or pharmacist about the risks and benefits of this medication.\nDihydrocodeine Comp oral Uses\nSee also Warning section.\nThis combination medication is used to treat moderate pain, fever, and swelling. The narcotic pain reliever (opiate-type) acts on certain centers in the brain to give you pain relief. Aspirin helps to decrease the pain, fever, and swelling. Caffeine helps increase the effects of aspirin on pain.How to use Dihydrocodeine Comp oral\nTake this medication by mouth as directed by your doctor. Take with a full glass of water (8 ounces or 240 milliliters) unless your doctor directs you otherwise. Do not lie down for at least 10 minutes after taking this medication. If you have nausea, you may take this medication with food. Consult your doctor or pharmacist about other ways to decrease nausea (e.g., antihistamines, lying down for 1-2 hours with as little head movement as possible).\nThe dosage is based on your medical condition and response to therapy. Pain medications work best if they are used as the first signs of pain occur. If you wait until the pain has worsened, the medication may not work as well.\nThis medication may cause withdrawal reactions, especially if it has been used regularly for a long time or in high doses. In such cases, withdrawal symptoms (such as runny nose, watery eyes, restlessness) may occur if you suddenly stop using this medication. To prevent withdrawal reactions, your doctor may reduce your dose gradually. Consult your doctor or pharmacist for more details, and report any withdrawal reactions immediately.\nThough very unlikely, abnormal drug-seeking behavior (addiction) is possible with this medication. To lessen the risk of becoming addicted, do not increase your dose, take it more frequently, or take it for a longer time than prescribed. Use the smallest effective dose. Properly stop the medication when so directed.\nWhen used for an extended period, this medication may not work as well and may require different dosing. Talk with your doctor if this medication stops working well.\nInform your doctor if your pain persists or worsens.\nDihydrocodeine Comp oral Side Effects\nNausea, vomiting, stomach upset, shaking (tremor), constipation, dry mouth, lightheadedness, dizziness, or drowsiness may occur. If any of these effects persist or worsen, notify your doctor or pharmacist promptly.\nTo lower your risk of dizziness and lightheadedness, get up slowly when rising from a sitting or lying position.\nTo prevent constipation, maintain a diet adequate in fiber, drink plenty of water, and exercise. If you become constipated while using this drug, consult your pharmacist for help in selecting a laxative (e.g., stimulant-type with stool softener).\nRemember that your doctor has prescribed this medication because he or she has judged that the benefit to you is greater than the risk of side effects. Many people using this medication do not have serious side effects.\nTell your doctor immediately if any of these unlikely but serious side effects occur: mental/mood changes, fast/irregular heartbeat, increased thirst/urination, hearing changes (e.g., ringing in the ears), easy bruising/bleeding, signs of infection (e.g., fever, persistent sore throat), heartburn, discomfort when swallowing, dark urine, yellowing eyes/skin, change in the amount of urine, difficulty urinating, unusual tiredness.\nSeek immediate medical attention if any of these rare but very serious side effects occur: fainting, seizure, black stools, severe stomach/abdominal pain, vomit that looks like coffee grounds, slurred speech, weakness on one side of the body.\nThis product may cause serious mental/mood changes and very serious (rarely fatal) breathing problems. This product may contain codeine or dihydrocodeine. Codeine and dihydrocodeine are changed into strong narcotic drugs (morphine or dihydromorphine) in your body. In some people this happens faster and more completely than usual, which increases the risk of very serious side effects. Get medical help right away if you notice any of the following: slow/shallow breathing, unusual drowsiness/difficulty waking up, confusion.\nA very serious allergic reaction to this drug is rare. However, seek immediate medical attention if you notice any symptoms of a serious allergic reaction, including: rash, itching/swelling (especially of the face/tongue/throat), severe dizziness, trouble breathing.\nThis is not a complete list of possible side effects. If you notice other effects not listed above, contact your doctor or pharmacist.\nIn the US -\nCall your doctor for medical advice about side effects. You may report side effects to FDA at 1-800-FDA-1088.\nIn Canada - Call your doctor for medical advice about side effects. You may report side effects to Health Canada at 1-866-234-2345.\nDihydrocodeine Comp oral Precautions\nBefore taking this medication, tell your doctor or pharmacist if you are allergic to aspirin, caffeine, or narcotic pain medications (e.g., codeine); or to salicylates (e.g., salsalate), nonsteroidal anti-inflammatory drugs (e.g., ibuprofen), or xanthine derivatives (e.g., theophylline); or if you have any other allergies. This product may contain inactive ingredients, which can cause allergic reactions or other problems. Talk to your pharmacist for more details.\nBefore using this medication, tell your doctor or pharmacist your medical history, especially of: asthma (including a history of worsening breathing with runny/stuffy nose after taking aspirin or other NSAIDs), bleeding/blood clotting disorders (e.g., hemophilia, von Willebrand\'s disease, thrombocytopenia), brain disorders (e.g., seizures, head injury, tumor, increased intracranial pressure), breathing problems (e.g., chronic obstructive pulmonary disease-COPD, sleep apnea), disease of the pancreas (e.g., pancreatitis), heart disease (e.g., irregular heartbeat, recent heart attack), certain enzyme deficiencies (pyruvate kinase or G6-PD deficiency), gout, liver disease, kidney disease, heartburn, growths in the nose (nasal polyps), a certain spinal problem (kyphoscoliosis), gallbladder disease, personal or family history of regular use/abuse of drugs/alcohol, mental/mood disorders, stomach/intestinal problems (e.g., ulcers, infectious diarrhea, colitis), adrenal gland problem (e.g., Addison\'s disease), difficulty urinating (e.g., due to enlarged prostate or urethral stricture), underactive thyroid (hypothyroidism), recent bowel/abdominal surgery.\nBefore having surgery, tell your doctor or dentist that you are using this medication.\nThis drug may make you dizzy or drowsy. Do not drive, use machinery, or do any activity that requires alertness until you are sure you can perform such activities safely. Avoid alcoholic beverages.\nThis medicine may cause stomach bleeding. Daily use of alcohol and tobacco, especially when combined with this medicine, may increase your risk for stomach bleeding. Limit alcohol and stop smoking. Consult your doctor or pharmacist for more information.\nThis medication contains aspirin. Children and teenagers less than 18 years old should not take aspirin if they have chickenpox, flu, or any undiagnosed illness, or if they have just been given a live virus vaccine, without first consulting a doctor about Reye\'s syndrome, a rare but serious illness.\nChildren may be more sensitive to the side effects of the narcotic pain reliever, especially extreme sleepiness, confusion, or slow/shallow/noisy breathing. (See also Warning section.)\nOlder adults may be more sensitive to the side effects of this drug, especially stomach bleeding/ulcers, confusion, dizziness, drowsiness, and slow/shallow breathing.\nBefore using this medication, women of childbearing age should talk with their doctor(s) about the risks and benefits. Tell your doctor if you are pregnant or if you plan to become pregnant. This medication is not recommended for use during pregnancy. It may harm an unborn baby and interfere with normal labor/delivery. Consult your doctor for more details.\nThis drug passes into breast milk and could have undesirable effects on a nursing infant. Therefore, breast-feeding is not recommended while using this drug. Consult your doctor before breast-feeding.\nDihydrocodeine Comp oral Interactions\nDrug interactions may change how your medications work or increase your risk for serious side effects. This document does not contain all possible drug interactions. Keep a list of all the products you use (including prescription/nonprescription drugs and herbal products) and share it with your doctor and pharmacist. Do not start, stop, or change the dosage of any medicines without your doctor\'s approval.\nSome products that may interact with this drug include: acetazolamide, adenosine, antacids, anticholinergic medications (e.g., scopolamine), beta agonists (e.g., albuterol), certain antibiotics (e.g., penicillin, sulfonamides such as sulfamethoxazole, macrolide antibiotics such as erythromycin), certain anti-seizure medications (e.g., phenytoin, valproic acid), certain drugs used to treat gout (e.g., uricosuric drugs such as probenecid, sulfinpyrazone), cimetidine, corticosteroids (e.g., prednisone), disulfiram, drugs that may affect hearing (e.g., vancomycin), lithium, MAO inhibitors (isocarboxazid, linezolid, methylene blue, moclobemide, phenelzine, procarbazine, rasagiline, selegiline, tranylcypromine), 6-mercaptopurine, methotrexate, mifepristone, naltrexone, other medications for pain (e.g., pentazocine, nalbuphine, morphine), pemetrexed, vemurafenib, zidovudine.\nThis medication may increase the risk of bleeding when taken with other drugs that also may cause bleeding. Examples include anti-platelet drugs such as clopidogrel, ""blood thinners"" such as dabigatran/enoxaparin/warfarin, among others.\nThe risk of serious side effects (such as slow/shallow breathing, severe drowsiness/dizziness) may be increased if this medication is taken with other products that may also affect breathing or cause drowsiness. Therefore, tell your doctor or pharmacist if you are taking other products such as alcohol, medicine for sleep or anxiety (such as alprazolam, diazepam, meprobamate, zolpidem), muscle relaxants, and other narcotic pain relievers (such as morphine).\nCheck the labels on all your medicines (e.g., cough-and-cold products, other headache medications) because they may contain aspirin, caffeine, or drowsiness-causing ingredients. Also keep in mind that certain beverages (e.g., coffee, colas, tea) contain caffeine. Ask your pharmacist about using those products safely.\nAlso, check all prescription and nonprescription medicine labels carefully since many medications contain pain relievers/fever reducers (NSAIDs such as ibuprofen, ketorolac, naproxen, aspirin) that may increase your risk for side effects if taken together with this medication. However, if your doctor has directed you to take low-dose aspirin to prevent heart attack or stroke (usually at dosages of 81-325 milligrams a day), you should continue taking the aspirin unless your doctor instructs you otherwise. Ask your doctor or pharmacist for more details.\nThis medication may interfere with certain medical/laboratory tests (including fasting blood glucose, cholesterol levels, prothrombin time, urine 5-HIAA levels, amylase and lipase levels, certain urine glucose tests, dipyridamole-thallium imaging tests), possibly causing false test results. Make sure laboratory personnel and all your doctors know you use this drug.\nDihydrocodeine Comp oral Overdose\nIf overdose is suspected, contact a poison control center or emergency room immediately. US residents can call their local poison control center at 1-800-222-1222. Canada residents can call a provincial poison control center. Symptoms of overdose may include: severe drowsiness, slow/shallow breathing, very small pupils, severe dizziness, persistent nausea/vomiting, ringing in the ears.NOTES:\nDo not share this medication with others. It is against the law.\nThis medication has been prescribed for your current condition only. Do not use it later for another condition unless told to do so by your doctor. A different medication may be necessary in that case.MISSED DOSE:\nStore at room temperature away from light and moisture. Do not store in the bathroom. Keep all medications away from children and pets.\nDo not flush medications down the toilet or pour them into a drain unless instructed to do so. Properly discard this product when it is expired or no longer needed. Consult your pharmacist or local waste disposal company.\nInformation last revised February 2014. Copyright(c) 2014 First Databank, Inc.']"	['<urn:uuid:46160306-77f2-45a4-9ac4-2830e77fd81a>', '<urn:uuid:299de736-af6c-494d-8087-d71e7dd88cf9>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T11:40:26.317848	17	81	2998
100	What are the publishing rules for PHM papers?	The Prognostic and Health Management Society uses a Creative Commons Attribution 3.0 United States license, which allows anyone to copy, distribute, transmit and adapt works with proper attribution while authors retain their copyright. Authors grant the society rights to publish and disseminate accepted works through electronic and print media.	['This paper outlines a benchmarking approach for evaluating the diagnostic and prognostic capabilities of the Vehicle Integrated Prognostic Reasoner (VIPR), a vehicle-level reasoner and an architecture which aims to detect, diagnose, and predict adverse events during the flight of an aircraft. A number of diagnostic and prognostic metrics exist, but these standards are defined for well-circumscribed algorithms that apply to small subsystems. For layered reasoners, such as VIPR, the overall performance cannot be evaluated by metrics solely directed toward timely detection and accuracy of estimation of the faults in individual components. Among other factors, the overall vehicle reasoner performance is governed by the effectiveness of the communication schemes between the different monitors and reasoners in the architecture, and the ability to propagate and fuse relevant information to make accurate, consistent, and timely predictions at different levels of the reasoner hierarchy. To address these issues, we outline an extended set of diagnostic and prognostics metrics that can be used to evaluate the performance of layered architecture, and we discuss a software architecture*as well as an evaluation plan for benchmarking VIPR.\nHow to Cite\ndiagnostic and prognostic metrics\nKurtoglu, T., Mengshoel, O, & Poll, S. (2008). A Framework for Systematic Benchmarking of Monitoring and Diagnostic Systems, International Conference on Prognostics and Health Management Conference, Denver, CO.\nSaxena, A., Celaya, J., Balaban, E., Goebel, K., Saha, B., Saha, S., & Schwabacher, M. (2008). Metrics for Evaluating Performance of Prognostic Techniques, in International Conference on Prognostics and Health Management (PHM08), Denver CO.\nLeao, B.P., Yoneyama, T., Rocha, G.C., & Fitzgibbon, K.T. (2008). Prognostics performance metrics and their relation to requirements, design, verification and cost-benefit. Proceedings of the International Conference on Prognostics and Health Management (PHM08), Denver, CO.\nKurtoglu, T., Narasimhan, S., Poll, S., Garcia, D., Kuhn, L., de Kleer, J., van Gemund, A., & Feldman, A. (2010). Towards a Framework for Evaluating and Comparing Diagnosis Algorithms. Proceedings of the 20th International Workshop on Principles of Diagnosis, Stockholm, Sweden.\nChristensen, M. (2010). Boeing 787 Central Maintenance Computing Function Summary, Technical Report, Honeywell Labs.\nNASA (2009). Integrated Vehicle Health Management Technical Plan, Version 2.03, National Aeronautics and Space Administration.\nButton, R.M. & Chicatelli, A. (2005). Electrical Power System Health Management, Proc. 1st International Forum on Integrated System Health Engineering and Management in Aerospace, Napa, CA.\nBiswas, G., Kapadia, R., & Xu, X.W. (1997). Combined Qualitative–Quantitative Steady-State Diagnosis of Continuous-Valued Systems, IEEE Transactions on Systems, Man, and Cybernetics— Part A: Systems and Humans, 27(2), pp. 167-185.\nThe Prognostic and Health Management Society advocates open-access to scientific data and uses a Creative Commons license for publishing and distributing any papers. A Creative Commons license does not relinquish the author’s copyright; rather it allows them to share some of their rights with any member of the public under certain conditions whilst enjoying full legal protection. By submitting an article to the International Conference of the Prognostics and Health Management Society, the authors agree to be bound by the associated terms and conditions including the following:\nAs the author, you retain the copyright to your Work. By submitting your Work, you are granting anybody the right to copy, distribute and transmit your Work and to adapt your Work with proper attribution under the terms of the Creative Commons Attribution 3.0 United States license. You assign rights to the Prognostics and Health Management Society to publish and disseminate your Work through electronic and print media if it is accepted for publication. A license note citing the Creative Commons Attribution 3.0 United States License as shown below needs to be placed in the footnote on the first page of the article.\nFirst Author et al. This is an open-access article distributed under the terms of the Creative Commons Attribution 3.0 United States License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.']	['<urn:uuid:978e13b8-0592-4932-a09a-4d68a9bcaa0b>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T11:40:26.317848	8	49	639
