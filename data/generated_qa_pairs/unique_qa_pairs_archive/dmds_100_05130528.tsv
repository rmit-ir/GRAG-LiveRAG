qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	What was used to build highways in Illinois during the 1960s?	During the 1960s, when the interstate highway system was being built, sand from a company operating in the outwash plain south of the Bloomington moraine provided all of the sand used in paving the stretch of highway from Champaign to Kankakee.	"[""Faculty AffiliateEmail: firstname.lastname@example.org\n- GEO 491-X ST - Exploration Geophysics\nMy grade school years were spent in east-central Illinois. My family lived at the toe of the Bloomington moraine. My grandfather operated a company that mined aggregates from the outwash plain south of the moraine. In the 1960's, as the interstate highway system was being built, my grandfather's company provided all of the sand that was used in paving the stretch of highway from Champaign to Kankakee. I saw a lot of interesting rocks in his gravel pits during my childhood. He was ahead of his time in voluntarily reclaiming pits as they became uneconomical. He developed home sites and recreational areas starting as early as the late 1940's..\nMy high school days were spent in the coal fields of eastern Kentucky and the karst country of southern Indiana.\nI started college intending to major in physics. While taking an elective in geology, I realized that sand, gravel, coal, and karst, and a lot of other geological things were calling my name. My interest in geophysics was sparked when I took a geophysics course during my senior year. Seismic methods became my primary interest the following summer when I worked on a seismic project in the Jefferson Basin near Whitehall, MT.\nB.S., Geology, 1969, Indiana University\nM.A., Geology (Geophysics focus), 1971, Indiana University\nPh.D., Geology (Geophysics focus), 1975, University of Montana\n- Application of reflection and refraction seismic methods in minerals exploration.\n- Maintenance of the Mansfield Library digital archive of the 1970 Flathead Lake seismic data and derivative works.\n- Mentoring students in processing and interpreting the 1970 Flathead Lake seismic data.\n- Research on the use of reflection seismic methods in hard rock exploration\nField of Study\n- Exploration Geophysics\nLankston, R. W., 2011, New display of the 1970 Flathead Lake seismic data: Northwest Geology, v. 40, p. 55-62.\nLankston, R. W., 2007, Revisiting the 1970 Flathead Lake seismic survey: The Leading Edge, v. 26, n. 8, p. 1058-1063.\nLankston, R. W., 1990, High‑resolution refraction seismic data acquisition and interpretation, in Geotechnical and Environmental Geophysics, Volume 1: Review and Tutorial, S. Ward (ed.): Soc. Explor. Geophys., p. 45‑78.\nLankston, R. W., 1989, The seismic refraction method: a viable tool for mapping shallow targets into the 1990's: Geophysics, v. 54, n. 12, p. 1535‑1542.\nLankston, R. W., 1988, The 1987 Geophysical Field Exercise Final Report: University of Arkansas Department of Geology, 15 p.\nLankston, R. W., 1988, High resolution refraction seismic methods: Symposium on the Application of Geophysics to Engineering and Environmental Problems Proceedings, p. 349‑387.\nLankston, R. W., and Hecker, B. W., 1988, Enhancing VLF‑EM data through application of frequency domain operators: Nat. Water Well Assoc., Second National Outdoor Action Conference Proceedings, p. 655‑673.\nLankston, R. W., Lankston, M. M., Schmidt, C. J., Ahlert, M. A., Hecker, B. W., Kaplinski, M. A., and Spellman, J. L., 1987, A geophysical study at the intersection of two fault‑bounded Neogene basins in southwestern Montana: Final Report of the 1986 University of Arkansas Geophysical Field Exercise: University of Arkansas Department of Geology 37 p.\nLankston, R. W., and Lankston, M. M., 1986, Obtaining multilayer reciprocal times through phantoming: Geophysics, v. 51, p. 45‑49.\nLankston, R. W., and Lankston, M. M., 1986, The microcomputer: a low‑pass filter of the effects of economic trends for small geophysical contracting companies, in Microcomputer Applications in Geology, J. T. Hanley and D. F. Merriam (eds.): Computers and Geology, v. 5, Pergamon Press, p. 17‑30.\nLankston, R. W., and Lankston, M. M., 1985, Analysis of the T6S, R7W magnetic anomaly, southwestern Montana: Montana Bur. Mines and Geol. Open File Report #165, 21 p.\nLankston, R. W., Lankston, M. M, and West, L. M., 1985, Seismic reflection profiling in the Cedar River Basin, western Washington: Nat. Water Well Assoc., Surface and Borehole Geophysics Methods in Ground Water Investigation Conference Proceedings, p. 146‑164.\nLankston, R. W., and Lankston, M. M, 1983, An introduction to the utilization of the shallow or engineering seismic reflection method (3rd ed.): Geo‑Compu‑Graph, Inc., 37 p\nLankston, M. M., and Lankston, R. W., 1979, Integration of NURE and other data sets with emphasis on their utilization in generating exploration models in the Lubbock, TX 1o x 2o Quadrangle (GJBX‑135(79)): Bendix Field Eng. Corp., 169 p.\nLankston, R. W., and Lankston, M. M., 1978, Integration of engineering seismic reflection data with other geophysical data sets, preliminary evaluation: 16th Annual Idaho State Highway Dept. Eng. Geol. and Soils Eng. Symposium Proceedings, p. 365‑379.\nRudman, A. J., and Lankston, R. W., 1973, Stratigraphic correlation of well logs by computer techniques: Amer. Assoc. of Pet. Geol., Bull., v. 57, n. 3, p. 577‑588.\n- Shallow target refraction and reflection seismic methods\n1980-81, University of Idaho.\n- General exploration geophysics.\n1983-90, University of Arkansas\n- General exploration geophysics\n- Advanced gravity, magnetic, and seismic methods\n- Geophysical Field Exercise in Jefferson-Beaverhead Basins\n- Computer methods in geology\nPart time 2008-2015, University of Montana\n- Seismic analysis in hydrocarbon exploration\n- General exploration geophysics\n1970, internship, Amax Coal, Indianapolis.\n- Application of gravity and refraction seismic methods to Illinois Basin coal exploration.\n1974-76, Gulf Research and Development Corporation (now Chevron), Houston.\n- Gravity and magnetic methods in Gulf of Mexico and Rockies oil exploration\n1976-78, post-doc, Washington State University, Department of Civil and Environmental Engineering, Pullman.\n- Application of surface geophysical methods to groundwater and uranium exploration in and around the Columbia Basin\n1978-83, Geo-Compu-Graph, Inc., Spokane.\n- Seismic surveying for placer gold, uranium, and groundwater exploration and environmental site evaluation.\n- Geophysics software development for microcomputers\n- Research in shallow target refraction and reflection seismic methods\n1990-2006, Conoco and ConocoPhillips, Ponca City and Houston.\n- Environmental geophysics\n- Environmental database management and GIS applications development\n- Seismic analysis\n- Geopressure interpretation and prediction software development and application and training\n- Gas hydrate research\n- Value of information modeling in North Sea oil development.\n2006-13, Geoscience Integrations, Missoula.\n- Gas hydrate exploration on the North Slope.\n2013-present. Childs Geoscience, Inc., Bozeman.\n- Research on reflection seismic applications in minerals exploration.\n- Society of Exploration Geophysicists, American Association of Petroleum Geologists, Montana Geological Society\n- Co-founder and life member of the Tobacco Root Geological Society\nGuitar playing, Amateur Radio operating, nature- and history-based touring by automobile, geophysical software development in Python""]"	['<urn:uuid:d843875f-721c-4bf9-89f1-157bbd07276b>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T05:28:56.360010	11	41	1048
2	How do the risks and challenges compare between piecemeal control system migration versus full data center outsourcing to a partner?	A piecemeal control system approach risks ending up with an eclectic system, while requiring less downtime. For data center outsourcing to a partner, the main challenges are around flexibility in migration hub location and the need to support a wide range of sources and targets in the migration process. Both scenarios require extensive upfront planning and careful consideration of security, with control systems needing cybersecurity risk assessment and data center outsourcing requiring data encryption during migration.	['Migrating controls: Plan ahead with any process control system migration to stay on track. Use front-end loading on the project, and ask these 10 questions about migrating from a legacy distributed control system (DCS) or other controls to upgraded options.\nWhat manufacturer doesn’t cringe at the thought of migrating legacy control system technology? Most control systems were installed or updated in the late 80s and 90s and are overwhelmingly fragmented and fraught with challenges. Taking a wait-and-see approach is no longer a viable option, as manufacturers face new standards and regulatory compliance mandates, along with disparate and obsolete systems requiring hard-to-find original equipment manufacturer (OEM) spare parts and higher support costs. Compounding these challenges are associated safety and cybersecurity risks in operating and maintaining legacy systems.\nWith all the new innovative technologies on the market, it’s a good time to reevaluate processes and systems and look at opportunities to improve operational efficiency and performance to stay competitive. For many manufacturers, tackling a project of this magnitude with limited bandwidth and resources is no easy feat. It’s also not easy deciding whether to replace systems piecemeal or start over with new systems. No matter the chosen path, upfront planning and a qualified team are key to moving into the execution phase of an upgrade or migration project for a distributed control system (DCS), process control system (PCS), or programmable logic controller (PLC)-based system.\nPlan and execute\nFirst, before deciding on which upgrade or migration path to take, it is important to take a holistic look at the entire facility’s operations. Regardless of a facility’s size, a solid plan from conception of the migration through startup must be considered for a successful project. A disciplined approach helps to define a scope that aligns with business goals and objectives and to outline facility requirements.\nThis front-end loading (FEL) engineering effort helps ensure an execution plan and schedule are in place to keep the project(s) on time and on budget. It also supports defendable control system solutions to determine the best platform, identify and define any risks, and provide accurate and justifiable cost estimates. The result is a successful project execution plan with a functional platform in place for operational efficiency and profitability.\nEngage a qualified DCS migration team\nWhere bandwidth and resource limitations are an issue, engaging third-party control system specific experts up front and using them throughout the project will increase the project’s success. Experienced guidance will pay dividends if obtained during the conceptual phase of the migration. Waiting to engage an external partner later in the project means many aspects of the scope will already be defined based on assumptions made up front that may or may not have included an all-inclusive view of the entire site’s needs. Changing these decisions late in the project will result in higher costs and schedule delays compared with getting them set correctly during the initial FEL phase.\nA platform-independent partner will deliver unbiased practical experience working on a variety of manufacturing processes and technologies. This qualified team of experts will utilize best practices and successfully help lead the project through the execution and implementation phases. It is best to work with a team early in the FEL stage to avoid delayed schedules and higher costs later.\nAdditionally, an ideal partner is one who is in it for the long haul and knows what is needed to improve performance moving forward. Continuous improvement initiatives are beneficial, especially once all is up and running smoothly. With these initiatives in place, manufacturers can optimize processes and increase operational safety and efficiency. To keep an ongoing competitive advantage, manufacturers can consult with their trusted partner to help incorporate these initiatives as part of the overall long-term plan.\nPiecemeal DCS replacement or rip and replace?\nThe pros and cons of whether to do a piecemeal replacement versus a rip and replace depend on the facility. Many factors need to be considered that are specific and unique to the facility and should be defined during the FEL effort. Neither approach is without some risk. For example, taking a piecemeal approach with a facility-wide plan risks ending up with an eclectic system. On the other hand, a full-scale control system migration requires extra planning to minimize unit downtime. In addition, the bigger the project, the longer it will take to program and test a new system.\nRipping and replacing the whole system, however, may not be required. Much depends on the lifecycle of the existing hardware/software and whether a manufacturer wants to stay with their existing control system original equipment manufacturer (OEM) or contract with an engineering, procurement, construction (EPC) company, or a process automation solutions provider.\nTen questions for DCS migration\nTo help choose a DCS migration path, consider the following 10 questions to help steer the overall project:\nIs there evidence new technology could improve output?\nIs there an existing installation base of new technology within the site that could be built or expanded upon?\nHow is the relationship with the existing OEM?\nDoes the current equipment allow for modern security?\nDoes the existing system have the modern amenities for ease of maintenance and engineering?\nDoes the existing system have the modern amenities for ease of future innovations?\nDoes the existing system have the necessary memory and space for expandability?\nIs the legacy system so old and antiquated that it would be easier to start over, especially if increased output could increase profitability?\nHave the risks for both migration paths been identified, and a cost put to them?\nFrom a conceptual perspective (FEL1 ±50%), which would cost more — piecemeal or rip and replace — and why?\nA trusted, unbiased third-party partner can address all these factors, offer professional guidance and help brainstorm the pros and cons of an upgrade or migration.\nKey to a successful DCS migration\nExpertise specifically for defining a control system plan, scope and budget are key to the success of the project regardless of the size and regardless of the control system platform. Depending on a project’s complexity, manufacturers should work with a partner who is knowledgeable in most DCS and PLC platforms and third-party interfaces.\nThey also should be able to create drawings or sketches for updates, create control system narratives, plan outages and give the project the attention it deserves. Sustaining services should be on the list as well. The third-party partner shouldn’t walk away once the project is commissioned and started up, either. With a planned approach and the right partner, manufacturers should end up with a fully integrated, highly-functioning and sustainable control system infrastructure.\nLynn Njaa is a senior consultant for Maverick Technologies’ DCSNext solution. Maverick Technologies is a Control Engineering content partner. Edited by Mark T. Hoske, content manager, Control Engineering, CFE Media, firstname.lastname@example.org.\nKEYWORDS: Distributed control system (DCS), migration, controls upgrade\nMigrating control systems requires extensive planning.\nEfficiency and cybersecurity risks increase as legacy systems expire.\nAsk these DCS migration questions to determine the best path.\nDon’t wait until it’s too late to migrate legacy control systems. A threat may exist, even if you don’t know about it yet.', 'Data Center Migration Tools and Real-time Replication\nData Center Migration: A Practical Approach\nIn today’s data-focused business environment, almost all organizations face data migration challenges. While it is possible to meet these challenges using script-based or manual approaches, many organizations prefer to use a proven real-time replication tool as a cornerstone of such projects, especially when migrating multiple databases as may occur in the following scenarios:\n- Moving data between on-premises and cloud systems\n- Cross-platform operating system migrations\n- Hardware migrations due to upgrade or failure\nThe technical complexity inherent in these scenarios can also be compounded by the need to ensure application availability and business continuity during the migration process. For organizations that provide 24×7 service, whether they be web-based businesses, airline systems, government or defense agencies, database and data center migrations cannot be allowed to interfere with their online operations.\nFour Steps to Migrating a Database with Minimal Risk\nMany organizations must, therefore, strive to deliver maximum application and data availability during the migration process. Migrating a database with minimum downtime and risk involves four steps:\n- Schema build\n- Initial data load\n- Change data capture\n- Validation and Repair\nIn a previous blog, we reviewed these steps and a best practice guide to database migration in detail. In this blog, we will focus on data center migration that, in many respects, requires the same approach as individual database migrations.\nData Center Migration Tool Requirements\nHowever, unlike migrating a single database which can be done either manually or using scripts, migrating a data center efficiently requires the use of scalable, robust and easily managed data replication tool that can automate the migration process and handle the variety and volume of data that needs to be migrated across data center environments. Based on our experience, there are two specific features that we believe data center migration tools must provide:\n- Hub and Spoke Architecture – in a complex project with many component tasks, it is critically important to have a single point of control and a single view of all relevant activity.\n- Network Efficiency – when moving large volumes across a network that is also supporting operational systems, it is important to be able to do this without impacting network performance. To achieve this, migration tools must offer advanced features such as highly efficient data compression and large block transfers.\nTwo Common Data Center Migration Scenarios\nRecently, HVR has seen an increase in two specific data center migration scenarios. First, migration of some or all data center assets to an outsourcing partner and, second, migration of selected data centers assets from an on-premises data center to the cloud. Although both of these scenarios have much in common, for example, the need for data encryption during the migration process, they each have some specific characteristics and needs, for example:\n- Outsourcing – it is important that there is flexibility in where the migration hub is located. Some companies will want or need the hub to be on-premises and under their management, while other companies will either be agnostic on this issue or will want the outsourcing company to host the migration hub at their hosting site. Also, there is likely to be a wide range of sources and targets to be supported in the migration process and, ideally, these should be supported by a single migration tool, such as HVR.\n- Cloud migration – the most obvious need for a successful cloud migration is that the migration tool supports all of the traditional SQL-based data sources and the appropriate cloud targets, forexample, Amazon RDS and/or Redshift and Microsoft Azure. One of the other needs or expectations that HVR has encountered for cloud migrations is that they can be accomplished quickly. With HVR, this rapid migration is easily facilitated by cloning as many HVR Virtual Machines as necessary to support the migration process within the customer’s timeline.\nMany of HVR’s customers and business partners use HVR as their migration tool of choice and cite the following reasons for this decision:\n- Simplification – HVR’s centralized hub concept provides the visibility and control to manage complex data center migrations.\n- Single tool for data migration, replication and real-time data integration – the ability to use a single tool in all three scenarios reduces licensing costs and the learning curve thereby reducing overall cost of ownership.\n- Best-in-Class Support – because HVR is a data migration specialist, we know that when you have a data migration problem, you need knowledgeable support now and our global support team is geared up to meet that expectation.\nIf you are considering migrating all or parts of your data center to an outsourcer, to the cloud or between hardware environments, please contact us for the opportunity to help you through what can be a challenging process.']	['<urn:uuid:3ff264e7-60fa-4856-a0f6-37c73fb2de91>', '<urn:uuid:fb34c314-17aa-4bc9-9d97-0cfd032e3873>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-13T05:28:56.360010	20	76	1983
3	africas kabalagala pancakes versus american camping pancakes compare ingredients needed	Kabalagala (Ugandan) pancakes require cassava flour, ripe bananas, salt, baking soda, and pepper powder, while American camping pancakes need regular flour, sugar, baking powder, salt, milk, fat (butter or oil), eggs, and extract (vanilla or almond).	['616. Kabalagala (Ugandan Pancakes)#BreadBakers\nKabalagala (Ugandan Pancakes)#BreadBakers\nSo many different types its unbelievable\nI didn’t have any theme in mind when I volunteered to host the month of February for the Bread Bakers group.As Christmas got over and the year was ending I knew I had to think of a theme pretty soon. I went back to the rules for participants and there the word pancake kept staring at me. The only pancakes I know are the usual Indian ones like dosas, uttappams, chilas etc and of course the typical fluffy American style ones dripping with melted butter and honey or maple syrup. Did a bit of research (what would we do without Google?) and hey presto the world opened up to me with a variety of pancakes I’d never heard of. So I challenged the members to the theme of pancakes from different parts of the world, to venture out to pancakes they’ve not made before.\nI am so grateful that the members took time to research and have come up with different pancakes. Check the list below and please visit each blog to find out the different names and recipes of the pancakes.\nSo basically the dictionary describes pancakes as a thin flat cake of batter fried on both sides on a griddle or in a frying pan. However, as I researched, the meaning of pancake widened. Some pancakes are baked, some are fried and some may appear like flat breads but are actually known as pancakes in the region of origin.Some have yeast as leavening agent others have baking agents. Pancakes can be sweet or savory, may contain different types of flours, fruits, vegetables. There’s a whole world of pancakes out there. Drop scones, waffles, crumpets, pikelets, oatcakes are classified as pancakes.(However countries of origin may refute that!)\nMy contribution towards this theme is a pancake from Uganda. Uganda is a landlocked East African country, neighbouring Kenya on the west side. When one mentions Uganda and the immediately one thinks of the dictator ruler Idi Amin. His rule ruined a country which at one time was known as the Pearl of Africa. Uganda has very fertile farmlands and amazing National Parks.Its exports coffee and other produces. The staple food in Uganda is maizemeal, plantains, peanuts, cassava along with meat. Kenyans love the small bananas or menvu as they are called in Uganda. They are sweet and irresistible.\nThese pancakes from Uganda are called Kabalagala. They are made from cassava (tapioca) flour, mashed sweet banana or plantains. Gluten free,sugar free they look more like doughnuts but every possible Ugandan blog post, article I read about food from Uganda describes Kabalagala as a pancake.The recipe is very simple and the pancakes were absolutely delicious with a hot cup of coffee. Kabalagala in the Luganda language means pancake made using sweet bananas and cassava flour.The original preparation made by Nubians was called kabalagara. An affluent area in the city of Kampala is named after the pancake. Kabalagala is a famous street food in Uganda, enjoyed with tea as breakfast or served with stew. It is believed that these pancakes became very famous as a cheap alternative to cakes and bread during the Idi Amin Regime as they were affordable and combination of banana and cassava keeps one’s tummy full for a long period of time.\nKABALAGALA (UGANDAN PANCAKES)\nRecipe source: Here\n2 big or 6 small over ripe bananas\n2-2¼ cups cassava(tapioca) flour\na generous pinch of salt\n¼ tsp soda bicarbonate (baking soda)\n¼ tsp pepper powder\noil for deep frying\nextra flour for dusting\n- Peel and mash the bananas.\n- Sift flour, salt and pepper powder together.\n- Add flour little by little into the mashed banana and mix with a spatula or a spoon.\n- Keep on adding the flour till its thick enough to knead.\n- Dust the worktop with some flour and knead the dough. The dough should not be sticky. I used about 2¼ cups of flour.\n- Roll it out into a ¼” thick circle. Using a cookie cutter or a glass, cut out round discs.\n- Gather up the remaining dough and roll again and cut. Keep on repeating the process till all the dough is used up.\n- Heat oil in a wok or deep frying pan over medium heat. The oil is ready when a small piece of dough put in the oil rises to the top immediately.\n- Fry the pancakes till they are golden brown.\n- Dust some icing sugar if you like before serving.\n- I found the sweetness from the ripe bananas was just right. If you have a sweeter tooth, add 1-2 tbsp sugar.\n- Original recipes do not add baking agent. Adding it makes it more chewable.\n- A little bit of pepper and salt balances the sweetness from the bananas.\n- The leftover pancakes next day became more chewy. I would recommend that you eat them the day they are prepared.\n- Before frying the pancakes, brush off the excess flour you’ve used for dusting.\nCheck out the Pancakes from different parts of the world that our fellow Bread Bakers have made this month:\n- Alagar Kovil Dosai from Sara’s Tasty Buds\n- Blueberry Dutch Baby from Hostess At Heart\n- Brown Rice Dosa (Indian Savory Crepes) from Spiceroots\n- Buckwheat, Blackberry and Saffron Drop Scones from A Shaggy Dough Story\n- Chinese Scallion Pancakes from Karen’s Kitchen Stories\n- Corn Pancakes from Kids and Chic\n- Crepes from A Baker’s House\n- Dutch Baby from Herbivore Cucina\n- Galettes de Sarrasin from The Bread She Bakes\n- Greek Tiganites from Gayathri’s Cook Spot\n- Hotteok (Korean Pancakes) from Cook’s Hideout\n- Hotteok (Korean Stuffed Pancakes) from Passion Kneaded\n- Kabalagala (Ugandan Pancakes) from Mayuri’s Jikoni\n- Keralan Yeast Appam from Food Lust People Love\n- Malpua (Sweet Indian Crepes) from SimplyVeggies\n- Oven Baked Tropical Pancakes from A Day in the Life on the Farm\n- Pannukkau (Finish Pancakes) from Cindy’s Recipes and Writings\n- Potato Latkes (Jewish Pancakes) from Sneha’s Recipes\n- Savory Finnish Baked Pancakes(Pannukakku) with Smoked Salmon from The Wimpy Vegetarian\n- Srilankan Hoppers from I camp in my Kitchen\n- Strawberry Nutella Crepes from Spill the Spices\n- Swedish Pancakes from Palatable Pastime\n- Sweet Potato Pancakes with Brown Sugar and Pecan Sauce from A Salad For All Seasons\n- Wholegrain Yeast Pancakes from Ambrosia\n#BreadBakers is a group of bread loving bakers who get together once a month to bake bread with a common ingredient or theme. You can see all our of lovely bread by following our Pinterest board right here. Links are also updated after each event on the #BreadBakers home page.\nWe take turns hosting each month and choosing the theme/ingredient. If you are a food blogger and would like to join us, just send Stacy an email with your blog URL to firstname.lastname@example.org.', 'Camping and pancakes just seem to go together. Sitting at the picnic table with a stack of fluffy round pillows, drizzled in maple syrup. If you serve them with a couple strips of bacon and a cup of coffee, it’s breakfast heaven.\nWhenever our troop goes camping, there is always at least one patrol that serves pancakes for breakfast. And, while pancakes are relatively simple to make, there are a few tricks to attaining those light, fluffy pillows we all dream about. Come on, admit it. I know you dream about pancakes, too.\nOn my Outdoor Cooking Skill Progression Chart, I put pancakes a couple steps up for the following reasons: You have to measure your ingredients because, technically, you’re baking and there is some chemistry involved. It is hard not to over mix them. Skillet and griddle work is a little more challenging because you have to manage your heat better. And, then there is the whole flipping the pancake that takes a bit of skill and coordination, and practice.\nSo, let’s dive into the principles of pancakes and I’ll share best practices and common mistakes.\nWhen prepping the pancakes, you want to divide the ingredients into two categories: dry and wet.\nDry ingredients include flour (AP, whole wheat, rice, almond, etc.), sugar (granulated or brown), baking powder (to make them light and fluffy), and salt (balances and enhances the flavors). Your dry ingredients can be prepped at home and loaded into a container or resealable bag. Just be sure you are making enough because you won’t be able to prep more in camp.\nWet ingredients include milk (buttermilk, milk or non-dairy milk), fat (butter or vegetable oil), eggs (for structure and adds to the light and fluffiness), and extract (vanilla, almond, etc.). Your wet ingredients can also be prepped at home and poured into a tightly sealed plastic bottle for the ride to camp in your cooler.\nPrepping and Mixing\nIn two separate bowls mix together all your dry ingredients and all your wet ingredients. When you are ready to combine, make a little well in your dry ingredients and pour in the wet ingredients. Gently stir together just until combined. DO NOT OVERMIX. Resist the urge to stir out every single lump until it’s totally smooth. Trust me, a few lumps are okay.\nIf you over mix the batter you will end up with stacks of tough and chewy pancakes instead of the light and fluffy ones you were probably dreaming about. Tough and chewy pancakes are only good for one thing: Frisbee.\nSo, mix the batter just until the wet and dry ingredients are combined, and there are no more visible wisps of flour. The batter will be lumpy and, again, that’s okay.\nResting the Batter\nI usually mix my pancake batter and then turn my stove, which allows my batter to rest while the griddle heats. Resting the batter anywhere from 5-15 minutes allows the glutens that were activated during mixing to rest and relax. Also, the starch molecules in the flour have a chance to fully absorb the liquid. This will give the batter a thicker consistency.\nIt’s important to allow time for the griddle to get good and hot evenly. You want medium heat or about 375°F. Too low and your pancakes just won’t cook. Too high and they will be burnt on the outside and raw on the inside. We’re going for the Goldilocks heat: Just right. And, you may need to adjust along the way so don’t be afraid to do that.\nA good way to test your griddle is to wet your fingers with water and flick it onto the surface of the griddle. The griddle is ready if the water droplets sizzle and dance before they eventually evaporate. Medium heat will give us pancakes that are golden-brown on the outside with slightly crispy edges, and soft but cooked through on the inside. Pancake perfection!\nGreasing the Griddle\nI can’t count how often my young chefs don’t do this and it always leads to disaster and pancake tragedy. Always pack extra vegetable oil when you plan to make pancakes.\nWhen the griddle is up to temp, add a light coating of oil. We prefer vegetable oil, which tends to have a higher smoke point than butter and won’t add any flavor to the pancakes like olive oil.\nAfter your oil has a moment to warm, you can begin adding batter to the griddle. Try to control your excitement. We’re not out of the woods just yet.\nI prefer to use a ¼-cup measuring cup. It makes for a nice-sized pancake that is easy to flip. Depending on the size of your griddle, you could also get 6-8 pancakes per batch. This is important because once the pancakes start going down, the vultures will start circling.\nIf you have the griddle real estate and you are a more experienced pancake flipper, you could bump up to a ½-cup to 1-cup measuring cup for bigger pancakes. You can also use a smaller measuring scoop to make little silver dollar pancakes. Those are always fun.\nLightly coat the griddle with vegetable oil about every other batch of pancakes. Keep it light! Don’t let oil accumulate on your griddle or let your griddle run dry. If you see oil accumulating on the griddle, just use your spatula to redistribute it around the griddle. Remember to adjust the heat if you need to.\nPancakes should be flipped once, and only once, during cooking. And as long as you didn’t flip them too soon, you won’t need to flip them any more than that. Flipping pancakes too many times causes them to deflate, losing some of that wonderful fluffy texture.\nAs the pancake cooks, bubbles will start to form on the surface. Do not be tempted to pop them. I know, it’s fun, but when you pop the bubbles, you are releasing air from the neighboring chambers, essentially “flattening” the finished cake by vacating the air that was giving it some of its rise and fluff.\nThe pancake is ready to flip when the bubbles start to pop on their own and your edges are lightly browned and a little crispy, and the pancake is looking dry around the outer edges. If you’re still a little unsure, it’s okay to gently lift an edge and sneak a peek underneath. Generally, it will take about 2 minutes for the first side and 1-2 minutes for the second side. The most important thing is for the middle to be cooked.\nIf you are adding extras like fruit to the pancakes, add after you pour the batter onto the griddle. Blueberries are a classic add on, but you could also add sliced bananas, chocolate chips, nuts, or whatever you like.\nWhen you’re ready to serve the pancakes, you can serve with butter and maple syrup; however, you can also add flavored syrups, chocolate sauce, caramel sauce, peanut butter, cream cheese, fruits, nuts, whipping cream, and sprinkles. Just to name a few.\nServe your pancakes immediately or keep them warm by wrapping them in foil. If I’m making pancakes for a crowd and I want us to all eat together, I’ll warm up one of my large Dutch ovens to about 200°F and line it with foil. As I off load each batch, I add them to the Dutch oven to keep them warm.\nIt’s no fun to put stone-cold syrup on warm pancakes. If it’s a cold morning, you should warm your syrup. Your campers will love you for it.\nHere are a few of our favorite pancake recipes (and we’re always adding more) because pancakes are something we dream about:\nSnoqualmie Falls Oatmeal Pancakes\nPumpkin Spice Pancakes\nCinnamon Roll Pancakes\nIf you like this blog and don’t want to miss a single post, subscribe to Chuck Wagoneer by clicking on the Follow Us button in the upper right corner and follow us on Facebook and Pinterest for the latest updates and more stuff!']	['<urn:uuid:a9e91ade-eac0-4ba8-bb76-47dc90b51cb3>', '<urn:uuid:ffb6b834-a627-472d-aba2-3972d734788d>']	factoid	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-13T05:28:56.360010	10	36	2486
4	Can I use a paperclip to check my PSU fan?	No, using conductive items like paperclips on power supply fans is dangerous - it can cause a severe electric shock and trip circuit breakers. There's enough charge in a computer power supply to be potentially lethal.	['What do you do when a fan on your computer is loud enough to be disruptive to your work-flow, or is ruining the fun when doing other things? Today’s SuperUser Q&A post looks at the best way to catch the ‘guilty culprit’.\nToday’s Question & Answer session comes to us courtesy of SuperUser—a subdivision of Stack Exchange, a community-driven grouping of Q&A web sites.\nPhoto courtesy of el_finco (Flickr).\nSuperUser reader ‘Annonomus Person’ wants to know how to find out which computer fan is being so loud:\nOn my new computer, the fan(s) is/are plain LOUD! Using SpeedFan, I find that my temps are all under 32 degrees C with normal usage (IDK how accurate that is, but the CPU itself says 27 degrees C).\nThe fan doesn’t sound unbalanced, and it isn’t making sounds in a rhythm (not the usual rrrrrrrRRRRRR… rrrrrrrRRRRRR… rrrrrrrRRRRRR… rrrrrrrRRRRRR), it just runs constantly. I am thinking about oiling the fans, but I can’t decide if it is the PSU fan, a case fan, or the CPU fan. If it’s the CPU fan, I will most likely just replace it with a quiet fan. How can I tell which fan is making the noise (if not multiple ones)?\nOne thing that I think may be the problem is my CPU fan isn’t PWM, so would swapping that out help? There is no “linear voltage” or etc. thing in BIOS, so I think it may be running at full speed. Also, there could be too much airflow because it is also making a whistling noise that you can hear when close to it, and sounds like when I accidentally put part of the side cover over my floor vent to get it out of the way.\nWhat is the best method for finding out which fan is being loud?\nSuperUser contributors Hefewe1zen, Darrel Hoffman, and Ross Aiken have the answer for us. First up, Hefewe1zen:\nUse a small piece of plastic (like a pen cap) to stop the fan from spinning. That is the easiest way to isolate the cause. It’s okay to stop it for a few seconds while on. Most fan noise is due to failure of the bearings. Sometimes, lifting the sticker on the hub and oiling it with 3 in 1 lube will help with the noise.\nFollowed by the answer from Darrel Hoffman:\nWe used to do this on one of our older computers and it worked fine, but a word of warning – while a plastic pen cap is probably okay, DO NOT use anything conductive like a paper clip, especially on the power supply fan. A colleague of mine knows this from experience after getting a very nasty shock and tripping all the circuit breakers. He’s lucky he wasn’t killed – there’s supposedly enough charge in a computer power supply to be lethal.\nAnd finally, the answer from Ross Aiken:\nI’ll just put this here as an alternative solution:\nUnplug each fan, one-by-one from the motherboard (or from the PSU, depending on the fan), and when you stop hearing the noise, plug them back in one-by-one until you hear it again (to verify that the one you thought was making the noise actually was). I’d do the CPU fan last; everything else will be fine with ambient cooling for an extended period of time. The CPU fan is the one most likely to have issues.\nGranted, if you have a GPU with fans on it, you’ll probably need to use @Hefewe1zen’s method for those.\nI just don’t like telling people to put hard objects and/or fingers near fast-moving objects. Too high of a chance of someone hurting themselves (especially if they’re un-coordinated).\nA fan-by-fan approach, with a good measure of caution to avoid a nasty shock or injury, is definitely a good way to learn which fan is being loud.\nHave something to add to the explanation? Sound off in the comments. Want to read more answers from other tech-savvy Stack Exchange users? Check out the full discussion thread here.']	['<urn:uuid:42ae5ba8-7c51-49c2-8a1d-3250713a3cb8>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T05:28:56.360010	10	36	671
5	ways buy house first time requirements	For first-time homebuyers, there are several mortgage options available. FHA loans are particularly accessible, being government-insured mortgages that allow low down payments and are suitable for those with credit challenges and modest incomes. To qualify for a mortgage, three main factors are considered: your monthly payments as a percentage of income, available cash for down payment and closing costs, and credit history. Before closing, the lender verifies employment status, bank funds, and tax status. Seven specific FHA disclosures must be signed, including the Important Notice to Homebuyers and the Energy Efficient Mortgage Disclosure. The lender also checks government databases to ensure eligibility for FHA programs.	"[""A mortgage is closed once lender-required paperwork is signed, loan funds are disbursed to the appropriate party and ownership conveys to the borrower. A loan that isn't clear to close results in a delayed closing until the lender's conditions are met. A loan backed by the Federal Housing Administration requires a few unique items, but follows the same basic protocol before closing. Specific forms, verifications and money are needed to gain clearance for closing.\nThe FHA insures mortgages funded by approved lenders. It promises to repay lender losses if borrowers default. The government guarantees make it possible for lenders to offer low down-payment financing to borrowers with credit challenges and modest incomes. Because lenders originate, process and underwrite FHA loans, the lender controls the closing timeline and clears a file to close, rather than the FHA. The lender follows FHA guidelines to ensure that the agency will insure the loan once it is made.\nYou must sign seven FHA-specific disclosures to gain clearance for closing. The Important Notice to Homebuyers discloses what an FHA loan entails and includes an explanation of mortgage insurance premiums, which you pay each month. The For Your Protection: Get A Home Inspection form explains the difference between a home inspection and an FHA appraisal inspection, their limitations and why you need them. The Informed Consumer Choice Disclosure Notice shows you the conventional financing options that might be available to you. All parties to the transaction sign the Amendatory Clause and Real Estate Certification, which protects your from paying more for the home than it is actually worth and affirms that no agreements have been made outside of the real estate contract. The Energy Efficient Mortgage Disclosure lets you know that the FHA insures loans for home improvements that decrease your energy costs.\nFHA lenders typically complete most employment and financial verifications before you sign the final loan agreement documents. The lender verifies that you have sufficient funds in the bank to pay for the down payment and closing costs with a verification of deposit. It verifies that you are still employed and expected to continue your employment for at least the next three years with a verification of employment. The lender uses IRS form 4506-T to order a tax transcript and ensure your taxes are up-to-date. All parties to an FHA transaction must be cleared to close. The lender checks the Limited Denial of Participation and General Services Administration government databases of parties excluded from participating in FHA programs.\nThe lender clears you to close once you have deposited a 3.5 percent down payment and the amount needed for your closing costs with escrow. FHA loans made to borrowers with credit scores lower than 580 require a higher down payment of 10 percent. Closing costs for an FHA loan typically range between 3 percent and 5 percent of the sale price, according to the Home Buying Institute. You must wire funds directly from your bank account or provide a cashier's check for the amount needed to close."", ""- What are the various closing costs involved in a mortgage transaction?\nClosing costs can be divided into 3 main categories:\nYou will be provided with an estimate of your closing costs soon after your application has been received. These estimates may change if your loan program or loan amount changes.\n- Lender fees. Fees paid to the lender for the processing of your loan, such as discount points and origination and application fees.\n- Third-party fees. Fees paid for services rendered by parties other than the lender, such as title insurance, flood determination and recording fees.\n- Prepaid costs. Costs that are collected at the time of closing for items such as prepaid or per diem interest, property taxes and hazard insurance.\n- How much home can I comfortably afford?\nThe amount of home you can afford is based on the amount of mortgage loan you can comfortably support. Generally, the amount of mortgage you qualify for is based on 3 factors:\n- Your monthly payments as a percentage of income.\n- How much cash you have for the down payment and closing costs.\n- Your credit history.\n- What types of mortgages are available?\nThere are many types of mortgages available, including mortgages that could be a good fit for first time homebuyers, homebuyers who need a low down payment and buyers who need very large loan amounts.\nThe most general concepts for types of loans are as follows:\nFixed-rate mortgage. You pay the same interest rate and same monthly payment of principal and interest for the duration of the mortgage. The most common terms are 30, 20 and 15 years. Fixed-rate mortgages are best if you plan on being in your home for many years.\nAdjustable-rate mortgage (ARM). The interest rate stays fixed for an initial interest rate period, which typically ranges from 3 to 10 years. After that, the rate and monthly payment will adjust up or down annually for the life of the loan based on a specified index. An ARM is a good option if you believe interest rates will go down over the next few years or if you plan on staying in your home for just a few years. Due to the potential for significant monthly payment increases, borrowers should carefully consider their ability to comfortably handle increases due if interest rates rise.\nCombination loan. “Combination loans” refer to taking out two loans at the same time; a 'first lien' mortgage and a ‘second lien’ home equity loan or home equity line of credit. Taking out two loans may help you avoid the higher rates of a jumbo first mortgage when paired with a conforming first mortgage. Combination loans may allow you to build equity faster and/or lower your total monthly payment.\nAffordable housing programs. I can assist in offering you information about affordable housing assistance programs in which we participate that can help with down payment and/or closing costs for qualified low-to moderate -income home buyers.\n- What are the benefits of a 15-year mortgage?\nA 15-year mortgage allows you to own your home in half the time of a conventional 30-year mortgage. Although payments are higher with a 15-year mortgage, you'll save a considerable amount of money in interest over the life of your loan and build equity faster.\n- What are the tax advantages of owning a home?\nIn most cases, the mortgage interest (and property tax) may be itemized and deducted from your taxable income, lowering your overall tax bill. This can make your after-tax cost of homeownership lower than renting. Please consult your tax advisor regarding interest deductibility.\n- Should I get prequalified for a mortgage before I shop for a home?\nGetting prequalified for your mortgage is an important step before you shop for a home. It tells you how much home you can buy and makes applying for your mortgage easier.Footnote 1\nAfter completing your online Mortgage Prequalification Request, you simply follow up by phone with a mortgage loan officer to complete the prequalification process.\nPre-qualification is neither pre-approval nor a commitment to lend; you must submit additional information for review and approval.\n- Can I complete a prequalification form I've already started?\nNo. You won't be able to return to an incomplete prequalification form.\n- What’s an impound/escrow account?\nAn impound/escrow account is an account set up by a lender to hold funds that are set aside for the payment of property taxes and insurance. In addition to the principal and interest payment on your mortgage loan, you may elect—or be required—to put aside additional funds each month in an impound/escrow account to pay for property taxes and mortgage and hazard insurance. The lender holds the money in an impound/escrow account and makes the payments from the account when they are due.\n- After I get a new mortgage, can I access my account information online?\nYes, it's easy with Bank of America Online Banking. You can review your current principal balance and interest rate, payment amount and due date, 24 months of loan activity and more.\n- Can I pay my mortgage online?\nIf you are a current Bank of America customer who uses Online Banking, you can make your mortgage payment through Bill Pay. With Bill Pay, you can set up recurring payments so your regular payments are automatically paid when you want. You can also make immediate transfers between your Bank of America mortgage account and other Bank of America accounts that are linked to it.\n- When will I receive my year-end statement of interest paid for tax purposes?\nYear-end interest-paid statements (IRS Form 1098) are mailed out by the end of January. Contact us if you do not receive it by February 15. You may also be able to access a copy of your IRS Form 1098 by going to the Account Details tab for your mortgage account in Online Banking.\n- What is Bank of America's Clarity Commitment® Document?\nWe believe getting your home loan from a lender you trust is important. That's why you'll appreciate how clearly the Clarity Commitment® document is written.\n- It serves as a simple, one page summary of key terms of your mortgage.\n- Details are in easy-to-understand language, so you know what you're getting.\nThe Clarity Commitment® summary is provided as a convenience, does not serve as a substitute for a borrower's actual loan documents, and is not a commitment to lend. Borrowers should become fully informed by reviewing all of the loan and disclosure documentation provided.""]"	['<urn:uuid:961f84b9-7290-44be-b089-3a46a41bdac3>', '<urn:uuid:a304e27e-a4e8-4046-9735-948fd34dd1e6>']	open-ended	direct	short-search-query	distant-from-document	three-doc	novice	2025-05-13T05:28:56.360010	6	105	1588
6	What helps workers stay employed and companies keep staff?	Workers can maintain their employability through lifelong learning and regular training to keep their skills updated, especially given technological and organizational innovations and increasing retirement age. Companies can retain staff by creating healthy work environments, offering attractive job opportunities, implementing employee training programs, and developing specific skills needed for their organization.	['The Research Centre for Education and the Labour Market (ROA) is a research institute of the Maastricht University School of Business and Economics, established in 1986. The overarching research theme of ROA is the acquisition and depreciation of human capital over the life course in relation to the skills demanded on the labor market.\nROA’s mission is to conduct high quality research that has a strong policy impact. Building on a strong position in academia, ROA aims to inform and inspire policymakers and academics, and thereby to contribute to both scientific research and public and organizations’ human resource development policies. ROA’s research programme is organised in five themes:\n- Education and Occupational Career\n- Training and Employment\n- Health, Skills, and Inequality\n- Labour Market Dynamics\n- Human Capital in the Region\nThe first three themes focus on education and training as point of departure and study the drivers and outcomes at the individual, organizational and societal level. These programs represent ROA’s focus on Education and lifelong learning. The last two programs study the developments in labour supply and demand and the interactions between the two at the national and regional labour markets and the implications for study- and career choices and future skill shortages.\nEducation and Occupational Career\nProgram director: Prof. Dr. Rolf van der Velden\nThe Educational Careers program aims to provide a better knowledge about the Dutch education system with the aim of optimizing the development and use of talent and the optimization choices during the school career and the transition to the labor market.\nEducation serves several functions in society: the skill production function, the selection function, the allocation function and the socialization function. These functions can reinforce each other but can also be contradictory. Moreover, the way in which education fulfills these functions can be assessed based on various criteria (effectiveness, efficiency, justice and freedom of choice). We aim to provide a balanced view on the functioning of the Dutch education system, taking these complexities explicitly into account. Moreover, we look at the performance of education at different levels: the macro level (national education systems), the meso level (schools / classes), the micro level (students), as well as the relationships between these levels.\nImportant questions are:\n- What is the best time to develop certain skills in education?\n- How are these skills best be developed?\n- What should be the mix of skills learned at school (e.g. professional versus general skills; key skills versus citizenship; knowledge versus 21st century skills, broad versus narrow specialization)?\n- Are there trade-offs between the different functions of education and can these trade-offs be avoided?\n- What are institutional barriers to an optimal performance of the education system?\n- Careers in education.\n- Transition to the labor market.\n- Development of performance and citizenship.\n- The role of institutions.\n- Equality of opportunity.\nTraining and Employment\nProgram director: Prof. Dr. Andries de Grip\nThe Training and Employment research program wants to contribute to strengthening the productivity and sustainable employability of the potential labour force. In this respect, current society faces two challenges:\n- The shifts in skills required on the labor market due to technological and organizational innovations.\n- The extension of working life due to the increasing retirement age.\nThese challenges reinforce the need for lifelong learning to keep the skills of the potential labour force at the required level and up to date. This challenge lies at the individual as well as organization and society level.\n- Relationship skill obsolescence, training, employability, productivity and labour participation.\n- Effects of HRD and HRM for organization and employees at the organization and sector level.\n- Sustainable employability from a multidisciplinary perspective (change in tasks, skills workload and health) and how this is anticipated or recovered.\n- Sustainable employability of groups with a weak labour market position: lower educated, flex workers, older workers).\n- Reintegration of groups at a distance from the labor market.\n- Labour market for the elderly and retirement decisions.\nLabour Market Dynamics\nProgram director: Prof. Dr. Didier Fouarge\nIn both research and policy, there is a growing attention for the cognitive and non-cognitive skills that allow workers to perform their tasks at work in an optimal way. An important challenge is to better understand what drives the dynamics in the demand for and the supply of skills in relation to the growing flexibility of the labour market, the growing complexity of work, and internationalisation and automation that affect the nature of workers’ tasks.\n- The educational choices of youngsters, and occupational sorting over life course.\n- Changes in the workers’ tasks of workers and how it affects the demand for skills.\n- Commonality of tasks between jobs and the transfer of skills when changing jobs.\n- Adjustments in labour supply over the career.\n- Relationship between work dynamics and wage dynamics.\n- Employability of the low-skilled and the elderly.\n- Skills and retirement.\n- Recruitment choices and training policies of employers.\n- Expected developments in labour supply and demand in the medium term.\nHuman Capital in the Region\nProgram director: Prof. Dr. Frank Cörvers\nHuman capital investments made at the regional level are important to match labour supply and demand, and to stimulate labour force participation, productivity, innovation and growth. Many regional policy makers are challenged by:\n- A lacking responsiveness of the regional educational system to new economic and technological developments.\n- Demographic transitions in the form of increasing migration flows and population ageing, with a declining or more diverse inflow of young people joining the workforce.\n- An insufficient regional pool of up-to-date qualified and highly-able teachers.\nThese challenges differ between central (‘Randstad’) regions on the one hand and peripheral (‘Randland’) regions on the other, with the latter often being border areas that are more prone to demographic shrinkage. Employers, schools, local governments and private and public employment services can improve the transition between (vocational) education and the labour market by cooperating at the regional level.\n- Regional push and pull factors with respect to working and living for people at the higher, intermediate and lower educational level in both the Randstad and Randland areas.\n- Geographic mobility of workers regarding commuting and internal and international migration.\n- Regional educational infrastructure of vocational schools and higher education institutes.\n- Impact of demographic transitions (shrinkage and growth, ageing, migration) on regional labour markets, including the teacher labour market.\n- Barriers for international and cross-border mobility, including differences in tax, pension and social security systems, inefficient diploma recognition, poor cross-border public transport and road connections, language and cultural differences.\n- Labour force participation of vulnerable groups at the regional and local level, such as migrants, low-skilled and disabled people.\nHealth, Skills, and Inequality\nProgram director: Prof. Dr. Mark Levels\nWith this program we explore how Western countries can best prepare today’s youth and workers for tomorrow’s labour market and society. We study how the complex interplay of social background, cognitive and non-cognitive abilities, skills, capabilities, culture, and health explains observed inequalities in education and on the labour market. We assess how current societal and economic trends and technological innovations help to shape the labour markets of the future, study what the implications of these trends are for social inequalities, and assess how individuals, firms, and governments can best respond. We especially focus on some of the most vulnerable groups in Western societies: marginalized adolescents, NEETs, kids from socially disadvantaged families and neighbourhoods, low-skilled workers, older workers, unhealthy children, teens, immigrants.\nImportant questions are:\n- Which skills and capacities are essential for successful participation in society and on the labour market?\n- To what extent do current technological revolutions affect social inequalities in successful participation in society and on the labour market, how, and why?\n- How, at what time and under which circumstances are the relevant skills best learned?\n- Which circumstances, capabilities, and lifestyle choices influence our capacity to learn, grow, and flourish?\n- Automation of work and future inequalities.\n- Acquisition of cognitive and non-cognitive skills.\n- Health, lifestyles, and social inequalities.\n- Vulnerable groups.', 'For several years, companies all over the world have faced a common problem – they have difficulties finding qualified workers. The European Company Survey (ECS- 2013) illustrated that nearly every second company in Luxembourg has trouble finding skilled employees. Moreover, the survey showed that this problem mainly concerned the industrial sector (Cedefop 2015). The European Centre for the Development of Vocational Training stated that this penury is primarily caused by unattractive job offers (two thirds of EU companies); the remaining firms face a genuine inability employing qualified workers (Cedefop 2015). Despite the different causes, the consequences are the same. Skill shortage can result in overtime hours, higher turnover, or loss of knowledge (Rod Van Rite 2019). It can also lower the productivity and lead to a loss in competitiveness (Cedefop 2015). These consequences could have a tremendous impact on the organization itself, as well as on the national economy.\nAlbeit, how can this problem be solved?\nThe literature suggests different solutions, which could be categorized into three main frames. The first frame focuses on approaches based on the organization’s already existing human resources, by implementing, for instance, policies empowering employees to participate regularly in training programs (Oliver & Turton, 1982). The second frame covers approaches concerning the recruitment; literature therefore suggests that companies cooperate with schools or universities. By offering internships or apprenticeships, the organizations become more visible and attractive and thus recruit new employees (Rod Van Rite 2019). The third frame could be considered as a hybrid of the previous frames. These approaches could avoid that companies lose their valuable human capital and help them generate new one, for example by creating a healthy work environment or attractive job offers.\nOffering an internship or an apprenticeship could be a cost-efficient measure with many accompanied advantages (second frame) (HR Booth Ltd N.G.). For instance, they allow organizations to observe potential employees in action and to separate in advance the wheat from the chaff. Moreover, employees who have previously interned at a company tend to stay longer. This could be beneficial, because it might be more reasonable for organizations to invest in potentially longstanding employees by offering them trainings. With these trainings, the employees are tailored to the organization’s specific needs, which could help the company bind its skilled workers. Lastly, by offering internships or apprenticeships, the companies give something back to the community. Hiring interns could reduce the unemployment rate and strengthen the workforce of the local area. This could positively impact the company’s Corporate Social Responsibility strategy.\nIt might not only be valuable cooperating with schools or universities but also with training centres, such as the Centre Formida. These institutions take in young unemployed people with different levels of education and competences. These young adults participate in different programs, where their skills are evaluated and strengthened or new ones developed. If needed, these institutions (e.g. Centre Formida) can also support companies by providing them with manpower; educators will accompany and supervise the apprentices. These educators are specially trained and could build a bridge between the young adults and the organization, which could be highly beneficial for both. By cooperating with these institutions, companies might be able to find longstanding employees in which it is worth investing (e.g. trainings) and to overcome the shortage of skilled workers. The effectiveness of this approach can be illustrated through several examples, such as Philipe’s story. Philipe joined our program with a CCP training course, with the intention of becoming a travel agent. However, due to his lack of language skills, this was impossible. We started to look for alternatives by assessing his skills and by working intensively with him. After a few months, we arranged for an internship at a cooperating company, the bakery subsidiary PAUL. The bakery was highly satisfied with Philipe’s work and after several weeks, the company offered him an apprenticeship, which he gratefully accepted.\nIn order to cope with skilled workforce shortage, it might be useful to bring these three frames together to see the bigger picture. To find new talents, organizations might need to cooperate with specific institutions, schools and universities and offer internships and apprenticeships. Moreover, organizations should be more actively involved in upskilling their potential longstanding employees. It might be time to change the paradigm of “I hire what I need” to “I train what I need”. However, it is also important to reduce the turnover and to avoid the loss of knowledge by creating a healthy work environment and offering adequate job opportunities.\nCedefop (2015). Skill shortages and gaps in European enterprises: striking a balance between vocational education and training and the labour market. Luxembourg: Publications Office. Cedefop reference series; No 102. http://dx.doi.org/10.2801/042499\nHR Booth Ltd (N.G.) The value of internships to employer and employee. Scotland, United Kingdom. HR Booth Ltd. https://www.thehrbooth.co.uk/blog/recruitment/the-value-of-internships-to-employer-and-employee/\nJeffrey A. Joerres (2012). Lack of skills is a serious problem for companies. New York; USA: Nytimes.com. https://www.nytimes.com/roomfordebate/2012/07/09/does-a-skills-gap-contribute-to-unemployment/lack-of-skills-is-a-serious-problem-for-companies\nRod Van Rite (2019). How the skilled labor shortage is impacting manufacturers. Wisconsin; USA: MCL Industries, Inc. https://www.mcl.bz/blog/skilled-labor-shortage-will-impact-manufacturers\nOliver, J., & Turton, J. (1982). Is there a shortage of skilled labour?. British Journal Of Industrial Relations, 20(2), 195-200. doi: 10.1111/j.1467-8543.1982.tb00097.x']	['<urn:uuid:44bcef13-2c6c-46ac-bfb7-055294fab622>', '<urn:uuid:30e5fd3d-27ff-483c-9a28-2581cb50275d>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T05:28:56.360010	9	51	2196
7	What's the connection between abrasive cleaning methods and potential damage when maintaining both guns and wooden floors?	For guns, while wire brushes and sandpaper can be used for stubborn rust, they must be used carefully to avoid damaging the bluing. Similarly for hardwood floors, harsh cleaning agents like Comet, SOS Pads, or Murphy's Oil Soap should never be used, and dirt/grit must be regularly removed with a microfiber dusting pad as they can scratch the surface through abrasion.	['The best way to remove light rust from a gun barrel is to use a product like WD-40 or Rust-Oleum Rust Reformer. These products will convert the rust into a black oxide that can be easily wiped away. If the rust is more stubborn, you can try using a wire brush or sandpaper to remove it.\n- Pour a small amount of gun oil onto a clean cloth\n- Rub the oil into the rust on the gun barrel, using circular motions\n- Continue rubbing until the rust is no longer visible\n- Wipe away any excess oil with a clean cloth\nHow to Remove Rust from a Gun Without Damaging Bluing\nIf you have a firearm that is covered in rust, you may be wondering how to remove the rust without damaging the bluing. Bluing is a type of finish that is applied to steel firearms and it can be easily damaged if not handled correctly. The good news is that there are a few ways that you can remove rust from your gun without damaging the bluing.\nHere are some tips on how to do this: One way to remove rust from a gun without damaging the bluing is to use WD-40. WD-40 is a great option because it will lubricate the metal and help to prevent further rusting.\nSimply spray WD-40 onto the rusted areas of your gun and then wipe away with a clean cloth. You may need to repeat this process several times in order to completely remove all of the rust. Another option for removing rust from a gun is to use vinegar.\nVinegar is an acidic substance which will break down Rust molecules . To use vinegar, simply soak a clean cloth in vinegar and then rub it over the rusted areas of your gun. You may need to let the vinegar sit on the gun for several minutes in order for it to work effectively.\nOnce the vinegar has had time to work, simply wipe it away with another clean cloth. If you have access to citric acid, you can also use this substance to remove rust from your gun . Citric acid works in much the same way as vinegar – by breaking down Rust molecules .\nTo use citric acid, mix equal parts water and citric acid together and then apply it to the rusted areas of your gun using a clean cloth . Let it sit for several minutes before wiping away with another clean cloth . You can also purchase commercialRust removal products which are designed specifically for removing Rust from firearms .\nThese products usually come in gel or liquid form and they work by dissolving Rust molecules . Follow The instructions on The product label carefully In Order TO avoid damaging The bluing on your gun .\nCan You Get Rust off a Gun Barrel?\nWhen it comes to cleaning a gun barrel, there are a few different ways to remove rust. The most common way is to use a chemical cleaner, which will break down the rust and allow you to wipe it away. You can also use sandpaper or a wire brush to scrub off the rust.\nWhichever method you choose, make sure you clean the barrel thoroughly before using it again.\nWhat Will Remove Light Rust?\nWhen it comes to removing light rust, there are a few different methods that can be used. One popular method is using a product called Rust-OleumRust Reformer. This product chemically changes rust into a black primer, making it ready for paint or another topcoat.\nIt can be used on light rust and heavy rust, and works best when applied to rusted metal surfaces that are clean and dry. Another method for removing light rust is sanding. This will require some elbow grease, but can be effective if the rust is not too deep.\nYou’ll want to use a medium-grade sandpaper and sand in small circular motions until the rust is removed. Once you’re finished sanding, make sure to wipe down the area with a clean cloth to remove any dust or debris before applying paint or another topcoat.\nWill Wd-40 Remove Light Rust?\nIf you have light rust on an object, WD-40 can help remove it. First, use a brush to scrub the rusty area to loosen the rust. Then, apply WD-40 and let it sit for a few minutes before wiping it away.\nYou may need to repeat this process a few times to remove all the rust.\nWhat is the Best Rust Remover for Guns?\nThere are a few different ways that you can remove rust from your gun. You can use a chemical rust remover, which will usually come in the form of a gel or spray. You can also use sandpaper to remove the rust manually.\nWhichever method you choose, make sure that you follow the instructions carefully and take all safety precautions. If you’re using a chemical rust remover, apply it to the rusted area and let it sit for the amount of time specified on the product label. Once the time is up, wipe away the rust with a clean cloth.\nIf there are still some stubborn areas of rust remaining, you can repeat the process until they’re gone. If you’re going to remove the rust manually with sandpaper, start with a coarse grit paper and work your way up to a finer grit until all of the rust has been removed. Again, make sure that you wipe away any debris as you go so that it doesn’t end up in your gun’s barrel or other sensitive areas.\nBrownells – How to Fix Light Rust on a Gun\nIf you have a gun that is starting to show some light rust on the barrel, there are a few things you can do to remove it. First, try using some WD-40 or another type of lubricant. Spray it on the affected area and let it sit for a few minutes.\nThen, use a soft cloth to wipe away the rust. If this doesn’t work, you can try using a slightly abrasive material like steel wool or sandpaper. Just be sure not to use anything too harsh or it could damage the finish on your gun.\nLeave a Reply', 'Wood floors are one of the easiest floor types to maintain. Here are a few do’s and dont’s that will help keep your wood floors looking their best. The vast majority of wood floors have a polyurethane finish. It is never a good idea to wax or oil a polyurethane finish. You also DO NOT want to use any harsh cleaning agents such as Comet©, SOS® Pads, or Murphy’s® Oil Soap on your hardwood floors. Also, do not use steam cleaners on your hardwood floors. For general day to day cleaning, we recommend Bona’s Hardwood Floor Care System. Please call us if you would like to purchase a cleaning kit. There are also some easy things you can do to maintain your hardwood floors:\n1. Sweep often\nDirt and grime are the enemies of your wood floors. Use a microfiber dusting pad to pick up the maximum amount of dirt. Dirt and grit will scratch the surface. If you do not own a microfiber dusting pad, we recommend sweeping with a soft bristle broom or vacuuming with a soft brush attachment. Remember to clean any throw rugs or door mats regularly because the dirt from them can easily spread to your hardwoods.\n2. Wipe Up Spills Immediately\nClean spills right away, especially if the spill has color to it. This can permanently stain your hardwood floor if left to soak in. Use a slightly dampened towel.\n3. Never Wet-Mop a Wood Floor\nFor general cleaning, never wet-mop a wood floor. Water can dull the finish, damage the wood, and leave a discoloring residue. Use the Bona Swedish Hardwood Floor Cleaner and spray directly on the floor or on the microfiber cleaning pad.\nIn addition to regular day to day maintenance, we also recommend the following preventative measures. These tips will extend the life of your floors by lengthening the intervals between resurfacing your floor. Remember, it’s not uncommon for hardwood floors to last 50+ years with proper maintenance.\n1. Schedule a Kimminau Clean and Coat.\nThis is the single most effective way to extend the life of your floors. This one-day process deep cleans your wood floors and applies a protective coating to the floors to help restore the shine. This can be done every year or two and helps prolong the life of your floor’s finish.\n2. Use Throw Rugs\nDirt and debris are enemies to your wood floor. Throw rugs capture grit and keep it from being tracked onto your wood floor where it can cause damage through abrasion. Some recommended places include both inside and outside of doorways, in front of kitchen appliances, and in high traffic areas. But don’t add throw rugs until 30 days after your floor has been refinished. This gives the finish time to cure properly.\n3. Use Fabric Pads or Glides Under Table Legs\nPut soft fabric pads or glides under the legs of tables and chairs to prevent scratching the floor when they are moved. Clean these pads regularly to keep grit from getting embedded in them. A pad with grit in it is no better than a hard chair leg. Occasionally, you will have to replace these glides if they get too dirty or lose their stickiness and fall off. They are available at most furniture and hardware stores and are inexpensive.\n4. Check Your Shoes\nSports shoes with cleats can damage a wood floor, so never walk on the floor while wearing these. Even women’s high heels can cause dents, so it’s best to take them off. Definitely check to make sure the heel pad is not worn down to the nail. This can do serious damage to a wood floor. Both men’s and women’s shoes should be examined regularly and properly maintained to avoid damaging the wood surface.\n5. Get a Dolly When Moving Heavy Furniture\nMoving heavy items like furniture can easily damage your floors. Sliding, rolling or dragging furniture, appliances or other heavy items across your hardwood floors can scratch, scrape and even gouge your floor. If you cannot pick up the item, we suggest using a dolly or cart with air-inflated rubber tires.\n6. Control Humidity\nHumidity in a home changes with the seasons. Although we take measures to acclimate your wood floors to the environment before installing them, gaps and cracks can appear in the winter months. Installing a humidity control system will help greatly. Ask your HVAC dealer to recommend a whole house humidifier to help keep moisture in the air in the winter months. In the summer, keep your air conditioner on or run a dehumidifier.\n7. Trim Your Pets’ Nails\nPets can cause major damage to wood floors. Be sure to keep dogs nails trimmed to minimize surface scratches. Even the most durable of finishes can be scratched by dogs so use rugs in traffic areas and other areas where your pets spend the most time.']	['<urn:uuid:5d8d7352-14ce-425e-8ec8-fb0d25efa809>', '<urn:uuid:4afcebe0-12fa-4e02-815f-f95fa1ba3d7e>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T05:28:56.360010	17	61	1861
8	how to use high pass and low pass filters in music mixing	High pass and low pass filters are essential tools in mixing. High pass filters remove lower frequencies from sounds with higher fundamentals (like hi-hats or rim shots), which helps clear up muddiness in the low end and gives more room for bass sounds. Low pass filters remove unnecessary higher frequencies from low-end sounds, creating more space in the higher range. However, since all instruments contain important information across frequency bands, you should only roll off what's absolutely necessary and always use your ear to determine the right amount.	['Strong EQing can be that special ingredient for making a good mix great.\nIt’s EQ month! We’re gonna dig in on all things EQ in the coming days. This article is the second part of the series.\nSo if you haven’t read it yet, check out our EQ Primer: What Is EQ before you jump in on this EQ cheat sheet. Ok, all set? Enjoy!\nHow to EQ: Super Freqs\nKnowing how to use an EQ to equalize audio is an essential skill for mixing music right.\nDoing it well all comes down to knowing your frequencies and how they interact in your track.\nAudio frequencies aren’t difficult to tame as long as you know what they’re up to. The first step to knowing how to EQ is understanding where all your instruments fit on the frequency spectrum.\nThat’s why we created this EQ cheat sheet for all your EQing needs.\nSeeing where each instrument fits on the frequency spectrum will help you identify which instruments and frequencies might be fighting each other in your mix, and will help you get the best possible mix before that final mastering step.\nChoose Your Instruments Wisely\nEQ is a valuable tool. But it can only do so much.\nIf you choose a ton of instruments that are all fighting for space in the same frequency range, you’ll have to stretch your EQ pretty far to fit them all in and keep it natural.\nSo you have to choose your instruments wisely and always aim for the best possible recording. EQ is something you apply to your mix, not add to it.\nKeep that in mind when you’re recording and get the best possible mix before you do any processing.\nEQing can’t make a bad recording good. But EQing CAN make a good recording GREAT.\nEQ Cheat Sheet\nMeet your new best friend for fitting those fantastic frequencies!\nStudy it. Know it. Love it. Download it to your desktop for easy reference, or print it out and hang it on your studio wall.\nRemember! This instrument chart is just a starting point. The sounds in your mix will always have their own context and characteristics. So use this chart as a jumping off point, but always use your mix as the ultimate reference for applying EQ.\nThis chart is not the ‘mixing law.’ Instead, it’s a good reference to get you started on thinking about where your performances sit in your mix—so use it accordingly.\nHow to Use A Frequency Chart\nUsing this instrument frequency chart is simple. Just think about the fundamentals of each instrument before you record anything.\nTry to picture where each instrument will sit in the mix before you even start recording.\nIf you choose your instruments based on the frequency fundamentals before you even start, the mixing and EQing phase will be much easier.\nSynthesizers and other electronic instruments can be tricky when it comes to finding the fundamental. They’re often capable of creating an extremely wide range of sounds and frequencies.\nThe simple fix is to hone in on what sound you’ll be trying to synthesize in your track with your gear.\nTry to picture where each instrument will sit in the mix before you even start recording.\nFor example: If you’ll be using an FM synth to create a brass lead sound, then the fundamental for the brass instrument you’re attempting to synthesize should apply.\nYou’ll never really know until you hear it in your mix. But try and be as prepared as possible before recording anything.\nThere are no ‘exact’ rules for which frequency bands will cause problems for what instrument.\nSo always listen, look and learn from the context of your own mix after you’ve recorded your parts.\nOf course you CAN use as many instruments as you want that fall into a certain frequency band. But the more instruments you stack, the easier it is to mask important information. So your EQing will have to be more complex to get everything sitting right.\nRoll the Beats\nMost parametric EQs come with high pass and low pass filters built right in. They’re a great place to start with some corrective EQing to free up some space in your mix.\nInstruments that have a higher fundamental, will also contain information in the lower frequencies as well.\nIn most cases, the lower frequencies for an instrument with a higher fundamental—and vice versa—can be rolled off. That’s where the high and low pass filters come in…\nInstead of performing specific cuts or boosts, the high and low pass filters will remove unneeded frequencies on a broader scale.\nA common use of the high-pass filter is to remove lower frequencies on your sounds with a higher fundamental like a hi-hat or rim shot.\nThe lower frequencies that these sounds contain may be muddying up your lows. Performing a simple high pass will give your bassier sounds more room to punch while giving your synths and higher percussion more clarity at the top of your mix.\nThe opposite applies for your low pass filter as well. Removing unnecessary higher frequencies from your lows will give everything more room to work up top.\nKeep in mind that all instruments contain important information in the high and low frequency bands. So only roll off what you absolutely need to. Always use your ear to determine the right amount.\nSweep Your Bands\nEQ sweeping is your best friend when it comes to finding the problem areas in your mix. EQ sweeps will help you pinpoint the exact frequency you’re looking to fix. So how do you do it?\nFor starters, set a point with a high gain and narrow bandwidth (the ‘Q’) in your parametric EQ. Start playing back your track and ‘sweep’ the point back and forth across the frequency spectrum.\nListen for when the problem you are trying to fix gets REALLY OBVIOUS (your ears might even get a bit sad for a second). But once you hear it, you know you’ve found the frequency where you’ll be making your cut. For example:\nIf you’re looking to take out some boominess from a guitar tone, sweep your EQ until you find the frequency where the boom is most noticeable. When you find the culprit, simply perform your cut in the frequency you pinpointed and play it back (I’ll talk more about cuts in a second).\nSometimes certain problems can be fixed during the high and low pass filtering phase that I talked about above. But performing an EQ sweep will help you to isolate really specific areas that might need a cut or boost.\nHot Tip: Make sure you stay focussed on the problem you’re trying to cut. Boosting the gain in any area will make parts sound less than ideal. Don’t cut everything that sounds bad automatically. Stick to pinpointing one specific fix at a time.\nKeep it Narrow for Cuts\nNow that you’ve isolated your problem frequency it’s time to apply some cuts to attenuate the nasty.\nAs a rule of thumb try to keep your cuts no more drastic than 3dB. Anything more can be a little heavy handed. But it’s always the taster’s choice.\nKeep your bandwidth narrow for cuts. Remember that a cut is removing information from your audio, so the wider your bandwidth (Q), the more character you’re removing from a sound. That’s why finding your specific problem frequency is so important. It allows you to cut only what you need to.\nKeep it Wide for Boosts\nEQ boosts need a wider Q. Boosting with a narrow bandwidth can make frequencies stick out like a sore thumb in the mix. A wider bandwidth (Q) helps to make your boosts a bit more natural to the ear.\nA wider bandwidth works because of the science behind how we hear —and you can’t bet against science right? The ear wants natural sounds. A wider Q helps to keep your boosts as organic as possible.\nBoosting is where you can get really experimental with your EQing and pull some interesting character out of your sound. Experiment with boosts outside of the fundamental. Small boosts to your sound’s harmonics can have interesting results for creating punch, presence and intrigue in your mix.\nHot Tip: Use a spectrum analyzer, or EQ plugin that has a spectrum analyzer built in (like the TDR NOVA), to pinpoint interesting harmonics to boost or attenuate.\nExperimenting with EQing harmonics can add richness and character to your sound. But they’re also a great place to look if you’re encountering cluttered band. Try both for best results!\nPanning is a great way to free up some space for two instruments that share the same frequency range.\nIf you’re hesitant to mess with the tone too much with EQ but you’re still hearing some masking in the mix, try panning one sound to the left and one to the right.\nGiving conflicting sounds their own space in the stereo field will bring out those unique frequency and timbral features and help individualize them in the mix.\nIt’s always the taster’s choice.\nUsing your pans to give your parts room means less frequency face-offs between your instruments. It’s not always a sure-shot fix. But panning is something you should be doing every mix anyways. So you might as well get the most out of it.\nThe high pass and low pass rules still apply for rolling off the unnecessary frequencies. But panning gives you options if you don’t wanna alter the character of your sound just to make 2 or more instruments fit.\nHear the Whole. Not the Solo.\nThe biggest trap when it comes to EQing is getting obsessed with your isolated sounds. ALWAYS listen to the whole mix and EQ accordingly. Your sounds are part of a whole and how you sculpt with EQ has to be dependent on their role within the entire project.\nIf you’re EQing just for the soloed sound you’re missing the whole point. That’s why it’s called “Mixing” and not “Soloing.”\nWhen in doubt try and think like the listener. They’re not gonna hear the elements of your track soloed like you do in the studio. They’ll only hear the whole song. So let your overall sound inform all your EQing decisions.\nCertain sounds might even sound ‘bad’ when they’re soloed. But when they go off in the whole mix they’ll sound peachy. It’s what good mixing is all about—especially when it comes to EQing.\nA good recording is the first step to a good mix. But the in between step that can make a good mix great is smart EQing.\nUse these basic tips to get started with your EQ experiments and find out what it can do for your tracks.\nAnd always keep it Freq-y!']	['<urn:uuid:46ff419b-3e82-49b2-9890-4eab85578639>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T05:28:56.360010	12	88	1797
9	im working at a museum help me understand how tk licenses work for preserving culture and what technical problems affect digital archives	TK licenses are legal tools that allow communities to establish culturally appropriate controls over their knowledge and cultural expressions. They can be used alongside copyright to add special conditions about how materials should be respectfully used according to local protocols. The licenses help clarify local knowledge management systems to external users and can assign rights to families or communities as collective rights holders. However, preserving these materials digitally faces multiple technical challenges: Digital storage media are vulnerable to rapid deterioration, technologies become obsolete every 18-24 months requiring constant updates, there's a proliferation of different file formats each requiring specific software, and questions of authenticity arise since digital records can be easily altered. Additionally, maintaining digital archives requires significant ongoing financial resources and technical expertise to ensure long-term access.	"[""Traditional Knowledge licenses and labels recognize that Indigenous, traditional and local communities have different access and use expectations in regards to their knowledge and cultural expressions. These different expectations of access and use depend heavily on the material itself and the local context from which it derives. These TK licenses and labels help identify this material and establish culturally appropriate forms of managing control and access.\nLocal Contexts is an open forum for the testing and application of tk licenses and labels to digital cultural content. The licenses are designed to be legally defensible across multiple jurisdictions and directly address the specific needs of Indigenous, traditional and local communities whose needs are not met by current legal solutions. The labels are designed as a non-legal, educational strategy that can deal with cultural material already in the public domain.\nWe encourage users of Local Contexts to make comments, post ideas and exchange information about uses of and issues surrounding the TK licenses and labels. Sharing these experiences and stories will help everyone. The more information we have the better we will know how the licenses and labels work in your local context.\nUsing the TK Licenses and Labels requires individual and community decision-making. This is especially the case for material that is not owned individually but should be controlled collectively. The decision making processes for using these TK Licenses and Labels should be established before you choose which one will suit your needs. This will enable dialogue about what option best suits your needs. Each family, clan or community will have different processes and frameworks for decision-making. Some communities are in the process of establishing cultural authorities to help make decisions about a range of IP issues facing their community. Depending on history and context, these decision-making processes will also accommodate perspectives from community members who reside in different regions.\nTraditional knowledge licenses and labels are designed to help clarify local knowledge management systems and cultural protocols to external and/or non-community users. They are not designed for internal and intra- family, clan or community use.\nIt is important to note that TK licenses and labels do not change already existing copyright conditions. However, you can use them separately or in combination with traditional copyright or Creative Commons licenses to add new conditions of use and/or to help educate people in how your material should be respectfully and ethically treated according to your local community expectations and obligations.\nThe TK licenses and labels are designed to accommodate individual, family, clan, community and multiple community ownership or custodianship of cultural materials. They are not developed to manage intra or internal ownership or custodianship – they are designed to help better the relationships between indigenous, local and traditional communities and external and third party users.\nBefore you decide on a license or a label for your materials you should:\n- Identify the nature of the material (text, photos, audiovisual, sound etc). This is because different copyright rules and considerations apply to each type of material;\n- Determine what rights exist in those specific materials. Once you have determined what rights exist you will be able to determine what system governs use and circulation. There are two main options: conventional IP rights (such as copyright) and rights under customary law;\n- Determine who owns or is responsible for managing those rights? This might include the author of the works. Conventional copyright rights may or may not vest in the communities themselves. They will often vest in the party who physically made the recordings that are in the archive, and these rights should be assigned to the communities so they can manage them. Rights under customary law belong to the communities but unfortunately they do not bind third parties. You will want to identify whether it is an individual or collective right.\n- If you are the copyright holder or have had copyright assigned to you, you can use a license;\n- If you are not the copyright holder or have not had copyright assigned, you should use a label;\n- The TK licenses and labels set guidelines for how people can and can’t use your materials outside your community. Choosing these helps people understand the different questions of access and use that your community might have and asks them to agree to use, remix, circulate and distribute the material according to your local cultural protocols.\nTo learn more about each individual license and label and your options for use check out the 4 licenses and 10 labels under development now.\nIf you are the copyright holder of the cultural material, you can choose a TK License that adds certain new culturally appropriate conditions for use, including an option that gives the custodians of the source material authority to make decisions about how this material should be shared. These licenses are additional agreements that acknowledge that with some material, special rules governing access and use of that material exist beyond those already established by copyright.\nWhy Use Licenses?\nThe TK licenses add to existing rights and responsibilities of copyright owners and copyright users. These licenses are additional agreements that acknowledge that with some material, special rules governing access and use of that material also exist. For instance, these licenses allow individuals to assign their copyright rights to their family or community and the community then becomes the legally recognized rights holder. It asks all parties to be sensitive to the Indigenous customs and laws that govern this material, and that some material in the archive or digitally circulating is sensitive, has restrictions and is not free to be used by anyone at any time.\nThe TK Labels are an educative and informational strategy to help non-community users of traditional knowledge understand the importance and significance of this material, even when it is in the public domain and appears as though it can be shared and used by everyone. This is often not the case for traditional knowledge, and the Labels are designed to identify and clarify which material has community-specific, gendered and high-level restrictions. This is especially with respect to important sacred and/or ceremonial material. The TK Labels provide an option for conveying important information about cultural materials. They can be used to include information that might be considered ‘missing’, for instance the name of community from where it derives, what conditions of use are deemed appropriate, how to contact the relevant family, clan or community to arrange appropriate permissions etc.\nWhy Use Labels?\nThe TK labels are designed for material that is already considered to be in the public domain and is no longer protected by copyright. Labels are best suited for older material or when ownership is not clear. The labels are an option for conveying important information about the material. They can be used to include information that might be considered ‘missing’, for instance name of community from where it derives, what conditions of use are deemed appropriate, how to contact the relevant community to arrange appropriate permissions. The labels help users identify sensitive cultural material and therefore also help users in making informed decisions about how this material should be used and in what ways. The labels are designed to promote the fair and equitable use of expressions of traditional knowledge.\nThese are explanations of main legal terms and concepts to help you make decisions about using Local Contexts and the TK Licenses and Labels. They are designed to make complex legal terms more accessible and understandable and to help you clarify key concepts that you might be unsure about.\nWhat is the process?\nWhat is a TK License?\nWhat is a TK Label?\nWhat is copyright?\nWhat is a copyright holder?\nWhat do you mean by assigning or transferring rights?\nHow long does copyright last?\nWhy do we need your name as we create the Label or License?\nWho are Licenses between?\nWhy does a License have a time-period?\nWhat is 'in perpetuity'?\nWhy are we asking a question about the content of the material?\nWhat is a Licensor?\nWhat is a Licensee?\nHow do I attach the License to the work?\nWhy do you need the name for a Label?\nWho are the Labels for?\nWhat is public domain?\nHow would I know if the material I want to license is already legally owned?\nWhy are we asking a question about the format of your material?\nWhy are we asking a question about where your materials are?\nWhy are we asking about multiple copies?\nHow do I attach the Label to the work?\nChoosing and using a license or label is a big decision. Here we provide you with a demonstration walkthrough of the TK license and label generator currently under development. We are soliciting feedback about the proecss based on this model. If you have suggestions for the process or would like to test the licenses and labels with your community or institution please contact us."", 'Digital preservation can be defined as the process and activities which stabilize and protect digital records and publications in forms which are retrievable, readable and usable over time. Digital preservation could also be defined as a set of processes and activities that ensure continued access to information and all kinds of records, scientific and cultural heritage existing in digital formats. This includes the preservation of materials resulting from digital reformatting but particularly information that is born-digital and has no analog counterpart. Digital preservation is an ongoing process of managing data for continued access and use.\nThe adoption of Information Communication Technologies (ICTs) has revolutionized the conduct of business and has greatly enhanced information accessibility. In particular, organizations are not only able to store large amounts of information but can also have quick access to it. This has improved service delivery and has ensured that policy makers react rapidly to social and economic developments. Further, the general public can also access information in remote areas. ICT has enabled archivists, records managers and librarians to carry out their mandate: that of information capture, preservation and dissemination. While use of ICT has occasioned these many benefits it has also brought challenges that have to be addressed. Principally, this new development has led to the generation of information in digital form which has to be managed. In spite of the benefits accruable, the technology has presented tremendous challenges which information professionals should be concerned with.\nThe purpose of preservation is to ensure protection of information of enduring value for access by present and future generations (Conway, 1990: 206). Libraries and archives have served as the central institutional focus for preservation, and both types of institutions include preservation as one of their core functions. In recent decades, many major libraries and archives have established formal preservation programs for traditional materials which include regular allocation of resources for preservation, preventive measures to arrest deterioration of materials, remedial measures to restore the usability of selected materials, and the incorporation of preservation needs and requirements into overall program planning.\nCHALLENGES OF DIGITAL PRESERVATION\nAn African perspective on preservation ought not to be different from other perspectives. However, digital preservation is often discussed in terms of technology, infrastructure and practices. Africa is largely composed of developing nations and thus has peculiar problems.\nIn African institutions these factors are attributed to:\nMost African countries have no policies on handling information be they in print; let alone in electronic format. In some African countries, years after independence they are still struggling with enacting a libraries act and as a result most institutions operate within a no policy framework. An enabling policy framework would allow institutions to implement various preservation strategies that are in line with their own parent institutions but operate within the overall country policy framework. These policy frameworks are essential especially if they can feed into broader continental policies such as the NEPAD initiative (The New Partnership for Africa’s Development which is a VISION and STRATEGIC Framework for Africa’s renewal). The NEPAD initiative itself is very silent on the preservation of Africa’s knowledge resources although it places prominence on the improvement of information and communication infrastructure (ICT). The improvement of ICT infrastructure will do well if there are policy frameworks at the country level that support the preservation and permanent storage of African knowledge resources wherever they might be found and in whatever format they might in.\nAfrica’s infrastructure is still lacking in handling large preservation of knowledge resources, especially resources that are in electronic form. Access to ICT facilities is a daily struggle for most institutions that are just barely managing to maintain access to print resources to be able to meet the daily requirement for academic learning in higher educational institutions.\nPreservation of knowledge resources is a continuous process not just a one off issue. To implement an effective and efficient preservation policy, there is need for commitment at both the institutional and national levels that preservation of the knowledge resources will be an incremental process that will be carried on from one generation to another. This effort entails that financial resources be committed to such a venture over long periods of time. This trend in funding has affected all areas of library operations including money that could be allocated for preservation of scholarly information materials. Financial commitments would also be needed to purchase and preserve the digital knowledge resources to permanently make them accessible to users, now and in the future.\nFinancial resources available for libraries and archives continue to decrease and will likely do so for the near future. The argument for preserving digital information has not effectively made it into public policy. There is little enthusiasm for spending resources on preservation at the best of times and without a concerted effort to bring the issues into the public eye, the preservation of digital information will remain a cloistered issue. The importance of libraries has been diminished in the popular press as the pressures from industry encourage consumers to see libraries as anachronistic while the Internet and electronic products such as Microsoft Encarta are promoted as inevitable replacements. Until this situation changes, libraries and archives will continue to be asked to do more with less both in terms of providing traditional library services, as well as new digital library services: preservation will have to encompass both kinds of collections.\nTechnical knowledge on the digital elements of electronic documents is largely lacking among staff that are in preservation departments. The presence of preservation departments in most of the libraries and information centers is really in name only as most of them concentrate on book and journal binding. This is coupled with the lack of preservation training. This lack of knowledge extends to deficient know-how on the equipment and software that is required for the preservation of digital information resources.\nDigital Technology Challenges\nDigital technology poses several challenges in the preservation of digital information resources. These are among others; technology comes in different formats, the cost of maintaining international standards of digital formats is expensive as it is often based on paying for upgrades to match the technology both the hardware and software. These come with subscriptions costs; so in essence a library/information center/archival center would have to subscribe to hardware; software and then to the electronic journal. This is unlike the paper format which has relatively changed very little since it was discovered as papyrus in Egypt 3000 BC. The electronic document is fairly new and has changed forms since then. If it is not the document changing from MS Word, PDF, html XML etc; it is the software requirement to be able to open and read the document. For example, if the document is in PDF you will need a PDF reader; JPEG would require a JPEG; just as a TIFF formatted document would require a Tiff reader. This means that institutions are always forced to change the facilities so they can meet various requirements such as software and hardware. Digital preservation presumes that there should be constant and continuous learning on the part of preservation staff both in software knowledge as well as hardware. This is because digital preservation methods are always changing depending on the nature of the hardware and software applied.\nDigitization of information requires obtaining copyright permission from various publishers to be able to duplicate anything in large quantities. However, most licensing agreements for journals or books produced by major publishers prohibit duplication of electronic documents or local storage of the document. What is allowed when one has a subscription is usually the online access to the particular journal for instance, without the subscribing institution having permanent access to content of the journal. Once subscription ends, access to the electronic content of journal is not possible. It is unlike in the print subscription model where once one has subscribed to the journal, the institution will have permanent access to the journal because the journal will be physically present the libraries own space.\nRecording media for digital materials are vulnerable to deterioration and catastrophic loss, and even under ideal conditions they are short lived relative to traditional format materials. Although librarians/archivists have been battling acid-based papers, thermo-fax, nitrate film, and other fragile media for decades, the threat posed by magnetic and optical media is qualitatively different. They are the first reusable media and they can deteriorate rapidly, making the time frame for decisions and actions to prevent loss is a matter of years, not decades. While acid paper is prone to deterioration, becoming brittle and yellowing with age, the deterioration may not become apparent for some decades and progresses slowly. It remains possible to retrieve information without loss once deterioration is noticed. Digital data recording media may deteriorate more rapidly and once the deterioration starts, in most cases there may already be data loss. This characteristic of digital forms leaves a very short time frame for preservation decisions and actions.\nMore insidious and challenging than media deterioration is the problem of obsolescence in retrieval and playback technologies. Information technologies are essentially obsolete every 18 months. Innovation in the computer hardware, storage, and software industries continues at a rapid pace, usually yielding greater storage and processing capacities at lower cost. Devices, processes, and software for recording and storing information are being replaced with new products and methods on a regular three- to five-year cycle, driven primarily by market forces. This dynamic creates an unstable and unpredictable environment for the continuance of hardware and software over a long period of time and represents a greater challenge than the deterioration of the physical medium. Many technologies and devices disappear as the companies that provide them move on to new product lines, often without backwards compatibility and ability to handle older technologies, or the companies themselves disappear. Records created in digital form in the first instance and those converted retrospectively from paper or microfilm to digital form is equally vulnerable to technological obsolescence.\nAnother challenge is the absence of established standards, protocols, and proven methods for preserving digital information. With few exceptions, digital library research has focused on architectures and systems for information organization and retrieval, presentation and visualization, and administration of intellectual property rights (Levy and Marshall). The critical role of digital libraries and archives in ensuring the future accessibility of information with enduring value has taken a back seat to enhancing access to current and actively used materials. As a consequence, digital preservation remains largely experimental and replete with the risks associated with untested methods; and digital preservation requirements have not been factored into the architecture, resource allocation, or planning for digital libraries.\nProliferation of document and media formats\nThere is a proliferation of document and media formats, each one potentially carrying their own hardware and software dependencies. Copying these formats from one storage device to another is simple. However, merely copying bits is not sufficient for preservation purposes: if the software for making sense of the bits (that is for retrieving, displaying, or printing) is not available, then the information will be, for all practical purposes, lost. Libraries will have to contend with this wide variety of digital formats. Many digital library collections will not have originated in digital form but come from materials that were digitized for particular purposes. Those digital resources which come to libraries from creators or other content providers will be wildly heterogeneous in their storage media, retrieval technologies and data formats. Libraries which seek out materials on the Internet will quickly discover the complexity of maintaining the integrity of links and dealing with dynamic documents that have multimedia contents, back-end script support, and embedded objects and programming.\nConcerns of authenticity and reliability\nThe authenticity and reliability of electronic records are often questioned because of possible changes to content or structure. Authenticity can be defined as the ability of the records to be reliable over time and act as evidence of organizational transactions. Reliability on the other hand, refers to a record’s authority and trustworthiness, and this is tied to the ability of a record to stand for a fact it is about. A number of authors among them, Hoffman and MacNeil, have argued that there are no guarantees of authenticity and reliability in the electronic environment, as records can be deleted or changed at any time. It is, therefore, important that electronic records are managed to ensure that they remain authentic and reliable as evidence. Perhaps in the paper environment, one can say that this is more straightforward, as records are physical objects, and this makes identification of their characteristics easier than it is in the virtual world. The records provide evidence of actions, but the computer systems may fail to capture the necessary information about the context of the creation and the use of records.\nAccess to electronic records and concerns of privacy\nThe use of computers has enabled organizations to create databases that now handle huge amounts of data on-line, which is made accessible anywhere and anytime. This has raised concerns that if the information is not properly managed, it may be made available too easily, resulting in lack of protection for the citizen’s individual rights. Further, the vast amount of information maintained about individuals by both government and private organizations threatens their privacy. Ojedokum has highlighted some of the privacy infringement as unauthorized acquisition of data, unauthorized penetration into computer networks. Computers allow fast and inexpensive communication of information and the collection and storage of large amounts of data. At the same time, these capabilities allow individuals and organizations to access information.\nPower cuts and backup strategies\nPower cuts and irregular electricity supplies are a major barrier. In most African countries there are limited power distribution networks which do not even reach rural areas where the majority of the population lives. African cities with higher population that have been experiencing power cuts include but are not limited to Accra, Dares Salaam, Lagos, Gaborone, Nairobi, Harare etc. These power cuts have disrupted business operations. Increased dependence on computers and their services for data processing also means increased reliance on the power supplies that keep the systems operating. Power failure means that organizations may lose valuable information and time. It is estimated that 50-70% of businesses that lose their data due to power cuts never recover it, and some go out of business. There is a need, therefore, for systems that will maintain quality power supply and protect electronic systems.\nInternet Bandwidth (Digital Divide)\nThe digital divide is still a major hindrance. In many parts of Africa there is little access to computers and the Internet. In those parts where there is Internet access, the resources, such as bandwidth, are severely limited or extremely expensive. Some digital preservation systems, such as LOCKSS, have questionable applicability. In the case of LOCKSS, a group of sites collaboratively maintain the integrity of collections. LOCKSS, however, does not cater for unstable and irregular bandwidth availability – its algorithms will not make the most efficient use of bandwidth and may exacerbate problems at sites with poor bandwidth. All online archives need to make use of bandwidth in a way that is both minimal and cognizant of the differences among sites.\nSkills and Education\nLibrarians, archivists and information professionals in African institutions are arguably not as technically skilled as their counterparts in other parts of the world. The availability of computer systems in some parts of the continent has the effect that curators of information do not receive sufficient training in electronic systems. Digital media is not the norm for many forms of communication and information storage. The level of education of the general population in many African countries also is a problem. The number of literate individuals, as well as the number of individuals with access to a computer and the Internet is lower than elsewhere in the world. This creates a challenge for digital preservation both in terms of collection building, especially for end-user submissions, and dissemination. Novel solutions are needed for both these problems to make digital archives effective.\nDigital collections facilitate access, but do not facilitate preservation. Being digital means being ephemeral. Digital places greater emphasis on the here-and-now rather than the long-term, just-in-time information rather than just-in-case. The research program for digital preservation has only recently been initiated to develop strategies, guidelines, and standards. The challenges to digital preservation are considerable and will require a concerted effort on the part of librarians and archivists to rise up to these challenges and assert in public forums the importance of protecting a fragile digital heritage.\n1. Douwe Drijfhout. 2006. Challenges in terms of Digital Preservation. LIASA Conference 2006.\nwww.nlsa.ac.za/...preservation.../Drijfhout.Challenges%20in%20terms %20of%20Digital%20P reservation.pdf\n2. Christine W. Kanyengo. 2006. Managing Digital Information Resources in Africa: Preserving the Integrity of Scholarship\n3. Hussein Suleman. An African Perspective on Digital Preservation\n4. Margret Hedstrom. Digital Preservation: A Time Bomb for Digital Libraries www.eric.ed.gov/ERICWebPortal/recordDetail?accno=EJ586788\n5. Margret Hedstrom. Digital Preservation: Problems and Prospects\n6. Terry Kuny. 1997. A Digital Dark Ages? Challenges in the Preservation of Electronic Information. 63rd IFLA Council and General Conference']"	['<urn:uuid:ea498674-237e-41ed-8904-1a7a8fa445dc>', '<urn:uuid:ee408182-9bc7-4f44-994d-f50aa7dea141>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T05:28:56.360010	22	128	4324
10	As a geomorphologist studying lake formation, I'm curious about how the lakes at the foot of the canyon were created. What role did glaciers play in their formation?	The lakes at the foot of the canyon were formed through different glacial processes. Some were created by terminal moraine dams, where receding glaciers deposited material during periods of less waste or greater snowfall. Between the large lateral moraines extending into the desert, there were several old lake basins that are now completely filled with stream-carried material, transformed into dry sandy flats covered by grass, artemisia, and sun-loving flowers. Some lakes in the canyon were formed in basins eroded from solid rock, where glacier pressure was greatest, leaving beautifully polished resistant basin rims.	['Now comes sundown. The west is all a glory of color transfiguring everything. Far up the Pilot Peak Ridge the radiant host of trees stand hushed and thoughtful, receiving the Sun’s good-night, as solemn and impressive a leave-taking as if sun and trees were to meet no more. The daylight fades, the color spell is broken, and the forest breathes free in the night breeze beneath the stars.We are now approaching the region of clouds and cool streams. Magnificent white cumuli appeared about noon above the Yosemite region,—floating fountains refreshing the glorious wilderness,—sky mountains in whose pearly hills and dales the streams take their rise,—blessing with cooling shadows and rain. No rock landscape is more varied in sculpture, none more delicately modeled than these landscapes of the sky;[Pg 19] domes and peaks rising, swelling, white as finest marble and firmly outlined, a most impressive manifestation of world building. Every rain-cloud, however fleeting, leaves its mark, not only on trees and flowers whose pulses are quickened, and on the replenished streams and lakes, but also on the rocks are its marks engraved whether we can see them or not.\nJune 15. Another reviving morning. Down the long mountain-slopes the sunbeams pour, gilding the awakening pines, cheering every[Pg 50] needle, filling every living thing with joy. Robins are singing in the alder and maple groves, the same old song that has cheered and sweetened countless seasons over almost all of our blessed continent. In this mountain hollow they seem as much at home as in farmers’ orchards. Bullock’s oriole and the Louisiana tanager are here also, with many warblers and other little mountain troubadours, most of them now busy about their nests.\nFrom the top of the divide, and also from the big Tuolumne Meadows, the wonderful mountain called Cathedral Peak is in sight. From every point of view it shows marked individuality. It is a majestic temple of one stone, hewn from the living rock, and adorned with spires and pinnacles in regular cathedral style. The dwarf pines on the roof look like mosses. I hope some time to climb to it to say my prayers and hear the stone sermons.After a long ramble through the dense encumbered woods I emerged upon a smooth meadow full of sunshine like a lake of light, about a mile and a half long, a quarter to half a mile wide, and bounded by tall arrowy pines. The sod, like that of all the glacier meadows hereabouts, is made of silky agrostis and calamagrostis chiefly; their panicles of purple flowers and purple stems, exceedingly light and airy, seem to float above the green plush of leaves like a thin misty cloud, while the sod is brightened by several species of gentian, potentilla, ivesia, orthocarpus, and their corresponding bees and butterflies. All the glacier meadows are beautiful, but few are so[Pg 204] perfect as this one. Compared with it the most carefully leveled, licked, snipped artificial lawns of pleasure-grounds are coarse things. I should like to live here always. It is so calm and withdrawn while open to the universe in full communion with everything good. To the north of this glorious meadow I discovered the camp of some Indian hunters. Their fire was still burning, but they had not yet returned from the chase.Looking up the cañon from the warm sunny edge of the Mono plain my morning ramble seems a dream, so great is the change in the vegetation and climate. The lilies on the bank of Moraine Lake are higher than my head, and the sunshine is hot enough for palms. Yet the snow round the arctic gardens at the summit of the pass is plainly visible, only about four miles away, and between lie specimen zones of all the principal climates of the globe. In little more than an hour one may swoop down from winter to summer, from an Arctic to a torrid region, through as great changes of climate as one would encounter in traveling from Labrador to Florida.\nrummy satta widrow\nSeptember 16. Crawled slowly four or five miles to-day through the glorious forest to Crane Flat, where we are camped for the night. The forests we so admired in summer seem still more beautiful and sublime in this mellow autumn light. Lovely starry night, the tall, spiring tree-tops relieved in jet black against the sky. I linger by the fire, loath to go to bed.\nOne of these ancient flood boulders stands firm in the middle of the stream channel, just below the lower edge of the pool dam at the foot of the fall nearest our camp. It is a nearly cubical mass of granite about eight feet high, plushed with mosses over the top and down the sides to ordinary high-water mark. When I climbed on top of it to-day and lay down to rest, it seemed the most romantic spot I had yet found—the one big stone with its mossy level top and smooth sides standing square and firm and solitary, like an altar, the fall in front of it bathing it lightly with the finest of the spray, just enough to keep its moss cover fresh;[Pg 49] the clear green pool beneath, with its foam-bells and its half circle of lilies leaning forward like a band of admirers, and flowering dogwood and alder trees leaning over all in sun-sifted arches. How soothingly, restfully cool it is beneath that leafy, translucent ceiling, and how delightful the water music—the deep bass tones of the fall, the clashing, ringing spray, and infinite variety of small low tones of the current gliding past the side of the boulder-island, and glinting against a thousand smaller stones down the ferny channel! All this shut in; every one of these influences acting at short range as if in a quiet room. The place seemed holy, where one might hope to see God.A log house serves to mark a claim to the Tamarack meadow, which may become valuable as a station in case travel to Yosemite should greatly increase. Belated parties occasionally stop here. A white man with an Indian woman is holding possession of the place.\nHave got my bed made in our new camp,—plushy, sumptuous, and deliciously fragrant, most of it magnifica fir plumes, of course, with a variety of sweet flowers in the pillow. Hope to sleep to-night without tottering nerve-dreams. Watched a deer eating ceanothus leaves and twigs.One of the smallest of the cascades, which I name the Bower Cascade, is in the lower region of the pass, where the vegetation is snowy and luxuriant. Wild rose and dogwood form dense masses overarching the stream, and out of this bower the creek, grown strong with many indashing tributaries, leaps forth into the light, and descends in a fluted curve thick-sown with crisp flashing spray. At the foot of the cañon there is a lake formed in part at least by the damming of the stream by a terminal moraine. The three other lakes in the cañon are in basins eroded from the solid rock, where the pressure of the glacier was greatest, and the most resisting portions of the basin rims are beautifully, tellingly polished. Below Moraine Lake at the foot of the cañon there are several old lake-basins lying[Pg 225] between the large lateral moraines which extend out into the desert. These basins are now completely filled up by the material carried in by the streams, and changed to dry sandy flats covered mostly by grass and artemisia and sun-loving flowers. All these lower lake-basins were evidently formed by terminal moraine dams deposited where the receding glacier had lingered during short periods of less waste, or greater snowfall, or both.It is easier to feel than to realize, or in any way explain, Yosemite grandeur. The magnitudes of the rocks and trees and streams are so delicately harmonized they are mostly hidden. Sheer precipices three thousand feet high are fringed with tall trees growing close like grass on the brow of a lowland hill, and extending along the feet of these precipices a ribbon of meadow a mile wide and seven or eight long, that seems like a strip a farmer might mow in less than a day. Waterfalls, five hundred to one or two thousand feet high, are so subordinated to the mighty cliffs over which they pour that they seem like wisps of smoke, gentle as floating clouds, though their voices fill the valley and make the rocks tremble. The mountains, too, along the eastern sky, and the domes in front of them, and the succession of smooth rounded waves between, swelling higher, higher, with dark woods in[Pg 133] their hollows, serene in massive exuberant bulk and beauty, tend yet more to hide the grandeur of the Yosemite temple and make it appear as a subdued subordinate feature of the vast harmonious landscape. Thus every attempt to appreciate any one feature is beaten down by the overwhelming influence of all the others. And, as if this were not enough, lo! in the sky arises another mountain range with topography as rugged and substantial-looking as the one beneath it—snowy peaks and domes and shadowy Yosemite valleys—another version of the snowy Sierra, a new creation heralded by a thunder-storm. How fiercely, devoutly wild is Nature in the midst of her beauty-loving tenderness!—painting lilies, watering them, caressing them with gentle hand, going from flower to flower like a gardener while building rock mountains and cloud mountains full of lightning and rain. Gladly we run for shelter beneath an overhanging cliff and examine the reassuring ferns and mosses, gentle love tokens growing in cracks and chinks. Daisies, too, and ivesias, confiding wild children of light, too small to fear. To these one’s heart goes home, and the voices of the storm become gentle. Now the sun breaks forth and fragrant steam arises. The birds are out singing on the edges of the[Pg 134] groves. The west is flaming in gold and purple, ready for the ceremony of the sunset, and back I go to camp with my notes and pictures, the best of them printed in my mind as dreams. A fruitful day, without measured beginning or ending. A terrestrial eternity. A gift of good God.\nJune 14. The pool-basins below the falls and cascades hereabouts, formed by the heavy down-plunging currents, are kept nicely clean and clear of detritus. The heavier parts of the material swept over the falls are heaped up a short distance in front of the basins in the form of a dam, thus tending, together with erosion, to increase their size. Sudden changes, however, are effected during the spring floods, when the snow is melting and the upper tributaries are roaring loud from “bank to brae.” Then boulders that have fallen into the channels, and which the ordinary summer and winter currents were unable to move, are suddenly swept forward as by a mighty besom, hurled over the falls into these pools, and piled up in a new dam together with part of the old one, while some of the smaller boulders are carried further down stream and variously lodged according to size and shape, all seeking rest where the force of the current is less than the resistance they are able to offer. But the greatest changes made in these relations of fall, pool,[Pg 48] and dam are caused, not by the ordinary spring floods, but by extraordinary ones that occur at irregular intervals. The testimony of trees growing on flood boulder deposits shows that a century or more has passed since the last master flood came to awaken everything movable to go swirling and dancing on wonderful journeys. These floods may occur during the summer, when heavy thunder-showers, called “cloud-bursts,” fall on wide, steeply inclined stream basins furrowed by converging channels, which suddenly gather the waters together into the main trunk in booming torrents of enormous transporting power, though short lived.\nHow boundless the day seems as we revel in these storm-beaten sky gardens amid so vast a congregation of onlooking mountains! Strange and admirable it is that the more savage and chilly and storm-chafed the mountains, the finer the glow on their faces and the finer the plants they bear. The myriads of flowers tingeing the mountain-top do not seem to have grown out of the dry, rough gravel of disintegration, but rather they appear as visi[Pg 153]tors, a cloud of witnesses to Nature’s love in what we in our timid ignorance and unbelief call howling desert. The surface of the ground, so dull and forbidding at first sight, besides being rich in plants, shines and sparkles with crystals: mica, hornblende, feldspar, quartz, tourmaline. The radiance in some places is so great as to be fairly dazzling, keen lance rays of every color flashing, sparkling in glorious abundance, joining the plants in their fine, brave beauty-work—every crystal, every flower a window opening into heaven, a mirror reflecting the Creator.']	['<urn:uuid:30106118-c9a9-484a-95a8-13c1a8f5815f>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T05:28:56.360010	28	93	2157
11	how early muslim scholars approached psychology and how modern therapy handles religious beliefs	Early Muslim scholars wrote extensively about psychology under the term Ilm-al Nafsiat (self-knowledge), blending their psychological insights with Islamic philosophy and religious ideas. Their works appear to be the original foundation for many modern psychological theories. In contemporary psychotherapy, practitioners like those following psychoanalytic approaches treat clients' religious beliefs with respect and acceptance, viewing religion and spirituality as significant contributors to clients' schemas and implicit beliefs about themselves and the world. Rather than conducting religious counseling, they address religious material similar to other client associations - as manifestations of conscious and unconscious mental processes, while recognizing that spiritual involvement can serve as both a coping resource and potential source of conflict.	"[""Psychology from Islamic Perspective: Contributions of Early Muslim Scholars and Challenges to Contemporary Muslim Psychologists\n- Amber Haque\n- … show all 1 hide\nPurchase on Springer.com\n$39.95 / €34.95 / £29.95*\nRent the article at a discountRent now\n* Final gross prices may vary according to local VAT.\nEarly Muslims wrote extensively about human nature and called it Ilm-al Nafsiat or self-knowledge. In many cases, their works seem to be the original ideas for many modern day psychological theories and practices. What is interesting however is that a lot of what the early scholars wrote was blended with Islamic philosophy and religious ideas. This paper covers major contributions of prominent early Muslim scholars to psychology and outlines the challenges faced by today's Muslims in adapting to the Western theories. It also offers a few recommendations on the indigenization of psychology for Muslim societies interested in seeking the Islamic perspective on human behaviors.\n- Achoui, M. (1998). ''Human Nature from a Comparative Psychological Perspective.'' American Journal of Islamic Social Sciences, 15: 4, 71-95.\n- Ahmad, J. (1984). Hundred Great Muslims. Pakistan: Forezsons Limited.\n- Ansari, Z. A. (1992). Quranic Concepts of Human Psyche. Islamabad, Pakistan: Islamic Research Institute Press.\n- Badri, M. B. (1979). The Dilemma of Muslim Psychologists. London: MWH Publishers.\n- Faruqi, I. R. (1982). Al-Tawhid: Its meaning and Implications. VA, USA: International Institute of Islamic Thought, Herndon.\n- Haddad, Y. (1991). The Muslims of America. New York: Oxford University Press.\n- Hamarneh, S. K. (1984). In M.A. Anees (Ed.), Health Sciences in Early Islam: collected Papers, Vol. 2, Blanco, TX: Zahra Publications, 353.\n- Haque, A. (1998). ''Psychology and Religion: Their Relationship and Integration from Islamic Perspective,'' The American Journal of Islamic Social Sciences, 15, pp. 97-116.\n- Haque, A. (2004). ''Religion and Mental Health: The Case of American Muslims.'' Journal of Religion and Health, 43:1, pp. 45-58.\n- Haque, A. and Anuar, K. M. (2002). ''Religious psychology in Malaysia.'' International Journal for the Psychology of Religion, 12:4, pp. 277-289.\n- Hofmann, M. (2000). Islam the Alternative. Lahore, Pakistan: Suhail Academy.\n- Hussain, A. and Hussain, I. (1996). A brief history and demographics of Muslims in the United States. In Asad Hussain, John Woods and Javed Akhtar (eds.)-Muslims in America: Opportunities and Challenges. Chicago: International Strategy and Policy Institute.\n- Jordan, N. (1995). Themes in speculative psychology. In David Cohen (Ed.), Psychologists on Psychology, New York: Routledge.\n- Kimble, G. (1984). ''Psychology's Two Cultures.'' American Psychologist, 39, pp. 833-839.\n- Mohamed, Y. (1998). Human Nature in Islam. A.S. Noordeen: Kuala Lumpur, Malaysia.\n- Murken, S. and Shah, A. A (2002). ''Naturalistic and Islamic Approaches to Psychology, Psychotherapy, and Religion: Metaphysical Assumptions and Methodology-A Discussion.'' The International Journal for the Psychology of Religion, 12: 4, pp. 239-254.\n- Nasr, S. H. (1988). A Young Muslim's Guide to the Modern World. Lahore, Pakistan: Suhail Academy.\n- Nasr, S. H. and Leaman, O. (1996). History of Islamic Philosophy. London, UK: Routledge.\n- Norager, T. (1998). ''Metapsychology and Discourse: A Note on some Neglected Issues in the Psychology of Religion.'' The International Journal for the Psychology of Religion, 6, pp. 139-149.\n- Polkinghorne, D. (1984). ''Further Extensions of Methodological Diversity for Counseling Psychology.'' Journal of Counseling Psychology, 31, pp. 416-429.\n- Reich, K. H., and Paloutzian, R. F. (2001). Editors' Note: From Conflict to Dialogue: ''Examining Western and Islamic Approaches in Psychology of Religion.'' The International Journal for the Psychology of Religion, 12: 4, pp. 215-216.\n- Zuberi, M. H. (1986). Aristotle and Al Ghazali. Karachi, Pakistan: Royal Book Company.\n- Psychology from Islamic Perspective: Contributions of Early Muslim Scholars and Challenges to Contemporary Muslim Psychologists\nJournal of Religion and Health\nVolume 43, Issue 4 , pp 357-377\n- Cover Date\n- Print ISSN\n- Online ISSN\n- Kluwer Academic Publishers-Plenum Publishers\n- Additional Links\n- Islamic psychology\n- early Muslim scholars\n- history of psychology\n- Muslim psychologists\n- indigenous psychology\n- Amber Haque (1)\n- Author Affiliations\n- 1. Department of Psychology, UAE University, AL Ain, United Arab Emirates, Malaysia"", 'Addressing Issues of Spirituality and Religion in Psychotherapy\nFor individuals in the U.S. & U.S. territories\nIn Addressing Issues of Spirituality and Religion in Psychotherapy, Dr. Edward P. Shafranske demonstrates his psychoanalytic therapeutic approach to handling issues of spirituality and religion within the context of therapy. This approach treats the client\'s beliefs with respect and acceptance, allowing the therapeutic work to incorporate the client\'s spiritual and religious life into the therapy.\nIn this session, Dr. Shafranske works with a man in his 20s who is a devout Christian, but who seems to hide behind his beliefs in the face of conflict, particularly to avoid direct emotional experience. Dr. Shafranske helps the young man to move from a cognitive to an affective understanding of how he uses religion in part as a defense mechanism as well as an authentic expression of faith. Dr. Shafranske concludes that over time in successful treatment, the patient would transform his God-representations and would more fully integrate his religious beliefs into his life.\nDr. Shafranske\'s therapeutic approach emphasizes the development of insight and aims for greater integration of cognitive and emotional experiences, assisting clients to better attain their personal desires by resolving intrapsychic and interpersonal conflicts and modifying defenses. Drawing substantively upon a unified composite theory of psychoanalysis, he pays close attention to unconscious mental processes and defenses, inferred in the client\'s free associations, reflected in the ""here and now"" of the therapy interaction, and through an analysis of the ""in vivo"" experience of transference.\nAs the client achieves greater psychological freedom and authenticity in the context of an empathic therapeutic relationship, awareness of the self and relationship with others deepens. Past experiences, which have limited the client\'s ability to be fully genuine, are worked through by means of insight and catharsis, and gradually the client comes to see relationships and desires in new ways, leading to changes in basic schemas, defensive operations, unconscious compromises, and improvement in daily living.\nClinical techniques include the use of free association, dream interpretation, analysis of defenses, and transference interpretation; attention is focused on the therapeutic alliance and on the ways in which implicit organizing principles shape unconscious meaning, conscious attributions, and behavior. Change is initiated within the immediacy of a trusting, empathic therapeutic relationship and under the influence of transference.\nDr. Shafranske views religion and spirituality as contributing significantly to many clients\' schemas and the implicit beliefs they actively construct about themselves and the world in which they live. In addition, religious and spiritual involvement often serves as a potential resource in coping, and for some, a source of conflict. In psychoanalytic treatment, spiritual experience (broadly defined) is seen to reflect fundamental ways of viewing the self in relationship to ""transcendent realities"" as well as to others, and provides a means of articulating personal experiences associated to religious motifs.\nRather than conducting religious counseling, actively modifying religious attributions, or bringing religious and spiritual practices into the consulting room, the psychodynamic approach addresses religious and spiritual material in a manner similar to considering other client associations—as manifestations of conscious and unconscious mental processes. This approach recognizes that the understanding that is constructed concerning a clients\' religious and spiritual experiences, is at best ""interpreted"" experiences, which have particular meaning when coming to mind in the unique setting of treatment. Such understanding is limited and does not allow the ability to assess the ""truth claims"" inherent in such beliefs and experiences—in this respect, psychoanalyses remains silent.\nIn the clinical session shown here, Dr. Shafranske initially places emphasis on understanding how the client\'s experience of the idea of a God of Grace conflicts with his implicit experience of God. His apparent inability to reconcile this theological belief with his personal experience is hypothesized to reflect both psychological and spiritual conflicts, shaped in part by past and present experiences of others in his life.\nDr. Shafranske views the client\'s explicit ""religious conflict"" as concerning conflicts related to basic issues of trust, relationships, control, experiences of loss and anomie, and vulnerability to narcissistic injury. Beyond the content of the client\'s religious associations, Dr. Shafranske became aware in the session of what appeared to be the client\'s use of ""thinking"" as a defense against experiences of true emotional feeling and engagement, which also limited his awareness of events and relationships involved with unresolved losses and conflicts. With this clinical hypothesis in mind, Dr. Shafranske shifted emphasis from inquiry concerning the contents of the client\'s religious beliefs to the exploration and later interpretation of the immediate process and his use of religious thinking at times as a means or an outcome of moving away from painful emotional experiences.\nPsychotherapy rests on the establishment of a strong and effective therapeutic alliance. In Dr. Shafranske\'s view, clinicians must rely upon multiple theories and approaches in an effort to facilitate clinical collaboration. Psychodynamic psychotherapy provides but one approach to addressing human suffering; its principles can be applied singularly or can be meaningfully integrated with other clinical perspectives.\nA relatively wide range of adult and adolescent individuals can benefit from psychodynamic psychotherapy. Psychodynamic treatment is not well suited to address the needs of clients with antisocial personality disorder, those who are significantly impaired and have not demonstrated positive use of insight-oriented, transference-based treatment, or are in states of significant and destabilizing crisis. Following stabilization and with a careful review of history and ego functions and psychological capabilities and motivation, insight-oriented approaches may be considered.\nThe decision to address religious and spiritual issues within psychological treatment first requires a systematic assessment of the salience of spirituality in a client\'s orienting system and, in Dr. Shafranske\'s view, informed consent to actively explore such issues, particularly, if explicit integration is an aim. In psychodynamic psychotherapy, the exploration of spiritual issues occurs implicitly. Typically, when religion and spirituality play important roles in individuals\' lives, these individuals bring such issues into the therapeutic discourse.\n- Corbett, L. (1997). The religious function of the psyche. New York: Routledge.\n- Miller, W. R. (1999). Integrating spirituality into treatment: Resources for practitioners. Washington, DC: American Psychological Association.\n- Nielsen, S. L., Johnson, W. B., & Ellis, A. (2001). Counseling and psychotherapy with religious persons: A rational emotive behavior therapy approach. Mahwah, NJ: Lawrence Erlbaum Associates.\n- Pargament K. I. (1997). The psychology of religion and coping. New York: Guilford.\n- Richards, I. S., & Bergin, A. E. (2005). A spiritual strategy for counseling and psychotherapy (2nd ed.). Washington, DC: American Psychological Association.\n- Richards P. S., & Bergin, A. E. (Eds.). (2000). Handbook of psychotherapy and religious diversity. Washington, DC: American Psychological Association.\n- Richards, S. & Bergin, A. E. (Eds.). (2004). Religion and psychotherapy: A casebook. Washington, DC: American Psychological Association.\n- Randour, M. L. (Ed.). (1993). Exploring sacred landscapes. New York: Columbia University Press.\n- Rizzuto, A.-M. (2005). Psychoanalytic considerations about spiritually oriented psychotherapy. In L. Sperry & E. P. Shafranske, (Eds.), Spiritually Oriented Psychotherapy (pp. 31–50). Washington, DC: American Psychological Association.\n- Shafranske, E. P. (2004). A psychodynamic case study. In Richards, S. & Bergin, A. E. (Eds.), Religion and psychotherapy: A casebook (153–170). Washington, DC: American Psychological Association.\n- Shafranske, E. P. (2005). A psychoanalytic approach to spiritually-oriented psychotherapy. In L. Sperry & E. P. Shafranske, (Eds.), Spiritually Oriented Psychotherapy (pp. 105–130). Washington, DC: American Psychological Association.\n- Sorenson, R. L. (2004). Minding spirituality. Hillsdale, NJ: Analytic Press.\n- Shafranske, E. P. (Ed.). (1996). Religion and the clinical practice of psychology. Washington, DC: American Psychological Association.\n- Shafranske, E. P. (in press). Psychology of religion in clinical and counseling psychology. In R. Paloutzian & C. Park (Eds.), The handbook of the psychology of religion. New York: Guilford Press.\n- Sperry, L. (2001). Spirituality in clinical practice. Philadelphia: Brunner-Routledge.\n- Sperry, L. & Shafranske, E. P. (Eds.). (2005). Spiritually oriented psychotherapy. Washington, DC: American Psychological Association.\n- Tan, S.-Y. (1996). Religion in clinical practice: Implicit and explicit integration. In E. P. Shafranske (Ed.), Religion and the clinical practice of psychology (pp. 365–387). Washington, DC: American Psychological Association.\n- Christian Counseling\nMark R. McMinn\n- Mindfulness-Based Cognitive Therapy for Depression\nZindel V. Segal\n- Spiritual Awareness Psychotherapy\nLisa J. Miller\n- Theistic Integrative Psychology\nP. Scott Richards\n- Casebook for a Spiritual Strategy in Counseling and Psychotherapy\nEdited by P. Scott Richards and Allen E. Bergin\n- Handbook of Psychotherapy and Religious Diversity, Second Edition\nEdited by P. Scott Richards and Allen E. Bergin\n- Integrating Spirituality Into Treatment: Resources for Practitioners\nEdited by William R. Miller\n- Religion and the Clinical Practice of Psychology\nEdited by Edward P. Shafranske\n- A Spiritual Strategy for Counseling and Psychotherapy, Second Edition\nP. Scott Richards and Allen E. Bergin\n- Spiritually Oriented Psychotherapy\nEdited by Len Sperry and Edward P. Shafranske']"	['<urn:uuid:eb1f8733-3999-4609-8673-c7cb9542b61b>', '<urn:uuid:2197b5d9-3937-4c75-a2a7-ffa03b06b031>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T05:28:56.360010	13	111	2123
12	What's normal infant sleep duration, and how does sleep deprivation affect parents?	Newborns typically spend 15-18 hours sleeping each day, but this sleep comes in small chunks spread across 24 hours. For adults, including parents, sleep deprivation can be dangerous - reducing sleep by just 90 minutes can decrease daytime alertness by up to 32%. Sleep deprivation can lead to decreased performance, memory impairment, increased stress, and poor quality of life. Additionally, lack of sufficient sleep can cause irritability and reduce the ability to cope with stress.	"['By Jacque Ordner BSN, RN, IBCLC, RLC\nOne of the biggest concerns and struggles of parenting young children is SLEEP! Sleep deprived parents of newborns and infants have undoubtedly been asked, “Is she sleeping through the night yet?” Many of us have received advice from well-meaning friends and family regarding how much our babies “should” be sleeping and how to get them to do so. But what does research tell us? What is developmentally normal for babies? What about the countless methods and internet-based courses focused on getting babies to sleep? Read on for some important truths about newborn and infant sleep.\nFirst things first, it is completely normal for your breastfed newborn to wake and eat every few hours around the clock. Babies’ stomachs are small, and they need to feed frequently in order to consume adequate calories and nutrition. For most breast milk fed babies, this translates to 8-12 feedings in 24 hours. It’s true that most newborns spend 15-18 hours sleeping each day, but that sleep comes in small chunks spread across 24 hours. Frequent waking is also a protective mechanism that is designed to reduce the risk of SIDS! Check this link out for additional info https://www.ncbi.nlm.nih.gov/pubmed/9346985 .\nScheduling or restricting feeds can be dangerous! Any sleep training method, sleep course, sleep product, or sleep coach that that advises limiting feeds, scheduling feeds, or only feeding during certain “windows” for babies is NOT BREASTFEEDING FRIENDLY despite any claims to the contrary. Babies should be breastfed on demand rather than on a schedule or within specific windows. Our IBCLCs are aware of several cases where scheduling and restricting feeds has led to poor infant weight gain and significant milk supply issues. Feeding on demand is the best way to ensure your baby is getting enough milk and to encourage your own milk production. In fact, studies show that babies can take as much as 20% of their nutrition requirements during nighttime feeds.\nIt is a myth that babies regularly sleep through the night at just a few months old. In fact, most babies do not sleep through the night until closer to age one or beyond! Sleep is a developmental skill. Babies cannot learn to fall asleep on their own until they are developmentally ready to do so, and they aren’t all ready at the same age. It’s important to let go of unrealistic expectations regarding sleep. Though unicorn babies, who sleep all night at 4 weeks old, do exist, they are the exception, not the norm.\nWe can encourage healthy sleep behavior in our babies. Though sleep is developmental, we can still create an environment that fosters healthy habits related to sleep. Babies are often tired, but need help falling asleep. Learning your baby’s sleep cues is a first step to helping him get off to dreamland before becoming overtired and cranky. Common sleep cues include rubbing the eyes, yawning, drooping eyelids, redness around the eyes, smiling less, vocalizing less, exhibiting a weaker than normal suck, and turning away from toys or people. Picking up on these cues and responding in a way that helps our babies go to sleep is key! Experts also tell us that encouraging adequate daytime naps positively contributes to more consistent night–time sleep. Breastfeeding to sleep is normal, and absolutely ok! Your baby sees you and breastfeeding as their greatest source of comfort and safety, so it is only natural that they would desire that comfort and safety when trying to fall asleep. Many breastfeeding mothers follow the Safe Sleep Seven to help their babies feel safe and secure all night long. Your child will eventually be developmentally ready to learn to fall asleep all on their own even if they need your help right now. As your baby gets older, you can start to transition to more independent sleep behavior slowly.\nWill cutting out nighttime breastfeeds help my toddler sleep? It’s important to note that babies often require night–time feedings well beyond the newborn stage. It’s also important to note that cutting out night-time breastfeeds for older babies and toddlers needs to be approached on a very individual basis with special consideration to the child’s overall intake. It is not uncommon for older babies and toddlers to reduce night-time feedings if their daytime caloric intake simultaneously increases. Many parents find that night-time weaning is often easier when their child is closer to 18 months of age because they are capable of communicating and understanding the negotiation surrounding the weaning process. There is evidence that reducing night-time feeds contributes to less night-time waking for toddlers. Keep in mind that hunger is not the only reason that babies and toddlers wake at night! When made to feel safe, secure, and comforted older babies and toddlers can learn to get themselves back to sleep without parental intervention.\nInfant sleep problems are a common concern among parents, but knowing what is normal can help put your mind at ease!\nOur team of International Board Certified Lactation Consultants is happy to discuss newborn, infant, and toddler sleep as it relates to breastfeeding and pumping. Click here to schedule a consultation.\nBurnham, M., Goodlin-Jones, B., Gaylor, E., & Anders, T. (2002, September). Nighttime sleep-wake patterns and self-soothing from birth to one year of age: A longitudinal intervention study. Retrieved May 18, 2020, from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1201415/\nMcKenna, J., & McDade, T. (2005, June). Why babies should never sleep alone: A review of the co-sleeping controversy in relation to SIDS, bedsharing and breast feeding. Retrieved May 18, 2020, from https://www.ncbi.nlm.nih.gov/pubmed/15911459\nKent, J., Mitoulas, L., Cregan, M., Ramsay, D., Doherty, D., & Hartmann, P. (2006, March). Volume and frequency of breastfeedings and fat content of breast milk throughout the day. Retrieved May 18, 2020, from https://www.ncbi.nlm.nih.gov/pubmed/16510619\nMiddlemiss, W., Granger, D., Goldberg, W., & Nathans, L. (2012, April). Asynchrony of mother-infant hypothalamic-pituitary-adrenal axis activity following extinction of infant crying responses induced during the transition to sleep. Retrieved May 18, 2020, from https://www.ncbi.nlm.nih.gov/pubmed/21945361', ""Share this infographic on your site!\nEmbed this infographic on your site!\nThe editors at Health Science Degrees decided to research the topic of:\nThe Waking Dead: Dangers of Sleep Deprivation\nSleep deprivation plays a major role in a variety of conditions and illnesses, including decreased performance and alertness, memory and cognitive impairment, increased stress, poor quality of life, increased food consumption and appetite, on-the-job injuries and automobile accidents.\n- Reducing sleep by 90 minutes for just one night can reduce daytime alertness by up to 32%.\n- Untreated sleep disorders can have long-term effects on a person's health including high blood pressure, heart attacks, heart failure, stroke, obesity, psychiatric disorders, attention deficit disorder, mental impairment, and fetal and childhood growth retardation.\n- The National Highway Traffic Safety Administration (NHTSA) estimates that drowsy driving is responsible for at least 100,000 automobile crashes, 71,000 injuries and 1,550 fatalities each year.\n- In 2009-2010, in a study of 150,000 adults at or over age 18 in 19 states and the District of Columbia, 4.2% said they'd fallen asleep while driving at least once in the last 30 days. People who snored or slept 6 or fewer hours per night were most likely to fall asleep while driving.\n- Mortality risk is increased for people getting less than six or seven hours of sleep per night. Severe insomnia triples the mortality risk in elderly men.\n- More than 85 sleep disorders are recognized by the American Sleep Disorders Association, more than 70 million Americans are affected by these sleep disorders, and as many as one-third of Americans exhibit some symptoms of insomnia.\n- One cause of sleep disorders is sleep apnea. More than 20 million Americans, 24% of adult men and 9% of adult women, have some degree of obstructive sleep apnea.\nSleep recommendations for different age groups\n- Newborns: 16-18 hours/day\n- Pre-schoolers: 11-12 hours/day\n- School-aged children: At least 10 hours/day\n- Teens: 9-10 hours/day\n- Adults (including the elderly): 7-8 hours/day\n- Percent of adults getting an average of 6 hours or less sleep per night: 29%\n- Percent of high school students getting an average of 8 hours or less of sleep each night: 31%\n- Estimated cost to U.S. employers in lost productivity caused by sleep loss: $18 billion\n- In 2011, 10% of those surveyed, on average, reported that phone calls, text messages or emails on their cell phones woke them up after trying to go to sleep at least a few nights a week. Twenty percent of Gen Yers (ages 19-29) said this happens at least a few nights a week; 18% of Gen Zers (ages 13-18) reports this happens at least a few nights a week.\n- Cognitive impairment, similar to someone with blood alcohol content (BAC) of 0.05%, occurs after a person has been awake for 18 hours. After 24 hours of being awake, cognitive impairment is similar to someone with a BAC of 0.10%, higher than the legal limit in all states.\n- Drowsy driving can be fatal. Sleeping while sleep deprived makes drivers less attentive, it slows reaction time and it affects a driver's ability to make decisions. Commercial drivers, shift workers, drivers with untreated sleep disorders, drivers on sedating medications and drivers without sufficient sleep are most suspectible.\n- WebMD reports that the biggest danger of sleepiness is reduced reaction time. Teenagers, young adults, particularly young men, are at the greatest risk for fatigue-related auto accidents.\n- Lack of sufficient sleep can cause irritability, increase in anger and may reduce the ability to cope appropriately with stress.\n- Chronic sleepiness puts people at a greater risk for depression. In a 2005 study, people with insomnia were found to be 10 times as likely to develop depression and 17 times as likely to have significant anxiety""]"	['<urn:uuid:7c424e10-3a00-410d-9ad5-4c916a4b3592>', '<urn:uuid:9167b303-c0c2-471e-96e5-fc19426fc67f>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T05:28:56.360010	12	75	1625
13	what are good writing guidelines for email and how can errors impact career	Good email writing guidelines include following the same rules as other written correspondence, using proper punctuation, organizing content logically, and keeping messages professional with proper capitalization. The impact of email errors on careers is significant - they can create negative lasting impressions, make you appear less intelligent and trustworthy to others, and even cost you job opportunities, as many executives won't hire candidates who make grammar mistakes.	"['This is a simple primer on written communications. Numerous books are available on writing for business and writing in general. I\'ll offer a piece of advice here: Writing for business is really not that different from any good writing.\nWhen it comes to a manual for effective writing, there is only one I need to recommend. Elements of Style by Strunk and White is considered the standard in the field. It covers basic writing rules and usage that, if adhered to, produce excellent results. Although many other books exist, I\'ll begin and end my recommendation here.\nWith that said, the sections that follow provide some guidelines to effective written communications.\nSome general guidelines for all forms of written communication are as follows:\n- Be brief— It\'s not about volume. The goal is well-understood and relevant content.\n- Make your point first, and then back it up— Those who write infrequently often begin by filling in details in an attempt to take the reader through their thought process. Don\'t! Instead, make the point and then fill in any necessary information. You will find in many cases that the point itself is sufficient.\n- Watch punctuation— A question is a question. Commas separate thoughts. It simply looks more professional and more intelligent to punctuate your work properly.\n- Break up your ideas— Use paragraphs to separate ideas. In the freeform world of e-mail, this rule is often abused. Long, unbroken paragraphs are difficult to read. In addition, use spacing between ideas to create a logical break for the reader.\nFew pieces of written correspondence have the impact of a well-written business letter.\nProperly address your letter. If you are unsure of the addressee, call the company or call the person who is receiving the correspondence.\nUse the person\'s name in the letter. This demonstrates your ongoing recognition of that person.\nCreate a logical flow in your letter. Use the tips for structure that are described in the section ""The Well-Crafted Page"" later in this chapter. This can help you organize your letter\'s ideas.\nKeep your intended recipient in mind as you write. Have a clear understanding of what this person is most interested in. Don\'t include tangential information that dilutes the primary message.\nMake a clear call to action or request. You are writing the letter to give information, request something, or spur some type of response. Make sure you let the person know what you expect as a result of your letter. Don\'t be vague.\nI have always found it surprising that individuals who normally write well-conceived and organized communications (letters or memos) throw all that out when it comes to e-mail.\nThis disregard takes the form of sloppy punctuation, lack of organization, a general disregard for capitalization, and other problems. For some reason, many believe that e-mail communications do not have to adhere to the normal rules that guide any of their other correspondence.\nIf that has been your approach, change it. With few exceptions, e-mail should follow most of the same guidelines that are used in other written correspondence. Just because it starts in electronic format does not mean it stays there.\nAlso, it is likely that your e-mail will be passed along to someone else—perhaps someone you do not know. Those scattered thoughts and run-on sentences might be some other individual\'s only exposure to you. Believe me, poor grammar in an e-mail can have an impact.\nIf your e-mail is a short clip of information, use the same guidelines as you would when creating a memo. If it is a longer correspondence, it should read—and be written—like a letter.\nI understand that there are exceptions. A quick yes/no type of clipped response can be appropriate. However, communications of any substance should not take on the roguish and unstructured format that is often present in e-mail correspondence.\nThe Well-Crafted Paragraph\nIf you have never been much of a writer, I am going to give you a basic formula for a well-crafted paragraph. Understand that I deviate from this formula often. It is a format to provide a guideline to help organize your writing. After you have learned this and practiced it well, you can depart from this form, too.\nYour standard paragraphs should have four to seven sentences. The introductory sentence introduces the main idea or topic. Two to five sentences form the body and should explain the idea introduced in sentence one. The final sentence concludes your thought by reiterating the original topic and the ideas presented in the body.\nCareer building is a long-term activity. It requires formulating a plan that involves coordinating skills and desires. After you\'ve formed the plan, you can modify it based on your circumstances. However, a big picture mindset is required to ensure that changes in the plan are not based on reactions or compulsive behavior. Keeping such a perspective greatly enhances career development and opportunities.\nI\'ll admit it: The paragraph isn\'t great. But it does have a natural starting and ending point. The last sentence effectively summarizes the information presented. You would do well to follow this structure and pattern.\nBut what about the rest of your document? You now have a formula for writing a paragraph, but maybe you still can\'t put together a document.\nThe Well-Crafted Page\nFortunately, the formula for writing an effective paragraph can be used quite nicely to write a longer piece, too. In fact, the formula works well to organize thoughts in general.\nWhen writing a longer piece, such as a memo, use the following formula:\n- Paragraph one— Introduces the main idea or ideas to be conveyed in the memo.\n- Paragraphs two through four— Provide content for the ideas to be conveyed. Each paragraph, of course, still follows the simple guideline outlined earlier.\n- Conclusion paragraph— Summarizes the ideas presented and wraps up the document by issuing a decision or call to action on the ideas presented.\nI am not advocating that all business correspondence should be 25 sentences long (5 paragraphs x 5 sentences each). The formula simply provides guidelines for those who are unfamiliar or unpracticed in writing.\nYour writing will improve with use. And as it does, you will feel comfortable enough to move away from the formulas presented here. However, I still find myself following these same guidelines when words or ideas are hard to come by. They are highly effective.', 'Bad Email Grammar Ain’t Good for Getting You a Job or a Date\nNo matter the position or industry that you’re involved with, writing has become a requirement for almost each and every one...\nNo matter the position or industry that you’re involved with, writing has become a requirement for almost each and every one of us. Whether it’s a text, social media update, blog post, or email, we rely on written messages to communicate with each other.\nWhile it may not seem like a major concern, making even the smallest of mistakes when composing these written messages can have a major impact on our careers since poor writing skills can give colleagues and customers the impression that we’re not really educated or skilled enough to do our jobs properly.\nOut of all forms of communication, email deserves special mention since it’s the preferred communication channel. In fact, Marketingsherpa discovered that an astounding 72% of customers prefer communication with companies through email.\nBecause email is such a powerful communication tool, it can have serious implications for your career if you don’t use proper grammar.\nCreates a lasting impression.\nIt’s no secret that first impressions are important. So, what kind of impression would you get if you received an email from a job applicant or client that was riddled with spelling and grammatical errors? In most cases, you probably would have serious doubts about whether or not you were going to work with this person or take the individual all that seriously.\nIn fact, in a study of 1700 online dating sites, 43 percent of users considered bad grammar decidedly unattractive and 35 percent thought good grammar was appealing. In another study conducted by psychologists Jane Vignovic and Lori Foster Thompson that focused solely on electronic communication, writers of the message were found to “be less conscientious, intelligent and trustworthy when the message contained many grammatical errors.”\nKyle Wiens, CEO of iFixIt, writes that, “If you think a semicolon is a regular colon with an identity crisis, I will not hire you. If you scatter commas into a sentence with all the discrimination of a shotgun, you might make it to the foyer before we politely escort you from the building.”\nWiens, like many other executives, places a high value on grammar because if you’re sending out an email with errors, it’s a reflection on your entire organization. If you can’t take the time to spell check an email, then how can customers or clients be sure that this organization can deliver quality products or services?\nSends the wrong message.\nDo you think your employer will take your pitch, suggestion, or proposal into consideration when your emails are full of unjustifiable mistakes like not knowing the difference between there and their? Such mistakes can hamper your ability to make a case for yourself since your boss or colleagues may constantly be second-guessing your work.\n“Even worse, including the wrong email grammar can completely change your intended message,” says John Rampton in his content marketing guide. “For example, if you email your boss that you have a ‘cleaver idea,’ what do you think their response will be? Unless you’re in the hatchet making industry, that message wouldn’t make a whole lot of sense. But your boss could sure have fun rubbing the error in your face for a week or two.”\nLeads to more mistakes.\nThere was an interesting study conducted at the University Michigan where it was found that spelling errors made while filling out the forms for peer-to-peer loan requests at LendingTree.com made an impact on the likelihood of funding. In most cases these errors lead to an unhappy outcome.\n“I’ve found that people who make fewer mistakes on a grammar test also make fewer mistakes when they are doing something completely unrelated to writing — like stocking shelves or labeling parts” writes Wiens. For example, “programmers who pay attention to how they construct written language also tend to pay a lot more attention to how they code.”\nIn other words, those who make fewer errors in their emails are more detail-oriented, which means that they’ll make fewer mistakes with their overall responsibilities.\nCan cost you thousands of dollars.\nThere’s an infamous story of how one little punctuation error cost NASA $80 million. When launching the Mariner 1 in 1962, the “omission of the hyphen, part of a code that set trajectory speed led to an explosion.”\nWhile you probably won’t be in a situation that could cost you that much money, improper email grammar could cost you thousands of dollars. If you were to send out an email blast to your customers detailing an upcoming sale, do you believe that they’ll find you credible if the email is full of mistakes?\nHow to avoid spelling errors.\nEmail grammar is an area that shouldn’t be overlooked. It showcases your professionalism and increases your trustworthiness — which can help you get hired, attract new clients, and ensures that your message is clearly delivered to customers and colleagues.\nTo help prevent you from making common mistakes, here are some pointers that you can use the next time you’re sending out an email.\nDon’t rely just on spell check. Spell checking software and tools have come a long way since the early days. And while they are beneficial, spell checking software isn’t 100 percent foolproof. Reread the email before you hit send. If there’s an extremely important email, ask someone else, like a colleague, to review the email. They’ll often spot any errors that you may have overlooked.\nRead it out loud. If you don’t have a second pair of eyes, read the email out loud. It’s a common proofreading technique that can help you pick up any errors like misused words, misspelled words or run on sentences.\nTake a break. Instead of quickly punching send on your email, step away and revisit it. This could be anytime from one-minute later to the next morning after the initial composition. Rereading the email with a fresh set of eyes can help you notice any mistakes.\nKeep it professional. Sometimes quick and casual emails are effective. The problem is that if you move too quickly you’ll be more inclined to make errors or mistakes. Even though this will take more time, always keep your emails professional by using full sentences, proper capitalization, punctuation, and professional language like neutral pronouns. Don’t forget to include a salutation and signature as well.\nDon’t use excessive punctuation marks or all caps. Emails containing exclamation marks and all caps come across as unprofessional and aggressive. If the message is that important then mark it as high priority.\nLook for one mistake at a time. When proofreading your emails you don’t have to look for all potential errors at once. Instead, look for one mistake at a time. For example, the first time you reread an email you might look for any spelling mistakes, followed by grammar, punctuation, and fact checking, if applicable.\nThis article originally appeared on Entrepreneur.com.']"	['<urn:uuid:a08b9bef-34f8-46e4-a3a7-4f9bca623f8c>', '<urn:uuid:f5d80714-9d72-43b2-a1a7-bf6f51dffd15>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T05:28:56.360010	13	67	2230
14	What are the main parts that make up a tooth's structure and what does each part do in protecting the tooth?	A tooth has three primary structures: the enamel, which is the hard, mineralized covering of the tooth; the cementum, which covers the root and helps stabilize tooth anchorage; the dentine, which makes up the majority of the tooth and has a protective function but is susceptible to cavities; and the pulp, which is the core of the tooth containing nerves and blood vessels.	['Did you know that the first process of food digestion occurs in the mouth? The first mechanical digestion stage occurs as food is chewed down by teeth, coupled with chemical digestion by salivary enzymes, and the masticated food then proceeds to the gastrointestinal tract for further digestion. Herein, we will be talking about dentistry basics, from tooth types down to their respective functions. Before that, however, it would be helpful if we first talk about the underlying tooth anatomy.\nWhat Is a Tooth Made Of?\nA tooth has three primary structures. These are:\n(1)the enamel – hard, mineralized covering of the tooth\n(2)the cementum – covers the root of the tooth and helps stabilize tooth anchorage\n(3)the dentine – consists majority of the tooth and it has a protective function; susceptible to cavities and decay\n(4)the pulp – core of the tooth, containing nerves and blood vessels\nAltogether, these structures make up the specific anatomical regions of a tooth. Specifically, the crown and the root are the two basic anatomical parts of the tooth which serve specific functions. The root is the part embedded under the gums which serve as an anchor to the tooth socket of the jaw. Conversely, the crown is the visible part which is covered with enamel and is subject to wear and tear. The crown and the root usually varies in shape and number respectively, depending on the type of tooth it comprises.\nDifferent Types of Teeth According to Functions\n(1) the Incisors – these are the front teeth (four on the upper jaw and four on the lower jaw) which usually takes the first bite of food and these are also the ones we see whenever we smile.\n(2) the Canines – these are two pairs of sharp teeth and it primarily functions to tear food (especially the ones that are difficult to chew)\n(3) the Premolars – found at the each side of the mouth and helps further masticate food\n(4) the Molars – act as reserves in any case that a premolar is lost; usually hard to reach when brushing.\nThese types of teeth appear at different stages of human development. Some of these teeth appear during infancy and slowly but surely, the others follow as a person grows and develops and this explains why numbers vary in a person’s lifetime.\nYour Number of Teeth Vary as You Develop\nCommonly, adult humans have more in number than children, having 32 permanent ones in a set. This set consists of 12 molars (along with four wisdom teeth), eight premolars, four canines and eight incisors. In contrast, children have a set of teeth consisting of 20 baby teeth, which eventually fall out and gets replaced as they grow. However, it should be noted that proper dental care should is needed regardless if the set of teeth is permanent or deciduous.\nDental Care In a Nutshell\nProper dental care is essential to maximize the functionality of our teeth. Besides its aesthetic value, these also serve as the primary mechanical digesters which help us break down food for our body to process. A tooth has several structures which altogether, forms the two major parts of a tooth, which vary in shape and size. These variations all lead to a person having different types of teeth which serve different roles in chewing. While precursors of these teeth types are present during childhood, completion of a set of teeth usually does not occur until puberty, where a full set of 32 teeth from each type are present.']	['<urn:uuid:e1ab1145-7b17-4313-911c-aa064b9e5cd5>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T05:28:56.360010	21	63	589
15	colorado restrictions rules wood burning fireplaces altitude limit	In Colorado, uncertified wood-burning fireplaces and stoves in new construction are not allowed at elevations below 7,000 feet, and they must be specially engineered units designed as principle heat sources, not for ornamental use.	"['Logs, wood chips, brush clippings, and grasses all serve as woody biomass—renewable fuels suitable for producing heat and power. Burning woody materials is the original home heating system since there is evidence that prehistoric peoples burned woody materials in caves to provide heating—millennia before anyone recognized that coal or other materials could be burned to produce heat. And even in the not-so-distant past, such as in colonial America, burning wood logs in a fireplace or wood stove was the principal means for heating a home.\nToday, wood-burning fireplaces and stoves remain a viable means of providing heat, although the use of traditional designs is now usually more a matter of nostalgia and atmosphere than practical function.\nWhen wood burns, it releases hazardous gases (nitrogen oxide and carbon monoxide) and soot (particulate matter). To avoid pollution issues, fireplaces, wood stoves, and wood energy appliances and facilities must be properly designed and receive permits that meet air quality regulations and standards.\nFrom 19th Century Fireplaces to Institutional Boilers\nWhere they still exist in today\'s homes, these old-fashioned open fireplaces are more for ambiance than for heating, because the design is inherently inefficient in terms of energy use. While an open fireplace feels cozy and warm in the immediate area around the fireplace, the natural convection currents actually cause an open fireplace to suck indoor air up and out of the house through the chimney. More than one homeowner has noticed that while a crackling fireplace blazes in the den or family room, it is also sucking in cold air around windows and doors in other parts of the house. In addition, the traditional wood fireplace or wood stove only makes use of roughly 15 percent of the potential energy stored in wood, so these older designs are increasingly giving way to more energy-efficient methods.\nWood fires also have the very real drawback of creating air pollution. In some parts of the country, there are legal restrictions on the use of wood-burning fireplaces and stoves. In Colorado, for example—a setting where you might think that a wood-burning fireplace is almost mandatory—wood smoke is such a pollution hazard that uncertified wood-burning fireplaces and stoves in new construction are not allowed at elevations below 7,000 feet, and where they are used, they must be specially engineered units designed as principle heat sources, not for ornamental use. At certain times of the year, deemed ""high-pollution days,"" wood-burning in uncertified fireplaces may be forbidden entirely.\nFor this reason, where wood-burning is desired as an actual home-heating strategy, the systems now usually involve sealed furnaces or pellet stoves that burn compressed biomass pellets. There are also fireplace and stove designs that use sealed burning chambers and mechanical air circulation features. A vibrant industry exists for converting older fireplace units to more energy-efficient ""certified"" designs. The design is aimed at burning the wood at higher temperatures that minimize the particulate matter that escapes as smoke and wrings as much energy as possible from the wood burned as fuel. The EPA has a certification program for wood stoves and fireplace inserts. Of the approved styles, catalytic wood stoves burn more cleanly (emitting no more than 4.1 grams of particulate per hour).\nOn a larger scale, woody biomass can operate boilers that heat schools, offices, institutions and manufacturing facilities. The largest wood-powered facilities typically produce both heat and electricity at the same time. Such thermal and electric power “cogeneration” systems are actually the most energy-efficient.\nLike any power source, woody biomass has its benefits and challenges as a fuel source.\nThe net fuel cost is cheaper than heating oil, natural gas, or coal.\nWoody biomass can be grown and purchased locally, thereby benefitting local economies.\nFuel (in the form of logs, wood chips, brush clippings, grasses, and lumber yard waste) is widely available, renewable, and sustainable.\nFuel prices are relatively stable.\nPellet stoves, which burn pellets made of compressed woody biomass, are relatively non-polluting and are approved by the EPA.\nWith proper engineering, wood-burning systems emit fewer pollutants into the air than coal and oil.\nOver its lifecycle, biomass is a carbon-neutral fuel source.\nUsing forest wastes improves forest health. By removing deadfalls from the forest, the practice can reduce forest-fire hazards.\nConstant use requires a continuous supply of logs, wood chips or other biomass.\nWood systems require more space to store bulky fuel.\nWaste ash that remains after burning needs proper, safe disposal.\nMost pellet stoves and sealed fireplaces require some electricity to operate circulating fans.\nThe upfront, capital cost of building a sizeable wood energy facility can be high; it can take years to realize any savings.\nCompared with conventional gas or oil boiler systems, wood systems require a larger boiler to handle the fuel.\nAutomated wood chip conveyor systems and fuel-handling equipment must be monitored closely to prevent jams and system shutdowns.\nWood chip fuel varies by size, moisture content, and energy content. Standard un-dried, or “green,” fuel contains 30 to 55% water, which slows combustion.\nEquipment to dry wood chips and improve efficiency is very expensive. Note: dry wood is highly flammable and requires a sophisticated boiler system.']"	['<urn:uuid:0dcc2939-8923-44c9-a9f4-06c485e1db06>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T05:28:56.360010	8	34	851
16	what is difference between json vs msgpack	JSON is simple, human readable, and has widespread support, making it typically the first choice for most uses. MsgPack, on the other hand, should be used instead of JSON when performance is a critical issue. While both are data serialization formats, JSON prioritizes readability while MsgPack prioritizes performance.	"['Part I: Architecture Styles\nAvailability measures the percentage of the time that the system is functional and working. It has a very simple formula:\nuptime ------------ x 100% total time\nThe availability clearly defines how well the system succeeds providing services to the customers. Improving the availability even a little bit needs a lot of men and efforts.\nIt\'s not surprising that more people are hired for maintaining a legacy system, instead of building from scratch. Improving the maintainability of a system reduces the cost and hence it\'s an over-going thing that most companies are thriving to achieve.\nThere is no easy way to measure the maintainability of a system, though we can take a lot of actions to make it simpler, make the life of operations easier, and make it easier to extend and grow.\nReliability defines if the system continuous to work correctly. To provide a reliable service, the system should be fault-tolerant, or resilient.\nProviding a reliable service requires not to just do everything that keeps system available, but also introducing more thorough validations and reviews. A lot of engineering practises makes the service more reliable, for example, code review, continuous integration, security review, health checking, alerting, etc. By leveraging the monitoring & alerting system, we can reduce the time that system is deviated from spec. If necessary, degrade the surrounding features and let the service process only critical business logics.\nThere are two ways of scaling.\n- Vertical scaling, or scaling up, by using more powerful machines with faster CPU, higher memory, and larger disk space.\n- Horizontal scaling, or scaling out, by adding more machines into a cluster. The performance of the cluster is the summary of all machines and the network in-between.\nMoore\'s law has reached the end, meaning the limit of vertical scaling is out there. As a result, if you want your system continuously grow, do horizontal scale!\nThe layered architecture has several other names, such as onion architecture, the clean architecture, etc. The basic theory is, you organize the components layer by layer in which only the upstream layer can make calls to the downstream layers.\nMicroservices architecture is an architectural style that structured applications as a set of loosely decoupled services. The advantage of microservices architecture is it enables large and complex application to continuously scale and evolve.\nPart II: Architecture Blocks\nProducer-consumer model is very helpful to decouple the system components. However, the situation is quite often in which producer produces jobs more rapidly than consumers can consume them. It\'s a challenge to manage a large number of unconsumed jobs.\nApplying back-pressure is one effective technique to handle high-load. We tend to handle already accepted requests and transactions first, and reject those can\'t be handled. If you have many components and the producing-consuming speed might mismatch, consider back-pressure.\nA container is merely an OS process, except that it\'s being isolated, secured, and limited. All values added to the process make the container the dominant technology in the cloud era.\nCGroups is a Kernel feature that organizes processes into hierarchical groups to limit and monitor their system usage such as CPU, memory, disk, network and so on.\nOne major use case of the namespace is to isolate processes belonging to a container from other containers or the system namespace.\nNSEnter is a utility enters the namespaces of one or more other processes and then executes the specified program. In other words, we jump to the inner side of the namespace.\nUnion File System or UnionFS variants such as AUFS, btrfs, vfs, and devicemapper are the file system that used by most container engines. It allows files and directories of separate file systems overlaid one by one, forming a final single coherent file system.\nPerformance issue on upstream service often leads to downstream application crash. By applying with Circuit Breaker on downstream application side, we can prevent the entire system from cascading failure. The state machine is in the core algorithm of Circuit Breaker. You can choose one of the listed library above and apply one of the listed API style above to improve your service.\n- JSON is usually your first choice. It\'s simple, human readable, and has most widespread support.\n- Use MsgPack instead of JSON if performance is an issue.\n- Use Protobuf if type check and schema check is essential. gRPC is recommended as an RPC framework based on Protobuf.\n- Use Thrift if you\'re developing RPC services and don\'t like Protobuf syntax.\n- Use TOML if you\'re serializing some config files.\n- Use CSV if you\'re serializing data to non-technical people.\n- Use INI if you want something simpler.\n- Use YAML if you want something more complex.\n- Use language built-in serialization functions or methods if the use case is only limited in a single language, and you don\'t care security that much (not good).\nLoad balancing is fundamental way to build a large distributed system, and hence knowing it well is important. To build a reliable system, a mature load balancer hardware or software is essential. If you have a lot of money, then buy a load balancing hardware. Otherwise, a load balancing software is recommended.\nDNS load balancing distributes requests across multiple IP addresses by configuring various DNS A records. Modern tools enable programmatically updating DNS records. When the incident happens, some of them can even automatically update the DNS records. The downside of DNS load balancing is that it cannot distribute requests evenly to the backend servers.\nIn-memory databases are faster than on-disk databases because disk access is slower than memory access. Meanwhile, to overcome the drawback of data losing from crashing, we have to introduce strategies like snapshotting, transaction logging, consistent hashing, high availability. Despite of all the complexity introduced, people love in-memory databases when response time is really a criterion since it\'s probably the best solution. And in most case, Redis could be the first choice.\nJob queue is an essential component to extend request-response model for handling time-consuming jobs. Choose a Job Queue framework that has API and features you like, and make sure that you have solutions to overcome the disadvantages.\nLoad is a set of numbers that describe performance of system. The meaning of numbers depends on what system is running.\nIf you are maintaining a production system, the high load averages or percentiles are things to worry about. When they\'re high, either identify the bottleneck or simply assign more servers or instances.\nUsing crontab is the easiest way to schedule periodic jobs. The limitation is that you can’t control the resource usage and it’s less flexible. To run periodic jobs in a fine-controlled environment, you might want to choose Kubernetes CronJob. To leverage the power of periodic scheduler, you might want to integrate a scheduler library into your application.\nSharding is a type of database partitioning technique that manages the subsets of data on several server hosts. It solves SPOF problem and single server resource limitation but introduces sharding logic to be implemented. Data sharding can be simple or complex depending on the sharding strategy.\nSidecar is a term for a one-wheeled device attached to the side of a motorcycle. In engineering, it signifies a deployment model that one or more separate processes or containers deployed along with the application. The solution is:\n- Place peripheral tasks like logging, monitoring, proxy, circuit breaker inside a standalone process or container.\n- The independent process or container co-locate with the supporting application.\n- Provide a generic interface for sending data whatever the programming language of the app is.\nThe status site is an individual website listing the particular component statuses that make up the product.\nIt shows below two information:\n- Display the status of each function of the business.\n- Display A list of incidents organized on a daily basis. If nothing happens, show ""No incidents reported."", otherwise, show the details of the incidents, such as when the incidents were detected, how the incidents were handled, and when the incidents were resolved.\nIt has below goals:\n- Show how reliable the platform is.\n- Show how well the platform is recovering from failure.\n- Show how performance is as it evolves.\nThe status site shows the statuses of components of the product. It\'s about to be transparent to users. Users know exactly where to look where there is downtime and staffs will be acting on the information they know is up-to-date.\nIt\'s impossible to achieve both goals without changing the execution model, to keep the system responsive all the time and to complete the time-consuming jobs.\nThere are at least three solutions: slicing jobs, pre-executing jobs, post-executing jobs.\nPart III: Architecture Examples\nSQLAlchemy might be the best ORM software in the Python world regardless of your taste. Though you need to learn several fundamental concepts, it\'s still easy to use. If you\'re writing a Web application and needs to manipulate data with databases, SQLAlchemy is always a strong candidate.\nAPScheduler is a job scheduling framework that executes code either one-off or periodically. People often integrate it into an existing Python application for running interval jobs.\nAirbnb is a website that operates an online marketplace and hospitality service for people to lease or rent short-term lodging. The challenges for the engineering team includes high-availability, quick-scaling, etc.\nSkipper is an HTTP router and reverse proxy for service composition. The internal modules are well-decoupled and extendable. It makes adding new data sources and new routing strategies without losing performance.']"	['<urn:uuid:c449f38c-1bf1-4a47-bb42-ffcbc975c76e>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-13T05:28:56.360010	7	48	1579
17	I heard Jerusalem surrendered to Muslims peacefully - how did it happen?	Jerusalem surrendered without a siege, but made a special condition that it would only surrender to Caliph Omar personally. Omar traveled 600 miles to Jerusalem in 638 with just one attendant, riding a camel and carrying only basic provisions. He met with the Patriarch of Jerusalem and they toured the Holy Places together. Under the surrender treaty, Christians were allowed to keep their churches and relics, only having to pay a poll tax.	"['Caliphs Abu Bekr And Omar\n( Originally Published Early 1900\'s )\nThe true embodiment of the spirit of Islam was not Muhammad, but his close friend and supporter Abu Bekr. There can be little doubt that if Muhammad was the mind and imagination of primitive Islam, Abu Bekr was its con-science and its will. Throughout their life together it was Muhammad who said the thing, but it was Abu Bekr who believed the thing When Muhammad wavered, Abu Bekr sustained him. Abu Bekr was a man without doubts, his beliefs cut down to acts cleanly as a sharp knife cuts. We may feel sure that Abu Bar would never have temporized about the minor gods of Mceca, or needed inspirations from Allah to explain his private life. When in the eleventh year of the Hegira (632) the Prophet sickened of a fever and died, it was Abu Bekr who succeeded him as Caliph and leader of the people (Kalifa = Successor\'), and it was the unflinching confidence of Abu Bekr in the righteousness of Allah which prevented a split between Medina and Mecca, which stamped down a widespread insurrection of the Bedouin against taxation for the common cause, and carried out a great plundering raid into Syria that the dead Prophet had projected. And then Abu Bekr, with that faith which moves mountains, set himself simply and sanely to organize the subjugation of the whole world to Allah with little armies of 3,000 or 4,000 Arabs according to those letters the Prophet had written from Medina in 62S to all the monarchs of the world.\nAnd the attempt came near to succeeding. had there been in Islam a score of men, younger men to carry on his work, of Abu Bekr\'s quality, it would certainly have sucseeded. It came near to succeeding because. Arabia was now a centre of faith and. will, and because .nowhere else in the world until. China was reached, unless it was upon the steppes of Russia or Turkestan, was there another community of free-spirited men with any power of belief in their rulers and leaders. Thé head of the Byzantine Empire, Heraclius, the conqueror of Chosroes II, Was past his prime and suffering from dropsy, and his empire was exhausted by the long Persian war. The motley of people under his rule knew little of him and cared less. Persia was at the lowest depths of monarchist degradation, the parricide Kavadh II had died after a reign of a few months, and a series of dynastic intrigues and romantic murders en-livened the palace but weakened the country. The war between Persia and the Byzantine Empire was only formally concluded about the time of the beginning of Abu Bekr\'s rule. Both sides had made great use of Arab auxiliaries; over Syria a number of towns and settlements of Christianized Arabs were scattered who professed a baseless loyalty to Constantinoplc; the Persian marches between Mesopotamia and the desert were under the control of an Arab tributary prince, whose capital was at Hira. Arab influence was strong in such cities as Damascus, where Christian Arab gentlemen would read and recite the latest poetry from the desert competitors. There was thus a great amount of easily assimilable material ready at hand for Islam.\nAnd the military campaigns that now began were among the most brilliant in the world\'s history. Arabia had suddenly become a garden of fine men. The name of Khalid stands out as the brightest star in a constellation of able and devoted Moslem generals. Whenever he commanded he was victorious, and when the jealousy of the second Caliph, Omar, degraded him unjustly and inexcusably,1 he made no ado, but served Allah cheerfully and well as a subordinate to those over whom he had ruled. We cannot trace the story of this warfare here; the Arab armies struck simultaneously at Byzantine Syria and the Persian frontier city of Hira, and everywhere they offered a choice of three alter-natives: either pay tribute, or confess the true God and join us, or die. They encountered armies, large and disciplined but spiritless armies, and defeated them. And no-where was there such a thing as a popular resistance. The people of the populous irrigation lands of Mesopotamia cared not a jot whether they paid taxes to Byzantine or Persepolis or to Medina; and of the two, Arabs or Persian court, the Arabs, the Arabs of the great years, were manifestly the cleaner people, more just and more merciful. The Christian Arabs joined the invaders very readily and so did many Jews. Just as in the west, so now in the east, an invasion became a social revolution. But here it was also a religious revolution with a new and distinctive mental vitality.\nIt was Khalid who fought the decisive battle (634) with the army of Heraclius upon the banks of the Yarmuk, a tributary of the Jordan. The legions, as ever, were without proper cavalry; for seven centuries the ghost of old Crassus had haunted the east in vain; the imperial armies relied for cavalry purposes upon Christian Arab auxiliaries, and these deserted to the Moslems as the armies joined issue. A great parade of priests, sacred banners, pictures, and holy relics was made by the Byzantine host, and it was further sustained by the chanting of monks. But there was no magic in the relics and little conviction about the chanting. On the Arab side the Emirs and sheiks harangued the troops, and after the ancient Arab fashion the shrill voices of women in the rear encouraged their men. The Moslem ranks were full of believers before whom shone victory or paradise. The battle was never in doubt after the defection of the irregular cavalry. An attempt to retreat dissolved into a rout and became a massacre. The Byzantine army had fought with its back to the river, which was presently choked with its dead.\nThereafter Heraclius slowly relinquished all Syria, which he had so lately won back from the Persiana, to his new antagonists. Damascus soon fell, and a year later the Moslems lems entered Antioch. For a time they had to abandon it again to a last effort from Constantinople, but they re-entered it for good under Khalid.\nMeanwhile on the eastern front, after swift initial success which gave them Hira, the Persian resistance stiffened. The dynastie struggle had ended at last in the coming of a king of kings, and a general of ability had been found in Rustan. He gave battle at Kadessia (637). His army was just such another composite host as Darius had led into Thrace or Alexander defeated at Issus; it was a medley of levies. He had thirty-three war elephants, and he sat on a golden throne upon a raised platform behind the Persian ranks, surveying the battle, which throne will remind the reader of Herodotus, the Hellespont, and Salamis more than a thousand years before. The battle lasted three days ; each day the Arabs attacked and the Persian host held its ground until nightfall called a truce. On the third day the Arabs received reinforccments, and towards the evening the Persians attempted to bring the struggle to an end by a charge of elephants. At first the huge beasts carried all before them; then one was wounded painfully and became uncontrollable, rushing up and down between the armies. Its panic affected the others, and for a time both armies remained dumbfounded in the red light of sunset, watching the frantic efforts of these grey, squealing monsters to escape from the tormenting masses of armed men that hemmed them in. It was by the merest chance that at last they broke through the Persian and not through the Arab army, and that it was the Arabs who were able to charge home upon the resulting confusion. The twilight darkened to night, but this time the armies did not separate. All through the night the Arabs smote in the name of Allah, and pressed upon the shattered and retreating Persians. Dawn broke upon the vestiges of Rustan\'s army in flight far beyond the litter of the battlefield. Its path was marked by scattered weapons and war material, abandoned transport, and the dead and dying. The platform and the golden throne were broken down, and Rustan lay dead among a heap of dead men.\nAlready in 634 Abu Bekr had died and given place to Omar, the Prophet\'s brother-in-law, as Caliph; and it was under Omar (634–643) that the main conquests of the Moslems occurred. The Byzantine Empire was pushed out of Syria altogether. But at the Taurus Mountains the Moslem thrust was held. Armenia was overrun, all Mesopotamia was conquered and Persia beyond the rivers.\nEgypt passed almost passively from Greek to Arab; in a few years the Semitic race, in the name of God and His Prophet had recovered nearly all the dominions it had lost to the Aryan Persians a thousand years before. Jerusalem fell early, making a treaty without standing siege, and so the True Cross which had been carried off by the Persians a dozen years before, and elaborately restored by Heraelius, passed once more out of the rule of Christians. But it was still in Christian hands; the Christians were to be tolerated, paying only a poll tax; and all the churches and all the relics were left in their possession.\nJerusalem made a peculiar condition for its surrender. The city would give itself only to the Caliph Omar in per-son. Hitherto he had been in Medina organizing armies and controlling the general campaign. He came to Jerusalem (638), and the manner of his coming shows how swiftly the vigour and simplicity of the first Moslem onset was being sapped by success. He came the six-hundred-mile journey with only one attendant; he was mounted on a camel, and a bag of barley, another of dates, a water-skin, and a wooden platter were his provision for the journey. He was met outside the city by his chief captains, robed splendidly in silks and with richly caparisoned horses. At this amazing sight the old man was overcome with rage. He slipped down from his saddle, scrabbled up dirt and stones with his hands, and pelted these fine gentlemen, shouting abuse. What was this insult? What did this finery mean ? Where were his warriors? Where were the desert men ? He would not let these popinjays escort him. He went on with his attendant, and the smart Emirs rode off well out of range of his stones. He met the Patriarch of Jerusalem, who had apparently taken over the city from its Byzantine rulers, alone. With the Patriarch 1 he got on very well. They went round the Holy Places together, and Omar, now a little appeased, made sly jokes at the expense of his too magnificent followers.\nEqually indicative of the tendencies of the time is Omar\'s letter ordering one of his governors who had built himself a palace at Kufa, to demolish it again.\n""They tell me,"" he wrote, ""you would imitate the palace of Chosroes,1 and that you would even use the gates that once were his. Will you also have guards and porters at those gates, as Chosroes had? Will you keep the faithful afar off and deny audience to the poor ? Would you depart from the custom of our Prophet, and be as magnificent as those Persian emperors, and descend to hell even as they have done ?""']"	['<urn:uuid:34a650ad-1720-4bb4-9a72-342e9b7b473f>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T05:28:56.360010	12	73	1915
18	risk skin cancer increase after using puva treatment many sessions	After more than 200 PUVA treatments or exposure exceeding 2000J/cm2, there is a 14-times increased risk of developing cutaneous SCC. There is also evidence of increased melanoma risk in these PUVA patients. PUVA lentigines can indicate higher risk of PUVA-related skin cancers. Additionally, having more than 200 phototherapy treatments of any kind is a contraindication for phototherapy in Biologics work up.	"[""Flashcards in NMSC Dan Deck (89)\nWhat are high risk features of SCC?\nBreslow thickness >2mm (esp >6mm) OR goes to fat\nPNI (in nerve >0.1mm diameter)\nSite; ear or nonhair-bearing lip\nPoorly differentiated or undifferentiated\nSCC arising in burn, XRT field or chronic dermatosis\nImmunosuppression - esp solid organ Tx or CLL\nNon-sun exposed site (e.g. sole of foot)\nAggressive subtypes (e.g. acantholytic, adenosquamous, spindle or especially desmoplastic)\nWhat are the guidelines for treating SCC by C+C?\nCan be used for low-risk tumours with three caveats:\n- Not on terminal hair baring areas\n- If subcutaneous layer is reached then surgical excision should be performed instead\n- Histology of the C+C should be performed to determine if still low-risk and if not then should be excised\n96% 5 year cure rate if these adhered to\nWhat are the recommended excision argins for NMSC?\nIEC - 3-5mm\nBCC - 4mm; 5-10mm if morphoeic or recurrent but Mohs preferred for these\nSCC - 4-6mm if low risk, 6-10mm or Mohs if high risk\nKA - 3-5mm margin\nMerkel Cell - 3cm (Robinson); others say 1-2cm\nAFX - 1-2cm; Mohs if large or on head/neck\nDFSP - 1.5-2cm if 2cm diameter (Rob/Rook); 2-4cm if >2cm diameter or Mohs\nMAC - Mohs recommended, adjuvant XRT if exc incomplete\nSebaceous carcinoma- 1-2cm; Mohs if large or on head/neck\nEMPD - Mohs first line, WLE with 5cm margin +/- mapping biopsies or preop efudix to delienate\nWhen may SLNB be considered for SCC?\nNot clear if of benefit or not\nIf AJCC-7 stage T2;\nover 2cm diameter OR any size with ≥2 specific high risk features;\n- Breslow thickness >2mm OR goes to fat (Clark ≥4)\n- PNI (in nerve >0.1mm diameter)\n- Site; ear or nonhair-bearing lip (vermillion)\n- Poorly differentiated or undifferentiated\nWhat is risk of skin cancer from PUVA?\n14x increased risk of cutaneous SCC in patients who have received >200 PUVA treatments or >2000J /cm2\nAlso evidence of increased risk for melanoma in these PUVA patients\nPUVA lentigines indicate higher risk of PUVA-related skin cancers\n>200 phototherapy treatments (of any kind) is PBS contraindication for phototherapy in Biologics work up\nNo evidence that UVB phototherapy increases skin cancer but probably does\nWhat are treatments for individual AKs?\nspot treat peel e.g. TCA 35-50%\nWhat are treatments for widespread AKs/field change?\n5% fluorouracil cream (Efudix); max 23x23cm area or 5-FU chemowraps for legs (as per AJD, 2013)\n5% Immiquimod cream (Aldara); max 5x5cm area\n-3x/wk 4 wks on, 4 wks off +/- further 4 wks on Or;\n-3x/wk for 4-16 wks\n3% Diclofenac gel (Solaraze); 0.5g per 5x5cm area, max 8g per day\n0.05% Treinoin cream (ReTrieve)\nIngenol Mebutate gel (Picato); max 5x5cm area\n- Face and scalp: 0.015% gel once daily for 2-3 consecutive days\n- Body: 0.05% gel once daily for 2 consecutive days\nSuperficial chemical peels (need a course)\nMedium depth chemical peel\n- TCA 35%-50%\n- Jessner’s solution + 35% TCA\n- 70% glycolic acid + 35% TCA\nPDT - single session per region\nDaylight PDT (not if hyperkeratotic; may rpt in 3 months)\nWhat methods are available for prevention of AKs and skin cancers?\n0.05% Treinoin cream (ReTrieve) nocte\nAcitretin at least 20-30mg daily or 0.2mg/kg\nNicotinamide 500mg BD\nNicotinamide 500mg BD reduces the incidence of new BCCs\nreduced AK, SCC and BCC\nsee effect from 2 mnths, further increased effectiveness until at least 12 mnths\nWhen do SCCs metastasize?\nHighest risk is in first 2 years\nFor high risk SCCs;\n- consider 6 monthly FSE for 1st 2 years\n- annual FSE thereafter\npalmoplantar and periungual SCC associated with HPV 5\nHPV 5 is associated with SCC in Epidermodysplasia verruciformis\nWhat is the risk of IEC progressing to SCC?\n3-5% for normal skin IEC\n10% for Erythroplasia of Queyrat\nVery low risk for Bowenoid papulosis\nIt may be reasonable to not treat IEC\nElderly patients with slowly progressive thin lesions esp on lower legs\nUse emollient containing LA/SA or urea to reduce scaling and make less obvious\nHow long do you use efudix cream for IECs?\nOD-BD for 4 wks face, 6wks limbs\nfor IEC, efudix is less effective than PDT\nalso less than immiquimod which is similar to PDT for IEC\nEfudix similar efficacy to cryo\nfor sfBCC efudix is less effective than PDT\nImmiquimod is non-surgical Rx of choice for sfBCC\nfor IEC, imiquimod is more effective than PDT\nthe same or slightly less than PDT\nBut for sfBCC, imiquimod is more effective than PDT\nPDT is an effective and suitable choice for IEC anywhere\nLimited by availability, cost and pain\nexcison of IEC should be avoided on the digits\ngood Rx choice on digits\nHow is IEC of nailbed treated?\nMohs is 1st line\nPDT reasonable choice\nCan try aldara, laser or XRT\narsenic exposure can cause a corn-like PPK\ncan turn into SCC\nRx with keratolytics and chemoprevention\nmonitor for skin Ca and visceral malignancy\nDSAP is inherited in a recessive fasion\nDSAP1 gene on chr 12\nWhat are treatments for DSAP?\n• 5-FU (also chemo wraps)\n• topical retinoids\n• Oral retinoids\n• Ablative lasers\nKeratoacathomas are less common in darker skin\nWhich syndromes get KAs?\nMuir-Torre (KA w/ sebaceous apearance)\nMultiple spontaneously regressing epitheliomas of Ferguson-Smith\nGeneralised eruptive keratoacanthomas of Grzybowski\nWhat are the features of\nMultiple spontaneously regressing epitheliomas of Ferguson-Smith?\nmultiple KAs in sun exposed areas in 20s\nregress over weeks to months\nWhat are the features of\nGeneralised eruptive keratoacanthomas of Grzybowski?\nThousands of papules resembling milia or early eruptive xanthomas; develop rapidly and persist indefinitely\nLesions inside oral mucosa\nMask like facies from skin thickening described\nLesion benign but persist indefinitely\nWhats the natural Hx of KA?\nreaches full size\nWhat are variants of KA?\nSolitary/craterfiorm (normal type)\nErruptive/Syndromic - Muir-Torre, F-S, Gryzbowski\nlocally destructive types;\n- Mutilating KA\n- Aggregated KA\n- KA centrigugum marginatum\nWhat are treatment options for KA?\nCryo - if small\nAblative laser - if small\nAcitretin if numerous\nLow dose XRT\nWhat are associations of Verrucous Carcinoma?\nLS if genital (Giant condyloma of Buschke-Lowenstein)\nleukoplakia, smoking if oral\nstrong assoc w HPV infection; 6 and 11\nXRT should be used for verrucous carcinoma\nDo not treat with XRT – triggers anaplastic change and rapid growth and invasion\nOral retinoids may be useful\nWhat is Epithelioma cuniculatum?\nRare tumour on sole of foot – variant of SCC\nDischarges foul smelling keratin debris\nComprised of sinus-like tracts\nOften infiltrate bone and overlyng nailbed\nHisto looks like pseudoepitheliomatous hyperplasia but lesion is progressive and destructive\nOther than treating BCCs what is the management of Gorlin's syndrome?\nStrict sun protection\nBaseline MRI brain and repeat yearly till 8y for medualloblastoma (ave age to develop is 2yrs; Mx by neurologist)\n6 monthly skin exams\ndigital panorex of jaw at 3-4y and repeat yearly till 21y (Mx by Maxfax surgeon)\npelvic USS of female at menarche (paeds)\nscoliosis assessment (ortho)\ndevelopmental screening (paeds)\nprenatal counselling (genetics)\nHow are BCCs maanged in Gorlin's?\nStrict sun protection + avoid XRT\n6 monthly skin exams\nBCC ‘s on central face - excision best\nuse non surgical methods to Rx BCC’s elsewhere;\nC+C’s/ Cryo/ PDT/ aldara/ 5 FU\nrole of retinoids unclear (isotretinoin)\nSmoothened receptor inhibitor\nOff label use for Gorlins; NEJM 2012 study\n93% reduction in new BCCs over 8 months, but 50% had to discontinue due to AEs (muscle spasms, alopecia, dysgeusia (altered taste), decreased appetite, wt loss, fatigue, nausea/vomiting, diarrhoea, constipation)\nchronic occupational sun exposure is the main environmental risk factor for BCCs\nintermittent intense esxposure more important for BCCs\nWhat are the high risk subtypes of BCC?\nWhat is Basisquamous (or Basosquamous) BCC?\nFeatures of both BCC and SCC on histo\naggressive subtype of BCC\n5% risk of mets\nmust completely excise\nWhich BCCs are not suitable for curretage (unless as palliative procedure)?\nLarge tumours (≥2 cm diameter)\nTumours at sites where curettage produces a poor cosmetic result, is technically\ndifficult or is associated with a high risk of recurrence (most of face and scalp off limits except mid cheek)\nMorphoeic, infiltrating or basisquamous BCCs\nTumours penetrating muscle, fat, bone, etc.\nTumours where an incisional biopsy has been performed\nWhat are the indications for Mohs surgery on BCCs?\nhigh risk sites = mask area on face / scalp/ anatomic fusion planes/ periorbital or eyelid\ntumour > 2cm\naggressive histological subtypes\nincompletely exc BCC\nPreviously RXT Rx area\nsituations requiring conservation of normal tissue or highest probability of cure\nsome SCCs can be treated by C+C\nIf well-differentiated, primary, slowgrowing SCC arising on sun-exposed sites and no high risk factors in tumour or patient\n>95% cure rate\nesp good for older patients\nHow does tumour thickness impact on SCC risk?\nless than 2mm virtually no risk of mets\n2-4mm some risk\n6mm thick or more = highest risk of mets\nWhich dermatoses predispose to NMSC?\nOcculocutaneous albinism - SCC\nXP - those SCC\nChronic non-healing wounds (Morjolin ulcer)\nLichen planus (erosive)\nSkin fistulae or discharging sinuses\nHS esp perianal\nWhat is the risk of SCC arising from AK?\nless than 0.1% per lesion per year\nWhat are the drawbacks of treating BCC with LN2?\nhistology not possible\nfibrous scar tissue may obscure recurrence\nBut 99% cure rate for DFTC\nBCCs with involved margins always need re-excision\nIf low rsik type in low risk site and involved lateral margin on direct side-to-side closure can observe\nIn other cases Mohs best\nPhotodynamic therapy has a 14% better chance of complete lesion clearance at 3 months after treatment than cryotherapy for thin AKs on the face and scalp\nJAMA Derm meta-analysis 2015\nHPV 16 is the most common type found in\npatients with penile cancer, followed by HPV types 18,\n6, and 11\nHPV found in 50-63% of penile SCC\nJAAD CME 2015\n75% of penile cancers are SCC\nJAAD CME 2015\nQLD has highest incidence of both SCC and BCC in Aus\nand Aus has highest rates in the world\nPerineural invasion occurs is seen in 10% of NMSC\nWhat is incidental PNI?\nWhen there are no clinical signs or symptoms or radiological evidence of PNI or perineural spread\nWhat are the histological high risk factors in PNI?\nMultifocal: >1 focus of PNI seen\nInvolves nerve of diameter >1mm (1mm and under is lower risk)\nExtratumoral PNI (implies possible perineural spread)\nWhen can clinical perineural invasion be diagnosed?\ndefined as one of the following;\nDysaesthesia (formication) and positive imaging for PNI\nFacial nerve palsy in setting of a cutaneous tumour w/ or w/out evidence of PNI on histo\nCranial or spinal nerve involved on histo\nConsider recurrence due to PNI if formication occurs at or near site of skin cancer excision\nCan do MRNeurography to assess\nWhat are complications of SLNB?\nallergic reactions to the blue dye used intraoperatively\ncutaneous lymphatic fistula\nWhat stain differentiates merkel cell carcinoma from metastatic deposit of small cell lung carcinoma?\n- positive in MCC, negative if SSLC\nWhat IHC stains are posiitve in Merkel cell carcinoma?\nMerkel Cell Carcinoma is a rare highly aggressive primary cutaneous carcinoma of skin with epithelial and endocrine features\nderived from skin mechanorceptor Merkel cells\nneuroendocrine in origin\nWhat are risk factors for Merkel Cell Carcinoma?\nsolar damage esp in fiar skin\nOlder age (>50)\nWhat are clues to the diagnosis of Merkel Cell Carcinoma?\nAsymptomatic/lack of tenderness\nOver 50 years\nUltraviolet-exposed site on a person with fair skin\nMost Merkel Cell Carcinomas occur on the trunk\nMost on head and neck - 53%\nextremities - 35%\nTrunk, oral and genital mucosa\n5-yr survival of Merkel Cell Carcinoma is approximately 50%\n10-yr survival of Merkel Cell Carcinoma is approximately 50%\nWhat are poor prognostic histo features of Merkel Cell Carcinoma?\nDense mononuclear inflammatory cell infiltrate\nDeep extension (esp into subcutis)\nDiffuse growth pattern including lymphovascular invasion\nWhat are poor prognostic clinical features of Merkel Cell Carcinoma?\nhead and neck site\ntumour Diameter over 5cm (stage T3)\nLymph node status is the most important independent predictor\nHow are Merkel Cell Carcinomas staged?\nshould always stage tumour\n5 yr survival 80% if stage 1A (tumour up to 2cm max diameter)\nHow is Merkel Cell Carcinoma managed?\nDiagnosis of MCC should prompt FSE and clinical palpation of regional LNs for nodal involvement\nUsually referred to plastic/soft tissue surgeon\nSLNB for all lesions at time of WLE\n- margin recommendations vary; 3cm (Robinson); others say 1-2cm\n- LN dissection if SLNB +ve\nPET CT for staging\nOffer Adjuvant XRT to tumour region – reduced local relapse but not improved mortality\nConsider adjuvant chemo if LN +ve or metastatic – but no proven benefit and signif morbidity\nClose folow up and consider PET-CT every year for 5yrs\nHow is DFSP managed?\nWLE - 1.5-2cm if 2cm diameter (Rob/Rook); 2-4cm if >2cm diameter or Mohs\nStaged excision with margin control (Slow Moh’s) is an option\nPost op radiotherapy may reduce recurrence\nCan use imatinib (Gleevac) if unresectable/metastatic – some data to support use as adjuvant\nDFSP has local recurrence rate of 15-60%\nMets to LNs or organs rare\nexcept for fibrosarcomatous variant (herring bone pattern); Presence and amount of fibrosarcomatous areas relates directly to recurrence rate and metastatic potential\nAFX only occurs in sun damaged skin of older people\nAFX is a diagnosis of exclusion\nStains should be done in all cases;\nsarcomatous SCC (pan-keratin; MNF116 or AE1/AE3)\nleimyosarcoma (desmin, SMA)\nCD10 and CD99 usually +ve in AFX\nAlso +ve for Vimentin, CD31, CD34, CD68\nDermatofibroma is always negative for CD34\nMay be focally positive for CD34 esp at periphery\nWhich subtypes of Dermatofibroma are classes as high risk?\nCellular(25%), Atypical(14%) and Aneurysmal(19%)\nCellular and Atypical\nthe 4th main variant is Epitheloid fibrous histiocytoma\nWhat are treatment options for Kaposis Sarcoma?\nIf HIV/AIDS-related best treated with HAART\nOrgan transplant KS – Reduce immunosuppression, substitute sirolimus for CsA can lead to improvement\nEOL, XRT, LN cryo, CO2 or PDLaser, PDT, 5FU, aldara, topical alitretinoin\nFor individual lesions - intralesional vincristine, cisplatin or bleomycin +/- prior diathermy\nXRT for multifocal disease\nChemotherapy (doxorubicin, vincristine, etoposide, bleomycin) or IFN alpha indicated if; Rapidly progressive/ pulmonary KS/ symptomatic visceral involvement/ lymphoedema\nSystemic IL-2 or Bexarotene also reported\nAngiogenesis inhibitors; thalidomide, COL3, antiVEGF\nAngiosarcoma can arise as a complication of lymphoedema\nT - classical presentation\nKnown as Stewart-Treves syndrome esp if arises after radical mastectomy or XRT for Ca breast\n4 types of Angiosarcoma - PILL;\n- Post XRT\n- Idiopathic (esp face of old men)\n- Lymphoedema associated\n- arising in Lymphatic or vascular malformation\nWhat is the management of Angiosarcoma?\nWLE + adjuvant XRT\nhigh risk of both recurrence and mets\nDoxorubicin chemo for palliation if unresectable mets\nWhat is an AFX if it extends into fat?\nUndifferentiated pleomorphic sarcoma\nconnective tissue lesion that needs to be dealt with by specialist surgeon/oncologist; worse prognosis\nWhat is Gryzbowski syndrome?\ngeneralised eruptive keratoacanthomas\nsudden onset acquired condition cause unknown\nyoung-mid aged adults, M=F\nsmall widespread KAs - benign behaviour\nB/g widespread erythematous rash\nscleroderma and ectropion of face\nwhite papules in mouth and nodules on larynx causing hoarseness\nResolves in months if treated;Acitretin, isotretinoin\nwhat is the risk of mets from an SCC?\nwhat is the risk of recurrence of an SCC?\noverall risk 3%\nEfudix has the best evidence of all topicals for field AK treatment\nImmiquimod has good long term clearance rates as thought to cause induction of specific memory T cells that maintain immune surveillance against AKs\nEfudix, immiquimod, picato and solaraze all have similar efficacy\nBut elsewhere JAAD rw states solaraze may have lower efficacy than other agents\nWhat non surgical options are available for sfBCC?\nWhich is best?\nXRT - good cure rates but risk of aggressive recurrence\nAldara has 80% CR confirmed in long term studies\naldara and efudix are effective for erythroplasia of Queyrat\nBoth have shown some effect in case reports but complete excsion is treatment of choice\nIf topicals used need close follow up as high rates of recurrence\nWhat is the genetic aberration responsible for DFSP?\nChr 17;22 translocation\nresults in ring chromasome on FISH (can also identify on PCR)\nInvolved genes are collagen type 1 alpha 1 (COL1A1) gene on chromosome 17 and platelet derived growth factor B (PDGFB) gene on chr 22\nBasis for Imatinib Rx (PDGF receptor inhibitor)\nAFX recurs in 25%\nWhat pts are at risk of multiple DFSPs?\npts with SCID due to adenosine deaminase deficiency""]"	['<urn:uuid:5e60e16a-2be5-4d28-b395-8cdc2139861e>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T05:28:56.360010	10	61	2732
19	threshold nickel concentration birch tree survival	The threshold nickel concentration with respect to both mortality and recruitment of mountain birch is 160 mg/kg in leaves. Birch recruitment was only observed in areas where nickel concentrations in leaves were below this level.	['ISSN 1067-4136, Russian Journal of Ecology, 2009, Vol. 40, No. 4, pp. 254–260. © Pleiades Publishing, Ltd., 2009.\nOriginal Russian Text © V.E. Zverev, 2009, published in Ekologiya, 2009, No. 4, pp. 271–277.\nAtmospheric emissions from the nonferrous metal\nindustry cause serious alterations in terrestrial ecosys-\ntems, including their degradation into industrial barrens\n(Kozlov and Zvereva, 2007). In economically devel-\noped countries, general reduction of industrial emis-\nsions was achieved 10–20 years ago due to environ-\nmental law enforcement and shutdown of some indus-\ntries. In Russia, industrial emissions of sulfur dioxide\nand heavy metals have also been reduced during the\npast 10–15 years (\nwhich gives hope for alleviation of degradation and\nsubsequent recovery of disturbed ecosystems.\nThe question arises as to whether rapid ecosystem\nrecovery can be expected soon after the reduction of\nemissions or this process will take decades or even cen-\nturies unless appropriate measures are taken. For exam-\nple, some specialists regard forest degradation near\nMonchegorsk as irreversible, at least on a decennial\nscale (Tsvetkov, 1991).\nThe concept of slow community recovery has pro-\nvided a basis for many rehabilitation projects, including\nthe well-known project implemented near Sudbury,\nRestoration and Recovery…\n, 1995). Neverthe-\nless, natural recovery of heavily polluted habitats some-\ntimes begins after only 5–15 years. For example, insec-\ntivorous birds returned to their former nesting sites near\nthe copper–nickel smelter in Harjavalta, Finland,\n7 years after emissions from this plant were reduced\n(Eeva and Lehikoinen, 2000). Obvious signs of forest\necosystem recovery in the Krusne Hory Mountains,\nCzech Republic, were revealed 16 years after consider-\nable reduction of sulfur dioxide emissions in the region\n(Cerny, 1995), and plant species richness in an indus-\ntrial barren near Karabash (Chelyabinsk oblast) proved\nto increase signiﬁcantly within 5 years after copper\nsmelter shutdown (Chernen’kova et al., 2001).\nUnfortunately, information on natural (without\nhuman interference) recovery of terrestrial ecosystems\ndisturbed by long-term industrial pollution is scarce: on\nthe one hand, situations involving a signiﬁcant decrease\nin emissions within a relatively short time are rare; on\nthe other hand, long-term monitoring is necessary for\nobtaining such information (Hawkins et al., 2002).\nMoreover, such monitoring should be initiated prior to\nthe onset of emission reduction, which is not always\npossible to plan beforehand. The recovery of lands pol-\nluted with heavy metals is a very slow process, since the\nleaching of pollutants from the soil may take several\ndecades or even centuries (Tyler, 1978; Barcan, 2002a).\nHence, the concept was formulated that degradation of\nMortality and Recruitment of Mountain Birch\n) in the Impact Zone\nof a Copper–Nickel Smelter in the Period of Significant\nReduction of Emissions: The Results of 15-Year Monitoring\nV. E. Zverev\nUniversity of Turku, FI-20014 Turku, Finland\nReceived May 7, 2008\n—Long-term monitoring of mountain birch populations (1992–2006) was performed in 14 test plots\nlocated at distances of 1 to 63 km from the copper–nickel smelter in Monchegorsk (Murmansk oblast) and dif-\nfering in the degree of disturbance. In the period from 1999 to 2006, atmospheric emissions of sulfur dioxide\nand heavy metals amounted to only one-third of those between 1992 and 1998, but birch mortality in heavily\npolluted areas (with nickel concentrations in leaves exceeding 160 mg/kg) remained at the same level, being\nabsent (as previously) in less polluted areas. Throughout the observation period, birch recruitment was observed\nonly in areas where nickel concentrations in the leaves were below 160 mg/kg; i.e., this concentration proved\nto be the threshold with respect to both mortality and recruitment of mountain birch. The course of demographic\nprocesses in its populations has remained unchanged after the reduction of emissions, conﬁrming the hypothe-\nsis of the “inertial” effect of industrial emissions on ecosystems. In some areas of industrial barrens, mountain\nbirch may perish completely within the next decade.\n: heavy metals, mountain birch, mortality, recruitment, industrial pollution, northern taiga.']	['<urn:uuid:89fc03b6-17c4-4011-91a9-f17477b148b2>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-13T05:28:56.360010	6	35	641
20	which company older aircraft parts manufacturer superior air parts founded 1967 or lycoming foundry machine founded 1845	Lycoming Foundry and Machine Company is the older manufacturer, having been founded in 1845 as the Demorest Manufacturing Company. Superior Air Parts was founded much later in 1967 by Burt 'Inky' Dedmon. While Superior Air Parts has been in business for over 50 years, Lycoming has been operating for over 175 years.	"['Superior Air Parts Celebrates 50 Years of ""Making Flying More Affordable"" During Oshkosh AirVenture 2017\nKeith Chatten, CEO of Superior Air Parts, Inc., announces that the company will kickoff its 50th anniversary celebration at this year’s Oshkosh AirVenture.\n“The 50th anniversary of any organization is big, but for Superior to achieve this milestone in today’s general aviation market is especially meaningful for everyone in our organization,” he says. “I would like to take this opportunity to thank every one of our suppliers, distributors and customers around the world for making our 50th anniversary possible.\n“Today, our tagline is ‘Making flying more affordable since 1967,’ and that commitment means a lot to us. Our customers have come to rely on us to bring some competitive cost control to the aftermarket parts business, and we take that role very seriously,” Chatten says. “Considering we started by making a simple valve guide in 1967 and have grown to hold 3,500 PMAs today, it’s clear that aircraft owner/operators around the world appreciate the pride we put into every product that carries the Superior Air Parts name.\n“We invite everyone to come to our tent (#257) during AirVenture to see what Superior is all about and attend any of our daily educational forums,” he says.\nSuperior Air Parts timeline:\n• 1967 – Company founded by Burt “Inky” Dedmon\n• 1967 – Shipped first FAA PMA valve guide for piston aircraft engine\n• 1991 – Introduced the Millennium Cylinder®\n• 2001 – Introduced the Millennium Generation II®\n• 2001 – Received the final FAA PMA needed to assemble an entire, four-cylinder aircraft engine entirely from FAA-approved PMA parts\n• 2001 – Introduced the XP-Engine™ Series kit engine for experimental aircraft\n• 2002 – Held first XP-Engine Build School™ at Coppell, TX, headquarters\n• 2004 – Received FAA type certificate for the 180-horsepower Vantage Engine™\n• 2005 – Vantage Engine FAA-certified on American Champion High Country Explorer\n• 2006 – Introduced the 215-horsepower XP-400™ Engine\n• 2015 – Introduced the 200-horsepower XP-382™ Engine\n• 2015 – Received FAA STC to install the Vantage Engine on Cessna 172 (R&S) aircraft\n• 2016 – Introduced series of educational forums at Oshkosh/AirVenture\n• 2017 – Received 3,499th and 3,500th FAA PMAs\n• 2017 – Sold the 2,300th XP-Engine kit\n• 2017 – Millennium Cylinder® shipments surpassed 230,000 worldwide\n• 2017 – Published Engine Management 101 by Bill Ross\n• July 25, 2017 – Celebrating Superior Air Parts’ 50th Anniversary at Oshkosh/AirVenture\nAs part of the 50th anniversary celebration, Superior Air Parts will be giving everyone who attends one of the company’s educational forums a free copy of “Engine Management 101,” written by Superior’s VP of Product Support and noted piston-engine maintenance and operations expert, Bill Ross.\nThe free, 45-minute forum sessions will be held at the Superior Air Parts tent (#257), just north of the Bendix/King Hangar B. Forum times are 9:00, 11:00 and 12:30, Monday through Friday.\n“Superior has been built on the simple goal of producing engine parts that deliver unbeatable performance at a reasonable price,” adds Scott Hayes, vice president, Sales and Marketing, Superior Air Parts. “Publishing Engine Management 101 is another way we’re continuing that commitment. Too many owners just don’t understand what they can do to get more life out of their engines. Bill’s book will give them a lot of valuable and proven insights and help them get to TBO at the lowest cost.”', 'Decades of pioneering spirit\nIt began in 1845 as the Demorest Manufacturing Company producing sewing machines and bicycles in Williamsport, PA. The machinery always hinted greater things were to come. Becoming Lycoming Foundry and Machine Company in 1907, Lycoming developed automobile engines in a market driven by the needs of a nation at war.\nInspired by Charles Lindberg trans-Atlantic flight, Lycoming began developing aviation aircraft engines. Starting in 1929 aviation would never be the same. The first Lycoming R-680, a 9-cylinder 200 hp piston-driven radial engine left the assembly line and established a new standard in general aviation. Our piston engines continue to evolve today as we develop advanced technologies to remain the power behind general aviation aircraft.\nInnovating for over 85 years\nDemorest Manufacturing Company founded\nMadame Ellen Curtis Demorest founded the Demorest Manufacturing Company. Over the next 60 years, Demorest produced sewing machines, bicycles, typewriters, duplicators, gas irons, and printing presses.\nCompany becomesLycoming Foundry and Machine\nThe company earned its manufacturing stripes through hands-on experience driven by the needs of a nation at war.\nFirst automobile engine produced\nProduces first automobile engine for Velie Motor Corporation. Over 250 automobile engine models followed for car manufacturers such as Cord, Auburn, Duesenberg, Locomobile, Paige, Graham, McFarlan and Checker.\nBegins powering military vehicles\nDuring World War I, Lycoming produces 15,000 engines to power military trucks and ambulances.\nErrett L. Cord purchases company\nErrett L. Cord purchased the company, and expanded the business to include designing and manufacturing marine racing engines, industrial motors, as well as venturing into the aviation aircraft engine arena.\nBuilds first aircraft engine\nThe Lycoming factory developed its first aircraft engine: the R-680 nine-cylinder radial. On April 3, 1929, a Beech-designed TravelAir biplane was the first aircraft to feature the R-680 on successful trial flights. The engine powered the nation\'s earliest airlines. In over 20 years almost 25,000 R-680s were produced. Thousands of R-680s flew with the armed services before and during World War II.\nLycoming becomes part of Aviation Corporation\nLycoming becomes part of Aviation Corporation, later known as AVCO.\nIgor Sikorsky flies O-145 powered helicopter\nLycoming develops the O-145 opposed cylinder engine. Igor Sikorsky flew the first successful helicopter powered by a 65-horsepower Lycoming GO-145. To this day, Lycoming produces the world\'s only certified piston helicopter engines.\nReceives Army Air Corps E-Award\nFor its assistance with the World War II production effort, the Army Air Corps presented Lycoming with the coveted E-Award for its outstanding contribution to the welfare and security of the United States.\nLycoming-powered Sentinel lands on Iwo Jima\nA Lycoming-powered Stinson ""Sentinel"" liaison plane was the first allied plane to land on Iwo Jima.\nDevelops aerobatic engine\nLycoming developed the first and only FAA-certified aerobatic piston engine.\nBuilds 250,000th piston engine\nOn May 27th, Lycoming reached a production milestone for producing their 250,000th piston engine.\nTextron purchases AVCO\nTextron purchased AVCO, which included AVCO Lycoming.\nRobinson Certifies R44 Helicopter\nRobinson Helicopter Company certified the R44 helicopter with a Lycoming O-540- F1B5\nBuilds 300,000th piston aircraft engine\nLycoming delivers its 300,000th horizontally opposed, piston aircraft engine.\nCelebrates 75th anniversary\nLycoming opens engine museum at company HQ in honor of the 75th anniversary of its first aircraft engine.\nReaches 1 million hours without a lost-time accident\nLycoming achieves 1,000,000 hours of operation without a lost-time accident.\nLaunches ATC and Thunderbolt Engine line\nLycoming launches the Advanced Technology Center (ATC), which is later exemplified for its dedication to innovation and excellence in research and development, by the Thunderbolt Engine, the company’s custom built engine line.\nLycoming receives AS 9100 Certification\nLycoming becomes a AS9100 registered company for meeting the standards, including ISO 9001:2008 and AS 9100:2009 requirements. Lycoming maintains this certification today.\nAnnounces iE2 Integrated Electronic Engine\nOn July 28th, at EAA AirVenture in Oshkosh, Lycoming introduces the iE2 Integrated Electronic Engine and other engine milestones.\nReceives Occupational Excellence Award\nLycoming receives National Safety Council\'s Occupational Excellence Achievement Award for outstanding safety record.\nAwarded Shingo Silver Medallion\nWe are honored with the Shingo Silver Medallion for Operational Excellence at the 21st Annual Shingo Prize Conference in Nashville, Tenn.\nLaunches in-house piston line\nLycoming celebrates the launch of our in-house piston line. The robotic manufacturing process of the line, complete with automated in-line quality checks, makes it a one-of- a-kind capability in piston aviation.\nAwarded Shingo Prize\nLycoming earns the Shingo Prize for Operational Excellence, referred to as the ""Nobel Prize for Manufacturing."" Lycoming became just the second business ever given the prize at the business-wide ""Enterprise Level,"" recognizing the operational excellence Lycoming has achieved throughout its organization.\nReceives Healthy Workplace Award\nLycoming wins Healthy Workplace of the Year Award for our initiatives to encourage the health and wellness of our employees through our wellness program.\nFlight test of Lycoming-powered New Firebird\nThe New Firebird features full capabilities envisioned in Lycoming\'s iE2 engine.\nPowering the next Goodyear airship\nGoodyear begins building their next airship, the NT Zeppelin, that is powered by three Lycoming IO-360 engines.\nSelected to power 2014 Red Bull Air Races\nThe 2014 Red Bull Air Race World Championship chose Lycoming to power each airplane with a Thunderbolt Engine that is standardized to Red Bull Air Race\'s specifications.\nCelebration of 85 years in aviation innovation\nMarking Lycoming’s 85th year in aviation innovation, the company celebrates this milestone through various activities and events.\nLycoming Engines Project Cited as an Aviation Climate Solution\nLycoming received this recognition for its ion nitriding project, which enabled the company to significantly reduce its environmental impact versus the previous process.\nLycoming Awarded “Most Popular Piston Engine OEM” in China\nThe total number of Lycoming engines in China reached 743, nearly 200 more than the other top four OEMs combined.\nA family built upon the legacy\n21 Years AVERAGE EMPLOYMENT RATE\nGenerations after generations grow the Lycoming family, and we owe our success to our dedicated employees.\nWith a 21-year average employment rate, it is known we invest in our employees. We develop within, recruit the best and brightest talent, and maintain the highest standard of U.S. manufacturing and employment.\nMeet Our Team\nGet to know our employees on a more personal level.View People\nOur Manufacturing Process\nTo understand today’s Lycoming Engines, learn how we build them.View Factory']"	['<urn:uuid:fcade8a9-f160-4df5-a2f6-f8f29ff46af5>', '<urn:uuid:ad80dbbc-dc32-4d79-97f8-ef66b588ed04>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-13T05:28:56.360010	17	52	1606
21	I'm helping my friend move his online store to a different web host. Can you tell me about both the technical challenges of previewing the site and the security requirements we should check with the new provider?	For previewing the site, there are technical limitations - temporary URLs are not recommended for sites with strict URL configurations like WordPress or PrestaShop, as they may cause redirections to the previous host or display issues. On the security side, the new provider must demonstrate compliance with data privacy standards through third-party audits, implement the UK Top 20 Critical Security Controls or ISO 27001, and ensure proper data encryption during transfer and storage. They should also have contracts with any third-party suppliers guaranteeing compliance with privacy standards.	['Transferring a website to a new web hosting provider might be a challenge, since the site ownercannot have a 100% clear picture of how the site will fit into the specifics of the new server before the DNS propagation is complete.\nLuckily, there are a few tricks that can help users get a clear idea of how the newly transferred content will load from the new server after the DNS propagation has been completed.\n1. Use the temporary host assigned to a hosting account\nAt signup, users are assigned a fully functioning temporary host, which they can use to preview their site. All they need to do is transfer their site contents to the host’s folder and then see how the site will load from the new location by using its URL.\nNOTE: Temporary URLs are not recommended for previewing sites that use a strict URL configuration (including WordPress, PrestaShop, osCommerce, etc. sites), since they may force a URL rewrite that goes back to the original domain name, which may result in a redirection to your previous host, or in the site not displaying at all.\n2. Use the local host of a computer\nUsers can preview their website with their own domain name before the DNS propagation has been completed by using their local computer’s HOSTS file.\nWith a simple HOSTS file tweak, their computer will resolve the site locally prior to looking up the domain’s DNS records.\nHere is how to use your computer’s HOST file on the two common Operating Systems:\n1. Find the HOSTS file. It’s usually located here:\n– Windows NT/2000/XP/2003/Vista/7 – C:\\windows\\system32\\drivers\\etc\\hosts\n– Windows 95/98/Me – C:\\windows\\hosts\n2. Make sure you make a backup of the file before changing it.\n3. Open the file with any text editor. There will be two columns, the first one containing IP addresses (e.g.: 127.0.0.1) and the second one – related hostnames (e.g.: localhost).\n4. Add a new line with the IP of the server where your domain is hosted and then add your domain.\n184.108.40.206 localhost 220.127.116.11 example.com 18.104.22.168 www.example.com\n5. Save all changes.\n6. Now restart any opened browser. When the browser opens again, enter your site’s address – it should load from its new location.\n7. If the site does not open, you may have to flush the DNS cache. Go to Start, and then Run, then type “cmd” and hit enter.\nType the following: ipconfig /flushdns\n8. Your website should now open without a problem.\nMac OS X\n1. Open the Terminal app (it’s located in the Utilities folder in your Applications folder).\n2. Create a backup copy of your existing hosts file with the following command:\nsudo cp /private/etc/hosts /private/etc/hosts-orig\n3. Enter your user password.\n4. Now, it’s time to edit the file. Enter the following command:\nsudo vi /private/etc/hosts\n5. Enter your user password.\n6. If you get a permission error, go to your “Help” menu, search for “root” and follow the instructions for “Enabling the root user.” When you are ready, go back to step 2.\n7. You will see a file with contents similar to the following:\n## # Host Database # # localhost is used to configure the loopback interface # when the system is booting. Do not change this entry. ## 22.214.171.124 localhost 255.255.255.255 broadcasthost ::1 localhost fe80::1%lo0 localhost\n8. Add your domain and IP address to the bottom of the list. Press “i” to enter the “Insert mode”. When you are done modifying the file, press “Esc” to exit the “Insert mode”. Type “:wq” to save the changes and quit.\n# Host Database # # localhost is used to configure the loopback interface # when the system is booting. Do not change this entry. ## 126.96.36.199 localhost 255.255.255.255 broadcasthost ::1 localhost fe80::1%lo0 localhost 188.8.131.52 example.com 184.108.40.206 www.example.com\n9. You may also need to grant yourself sudo privileges. In your “Help” menu, search for “root” and select the instructions for “Enabling the root user.” Follow these.\n10. Now restart any opened browser. When the browser opens again, enter your site’s address – it should load from its new location.\n11. If the site does not open, you may have to flush the DNS cache. To do that, type the following in the Terminal:\n12. Your website should now open without a problem.\n3. Use our site transfer services\nIf you or your customers consider site transfer as being too much of a challenge, you can make use of our site transfer service, which is included for free with each hosting package.\nOur technicians will need authorized access to your hosting account with your previous host so that they could transfer the files over to our platform. They will then test your site on our servers and will make sure it resolves flawlessly from the new location as soon as the DNS propagation has been completed.\nAll you will need to do on your end is update the domain’s nameservers in your domain management account.', 'One of the main barriers to cloud adoption is data privacy. This is an issue because, for the majority of cloud providers, EU/EEA and US data privacy and Information Security standards are minefields which are very difficult to cross. And that is because their focus has been on the ease of use and functionality of their services rather than the all-important data privacy, information security, data integrity and reliability requirements around providing these services responsibly.\nBy submitting your personal information, you agree that TechTarget and its partners may contact you regarding relevant content, products and special offers.\nBut, when looking through the plethora of cloud service providers, you can immediately sort the ‘wheat from the chaff’ once you start drilling down into the data privacy, information security, data integrity and reliability capabilities offered to ensure the protection of your and your customers’ data.\nIn this guest blog post, Mike McAlpen, the executive director of security & compliance and data privacy officer at 8×8 Solutions outlines the questions cloud users must ask their providers before signing a contract.\nHave you chosen the right cloud services provider?\n– Mike McAlpen\nBy asking your cloud services provider the following questions you will be on the way to knowing whether you can entrust your data into its care.\n- Compliance with EU/EEA data privacy standards\nThe most important question is whether your provider can provide third-party verification/audit assurance of their compliance with EU/EEA and/or US data privacy standards? It is not enough for the provider to simply produce this verification/audit assurance, it must show that it has fully implemented the UK Top 20 Critical Security Controls For Cyber Defence and/or ISO 27001 and/or rigorous US standards such as the Federal Information Security Act (FISMA) and International Information PCI-DSS v3.0 security standards.\nIf this verification/audit assurance is not available then your business is at peril of not meeting EU/EEA and/or US standards.\nIn the US, many EU/EEA and other countries it can be a criminal offence if a breach of personal data privacy occurs and an individual employee or senior management, depending on the circumstances of the breach, is deemed to be responsible.\n- Onward Transfer of Data\nDoes your provider work with third-party suppliers in order to deliver the cloud services it offers? If so you must check that it has contracts in place with its third-party suppliers that provide assurance that they are, and will continue to be, compliant with EU/EEA and/or US standards.\n- Data Encryption\nDoes the cloud solutions vendor provide the capability to encrypt sensitive data when it is being transferred across the Internet and importantly again when it is ‘at rest’? (i.e. Stored by your cloud services provider, or in files on a computer, laptop USB flash drives or other electronic media?).\n- The Right to be Forgotten\nHas your provider’s solution been engineered to enable it to identify and associate each user’s personal information data? It must also provide the capability for each user to view and modify this personal data. In addition, if the user wishes this data to be deleted, the provider must then be able to completely erase all of that person’s personal data without affecting anyone else’s data.\n- Service Level Agreements (SLAs)\nOutside of compliance with data privacy standards, another key issue is asking your provider how you will determine and then document, within your services contract, the required service level agreements (SLAs). It’s no use whatsoever having the cloud services you have always wanted if you have no way of measuring or monitoring if they are actually being delivered to an acceptable level or if there are no financial penalties for non-compliance.\nIf your provider cannot answer “yes” to the above questions and you cannot agree to mutually acceptable SLAs – look for another provider!']	['<urn:uuid:2ab000f8-7bfe-42df-a4b0-68c3b617e290>', '<urn:uuid:23407dd1-0b20-4921-b3d0-b94fbfb1cb61>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T05:28:56.360010	37	87	1459
22	share business continuity management plan with insurance carrier what information	While sharing BCM plans with insurers is not required, many carriers strongly encourage it during underwriting. It's recommended to be general in response by informing the carrier about having a program in place that includes emergency response, crisis management, and IT disaster recovery plans. Business continuity leaders advise treating BCM plans as operational intellectual property and being cautious about sharing specific details outside the organization. The exception is when complying with federal regulations like FDA, USDA, SEC, and FDIC requirements.	['How business continuity management can elevate your risk conversations with insurers and vendors\nBusiness Continuity Management (BCM) has grown in critical importance and has come to the forefront of companies’ risk management strategies during the COVID-19 pandemic, and will continue to do so as companies plan for the future. In the current hard insurance market, many companies are seeking ways to take control of their insurance program. Having a strong BCM strategy can help.\nThe stakes are certainly high. An infrastructure failure can cost an average of $100,000 per hour, and a critical application failure can cost between $500,000 and $1 million per hour.\n- Because of this, carriers are increasingly encouraging insureds to share their own BCM plans as part of the underwriting process. However, be careful of what information you share with anyone outside your organization.\n- Successful organizations are also requesting BCM plans from their vendors as a best practice to better understand their suppliers’ resiliency and to also move risk upstream.\n- At the same time, you will want to assess your own BCM plans’ agility to test your own resiliency in the event of a disruption.\nHere are further detailed recommendations for each of these three important areas:\nWhat to share with your carrier when they ask for the BCM plan.\nIt’s not a requirement to share BCM plans with your insurer as part of the underwriting process, however, many insurance carriers are “significantly encouraging” insureds to do so. Underwriters are looking for evidence that you have taken the planning steps necessary to protect your business operations, including growing cyber risks. In the current hard market, some carriers are even asking to see copies of the plan.\nBusiness continuity leaders recommend thinking of your BCM plan as operational intellectual property, and risk managers should be cautious about sharing details in their entirety outside the organization—even with your underwriter. This can be a tough line to walk, because you want to make sure the underwriter gets what they need, without locking yourself into specific promises.\nIf the request to see your BCM plan is made by your underwriter, be general in your response by alerting the carrier that you have a business continuity management program in place, which consists of such elements as an emergency response plan, crisis management plan, IT disaster recovery plan, etc., that you identified critical processes and resources surrounding them, and have exercised them over time. Be wary of sharing your plan in detail with any outside organization, with an exception being compliance with FDA, USDA, SEC, FDIC and other federal regulations in the US, where you do need to give specifics.\nThere are three primary organizations that can provide business continuity standards and help you understand what to share with the insurance carrier, now and in the future: the Disaster Recovery Institute International (DRII), the Business Continuity Institute (BCI) and the National Fire Protection Association (NFPA). You can also seek out ISO 22301 and ISO 22330 certifications to show underwriters that you meet overarching standards for general business continuity.\nDo a vendor resiliency analysis.\nA supply chain disruption on the vendor side can have a ripple effect—and companies need to show they’ve managed their risk and put in the work to maintain business continuity from multiple angles.\nEnsuring vendors have a BCM plan in place can push risk upstream and minimize your downtime and costs associated with a vendor service disruption. Alerting your carrier that you have reviewed vendor BCM plans will also strengthen your position at renewal time.\nIn a worse-case scenario, if suppliers don’t have a BCM plan in place, they’ll need to identify recovery actions and procedures on the fly in the event of a disruption. That can lead to missing important steps, overlooking risks and increasing a disruption on your end.\nIt’s important that risk managers know what their supply chain risks are, to gauge how resilient their suppliers are and ensure that the supplier’s problems don’t cascade upon your business and threaten your operations. To figure out your risks, conduct a vendor resiliency analysis of your key vendors—any supplier whose missed commitments might cause the organization to not achieve a stakeholder’s significant expectation, or who is crucial to recovering from a crisis event.\nA vendor resiliency analysis is the examination of a critical vendor to ensure that, in the event of a crisis, they can continue to support your organization with their products and services. Business continuity management leaders recommend doing this analysis with all of your vendors, and even asking them to do so with their vendors—as far upstream as you can go, to find out how resilient your supply chain is.\nTo start, make sure you’re not dependent on any one third-party vendor, which could threaten your own business continuity management program. A single-source vendor could potentially disrupt your business operations, should they be knocked out. Or, if you do use a single-source vendor, make sure you’ve had a conversation with leadership and it’s a conscious decision that fits within the company’s overall risk appetite and business strategy.\nNext, ask your suppliers what they’ve done to identify risk and quantify impact to their business in these four areas of BCM: emergency response, crisis management, business unit continuity and IT recovery.\n- Emergency response. Have they planned a coordinated, effective and timely response to an emergency? The goal is to avoid or minimize injury to personnel and damage to company assets.\n- Crisis management. Have they determined their strategies to manage an event, including the internal and external communications necessary to protect corporate reputation and brand image?\n- Business unit continuity. Have they made necessary preparations to identify the impact of potential business interruptions? These include formulating recovery strategies, developing business continuity plans and administering a training, exercise and maintenance process.\n- IT disaster recovery. Have they listed the technological tenets of their business continuity program, with a focus on restoration, possibly at an alternate location, of data center services and computing capabilities?\nThen, ask what they will do to support your company in the event of a crisis, through an increase or decrease in materials, service or information. You’ll typically receive one of three responses:\n- They have a full BCM plan: the best-case scenario. You can depend on this vendor.\n- They have some business continuity plans in place.\n- They say that contractually, they don’t have to tell you anything. You may not be able to depend on this vendor to support you should a disruption occur.\nNo matter which response you receive, you’ll gain important information about the vendor’s operational well-being. You’ll get a better understanding of your risk relative to external suppliers and a way to address potential loss of vendor support.\nOnce you are aware of your vendor’s BCM plan, you can ask them how your business fits within their overall customer hierarchy: are you the top customer, or further down the line? You can also use this opportunity to identify emergency contacts and procedures at both organizations. Business continuity management leaders encourage companies to include the vendor’s BCM plan in your service level agreement.\nAssess your own agility.\nAs part of a robust risk management practice, business continuity leaders recommend auditing your own strategies, structures and processes to find out how ready—or not ready—your business is to adapt to change in the event of a crisis or loss of vendor support. The goal is to protect the five key variables at risk if an event occurs: operations, finance, customer service, brand reputation, and regulatory compliance.\nAudit your strategy: What are you trying to accomplish, and how will your crisis response align with the core company mission?\nAudit your structure: What are the resources you’ll need if an event occurs? How will you recover, and in what order? Do you have a clear, flat structure, role accountability and hands-on governance?\nAudit your processes: Can you iterate rapidly, is your technology up to date, do you have a standardized way of working and do you promote continuous learning?\nA well-documented BCM plan should include recovery solutions based on the following possible loss scenarios:\n- Loss of facility\n- Loss of IT (corporate and/or local)\n- Loss of key personnel\n- Loss of key vendor/supplier']	['<urn:uuid:f1581459-a40c-429b-a6dd-b049c023ed45>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T05:28:56.360010	10	80	1374
23	steel bikes durability environmental effects	Steel bike frames are favored for their durability and repairability, being able to survive dents and scratches that would be problematic in other materials, particularly for touring bikes. From an environmental perspective, steel frames can be recycled, though paint must be removed first, while other materials like carbon fiber are very difficult to reuse, contributing to cycling's environmental impact that the industry is trying to address through initiatives like the commitment to reduce carbon emissions by 55% by 2030.	['About Touring Bike Frames & Touring Bike Framesets\nThe frame and forks are the heart of your bike and the most important part of it. Bike frames are mostly designed for general use which doesn’t include loaded, open-road touring. What specific things should we factor into frame and fork design for a long distance bike?\nTouring riders tend to spend more time on the bike so run the risk of becoming uncomfortable. A touring bike frame needs to be stiff sideways (laterally) so as not to lose energy but to be a bit springy or soft when it hits a bump.\nUnlike racing cyclists, tourers don’t need high maneuverability and happily trade it in for greater steering stability. Then they don’t need to hold on as firmly and can ride comfortably for much longer.\nWe don’t tour to stare at the road. We want to see the world so touring cyclists prefer to sit up a bit and to have the handlebars almost as high as the seat. Being able to vary the sit-position is also good.\nA touring bike is a beast of burden and needs to be prepared for carrying loads. This includes ensuring there is heel clearance from rear panniers.\nYou also need clearance to allow for wider tyres, should you want to fit them. With bigger tyres and front mudguards, toe clearance needs to be allowed for.\nYou also need to consider attaching things like multiple water bottles, a kick stand, a steering stabilizer, front and rear carriers, spare spokes, lighting systems and mudguards. Apart from frame geometry and materials these are critical to a fully integrated touring bike.\nFor all that is said about steel as a frame material, it is that it survives being bashed around that wins the day. Not that one expects their bike to be bashed around. But it is not rare to see a dent on a touring bike frame and had it been most other materials that would have signaled a real problem.\nTouring Bike Frame Design\nThis is not need-to-know stuff but if you would like to go through the frame design steps, start with the wheel size, then the fork trail, the bottom bracket shell drop and take it from there. The wheel size is the subject of our separate wheels consideration page. Except for the extra small size, we are working with 700C.\nFork Trail on Touring Bikes\nFork trail is the distance that the contact point of the front wheel trails behind the steering axis (a line through the middle of the head tube), producing a caster effect.\nThe less trail or, the closer the trail point is to the axis, the ‘twitchier’ the bike is. As the trail gets longer you have to hold on less firmly. The ‘steeper’ the head (tube) angle, the more the point projected on the ground moves back, thereby reducing the trail. The less the fork rake (the amount the fork bends forward at the bottom), the further back the tyre contact point is from that projected point. So less fork rake equates to greater trail and ‘slower’ steering making it easier to ride. In reality, it makes it possible to ride. It links the “lean angle” of the frame with the turning angle of the fork. Leaning the bike causes the fork to turn in that direction, because the frame is lower after the fork has turned.\nAnother issue with touring bikes is that they need to steer and behave properly when they have a front load and are going fast on long descents. The slower steering and greater stability of higher trail provides this.\nLess trail will require less force to change directions and more trail will require more force. It is far easier to ride a touring bike with no hands simply because the caster effect is stronger. Touring bikes are mostly going straight ahead unlike racers who want to maneuver. In practice, race bikes have trail in the 45-60mm zone and tourers are in the 60-70mm zone.\nSome makers’ sites don’t show the trail dimension so you’d need to calculate it yourself. The formula is easy to pick up. You can see a Vivente Bike’s trail data on the Bike Sizing page. Those figures are for 32C tyres and it is interesting to note that the 64mm of trail expands to 69mm if you fit a 45C tyre. Tyre (bag) size is part of the trail calculation.\nAdding a steering stabilizer is possible but very rarely seen. We have been fitting most Vivente Bikes with a German Hebie stabilizer since 2017. This is an elastomer connection from the back of the fork crown to the underside of the frame downtube.\nBottom Brackets & Touring Bike Frames\nThe bottom bracket shell sits at a distance below the line between the wheel axles. We call this distance the BB Drop. On cyclocross bikes and on both track and crit bikes, the drop is normally 55-65mm, on open road racing bikes 65-70mm and on touring bikes they are typically around75mm. A lower bottom bracket (greater BB drop) makes a bike easier to get on and off, your centre of gravity is lower the bike handles better. It is easier to ‘steer’ through your bum on the seat. With a lower bottom bracket in relation to the wheel centres, you feel more stable. At speed this is even greater and touring bikes are more stable (than for example cyclocross bikes) on fast descents.\nNext we position the seat by adopting a seat tube angle and length. There is nothing very special about this. On an average sized frame, 1° of variation moves the seat forward or back about 1cm. On a road racing bike the rider is a bit more ‘over’ the pedals and the seat tube may be more vertical. In the extreme cases of TT (time trial) bikes the seat angle is around 78°. Some frame makers say that touring riders don’t need efficiency so much and ‘slacker’ seat tubes down to 72° are best. Their suggestion is that there is a trade off of efficiency for comfort by being back further.\nHowever the real issue is where the handlebars are in relation to the seat. How far? And what is their height relative to the seat? Nail down seat tube angle and length first and then select the bar position.\nSo is efficiency less important on touring bikes? There is a lot of physical work in touring and if locating the Knee Over the Pedal Spindle (KOPS) is more efficient, then, let us have it. Depending on frame size and crank length this leads to seat angles from 73° up to 74.5°. This puts you over the cranks rather than slightly behind them. However the differences are small and you can move the seat forward or back 2cm anyway.\nChain Stay Length in Touring Bike Frames\nNext we look at the chainstay length. This is a bike that needs room for luggage. Making the stays longer is sometimes covered in a discussion about wheel base (the distance from front to rear wheel axle). Wheelbase is the sum of sections. The front sections are governed by your body size and steering choices. The lengthening of the frame happens mainly at the back end. So it’s best to talk about chainstay length and wheelbase as more of an outcome. Correct chainstay length gives you heel clearance from panniers.\nIn millimeters, 430 is a bit too short except on the smallest frame where riders have smaller shoes. Generally 450 is good. But many top touring frames have a kick-stand mounted near the end of the left hand chainstay so this can be hit by the heel of tall riders with large shoes unless it is at least 460.\nStack and Reach\nAll that is left for the fundamental geometry on this frame is to locate the handlebars in relation to the seat. In the world of bike geometry discussion people talk about stack and reach as this graphic of a racing bike shows. Notice though that the stem is horizontal and it is immediately above the top of the head tube.\nOn touring bike frames the steering tubes can often extend vertically 100mm to position the stem higher. Also the handlebar stem itself is normally sloping up. The “actual reach” depends on your set-up. So an alternative approach is to start from where the sit-bones rest on the seat. Then, measure to the upper center of the handlebars. This is not just frame and fork geometry now but it is taking into account other components that govern where our hands are.\nRegardless of the handlebar style, relative to the competitive cyclist, tourers tend to be sitting up with their head in a higher hand position. It is best not to achieve this just by having a super-long fork steerer and lots of spacers as there is a point where the front end of the bike is not stiff and stable enough with front panniers. On the smaller wheeled bikes (including 26”) by the time you get the bars where you want them there might be flex you would not have had on a 700C with a longer head tube. This is also why touring frames have sloping top tubes in all but the largest sizes.\nFrame and Fork Materials for Touring Bikes\nSteel is the material of choice. It is the most repairable material. It can be strong yet still lightweight, and it is really stiff. But it has the winning feature of not being ‘notch sensitive’. Scratches and dents? It would have to be major damage before it could lead to failure of tubes, especially of fork blades. Tubes though still need to be the correct diameter and wall thickness. These dimensions are critical. Imagine a loaded bike, with front and rear panniers and handlebar bag, whizzing down a mountain and hitting a bump. Or strongly applying disk brakes ahead of a bump. The forces are big. The solution is not overbuilding as then you lose the suppleness of the ride given by well designed forks. That’s why we recommend testing to EN standards.\nIn anticipation of the bike having both front and rear panniers, the downtube has thicker walls. Top tubes may have thinner walls on the smaller sizes. Not having rear rim brakes allows use of thinner walls on the seat stays. Another weight saving idea is to machine away some of the wall of the head tube as the thickness it needs at the ends is greater than what is needed for 90% of the length.\nSteel is not all the same and in this zone it is actually a steel alloy that is used. The bike industry refers to it as chrome-moly or cr-mo. 4130 is by far the most common of all the steels used to build high quality bicycle frames. The 4 in 4130 represents steel containing nickel, chromium and molybdenum, as distinct from plain carbon, nickel, nickel chromium and other steels. The 1 in 4130 defines the percentage of chromium and molybdenum in the alloy. The last two numbers, 30, mean that the percentage of carbon, expressed as hundredths of a percent, is 0.30%.\nRegardless of who made it, the Society of Automotive Engineers (SAE) specifies the grades of steel. If it is 4130, as used in bike frames, the composition is fixed and controlled by SAE.\nIn double butted tubes, the outside diameter remains the same but the wall thickness varies over the tube’s length. This produces a lighter but still strong tube. The desired level of stiffness is given by the particular diameter and wall thickness.\nWhat About Cyclocross Frames and Forks?\nCyclocross is a bike sport that involves riding off-road in often muddy conditions. The mud gave rise to two design features, 700x35C knobby tyres and cantilever brakes with ample clearance where the tyres pass by the brakes. There is not much mud in Australia but some of these bikes find their way there, and sometimes these are thought, because of the tyre size and brake clearance, to be suitable for touring. In some cases, the marketing departments have asked for rack mounts to be added. This further confuses people who don’t measure everything up to check.\nCyclocross bikes or frame-sets generally do not have the frame geometry you will end up preferring if you go touring. They usually have 430mm chain-stays and consequently heels would clip the panniers. You can possibly move the bags back but then you have a tail-wagging-the-dog scenario. Or you can use front bags on the back and go “lightweight” touring. They have higher bottom brackets for cornering in races so don’t have as comfortable a feel. They don’t have the thicker main-tube walls that touring bikes should have for loads. They have fork offsets akin to road racing bikes. This is good for racing but not so good for all-day riding. That “twitchiness” gets annoying, especially when contrasted to touring geometry where the bike seems to steer itself. Proper touring bikes require little effort to steer and you don’t need to hold the bars tightly.', 'The green revolution: Can the cycling industry become environmentally friendly?\nThe activity of cycling is one of the greenest out there, but does the bike industry match up to that? What is the environmental impact of making a bike?\nHeading out the door? Read this article on the new Outside+ app available now on iOS devices for members! Download the app.\nThis is the first article in a series of four to be published on VeloNews on the subject of the environment and sustainability in cycling. Read part two, part three, and part four of the series.\nThe cycling industry is an environmental contradiction, but can it change?\nCycling as an activity is a key component in the ongoing fight against climate change as countries look to reduce carbon emissions by enticing people away from fuel-powered vehicles.\nHowever, the sport and the wider cycling industry cannot boast the same green credentials with riders, races, teams, and journalists traveling all over the world throughout a season and the endless push for the next new piece of equipment.\nIt might be more environmentally friendly than the car, but cycling cannot rest on its laurels. If the cycling industry and professional racing want to keep up with the push for sustainability, it needs to make changes.\n- How InstaFund Racing will race around the world on a carbon-neutral budget\n- CEOs of Specialized, Rapha, Assos, BMC and others sign Climate Commitment letter\nErik Bronsvoort is the co-founder of Circular Cycling — and author of the book From Marginal Gains to a Circular Revolution — a project that aims to reduce waste in the cycling industry and bring down its carbon emissions. He believes that by utilizing its creative resources, the cycling industry can redesign itself as a more environmentally friendly one.\n“In the last 20 to 30 years, there’s been a lot of innovation going on in the cycling industry, ranging from new materials like carbon fiber to the way mountain bike geometry has changed, the introduction of e-bikes, and entirely new concepts on how to put bikes on the street like bike-sharing schemes,” Bronsvoort told VeloNews.\n“Having said that, sustainability has never been on the agenda other than we are the green alternative to the car and that is what needs to change. The way companies are built with a lot of innovative power they’re creating new products every two or three years to update the line of products.\n“These organizations are probably better suited than any other on the planet to innovate their way out of this mess. I’m quite optimistic that in the next few years we can see a big change in the way we see bikes.”\nWhat is the environmental impact of a bicycle?\nIn a report on sustainability published last year, bike manufacturer Trek estimated that you would have to ride about 430 miles (692km) to achieve carbon neutrality on your new bike purchase. Those miles would have to be done in place of a car journey, rather than a leisure ride.\nThe American company estimated that the carbon emissions on its Madone model were in the region of 197kg of CO2, while its Rail e-bike resulted in 229kg of CO2 emissions. Trek is one of the few companies to publish this kind of information.\nAlso read: Meet the team tackling cycling’s garbage problem and eliminating single-use plastics\nThere is also the problem of recycling the materials when they’re done with them. Aluminum frames can be recycled but the paint must be removed before it goes through that whole process, while carbon fiber is very difficult to reuse.\nThese numbers are tiny in comparison to what goes into manufacturing something like a car, but it can add up with all the new parts and it doesn’t mean that it should be ignored. Tackling cycling consumption and waste can have an impact, especially as more people turn to two wheels for getting around.\n“Marketers have been trying to sell all these marginal gains to us, which really don’t make that much of a difference,” Bronsvoort told VeloNews. “The next big thing for marketers and brands to set themselves apart from the others is to introduce more sustainable alternatives to the stuff that we’re used to.\n“Stepping away from the ‘we need to shave off two grams’ to ‘we’re going to use some stuff that is not harming earth’ is a very good source of energy for innovation teams to work on. Also, brands need to come up with entirely new ways of doing business with customers.\n“If I’m buying a groupset for my bike, I’m also buying the problems that will come with it. Worn out chains, stuff breaking, etc. I don’t want these problems I just want my drive train to work. I’d be happy to pay a monthly fee as long as Shimano, SRAM, or Campagnolo takes responsibility to make sure it always performs as it should. You can make a distinction between buying a product and paying for the use of a product.”\nAlso read: The Outer Line: Climate change and endurance sports\nPaying something akin to a membership fee when you buy a new piece of equipment might not be the direction that the industry moves in, but there is a growing drive to clean up cycling’s environmental impact.\nIn November last year, the CEOs of Specialized, Rapha, Assos, BMC, and several other cycling companies issued an open letter, which was published by Shift Cycling Culture — a project that Bronsvoort is involved in. The letter asked companies to commit to reducing their carbon emissions by 55 percent by 2030 and by 100 percent by 2050.\nSince the letter was published, almost 40 CEOs or founders have signed the letter, including from SRAM, Ridley, and Ribble Cycles.\nCan professional cycling go green?\nIt’s not just the industry that needs to work on its green credentials, the professional sport does too. There are efforts to improve the sustainability of the sport, from a rider, team, and race organizer level — we’ll dig into that a little deeper in the next few days — but there’s still a long way to go.\nThere’s all the international travel to races and training camps, the mountains of new kit each year, the mighty race caravan that circumnavigates many a country, and the tonnes of trash created by gel wrappers, plastic bottles, and race goodies. It’s an eco nightmare.\nCanadian racer Michael Woods calculated his own personal carbon emissions of 60 tons of CO2 — approximately three times the average person living in his region — for the 2019 season. More than half of his carbon footprint was down to travel to races.\nIn making his calculation, Woods did his best to include additional items such as kit, clothing, and the other pieces of equipment he used throughout the year.\nThat’s over 10,000 tons for a year when you look at a race peloton of 180 riders. There will be riders will larger and smaller carbon emissions across a year, but Woods’ emissions are likely to be around average.\nIf professional cycling wants to clean up its environmental act, then it will have to overhaul all aspects of how it operates. Bronsvoort believes that the UCI must oversee this change and not be a passive actor in it.\n“I think the best way to approach it is through regulation. So, the UCI is basically determining the way bikes should look or safety standards and also the geometry standards,” Bronsvoort said. “If the UCI is able to set a strict set of rules, say by 2025, a bike should be built from 50 percent recycled or bio-based materials and should be 100 percent recyclable, and by 2028, the bikes are entirely circular then the companies providing the teams with equipment will have a good incentive to start developing these products.”\nOnce innovations are made for the professionals, it can then bleed out into the wider cycling industry, according to Bronsvoort.\n“Then what happens, because the UCI says bikes operating in the pro peloton should also be available for regular consumers, you’ll see the same bikes in shops,” he said. “Companies will have another incentive to develop not just the pro bikes, but the entire line-up with these new innovations.\n“Then you’ll see a very massive spread of these concepts within the entire cycling industry. The pro peloton does have a big impact and the way it should be done is to make sure that there’s a level playing field supported by UCI.”']	['<urn:uuid:0a30f57e-c04f-4813-826f-26bed8db043e>', '<urn:uuid:8b6b3751-7e0b-4bf9-97f2-740da6dbdb88>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T05:28:56.360010	5	79	3615
24	I've been studying both traditional art movements and modern sustainability practices. Could you explain how artistic composition principles evolved from classic photography to modern farming, and what challenges both fields face in terms of space utilization?	In photography, as explained by Cartier-Bresson, composition was about seeing and capturing a specific moment when elements aligned perfectly in a frame. Similarly, vertical farming represents a new approach to spatial composition, but instead of arranging elements in a two-dimensional photograph, it involves organizing crops in vertical space. While traditional agriculture, like traditional photography, spread horizontally and required considerable space, vertical farming systems can be designed flexibly - from vertical walls to large hangars or shipping containers. However, both fields face challenges: in photography, missing the decisive moment means losing the shot forever, while in vertical farming, despite its space efficiency, there are difficulties in scaling up the systems due to high energy requirements and investment costs.	['A bible for photographers. That is how Robert Capa described The Decisive Moment by Henri Cartier-Bresson. After almost 70 years it was first published, this book has still a lot to say to photographers and especially to street and documentary photographers.\nThe book said to be an essential one for any photography collection. But is it? I am going to give you this brief overview of this legendary book.\nHenri Cartier-Bresson is said to be a founder of modern photojournalism. I have actually talked about his life and photography in one of my very first videos. He actually came up with this idea of the “decisive moment.” I wouldn’t say he invented it but he definitely gave it a name and introduced it to a wider audience.\nHere’s what Cartier-Bresson told the Washington Post in 1957:\nPhotography is not like painting. There is a creative fraction of a second when you are taking a picture. Your eye must see a composition or an expression that life itself offers you, and you must know with intuition when to click the camera. That is the moment the photographer is creative, oop! The Moment! Once you miss it, it is gone forever.\nThe book was published in 1952 originally titled Images à la Sauvette (“images on the run”) in the French, published in English with a new title, The Decisive Moment. The words were actually taken from a quote by the 17th century Cardinal de Retz, who said, “There is nothing in this world that does not have a decisive moment.”\nYou can find the first edition on eBay for $1,000 and more depending on the condition. It was printed in 10,000 copies; 3,000 of the French edition and 7,000 of the American edition. The original price was actually $12.50 in North America.\nThe new print run of the book has a hardback and extra hard case to house the book itself and a pamphlet that I will talk about later.\nThe artwork on the case and on the book itself is an artwork by Henri Matisse — it’s not a photograph but a signature cut-out of Matisse.\nIn the top right, we can see the sun that shines over the Blue Mountains. In the middle is a bird holding a branch. Then there are a few green and black vegetal forms and a stone at the bottom. On the back cover, we can see color sparkles of green and blue. The spirals are supposed to evoke the pace of time.\nWhat is interesting about this cover is that when you look closely, the name Cartier-Bresson is actually missing the hyphen. I don’t actually know why that is. Maybe Matisse forgot to draw it there and then they just didn’t want to tell him to fix his artwork. The pamphlet suggests that the missing hyphen could be deliberate, as if Matisse wanted to express Cartier-Bresson’s duality and his shifting temperament.\nThe book is pretty big. Even without the case, it is the biggest book I own. It is 37x27cm so the ratio works for photographs of 24×36 photographic film Cartier-Bresson used. Each page can fit one horizontal photograph or two vertical ones. The pages are stitched in a way that allows proper flat opening. It has 160 pages with 126 photographs and weighs over 5.5lbs/2.5kg.\nThe pamphlet is a very nice behind-the-scenes book composed of quotes by Henri Cartier-Bresson and some other information about how the book was made. It tells the story of Cartier — Bresson working on the book, the sequencing, work with the publisher, the story of the cover, and so on.\nThe publisher insisted that the images should be matched with a text in the book. The idea was to provide technical and “how-to” information, but it was something Cartier-Bresson wasn’t fond of too much.\nThe book starts with an introduction by Henri Cartier-Bresson followed by photographs split into two sections, chronologically and geographically. The first one contains photographs taken in the West from the years 1932 to 1947. The second contains photographs taken in the East from 1947 to 1952. Since the introduction wasn’t technical enough, there is also a technical text by Richard L. Simon at the end of the book, which was only included in the American version.\nWe can also consider the two sections to be the phases of Cartier-Bresson’s carrier. During his career, Cartier-Bresson oscillated between art and photojournalism. It is obvious from his photographs before he joined Magnum and after that. When selecting the photographs, Cartier-Bresson clearly favored the reportage images he made as a member of Magnum Photos as some of his well-known images like Hyeres, France, 1932 (AKA The Cyclist) from the early 30s are missing.\nAs The Decisive Moment was published 5 years after he joined Magnum, he left out a lot of his surrealism inspired images and used more of his photojournalism work, especially in the second section which displays only Magnum images. Those photos also have much longer captions as they were mostly shot for press.\nThe book is evidence of Cartier-Bresson’s shift from art to photojournalism during that time. He would later switch back when he focused on exhibitions and wanted to be perceived more as an artist rather than a photojournalist.\nAll photographs are in black and white, as Cartier-Bresson didn’t like to shoot colors. He saw color as technically inferior (due to the slow speeds of color films) as well as aesthetically limited.\nAt the time the book was published, it was immediately clear the book was unique, not only in terms of size but also quality. Even though it was accepted very well by critics, the first sales were not so good: that was the reason there was never the second print (prior to 2015).\n“The Decisive Moment” wasn’t actually the only title Cartier-Bresson considered. One of the favorite possible titles was “À pas de loup” (“Tiptoeing”), which expressed the way in which Cartier-Bresson approached those whom he photographed. “The subject must be approached tiptoeing,” he once said.\nThe second possible title was “Images à la Sauvette” (roughly translated as “images on the run”), which ended up being the name of the French edition, related to small street vendors ready to flee when being asked for their license. It also expressed his concept of photography.\nMy favorite picture from the book is one from Kashmir showing Muslim women on the slopes of Hari Parbal Hill in Srinagar praying toward the sun rising behind the Himalayas.\nThe #ICPMuseum is the first venue in the United States to present “Henri Cartier-Bresson: The Decisive Moment.” The exhibition shares the behind-the-scenes story and offers a rare chance to see first edition printing. https://t.co/blKiNC0Wgo 📷 Srinagar, India, 1948 pic.twitter.com/UJY54xTG3P\n— ICP (@ICPhotog) July 9, 2018\nAs you see when you look at Henri Cartier-Bresson’s photographs, he adapted many styles. When I first saw this photograph, I just thought how timeless it looked. It could easily be taken thousands of years ago if cameras had existed then. Cartier-Bresson shot some very important events but also some casual photos.\nI think The Decisive Moment an amazing book even though it is expensive. But the valuable information it provides and the forms makes you really feel like you are holding a piece of history. It would be a great gift for any photographer but especially for someone shooting a street or documentary style.\nAbout the author: Martin Kaninsky is a photographer, reviewer, and YouTuber based in Prague, Czech Republic. The opinions expressed in this article are solely those of the author. Kaninsky runs the channel All About Street Photography. You can find more of his work on his website, Instagram, and YouTube channel.', 'Agricultural systems around the world need to adapt to the rapidly changing environmental, demographic, and socioeconomic landscapes, and new alternative practices, such as vertical agriculture, may offer new opportunities to accelerate such adaptation.\nNext Gen Farming Without Soil and 90% Less Water | GRATEFUL\nWhat is vertical farming and why is it important\nModern agricultural systems encompass an estimated 1.5 billion hectares of the world’s surface area. With a growing population and resource needs, the availability of arable land is shrinking rapidly.\nSince the agricultural revolution, conventional agriculture has focused on practices requiring considerable quantities of space, water, fertilizer, and pesticides. The past 50 years have seen an accelerating rate of increase in these requirements as modern food production aims to increase productivity in the hopes of addressing growing food insecurity.\nLooking into the future, yield production is forecasted to decrease due to widespread environmental and socioeconomic changes that will generate unpredictable consequences on food systems.\nIn response, many strategies have been developed as alternatives to conventional agricultural practices. These strategies have focused on key principles and their combination to be effective: require less space, less water, and increase yield per unit of area. Moreover, due to the negative effects of agrichemicals, modern practices have also aimed to use significantly less to avoid potentially adverse effects for humans and animals.\nOne such alternative is the development of vertical agriculture, also referred to as vertical farming. As the name implies, vertical agriculture relies on expanding production vertically and not horizontally. Vertical agriculture is a multilayer indoor plant production system that allows for precise control of growth factors, such as light, temperature, humidity, carbon dioxide concentration, water, and nutrients.\nThis allows for the growing and production of crops year-round, completely independent of solar light and other external conditions. Indeed, vertical agriculture makes use of key concepts within ecology and physiology to optimize growing and fertilization within controlled conditions. For instance, elements of photobiology, thermomorphogenesis, hydroponics, and genetic breeding, are all used commonly across systems of vertical agriculture.\nBenefits, challenges, and disadvantages moving from horizontal to vertical farming\nAs a result of tight control over crop breeding, growing, and harvesting, vertical farming provides several benefits relative to conventional methods of ‘horizontal’ food production. This was the topic of a literature review by Kalantari et al. published in 2016 in the Journal of Landscape Ecology.\nFrom a systems perspective, the enclosed design prevents pests and diseases from entering by the adoption of a high level of hygiene, continuous monitoring, and non-chemical disinfection, providing security from crops. Moreover, recent technology has also allowed for automated control over environmental conditions by using sensors and imaging techniques in combination with crop simulation models and artificial intelligence, limiting the need for physical labor.\nVertical farming also allows for flexible organization, with designs ranging from large vertical walls covered with crops to large hangars or re-used shipping containers that can be transported. Consequently, vertical agricultural systems, can comprise many varying sizes and be located within many different areas from the middle of highly urbanized cities to more suburban or rural areas.\nMoreover, the verticality element of this system also provides nutrient and water flow, helping to reuse costly resources. The reduction in space also means there is a significant increase in yield per area, holding extensive potential for a future world of urbanization.\nFrom an economic perspective, vertical farming also provides for more jobs in localized areas and is community-focused by addressing the needs of immediate areas, which in turn can provide food at a lower price. Finally, the optionality of location for vertical systems also allows producers to reduce transport costs, as consumers may access them within urban areas, or transport can be minimized to nearby areas.\nHowever, despite the design, environmental, and economic advantages, vertical farming also incorporates several issues that remain a challenge to its broader implementation as a system.\nVertical farming has a high energy requirement and needs extensive investment costs to implement and develop successfully. Moreover, indoor issues relating to excessive UV, heat, and ozone-induced plant damage may have unpredictable repercussions for plant growth.\nAdditionally, vertical systems are difficult to adapt to a larger scale. They are costly to build and maintain and have yet to demonstrate the ability to provide food for larger areas than community-scale populations. This would make it difficult to implement in areas at higher risk of food insecurity, such as developing agricultural nations.\nThe lack of empirical research on a broader scale has meant that vertical farming has yet to develop past the concept stage on community levels, as persistent issues make it difficult to break through to a larger scale.\nImage Credit: YEINISM/Shutterstock.com\nGrowing skywards - the implications of vertical farming in a rapidly populating and changing world\nAmong the development of alternative agricultural practices, vertical agriculture provides a promising solution for many of the challenges facing current agricultural policies. However, for vertical systems to be integrated on a larger scale requires further technological progress and economic investment.\nNevertheless, gradually implementing more verticality, or combining it with other practices aiming for more sustainable practices may be promising. For instance, the combination of verticality with other practices such as intercropping may be particularly beneficial for developing more sustainable food systems.\nIncorporating technological progress into vertical systems also holds promise, with automated sensors and machinery able to operate near-independently. Progress in gene editing and plant genome modifications will also allow for faster, bigger, and healthier crops, allowing vertical agriculture to produce more over time.\nThroughout agricultural history, farming systems have typically spread over large spans of land, yet the reduction in arable land, as well as the increase in demand to house growing populations, means that such strategies need to be reconsidered, and vertical agriculture may play a role looking into the future.\nContinue Reading: Benefits of Vertical Agriculture and Hydroponics\n- Beacham, A. M., Vickers, L. H., & Monaghan, J. M. (2019). Vertical farming: a summary of approaches to growing skywards. The Journal of Horticultural Science and Biotechnology, 94(3), 277–283. doi: 10.1080/14620316.2019.1574214\n- Chaudhry A. R. and Mishra V. P.,(2019) A Comparative Analysis of Vertical Agriculture Systems in Residential Apartments, Advances in Science and Engineering Technology International Conferences (ASET), 2019, pp. 1-5, doi: 10.1109/ICASET.2019.8714358\n- Sarkar, A., & Majumder, M. (2015). Opportunities and Challenges in Sustainability of Vertical Eco-Farming: A Review. Journal of Advanced Agricultural Technologies, 2(2). doi: 10.12720/joaat.2.2.98-105\n- SharathKumar, M., Heuvelink, E., & Marcelis, L. F. (2020). Vertical Farming: Moving from Genetic to Environmental Modification. Trends in Plant Science, 25(8), 724–727. doi: 10.1016/j.tplants.2020.05.012']	['<urn:uuid:0c808295-8220-47c6-844a-190b396a08a6>', '<urn:uuid:3566e03d-b8ac-4d36-b13c-9d56f4d7ba0e>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T05:28:56.360010	36	117	2367
25	I'm interested in community farming. How did this project in Buffalo begin?	The project began on June 3, 2017 as the Somali Bantu Community Garden on an eighth-acre parcel in East Aurora, NY. It was a collaborative effort between the Somali Bantu Community Organization of WNY, Providence Farm, and the East Aurora Huddle. The initial planting day brought together 45 volunteers from various communities, including Somali Bantu refugees, East Aurora residents, and others from across the region, who worked together to establish the farm.	['Empowering Just and Equitable Access to Food and Farmland\nCultivating farmer-led and community-rooted agriculture and food systems to actualize the rights of under-resourced peoples.\nProvidence Farm Collective (PFC) has its roots in the Somali Bantu Community Farm, a three-year pilot project. The pilot explored the challenges and opportunities of addressing fresh food insecurity and farmland inequity. PFC supports under-resourced farmers in Western New York needing access to clean, rural farmland, farming and business education, technical assistance, access to markets and the opportunity to farm for income. PFC also offers youth education and employment programs.\nFarmer Advisory Board\nMahamud Mberwa – Somali Bantu Community Organization\nAli Macheremo – Somali Bantu Community Organization\nAllison DeHonney – Buffalo Go Green, Urban Fruits and Veggies\nDao Kamara – Liberian Association of Buffalo\nNelson Nagbe – Liberian Association of Buffalo\nEtando Omari – Congolese Babondo Buffalo\nDunia Mchindewa- Congolese Babondo Buffalo\nGabriel Khoon – Karenni Community of Our Lady of Hope\nFaz Niyigoba – Burundian Community of Our Lady of Hope\nSaw Set, Buffalo Myanmar Indigenous Christian Fellowship\nBoard of Directors\nKristin Heltman-Weiss, President\nEileen O’Brien-Scannell, Vice President\nHeather Chudzik, Treasurer\nJanet Coletti, Secretary\nProvidence Farm Collective has its roots in the Somali Bantu Community Farm, a three-year pilot project that explored the challenges and opportunities of addressing food scarcity and land inequity by providing Buffalo’s Somali Bantu refugee community with access to farming in a rural setting.\nOn June 3, 2017, the Somali Bantu Community Garden broke ground on an eighth-acre parcel in East Aurora, NY, as a collaborative effort of the Somali Bantu Community Organization of WNY, Providence Farm, and the East Aurora Huddle. On May 26, these three groups met to explore the possibility of a rural farm for the Somali Bantu refugees of Buffalo to grow affordable produce to bring back to their West Side community. After securing farmland, efforts were underway to collect donations of tools, plants, and seeds. 45 volunteers of all ages, from the Somali Bantu community, East Aurora, and across the region showed up to plant. That summer, the farm became a place where generations of families, immigrant and native-born Western New Yorkers, urban and rural residents, worked side-by-side toward a common goal.\nThe incredible growth of this project continued. In April 2018, the farm received a nearly $74,000.00 grant from the United Way of Buffalo and Erie County and the General Mills Foundation. Through this generous grant, the farm purchased a pickup truck, a passenger van, a tractor with attachments, two storage trailers, fencing, irrigation equipment and seeds. These funds helped the farm expand in ways unimaginable a year ago. Fresh farm produce continued to be sold to under-resourced communities at affordable prices; unique African crops were successfully grown and brought back to the west side; cash crops of garlic and flowers helped the project establish a revenue source; and 1,500 pounds of vegetables were given away to people in need.\nIn October of 2019, the Somali Bantu Community Farm received a $10,000.00 grant from the Newman’s Own Foundation, as part of $2 million in grants awarded to nonprofit organizations that are focused on fresh food access and nutrition education. The Newman’s Own grant funding supports nutrition programs all over the country and internationally, helping nonprofits address various needs such as nutrition education, healthy diets, food rescue, gardening, and access to fresh fruits and vegetables in under-resourced communities. Newman’s Own Foundation has been a long-time supporter of nutrition, and in October 2019, more than $1 million of the funds will be focused on grassroots organizations, like the Somali Bantu Community Farm, that provide impact at a local level.\nLarge areas of accessible and contaminant-free farmland cannot be found within the city of Buffalo. Although there are over one hundred much needed urban community gardens and farms throughout the city, these plots are not capable of yielding enough fruits and vegetables to provide fresh affordable produce for Buffalo’s under-resourced residents. Furthermore, usable farmland is dwindling in Erie County, and the land that is available is located in rural areas such as East Aurora, thus posing the challenge of accessibility. There is tremendous need among New Americans and African American farmers for accessible farmland and to provide fresh affordable produce to the communities in which they live.\nThere is a strong desire to farm in the Somali Bantu community, but as the project progressed, typical challenges emerged: finding funding; coordinating schedules; communication. Although transportation was not an issue, as the SBCF has a 15-person passenger van to get volunteers to and from the farm, the cost of maintenance and operation of the vehicle posed a constant problem.\nMost challenging, however, were collecting data and implementing communication protocols; keeping volunteers motivated; operating without any paid staff or an executive director; and lastly, working within the constraints of the SBCF being a program of the Somali Bantu Community Organization of WNY. Regarding this last point, the administration of the SBCF, although an important program of the Somali Bantu community, was a burden on the SBCO leadership and the community as a whole. Marginalized communities already shoulder heavy economic burdens, as well as limited, if any, leisure time. Volunteerism by low-income individuals is not always possible simply due to the constraints and affordability of transportation, having to work several jobs to make ends meet, and the lack of affordable childcare.\nThe idea of a collective of farms, under the umbrella Providence Farm Collective, provides a solution and a way forward. With a dedicated farm manager, executive director, and 2 farmhands, the land will be prepared by PFC; seeds will be provided, as will fencing, hand tools and irrigation. All participating partner organizations and individuals will have the freedom to divide and farm the land as they wish, with the one condition that whatever is grown is shared with or sold affordably to an under-resourced community. Incubator farmers will have the opportunity to farm for a living, with the option of a plot tenancy at Providence upon completion of a three-year farm program. With 37-acres of farmland and woodlands available to share with nonprofit organizations and low-income farmers, Providence Farm Collective has the potential to be a game changer in Western New York in addressing the issues of land inequity and food insecurity and scarcity.']	['<urn:uuid:ecd94bc3-f49c-4a0f-b7e8-ecd241802e60>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T05:28:56.360010	12	72	1043
26	I'm planning to travel across Europe next month and I've heard there's some kind of train pass that lets you travel to many countries. What countries can I visit with this pass and how does it work?	The Eurail Pass allows you to travel across 33 European countries, including both EU and non-EU member states like Turkey and Serbia. With this pass, you can hop on and off trains as many times as you want within your chosen time period. To use it, you'll need to buy the pass through the official website, and you can either get an e-pass or have a paper ticket mailed to you. When using the pass, you'll need to keep a travel diary recording your journeys, and for some high-speed and night trains, you'll need to make free seat reservations in advance. The pass needs to be activated before your first journey, either at major train stations for paper tickets or online for e-passes.	['Are you happening an Euro backpacking journey anytime quickly?\nYou could wish to rethink no matter itinerary you have got deliberate, as we’ve got simply come throughout an irresistible deal that may make your travels a lot less complicated. Instead of a number of, ultra-expensive flights that may doubtless get delayed in the long run, how about reserving a single practice ticket enabling you to journey throughout 33 international locations for an entire month, or longer, for lower than $200?\nThe Eurail Pass is a Europe-wide initiative aimed toward boosting practice ridership at a time when carbon emissions are rising, particularly after the resumption of worldwide flights, and strengthening connectivity throughout the continent. Essentially, cross holders are allowed to hop on and off trains as many occasions as they need, inside a pre-determined time period, as per current fare guidelines.\nPretty thrilling, proper? Here’s every thing it’s essential know concerning the more and more common ‘Europe train pass’:\nTravel All Of Europe With A Single Ticket\nKnown because the Eurail Pass, this ticket grants unrestricted journey to 33 international locations throughout the European area, together with each European Union (EU) and non-EU member states, reminiscent of Turkey and Serbia. In sum, it grants its holders the precise to hop on any practice of their liking for a variety of days inside a selected timeframe (e.g. 7 days inside 1 month).\nEffectively, it’s a fairly whole lot for backpackers visiting Europe for longer, because it permits them to journey the continent with a single cross, versus reserving separate tickets for every part of their journey. There’s extra: when you’re a younger traveler, aged 12 to 27, you get a 25% low cost off the fare, whatever the longevity of the cross.\nThis means it’s possible you’ll be eligible for no less than 4 days of limitless journey inside Europe for less than $188.\nTrain Travel Is The Way Forward\nIf there’s one development choosing up momentum now that the aviation business has descended into chaos is long-distance practice journey. Originally a transport modality that solely a distinct segment of vacationers used, both for environmental issues or for the scenic views, trains are getting increasingly more inexpensive now that flight costs have gone through the roof.\nThe disaster is hitting some international locations more durable than others, significantly in Europe, the place a variety of airports have been compelled to cap the variety of day by day flights as a result of overwhelming demand, mostly to no avail. Luckily, not like the Americas, Europe has a well-developed rail system providing hyperlinks between a number of cross-border locations.\nSurpirisngly, many Americans will not be conscious the Eurail Pass exists, however the most effective methods to journey Europe is reserving this multi-journey ticket legitimate between one to 3 months, or the precise period of time vacationers are usually allowed to remain in Europe. Needless to say, practice journey is the best way ahead.\nWhat Are The Eurail Fares And Validity?\nEurail Pass Fares\nUnlimited Travel Days Within 1 To 2 Months\n- 4 limitless journey days inside 1 month – $188 (2nd class) / $250 (1st class)\n- 5 limitless journey days inside 1 month – $215 (2nd class) / $287 (1st class)\n- 7 limitless journey days inside 1 month – $255 (2nd class) / $341 (1st class)\n- 10 limitless journey days inside 2 months – $306 (2nd class) / $408 (1st class)\n- 15 limitless journey days inside 2 months – $376 (2nd class) / $502 (1st class)\n- 4 limitless journey days inside 1 month – $250 (2nd class) / $334 (1st class)\n- 5 limitless journey days inside 1 month – $287 (2nd class) / $383 (1st class)\n- 7 limitless journey days inside 1 month – $341 (2nd class) / $454 (1st class)\n- 10 limitless journey days inside 2 months – $408 (2nd class) / $544 (1st class)\n- 15 limitless journey days inside 2 months – $502 (2nd class) / $669 (1st class)\n- 4 limitless journey days inside 1 month – $225 (2nd class) / $300 (1st class)\n- 5 limitless journey days inside 1 month – $258 (2nd class) / $344 (1st class)\n- 7 limitless journey days inside 1 month – $307 (2nd class) / $408 (1st class)\n- 10 limitless journey days inside 2 months – $367 (2nd class) / $490 (1st class)\n- 15 limitless journey days inside 2 months – $452 (2nd class) / $602 (1st class)\n- 15 consecutive days of limitless journey – $338 (2nd class) / $451 (1st class)\n- 22 consecutive days of limitless journey – $396 (2nd class) / $527 (1st class)\n- 15 consecutive days of limitless journey – $451 (2nd class) / $601 (1st class)\n- 22 consecutive days of limitless journey – $527 (2nd class) / $702 (1st class)\n- 15 consecutive days of limitless journey – $406 (2nd class) / $540 (1st class)\n- 22 consecutive days of limitless journey – $474 (2nd class) / $632 (1st class)\n- 1 month of limitless journey – $503 (2nd class) / $682 (1st class)\n- 2 months of limitless journey – $558 (2nd class) / $744 (1st class)\n- 3 months of limitless journey – $689 (2nd class) / $918 (1st class)\n- 1 month of limitless journey – $682 (2nd class) / $909 (1st class)\n- 2 months of limitless journey – $744 (2nd class) / $993 (1st class)\n- 3 months of limitless journey – $918 (2nd class) / $1224 (1st class)\n- 1 month of limitless journey – $614 (2nd class) / $819 (1st class)\n- 2 months of limitless journey – $670 (2nd class) / $894 (1st class)\n- 3 months of limitless journey – $827 (2nd class) / $1102 (1st class)\nIf you might be touring inside a single European nation, there’s a separate Eurail One Country Pass that may be bought. Unlike the usual Eurail Pass, costs range amongst international locations. In Italy, for example, they begin at $107 for 3 days of limitless journey inside 1 month, although it may be as costly as $326 for a 5-day cross.\nWhat Countries Are Included In The Pass?\nA complete of 33 European international locations take part within the Eurail scheme. Those are:\n- Czech Republic\n- Greece (together with islands)\n- Great Britain (England, Scotland and Wales)\n- The Netherlands*\n- North Macedonia\n*Only these international locations nonetheless have Covid entry necessities in place. If you’re flying into Europe by way of Luxembourg, The Netherlands or Spain, be sure to carry legitimate well being documentation. This could embody a compulsory vaccination certificates and/or damaging check issued previous to departure.\nEurail passes are additionally legitimate for bus and ferry journeys in choose places, reminiscent of Spain, the place AML, Balearia and Grimaldi Ferry Lines are included. In Sweden, a number of the collaborating ferry operators are Finnlines, Fjord Line, Stena Line and Viking Line, amongst others. Tourists can discover the whole record of collaborating transport operators on this link.\nHow Do I Buy And Use An Eurail Pass?\nIn order to be eligible to purchase an Eurail Pass, you should fulfill two easy necessities:\n*This doesn’t imply Europeans are barred from touring by practice with out limits – they’re merely required to use for the Interrail Pass as an alternative, which works identical to the Eurail.\nTickets will be bought by way of the official website, the place vacationers can be anticipated to create an account, offering particulars reminiscent of age, nationality and nation of residence (for eligibility functions). They may even be required to pick out the precise length of their cross. The course of is kind of simple and the product is delivered on to their Eurail account.\nThe cross may also be despatched by publish, in paper type, to their dwelling within the U.S., Canada, or different places forward of their flight to Europe. Personally, we’d suggest sticking to the web ticket as it’s delivered inside seconds of shopping for, and also you received’t danger your mail getting misplaced, or damaging the precise ticket throughout your travels.\nOnce they’ve the Eurail Pass in fingers, Americans are free to discover the entire of Europe by practice, North to South, East to West and Central, in accordance with their pre-selected variety of journey days. It is price noting it might be essential to make a free seat reservation on some routes upfront, or on the station previous to departure.\nThis applies largely to high-speed and evening trains, the place seating availability could also be restricted. Using the cross is tremendous simple as properly: when you’re carrying a paper ticket, you’ll have to have an official ‘activate’ it earlier than your first journey, ideally at bigger practice stations reminiscent of Paris Gare du Nord, London’s King Cross, or Amsterdam Centraal. Activation consists of getting a stamp indicating the ‘start and end day’ of the cross.\nAlternatively, e-pass holders can merely activate it themselves after ordering from the official web site. Lastly, it’s of utmost significance that each one Eurail clients manually fill of their journey diary, which is normally connected to their paper ticket or their on-line reserving, detailing their departure station, departure time and vacation spot for every journey.\nIn case they don’t maintain a document of their touring within the Eurail diary, together with on-line, their cross won’t be thought-about ‘valid’ by employees, and fines could incur. For extra data on the best way to use the cross and fill within the right journey data, vacationers are suggested to contact Eurail or certified employees at stations. Once the Pass is validated forward of the inaugural journey, the ‘clock’ begins operating.\nWhat Are Some Of The Recommended Train Journeys?\n- The Highlands of Scotland, the place unbelievable pure surroundings will be discovered, together with the paranormal Loch Ness and Great Britain’s highest peak, Ben Nevis\n- The evening practice from Vienna to Budapest, well-known for being one of the vital scenic practice rides in Europe\n- The dreamy Turkey journey between Ankara, Turkey’s capital, to the historic Kars, with breahtaking views o the Anatoliann highlands\n- The stress-free 7-hour journey from Paris to Milan with the world-class Frecciarossa, passing via the idyllic French countryside\n- The Glacier or Bernina Express, touring via the majestic Swiss Alps into Northern Italy\n↓ Join the neighborhood ↓\nThe Travel Off Path Community FB group has all the most recent reopening information, conversations, and Q&A’s occurring day by day!\nSUBSCRIBE TO OUR LATEST POSTS\nEnter your electronic mail handle to subscribe to Travel Off Path’s newest breaking journey information, straight to your inbox\nDisclaimer: Current journey guidelines and restrictions can change with out discover. The determination to journey is in the end your duty. Contact your consulate and/or native authorities to verify your nationality’s entry and/or any adjustments to journey necessities earlier than touring. Travel Off Path doesn’t endorse touring towards authorities advisories']	['<urn:uuid:831fc3ac-0e58-4006-ae6b-855aaf77d11e>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T05:28:56.360010	37	123	1822
27	How do treatment costs compare between hospice and palliative care?	For palliative care, coverage through Medicare, Medicaid or private insurance may vary depending on the type of care provided, and some services may require private payment. There are funds available for those who qualify based on financial need. For hospice care, the coverage is more comprehensive - it is covered for most patients by Medicare, Medicaid or private insurance, with financial assistance also available for qualifying individuals. The Hospice Medicare Benefit specifically covers services for Medicare beneficiaries who choose hospice care and have a life expectancy of 6 months or less if the illness follows its normal course.	"['What Type of Care is Right for You?\n|Palliative care||Hospice care|\n|For anyone with a serious or chronic illness||For anyone who has a life-limiting condition and a life expectancy of six months or less|\n|Can be provided in addition to treatment that is intended to cure the illness||Available to people who are no longer seeking curative treatment and whose focus is comfort|\n|Goals: Help patient live as independently as possible; manage complex symptoms; prevent or treat pain; restore comfort; reduce stress; help patient comply with treatments, medications and doctor’s instructions; identify community resources which may provide additional support||Goals: Help patient and family meet their unique end-of-life goals; manage complex physical symptoms and improve comfort; address emotional and spiritual concerns; educate patient and family about what to expect at the end of life|\n|Visits provided at home, in a long-term care facility or at a clinic||Care usually provided where the patient lives—in their own home, a long-term care or assisted living facility or a residential hospice center|\n|Care team may include physicians/nurse practitioners, nurses and social workers and hospice counselors||Care team often includes physicians/nurse practitioners, nurses, certified nursing assistants, social workers, hospice counselors, pharmacists, trained volunteers and others|\n|Coverage for care by Medicare, Medicaid or private insurance may vary according to the type of care provided; some services may require private pay. Funds available for those who qualify based on financial need||\nCovered for most patients by Medicare, Medicaid or private insurance\nFunds available for those who qualify based on financial need\nAgrace’s palliative care specialists can help in two ways:\n- Agrace provides in-home palliative care consultations to identify the medical, emotional and social issues that need to be addressed to improve the patient’s quality of life.\n- Agrace Care Navigation can help people manage a serious illness more confidently at home with guidance and education from a registered nurse. The monthly cost of the service is often offset by the savings that can occur by reducing the need for emergency department visits or repeat hospitalizations.\nAgrace has provided compassionate care to residents of south central Wisconsin since 1978.\nQuestions about which type of care is right for you? Please call (800) 930-2770. Our helpful admissions staff is just a phone call away, day or night.\nManaging Chronic Illness\nLiving with a chronic illness can be a stressful challenge. You may struggle to stay comfortable, even while taking multiple medications. Each new symptom or side effect that develops can set off a wave of worry and uncertainty. Besides keeping your physical symptoms under control, you may other concerns, such as:\n- managing medicines\n- coordinating appointments with doctors\n- understanding instructions from doctors and other caregivers\n- getting to and from medical appointments\n- losing independence\n- needing more help with personal care\n- knowing what to expect from the illness\n- managing stress and/or family concerns\n- paying for care\nAgrace can guide you and your family through the many challenges of living with a serious illness. To get started, Agrace’s experts can meet with you where you live to evaluate your condition, learn about your concerns and recommend the types of support that could be most helpful to you.\nTo request a visit from an Agrace nurse, please call us in Madison or Janesville at 800-930-2770 or click here to send us an email. You may also ask your doctor to contact us on your behalf, if you prefer.\nAgrace HospiceCare - The best Place to Work\nAgrace HospiceCare was named ""#1 Best Place to Work"" by Madison Magazine and WISC-TV 3. Read the full article to find out why.\nVolunteer Your Time\nVolunteers are at the heart of Agrace\'s mission and help us to provide the best service to patients, families, and our staff. There are many different volunteer roles to choose from depending on you skills, interests, and schedule.\nPrior to volunteering, new volunteers complete: a phone interview, application, 2-step TB testing (except Thrift Store Volunteers), background checks, references, orientation, and role specific training.\nMost volunteer roles require a volunteer commitment of 50 hours within the first year of service. Some one-time opportunities are available for individuals or groups.\nDoc Rock Cafe volunteer, Judy Seip, shares why volunteering at Agrace is important to her.\n""Empathizing with people and just\nAgrace Volunteer Information Center\nOnline scheduling for current volunteers.\nFor more than 35 years, Agrace has been a nonprofit, community based health care agency dedicated to providing expert physical, emotional and spiritual support to patients and families through the stages of serious illness. Agrace HospiceCare is nationally recognized for the comprehensive hospice services we provide in patients\' homes, the Agrace HospiceCare inpatient unit and skilled nursing and assisted living facilities throughout south central Wisconsin.\nNow Agrace offers palliative care services to provide patients more options for feeling better, even while receiving treatment.', 'This article was originally published by Barbara Rubel on OpenToHope on Wednesday, October 12, 2011\nWhat is Palliative Care and Hospice?\nIf you have been told that your loved one is terminally ill, this article will help you identify palliative care, hospice, advanced care planning, Five Wishes, and questions to ask during this difficult time. Let’s first look at palliative care,which helps individuals improve their quality of life by providing prevention and relief of suffering, early identification, holistic assessment and treatment of pain, and support for physical, psychosocial, spiritual and bereavement issues (WHO, 2008). Hospice, on the other hand, offers care when curative medical treatments no longer enhance quality of life. Although Hospice is most often provided only at the very end of the terminal illness, it can be provided at any point once the patient is told they are terminally ill. The Hospice Medicare Benefit specifies the services to be provided to Medicare beneficiaries who choose to receive hospice care if they have a medical prognosis with a life expectancy of 6 months or less if the illness runs its normal course (www.hospicefoundation.org)\nWhat is Advance Care Planning?\nYou might be struggling with whether or not to tell your loved one that he or she is dying. As a hospice bereavement coordinator I helped many families break the bad news. Ira Byock, author of The Four Things That Matter Most, maintains that people who are aware they are dying, can improve relationships in their life by saying: Please forgive me, I forgive you, thank you, I love you and good-bye. In anticipation of death, advance care planning is essential if a person’s preferences for end-of-life care are to be communicated and honored.\nAdvance care planning involves decision making, expressing treatment preferences, and completing documents that communicate the patient’s values and beliefs for their health care when they can no longer speak for themselves. An important conversation is medical power of attorney which is a health care proxy or health care surrogate. It allows your loved one to name a representative to make health care decisions on their behalf should he or she become physically or mentally incapacitated. Is that person you?\nWhat Does Your Loved One Wish For?\nDo you remember as a child asking for three wishes? Well now you have an opportunity to ask for five. Five Wishes has become America’s most popular living will because it is written in everyday language and helps start and structure important conversations about care in times of serious illness. Five Wishes is available at Aging with Dignity, and will guide your loved ones who are terminally ill in speaking with their loved ones about their wishes should they not be able to speak (http://www.agingwithdignity.org). Five Wishes lets the family and doctors know:\n- Who you want to make health care decisions for you when you can’t make them.\n- The kind of medical treatment you want or don’t want.\n- How comfortable you want to be.\n- How you want people to treat you.\n- What you want your loved ones to know.\n20 Questions to Ask Your Terminally Ill Loved One\nAs a Bereavement Coordinator for hospice, I was privileged to sit at the bedside of many terminally ill patients and asked them the following questions. Look over the list and use the questions as a guide for the conversation you will have with your loved one.\n- Do you feel as though you are being including in your health care decisions?\n- What are you most afraid of?\n- What are you most worried about?\n- Do you worry about becoming a burden to anyone in particular?\n- Is there anything that is making you feel uncomfortable?\n- What is most difficult about leaving your loved ones behind?\n- What do you think will happen to your loved ones after your death?\n- Are there any relationships you want to mend?\n- What tasks do you need to complete before you die?\n- Would you prefer to die at home or in the hospital?\n- What does a good death mean to you?\n- What brings you the greatest sense of comfort?\n- What are you most proud of?\n- Do you have any regrets?\n- What cultural beliefs sustain you?\n- What is your role in the family?\n- What role has faith played during your illness?\n- Is there one thing that you want to pass along to those left behind?\n- What does your illness mean to you?\n- What is the meaning of your life?\nMy hope is that this article has helped you with some of the issues you are facing. Open to Hope has many other articles on coping with loss. Take the time to read what you can and know that you are not alone.\nFor more on Barbara Rubel and OpenToHope, see http://www.opentohope.com/20-questions-to-ask-your-terminally-ill-loved-one/']"	['<urn:uuid:8ec6df8e-e4a4-499c-afa8-3ffed4e21ba5>', '<urn:uuid:9dfa0151-ecb3-4b8b-b29c-a1bdba616319>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T05:28:56.360010	10	98	1618
28	gold crown teeth benefits bacteria	Studies have shown that certain gold composite metal crowns (specifically Captek) can reduce bacterial presence by up to 91% compared to normal tooth surfaces and approximately 96% less bacterial adhesion compared to ceramic-fused-to-noble-metal restorations. This composite gold alloy material is made of 97% gold reinforced with platinum/palladium particles, does not oxidize, and provides better tissue health compared to traditional crown materials.	['Increasing attention is being given by our patients to their physical health. With increasing life expectancy they have a great desire to live their remaining years feeling well, maintaining a good appearance, and a healthy smile.\nORAL-SYSTEMIC HEALTH: A BRIEF BACKGROUND\nCoronary heart disease (CHD) is the leading cause of death and morbidity in the United States and many developed countries. Nearly 500,000 people in 2002 died from CHD and millions of others are living with it.1 It is estimated that, in the United States in 2006, heart disease cost more than 258 billion dollars in health-related costs and lost productivity, affecting more than 80 million adults, with 38.2 million estimated to be less than 60 years of age.2\nRecently, chronic inflammation has been implicated etiologically in CHD and cardiovascular disease.3 Periodontal disease, a form of chronic inflammation, affects tooth-supported structures, with an estimated prevalence of as high as 75% of adults in the United States.4 The disease is now associated with elevations of blood markers which signify chronic inflammation.5 Because of the evidence implicating chronic inflammation in the etiology of CHD, and possible etiologic relationship between periodontal disease and CHD, every dental treatment procedure should be directed toward addressing and reducing inflammation.\nSince many of these patients identified with cardiac risk want to maintain or improve their smile, cosmetic dental materials, which have antibacterial, anti-inflammatory properties, must be utilized. Full crowns with antibacterial subgingival margins should be the preference for this category of patients. This article will focus upon this topic of discussion.\nPERIODONTAL EFFECTS OF RESTORATIVE MATERIALS AND MARGIN PLACEMENT\nUnfortunately, dental restorations have been considered a major contributory factor in the etiology of periodontal disease, as discussed by Loe6 in his classic 1968 paper. His paper reviewed the reactions of the periodontal tissues to restorative procedures, and the effect of these restorative materials on the periodontal tissue. He believed and stated that any known type of dental restoration that extends into the subgingival area causes damage to the periodontal tissue, either by providing possibilities for bacterial retention, and/or by a direct irritation effect from the material. In the author’s opinion, if the connection between periodontal disease/ inflammation and cardiac disease is a valid one, then these procedures must either be avoided wherever possible or, they should be done using a periodontal-friendly material that reduces the bacterial load (contamination).\nBecause of the possible harm to the periodontal tissue, it has been suggested that margins of full crowns, when done, be placed coronal to the gingival margin.7 However, Austin et al,8 suggested such a placement is often contraindicated due to aesthetic requirements, subgingival caries, existing subgingival restorations, or a short clinical crown. With the patient’s primary concern being aesthetics, it is obvious that most margin placements must now be subgingival.\nSubgingival margins are of great concern because of the potential problems that can occur (Figure 1). Waerhaug9 suggested that the subgingival margin facilitates bacterial plaque retention, which could later contribute to the destruction of the periodontal supporting apparatus. Silness,10 20 years following Waerhaug, in 1980, described subgingival margins to exhibit more severe gingival lesions and deeper gingival pockets than margins even-with or above the gingival crest. Considering the possible connection between periodontal and cardiac disease, these effects of cosmetic treatment must be avoided at all costs to reduce the risk of systemic disease occurring from oral conditions.\nAmsterdam11 wrote a classic treatise on periodontal prostheses, more than 30 years ago, establishing the standard of care for a crown. Most critically, he described the optimal margin/finish line. When full-coverage restorations were indicated, this margin/finish line should be placed (in a healthy sulcus) at minimal depth, just short of the junctional epithelium; or completely away from the sulcus, preferably on the anatomic crown. He suggested that the least desirable location to place a margin is just supragingival, where the area of greatest plaque accumulation occurs. Finally, in guidelines still followed today, he suggested that to prevent plaque buildup it is necessary to create optimal crown contours with proper coronal form, embrasure form, and good subgingival fit at the margin (Figure 2).\nDespite this excellent treatise, one which defined the standard of care for the placement of full crowns, Morman, et al12 reported that gingival inflammation could result from the placement of gold inlays, even with perfectly adapted and well-polished margins. With the most precise techniques utilized to avoid adverse effects―including proper preparation, impressions, well-fitted provisionals, and definitive crown restorations,―there could still be gingival irritation and increased plaque retentions.\nRESTORATIVE MATERIALS VERSUS INFLAMMATION\n|Figure 3. Goodsen, et al13 clinical study results comparing surface material of tooth versus amount of bacteria present in sulcus.|\nFollowing the standard of care established by Amsterdam,11 all dentists should consider using materials that will maintain good periodontal health with optimal fit, reducing possible periodontal risks for systemic disease. When the need exists to employ full-coverage crowns, Captek (Precious Chemicals) provides one example of a cosmetic restorative crown material available that can help satisfy the goal of excellent health. This ceramometal crown incorporates the use of a gold composite metal coping. Goodson, et al13 (Figure 3) documented a reduction of up to 91% in the number of bacteria observed surrounding Captek restored teeth versus normal tooth surfaces in the same mouth. Additionally, there was approximately 96% less bacterial adhesion compared to ceramic-fused-to-noble-metal restorations.\nChronic inflammation in the periodontal tissue has been traditionally reduced with nonsurgical periodontal care, involving scaling, root planing, and effective oral hygiene to remove the bacteria plaque responsible for the problem. It has always been understood, following the suggestions of Amsterdam,11 that proper placement would help to prevent plaque buildup, in effect reducing inflammation. With significantly positive tissue responses to crown materials, dentists can now achieve predictable aesthetic restorations with a healthy supporting periodontium. These crowns might best be described as “periodontal crowns,” since healthy tissue results from placement on any affected teeth. In the author’s opinion, this type of crown management should be followed, if there is even the smallest possibility of reducing periodontal risk for cardiac disease with placement of these crowns.\nCOMPOSITE GOLD ALLOY MATERIAL DESCRIPTION AND PROPERTIES\nThe Captek composite gold alloy coping is fabricated using a capillary casting technique. It is not a traditional gold alloy, but is best described as a composite metal―of 97% gold reinforced with small particles of a very high-fusing, high-strength, platinum/palladium core.14 The resulting warm hue presents an ideal background for natural/vital porcelain aesthetics in the cervical areas of restorations, virtually eliminating tissue shadowing often caused by the gray color of traditional cast ceramometal alloys. The material does not oxidize, which can create dark color reactions with porcelain, creating a weak link at the bonding surface. Captek has a small particle size under 15 to 20 µm, allowing margins to be finished to a fine edge without a loss of integrity. With no oxides15 or gray metal to mask out, only 0.05 mm is required for a proper opaque layer compared with an average of 0.3 mm on cast metal.\nBecause of the bacterial inhibition properties, the author prefers a chamfer bevel with a prescribed small metal collar used in all nonaesthetic areas, generally from midproximal (Figure 4).13,16-18 These margin designs also work well with bridges that in the past restricted pontic length to 15 mm. However, an all new Captek material, Bridge & Implant, has been reinforced with twice the number of platinum/palladium particles, making it more than 30% stronger than original Captek.19 Developed using nano technology and in use for over 2 years, this stronger Captek can be used for cantilever and long-span bridges offering at least the same level of fit and plaque inhibition as the original Captek.19\nTYPICAL CLINICAL CASE EXAMPLE\nThis example patient case shows a concern about tissue health, aesthetics, strength, and fit.\nA 45-year-old female presented with congenitally missing teeth maxillary lateral incisors; teeth Nos. 7 and 10. Previously she had had dual-cantilever bridges made to replace these teeth. After consultation, it was decided to complete two, 3-unit bridges for teeth Nos. 6 to 8 and 9 to 11. When the old crowns were removed on the upper left side, the inflammation of the tissue where the previous fitted semiprecious metal crowns were in place was obvious. With the bilateral placement of “periodontal crowns” (using Captek composite metal in this case), the improved fit and tissue health is easily visible―even 4 years after placement (Figure 1).\nCONCLUDING REMARKS: SHOULD THE STANDARD OF CARE CHANGE?\nWhile Dr. Amsterdam11 established the initial standard of care for crown design and margin placement, clinical studies and case histories now dictate an amendment of this classic standard. Whenever any full-crown restoration is considered or required for a tooth (with bleeding in the tissue when probed or where periodontal disease is present) it is now suggested, based on the knowledge of crown contour and possible reduction of tissue inflammation, that using periodontally healthy crowns is an appropriate indication. Because the crowns (Captek) discussed in this article have been shown to be one example of a material that reduces plaque on the associated tooth (per the studies of Goodsen, et al13 cited previously), perhaps the term “periodontal crown” is a very appropriate one to use.\nIn the author’s opinion, restoring a tooth with a material that has been shown to reduce the inherent risk of additional plaque retention should be considered the treatment of choice. Use of these crowns, in the presence of periodontal disease, should be routine. Should this be a future standard of care? The hope is that more attention will be given to study materials that can provide the periodontal benefits of a hybrid gold. When considering the possibility that risk of cardiac disease and adult onset diabetes might be reduced with periodontal crown treatment, the future is exciting!\n- Kochanek KD, Murphy SL, Anderson RN, et al. Deaths: final data for 2002. Natl Vital Stat Rep. 2004;53:1-115.\n- Thom T, Haase N, Rosamond W, et al. Heart disease and stroke statistics—2006 update: a report from the American Heart Association Statistics Committee and Stroke Statistics Subcommittee. Circulation. 2006; 113:e85-151.\n- Ridker PM, Hennekens CH, Buring JE, et al. C-reactive protein and other markers of inflammation in the prediction of cardiovascular disease in women. N Engl J Med. 2000;342: 836-843.\n- Brown LJ, Brunelle JA, Kingman A. Periodontal status in the United States, 1988-1991: prevalence, extent, and demographic variation. J Dent Res. 1996;75:672-683.\n- Noack B, Genco RJ, Trevisan M, et al. Periodontal infections contribute to elevated systemic C-reactive protein level. J Periodontol. 2001;72:1221-1227.\n- Loe H. Reactions of marginal periodontal tissues to restorative procedures. Int Dent J. 1968;18:759-778.\n- Richter WA, Ueno H. Relationship of crown margin placement to gingival inflammation. J Prosthet Dent. 1973;30:156-161.\n- Austin GB, Vogel R, Deasy M, et al. Effect of rough and smooth margins on interproximal gingival health. Clin Prev Dent. 1979;1:19-26.\n- Waerhaug J. Histologic considerations which govern where the margins of restorations should be located in relation to the gingiva. Dent Clin North Am. 1960;5:161-176.\n- Silness J. Fixed prosthodontics and periodontal health. Dent Clin North Am. 1980;24:317-329.\n- Amsterdam M. Periodontal prosthesis: Twenty-five years in retrospect. Alpha Omegan. 1974;67:8-52.\n- Mormann W, Regolati B, Renggli HH. Gingival reaction to well-fitted subgingival proximal gold inlays. J Clin Periodontol. 1974;1:120-125.\n- Goodson JM, Shoher I, Imber S, et al. Reduced dental plaque accumulation on composite gold alloy margins. J Periodontal Res. 2001;36:252-259.\n- McLaren E. Forward to the past: a renaissance in ceramometal technology. Contemp Esthet Restor Pract. 1998;2(spec issue):6-13.\n- Zappala C, Shoher I, Battaini P. Microstructural aspects of the Captek alloy for porcelain-fused-to-metal restorations. J Esthet Dent. 1996; 8:151-156.\n- Shoher I, Whiteman A. Captek: a new capillary casting technology for ceramometal restorations. Quintessence Dent Tech. 1995; 18:9-20\n- Knorr S, Combe EC, Wolff LS, Hodges JS. The Surface Free Energy of Gold Alloy Systems. Abstract presented at: 32nd Annual Meeting of the American Association for Dental Research; March 2003; San Antonio.\n- Wynne WPD. Margin Design in the Most Overlooked Aesthetic Zone. Dentistry Today. October 2006;25:126-129.\n- DiTolla M. From the Lab. Dental Economics. 2007;97:44.\nDisclosure: Dr. Gottehrer reports no conflict of interest.']	['<urn:uuid:01cd34f1-f4a2-4d29-827a-9196d29163ed>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-13T05:28:56.360010	5	61	2021
29	What makes measuring pollution particles from engines challenging, and who are the people most vulnerable to their harmful effects?	Traditional measurement methods using filter paper are unreliable for particles smaller than 20 nm which can slip through, and errors occur due to particles sticking to extraction tube walls. Additionally, these methods don't provide immediate results or imaging information. Regarding vulnerability to these pollutants, children, the elderly, pregnant women, and those with compromised immune systems or illnesses are especially at risk, with recent studies showing higher exposure and health risks for people living near ports and roadways.	['LASER INDUCED INCANDESCENCE FOR SOOT MEASUREMENT\nWhen monitoring the performance of jet engines, aerospace engineers typically are concerned with intake phenomena, such as how wind velocity, rain and shock waves affect the turbine at subsonic to supersonic speeds. But a growing awareness of the adverse health effects of air pollutants has led to a renewed interest in understanding what happens at the other end of the engine, at the exhaust. Tom Jenkins of MetroLaser Inc. in Laguna Hills, Calif., has devised a non-extractive measurement technique that uses laser-induced incandescence to obtain real-time images of soot concentration in engine exhausts and other flow fields.\nParticulate matter emission in the form of soot is one of the most significant pollutants from commercial jet engines. It originates from the incomplete combustion of fuel and is particularly problematic during takeoff, which requires high fuel consumption. Researchers have measured these emissions using an extraction technique in which they sample the exhausts through filter paper and analyze the soot they capture under a microscope. However, this approach is prone to errors due to particles adhering to the walls of the extraction tube, and is unreliable for particles smaller than 20 nm, which can slip through the filter. Moreover, the results are not available immediately, and the method does not yield imaging information.\n[caption id=”attachment_131″ align=”alignnone” width=”642″] Figure 1. Researchers have developed a laser-induced incandescence system to measure the concentration of soot in jet exhaust. The laser is located outside the test chamber, and the user remotely operates the system by computer.[/caption]\nFigure 1 shows a setup implemented at a Sea Level Gas Turbine Facility at Arnold Engineering Development Complex in Tennessee. A 1064-nm beam from a pulsed Nd:YAG laser is launched across the exhaust plume of an engine under test. Soot particles in the exhaust are heated by the 600-mJ laser pulses to about 4000 K, which causes them to emit incandescent light. The exhaust temperature depends on the throttle conditions, varying from just above 300 K to approximately 1000 K, but this background temperature is sufficiently lower than the incandescence temperature to not interfere with the measurement. To collect the signal from the heated particulates, the researchers employed an intensified CCD camera. It features a controllable intensifier gating to offer a 1000 times increase in gain. Both the laser and the camera timing are controlled with digital pulse generators that enable a precise delay to be applied to the camera gate pulse with respect to the laser. The camera gate is synchronized with the 20-Hz pulse rate of the laser to improve the signal-to-noise ratio, typically opening approximately 10-ns after the 8-ns laser pulse and remaining open for 50-ns.\nIn ground tests conducted at AEDC, the instrument imaged a 1-m path through an exhaust plume with a spatial resolution of 5 cm with a temporal resolution of 0.1 s. Data from these tests indicate that the soot concentration jumped to 4 mg/m3 immediately after the throttle was increased to full power and remained high for several seconds before dropping back to a steady state of 0.3 mg/m3 (Figure 2).\n[caption id=”attachment_132″ align=”alignnone” width=”900″] Figure 2. Data taken on a jet engine in operation show a transient spike in the soot mass concentration as the throttle setting jumps to maximum power.[/caption]', 'Air Pollution & Your Health\nOur long-term vision is for all people to benefit from clean healthy air all the time, everywhere. To achieve this, we target the largest sources of the most harmful pollutants in our region to protect public health.\nThreat of Air Pollution\nWe believe all people would benefit from clean healthy air all the time, everywhere. In our region, particle pollution, smog, and air toxics pose the greatest risk to our well-being. Outdoor air pollution can cause heart attacks, asthma, strokes, cancer, and premature death. An estimated 1,100 people die each year in Washington State due to outdoor air pollution.\nBecause we are concerned about our climate we also focus on the reduction of greenhouse gases, which are the leading cause of climate change. In our region, climate change will likely lead to warmer, drier summers which increase levels of smog pollution, posing health risks to those with lung and heart diseases.\nIf you have plans to be active outdoors and are sensitive to air pollution, check the air quality forecast on our website.\nLearn about how to construct a DIY air filter to help reduce particle levels in your home.\nWood Smoke & Your Health\nSmoke from fireplaces, wood stoves, backyard and land-clearing burn piles and wildfires contains fine particle pollution, which is one of the most serious air quality problems in the Puget Sound region. Fine particles are tiny, microscopic pieces of pollution that can easily enter your bloodstream and cause breathing and heart problems. The health effects even from short-term exposure are serious, and include:\n- Asthma attacks\n- Heart attacks\n- Premature death\nFine particle pollution is especially dangerous for children, the elderly and people with sensitive immune systems.\n- How Wood Smoke Harms Your Health (PDF) - Washington State Department of Ecology\n- Outdoor Air Quality - Washington State Department of Health\n- Particle Pollution - American Lung Association\n- Particle Pollution and Your Health - United States Environmental Protection Agency\n- Smoke From Fires - Washington State Department of Health\n- Wood Smoke - Your Health, Your Wallet and the Law (PDF)\nDiesel Exhaust & Your Health\nWhat Are the Health Effects of Diesel Exhaust?\nBreathing diesel exhaust can cause serious health problems. The tiny particles in diesel exhaust are highly toxic.\nDiesel exhaust represents 78% of the potential cancer risk from all air toxics in the Puget Sound area. It is also linked to respiratory and cardiovascular problems, such as:\n- Heart attacks\n- Premature death\nChildren, the elderly, pregnant women, and those with compromised immune systems or illnesses are especially vulnerable. Recent studies show people living near ports and roadways have higher exposures and health risk.\nHow can I protect myself from Diesel Exhaust?\nIf you live near freeways, highways, or near industrial areas check our website on how to construct a DIY air filter to help reduce particle levels in your home.\nWhat Are the Main Sources of Diesel Exhaust?\nThe majority of diesel exhaust in the Puget Sound region comes from goods and people movement, which can be broken down into the following transportation sectors:\n- Maritime vessels/ships: This sector consists of ocean-going vessels such as container ships, bulk carriers, and articulated tug barges and harbor vessels, such as tugboats, work boats, ferries, fishing vessels, and excursion vehicles.\n- Off-road equipment: This sector includes construction equipment, aircraft-support equipment, and cargo-handling equipment used at seaports, rail yards, distribution centers, and waste transfer stations.\n- On-road vehicles: This sector consists of trucks, vans, buses, waste haulers, and emergency-response vehicles.\n- Rail: This sector consists of locomotives that transport goods and people, including switcher, short-line, and line-haul locomotives.\nHow Is Diesel Exhaust Being Reduced?\nWe address diesel pollution through partnerships with other agencies that focus on reducing diesel pollution, administering incentive programs and projects that employ diesel emission reduction technologies, and help inform policies that encourage cleaner, low carbon solutions.']	['<urn:uuid:5ef08658-41e0-4abe-aa77-56de82d47159>', '<urn:uuid:b900df86-5be8-45b5-b268-d655ebf8d905>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T05:28:56.360010	19	77	1196
30	What considerations affect the optimal TV viewing experience in terms of resolution, screen size, and viewing distance, taking into account both human visual acuity and typical room setups?	The optimal TV viewing experience depends on multiple factors. For resolution, research by NHK shows that humans can perceive up to 310 pixels/degree, and people with 20/15 vision (which is common, especially in younger adults) require at least 160 pixels/degree for optimal viewing. This means 4K resolution is beneficial at closer distances than many people assume - for a 50-inch 4K TV, benefits are visible up to 8.6 feet away. Regarding screen size and viewing distance, there are established guidelines: smaller 32-inch TVs work well at 4.5 feet viewing distance in bedrooms or kitchens, while larger 65-inch TVs are suited for living rooms with 9.1 feet viewing distance. The Society of Motion Picture & Television Engineers recommends the TV should occupy at least a 30-degree arc in your field of vision. Additionally, proper digital sampling and filtering of content is crucial - this is why computer-generated content often appears sharper than camera-captured content at the same resolution.	['I read many articles that claim quadrupling the number of pixels with 4K over 1080p is subtle or requires a very large screen to see the benefit. During the course of my work I am seeing a considerable amount of 4K/UHD source material and display product. It is obvious to me that doubling the resolution in the vertical and horizontal plane (4K) done properly is actually a big improvement when done well. This article addresses the benefits of additional pixels above 1080p along with sample photos, experimental proof and theoretical analysis that this is the case.\nLet’s first look at a series of experiments done by NHK that compared a plaster bust, model ship and butterflies to a display. These results can be found in ITE Technical Report Vol. 35, No. 16. The summary of the results are shown in the chart above. NHK claimed the tests showed 310 pixels/degree are needed for an image to reach the limit for human resolution. If this is true we should be able to benefit from 4K and even higher resolutions without changing the viewing angles used for comfort reasons. At a THX recommended 36 degree viewing angle this corresponds to a 11K display to hit the 310 pixels/degree limit NHK observed. 1080p is only 53 pixels/degree and far short of what the tests indicate we can see at THX recommended viewing angels. My own observations of properly performing 4K displays using quality 4K sources are that 4K is almost as big an upgrade as 480p was to 1080p.\nAssuming these tests are correct 4K is useful out to about a 12 degree viewing angle. This corresponds to 17 feet for a 50 inch diagonal and 21 feet for a 60 inch diagonal display. Far beyond distances many people suggest for 4K. 1080p with the same criteria limits to about 6 degree viewing angle doubling the distances shown for 4K.\nThe ARRI film scanner results above show the results for a 2K, 4K and 10K film scanners. These scans are magnified from a section of 35mm film. I have heard in my work with the film industry that 12K is what is needed to replicate the best 35mm film. It is interesting that the NHK data indicated that 11K is needed to hit the limit of human vision for these applications.\nThe image above is a segment of 35mm film showing the difference in an image for 2K versus 4K. I have also seen “show prints” in Los Angeles which are prints that were made from masters instead of several copies removed. A 35mm show print in pristine quality is an amazingly detailed and high contrast image that exceeds what I have seen with 4K to this point, but it is getting much closer. Unfortunately, this media is extinct with the loss of film in theaters.\nHow can so many people be so wrong about 4K? I think it boils down to two false assumptions. The first is that vision is limited to 20/20 and second is that the digital media can capture all of the resolution at the resolution of the imager. Both of these are false. Here is a quote from an expert on human vision. August Colenbrander, M.D. (Smith-Kettlewell Eye research Institute and California Pacific Medical Center) “…emphasizes that, contrary to popular belief, 20/20 is not actually normal or average, let alone perfect, acuity. Snellen, he says, established it is a reference standard. Normal acuity in healthy adults is one or two lines better. Average acuity in a population sample does not drop to the 20/20 level until age 60 or 70. This explains the existence of the two lines smaller than 20/20: 20/15 and 20/10.”\nThe chart above shows the results of human visual acuity (corrected if necessary) versus age for several different experiments. It is obvious from the results that 20/20 is not where most people can be with their vision. The data is not as simple as this chart shows as these are trends among scattered results, but it is clear that the average for many people is closer to 20/15. This data clearly challenges the assumption that 20/20 is the limit for the typical person. I know in my own case my vision was about 20/10 when I was younger and has degraded to 20/15 at the current time. I am still better than 20/20 at 55.\nThe other important thing to realize is that one of the primary benefits of 4K is the fact that it reduces the image artifacts and softening caused from digitally sampling the analog world. The dominant theory for this was proposed in 1928 by Nyquist. This theorem simplistically states that when you sample a system digitally you need to sample it at twice the frequency of the information you are interested in at a minimum. This limit is why CD’s are sampled at 44kHz to be able to to pass 20kHz which is the common limit for human hearing. 4K has the bandwidth to allow the camera or film scanner to transmit an image properly to get a 1080p resolution on your 4K display if everything is done properly. This digital sampling problem is the reason that an animated movie from Pixar looks so much sharper on Blu-Ray than the best camera image which must be filtered to meet Nyquist to avoid horrible artifacts. Many 1080p cameras do not conform to Nyquist and have insufficient filtering which results in many image artifacts. Computer animation is not subject to the analog sampling problem and does not need to have sampling rate artifacts at 1080p because the image is actually rendered at 1080p and not sampled from the analog world.\nTaking these two pieces of information into account one can calculate the number of pixels per degree required for a person with 20/15 vision which would be considered relatively common. 20/15 vision corresponds to 0.75 arc minute of resolving power. 2 pixels are required at the resolution limit at a minimum to meet the Nyquist rule for digital sampling.\nPixels/Degree = (60 arc minutes/ degree)(2 pixels/0.75 arc minutes) = 160 pixels/degree\n(based on 20/15 vision & Nyquist sampling requirement)\nThe NHK data shown above indicates that more than 160 pixels is a benefit, but 160 pixels is where the chart takes a definite turn in slope. This indicates a more subtle improvement in pixels/degree beyond 160 pixels/degree. If you want to estimate where 4K is worth it based on this limit halve the distances above. This means a 50″ display is useful for 4K to about 8.6 feet, 60″ to 10.2 feet and a 50″ 1080p display is useful to about 17.2 feet. There are many likely reasons that more than 160 pixels per degree were required in the NHK experiment to hit the limit for human vision. These include factors like peoples vision can be better than 20/15 and less than optimal filtering from digital sampling increased the number of pixels required to avoid digital sampling degradation of the image. I would use 160 pixels/degree as the maximum distance for 4K at this time.\nI believe these calculations along with the samples and test results shown give strong proof that screen resolutions greater than 1080p can offer significant improvements in picture quality at distances found in many applications. Real world experience will depend on source quality, display quality and system setup. As we push higher resolution more careful attention to detail will need to be exercised to obtain all of the quality improvements available.\nLink to display resolution calculator based on your vision, display size and distance to the screen https://www.homecinemaguru.com/display-resolution-calculator/.', 'Many consumers consider multiple possible TV sizes when determining the size of their next TV. There is a wide range of TV sizing, spanning 24” to 75” for standard TV sizes. As a result, many find themselves considering what sizes TVs come in when looking for the best TVs in the market. To understand the different screen sizes, you should know the actual dimensions of the TV screen and the recommended viewing distance for viewers to sit away from the TV.\n- Brands manufacture smaller-sized TVs in the 24 inches, 32 inches, and 40 inches sizes, though fewer and fewer make 24-inch TVs today.\n- Consumers find displays around 55 inches, 60 inches, and 65 inches for medium-sized TVs to be the right size.\n- Larger-sized TVs measure about 65 inches, 70 inches, and 75 inches, though several models are larger than 75 inches.\nMost Common TV Sizes\nWhen buying a new TV, you should consider what size you want and what room the TV would be going in. The most common TV sizes include:\n- Smaller size TVs are perfect for a smaller room, such as the bedroom, guest room, or kitchen. These are also ideal if you have to move a TV around a lot using one of the best rolling TV stands, as it won’t weigh too much.\n- 24” – People consider 24-inch TVs to be “small” in size. The height is roughly 11.7 inches, and the width is 20.9 inches. The viewer should sit just over 3 feet away from the TV. Some manufacturers no longer make 24” displays, but if you’re set on 24 inches being the perfect size, the best 24 inch TV is still on the market.\n- 32” – These 32 Inch TVs are still considered small size. 32” TVs offer the same comfort as the 24 inches but with a slightly larger screen. These TVs typically measure around 15.7 inches tall by 27.9 inches wide. For a field of view of 30°, you may want to sit about 4.5 feet away. Many manufacturers make 32” TVs as their smallest size.\n- 40” – Many 40-inch TVs are considered to be small in size as well. These TVs typically stand about 19.6” tall by 34.9” wide, and the guidelines for a 40” TV suggest sitting approximately 2.5 feet away. You can read our Sansui 40 Inch 1080P FHD DLED TV review for a good budget option.\n- Medium size TVs are perfect for smaller living rooms and other medium-sized rooms. Many consider these TVs ideal for narrower rooms as well.\n- 55” – A 55-inch TV screen comes in at 47.9 in. wide and 27 in. tall with a viewing distance of 7.7’. Check out our guide on the best 55-inch TV for some great options.\n- 60” – Standard 60-inch widescreen TVs are approximately 29.4 inches tall with a width of 52.3 inches. Therefore, a viewing distance of 8 feet follows the guidelines for the best 60-inch TV.\n- Many living rooms and outdoor areas feature larger TVs, many of which include the best features.\n- 65” – The 65-inch TV is one of the larger TVs widely available. It measures about 31.9”x56.7” and has a recommended viewing distance of 9.1 feet. These larger sizes are great for both the living room and the workplace as well in case you need to present something and are looking for something like the best conference room TV. You can read our Samsung UN65MU8500 review for a quality, curved screen option.\n- 70” – Many 70” TVs are about 34.3×61 inches, while the recommended viewing distance is about 9.8 feet. If you have the room, something like the best 70″ TV will be a great addition to your space.\n- 75” – A 70-inch TV remains one of the bigger TVs available, though there are some that are substantially bigger. The 70” TV stands approximately 36.8 inches tall by 65.4 inches wide. The recommended viewing distance is about 10.2 feet away. Check out our Samsung QN75Q8FN review for a 75″ TV. If you want to see what a larger TV offers, you can read our LG OLED77C9PU review for a 77-inch model.\n|TV Size (inches)||Dimensions (W x H, inches)||Recommended Viewing Distance (feet)||Ideal Location|\n|24”||20.9 x 11.7||3+||Bedroom, Guest room|\n|32”||27.9 x 15.7||4.5||Bedroom, Kitchen|\n|40”||34.9 x 19.6||2.5||Small Living Room|\n|55”||47.9 x 27||7.7||Living Room|\n|60”||52.3 x 29.4||8||Living Room|\n|65”||56.7 x 31.9||9.1||Living Room, Workplace|\n|70”||61 x 34.3||9.8||Large Living Room|\n|75”||65.4 x 36.8||10.2||Large Living Room|\nIt should be noted that if you see a class the size of a 40″ class TV, it means that the TV is not actually a 40-inch TV. We have a great article on what class means on a TV that you can check out as well. Knowing this can also help you understand how to choose a TV screen size and weight, among other considerations.\nIn that case, you may want to read our list of the best TVs for the bedroom.\nAdditionally, knowing the size and weight of the TV helps tremendously if you are going to be mounting it on the wall. Just be sure to take into consideration if it is a curved TV, as this could affect where you mount it.\nMounting a TV is also one option that is great when figuring out how to reduce glare on a TV if you are having that issue.\nCalculating TV Screen Sizes\nTo calculate the size of a TV screen, you would take the length of the diagonal, which is stated by the size in the name, and the most common aspect ratio for TVs, which is 16:9. You could then use the Pythagorean Theorem to find the approximate size of the screen. Be sure to add a few inches on each side to accommodate the TV’s bezel. You may also want to find out what is streaming tv and how it works as it will also influence the TV size you purchase.\nWhen considering image quality, personal preference, and price range, always remember that the bigger the TV size, the higher the cost. However, it does not necessarily mean that image quality will also be better with larger sizes since image resolution also plays an important role in image quality. Take a look at different television models and their features to find one that meets your needs.\nBe sure to add a few inches on each side to accommodate the TV’s bezel. You may also want to find out what is streaming tv and how it works as it will also influence the TV size you purchase.\nSTAT: According to the Society of Motion Picture & Television Engineers (SMPTE), your TV should occupy at least a 30-degree arc in your field of vision. (source)\nWhat is the viewing angle?\nViewing angle refers to the amount of your view the screen takes up. It is also referred to as the field of view.\nWhy is viewing distance significant?\nViewing distance guidelines help to prevent eye strain and other eye problems. Additionally, you can view the entire screen at these distances without having to turn your head a lot.\nWhat is the most popular TV size?\nIn 2019, the most popular TV size in North America was 65 inches.']	['<urn:uuid:03206799-cdc2-4b67-a2f8-514e84821f23>', '<urn:uuid:4bf48024-a2f3-483e-b40b-9849e5625d38>']	open-ended	direct	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-13T05:28:56.360010	28	157	2479
31	I'm fascinated by historical trade routes. How did ancient traders from the East typically reach Jerusalem, and were Persian merchants common visitors there?	During the period around 4 B.C., it was actually very unlikely that traders or travelers from the Parthian Empire (Persia) would have visited Jerusalem, despite common beliefs. This is because the Roman Empire and Parthians were in serious conflict during this time. If Parthian travelers did attempt the journey, they would have had to travel via Damascus and would have approached Jerusalem from the north, not the east. The more historically likely traders coming from the east would have been the Nabataeans, who operated from their capital of Petra (near Amon, Jordan) which was directly east of Jerusalem.	['The Real Wise Men from the East (the Journey of the Biblical Magi and the Adoration of Christ)\nThe gospel of Matthew 2:1–12 describes the visit of the Magi to Bethlehem and their adoration of Jesus the Christ. The magi (and the shepherds) of Bethlehem are the first people to adore Mary’s infant son Jesus as the promised Christ of God, the messiah and savior of the humankind:\n“Now when Jesus was born in Bethlehem of Judea in the days of Herod the king, behold, there came wise men [Greek: magos or magi] from the east to Jerusalem, Saying, Where is he that is born King of the Jews? for we have seen his star in the east, and are come to worship him. When Herod the king had heard these things, he was troubled, and all Jerusalem with him. And when he had gathered all the chief priests and scribes of the people together, he demanded of them where Christ should be born. And they said unto him, In Bethlehem of Judea: for thus it is written by the prophet, And thou Bethlehem, in the land of Judah, art not the least among the princes of Judah: for out of thee shall come a Governor, that shall rule my people Israel. Then Herod, when he had privily called the wise men, enquired of them diligently what time the star appeared. And he sent them to Bethlehem, and said, Go and search diligently for the young child; and when ye have found him, bring me word again, that I may come and worship him also. When they had heard the king, they departed; and, lo, the star, which they saw in the east, went before them, till it came and stood over where the young child was. When they saw the star, they rejoiced with exceeding great joy. And when they were come into the house, they saw the young child with Mary his mother, and fell down, and worshiped him: and when they had opened their treasures, they presented unto him gifts; gold, and frankincense, and myrrh. And being warned of God in a dream that they should not return to Herod, they departed into their own country another way.” Matthew 2:1-12 KJV\nWestern medieval traditions have greatly embellished Matthew’s testimony declaring the wise men to have been “three kings”, named Melchior (a Persian, i.e., Parthian Zoroastrian King and/or Priest or an Arbian who brought the gift of gold), Caspar (a black king from either Africa or India) and Balthazar (described as either a king of Arabia or a scholar from Babylon who brought the gift of Myrrh), and the three were formally canonized (sainted) by the western church. These names were derived from a Greek manuscript composed around A.D. 500 in Alexandria, Egypt which has been translated into Latin with the title Excerpta Latina Barbari.\nDespite the medieval conjecture it is very unlikely that the “wise men from the east” came from or through the Parthian Empire. Around 4 B.C. the Roman empire was at odds with the other regional superpower of the time, the Parthians. In 64 B.C. Pompey invaded and took Syria and established it as a Roman Province. In 53 B.C. the Roman triumvir Crassus attempted to invade and conquer the Parthian empire and he was soundly defeated at the Battle of Carrhae and he lost multiple legionary battle standards (aquilae) to the Parthians (which was an important shame to the Romans). In 44 B.C. Julius Caesar was amassing troops in the East and about to depart Rome to launch a war of retribution and conquest against the Parthians when he was assassinated (and the Parthians knew about his plan). In 42 B.C. the Parthians supported the assassins Brutus & Cassius when they were defeated at the twin Battles of Philippi, after which the Parthians invaded the Roman Province of Syria and added it to their empire. In 39 B.C. Mark Anthony’s general Bassus drove the Parthians out of Syria and re-established it as a Roman Province. In 36 B.C. Mark Anthony invade the Parthian empire in a new war of retribution and conquest and he too was soundly defeated by the Parthians at the Siege of Phraaspa.\nTravelers from the Parthian Empire would have traveled via Damascus and from a Jewish perspective would have been said to come from the north, not the east. It is exceedingly unlikely that the biblical wise men were “Persians.” The far more probable, and historically likely reality is that the “wise men from the east” were wealthy Semitic Arab traders from the northern Arabian people of Nabataea (formerly the territory of the biblical Edomites). The Nabataeans prospered in Jordan, the southern Levant, the Sinai Peninsula and northwestern Arabian Peninsula from sometime before the time of Antigonus I (see 301 B.C.) and the Nabataean capital of Petra (near Amon, Jordan) which is due east of Jerusalem. These Nabataean traders were well known for mining gold in Nabataea, and trading in frankincense, and myrrh from Arabia Felix (southern Arabia around modern day Yemen) and fine silks (from China, see the silk routes c. 400 B.C.). One of the two southern maritime silk route terminal ports was the Nabataean port of Leuce Come (on the Red Sea; the other being the Egyptian port of Myos Hormos) The Nabataeans had become prosperous carrying gold, frankincense, myrrh and fine Chinese silks from Petra and Leuce Come to resell them at the Mediterranean port of Gaza. This Nabataean trade is well documented by the Greek historian Diodorus Siculus (50 B.C.) & the Greek geographer Strabo.']	['<urn:uuid:63bc3b71-cbd7-4369-b0dc-d63543a5456f>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T05:28:56.360010	23	98	931
32	I understand that SOS emergency evacuation service is different from regular travel insurance. Could you explain what specific medical assistance services SOS provides for travelers under 75 in Southeast Asia?	SOS provides several medical assistance services including: arranging hospital admission, emergency medical evacuation to the nearest appropriate hospital, monitoring medical conditions during and after hospitalization, guaranteeing medical expenses during hospitalization, delivering essential medicine not available at the traveler's location, arranging emergency medical repatriation to the home country after hospitalization, transportation of mortal remains, arrangement of compassionate visits, and return of minor children. They also provide contact information for local medical facilities and help coordinate with travel insurance companies for reimbursement.	['Things can happen when you’re travelling that are out of your control, so insurance comes in handy. Considering the unpredictable nature, remote destinations and conditions encountered on Indochina Pioneer tours it is required to have travel medical insurance, and trip cancellation insurance is highly recommended. If you’re going to spend your hard-earned money to enjoy your holiday – why not spend a bit more to safeguard it?\nUpon starting your tour, the Indochina Pioneer leader or guide will ask everyone to provide a copy of their insurance details to keep on record. Failure to provide this information can, and has, resulted in travellers being unable to continue on tour, so don’t leave home without it. Where can I purchase travel insurance?\nLet’s find out the nearby travel insurance agency where you are living or the most convenient way is to purchase insurance online. See our Travel Insurer section for more details.\nWhat insurance is required?\nTravel medical insurance is mandatory. With a wide variety of company, credit card and travel policies out there, how do you know if your coverage is sufficient for our style of adventure travel? Our minimum insurance policy coverage for medical emergency evacuation and repatriation is USD$200,000 (or equivalent in other currency). If your pre-existing coverage does not meet these needs, you will be required to purchase travel medical insurance.\nWhy should I get cancellation insurance?\nAdding cancellation insurance to your medical insurance is optional, but always recommended. By having ‘all-inclusive’ coverage you do not need to worry about emergencies that can happen any time, any where, before or during your travels. Including the cost of your airfare, as well as your tour(s), ensures that you do not have to worry about any unforeseen situations such as having to rush home to a family member or having a tour changed or cancelled because of a natural disaster.\nHow much does it cost?\nLess than you’d expect! Travel medical insurance is based on the number of days of coverage – so how long you will be away. All-inclusive insurance, which adds cancellation to your medical insurance, is based on the dollar value of the tours, airfare, hotel nights and other services being covered. Typical costs, in USD, on a 10-day tour valued at $1000 with airfare coverage at $1000 as well would be:\n›Medical insurance only for 10 days = around $50\n›All-inclusive insurance (medical + cancellation) for $2000 (tour + airfare) = around $130\nWe believe that your safety and enjoyment when travelling with us is very important. To purchase chase travel insurance online, we suggest the following insurers:› Cover . More\nChoose Cover-More Travel Insurance and benefit from top quality cover, affordable rates, professional service and fair claims handling. www.covermore.co.au\n› World Nomads\ntravel insurance is available to people from over 150 countries and is designed for adventurous travellers with cover for overseas medical, evacuation, baggage. www.worldnomads.com\n› Travel Guard\nMore than 12,000 travel companies through virtually every distribution channel in the travel industry sell Travel Guard.www.travelguard.comYou can also cantact us by email if you have any other questions about your insurance and booking a trip. Direct Customers: email@example.com. Website: www.indochinapioneer.com\nYOUR TRAVEL SAFETY\n› An Indochina Pioneer holiday opens up a world of excitement, wonder and opportunity. With that, there are inherent risks and rewards beyond which you could reasonably expect by staying at home. Some of the reasons you may be exposed to different and broader experiences and risks include:\n› Different countries having different laws and regulations governing transport, infrastructure, safety etc, and may be lacking when compared to laws you may have at home.\n› The itineraries we operate often times include strenuous activities and physical demands beyond which you may usually partake in at home. In addition, these activities are on occasion undertaken in remote and isolated areas.\n› Natural events such as hurricanes, earthquakes, storms, flooding etc can happen the world over. Sometimes they strike in locations in which we operate a tour. The remote location of some of our tours may compound the impacts beyond the natural event itself.\n› Most of the people you meet on your travels will be genuine, welcoming and honest. However, as anywhere, there is a small element preying on the unsuspecting tourist.\n› The remote locations of some of our tours mean support and assistance is not always readily available.\n› The infrastructure (hospitals, transport, emergency services etc) of some of the countries we visit may be lacking when compared to the ones you may have at home.\n› Indochina Pioneer takes all reasonable measures to ensure your safety and enjoyment while traveling with us. The aim of this document is to give you an insight into ways you may further enhance your own personal safety while abroad. “The door to safety swings on the hinges of common sense”.\nFirst things first\nThere are a number of things that you may do before even leaving home that will help you to have a safe and enjoyable travel experience:\n› Take copies of your passport and other important documents with you. This way, if you lose your documents, you can at least present a copy to your embassy or consulate while abroad and have replacement or temporary travel documents issued. A tip we received from one of our seasoned travelers was to scan the documents, and send them to your e¬mail. That way you have additional access to them on¬line.\n› We suggest registering your travel plans with your embassy or consulate before leaving home. Should a serious event occur, your Government will be able to make contact and account for you.\n› Take out appropriate insurance. No one ever expects an injury or accident to happen. However, we have a number of unfortunate stories from travelers and they usually start with: “I never expected…”\n› Leave a copy of your itinerary with friends or family. Health information\nSome of our trips may lead you to some exciting off the beaten tracks. With that, comes exposure to new and unfamiliar bugs, viruses and infections. Consult you local medical professional well before leaving home. You may need injections to prevent against common diseases’, and they may need some time to incubate before they are effective. Your medical professional or travel clinic will advise on what is needed.\nWe also ask you to complete our online medical questionnaire. This will help us to give you the facilities you need while with us, and allow us to give special attention to those who need it.\nThe travel experience will expose you to many different types and forms of transport, and many risks and rewards. Some of these may be familiar to you and just a matter of commonsense, and some may not. Take note of the top 5 below tips received from experienced Indochina Pioneer travelers:\n› Pay attention to the in¬flight briefing. It may seem monotonous or boring to those who travel often, but in the event of an emergency, you will need to know it instantly.\n› Count the rows to the emergency exit. In an emergency, lighting may be reduced.\n› Don’t drink too much alcohol. Cabins are pressurized so the effect of alcohol is h4er than normal.\n› Listen to the flight attendants. They are primarily there for your safety.\n› Keep your seatbelt fastened. Turbulence can occur at anytime.\nBuses and trains\n› Keep your personal belongings in your sight, or preferably on you.\n› A lock placed on zippers is a good deterrent to a would¬ be thief.\n› Watch for uneven, slippery surfaces or other obstacles as you enter and exit the vehicle.\n› Remain alert to surroundings. Be aware of distractions, as they may be opportunities for pickpockets.\n› Know your stop.\n› If you are staying at a hotel, have them call you a taxi, rather than waive one down on the street.\n› Use taxies from a taxi stand where possible. Typically only registered operators may use a taxi stand.\n› Keep your belongings at your side or feet. If you need to exit quickly they will be easily accessible.\n› If in an emergency situation, leave the bags. You can run faster without it and items can be replaced.\n› Check the Identification of the driver before getting in. It should be prominently displayed in the vehicle if it is a legitimate taxi.\n› Listen to the safety briefing. You will need to know what to do in an emergency.\n› If lifejackets are supplied – use them.\n› Always have at least one hand free to hold onto something to help with your balance.\n› The sun reflects up off the water. Ensure you have adequate sunscreen protection.\n› Take medication for seasickness prior to boarding – it may take some time to become effective.\nAccommodation is generally considered a safe haven for weary travelers. However the reality is it is often the most likely place that accidents and thefts occur. The following things can be done to increase your personal safety as told to us by fellow travelers:\n› When arriving, stay with your bags until they are transferred from the taxi or bus to the Lobby.\n› Lobby’s can be chaotic places with many people coming and going. Keep an eye on your bags at all times when checking in and out. The busy atmosphere is a welcome distraction to opportunists.\n› When you enter your hotel room, make sure the door closes firmly behind you and it locks.\n› When in your room lock the door, use the security latch, lock windows, and connecting room doors.\n› Always use the ‘spy hole’ to see who is at the door before opening it.\n› Know the emergency assistance number, and how to operate the phone system.\n› Store all unneeded personal items, cash, valuables and travel documents in the in¬ room safe.\n› Take note of the emergency plan on the back of the hotel door. You will need to know it in an emergency.\n› Place your room key in the same place each time. It avoids losing it, and you will know where it is in an emergency situation.\n› Use the non¬slip mats in the showers if provided.\nOut and about\nOne of the great ways to experience a new destination is to simply immerse yourself into it! However, as with anywhere, there are select elements of the local community who prey on the vulnerable. We have received many tips and advise from our travelers on the best way to do reduce vulnerability, protect safety at the same time enjoy the amazing sights. Some of these ideas may be simple commonsense, while others you may not have thought about. These are the top 10 ways to enhance your personal safety while out and about:\n› Blend in as much as possible, especially in your dress and appearance. Try to avoid an obvious tourist appearance.\n› View maps discreetly. A map identifies you as a tourist, and unfortunately, a target.\n› If disorientated, only ask for directions from uniformed officers or persons of obvious authority.\n› Stay on well lit or well trafficked areas and roads. Danger often lurks in dark and less populated areas.\n› Photos are a must when traveling. However, keep the camera discrete. Many ‘point¬and¬shoot’ cameras will fit in your pocket out of sight when not in use. Do not leave larger ones dangling around your neck, or other places in plain view.\n› When withdrawing money from ATM’s try to do so during daylight, in well trafficked areas and use machines that are associated with a recognized bank where possible. As with anywhere, protect your personal pin number by covering the keypad from prying eyes with your other hand. Once the cash is withdrawn, take time to ensure it is stashed away safely. Do not do so while walking down the street advertising the fact you are carrying amounts of cash with you.Leave excess cash, travel documents, jewelry in the hotel safe, or better yet at home if you don’t need it while away.\n› An experienced pickpocket can usually pick a tourist out in a crowd. Wear your purse over one shoulder with the opening flap against your stomach so that it’s impossible to get into. If you’re carrying a daypack wear it in front, kangaroo style.\n› Where possible avoid walking alone – especially at night. Traveling with friends makes you much less of a target.\n› Take something with the hotel name and address on it with you like a notepad or matchbox. This way, even if you do not speak the language, a taxi will be able to take you back to the hotel.\nYou can also contact us by email if you have any other questions about travel safety issues and booking a trip. Direct Customers: mailto: firstname.lastname@example.org Website: http://www.indochinapioneer.com\nWhen you are traveling in this region and if you need medical assistance, Indochina Pioneer always recommend SOS emergency evacuation service to include in a travel package beside Travel insurance service. International SOS emergency air evacuation service is different from Travel insurance. When an accident happens SOS international system will be accessed immediately by Indochina Pioneer.\nSCOPE OF SERVICE WE (SOS) SUPPLY:\n1, Travelers are under 75 years old\n2, Travelers travel in Vietnam, Laos, Cambodia, Thailand, Myanmar.\n3, Medical Assistance\nIntl.SOS shall provide Traveler, upon request, with name, address, telephone number and, if available, office hours of physicians, hospitals, clinics, dentists and dental clinics during traveling. Intl.SOS shall not be responsible for providing medical diagnosis or treatment.\n› Arrangement of Hospital Admission\nIf the medical condition of traveler is of such gravity as to require hospitalization, Intl.SOS will assist hospital admission.\n› Arrangement and Payment of Emergency Medical Evacuation\nIntl.SOS will arrange for the air and/or surface transportation and communication for moving the traveler when in a Serious Medical Condition to the nearest hospital where appropriate medical care is available. Intl.SOS shall pay for the medically necessary expenses of such transportation and communications and all usual and customary ancillary charges incurred in such services arranged by Intl.SOS.\nIntl.SOS retains the absolute right to decide whether the traveler’s medical condition is sufficiently serious to warrant Emergency Medical Evacuation. Intl.SOS further reserves the right to decide the place to which the traveler shall be evacuated and the means or method by which such evacuation will be carried out having regard to all the assessed facts and circumstances of which Intl.SOS is aware at the relevant time.\n› Guarantee of Medical Expenses Incurred during Hospitalization\nMonitoring of Medical Condition During and After Hospitalization Intl.SOS will, when authorized by Traveler/Sisters Tours, assist the traveler by guaranteeing on behalf of the traveler medical expenses incurred during hospitalization.\n› Access with Travel Insurance Company to get reimbursement\nIntl.SOS will, on behalf of the traveler, access with Travel insurance company who signs a travel insurance contract with the traveler to get reimbursement for the traveler’s lost.\n› Delivery of Essential Medicine\nIntl.SOS will arrange to deliver to the traveler essential medicine, drugs and medical supplies that are necessary for a traveler’s care and/or treatment but which are not available at the traveler’s location. The delivery of such medicine, drugs and medical supplies will be subject to the laws and regulations applicable locally. Intl.SOS will not pay for the costs of such medicine, drugs or medical supplies and any delivery costs thereof.\n› Arrangement and Payment of Emergency Medical Repatriation\nIntl.SOS will arrange and pay for the return of the traveler to the Home Country by air and/or surface transportation after hospitalization. Intl.SOS reserves the right to decide the means or method by which such repatriation will be carried out having regard to all the assessed facts and circumstances of which Intl.SOS is aware at the relevant time.\n› Arrangement and Payment of Transportation of Mortal Remains\n› Arrangement and Payment of Compassionate Visit\n› Arrangement and Payment of Return of Minor Child\nPlease contact us for further information about SOS International Emergency Air Evacuation Service']	['<urn:uuid:d1b25456-e1c8-459a-a99b-9b4f02835d3e>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T05:28:56.360010	30	80	2679
33	Which measurement tools monitor indoor air quality issues?	For mold issues, digital measuring devices or thermal imaging cameras are used to measure surface temperatures and dew points. For ammonia issues, data loggers are used to monitor concentration levels over time, recording values at different locations to track the effectiveness of ventilation.	['The focus is on room air temperature and relative humidity\nVisible mold growth in the apartment always causes discomfort for those affected. After all, no one wants to have the unloved co-inhabitant in their neighborhood. To analyze the causes, it is essential to take a look at the physical relationships in the building. The focus is on room air temperature and relative humidity as well as on the surface temperatures of the surrounding surfaces. Mold always grows when it finds enough food and sufficient moisture in its environment. The fungus is not choosy about its “food”. A little house dust is enough, as it can be found in every apartment. In the most frequent cases of damage, condensed water vapor from the air in the room is responsible for the influx of moisture. Mold experts use the dew point calculator to determine cause and effect.\nDetermine dew point values with the help of tables or a computer program\nThe most convenient way to calculate the dew point is electronically; either with a digital measuring device or online with the help of one of the many offers on the Internet. If you don’t have a calculator at hand, you can use a printed table. On one DIN A4 page alone, the most common combinations between room temperature and relative humidity can be displayed. The following examples serve for understanding. At a room temperature of 20 degrees Celsius and a relative humidity of 70 %, a dew point temperature of 14.4 degrees is calculated. For example, if the room temperature is 18 degrees Celsius and the relative humidity is 60 %, the dew point is 10.1 degrees.\nComparison of dew point and surface temperature\nTo determine the cause of mold damage, the expert needs the surface temperature as well as the dew point. Either an infrared thermometer or a thermal imaging camera is suitable as a measuring device. The camera offers the advantage that the temperature conditions on the wall or ceiling can be documented immediately and can later be included in the investigation report. If the surface temperature at a certain measuring point is at a maximum of 10.1 degrees Celsius (see example above), then at given values the dew point temperature has been reached and the water vapor condenses at this point. The difference between the room temperature and the surface temperature in the example is 7.9 degrees Celsius. This serious difference suggests defects in the building fabric.\nCauses of thermal bridges\nExperts call stark differences between room temperature and surface temperature at certain building components a “thermal bridge”. From a physical point of view, these are localized spots that have a higher heat flux density compared to the adjacent building components. The increased heat flux density causes additional heat loss and a reduced surface temperature. Thermal bridges are thus energy weak points in a building structure. Thermal bridges are particularly common at window or door reveals when the thermal insulation of the facade is inadequate. But projecting concrete ceilings, ring beams, roller blind boxes or radiator niches also cause thermal bridges. In these cases, the only solution is to improve the thermal insulation by retrofitting exterior insulation or, in individual cases, by professional interior insulation.\nA look at resident behavior\nThe dew point determination in connection with the surface temperature also allows conclusions to be drawn about the behavior of the occupants. A relative humidity of more than 70% over a longer period of time should be avoided. Even if the building fabric is intact, mold can form behind large cabinet walls when conventional radiators are used. This is because radiators distribute heat throughout the room via natural convection. Corners and areas behind furniture that are too close to the walls are reached less easily by the flow of warm air and are therefore less warm. To document user behavior, records of the room climate using data loggers are a suitable method. The measurement should last at least four weeks and record the values at different places in the apartment, including the places with mold growth.', 'Ammonia concentration versus ventilation rate\nMonitoring ammonia levels in poultry houses can help producers to adjust ventilation rates and thus prevent problems, say Michael Czarick (Extension Engineer) and Brian Fairchild (Extension Poultry Scientist) at the University of Georgia in the latest in the series of ‘Poultry Housing Tips’.\nOne of the biggest challenges with maintaining optimal air quality during cold weather is the fact that much of what we are trying to control is constantly being generated. For instance, if we had an empty house and filled it with smoke we could turn on a few exhaust fans and within minutes the smoke would be gone – problem solved.\nBut, when it comes to gases such as ammonia and carbon dioxide which are constantly being generated, it is a never-ending process to keep them under control. For example, during the minimum ventilation, “off cycle” ammonia levels will slowly rise as the ammonia produced by the litter builds. When the minimum ventilation fans turn on the ammonia levels will quickly decrease as fresh air is added to the house. But as soon as the exhaust fans turn off, the ammonia level will start to rise again and by the end of the “off cycle” it will be back to where it was prior to the fans turning on.\nThe cyclical nature of ammonia concentrations in poultry houses can an be seen in Figure 1 where between flocks two 48-inch fans were operating three minutes out of every 30 to help limit the build-up of ammonia and moisture.\nThough operating the exhaust fans lowered the concentration of ammonia in the air by approximately 15 parts per million (ppm), as soon as the fans turned off ammonia concentration started to rise and within 30 minutes, it was back to what it was before the exhaust fans turned on. The pattern continued for the next 14 hours of the study (Figure 2).\nLittle if any progress was made in lowering the overall average ammonia concentration.\nThe same cyclic pattern of ammonia concentrations can be seen in Figure 3 where minimum ventilation fans were operating 90 seconds out of five minutes in a house with three-week-old birds. Minimum fan operation decreased ammonia levels by approximately 7ppm but by the end of the minimum ventilation fan “off cycle” they rose the same 7ppm. The average ammonia concentration remained essentially the same, 33ppm, over the course of the night.\nFor any minimum ventilation fan setting, the ammonia levels will tend to reach a level of equilibrium. In Figures 1, 2 and 3, though the minimum ventilation fans reduced ammonia concentrations quickly, the reduction was temporary and over the short-term, average ammonia levels did not significantly change. The equilibrium ammonia concentration can, of course, be lowered by increasing the minimum ventilation rate.\nAn important fact to keep in mind is that the reduction in the equilibrium or “average” ammonia concentration will be roughly proportional to the change in exhaust fan run-time. This means that in order to make large changes in average ammonia concentrations, relatively large changes in minimum ventilation fan runtime need to be made. For instance, the minimum ventilation fan run-time in the house illustrated in Figure 3 was increased from 90 seconds to 120 seconds out of five minutes (4:00am).\nThe increase in fan run-time of 25 per cent reduced average ammonia concentration by 22 per cent (from 33 to 27ppm; Figure 4). Increasing fan runtime from 90 to 150 seconds, a 40 per cent increase, reduced ammonia concentration by 35 per cent (from 33ppm to 22ppm; Figure 5).\nSo in short, if you want to cut your ammonia levels in half, double your minimum ventilation rate. This holds true if the ammonia concentration is 80ppm and you want to drop it to 40ppm or even if the ammonia concentration is 40ppm and you want to decrease it to 20ppm.\nDoes this mean that if you have high ammonia concentrations that you may have to double or quadruple your minimum ventilation rates indefinitely? Not necessarily. Though high minimum ventilation rates will help to keep ammonia levels to a minimum, they will also tend to address the root cause of ammonia: litter moisture.\nOver time, higher ventilation rates will tend to reduce litter moisture levels, which in turn will reduce the ammonia generation rate, which in turn leads to lower ammonia levels and an overall reduction in the minimum ventilation rate required to keep ammonia concentrations to a reasonable level.\nCan this method of ammonia control prove expensive? Sure. But, the fact is that once you have an ammonia problem it is expensive to solve. The key to keeping ammonia levels and heating costs to a minimum is preventing high ammonia levels in the first place. Between flocks, remove cake from the house as soon as the birds leave and ventilate the house to help dry the litter. Use a litter treatment according to manufacturer’s recommendations prior to chick placement.\nMost importantly, closely manage house moisture levels by monitoring the relative humidity of the air in the house. The relative humidity of the air in a house is an indirect measure of litter moisture. Generally speaking, the ideal relative humidity would be approximately 50 per cent. As the relative humidity of the air in the house climbs, so does litter moisture. Increased levels of litter moisture results in higher ammonia production rates.\nIf you allow the average relative humidity climb to 70 to 80 per cent, you will have wet litter and high ammonia levels, which can only be reduced to acceptable levels by a dramatic increase in minimum ventilation rates.\nTo avoid this situation, make relatively small adjustments to the minimum ventilation rates on a daily basis. Record the relative humidity in your houses each morning. If you see the relative humidity climbing, make larger increases.\nBottom line: don’t wait until you have an ammonia problem to try to solve it. After all, “an ounce of prevention is worth a pound of cure”.\nPosted on May 26, 2015']	['<urn:uuid:6494ea05-a009-441b-b252-efe58f6bd0b2>', '<urn:uuid:34d40b29-9717-40a5-ba6c-8ad6e74aa2b5>']	factoid	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-13T05:28:56.360010	8	43	1680
34	what numerical readings indicate battery cell problem using hydrometer test	If the readings between cells has a difference of more than 0.050 between the highest and lowest readings, the lowest cell is either weak or dead, indicating the battery needs to be replaced. Also, if the average reading of all cells is less than 1.277, the battery needs to be charged.	"['How to Test RV Deep Cycle Battery? Top 3 Methods are Well Explained\nYou don’t want to be stranded on the side of the road because your battery died. Nor do you want to risk a disaster because you tried to charge a dead battery. This means checking the RV battery’s capacity. We’ll discuss how to test RV deep cycle battery levels in detail for each method and how to stay safe while you do so.\nIf you don’t know how to test RV deep cycle battery, then read on.\nHere are the top 3 methods to test an RV battery\nMethod 01) Monitor Panel\nThis is the simplest method available, though it can be prone to errors since the user can make mistakes and the monitor panel doesn\'t always read right. It isn\'t uncommon for a battery level indicator to say 100% when it is rounding up from 80%.\nThe method itself is to look at the dashboard monitor panel while the recreational vehicle is not plugged into an electrical outlet. This will tell you the battery\'s current level. Disconnect the RV battery from any power source\nIf the level is close to where it was without this load, you know the monitor panel is accurate If the level is somewhat lower, use the lower value as the current indicator of the battery\'s level. And turn off all these loads once you have your answer so you don\'t drain the battery.\nMethod 02) Digital Voltmeter\nBefore you do this, know what voltage the battery is After all, a 6 volt battery that reads 6 volts is fully charged whereas a 12 volt battery that reads at 6 volts is dangerously low. Avoid testing a battery that has been charged or discharged in the past six hours.\nHow to Test the Battery using a Digital Voltmeter Make sure your test leads are properly plugged into the voltmeter. The red test lead should be plugged into the voltage (ohm) port while the black lead should be plugged into the ""com"" port.\nPut the red lead on the positive terminal of the battery. Put the black test lead on the negative battery terminal. A digital voltmeter will give you the voltage in numbers. An analog voltage meter will point to the approximate value.\nHow to understand the Result? (Good or Bad)\nFor example, a reading of 11.9 volts on a 12 volt battery means it has totally dissipated. A reading of 12.06 volts means the battery is 25% charged, whereas you need 1245 volts or 75% to be able to start the engine most of the time.\nIf you get a generic error message, retest or replace the batteries in your voltmeter before taking another reading. If charging the battery doesn\'t improve the voltage levels, the RV battery itself probably needs to be replaced.\nMethod 03) Hydrometer/Specific Gravity\nThe hydrometer technique tests the specific gravity of a battery\'s electrolyte; in short, a hydrometer test is a specific gravity test of your battery. You can find both manual hydrometers and digital hydrometers for this test.\nNote that you can\'t get an accurate reading via this method if you just added water to the battery; let it sit for at least six hours before you check the battery with the hydrometer technique.\nThis method is an option if you aren\'t sure how to check the battery level with a voltmeter or just don\'t have one A benefit of this approach is price — you can find hydrometers for a couple of dollars and get it at almost any auto parts store.\nHow to check RV battery using Hydrometer?\nThis test method requires you to put on protective gear and open up the battery. This isn\'t possible if you have a sealed maintenance-free battery. Nor should it be done while the battery is hot, since you don\'t want acid-laced steam escaping from the battery and hitting you in the face.\nSuppose to understand Easily\nSuppose the battery is cool and you\'re wearing the proper safety gear. The electrolyte will typically be in two different levels, an upper level and a lower level.\nFill the hydrometer and drain it twice before you take a reading. This helps clear out any moisture or residue in the hydrometer that count the results. Now test the specific gravity of the electrolyte in one cell.\nIf you\'re using American units, 1.225 to 1.250 is suitable, and anything heavier is ideal. Anything lighter than this indicates a problem. Once you have the reading, drain the electrolyte into the cell it came from. Then repeat the process on the other cell.\nThe Final Step of your hydrometer test\nAfter you test all the cells and record the values, replace the vent covers. If the readings of all the cells averages to less than 1277, the battery needs to be charged. If the readings between cells has a difference of more than 0.050 (between the highest and lowest), the lowest cell is either weak or dead. This suggests the battery needs to be replaced.\nYou May Also Like_\n- How to Avoid Deep Cycle Battery Explosion? – Read Expert’s 7 Things\n- Where Can I Park My RV Long Term Near Me? – Read Experts Things\n- How to Maintain an RV Generator Like an Expert?– 10 Cleaning Tips\n- What is An RV? Definitions, Differences, History, Usage, and More\n- How to Charge RV Deep Cycle Battery? Everywhere From All Electricity']"	['<urn:uuid:6b028571-d2f0-4ade-bd23-6ff3d6b278b0>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-13T05:28:56.360010	10	51	909
35	How do HDR photography and focus stacking differ in their approach to combining multiple images for achieving technically challenging shots?	HDR photography and focus stacking differ in their fundamental approach to image combination. HDR photography combines multiple images taken at different exposure values to capture a full range of tonal values from dark shadows to bright highlights that cannot be captured in a single photo. Each image in the sequence is typically separated by two stops of exposure, adjusted through shutter speed while keeping aperture and ISO fixed. In contrast, focus stacking merges several photos taken at different focal points to achieve greater depth of field than would be possible in a single shot. This technique is particularly useful in macro photography where depth of field is very narrow, or in landscape photography when foreground elements would otherwise be out of focus. Both techniques require a tripod for optimal results and specialized software for processing, but they address different photographic limitations - HDR tackles dynamic range issues while focus stacking addresses depth of field constraints.	['HDR Photography: A Beginner’s Guide\nAs a landscape photographer, you’ve no doubt experienced situations where your camera simply wasn’t able to capture the full tonal range within a scene. From the dark shadows in the foreground to the bright white clouds in the sky, the camera simply couldn’t retain full detail, so either the bright highlights or the dark shadows were clipped. The result would be less detail than you had hoped to present in your photo. Fortunately, HDR photography (high dynamic range) provides a solution to this.\nWhat is HDR Photography?\nHDR photography (high dynamic range) is a technique that involves capturing multiple images of the same scene using different exposure values, and then combining those images into a single image that represents the full range of tonal values within the scene you photographed.\nWhenever you experience a situation where the tonal range in a scene before your lens is simply too great to contain within a single photograph, HDR imaging provides a solution. Multiple captures with varying exposure values ensure that you have detail for all tonal values in the scene. You can then use special software to process those images, interpreting the result to create an image that contains far more detail than is possible with a normal photographic image.\nCapturing the Sequence\nAs a landscape photographer you are no doubt already using a tripod, which can be a critical accessory for HDR photography. After finding a composition you like, but that features a tonal range that exceeds what you can capture with a single photo, you can capture a series of images that can later be assembled into an HDR image with full detail from the darkest shadows through the brightest highlights.\nFor an HDR sequence you want to capture a photo that retains full highlight detail, as well as a photo that retains full shadow detail. Of course, in most cases you’ll also need additional photos representing exposures in between those two extremes. As a general rule you should capture a series of photos that are each separated by an exposure value of two stops.\nWith very few exceptions, you will generally want to keep the lens aperture and the ISO setting fixed, changing the shutter speed from one frame to the next. You can start by capturing a photo that retains full detail in the highlights, with no clipping shown on the right side of the histogram when reviewing the photo.\nFurther Reading: Understanding Histograms and How to Use Them\nYou can then adjust the shutter speed for the next photo to achieve an exposure that is two stops brighter. For example, let’s assume that your exposure settings for the first photo in the sequence included a shutter speed of 1/500th of a second. With the lens aperture and ISO setting fixed, you can adjust the shutter speed by two stops to 1/125th of a second.\nYou can then capture an exposure with a shutter speed of 1/30th of a second, and continue with this process until you have captured an image that retains full detail in the darkest shadow areas, reviewing the result on the back of your camera to ensure there is no clipping on the left end of the histogram for that final image.\nUsing Automatic Bracketing for HDR Photography\nWhen you are capturing a series of photos that will later be assembled into a final HDR result, you’re actually capturing what is referred to as a set of bracketed exposures. In addition to capturing those bracketed exposures manually, you can streamline your workflow by making use of the automatic exposure bracketing feature on your camera.\nThe basic process for employing automatic exposure bracketing generally involves first configuring the bracket settings, and then using a well-timed shutter release so that all of the photos in the bracketed sequence will be captured as quickly as possible. Of course, you can consult the manual for your specific camera model to determine how to make use of this feature.\nAs noted above, you can separate the photos included in your HDR sequence by two stops. With most newer camera models you can generally capture five, seven, or even nine exposures as part of an automatic exposure bracketing sequence. Older cameras only enabled you to capture three photos with automatic exposure bracketing, but even that limitation will generally provide enough of an exposure range if you separate each exposure by two stops.\nWith most cameras you will find it best to use a timer to automatically trigger the automatic exposure bracketing sequence. If you use the timer option (most cameras offer a two-second or ten-second timer setting), when you trigger an exposure the timer will start, and then the full sequence of exposures will be captured as quickly as possible.\nThe beauty of automatic exposure bracketing is that you can configure the settings with your camera, and then capture a full range of exposures for an HDR image with a single press of the shutter release button. This approach makes it remarkably easy to incorporate HDR into your landscape photography.\nConsider Including the Sun\nI think it is fair to say that landscape photographers are up for sunrise more than most other photographers, and of course also tend to be out for “golden hour” at the end of the day. These times provide a perfect opportunity to include the sun within the frame of a composition you’ll capture as an HDR image.\nWhen you include the sun in a landscape photo and employ HDR techniques, you can produce a scene that is truly impossible to fully experience with the naked eye. Of course, since the sun is so bright and the surrounding landscape will likely be relatively dark, you may need to capture a relatively large number of exposures to produce an optimal HDR result. In most cases when including the sun in the frame while employing two stops of exposure variation between captures, you’ll still need around seven (or more) photos to render a complete tonal range for the scene in the final image.\nIt is important to pay attention to every detail when including the sun in the frame for an HDR image. For example, be sure that the front lens element is completely free of dust, to help avoid bright artifacts in the image. You’ll also want to take special care with the overall composition, since it can be difficult to review the overall scene with the overwhelmingly bright light of the sun. And needless to say, you’ll want to make sure not to look directly into the sun while setting up your shot.\nProcess with Restraint\nOnce you are back at your computer and ready to process the sequences of images you captured in the field in order to create the final HDR result, there is a workflow you’ll need to employ. In general that workflow involves assembling a sequence of photos into a special image that contains the full range of tonal values represented by that sequence, and then performing a “tone mapping” step that translates that tremendous range of tonal information into a final image.\nOne of the tools I often employ for HDR imaging is Aurora HDR from MacPhun Software, for example. This type of software streamlines the process of creating a final HDR image, by enabling you to perform the full workflow in a single overall process.\nThe initial part of this process is relatively straightforward, in that you will basically select the images you want to process and then initiate that process. At this stage you can choose among a variety of options. You will most certainly want to take advantage of the option to align the individual captures, even if you used a tripod in your photography. You will also generally have the option to reduce the appearance of color fringing (chromatic aberrations), which I recommend taking advantage of. In addition, the option to apply “ghost reduction” will help compensate for any movement within the scene from one capture to the next.\nOnce the initial processing is complete, you can apply a variety of adjustments during the “tone mapping” phase of the workflow for HDR images. Most HDR software applications, including Aurora HDR, enable you to employ adjustment presets as part of this process. This approach can certainly speed up your workflow, but perhaps more importantly the use of presets can also provide you with a degree of creative inspiration for how you want to interpret your photo.\nWhile HDR imaging enables you to present an extreme amount of detail in a scene, and many of the adjustments available for processing your HDR images can produce images with significant impact, a bit of restraint is generally best. I highly recommend that you focus on creating HDR images that don’t immediately jump out as being anything other than a great photograph. In other words, start with a great scene and composition, and then process the result with a certain amount of restraint, presenting a unique view of the world without going overboard.\nGet started: Download Aurora HDR to process your HDR images\nThe use of HDR techniques can open up a whole new world of potential in your landscape photography. To be sure, creating great results can take a bit of practice. By understanding the basics of HDR photography you can then explore the possibilities and experiment with different scenes. Along the way you’ll surely create some great images, have fun, and learn how to truly make the most of all that HDR has to offer.', 'The Ultimate Guide to Learning Photography: Focus Stacking\nWhat is focus stacking? Try this simple technique for sharper shots\nDigital cameras are powerful tools — over the past few years, they’ve added megapixels while shedding weight and dramatically improving low light performance. But, there’s still only so much you can do with a single photograph. Focus stacking enters the arena alongside high dynamic range as a photo editing trick that uses multiple images to achieve what’s impossible with a single photograph.\nSo what is focus stacking and how do you use it? Focus stacking merges several photos taken at different focal points to achieve a depth of field that’s either impossible because of the gear on hand, difficult because of the light that’s available or otherwise unworkable because of depth of field parameters. While the technique is a bit more complex than shooting and editing a single image, with a tripod and a few tricks, focus stacking can keep all the details in focus.\nWhat is focus stacking?\nA distant relative of high dynamic range, focus stacking layers images taken at multiple focus points instead of different exposures. The result is an image that’s sharper than any of the original photos by themselves.\nSo why focus stacking? Focus stacking tackles depth of field problems, or how much of the image is in focus. Depending on the aperture settings and how close the camera is to the subject, the objects in the photo may be several feet apart and still in focus, or they may be millimeters apart and out-of-focus.\nShooting up close to a subject, like in macro photography, creates a very narrow depth of field — so narrow, in fact, that a bumble bee’s antennae may be in focus, but his stinger is blurred into the background. While using a wider aperture can help, often, aperture alone isn’t enough to add that detail back in. By capturing the subject in a series of images instead, those details can be layered back in.\nWhile macro photography is one of the biggest reasons to use the focus stacking technique, it’s not the only reason. A wide aperture will let in a lot of light, but leaves most of the image out of focus. If you are shooting a landscape at night, focus stacking will create the look of a wider aperture, even though it wasn’t possible to use one.\nFocus stacking is also occasionally used in landscape photography, often when there’s an element in the foreground that, even with a large aperture, would still be out of focus.\nWhatever the subject is, focus stacking works to capture a sharper image than the camera can capture with a single shot.\nHow to shoot a series of photos for focus stacking\nTo get that sharp final image, you need to shoot a series of photos with a different focal point in each one. First, pick your scene and frame up your composition. Ideally, you should use a tripod. While software is now able to align the images automatically, with the auto align method, you lose resolution since you’ll need to crop off the edges that didn’t quite overlap. If bringing a tripod isn’t possible, keep your composition as close as possible throughout the series.\nA focus stack is best shot in manual mode so that the exposure is uniform across all the images in the series. (If you’re not yet comfortable in manual mode, turn the camera to programmed auto, and take note of those settings. Then, go back to manual and use those same settings). Occasionally, focus stacking is done with varying apertures to merge images with a different depth of field, but for simplicity’s sake, stick with manual mode and an even exposure until you’re more experienced with focus stacking.\nNext, identify how much of the image you would like to keep in focus. Perhaps you have a row of three trees, for example, and you want all three trees to remain in focus, or maybe you’re shooting a close-up of a flower and you’d like every petal to be tack sharp.\nThe tricky part of focus stacking is determining how many images you need to take to get the level of sharpness that you’re envisioning. The more images you stack together, the sharper the image will be — but the trickier it will be to shoot it all. The closer you are to a subject, and the wider your aperture is, the more images you’ll need to shoot.\nIn that row of three trees, for example, shooting three images, one focused on each tree is a simple way to stack focus. That flower, however, may need more images, since depth of field changes dramatically with distance.\nHow do you decide how many images to shoot? Try first taking a test shot focusing on the front edge of the closest object you’d like to remain sharp. Then, take a look at that test shot in the preview on the LCD screen (don’t forget to use the zoom button). How quickly does that sharpness fall off? If that flower is still sharp one third of the way through, you can get away with three to four shots. If that flower is only sharp for the first tenth of that blossom, however, you’ll want to use more than ten images. You want to avoid gaps, or sharpness will fall away then return, fall away and then return. Focus stacking takes some practice, so if you’re not sure, taking multiple sets to experiment with later.\nOnce you’ve determined what you need to remain in focus and how many shots you’ll need to get there, start shooting. Put the focal point on the first point and shoot. Move the point deeper into the photograph and shoot again. Continue shooting, moving the focal point deeper with each photo, until you’ve captured the entire range.\nHow to create a single image from focus stacking in Photoshop\nWhile focus stacking used to involve manually blending each sharp area together into one photograph, with the latest version of Photoshop (and even variations back to 2014), the program can automatically blend a stacked image for simple focus stack editing.\nStart in Photoshop by going into File > Scripts > Load Files Into Stack. In the dialog box that opens, select all the photos in the series by clicking browse. Leave the other options unchecked. Now, your photos will automatically be loaded into separate layers.\nNext, make sure your photos are perfectly aligned. Highlight each layer in the layers panel, then go up into Edit from the top toolbar and choose Auto-Align Layers. The Auto option does a good job most of the time, but for more accuracy, choose “collage” for a focus stack, then press okay.\nIf you didn’t use a tripod, your layers likely don’t quite meet up — use the crop tool to create a uniform image by cropping off any areas that don’t overlap with all of the layers. Occasionally, you may still have some crop when using a tripod, particularly if it was windy.\nNext, you need to tell Photoshop that you want all of those layers to merge into one image, but one image that keeps the sharpness from the series. To do that, with all those layers still selected, head into Edit > Auto-Blend Layers. Make sure “Stack Images” is selected, then hit okay.\nPhotoshop will now work it’s magic and blend all those focal points — the “magic” may take a few minutes. Once the process is complete, you should see a sharp image, and over in the layers box, masks showing which part of which layer is visible in the final image. If you need to tweak what part of the each photo is included, you can do so by editing that layer’s mask.\nNow that the series of images is merged, you can flatten the image (Layers > Flatten Image) and then edit the photo just like you would with a single shot.\nPhotoshop is just one way to merge a focus stack, several photo editors can handle the task and there are even programs dedicated entirely to focus stacking, like Helicon Focus and Zerene Stacker.\nFocus stacking defies the limitations of photography and creates a single image that isn’t possible from one press of the shutter release. By merging multiple photos together at different focal points, you can overcome the limitations of your gear, combat low lighting or create dramatically sharp macro photographs. Focus stacking may be more complex than a single shot, but in many scenarios, the extra shooting — and extra editing — is well worth the effort.\n- Photography Guide Home\n- Photography Lighting\n- Photography Post Processing Techniques\n- What is Aperture?\n- Exposure Bracketing\n- Creating Bokeh Backgrounds\n- What is Camera Raw?\n- Composition Rules\n- What is Shutter Speed?\n- Hyperfocal Distance\n- How Does A Camera Work?\n- Composition Techniques\n- Aperture & Depth of Field\n- Long Exposure\n- What Makes A Good Photo?\n- Choosing Photography Subjects']	['<urn:uuid:720f4a76-be4f-453a-a72b-358a4e68aba5>', '<urn:uuid:97593066-91e7-4988-bb30-923d4931612b>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-13T05:28:56.360010	20	155	3097
36	As a radio astronomer, I need to understand the observation capabilities of the FAST telescope - what range of frequencies can it detect with its instruments?	Several detectors are used in the FAST telescope to cover a frequency range of 70 MHz to 3 GHz.	"[""Five-hundred-meter Aperture Spherical radio Telescope (FAST)\nThe 500 m diameter → radio telescope which is the largest → single-dish antenna in the world. It is an Arecibo type telescope nestled within a natural basin in China's remote and mountainous Dawodang, Kedu Town, in southeastern China's Guizhou Province. The → reflector consists of 4,450 triangular panels, each with a side length of 11 m. More than 2,000 → actuators are used, according to the feedback from the measuring system, to deform the whole reflector surface and directly correct for → spherical aberration. Several detectors are used to cover a frequency range of 70 MHz to 3 GHz.\nHaving the form of a sphere; of or pertaining to a sphere or spheres.\nFr.: aberration sphérique, ~ de sphéricité\nAn aberration of a spherical lens or mirror in which light rays converge not to a single point but to a series of points with different distances from the lens or mirror. Spherical aberration is corrected by using parabolic reflecting and refracting surface\nFr.: angle sphérique\nAn angle formed on the surface of a sphere by the intersection of two great circles of the sphere.\nostorlâb-e sepehri, ~ kore-yi\nFr.: astrolabe sphérique\nA type of → astrolabe in which the observer's horizon is drawn on the surface of a globe, mounted with a freely rotating spherical lattice work or 'spider' representing the celestial sphere. The earliest description of the spherical astrolabe dates back to the Iranian astronomer Nayrizi (865-922).\nFr.: astronomie sphérique\nThe branch of astronomy that is concerned with determining the apparent positions and motions of celestial bodies on the celestial sphere. Same as → positional astronomy.\nFr.: coordonnées sphériques\nA coordinate system using an origin (O) and three perpendicular axes (Ox, Oy, Oz), in which the position of a point (P) is given by three numbers (r, θ, φ). The coordinate r is the distance from the origin, θ the angle between the z-axis and the r direction, and φ the angle between the projection of r on the xy-plane and the Ox-axis. The coordinate φ is also called the → azimuthal angle.\nfozuni-ye sepehri, ~ kore-yi\nFr.: excès sphérique\nThe difference between the sum of the three angles of a → spherical triangle and 180° (π radians).\nFr.: géométrie sphérique\nThe branch of geometry that deals with figures on the surface of a sphere (such as the spherical triangle and spherical polygon). It is an example of a non-Euclidean geometry.\nFr.: fonction harmonique sphérique\nA solution of some mathematical equations when → spherical polar coordinates are used in investigating physical problems in three dimensions. For example, solutions of → Laplace's equation treated in spherical polar coordinates. Spherical harmonics are ubiquitous in atomic and molecular physics and appear in quantum mechanics as → eigenfunctions of → orbital angular momentum. They are also important in the representation of the gravitational and magnetic fields of planetary bodies, the characterization of the → cosmic microwave background anisotropy, the description of electrical potentials due to charge distributions, and in certain types of fluid motion.\nvarunâ-ye kore-yi, ~ sepehri\nFr.: latitude sphérique\nspherical polar coordinate\nhamârâhâ-ye kore-yi-ye qotbi\nFr.: coordonnées sphériques polaires\nSame as → spherical coordinates.\nFr.: symétrie sphérique\nA configuration in which the constituting parts are arranged concentrically around the center of a sphere.\nFr.: triangle sphérique\nA triangle drawn on the → surface of a → sphere. A spherical triangle, like a plane triangle, may be right, obtuse, acute, equilateral, isosceles, or scalene. The sum of the angles of a spherical triangle is greater than 180° (π) and less than 540° (3π). See also → spherical excess.""]"	['<urn:uuid:ba1f5fd8-4a66-41f1-a1a7-26b1550e1a6d>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T05:28:56.360010	26	19	603
37	I'm new to painting, what makes impressionist art different?	Impressionist art is characterized by its use of separated, unblended brushstrokes and distinct planes of color, which was a significant departure from the traditional academic standard of fully blended, detailed painting that was common during the Renaissance and afterward. While this technique was initially criticized, it creates a unique energy and glittering play of light that many art lovers now appreciate. The technique also makes the paint itself become a theme of the work, allowing viewers to appreciate both the subject matter and the beauty of the paint strokes themselves.	['What is it?\nHappy Tuesday! This week we’re looking at a style of painting in which depth, roundness, changing values, and changing colors are depicted using separate, unblended brushstrokes. If that still doesn’t sound very clear, no worries, let’s look at a visual example! Here are details from two paintings from our current (FANTASTIC!) exhibition, “Graceful Subtleties,” side by side:\nTake a moment and observe the two different ways in which these artists used the paint to describe rounded forms. Louise’s brushstrokes are blended beautifully and give a soft and even slightly blurred appearance to the roundness of the young woman’s face and shoulder, and to the body of the little bird. The colors, values, and brushstrokes are blended seamlessly, one into another, and present a more true-to-life three dimensional effect. Now look at the contrast between that and the roundness of Jussi’s coconuts. Jussi’s brushstrokes are decidedly more defined, and rather than blending seamlessly, the different colors and values present as separate brushstrokes, and visually as separate planes. It creates a fascinating three dimensional effect, that is less strictly realistic and more painterly, and serves to create a glittering effect of light on these forms. It is this unblended, planar approach to describing form with brushstrokes that we’re going to take a look at today.\nExamples from art history:\nFor a long time, during the leap forward in realistic painting seen during the Renaissance and through many centuries after, the academic standard in painting was a detailed, fully blended, fully rendered depiction of form. This is why, during the emergence of Impressionism in the 19th century, the effect of separated, unblended brushstrokes and the focus on separated planes of color was so jarring, and at first, frowned upon. Perhaps anything less than the academic standard to which critics were accustomed at first appeared primitive and lacking in artistic merit– but the innovation and brilliance of the Impressionist and Post-Impressionist style did not take long to win many over, and today is still a popular favorite among art lovers. There is an energy in this type of painting, a glittering play of light and exaggeration of form that is visually very appealing. Furthermore, the paint itself becomes a theme of the work, rather than solely the subject which the paint depicts. One begins to see the beauty beyond the image portrayed, and finds it also in the simple application and texture of the paint strokes themselves. Some of the most notable innovators of this technique include Paul Cezanne, Henri Matisse, and Vincent Van Gogh, and some excellent examples can be seen here:\nAs a fun side note, the separation of rounded forms into more geometric planes, particularly in Cezanne’s work, also gives us an exciting glimpse at an art historical movement still to come in the early 20th century, Cubism! Playing with paint in this new way truly opened up the minds of so many artists of the 19th and 20th centuries, and of this century as well!\nExamples from Principle Gallery:\nMany artists that we show here at Principle Gallery make use of this technique of unblended brushstrokes and planes of color, some in more subtle ways, and some in more obvious ways. Jussi Pöyhönen and Paula Rubino, however, are two of the most striking examples, and their works featured in the current Graceful Subtleties exhibition are excellent samples of how lovely the effect of this technique can be. Take a look below, and if you haven’t yet, be sure to check out the entire exhibition on our website here!']	['<urn:uuid:ddb7d757-7cd2-49b9-a286-5f1274ab7f6d>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T05:28:56.360010	9	90	591
38	As someone who visits both physical libraries and library websites frequently, how do they handle advertising differently in their physical programs versus their online presence?	Libraries' physical programs are typically free or low-cost community activities without commercial focus, while for their websites, the ALSC specifies that advertising must be limited and appropriate, and sites devoted strictly to sales will not be considered. This shows a consistent approach across both physical and digital spaces to prioritize education and community service over commercial interests.	"['Today’s libraries are nothing like they were, even as recently as a decade or two ago. In the late 1990’s some libraries were still transitioning over from card catalogues and many still had microfilm catalogues until very recently. And if the truth be told, it was only in October 2015 that the Online Computer Library Center announced that they had ‘just’ sent their last order of cards to a library in the Bronx.\nWhile some schools still have a paper card catalogue as a backup to a digitized online catalogue, today’s libraries across the nation have gone hi tech. Because of this, libraries have expanded to more than just a book repository and are now a source of education and community activities that are affordable as well as entertaining. If you are looking for free or cheap family entertainment, you just might want to see if your county library has any programs or workshops you could attend.\nAdvice from the ALSC\nA head librarian with a library science degree such as an online masters in library science has much to offer the community. The Association for Library Services for Children, ALSC, suggests that librarians seek out and offer their communities more than just book related activities. As a division of the American Library Association, the ALSC suggests that other activities that are educational, informative, entertaining and offer a ‘hands-on’ experience to adults and children alike should be offered.\nToday’s master of science librarian would be unrecognizable in yesteryear. They now offer classes in computer technology and help with their school district’s literacy programs as an adjunct to what county schools are doing. It’s amazing to see that a library is not a multifunction space where people can learn to sew, knit, cook, fix automobiles and even how to write job résumés.\nPrograms Suggested by the ALSC\nIt only stands to reason that librarians of today, and going forward into our digital future, would earn an online MMLIS degree. After all, they are working in a digital age and many of the programs suggested by the ALSC focus on computer science. For example, they suggest that librarians teach a kids’ e-reader class so that kids can learn how to use their Kindles to read e-books. They also suggest a Pokémon Club with kids working in groups with adults on Pokémon GO activities and playing Pokémon Wii games. The categories they suggest county libraries offer include:\n- DIY Classes (i.e. basic home and auto repairs)\n- Tae Kwon Do and Yoga classes\n- Petting ‘Zoo’ Days (for domestic pets like cats, rabbits, mice and dogs)\nWith a huge assortment of other educational and fun activities. They suggest libraries offer the traditional puppet shows and reading clubs but in addition, they suggest librarians offer help in key areas such as how to operate an office suite, format a spreadsheet and do intensive online research that would not have been possible just a generation ago.\nInto the Future\nLibraries are nothing like they were and as they continue to move forward into the future, it only stands to reason that a library science degree would change along with the times. More and more emphasis is being placed on Information Technology, IT, so most masters degrees are now inclusive of this forward-leaning trend and are now listed as MMLIS degrees, Master in Management in Library and Information Science.\nThat’s what the future holds for librarians and so activities will continue to expand along with the times. Looking for free (or cheap) entertainment for the family? Start your search at your public library. There’s always something on of interest to people of all ages.', ""Sponsored by the\nASSOCIATION FOR LIBRARY SERVICE TO CHILDREN\na division of the\nAmerican Library Association\nYou are here\nGreat Websites for Kids Selection Criteria\nEstablished by the first ALSC Children and Technology Committee, 1997; revised by the ALSC Great Websites Committee/revisions approved by the ALSC Board of Directors, 2013\nThe Web is a lot like a flea market: there’s a vast selection of sites to choose from but not a lot of order to it. Some sites are offered by reputable “dealers” and some from individuals who want to show off their personal favorite items. Sometimes it’s hard to tell what’s a hidden treasure, and what’s a waste of time. It’s not hard to find sites if you use a search engine like Google, or a subject directory like Yahoo (or YahooKids!) A website domain can provide a hint about its content. Sites from commercial businesses usually include “.com”; federal government sites end in “.gov,” K12 school sites often include “k12” in the address, and college and university sites often include “.edu.” Sites from non-profit organizations often include “.org.”\nChildren’s librarians evaluate many types of media for children including books, audio books, e-books, videos, apps and websites. At this site, we have assembled a collection of websites that are noteworthy for excellence and appropriateness for young people, whom we define as children up to and including age fourteen. In making our selections, we consider the following points: authorship, purpose, design and content.\n•We recognize that each site we include here may not meet all of the criteria to be considered a great site; however, we will use our discretion to make the best choices possible.\n• We will be selective in choosing sites that offer reviews from other sources for any type of media. Sites must be consistent with the Library Bill of Rights.\n• The content on the Web changes faster than anything we have ever seen in our culture. Therefore, in any recommended list of Websites, the recommendations apply only to the primary sites that are listed, not to every site linked from the primary sites.\nA. Authorship/Sponsorship: Who Put up the Site?\n• The name of the individual or group creating the site should be clearly stated.\n• The creator should give a source for information in the site where necessary.\n• The Web site author or manager should provide a way for users to make comments or ask questions.\n• The Web site author or manager should be responsive to any questions regarding copyright, trademark, or ownership of all material on the site. Sites that knowingly violate copyright statutes or other laws should not be linked, listed, or recommended.\nB. Purpose: Every Site Has a Reason for Being There.\n• A site’s purpose should be clear and its content should reflect its purpose.\n• Advertising should be limited and appropriate.\n• Sites devoted strictly to sales will not be considered.\n• A good site should enrich the user’s experience and expand the imagination. Sites promoting social biases rather than enlarging the views of the child will not be considered.\nC. Design and Stability: A Great Site Has Personality and Strength of Character.\n• The information on the site should be easy to find and easy to use.\n• The site design should be appealing to its intended audience.\n• The text should be easy to read, and not cluttered with distracting graphics, fonts, and backgrounds.\n• Users should be able to get around the site easily.\n• Pages consisting mainly of links should be well organized and appealing to the site's intended audience, and the collected links should be well-chosen and useful to targeted users.\n• The site’s design should be appropriate for the intended audience.\n• The site should be ADA (Americans with Disabilities Act) compliant, as much as possible.\n• A game or recreational site should have a clear interface and playing instructions.\n• The page should load in a reasonable amount of time.\n• The page should be consistently available and load without problems; stability is important.\n• Required “plugins” or other helper applications should be clearly identified.\n• The design elements and features on the site, such as searchable databases, animations, graphics, sound files, introductory and transitional pages, etc., should enhance and not hinder the accessibility and enjoyment of the site.\n• The interactive features should be explained clearly.\n• Graphics on the site should be relevant and appropriate to the content.\nD. Content: A Great Site Shares Meaningful and Useful Content that Educates, Informs, or Entertains.\n• The title of a site should be appropriate to its purpose.\n• A site’s content should be easy to read and understand by its intended audience.\n• There should be enough information to make visiting the site worthwhile.\n• If there are large amounts of information on the site, some kind of search function should be provided. There should be at least an outline of topics covered, allowing the users to find topics and move among them easily.\n• Spelling and grammar always should be correct.\n• The information should be current and accurate, and if the topic of the site is one that changes, it should be updated regularly. A “last updated” date is a plus.\n• Links to more information on the topic should be provided.\n• The subject matter should be relevant to and appropriate for the intended audience.\n• The viewpoint presented should be comprehensible to the intended audience.\n• The skills required to use the site’s features and structure should be appropriate or appropriately challenging for its intended audience.\n• In informational sites, especially those used to support school assignments, quality of content should be most important. Appealing sites for general audiences that are accessible to young people sometimes provide the highest quality content.\n• Some sites, such as health and life education sites, may include mature content. Such material should be developmentally appropriate to the information needs of youth.\n• A user should not need to pay a fee before using the site. Requiring users to supply names, email addresses or other personal information is not acceptable except to access limited or restricted portions of the site.""]"	['<urn:uuid:a11e199c-1472-487c-9483-c793c4258654>', '<urn:uuid:86d6999b-b1f5-48dd-86cd-7ef7eecaa79d>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-13T05:28:56.360010	25	57	1638
39	Did Marrakech and medieval England both mix medicine with star-reading?	Yes, both locations combined medicine with astrology. In medieval England, John Arderne believed in practicing astrology for diagnosis, treatment and predicting ailments. Similarly, in Marrakech, Ibn Rushd conducted astronomical observations while serving as chief physician, combining medical practice with celestial studies.	"[""Answered By: Katie Hutchison Last Updated: Aug 24, 2016 Views: 266\n- Medical/astrology images from various texts. Brief descriptions - http://www.bl.uk/learning/artimages/bodies/astrology/gallery/astrogallery.html and http://www.bl.uk/learning/histcitizen/medieval/medicine/medievalmedicine.html\n- Also check out the database ARTstor. All artwork is considered a primary source and you could do a “reading” of the piece if one fits into the theme of your paper.\n- Harley Science Project Digitised Manuscripts - Images and descriptions of 150 medieval and modern scientific manuscripts from the British Library’s Harley collection. You can limit to a year range and do a keyword search.\nJohn Arderne was a 14th century doctor from England who learned surgery during the 100 Years War and had breakthroughs treating anal fistulas. Although pioneering in some ways, Arderne still believed in some of the rather unscientific medical practices of the Middle Ages. His methods are often obscured by his use of nonsensical names for ointments and plaisters, while many of his recipes for prescriptions are vague - apparently deliberate ploys to prevent his competitors from stealing his ideas. He was prepared to include folk charms and popular remedies in his texts, and believed in practicing astrology in the diagnosis, treatment and prognostication of ailments, as was the norm.\n- Secondary source info on Arderne: https://universityofglasgowlibrary.wordpress.com/2012/09/25/john-of-arderne-the-father-of-english-surgery/\n- Full Text of Treatises of fistula in ano : haemorrhoids, and clysters http://quod.lib.umich.edu/c/cme/ArderneFistula?rgn=main;view=fulltext\nYou can also order this via OhioLink if you prefer a hard copy: http://olc1.ohiolink.edu/record=b10526281~S0\n- Full text of De arte phisicali et de cirurgia (The Art of Medicine and Surgery) – English translation\nhttps://archive.org/stream/deartephisicalie01ardeuoft/deartephisicalie01ardeuoft_djvu.txt Sorry it isn’t the most user friendly. It was the best I could find.\n- The medieval surgery by Tony Hunt. Presents the complete set of illustrations which accompany a 13-th century Anglo-Norman translation of Roger of Parma's Surgery (c.1180), the first original treatise on surgery to be written in the medieval West. His commentary relates the drawings precisely to the sections of text they illustrate, providing accurate identification of the different medical treatments depicted.\nAvailable via OhioLink: http://olc1.ohiolink.edu/record=b33135467~S0\n- Tacuinum sanitatis – a medieval health handbook\nAvailable at Walsh: http://cat.opal-libraries.org/record=b1696576~S20\n- The Middle English Translation of Guy De Chauliac's Anatomy\nAvailable via OhioLink: http://olc1.ohiolink.edu/record=b11098427~S0\n- The 'Liber de Diversis Medicinis.' - A collection of medical prescriptions and charms written in a mid-fifteenth century hand; the scribe, a Robert Thornton, has been variously identified with three persons of the name who flourished during that period.\n- Calendar and the Cloister - The Calendar and the Cloister is a scholarly resource devoted to a single medieval manuscript: Oxford, St John's College 17. This splendid volume was created in the first decade of the 12th century at Thorney Abbey, a Benedictine monastery in Cambridgeshire. St John's 17 is a compilation of texts, tables, maps and diagrams. It is organized around the central theme of time-reckoning and calendar construction — what in the Middle Ages was called computus.\n- Medieval and Early Renaissance Medicine: An Introduction to Knowledge and Practice\nWalsh e-book: http://cat.opal-libraries.org/record=b2865327~S20\n- Medicine & Society in Later Medieval England\nAvailable via OhioLink: http://olc1.ohiolink.edu/record=b25131071~S0\n- Harry Bober, “The Zodiacal Miniature of the Très Riches Heures of the Duke of Berry: Its Sources and Meaning,” in Journal of the Warburg and Courtauld Institutes 11 (1948).\n- Visualizing Medieval Medicine and Natural History\nAvailable via OhioLink: http://olc1.ohiolink.edu/record=b25145539~S0\n- Hilary M. Carey, “What is the Folded Almanac? The Form and Function of Key Manuscript Sources for Astro-Medical Practice in Later Medieval England,” Social History of Medicine 16 (2003): 481-2.\n- Hilary M. Carey, “Astrological Medicine and the Medieval English Folded Almanac,” Social History of Medicine 17:3 (2004).\n- Popular medicine in thirteenth-century England : introduction and texts – looks like this may contain primary sources within it?\nAvailable via OhioLink: http://olc1.ohiolink.edu/record=b14868790~S0\n- Doctors and medicine in medieval England, 1340-1530\nAvailable via OhioLink: http://olc1.ohiolink.edu/record=b10588656~S0\n- Medieval medicus: a social history of Anglo-Norman medicine\nAvailable via OhioLink: http://olc1.ohiolink.edu/record=b10667585~S0\nFor more help, contact the history liaison librarian Katie Hutchison at 330.244.4968 or email@example.com."", 'Marrakech became, due to the ambitions and sponsorship of its rulers, the centre of attraction for numerous scholars including Ibn Rushd who served as the Chief Physician and where he pursued many works in science. This is a glimpse into the important role of Marrakech in Islamic science.\nMarrakech was founded in about 1070 CE by the Almoravids. The city became the capital of the empire which was in the Sahara as well as the Maghrib. It was strategically placed on the plain of the Tensift River, just within the arc of the Atlas Mountains at the convergence of two major routes across the mountains. In 1147 Marrakech fell to the Almohads of the High Atlas, who made it their own capital. Even when the capital was later moved to Seville, the city was the centre of the Almohad community with its scholars and military. Marrakech became, due to the ambitions and sponsorship of its rulers, the centre of attraction for Maghribi scholars and even a certain number from Spain. In 1153 Ibn Rushd became engaged in astronomical observations in Marrakech. In 1163 he became associated with the Almohad court; the philosopher Ibn Tufayl (1105-1185) introducing him to the Almohad ruler, Abu Yaqub, who was greatly interested in philosophy. Ibn Tufayl recommended Ibn Rushd for this task.\nThe love of books is as strong in the Maghrib (literally ""the west"") as in the Machrek (the east). Marrakech is known for the Kutubiya Mosque which is famed for its books, manuscripts, libraries and book shops, all of which gave the mosque its name. The Kutubiya with its hundred or so book sellers gathered in the shade of the minaret, and the many intermediaries who rushed between places searching for rare and new manuscripts to copy; and also the dallals (traders) who bought and sold ancient works from and to the scholars of the city. The sultans themselves collected both works and their authors, whom they wanted to have very close to them. In Marrakech there was also a great tradition of constructors of astrolabes,  Detailed information on such people and their accomplishments can be found in Mayer.\nMany historians flourished in Marrakech, most living in close proximity to the Caliphs, such as Abu Bakr al-Sanhadji, who wrote extensively on the Almohads, and whose works were traced by Levi Provencal to the Spanish collection at the Escorial. Because he observed the events from close at hand, what he describes bears the stamp of authenticity on the Almohad movement in history. Another historian born in Marrakech in 1185, but who studied at Fes, was Abd al-Wahid al-Marakushi. In about 1224, he wrote his Kitab al-Mujib fi talkhis akhbar al-Maghrib, which is a good personal account of the history of the Western Maghrib; many details found in the work are unique in their genre.\nOne of the greatest accomplishments of Almohad rule was the Marrakech hospital, also called the Bimaristan of Amir al-Muminin al-Mansur Abu Yusuf. Al-Mansur ruled Morocco from 1184 until his death in 1199. Abu Yusuf ordered the masons and the builders to carry out his plans to the highest standard. He decorated the hospital with inscriptions and designs of exceptional beauty. He ordered that flowers should be planted and cultivated in the courtyard, as well as fruit trees, and to have flowing water conducted to all the wards and rooms. He ordered the hospital to be equipped with furniture and to be covered with tapestries of wool, linen and silk, which gave an indescribable richness. He endowed it with ample waqfs and donations, providing the hospital with a daily sum of forty dinars for its expenses.\nTwo great scientists repute had links with Marrakech: Ibn Rushd and the mathematician Al-Marrakushi.\nIbn Rushd was born in 1126 in Cordoba into a family of learning and culture; both his father and grandfather were distinguished judges. In 1153, he was in Marrakech engaged in astronomical observations. Ibn Rushd succeeded Ibn Tufayl as chief physician in 1182 and served Abu Ya\'qub Yusuf until the latter\'s death in 1184. He then served his son and successor, Ya\'qub al-Mansur. Although engaged in astronomical observations in Marrakech, and his scientific treatise on the motion of the sphere (Kitab fi harakat al-falak), Ibn Rushd is best known for his very influential medical work, Kitab al-kulliyat fil-tibb (hence the Latin name Colliget from al-Kulliyyat (The generalities). It was written before 1162, in seven books treating respectively of anatomy, health (physiology), general pathology, diagnosis, materia medica, hygiene, and general therapeutics. Ibn Rushd recognized that no one catches smallpox twice. He also explained the function of the retina. Ibn Rushd\'s Kulliyat was translated into Latin by Bonacossa. With Ibn Zuhr\'s Taisir, it was according to Sarton `the most valuable of their kind in medieval times.\'\nAbd al-Wahid Al-Marrakushi was born in Marrakech in 1185; studied there, also in Fez, and after 1208 in Spain, but left it in 1217. In 1224, he completed a history of the Almohad dynasty, preceded by a summary of Spanish history from the Muslim conquest to 1087 (Kitab al-mujib fi talkhis akhbar ahl al-Maghrib). The text has been edited by Dozy. There is a French translation by Fagnan. Extracts can be found in Wustenfeld and Levi Provencal. Hassan-al-Marrakushi’s main work is Jami al-Mabadi wal-ghayat (the Unity of the beginnings and ends; i.e: principles and results), probably completed in 1229-1230. This is a very good compilation of practical knowledge on astronomical instruments and methods, trigonometry and gnomonics. Part of this work has been translated by Sedillot. The Jami of Hassan al-Marrakushi was, Sarton holds, the most elaborate trigonometrical treatise of the Western caliphate, the best medieval treatise on practical astronomy, on gnomonics, the best explanation of graphical methods. The part dealing with gnomonics contained studies of dials traced on horizontal, cylindrical, conical, and other surface for every latitude. Al-Marrakushi gave a table of sines for each half degree, also tables of versed sines and arc sines (this last one he called the table of al-Khwarizmi). To facilitate the use of gnomons he added a table of arc cotangents. The second part of al_jami was devoted to the explanation of graphical methods of solving astronomical problems. Al-Marrakushi\'s work develops the construction of planispheres, astrolabes, quadrants and the needs of gnomonics, which was the great interest of Sedillot who has written by far the best account on Muslim astronomical instruments.\nAl-Marrakushi shows his familiarity with the mathematical and astronomical works of al-Khwarizmi, al-Farghani, al-Battani, Abu\'l Wafa, al-Biruni, Ibn Sina, al-Zarqali, and Jabir Ibn Aflah. For example, he shared al-Zarqali\'s belief that the obliquity of the ecliptic oscillates between 23 degrees and 33\' and 23 degrees 53\', a belief which tallied with the notion of the trepidation of the equinoxes.\nIt is interesting here to note that Al-Marrakushi, on the evidence provided in his manuscripts, has devoted much study to trigonometry and associated subjects, and then to read in works on the history of science, including by Crombie one of the most renowned figures of such history, who says the following:\n`The development of modern trigonometry dates from mathematical work done in Oxford and France in the fourteenth century in connection with astronomy.\'\nHad Crombie, just briefly consulted al-Marrakushi, he would have realised how far from the truth he was.\n-M. Brett: Marrakech in Dictionary of the Middle Ages; Charles Coulston Gillispie Editor in Chief; Charles Scribner\'s Sons; New York; 1980 onwards; vol 8; pp 150-1.\n-R. Brunschvig: Un aspect de la literature history-geographique de l\'Islam; Melanges Gaudefroy Demombynes.\n-A.C. Crombie edition: Scientific Change, Heinemann, London, 1963.\n-G. Deverdun: Marrakech; Editions Techniques Nord Africaines; Rabat; 1959.\n-R.P. A. Dozy: The history of the Almohads; Leiden 1847; again, 1881.\n-R. Landau: Morocco: Elek Books Ltd, London 1967.\n-A.L. Mayer: Islamic astrolabists, Albert Kundig edition, Geneva, 1956.\n-Abdel Wahid al-Marrakushi, The History of the al-Mohades, edited by R.Dozy; Leiden; 1881.\n-E.L. Provencal: Al-Maghrib; in Encyclopaedia of Islam; New edition; Vol. 5; 1986; Leyden; pp. 1208-9.\n-G. Sarton: Introduction to the History of Science; 3 vols.; The Carnegie Institute of Washington; 1927-48.\n-L. Sedillot: Memoire sur les instruments astronomique des Arabes, Memoires de l\'Academie Royale des Inscriptions et Belles Lettres de l\'Institut de France 1: 1-229; Reprinted Frankfurt, 1985.\n E.L. Provencal: Al-Maghrib; in Encyclopaedia of Islam; New edition; Vol. 5; 1986; Leyden; pp. 1208-9.\n M. Brett: Marrakech in Dictionary of the Middle Ages; Charles Coulston Gillispied Editor in Chief; Charles Scribner\'s Sons; New York; 1980 onwards; vol. 8; pp. 150-1.\nR. Landau: Morocco: Elek Books Ltd, London 1967. p.80\n G. Deverdun: Marrakech; Editions Techniques Nord Africaines; Rabat; 1959. pp. 264-5.\n G. Deverdun: Marrakech; 1959.pp. 264-5.\n G. Deverdun: Marrakech; .p. 262.\n A.L. Mayer: Islamic astrolabists, Albert Kundig edition, Geneva, 1956.\n G. Deverdun: Marrakech; op cit.; .p. 263.\n R. Brunschvig: Un aspect de la literature history-geographique de l\'Islam; Melanges Gaudefroy Demombynes.\n G. Deverdun: Marrakech; op cit.; p. 263.\n Abdel Wahid al-Marrakushi, The History of the al-Mohades, edited by R. Dozy; Leiden; 1881; p.209.\n G. Sarton: Introduction to the History of Science; 3 vols; The Carnegie Institute of Washington; 1927-48. Vol. 2; p. 69.\n G. Sarton: Introduction; vol. 2; p.681.\n R.P. A. Dozy: The history of the Almohads; Leiden 1847; again, 1881.\n French translation by Edmond Fagnan (Revue Africaine, Vols. 36 and 37, passim; separate edition, 332 p., Alger 1893).\n Critiscism: F. Wustenfeld: Geschichtschreiber der Araber (109, 1881). C. Brockelmann: Arabische Litterartur (vol.1, 332, 1898). E. Levi Provencal: Documents in edits d\'Histoire almohade (440p., Paris, 1928, p; Isis, 13, 221).\n G.Sarton: Introduction; op cit.; vol. 2; p.621.\n E.L. Provencal: Al-Maghrib; in Encyclopaedia of Islam; pp. 1208-9.\n G Sarton: Introduction; op cit.; p. 508.\n G. Sarton: Introduction; op cit.; vol. 2; p.621.']"	['<urn:uuid:ad74b5b1-269c-4066-bb77-c6b180eee3ab>', '<urn:uuid:093ba514-1edc-4b23-9b88-0a8f32516a50>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T05:28:56.360010	10	41	2263
40	I work with graphs often. What's the meaning when data points form a flat line?	When data points form a horizontal line on a graph, it indicates that the Y scores are not changing as the X scores change, and there is no relationship between the variables.	"[""With respect to other scores in a distribution, measures of central tendency...\nare the points around which most of the scores are located\nIn order to decide which measure of central tendency is appropriate, you must first determine\nthe scale of measurement being used and the shape of the distribution\nWhich measure of central tendency should a researcher use to describe the sex of participants in a study?\nWhich measure of central tendency is appropriate if the shape of the distribution is severely skewed?\nWhy si the median unaffected by extreme scores occurring in only one tail of the distribution?\nBecause the median doesn't take into account the actual values of all scores\nWhich measure of central tendency shiould an academic counselor use to describe a student's rank in his/her classes?\nTo obtain the mean, we would\nadd all the scores (sigma X) and divide by the total number of scores (N)\nWhich measure of central tendency is appropriate if the shape of the distribution is symmetrical and the measurement scale is interval or ratio?\nThe mean is the preferred measure of central tendency when\nthe distribution is symmetical and the scale of measurement is interval or ratio\nAn experimenter investigated the ability to concentrate as a function of crowding. Concentration was measured as the amount of time it took the participant to complete a word puzzle. How should the experimenter summarize teh scores on the dependent variable?\nFind the mean amount of time it took to solve the puzzle, if time scores are normally distributed.\nIn a skewed distribution, the mathematical center is\nthe mean, which is not the point around which most of the scores tend to be located\nThe mean is an inappropriate measure of central tendency when the distribution is severely skewed because\nit does not accurately describe a skewed distribution\nWhat happens to the mean of a distribution if every score is divided by 10?\nIt's value is divided by 10.\nThe mean si used most often in behavorial research because researchers tend to\nmeasure variables that have interval or ratio scores, and the scores form approximately normal distributions.\nA score's deviation conveys two pieces of information about the score's location: the number indicates _____, and the sign indicates _____.\nthe score's distance from the mean, whether the score is greater or less than the mean\nThe best predictor of an individual score in a sample of scores is the\nmean of the sample of the scores\nWhen the mean is used to predict the scores, a deviation (X - X bar) indicates\nthe difference between the X bar we predict and the score an individual actually gets\nA deviation score is more informative than a raw score because it\ngives the score's location relative to the mean\nWith respect to a graph of a frequency distribution, a positive deviation score\nwill be located to the right of the mean\nWhen it is impossible to obtain all the score in a population, the best estimate of the population mean is the\nThe population mean is estimated by\ncalculating the mean of a random sample drawn from the population\nWhen deciding which type of measure of central tendency is appropriate, we consider the scale of measurement used to measure the\nA researcher has conducted an experiment in which the independent variable is room temperature. Two conditions (a hot room and a cold room) were used. The dependent variable was the length of time required to complete a jigsaw puzzle. What is the best way to state that there is a relationship?\nThe mean times to complete the puzzle are different for the two rooms.\nIn a graph of the relationship between the level of noise in an environment and the number of errors a person makes, the _____ is on the X axis and the ______ is on the Y axis.\nlevel of noise, number of errors\nWhen deciding which type of graph is appropriate, we consider the characteristics of the\nWhen we graph the results of an experiment, the Y axis indicates the\nmeasure of central tendency we have used for the dependent variable\nWhen we graph results from an experiment, a line graph is appropriate when\nthe independent variable is ratio or interval\nOn any graph a horizontal line of data points indicates that\nthe Y scores are not changing as the X scores change, and there is no relationship.\nWhen we graph results from an experiment where the independent variable is on a nominal scale, which type of graph is appropriate?\nAn experimenter investigated the abilitty to concentrate as a function of eye color. Which type of graph should the experimenter use to display the results?\nMeasures of variability are used to\nsummarize and describe the extent to which scores in a distribution differ from one another\nMeasures of central tendency indicate the _____ of a distribution while measures of variability indicate the _____ between the scores in a distribution\nThe greater the variability in a set of scores,\nthe less accurately the scores are represented by one central score.\nThe range is the descriptive statistic that indicates the\ndistance between the two most extreme scores\nThe average of the deviations can never actually be computed because\nthe sum of all deviations from the mean always equals zero\nWhen computing the variance, why do we square the deviations from the mean?\nto compensate for the fact that deviations about the mean always sum to zero\nIf the variance for a sample is computed and it is found to be rather large, the numbers\nare spread out around the mean\nStandard deviation is defined as the square root of the\naverage of the squared deviations around the mean\nAdding or subtracting a constant from each of the scores in a distribution\ndoes not change the value of the standard deviation\nMultiplying each of the scores in a distribution by a constant\nmultiplies the standard deviation by the same constant\nSample standard deviation and sample variance are considered biased estimates for the population standard deviation and variance because, over many calculations, they tend to be\nSample standard deviation and sample variance are considered biased estimates for the population standard deviation and variance because\nthey reflect the random variability of only N - 1 scores\nUnbiased estimators of the population parameters will produce values that are _____ those produced the biased estimators of the sample statistics.\nIf we are going to predict future performance on the basis of a sample mean and the sample standard deviation, it is desirable to have a\nsmall standard deviation\nIn the language of statistics, when we know that a relationship exists between two variables, we can use knowledge of that relationship to\naccount for the variance\nThe proportional improvement that resutls from using the relationship between two variables to predict scores compared with not using the relationship to predict scores is called\nthe proportion of variance accounted for.\nOf the three kinds of variances, which uses N-! in the final division?\nestimated population variance\nThe absolute value of a number is the\nnumeric magnitude of the number, regardless of whether is it positive or negative\nAn evaluation of where a score is located in relation to other scores in the distribution reflects its\nThe z-score transformation is a useful statistical tool because it enables statisticians to\ncompare and interpret scores from virtually any distribution\nWhen the standard deviation of a raw score distribution is large, the ocrresponding z-score distribution will be\nrelatively spread out\nGiven any z-score, it is safe to say that the absolute value is a good indicator of ______ and the sign is a good indicator of _____.\nGiven a normal distribution, as z-scores' absolute values increase, those z-scores and the raw scores that correspond to them occur\nThe mean of a z-score distribution is always _____, and the standard deviation is always _____.\nWhen two normal z-distributions are plotted on the same graph, what can we say about the relative frequency of each z-score?\nIt will always be the same.\nThe proportion of the total area under a normal curve between two z-scores corresponsd to the _____ of the range of scores.\nA theoretically perfect normal curve, which serves as a model of the perfect normal z-distribution, is called the\nstandard normal curve\nThe relative frequency obtained from the standard normal curve is the _____ of the raw scores in our data, if the data formed a perfect normal distribution.\nexpected relative frequency\nHow accurately the standard normal curve model predicts the actual relative frequency of raw scores depends on three aspects of data:\n1) the raw scores form an approximately normal distribution, 2) there is a large sample N, 3) the raw scores are theoretically continuous scores measured on an interval or ratio scale\nWe can use the standard normal curve as our model for\nany approximately normal distribution, when transformed to z-scores\nIn sampling distributions, all the samples contain sets of raw scores\nthat are representative of the population mean\nWhich of the following statements accurately describes the sampling distribution of means?\nThe distribution of all possible sample means when an infinite number of samples of variously sized Ns are randonmly selected from several raw score populations.\nAs the N of the samples used in a sampling distribution _____, the sampling distribution becomes _____.\nincreases, more like a perfect normal curve\nA sampling distribution is an approximately normal distribution\nonly when the shape of the raw score distribution is approximately normal\nAccording to the central limit theorem, the sampling distribution of means always approximates a _____ distribution.\nThe mean of the sampling distribution always equals\nthe mean of the underlying raw score population.""]"	['<urn:uuid:51340026-0353-4b9a-853e-ea686da06176>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T05:28:56.360010	15	32	1619
41	I work with both dairy and meat processing facilities. What are the key differences between interstate shipping requirements for Grade A milk products versus custom-exempt meat processing operations?	For Grade A milk products, facilities must comply with the Pasteurized Milk Ordinance (PMO) standards and be listed on the Interstate Milk Shippers (IMS) list to ship interstate. They must meet specific construction, milk quality, and operation standards, including time, temperature, and equipment specifications. In contrast, custom-exempt meat processing operations cannot sell or ship their products in interstate commerce at all - the products from custom slaughter can only be used by the owner, their family, household guests and employees. Custom slaughter is exempt from regular inspection requirements but must still maintain sanitary conditions as outlined in 9 CFR 416.1-416.5.	"[""Dairy product examples include fluid milk, cheese, ice cream, butter, cottage cheese, yogurt, and sour cream.\nA dairy license is required for the following activities:\n- Producing milk on a Grade A dairy farm\n- Marketing milk for producers as a milk marketing agent or handler\n- Transporting milk in a milk tanker or transferring milk to a transfer or receiving station\n- Processing milk to manufacture any fluid milk or dairy products, including the wholesale manufacture of frozen dairy desserts (as defined in 21CFR Part 135)\nDairy operator license\nA dairy operator license is required for the following operators:\n- Individuals who collect raw milk samples to be used for regulatory purposes\n- Individuals responsible for the pasteurization of milk or dairy products\nWhat kind of dairy operators have an exception from licensing?\n- A person owning no more than three dairy cows (no more than two producing) that have calved at least once\n- A person owning no more than nine sheep that have lactated at least once\n- A person owning no more than nine goats that have lactated at least once\nThe fluid milk from these animals may be sold for human or other consumption only if the milk is sold directly to the consumer at the premises where produced.\nWhat activities are prohibited?\n- Sales of unlicensed milk or milk products where a license is required\nRetail sale of unpasteurized milk from cows (unless meeting the limits of the exception above) is prohibited.\nA person may not sell, or distribute for sale, the following products:\n- Unpasteurized milk or fluid milk from cows\n- Dairy products from unpasteurized milk or fluid milk from cows, unless sold to a\n- Dairy products plant licensee\n- Non-processing cooperative\nThis does not apply to the sale or distribution of cheese by a licensed facility otherwise exempt from pasteurization requirements or to sales or distributions by a person described under ORS 621.012.\nPlease see the advertising raw milk directive for more information on 621.012(1).\nDistributor or producer-distributor may only sell milk that is pasteurized, or from disease-free goats or sheep.\nA distributor, producer-distributor, or dairy products plant licensee shall not sell, offer, or expose for sale any dairy product, or fluid milk, for human consumption unless the milk used in the dairy product, or fluid milk, has been pasteurized or is goat or sheep's milk that was produced by a disease-free herd.\nPositive brucellosis testing\n- When not more than one reactor animal appears after the goat or sheep herd is tested for brucellosis, the milk, dairy products, or fluid milk may still be sold\n- If the animal is slaughtered\n- No additional reactor animals appear when the herd is retested\n- When one or more reactor animals appear after the herd is retested, no milk, dairy products, or fluid milk from the herd may be sold until the herd regains a brucellosis-free status\nPasteurized Milk Ordinance (PMO)\nThe PMO is developed by the National Conference of Interstate Milk Shippers (NCIMS) and covers construction, milk quality, and operation standards for all dairy operations including:\nThe PMO includes standards for time, temperature, and equipment specifications.\nCompliance with the PMO is required for plants or farms to produce and ship Grade A milk. Grade A milk or milk products that are shipped interstate must be from production facilities listed on the Interstate Milk Shippers (IMS) list. This list includes dairy operations that are approved for interstate milk shipment.\nAdvertising raw milk directive\nHB 2446 would repeal ORS 621.012(1), which prevents a person from advertising raw milk for sale. Document\nDairy Processing Program: OSU Extension\nPromote the production of safe and high quality dairy products and to support a healthy and sustainable dairy industry in Oregon. Website\nFDA Milk Guidance Documents and Regulatory Information\nGrade A Pasteurized Milk Ordinance\nInterstate Milk Shipper's List"", 'Following are some frequently asked questions that will hopefully answer some of the confusion: NOTE: Throughout this article, the use of “his” can mean “his” or “hers”. ""Individual"" denotes only one.\n1. Q: Do all states that have meat inspection programs have to follow federal regulations?\nA: Yes. To have a meat inspection program, a state must enforce laws and regulations that are “at least equal to” the federal meat and poultry regulations. In fact, many state meat inspection programs enforce laws that are more restrictive than the federal (or Vermont) statutes. If a state does not have a state meat inspection program, the USDA Food Safety Inspection Service has jurisdiction and enforces the federal meat and poultry laws and regulations for slaughter and processing activities that take place within the state (intrastate).\n2. Q: What does “on farm slaughter” mean?\nA: In the context of this document, “on-farm slaughter” is a broad term that simply means the activity of killing an animal on a farm, to use the meat and meat food products that are derived from that food animal.\n3. Q: What does “on farm slaughter” have to do with exemptions from inspection?\nA: “On farm slaughter” can take place on a farm without inspection if the farmer qualifies to use one of the exemptions from inspection. Product produced without inspection is only for the use of the owner, his family, household guests and employees, and cannot be sold. Depending on what criteria the farmer choses to meet, the various exemptions are:\n1) the Personal Use Exemption,\n2) the “On-farm” Personal Use Exemption”, or\n3) the Custom Slaughter Exemption.\n“On farm slaughter” can also take place on a farm if there is a state or federally inspected slaughter facility located on that farm. Product produced under inspection can be labelled and sold.\n4. Q: Can an individual slaughter animals of his own raising on his farm?\nA: Yes. Per 9 CFR 303.1(a)(1), and 6 V.S.A §3305(8), this is permissible if the Personal Use Exemption criteria are met.\n5. Q: Can I sell a live animal and then raise it on my farm for the buyer?\nA: Yes. The buying and selling of live animals, and contract growing, is governed by laws of other local, state, and federal agencies. However, if you want to have those animals slaughtered on your farm, see Question numbers 5, 6 and 7.\n6. Q: Can I sell a live animal to an individual, and let the buyer slaughter that animal on my farm?\nA: Yes. Per 9 CFR 303.1(a)(1), and 6 V.S.A 3311(c), this is permissible if the “On-Farm” Personal Use Exemption criteria are met.\n7. Q: Can I sell a live animal to an individual, and then slaughter the animal for that buyer?\nA: This would fall under the definition of a custom slaughterhouse, as a custom slaughter operation. Therefore, this is permissible, if the Custom Slaughter Exemption criteria are met. Note: this activity would not meet the criteria of the ""On-Farm"" Personal Use Exemption because the farmer who sold the animal cannot perform the slaughter.\n8. Q: Can multiple owners own a live animal together?\nA: Yes. The buying and selling of live animals, and contract growing, is governed by laws of other local, state, and federal agencies. Also see question 8 related to the slaughter of an animal that has multiple owners.\n9. Q: Can I sell one live animal to multiple buyers (i.e. animal sharing) and have that animal slaughtered on my farm?\nA: The slaughter of animals with multiple owners falls under the definition of custom exempt activities. Therefore, this is permissible, if the Custom Slaughter Exemption criteria are met. The “On-Farm” Personal Use Exemption does not apply to animals with multiple owners. Also see question number 10.\n10. Q: If I sell a live animal to an individual, what options exist for the buyer to have that animal slaughtered?\nA: There are several options:\n1. The buyer can bring that animal to his own farm and slaughter it under the Personal use exemption (see criteria for Personal use exemption);\n2. The buyer can slaughter the animal on the farm where it was purchased under the “On-farm” Personal use exemption (see criteria for “On-farm” Personal use exemption);\n3. The buyer can bring that animal to a licensed custom slaughter facility;\n4. The buyer can bring the animal to any of the state or federally inspected slaughter facilities in the state; or\n5. The buyer can have the farmer deliver the animal for them to #3 or #4 above.\n11. Q: If I sell a live animal to multiple owners (i.e. animal sharing), what options exist for the buyers to have that animal slaughtered?\nA: The options include:\n1. The buyers can bring that animal to a licensed custom slaughter facility; or\n2. The buyers can bring the animal to any one of the inspected slaughter facilities in the state; or\n3. The buyers can have the farmer deliver the animal for them to number 1 or 2 above.\n12. Q: Are there any requirements for the transportation of finished carcasses?\nA: It is recommended that care should be taken to protect the carcasses from becoming contaminated or adulterated by dust, dirt, debris, flies, etc. during transportation.\n13. Q: Can an animal that can no longer walk be slaughtered for human food purposes?\nA: No, a nonabulatory animal (i.e. a downer) cannot be slaughtered for human food purposes. See official answer from the Food Safety Inspection Service.\n14. Q: What are the sanitation requirements in 9CFR 416.1-416.5, and where can I find them or other inspection requirements?\nA: The sanitation requirements are a common-sense list of sanitary facility requirements, which if followed, can decrease the risk of foodborne illness and adulteration of product. Some examples include: washable and sanitizeable walls, floors, ceilings; pest control; sufficient plumbing, sewage, and drainage to handle the waste; hot/cold running water that is potable; washable and sanitizeable utensils and equipment. There is also a requirement for recordkeeping.\nWe encourage you to contact the VT Agency of Agriculture, Food and Markets Meat Inspection Section at 802-828-2426 for specific questions or clarification. Also visit our web site at www.vermontagriculture.com.\nPlease be advised that if you do not follow the guidelines described above, you could be in violation of Title 6 Chapter 204 of the Vermont Statues, the Federal Meat Inspection Act, and or the laws of other local, state and federal agencies. Violators of Title 6, Chapter 204 of the Vermont Statues may be liable for a civil penalty up to $1000.00 per violation, and/or subject to other penalties or criminal prosecution.']"	['<urn:uuid:0184c641-c079-4f3e-9fdd-1222a5cdf089>', '<urn:uuid:99ba0388-e650-408b-8e5e-cd849c0328b7>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-13T05:28:56.360010	28	100	1748
42	what freedoms first amendment guarantees	The First Amendment guarantees five key freedoms: freedom from establishment of religion, freedom of speech, freedom of the press, the right of people to peaceably assemble, and the right to petition the Government for redress of grievances.	"['The flashcards below were created by user\non FreezingBlue Flashcards.\n4 key roles of the courts\nGraphic of the U.S. Court System\nWhich are “appellate” (lower-case “a”) courts?\nWhich are trial courts?\nKey differences between trial and appellate courts\nOnly judges decide the outcome of trials in the U.S. Circuit Courts of Appeals, because appellate courts decide only on matters of law. Juries only serve on trial courts.\nan action taken by an appellate court in which it sends back a case to the trial court or lower appellate court for further action.\nGeneral jurisdiction courts\nA court of general jurisdiction is one that has the authority to hear cases of all kinds - criminal, civil, family, probate, and so forth.\nHow cases get to the Supreme Court.\nThe Supreme Court screening process consists of trying to hear those cases that raise the most significant legal issues, cases where the lower courts have flagrantly erred, and those where conflicting lower court decisions must be reconciled. The three ways a case can reach the Supreme Court are: The Constitution gives the Supreme Court original Jurisdiction over few types of cases. This means, the Supreme Court ruling is above the ruling of the first court to hear the case. Second, there are a few cases in which the losing party in the lower courts has an automatic right to appeal to the Supreme Court. Lastly, there are a vast number of cases that the Supreme Court may or may not choose to review; it is not required to hear those cases, but some raise very important questions.\nWrit of certiorari (cert denied, and rule of four).\nan order from the Supreme Court to send up the records of the case. certiorari granted means the court decided to hear the appeal, and certiorari denied means they chose not to hear it.\nMajority rule from Supreme Court\na written opinion by one or more judges of a court which agrees with the decision made by themajority of the court, but states different reasons as the basis for his or her decision.\nan opinion in a legal case written by one or more judges expressing disagreement with the majority opinion of the court which gives rise to its judgment\nResidual jurisdiction (as it pertains to state courts).\nAnything that is not a federal question, or an area in which the federal government has subject has subject jurisdiction, automatically falls within the jurisdiction of the state courts.\nan amorphous collection of legal principles based on court decisions passed down over centuries. It is an unwritten law. Common law includes rules concerning everything from crimes to non-criminal matters.\nStatutory Laws are written down in a systematic way . They are often organized into codes. A code is a collection of laws on similar subjects, indexed arranged by subject matter\n- Judge has authority to act in an ""emergency""\n- Injunction- righting a wrongdoing or preventing a wrongdoing.\n- one judge one order\n- ex. restraining oreder\nWhat’s a plaintiff? What’s a defendant (respondent)?\nIn a citation (for example, Smith v. Jackson), which is the plaintiff? Which is the respondent?\nGenerally, familiarize yourself with the process of a civil suit, as explained in class (complaint à response à courts waiting à “matter at issue” à appeal (appellant vs. appellee).\n- Dispute between two parties (plaintiff and defendant)\n- Failure to settle dispute (""matter is at issue"" ready for trial.\n- During an appeal if one party doesn\'t agree on decision, they can appeal it\n(a) What matters fall under the jurisdiction of state courts? (b) What makes an issue a federal question? (three areas).\n- Anything that is not a federal question, or an area in which the federal government has subject has subject jurisdiction, automatically falls within the jurisdiction of the state courts.\n- The factors that make an issue a federal question are: The Constitution declares that certain areas of law are inherently federal questions, federal courts may intervene in state cases if a state court ruling conflicts with the U.S. Constitution, and diversity of citizenship, which is when one party to a lawsuit is from one state and the other is from another state.\n(a) Can a law be enacted or enforced if it violates the U.S. Constitution? (b) What is “Common Law?” (c) What is “Statutory Law?”\n- No law may be enacted or enforced if it or enforced if it violates the Constitution.\n- Common Law is an amorphous collection of legal principles based on court decisions passed down over centuries. It is an unwritten law. Common law includes rules concerning everything from crimes to non-criminal matters.\n- Statutory Laws are written down in a systematic way . They are often organized into codes. A code is a collection of laws on similar subjects, indexed arranged by subject matter.\nHenry VIII’s licensing system and prior restraint\n- 15th century kings from same family.\n- Gutenberg printing press\n- Rulers feared power of press.\n- royal permission to print\n- prior restraint- leader steps in before communication is launched\n- Henry formed church of England\nincitement of resistance to or revolt against the gov.\nJohn Milton (marketplace of ideas) and John Locke (social contract theory). Study Q&A answers to questions 1 and 2.\n- John Milton\'s marketplace of ideas was the notion that there should be freedom of speech so that all ideas would have a chance to be heard, considered and compete for attention and believers. However, he did not think free speech should be granted to those who advocated ideas that he considered dangerously false or subversive.\n- John Locke\'s Social Contract theory states that a government where the people give up some of their rights to enjoy other rights, moving from a state of matter to a state of cooperation for self-governance; He thought that people should have rights to life, liberty and property rights. Locke thought freedom of expression was central.\n5 key freedoms guaranteed by First Amendment\n- • an establishment of religion\n- • freedom of speech\n- • freedom of the press\n- • the right of the people peaceably to assemble\n- • to petition the Government for a redress of grievances\nAttorney General v. Zenger: circumstances, court finding, precedent/legacy.\nZinger was charged by royal governor William Cosby with printing and publishing a false, scandalous and seditious libel, in which the governor is scandalized as a person that has no regard to law or justice. Zenger\'s attorney, Andrew Hamilton, urged the jurors to ignore the maxim of ""the greater truth, the greater the libel"" and to decide for themselves whether the statements in question were actually true, finding them libelous only if they were false. Hamilton urged the jurors to ignore the judge\'s instructions and Zenger was found not guilty, because the things he printed were true.\nPeople v. Croswell: circumstances, court finding, precedent/legacy.\n- • Reporter attacked Jefferson in print\n- • Convicted in New York state court\n- • Lost appeal\n- • Result: Hamilton Doctrine\n- • Hamiltion doctrine came from a loss.the truth + good motives should be a liable end.\nSchenck v. U.S.: circumstances, court finding, precedent/legacy.\n- Socialist leader undermined military draft\n- publishes 15k leaflets to people signing up to serve in military.\n- Convicted under Espionage Act of 1917\n- Supreme Court upheld conviction\n- “Clear and present danger” test\nAbrams v. U.S.: circumstances, court finding, precedent/legacy.\n- Leaflets critical of U.S. military decisions\n- Convicted under Espionage Act\n- Supreme Court upheld conviction\n- “Danger of immediate evil”\nGitlow v. New York: circumstances, court finding, precedent/legacy.\n- Writings advocated anarchy\n- Convicted for violating state statute\n- Supreme Court affirmed conviction\n- Incorporation doctrine\nWhitney v. California: circumstances, court finding, precedent/legacy.\n- Convicted in California for subversive group membership\n- women\'s rights group- Communist Labor Party\n- a member spoke of overthrowing gov/ arrested for being a member. 40 yrs later, conviction overthrown.\n- Supreme Court upheld conviction\n- Brandeis: “more speech, not enforced silence.”\nDennis v. U.S.: circumstances, court finding, precedent/legacy.\n- Convicted of under Smith Act\n- Appellate and Supreme Courts upheld conviction.\n- Hand’s “gravity of evil” test\n- He ruled that calls for the violent overthrow of the American government posed enough of a ""probable danger"" to justify the invasion of free speech\nYates v. U.S.: circumstances, court finding, precedent/legacy.\n- Yates, 13 others convicted under Smith Act\n- Supreme Court reversed convictions\n- Distinction: teaching abstract theory and advocating violent action\n- Only used of people using concrete action not theoretical.\nBrandenburg v. Ohio: circumstances, court finding, precedent/legacy.\n- Klan speaker called for government overthrow- Claims the white race had a more difficult time than any other in America.\n- Convicted under criminal syndicalism law- syndicalism law- no forming groups to preform acts of violence.\n- Supreme Court reversed & Ohio law invalidated\n- “Imminent lawless action” test- act against only if producing imminent lawless action. ""They can actually pull it off""\n- Reversed Whitney case after this case.ct against only if producing imminent lawless action. ""They can actually pull it off""\nIn the 1644, John Milton wrote a speech called Areopagitica (Air-e-o-pa-jitica) that denounced government censorship. (a) Describe his “marketplace of ideas” and the self-righting process that he was advocating; (b) Who did he believe SHOULD NOT have rights to free expression?\nJohn Milton\'s marketplace of ideas was the notion that there should be freedom of speech so that all ideas would have a chance to be heard, considered and compete for attention and believers. However, he did not think free speech should be granted to those who advocated ideas that he considered dangerously false or subversive.\n(a) Describe John Locke’s “social contact theory” and the natural rights that he believed were endowed to every person. (b) Which of these rights did he believe was central?\nJohn Locke\'s Social Contract theory states that a government where the people give up some of their rights to enjoy other rights, moving from a state of matter to a state of cooperation for self-governance; He thought that people should have rights to life, liberty and property rights. Locke thought freedom of expression was central.\n(a) Describe the Hamilton Doctrine and its origins with Alexander Hamilton as the defense attorney in People v. Croswell (1804)? (b) What was the doctrine’s relationship to the ANDREW Hamilton defense offered in the Zenger trial?\nThe Hamilton Doctrine dded a provision empowering the jury to determine whether the statement in question was actually libelous. The relationship between the doctrine and Hamilton is the defense he used in the Zenger trial. It gained him general acceptance in American law only after another lawyer named Hamilton made it his cause as well.\nWhen a gov or gov agent stops speech or expression before its communicated\nSocial impact of prior restraint (also found in your answer to Q&A question #1).\nsometimes government officials attempt to sensor the news media to prevent the dissemination of information that they see as a threat to national security.\nNear v. Minnesota: circumstances, court finding, precedent/legacy.\n- accused gov of being in the back pocket of mobsters\n- Trial court rules paper to be a public nuisance\n- prior restraint is unconstitutional\n- Supreme Court throws out law\n- censorship possible only under certain finely defined precedents\nNew York Times v. U.S. (Pentagon Papers case): circumstances, court finding, precedent/legacy.\n- Pentagon report about Vietnam War leaked to 2 newspapers\n- Papers initially ordered to stop publishing\n- Supreme Court sets aside prior restraint\nCan government employees be made to sign away their rights to free expression? Explain.\n- There are limits on those near sensitive information\n- No protection: National security, government secrets\n- Yes protection: Acting privately on matter of public concern\ntreating people a certain way based on who they are\nWhat is the fighting-words doctrine? Can hate speech be banned based on its content?\nThe Supreme Court’s stand on on the “fighting words doctrine” was “speech likely to cause a fight may be prohibited”. However, in 1992, they ruled that “hate speech” cannot be banned on the basis of its content, although violent action can and will be.\nLegality of “symbolic political expression” vs. words to incite violence.\nIs flag desecration legal?\nyes. US v Eichman- if it is your flag, you can do whatever you want with it.\nFamiliarize: The hierarch of “protected free expression”\nLiterature distribution and rallies:\nDifference between distributing materials in a mall walkway and a mall store.\n- Distribution of materials protected\n- A walkway belongs more so to the public\nThe FACE Act and abortion protests\n- Public streets, yes; private homes, no\n- “The Nuremburg Files”\n- “Floating bubble”\nDefined: Defamation, libel and slander.\n- damage to ones rep with a statement that isn\'t true\n- written defamatory statement that can\'t be proven as true\n- spoken statement"" ""\nAre defamation, libel and slander cases civil or criminal trials?\nWhy are only some instances of libel “actionable?”\n- can get someone sued\n- have to meet 5 elements\nTo whom does the burden of proof belong in defamation trials? What does this mean?\nFalls to the plaintiff. Plaintiff has to prove it is true\nCan a family sue for libel on the behalf of their deceased love one?\nNo, once you die, you lose your libel rights.\nCan a company sue?\nWhat about small groups? Large groups?\nCan be a plaintiff: Individuals, companies, individual members of a group, individual government officials\nCan an online provider be sued when someone libels someone else in a chatroom?\nLibel per se and libel per quod.\n- Libel per se is the classic kind of defamation where the words themselves can hurt person\'s reputation such as rapist or murderer.\n- When it is not immediately apparent that the words are libelous, or when one must know additional facts to understand that there is defamation, it is libel per quod.\n- The types of damages affiliated with these are special damages (provable monetary loss), general damages ( pain and suffering or embarrassment due to loss of reputation)\nFamiliarize: “Danger zones” of defamation.\n- Criminal behavior\n- Sexual implications\n- Would expose to ridicule\n- Business side: defaming business practices, credit, performance of service or product\nLibel element #2 (Identification)\n- Oblique reference-Clues that a reasonable person would understand\nLibel element #3 (Communication)\nWhat’s not protected.\nLibel element #4 (Fault)\n- Strict liability\n- Actual malice (fault for public figures): 1. Knew it was false and printed it anyway; 2. Printed it with “reckless disregard for the truth.”\n- Negligence (fault for private individuals): absence of reasonable care; failing to adhere to standards of good journalism.\n- plaintiff must prove the statement is false (rather than defendant prove it’s true).\n- Absolute and qualified privilege\nFair comment and criticism\n- Based on facts, correct & accurate\n- Concerning public performance\n- Pure expression of opinion > no provable facts\nNew York Times v. Sullivan: circumstances, court finding, precedent/legacy.\n- Ad charges civil rights abuses\n- Enough people could say who the ad was about\n- Alabama courts rule against Times\n- Supreme Court reverses\nCurtis Publishing v. Butts, joined with Associated Press v. Walker: circumstances, court finding, precedent/legacy.\n- CPvB- UGA director giving game film to bear bryant\n- Butts wins lower level and SC\n- APvW- Ole Miss first colored student\n- starts a protest ends in one dead\n- Walker won on lower level but SC overturned because there was proof that he started the protest\nGertz v. Welch: circumstances, court finding, precedent/legacy.\n- Gertz Chicago attorney representing family of a child shot by Chicago police. Welch wrote that Gertz is a communist fronter. Gertz wins on trial court level and is awarded $50k. Judge negates jury verdict because they are ruling on emotion and not by law. His comments needed to be malicious to win.\n- Gertz not a public figure, celebrity, or gov official. Private individual\n- Supreme Court says all you have to do is prove negligence which it did.\n- Jury changed $40k ruling to $400k ruling.']"	['<urn:uuid:9cc42858-aab1-4916-8680-9bf3d46678d3>']	factoid	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-13T05:28:56.360010	5	37	2690
43	What time does the potter start his working day and what is the first task he does when he begins?	The potter starts his working day at 6am, and his first task is to check the kiln.	['During his long career as a potter, Peter Beard has built up a reputation for originality and excellence in his glazed stoneware vessels\nSome artists’ studios are neater than a new pin, but Peter Beard’s Warwickshire workshop is a jumble of objects and tools. A throwing wheel lurks beneath spatters of clay, plastic tubs of soupy glaze are gathered together against one wall, and ceramic vessels in various stages of completion sit higgledy-piggledy wherever there’s space. ‘I sometimes think about having a bigger workshop, but I know I’d end up filling that too,’ says Peter.\nGritty though his workshop, a converted barn at the end of the garden of the farm cottage he shares with wife Hilary, undoubtedly is, from it Peter produces exquisite, intricately glazed stoneware, priced at £120 to £1,500. ‘Everything is a one-off piece of work. I’m not interested in fashion – I want to make beautiful things for people to enjoy all their lives,’ he says of the hand-thrown and hand-built wares that he has been making since the 1970s.\nPeter, 57, moved from Kent to rural Warwickshire 12 years ago, but his passion for potting goes back to childhood when, as a teenager, a local potter gave him a Saturday job and encouraged him to make things. Though it seemed obvious to almost everyone, including Peter, that he was destined to be a potter, his parents wanted him to get a ‘proper job’. As a compromise, he studied Industrial and Furniture Design at Ravensbourne College of Art in London. The pottery bug wouldn’t go away though. ‘I ended up spending all my time, from dawn to dusk, in the pottery department. That said, I did learn about lots of other materials such as wood and plastics too, which has proved invaluable.’\nAfter his degree, he followed his heart and moved to Scotland to help establish a pottery making domestic wares. He stayed a year then returned south. ‘It was a good experience because I realised it wasn’t for me – I wanted to make things of a decorative nature,’ recalls Peter.\nHe opened his first solo studio in Kent in 1975. Now his work is in over 16 public and private collections, including Norwich Museum, Stoke on\nTrent City Museum and ceramics centres in Germany, Switzerland, Japan and Korea. He has won several awards, including the peer-judged Pot d’Or at Keramisto, Holland, in 2000. And, in 2004, a research grant from the Arts Council led him to start working with bronze and stone, too.\nPeter’s working day is intense, starting at 6am, when he rises to check the kiln. Then he begins a patchwork of tasks, such as drawing, throwing, glazing, and mounting pieces on hand-cut slate, that continues through to the evening. ‘It’s not a production line – I make the things I like and try to sell them. I throw a maximum of 10 items at a time, and something could wait a few months until I feel ready to decorate it. I tend to make pieces in series, but I’ll stop doing a design if I’m not getting any more stimulation from it.’\nHis simple, organic shapes are very rarely a direct representation of anything specific, rather a distillation of various observations ‘that get tumbled around inside my head’. He likes Neolithic axes, shells, sails and rippled water, for example, and began making his slab-built discs after a trip to China in 1978, where he was inspired by jade rings.\n‘I never go anywhere without a sketchbook – if I see a nice stone or a piece of wood I sketch it. I do a lot of drawing before I make something,’ says Peter. A travelling scholarship to Egypt in 1990 was a landmark experience. ‘I’ve always been interested in ancient Egypt and there are resonances in my work of the things I saw there, such as the wind-driven sand in the desert and the traditional falucca sailing boats.’\nWhile shape is a key element of his work, Peter’s decorative style makes it truly distinctive. He is constantly experimenting with glazes and, even after a lifetime’s experience, every kiln-load of work includes a glaze test to assess a new colour or the viscosity of glaze mix. ‘I don’t find it difficult to get ideas – it’s the execution and the decoration that takes the time,’ he explains.\nHis favoured oxide glaze colours are blues and greys, burnt oranges, lime and bright greens and pinks – all eye-catching in their own right, but made even more arresting when combined with Peter’s method of using wax relief to create texture. Masking glazes and areas with wax allows layers of pattern to build up, making the surface almost three dimensional and very tactile – ‘wanting to pick it up is part of the enjoyment’ – and some pots feel rougher than others. The interiors of the vessels are glazed too. ‘The inside of a pot is very different to the outside – it taps into another set of emotions,’ he says.\nWhen he’s applying glaze, Peter works intensely, hermit-like, as the process requires maximum concentration. Before being decorated, the biscuit-fired vessels and objects are dipped into a bucket of thick, matt base glaze. For the subsequent wax relief technique to work the coverage has to be perfectly even, so he spends time sponging and scraping away excess glaze.\nOnce the base glaze is smooth and dry – around 20 minutes or so – the vessel is ready to be decorated with between two and four oxide glazes. First, to compose the pattern, Peter draws freehand lines using pink food colouring applied with a delicate brush. Next, he paints liquid wax following the lines to mask certain areas. Finally he paints on the coloured glaze, again following the lines, with the wax acting as a resist. He does this with great precision, despite the lines’ wobbly appearance. ‘I like there to be movement in a line,’ he says.\nIt takes him several days to decorate enough work to pack a kiln, and as each vessel is finished and ready to fire, Peter gently places it onto the firing shelves, checking that there is no unwanted chip or glitch on the glaze. Seconds are never available because he simply discards work that he doesn’t regard as perfect. ‘Because I put so much of me into my work, it has to be good. After all, that’s what people are paying for.’\nCOLLECTING PETER BEARD CERAMICS\nPeter Beard’s ceramics first caught Jon’s eye at a London ceramics fair. ‘I’ve worked in collectables all my life,’ says the Bonhams director, ‘and contemporary ceramics are a real bargain compared to modern art and sculpture.’\n‘Peter has a distinctive, daring style. I love his glazes – his use of colour is very unusual, and I particularly like his shell shapes. My wife and I now own four pieces of his work, which look beautiful among our collection of antique ceramics such as Royal Lancastrian and high-fired Ruskin wares. It’s hard to say how good an investment it is but, if you like someone’s work, you should form a collection. Buy larger pieces if you can afford to – the more you spend, the more important the piece is, because it will have been more difficult to make.’\nPeter’s work is stocked at Aldeburgh Contemporary Arts, Suffolk, 01728 454212; The Old Courthouse Gallery, Ambleside, Cumbria, 015394 32022, and The Stour Gallery, Shipston-on-Stour, Warwickshire, 01608 664411\nFEATURE: Caroline Wheater\nPHOTOGRAPHS: Lydia Evans']	['<urn:uuid:cbebfe9a-7ce4-48ef-9e80-27615bfbec2a>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T05:28:56.360010	20	17	1248
44	jasmine basmati rice properties bacteria resistance	Jasmine rice contains less amylose than regular long-grain rice and cooks up slightly sticky, while basmati rice has higher amylose content making it fluffier. Both types, when cooked, can harbor Bacillus cereus spores that survive standard cooking temperatures. These spores can only be eliminated through high-temperature processing used for canned foods, not through regular cooking.	"[""Fried rice is at its most delicious when it's fluffy and not too sticky. It should have distinct, individual grains. Here are tips to help you make fried rice that tastes as good as it does at your favorite restaurant.\nBest Rice For Fried Rice\nLong-grain white rice is perfect for fried rice. It cooks up fluffy and not sticky, with individual grains remaining firm and distinct. It all comes down to two starch molecules: amylose and amylopectin. It's the amylopectin that makes rice stickier as rice science says the molecule is highly branched. Long-grain white rice contains a significantly higher amount of amylose (19 to 23 percent, according to the USA Rice Federation) and less amylopectin than other types of rice. By comparison, glutinous rice, also known as sticky rice, is high in amylopectin and contains a maximum of 1 percent amylose. It's preferred when you want rice that will clump together and make it easier to eat with chopsticks.\nShould You Use Jasmine Rice?\nMany people prefer Thai jasmine rice over all other types due to its popcorn-like aroma and sweet, slightly nutty flavor. While jasmine is a long-grain rice, it contains less amylose than regular long-grain white rice and cooks up slightly sticky. Basmati rice, which contains a higher percentage of amylose, is a better choice. It is grown in India and Pakistan and also has a distinctive fragrance and flavor. Nonetheless, it all comes down to personal preference. Feel free to use jasmine rice if you like.\nHow to Prepare the Rice and Save It for Later\nThe amount of water you add when cooking rice will make a difference. If you find your fried rice is turning out rather mushy, try reducing the amount of water to cook the rice. A typical recipe for cooking long-grain white rice uses a 1-to-1.5 ratio, which means you would add 1 cup of rice to 1 1/2 cups of water.\nYou'll frequently find fried rice recipes calling for cold cooked, previously chilled, or leftover cooked rice. Ideally, the rice you use to make fried rice is cooked at least one day earlier. Day-old rice is drier and reduces the chance of the dish turning out wet and mushy. However, be sure to fluff up the rice before cooling and storing so that it does not harden into a block.\nStoring and Using Leftover Rice Safely\nHowever, using leftover rice has its downsides. Bacillus cereus is a spore-forming bacteria that flourishes in starchy foods such as potatoes and rice. It thrives best in warm, moist environments—such as a pot filled with freshly cooked rice left on the stovetop. When using leftover rice, it's important that the cooked rice be cooled down to room temperature and stored in the refrigerator within one hour of cooking (if possible), and two hours at the maximum. Before re-cooking, use a fork or wet fingers to break up any clumps in the rice. And remember, according to the Food Standards Agency, chilled cooked rice should be eaten within 24 hours to be safe."", 'Bacillus cereus is the bacterial equivalent of a bipolar personality. Depending on the strain, it can cause either of two syndromes – a rapid onset emetic syndrome, or a slower onset intestinal upset.\nThe first reported description of Bacillus cereus food poisoning was published in 1950. The earliest documented US outbreak occurred in 1969. Two years later, the United Kingdom confirmed its first outbreak of Bacillus cereus food poisoning.\nWhat is Bacillus cereus, and where is its natural habitat?\nBacillus cereus is a spore-forming soil microbe. It is widely distributed around the world, but has no known animal reservoir. The microbe is able to grow either in the presence or in the absence of oxygen. Its spores are sufficiently heat-resistant to survive pasteurization treatment of milk and standard cooking temperatures reached in domestic kitchens. It cannot survive the high-temperature treatment used to process canned foods.\nHow is Bacillus cereus transmitted? What is the incubation period of the illness?\nBacillus cereus strains produce one of two toxins – a heat-stable emetic toxin that is produced in the food as the microbe grows, and a heat-sensitive enterotoxin that is produced both in the food and in the intestines. Ingestion of the emetic toxin triggers the onset of nausea and vomiting, which can begin as soon as 30 minutes after a victim eats food containing the preformed toxin. Ingestion of the heat-sensitive enterotoxin (or its production in the intestines) results in the development of a diarrhetic syndrome that can begin some 8 to 16 hours after the toxin enters the body.\nWhat are the symptoms of Bacillus cereus food poisoning?\nEmetic syndrome:- In most individuals, the preformed heat-stable emetic toxin of Bacillus cereus triggers a bout of nausea and vomiting that can being from 30 minutes to 5 hours after eating a contaminated dish and typically lasts for 6 to 24 hours. This form of food poisoning can be confused with Staphylococcus aureus food poisoning.\nDiarrhetic syndrome:- The onset of this form of Bacillus cereus food poisoning is slower than for the emetic syndrome – approximately 8 to 16 hours. The main symptoms are a profuse, watery diarrhea and abdominal pain, usually lasting about 12 to 24 hours. This food poisoning syndrome mimics the symptoms produced by Clostridium perfringens enterotoxin.\nWhat is the prognosis of Bacillus cereus food poisoning?\nBoth forms of Bacillus cereus food poisoning are typically mild and self-limiting. Individuals who are already suffering from a debilitating illness or someone with a compromised immune system may suffer from dehydration. Someone whose swallowing reflex is impaired could accidentally aspirate vomit, resulting in lung complications.\nWhat foods carry Bacillus cereus?\nAs a common soil inhabitant, Bacillus cereus is most likely to be found on harvested crops – grains, fruits and vegetables. Because the spores are heat-resistant and survive normal cooking conditions, Bacillus cereus food poisoning is most commonly associated with consuming cooked, starchy foods – such as rice dishes – that have been held at room temperature for several hours after cooking.\nHow can people protect themselves from Bacillus cereus?\nFirst, by paying attention to food recall announcements and immediately discarding any recalled food or returning it to the store. Secondly, by not allowing a cooked food to stand for extended periods of time at room temperature. Food that is not to be eaten immediately should be refrigerated or frozen promptly. A frozen, cooked food should be thawed in the refrigerator, and not at room temperature.']"	['<urn:uuid:7ec58ebf-0224-45c5-aa40-8f27f781c3ce>', '<urn:uuid:7fc1b9c5-d896-47a4-89af-fd8dace02ef2>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T05:28:56.360010	6	55	1082
45	When it comes to old farming methods, what are some ways that both Mediterranean farmers and African tribes have shown respect for nature and religious beliefs in their practices?	Both Mediterranean and African traditional practices show deep connections to nature and beliefs. In Mediterranean agriculture, farmers practice sustainable methods like crop rotation, natural fertilizers, and fallowing to maintain soil health. They also carefully select drought-resistant crops adapted to the climate. In Oshiwambo traditions, there are spiritual practices like throwing twigs of omusati and omugolo into furrows for peace and harmony, and conducting blessing ceremonies at entrances. The main entrance faces east in Ondonga to receive blessings from ancestors (Aakwampungu), and special trees like omudhime are used to neutralize bad intentions. Both cultures developed techniques to work with their environment - Mediterranean farmers use terracing on slopes to prevent erosion, while the Aawambo used thorn trees and mopane poles for protection.	['Entrances and the blessing of the Main Entrance in Oshiwambo\nThe Oshiwambo homestead is called egumbo which comes from the verb stem – gumba (to fence).\nThis can be attributed to the fact the enclosure of homestead was fenced with thorn trees. In some Oshiwambo dialects it is called onghanda. Etymologically, onghanda is a ceremonial baby carrier given to the bride by the bridegroom’s family to indicate that she had been engaged. She carries it to symbolise her readiness to start her own family.\nThe onghanda symbolises enclosure and protection for a human being hence it has been used as a designate for a dwelling.\nThe thorns were used as a means of protecting the home dwellers against enemies. The thorns were placed against the poles as an extra protection measure. The fence is called ongandjo. The ongandjo is constructed with large mopane tree poles sharpened into points at the top and was used as a defence against enemies and wild animals. Before poles are planted in the furrows of a new homestead for ongandjo, twigs of omusati (mopane) and omugolo (terminalia) are thrown into the furrows by the hegona (a paternal relative). This is done to ensure that there will be peace and harmony in the homestead. Some leaves of omusati and omugolo are also thrown to the poles of the new dwelling for the same purpose. The beer dregs are poured into the furrows to wish the family the large iigadhi (mahangu storage baskets).\nThe homestead consists of three entrances: The main entrance (eelo/onhu) which faces the eastern direction to symbolise prosperity and good luck in Ondonga. It is believed that the Aakwampungu (ancestors) from the east bless the people are source of fortune while the Aakwampungu from the West bring bad luck. The sun rises in the east and it brings good atmosphere.\nTraditionally, people who fall sick are taken to the east to be cured of their ailments. The origin of the Aawambo also lies in the east. But entrance of homestead in Uukwambi, Uukwaluudhi, Ongandjera, Ombalantu and Uukolonkadhi is situated on the western side. The reason why this direction is chosen for the entrance cannot clearly be established. Every commoner has to enter the house through the onhu. Traditionally, the main entrance is sealed with the omunkono tree especially at night. But some people tend to seal their entrances with poles. When one enters the homestead, one pole or two, depending on the size, is lifted and put back. Such an entrance remained closed and it is only opened if the need arises.\nThe okanto (a small back entrance) is situated on the western side of the homestead. The okanto is used in case of emergency, for example the entire family can use it in case of attack. When an enemy enters the house through the main entrance, the house dwellers can escape through the okanto. The okanto is never used by a stranger. Its shape is different from the shape of the main entrance. The okanto is y-shape passage in a fence.\nThe okanto is also used by someone who comes from the kraal after milking the cows, because it is believed that if milk travels through main entrance it will go stale. Another small entrance that leads from elombe (the main kitchen) to the granaries section is called etambo. When a king pays a visit to somebody, he enters the homestead through this entrance because it is believed that if he passes through the entrance that is used by the commoners, he will die or become permanently disabled.\nThere are taboos that are related to entrances such as nobody is allowed to sit in the entrance for no apparent reason. A widow can sit in the entrance after the death of her husband for purification purpose. It is here where water is poured over her body to cleanse her of misfortune.\nThis practice, however, has been abandoned as it is seen degrading and humiliating for a woman. The main entrance is sealed when the owner of the homestead passes away so that death loses its direction when it wants to strike again. This is also a way of consoling the bereaved people, because they may no longer associate the deceased with the entrance.\nThe main entrance of new homestead is officially opened by one’s paternal relatives. The ceremony is known as ekulo lyeelo (planting of gate- posts). The paternal relatives prepare the four ondjege (Kalahari Christmas tree) or omupanda (lance tree) poles. He plants two poles on each side of the entrance. The Aawambo just like other Africans prefer even number as odd number is seen as sign of bad luck. He uses his right foot or hand only when he covers the furrows in which he puts the poles.\nThe right hand or foot is a symbol of good luck. He digs a hole in the middle of an entrance and throws in a piece of omudhime tree to neutralize people who come with bad intentions. It is believed that if a witch or wizard walks over this stick he/she goes into ‘trance’ and loses their power to cast spell on the home dwellers. They seal the hole with the right foot. The stick thrown in the hole remains there for indefinite period to ensure safety and welfare of the home dwellers. The word omudhime come from the verb stem – dhima (to extinguish, neutralize) therefore the pieces of leaves of this tree nor are used wipe away misfortunes. On the contrary, the omudhime sticks are not used as fire woods nor they used as poles for the homestead. Should that happen, the death of the homeowners is accelerated. The seeds of beans, mahangu, sorghums, etc. are scattered in the entrance to wish the family wealth and prosperity. The posts are smeared with red ochre to wish the family good luck. When the entrance is blessed, the hegona (a paternal relative) conducts a prayer:\nMay more millets travel through this entrance...\nMay more cattle and goats fill this homestead...\nGod of Nangombe blesses this entrance...\nTo be continued...', 'Understanding Mediterranean Agriculture: A Time-Honored Farming Tradition\nMediterranean agriculture, also known as dry farming or rain-fed agriculture, is a traditional farming method that has been practiced for centuries in regions with a Mediterranean climate. This unique agricultural system is prevalent in countries bordering the Mediterranean Sea, such as Spain, Italy, Greece, and parts of North Africa and the Middle East. Mediterranean agriculture is characterized by its ability to adapt to the challenging climate conditions of hot, dry summers, and mild, wet winters. In this article, we’ll explore the essence of Mediterranean agriculture, its key features, and its significance in sustainable farming practices.\n1. Climate and Geography\nThe Mediterranean climate is characterized by its distinct seasonal pattern, with hot, dry summers and mild, wet winters. The long, dry summers pose challenges for agriculture, making irrigation essential for crop survival. The geography of Mediterranean regions often consists of hilly or mountainous terrain, which influences farming practices and land management.\n2. Crop Selection\nMediterranean agriculture focuses on cultivating crops that are well-adapted to the climate. Drought-resistant crops such as olives, grapes, figs, citrus fruits, and various grains like wheat and barley are commonly grown. These crops have evolved to withstand the long, hot summers and rely on the moisture accumulated during the winter rainy season.\n3. Terracing and Slope Cultivation\nTo optimize arable land in hilly or mountainous regions, farmers often practice terracing. Terraced fields involve creating flat platforms on sloping land, reducing soil erosion and conserving water by capturing rain runoff. This ancient agricultural technique allows for efficient land use and prevents valuable topsoil from being washed away.\n4. Dry Farming Techniques\nMediterranean agriculture relies heavily on dry farming techniques, where crops are cultivated without irrigation. Instead, farmers rely on the winter rains to provide sufficient moisture to sustain crops during the dry summer months. Techniques such as deep plowing, mulching, and selecting drought-resistant crop varieties are employed to conserve soil moisture.\n5. Crop Rotation and Fallowing\nCrop rotation and fallowing are integral to maintaining soil fertility and preventing soil depletion. Farmers alternate the cultivation of crops to replenish soil nutrients naturally. Fallowing, the practice of leaving fields uncultivated for a period, helps the soil restore its nutrients and moisture content.\n6. Sustainable Farming Practices\nMediterranean agriculture embodies sustainable farming practices that prioritize long-term environmental health. The use of natural fertilizers, such as compost and animal manure, helps maintain soil fertility without relying heavily on synthetic inputs. Additionally, traditional methods of pest and weed control reduce the need for chemical pesticides.\n7. Importance of Biodiversity\nMediterranean agriculture fosters biodiversity by cultivating various crops suited to the region’s climate and terrain. This diversity contributes to the preservation of local plant varieties and enhances ecosystem resilience.\nMediterranean agriculture is a time-honored farming tradition that has sustained communities in regions with challenging climate conditions for generations. The interplay between seasonal rainfall and drought-resistant crops, along with innovative techniques like terracing and dry farming, exemplifies the adaptability and resilience of this agricultural system. By embracing sustainable practices, conserving natural resources, and promoting biodiversity, Mediterranean agriculture serves as a model for environmentally conscious and efficient farming methods. Its rich history and ability to thrive in adverse conditions make it a valuable asset in the pursuit of sustainable agriculture worldwide.']	['<urn:uuid:ccc0ffdb-baff-4a23-b297-82fee779c100>', '<urn:uuid:465cf810-99f3-4720-a272-16a3c5fcd870>']	open-ended	direct	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-13T05:28:56.360010	29	121	1558
46	property transaction escrow deposit refund rules stigmatized property disclosure rules compare	Real estate transactions have distinct rules for deposits and disclosures. For deposits, their refundability can be established as an independent condition in the contract, separate from other obligations like deed delivery and purchase price payment. Regarding stigmatized properties, disclosure requirements vary - while material defects must always be disclosed, sellers generally don't have to inform buyers about stigmas like past criminal activity, deaths, or cult activity unless they notably impact the property's value. However, in some states like California, deaths on the property within the last three years must be disclosed, showing how disclosure requirements can vary by jurisdiction.	['A condition in a contract is a fact, the happening or nonhappening of which creates or extinguishes a duty on the part of the promisor. If the promisor makes an absolute or unconditional promise, he must perform when the time arrives. But if the promisor makes a conditional promise, he must perform only if the condition precedent occurs. The promise may be dependent upon the performance of another condition, in which case they would be dependant and concurrent conditions. In this case neither party is in default until one party performs or tenders performance. In the typical real estate contract seen by Sacramento real estate attorneys, delivery of the deed and payment of the purchase price are dependent and concurrent conditions. There must be performance or tender thereof by one party to put the other in default. In a recent decision, the court agreed with the swindled would-be buyer, who argued that return of their $3 million dollar deposit was an independent condition\nIn Rutherford Holdings, LLC v. Plaza Del Rey, Rutherford contracted to buy a mobile home park from Plaza, and provided a deposit of $3 million dollars. The agreement provided that the deposit was nonrefundable unless Plaza materially breached the purchase agreement or failed or refused to close.\nPrior to the closing date, Plaza told the buyer that Plaza could reduce its property tax bill for the year if it was not in this contract for sale. The contract would increase the value that the tax was based on. If they did not close by the closing date, the tax would be based on a lesser value. Plaza promised the buyer that they would sell the property after the closing date, and after Plaza filed it tax returns. The buyer agreed! The closing date came and went and neither party performed; Plaza never tendered the deed to Rutherford, and Rutherford never tendered the full purchase price to Plaza. Plaza paid less in taxes, then said they would not sell the property to Rutherford, plus they were keeping the deposit, ha ha! This suit followed.\nThe court first noted that in a contract for the sale of real estate the delivery of the deed and the payment of the purchase price are dependent and concurrent conditions. Where the parties’ contractual obligations constitute concurrent conditions, neither party is in default until one party performs or tenders performance. However, here, the buyer argued that the seller’s obligation to return the deposit was independent of the Buyer’s promise to pay the full purchase price. If the two covenants are independent, breach of one does not excuse performance of the other. The buyer’s failure to place the money in escrow did not excuse Plaza’s failure to return the deposit.\nIn this case the Court was looking at whether the Complaint sufficiently described a legitimate claim. Where an ambiguous contract is the basis of an action, the parties are expected to provide their own interpretation of its meaning. If their interpretation is not clearly incorrect, the court accepts as correct plaintiff’s allegations as to the meaning of the agreement.\nHere, the purchase agreement can be reasonably interpreted to mean what the Buyer has claimed. “While [that] interpretation … ultimately may prove invalid,” at the pleading stage, it is sufficient that the agreement is reasonably susceptible of this meaning.” Thus, the Buyer had properly made the claim that return of the deposit was an independent condition, and should have been returned. It’s too bad this buyer had to go through the appeal process at this stage of the lawsuit. The seller seems to be a real con artist, convincing the buyer to let the contract lapse with a promise to sell on the same terms, then keeping the $3 million dollar deposit.', 'Real estate and property law involve buying and selling property. Part of buying and selling property requires disclosure, which is the action of the seller demonstrating any defects the property has, such as electrical issues or water damage.\nIn addition to disclosures from the seller, a buyer must inspect and examine the property they are interested in buying. This notifies them about any potential defects unknown to the seller. A person who fails to conduct a proper examination may not have a legal alternative to reverse the real estate transaction.\nWhat Are Disclosures In Real Estate? What Are Property Owners Required To Disclose?\nReal estate regulation requires that the owner disclose certain information when selling property. This information includes any material defects and any problems with the property. In several states, the owner may be held legally responsible if they fail to disclose this info to the buyer upfront. Additionally, the seller is legally prohibited from concealing any known material defects from the buyer. What constitutes a material defect can vary from state to state.\nGenerally speaking, a material defect is any fact that may have a substantial and reasonable impact on the market value of the property. A material defect can also be any situation that poses an unreasonable risk to other individuals.\nIn addition to disclosing any material defects, the following must also be disclosed:\n- Any zoning issues;\n- Environmental hazards; and\n- Easement violations.\nIt is essential to comment that property owners are only required to disclose any information within their knowledge. This means that sellers are not required to employ an inspector to uncover any issues that the property owner did not know existed. Nonetheless, if a seller does employ an inspector and that inspector then uncovers multiple defects, the seller is legally bound to reveal those defects to any potential purchasers. Further, a seller can only be held responsible for failing to disclose defects if the buyer exercised proper diligence when inspecting the property’s condition.\nA buyer may not lawfully later sue the seller for material defects they should have recognized during a prior inspection. And, buyers may not later sue the seller if they were aware of the flaws before completing the sales transaction. Other parties liable for disclosing any defects, material or otherwise, include seller’s agents or brokers. These parties must reveal all known material defects to the buyer and any limitation on the ability of the seller to complete the real estate transaction.\nWhat Does Caveat Emptor Mean?\nCaveat emptor is Latin for “let the buyer beware.” In real estate law, the property buyer has the burden of exercising proper care when buying real property. This means that the buyer must conduct the appropriate research and caution when selecting the property before the sale.\nWho Does This Rule Protect?\nIt safeguards the seller from liability when a buyer has remorse for buying something they failed to detect because they did not make any effort to look at the property before purchasing it.\nDoes This Rule Apply to New Home Purchases?\nTypically, caveat emptor applies only to forced sales, “as is” sales, and sheriff’s sales. Whether the rule of law also applies to new home purchases hinges on the jurisdiction.\nDoes a Seller Have to Inform Me If a Property Is Stigmatized?\nNo, unless it will noticeably impact the value of the property.\nA stigmatized property is a real estate where something typically considered unpleasant occurred, such as:\n- Cult activity\n- Criminal activity\n- A sex offender lived in the residence\n- The previous owner had AIDS/HIV\nWhat Is Stigmatized Property?\nStigmatized property is a home or apartment where there has been a suicide, murder, cult activity, or other accidents and criminalities. Examples include Nicole Simpson’s house, which sold for much less than desired. Homes that Christie Brinkley’s ex-husband had affairs in had to be taken off the market.\nThe prevailing rule in property buying and selling is caveat emptor – let the buyer beware. Also, a seller cannot be held accountable for failing to do something. Based on these standard legal rules, sellers do not have to inform buyers whether the property is stigmatized.\nAre There Any Exceptions To Caveat Emptor?\nThe exception to this rule is when the seller makes a “misrepresentation” or lies about an aspect of the house. A seller must reply truthfully about essential facts and not aim to deceive the buyer. Even silence or evasive answers can constitute misrepresentation if an average buyer has been misled.\nThe next exception to the general rules is that a seller must disclose a vital fact affecting the property’s price where the buyer would not have thought of that fact. For instance, a buyer must notify a seller that a house is haunted in some states because a reasonably prudent buyer could not be expected to consider this possibility.\nHowever, sellers do not have to inform buyers if earlier owners had AIDS or if registered sex offenders live nearby. The buyer is responsible for finding out this information. Ultimately, a seller should use common sense and reveal details that materially affect the property’s price and that the buyer wouldn’t have been expected to ask about.\nWhat Types of Stigmas Must Be Disclosed?\nThe seller must disclose stigmas that could affect the property’s value.\nThese stigmas can include, but are not limited to:\n- Phenomena: Haunting, ghost sightings, and other unexplained occurrences that could impact the property’s value must be disclosed.\n- Murder/Suicide: Some states require that murders and suicides on the property be disclosed to the buyers. Many of these laws have a time limit, though. For instance, California only demands that the deaths be announced if they took place within the last three years.\n- Other Criminal Activity: Most states demand that criminal activity be disclosed, such as drug dealing or prostitution.\nShould My Agent Disclose The History of My Property?\nThe answer will be on a state-by-state basis. If your property is in California, you and your real estate agent must disclose murders or suicides. In Tennessee, though, no such duty exists. In a state which does not require a stigmatized property to be disclosed, a seller’s agent can be liable for breaking the fiduciary relationship if the agent discloses the information without your permission.\nWhat Should I Do if I Bought Property and Discover It Is Stigmatized?\nYou may want to consult an attorney. In a lawsuit against the seller, you may be entitled to compensation for any necessary repairs. An experienced property lawyer can advise you of your rights and represent you in court.\nIs It Possible for Caveat Emptor to Not Apply?\nYes. A seller does have a duty to reveal significant facts about the property truthfully. Whenever the seller lies or misrepresents facts about a property, then caveat emptor no longer applies.\nWhat Constitutes Misrepresentation?\nA seller can misrepresent facts about a property by:\n- Staying silent when they are required to say something\n- Not telling the truth\n- Giving evasive answers\nDo I Need an Attorney to Assist Me With Caveat Emptor Laws?\nReal Estate and Property Law includes an expansive range of topics, such as purchasing, selling, utilizing, and leasing residential or commercial property. Typical disputes involve establishing property title and boundary lines, landlord and tenant disputes, and zoning/land use issues.\nReal estate and property law also comprise the financing elements, such as mortgages, liens, and foreclosures. Suppose you have purchased or are thinking about purchasing real estate property, whether a home, commercial building or land. In that case, you may want to consult with a real estate lawyer to protect yourself from any unforeseen liabilities attached to the property.']	['<urn:uuid:44290366-106f-4fbe-a958-07c238955dc1>', '<urn:uuid:b3b9ba4c-5b6a-42ba-b899-5dc80c4fceaa>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T05:28:56.360010	11	99	1907
47	What are Russian and Chinese objectives within SCO?	Russia aims to keep SCO security-oriented rather than economic-focused and uses it to limit NATO expansion in post-Soviet space. China created SCO to advance its interests, curb US influence in Central Asia, and strengthen its economic outreach, though it primarily wants stability to secure its Belt and Road investments. Both powers use SCO as a platform to moderate US unipolar influence and maintain regional control.	['Uzbekistan is hosting the 2022 Shanghai Cooperation Organization (SCO) Heads of State Council on 15-16 September in Samarkand. The meeting will be chaired by Uzbekistan’s President, Shavkat Mirziyoyev.\nThis will be the first SCO Heads of State Council meeting since the intervention by Collective Security Treaty Organization (CSTO) peacekeeping troops in Kazakhstan in January 2022, the start of the Russia -Ukraine war in February 2022, the surge in energy and food prices in which have been highest in emerging market and developing economies, and the unrest in Uzbekistan’s Karakalpakstan region.\nMany of the leaders have met in other for a since the last Heads of State Council meeting in 2021, but China takes a proprietary interest in the success of the SCO platform so will make decisions based on its observations at the summit, and is determined it be a success.\nThere are many internal and external issues the Council will have to address, but one on the attendees’ minds will be the G7’s just-announced Partnership for Global Infrastructure and Investment (PGII) which will mobilize $600 billion (yet to be raised) to counter China’s Belt and Road Initiative. PGII is the rebranded Build Back Better World,” which was announced in 2021 but stalled.\nHow PGII will complement the European Union’s EUR 300 billion Global Gateway is unknown, but PGII will focus on clean energy, health systems, gender equality and information and communications technology, reinforcing the old saw, “China does civil engineering and the West does social engineering.” (Or, as Afghanistan’s Reds said, Does that ideology come with electricity?)\nSome external factors the council will address are how to mitigate the inflation in food and fuel prices, the recovery of Afghanistan’s economy and society in the wake of NATO’s 20-year intervention, ongoing instability in Pakistan, and the drop-off in interest in Afghanistan by the U.S. and NATO in the wake of the retreat from Kabul.\nAs most SCO members are in Central Asia, South Asia, and the Caucasus, the situation in Afghanistan of keen interest to them, the country’s recovery being hampered by Washington’s seizure of over $9 billion in central bank reserves that were kept overseas.\nUzbekistan recently hosted a meeting on Afghanistan, advocated engagement with Kabul over isolation, and urged the Taliban government to sever ties with terrorist groups, though the Taliban may have to first explain why former Al-Qaeda leader Ayman al-Zawahiri was living in Kabul until his recent killing by U.S. forces.\nInternal issues that will have to be addressed are stresses between Kazakhstan and Russia, after the CSTO deployment of Russian troops to Kazakhstan in January after an outbreak of unrest caused by a sudden increase in the price of liquefied gas.\nBefore the Russia-Ukraine war, Russian President Vladimir Putin ruminated on the Bolshevik giveaway of “significant territories…to quasi-state entities” which probably put the region’s leaders on edge at the prospect more “frozen conflicts” just as they were attempting the post-pandemic economic recovery.\nKazakh President Kassym-Jomart Tokayev opposed the Russian invasion of Ukraine, and his administration provided aid supplies to Ukraine (as did Uzbekistan), offered Kazakhstan as a business location to companies leaving Russia, and decoupled the Kazakh and Russian currencies. If Kazakhstan draws closer to the U.S. or Europe that will cause tension with Russia and China.\nKazakhstan reflects a broader trend of Central Asia states drawing away from Russia, partly because of the Russia-Ukraine war, and partly as a function of time and opportunity, such as China’s Belt and Road Initiative.\nThe leaders of the five republics recently met and pledged to address social unrest (Kazakhstan, Kyrgyzstan, Tajikistan, and Uzbekistan have seen unrest in the past two years) and stay focused on the situation in post-NATO Afghanistan (especially as Al-Qaeda may not be “gone” as U.S. President Joe Biden declared in August 2021).\nAt the meeting, Toqaev offered to help resolve a border dispute between Tajikistan and Kyrgyzstan, maybe to check Russia inserting itself in the problem, and in line with the republics’ interest in self-help.\nBut despite the pledges of greater cooperation, only three of five leaders signed the “Agreement on Friendship, Neighborliness, and Cooperation for Development of Central Asia in the 21st Century,” highlighting limits on cooperation in Central Asia and possibly later in the SCO. So, the republics may be drawing away from Russia, but they may not be drawing much closer to each other, which will provide opportunity for foreign interference.\nChina-India border tensions persist, lately due to highway construction in a contested area. Also, India has started to engage the Taliban and recently sent a team of diplomats to Kabul to reassert itself as a stakeholder in the region.\nPakistan’s political class will likely attempt to capitalize on India’s nascent return, wasting time and energy that should be devoted to the economy, though if Afghanistan hopes to become a full member of the SCO, Pakistan may vote Nay until Kabul “does something” about the Tehrik-i-Taliban Pakistan.\nPakistan is China’s “all weather friend,” but Beijing will want to avoid getting pulled into the middle of the scrap between Delhi and Islamabad which may hazard its BRI investments in Pakistan, especially restive Balochistan.\nAt the September meeting, Iran may be elevated to full membership. The Islamic Republic is the second-most sanctioned country in the world and, if the U.S. increases sanctions in retaliation for a failed attempt to revive the Joint Comprehensive Plan of Action, Iran’s economy will be under more stress, limiting cooperation opportunities with other SCO members, both because of Tehran’s lack of resources and the threat of secondary sanctions or political retaliation by the Americans.\nAfghanistan wants to be elevated to full membership in the organization but the Taliban regime has been ostracized by the West for defeating NATO, the leaders of the emirate have been sanctioned for past and recent offenses, and the country is broke as the previous government was heavily dependent on Western aid (and the U.S. seized the central bank reserves).\nKabul will be a problematic SCO member but the country’s location is key to regional connectivity, and its mineral wealth could help finance the country’s development, provided the Taliban government can ensure security and respect investors’ rights, both tall orders as China will be reluctant to get involved to encourage the latter, being held back by its policy of non-interference on others’ internal affairs.\nIn view of the unrest in the Central Asian republics, SCO members may have greater interest in tracking foreign cash flowing to local actors, which will be opposed by the West that uses the subventions to build “civil society,” but which local leaders will see as foreign interference.\nChina just wants everything to be tranquilo and will press the republics to address the causes of the unrest by ensuring more economic opportunity, both to secure its BRI investments and minimize the potential for popular unrest and subsequent foreign interventions (other than its own, of course).\nThe SCO is a platform that, though created by China to advance China’s interests and curb U.S. influence in Central Asia, is most useful to its members when it promotes policies and institutions that ensure shared economic development.\nThe SCO also gives its members the opportunity for direct dealing with two permanent members of the UN Security Council, and is a way to balance between the U.S. and China/Russia. In light of unrest in SCO countries, and tension between NATO/EU, Russia, and China, the meeting will be closely watched for results that put Eurasia on a path to economic development that is politically sustainable and that prioritizes local sovereignty, independence, and territorial integrity, which will rebound to the long-term economic benefit of the big powers of the East and West.\nJames Durso (@james_durso) is a regular commentator on foreign policy and national security matters. Mr. Durso served in the U.S. Navy for 20 years and has worked in Kuwait, Saudi Arabia, Iraq, and Central Asia.', 'India is riding high on multilateralism in 2023. From the Shanghai Cooperation Organization (SCO) to the G-20 meetings, the Indian establishment is busy playing host to leaders from the Global South and the North. Last week, India was busy with the SCO Foreign Ministers’ Meeting in Goa on the Indian west coast.\nHosting the meeting, Chief Minister of Goa Dr. Pamod Sawant tweeted, “Indian presidency is driven by a commitment to SECURE SCO. Its key focus areas are startups, traditional medicine, youth empowerment, heritage and science & technology.” Highlighting the importance of India’s engagement with the SCO, an official statement reportedly said that the engagements with SCO have “helped India promote its relations with the countries in the region with which India has shared civilizational linkages, and is considered India’s extended neighborhood.”\nThe SCO is one of the older minilateral regional groupings that India is a part of. The group consists of Russia, China, and the Central Asian states. Pakistan and India became full members of the SCO in 2017. There are also currently four observer states – Afghanistan, Mongolia, Iran, and Belarus – as well as six dialogue partners – Armenia, Azerbaijan, Cambodia, Nepal, Sri Lanka, and Turkey. Iran and Belarus are anticipated to become full members at this year’s SCO Summit in July. This, along with the fact that Russia and China dominate the SCO, indicate the anti-Western character and goals of the grouping.\nEstablished in 1996 as the Shanghai Five before expanding into the SCO in 2003, the group was meant to moderate the U.S. unipolar moment and limit Washington’s strategic pursuit in Russia’s backyard, or its “near abroad” as Russia calls it. From China’s viewpoint, it was also a platform to strengthen its economic outreach in Central Asia, although Russia has taken pains to ensure that the SCO remains a security-oriented institution rather than an economic one. For both Russia and China, it was also an instrument to limit the NATO’s expansion into the post-Soviet strategic space.\nAll of these ideals may have had some resonance in New Delhi until even a decade ago when India was less aligned with new security partners such as the United States, Japan, France, and Australia. It is not clear if these SCO goals are still consistent with India’s strategic goals, however. Today, under changed geopolitical circumstances, driven by the structural changes of Asian and global geopolitics, India has more in common with its new security partners than the SCO partners, especially Russia and China.\nThat being said, what were the substantial outcomes at the SCO Foreign Ministers’ Meeting? India’s External Affairs Minister Dr. S. Jaishankar in his opening remarks at the meeting highlighted what Prime Minister Narendra Modi had articulated as India’s goal for the SCO under its presidency – a “SECURE” SCO. “SECURE” is an acronym for “Security, Economic development, Connectivity, Unity, Respect for sovereignty and territorial integrity and Environmental protection.”\nJaishankar then went on to highlight major challenges including global supply chain disruptions, which have resulted in adverse effects on the supply of energy, food, and fertilizers, with most of the impacts being felt particularly by developing countries. Jaishankar said that these disruptions have led to “a credibility and trust deficit in the ability of global institutions to manage challenges in a timely and efficient manner.” In the minister’s view, the continuing failures and challenges of multilateral institutions can be a positive trigger for groupings like the SCO to frame a cooperative agenda. But this is going to be an uphill task for India.\nWith India’s ties with China and Pakistan going through one of their worst phases, it is difficult for India to pursue whatever limited agenda it may have with the SCO. Also, many of these disruptions were also a direct result of the Russian invasion of Ukraine in February 2022, which was not mentioned even once at the SCO meeting. With Russia and China continuing to form a united front against the United States and the liberal international world order, it is unclear what India can hope to achieve through mechanisms such as the SCO, or the RIC (Russia-India-China) or BRICS (Brazil-Russia-India-China-South Africa) groupings. But these efforts also are a strain on India’s limited diplomatic capacity, as former Indian Foreign Secretary Ambassador Shyam Saran wrote in a recent article.\nDespite growing concerns from outside the Indian government, New Delhi appears quite confident that it can straddle the intensifying global divisions and keep all of India’s various partners happy. Whether this confidence is well founded or not is something that will become clear in the next few years.']	['<urn:uuid:2ee5e09d-7220-4b07-a16a-ad65332f6577>', '<urn:uuid:85866948-0e19-436c-9cfd-33180d25970d>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-13T05:28:56.360010	8	65	2074
48	As an electrical engineer, I want to understand exactly how batteries work - can you explain the science?	Batteries work through a process called the Galvanic Process. When two dissimilar metals are placed near each other in an electrolyte (a fluid that allows electron flow), one metal gives up electrons to the other metal if there's a complete circuit. The electrons flow from one plate through the wire and load, then back to the plates. During this process, one plate gradually disappears as it gets plated onto the other plate. This electron flow continues until one plate is gone, at which point the battery dies.	"['SEE: NEW BOAT BUILDERS HOME PAGE http://newboatbuilders.com/pages/electricity1.html\nBASIC ELECTRICTY PAGE 1\nI am going to try to keep this a simple as possible, not because I think you can\'t understand it. I assume that to master the art of boat building you have to be smart. Mainly, I don\'t want to end up writing a book on electricity. I want to this to be simple, but complete enough to give you a basic understanding of how this electrical stuff works. When I was much younger I worked on radars and computers on Coast Guard ships and when people asked us ""how does it work?"" we would say (Smart alecs that we were) magically, mystically, wonderfully, electronically. But it\'s actually a lot simpler than that. However, to some it still seems like magic.\nSo what is electricity? As we all learned in grade school the world is made up of atoms. Atoms are made up of electrons, protons and neutrons. What we are interested in, is the electrons, surrounding the nucleus of the atom. Electrons can be dislodged from their atom and attached to another one giving it an excess of electrons. Excess electrons give an atom a negative charge. Atoms with not enough electrons have a positive charge. Through a chemical reaction or other force, electrons can be made to flow though a conductor from the negative to the positive and used as electricity. So electricity is really just a stream of electrons flowing through a conductor from point A to point B and back to where they came from. (a complete circuit) This flow is called current.\nAll of us have experienced this either by getting a static discharge off of a door knob or other metal object, or by seeing nature\'s ultimate display of electricity, lightning. This is simple exchange of electrons from one point to another. We use this every day in our homes, our cars, our cell phones and every other electric or electronic device we use, but we give little thought to what is actually happening.\nIn grade school most of us had a science teacher show us an experiment where a strip of lead with a wire attached and a strip of zinc with a wire attached, were put into a glass jar filled with acid. This is what is called a voltaic cell, or battery. The other ends of the wire were connected to a light and lo and behold it lit up!.\nThat is about as basic a battery as there is. What is happening here is not magic, just simple chemistry. The acid is an electrolyte. An electrolyte is a fluid that allows electrons to flow through it from one pole to another. Salt water is a pretty good electrolyte. Fresh water is not but will still conduct current and is actually more dangerous if current gets into the water.\nWhen you put two dissimilar metals near each other in an electrolyte, one of the metals gives up electrons to the other metal. But this only happens if there is a complete circuit, which is the wire and the light. So the electrons flow from one plate to the other through the wire and the light and back to the plates. This doesn\'t go on forever though. One plate will gradually disappear and soon there will be no more electron flow. Then the light goes out. Where does it go? It gets plated onto the other plate. Remember this, because this is crucial to how batteries work, and how galvanic corrosion works. In fact this is called the Galvanic Process.\nIf we didn\'t have a light in the circuit, just a wire, electrons would still flow, but it would happen so fast that our primitive battery would be dead in no time at all, or the wire would get too hot and melt. This is called a short circuit. The positive side is connected directly to the negative side. The light we put in is called a load. It provides some resistance to the flow of electrons and slows down the process. Also the filament in the light gives off some of the energy of the electrons in the form of light and heat. Otherwise no work would get done by the electrons. So for electricity to do work there needs to be a load in the circuit. This can be lights, appliances, electronic equipment, or motors, all of which put the electrons to work.\nThe wire getting hot and melting is the basic principle behind a fuse. If too many electrons are flowing through the wire, the fuse is designed to get hot and melt at a predetermined amount of current flow. This breaks the circuit, or what is called an open circuit, and stops the current flow, the same way throwing a switch does.\nSo now we know what current is. It is electron flow. The amount of current is measured in Amperes, or milliamperes. The symbol for Amperes is A or milliamperes mA.\nWhat drives this current is called voltage. Voltage is a measure of potential energy contained in the battery and is measured in volts. The symbol for volts is V.\nThe load is resistance to current flow and is measured in something called ohms, named after the guy who discovered it. The symbol for this is either R or the greek letter Omega. I will just use R because the Omega symbol is too hard to find on my computer.\nTypes of Electricity\nYes, electricity comes in different flavors, AC and DC. No that is not a rock group. AC stands for alternating current, which is what we all have in our houses, and most appliances run on AC, and is usually 110 volts, or 220 volts for some large appliances such as clothes dryers. Where does AC electrcity come from? See https://www.saveonenergy.com/how-electricity-works/\nDC stands for Direct Current and is usually low voltage such as 6 volts or 12 volts. Many of the small electronic devices we commonly use, such as cell phones, calculators, and IPods all run on DC. Your car also uses DC and most small boats use only DC. However, as the boats get bigger they use both AC and DC, until you get into the ship sized yachts that use only AC. First though, I will deal with DC because it is simpler to explain and what most small boats use.\nDirect current comes primarily from batteries. You can get it by converting AC to DC but for now I will stick with batteries. The simplest batteries are a single cells such a the D cells used in flashlights, or AA and AAA cells used in most small electronic devices. A long time ago, someone discovered that any single cell battery, no matter how big, puts out about 1.2 volts. Over the centuries that has been upped to about 1.5 volts. The larger you make this cell, the longer it will last, and the more current you will get out of it (amperes) but you still won\'t get any more than 1.5 volts.\nSo how do we get typical 12 V and 6 V batteries? These are lead-acid type batteries and have about 2.0 volts per cell, compared to the 1.5 volts of most flashlight-type cells. If you tie a lot of these cells together in series (I\'ll explain series in a minute) they add up. So three cells is six volts, and 6 cells is 12 volts. So actually that 12 V battery in your car or boat is really six cells (count the vent caps) wired together to make twelve volts.\nSeries means battery cells that are connected together thus; The positive pole on the first cell is wired to the negative on the second, and the negative on the first is wired to the positive on the second, and so on until you have the voltage you need.\nParallel means just the opposite. All the positive poles are connected to each other and all the negative poles are connected to each other. This still only gives you 1.5 volts but it increases the amount of current. This is commonly done in boats and recreational vehicles to make a large battery bank to power all the DC equipment on board. Two or more twelve volt batteries will be connected in parallel to increase the current, to run more equipment, or to run your equipment for a longer time. However, if you have something that runs on 24 volts or 48 volts, you can connect 12 volt batteries in series to get the desired volts. In series you add the voltages. In parallel you add up the amperages.\nSee the Figure below, and the photo of two deep cycle batteries in parallel. The battery on the right is the starting battery and not connected to the two deep cycle house batteries. However, in this instance there is a switch which allows the house batteries to be used to start the engine.\nSo why DC? DC has certain advantages. It comes in convenient sized packages; for example, your typical automotive batteries. Also, DC does not present the big shock hazard that AC does. Plus that, most equipment used on small boats is designed specifically to run on DC. You don\'t need a large generator to produce DC, and you can easily recharge the batteries from an alternator on your engine, or with a charger from shore power.\nWhy is it called direct current? Because DC current flows in only one direction, as opposed to AC which flows both directions. For low power devices, DC is more than adequate to supply your power needs. See Wikipedia on Direct Current http://en.wikipedia.org/wiki/Direct_current\nSo what about AC. It\'s called Alternating Current because the current flow switches direction every 1/120th second, meaning it takes 1/60 of a second to go through a complete cycle, or 60 cycles per second, now called Hertz after the inventor, and shown as Hz. See Wikipedia on Alternating current. http://en.wikipedia.org/wiki/Alternating_current\nAlternating current\'s biggest advantage is that it can be easily transformed to higher or lower voltages with a transformer. To send lots of power a long distance, a high voltage up to hundreds of thousands of volts is used (""High Tension"" lines). The current is much less, so the voltage drop is much less. Some of the newest long-distance lines now use DC, as new methods of converting DC up and down in voltage have been developed.\nSomeone discovered that electrical equipment such as AC motors run best at between 50 and 60 cycles. We in America chose to use 60, the rest of the world opted for 50. Also, most of the world outside the US uses 220 volts instead of the 115 we usually use. So if you go anywhere else, to run your US electrical equipment, you need an adapter that changes 220 volts to 115 volts. In most cases the difference between 50 and 60 Hz is not a problem. That\'s why when you go to Europe and take your laptop computer or a shaver, hair drier, or curling iron, you need appliances that have a switch for 115V to 220V, or a separate converter. Recently though many laptop chargers and small appliances will run from 115 or 220V and 50 or 60 Hz. Read the small print on the back of your charger!\nAC can be changed easily to DC of various voltages as well, by power supply devices that change the voltage and rectify AC to DC.\nAlso AC can be transformed, that is changed easily to another voltage by use of a transformer, so you can have 110 volts AC, 220 or 440 AC, or whatever you need. It can be changed easily to DC as well, by a converter. Out on the power pole, the power in the lines coming into your neighborhood is around 10,000 volts AC. But the utility company mounts transformers on the poles to transform it to 110 volts AC, 220 or 440 AC so you can use it in your house, or on your boat.\nWhy AC on a boat? As boats get bigger the owners want the convenience of appliances that run off of AC. Probably number one is air conditioning. There are 12 V air conditioners but AC ones are more available and more powerful. Also, house type appliances such as large screen TV\'s, VCR\'s, computers, refrigerators, etc., all run on AC. So AC has come aboard our boats and is here to stay. It is also appearing on smaller and smaller boats every year as the market gets more competitive.\nLinks to Offsite References:\n© newboatbuilders.com 2007 All rights reserved. revised 11/02/2015']"	['<urn:uuid:5be6eb94-c3ee-49a8-bf3c-b74796ee5a36>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T05:28:56.360010	18	87	2116
49	What's the connection between presidential libraries and lighthouses when it comes to preserving American history from the 1800s to modern times?	Both presidential libraries and lighthouses serve as important historical preservation sites. The LBJ Presidential Library preserves crucial civil rights documents like the Civil Rights Act of 1964 and Voting Rights Act of 1965, while historic lighthouses like Piney Point (built in 1836) and Lime Rock preserve maritime history and stories of notable Americans. Both types of institutions are now museums that educate the public about different aspects of American history.	"[""Archive for 'National Archives Near You'\nToday’s post comes from Ashley Mattingly, who is an archivist at the National Archives at St. Louis, where she manages the collection of archival civilian personnel records.\nThe most well-known lighthouse keeper in the world was an American woman who was a Federal civil servant. Ida Wilson Lewis, lighthouse keeper of Rhode Island, saved somewhere between 13 and 25 lives, including men stationed at Fort Adams and a sheep.\nIda Wilson Lewis was born Idawally Zorada Lewis in 1842. In 1870, she married Capt. William Wilson. Although they separated two years later, Ida used “Wilson” as her middle name for the rest of her life.\nIn 1853, Ida’s father, Capt. Hosea Lewis, was appointed the first lighthouse keeper at Lime Rock, an island in Newport Harbor. A few months after his appointment, Captain Lewis was stricken by a paralytic stroke. As a result, his wife, Zorada, and Ida carried out the lighthouse duties in addition to their everyday household chores.\nPerforming numerous lighthouse and domestic duties groomed Ida for an appointment as the official lighthouse keeper of Lime Rock in 1879 and sent her down the path to becoming a renowned rescuer. … [ Read all ]\nPosted by Hilary on March 24, 2015, under National Archives Near You, Unusual documents.\nTags: civil servants, history, lighthouse, lighthouse keeper, Rhode Isnad, WHM2015, women, Women's History Month\nMark K. Updegrove is Director of the Lyndon Baines Johnson Presidential Library in Austin, Texas.\nThe first time a sitting President came to the Lyndon Baines Johnson Presidential Library was on May 21, 1971, when President Richard Nixon boarded Air Force One and journeyed to the campus of the University of Texas at Austin to help former President Johnson dedicate the library to the American people.\nIt had been a little more than two years since Johnson had yielded the Oval Office to Nixon, and Johnson’s place in history was very much in the balance.\nThe war in Vietnam that Johnson had escalated and that continued to divide the nation hung balefully over his legacy. This, despite the profusion of landmark laws LBJ left in his wake, including the passage of a triumvirate of seminal civil rights legislation: the 1964 Civil Rights Act, the 1965 Voting Rights Act, and the 1968 Fair Housing Act.\nAs library’s inauguration played out, the voices of 2,100 Vietnam protesters rumbled in the distance, their chants of “No more war!” carried by 25-mile-an-hour winds that swirled throughout the day.\nOn April 10, 2014, when Barack Obama became the second sitting President to visit the LBJ Library, the weather, which topped out at 88 degrees, was far less tempestuous—and Lyndon Johnson’s legacy had become far clearer.\nPosted by Hilary on May 8, 2014, under - Civil Rights, - Presidents, National Archives Near You, News and Events.\nTags: Austin, Bush, Carter, Clinton, LBJ, Mark Updegrove, Nixon, Obama, vietnam\nThroughout the month of April, the Lyndon B. Johnson Presidential Library will be exhibiting four cornerstone documents of civil rights. The “Cornerstones of Civil Rights” exhibit will run from April 1 through 30.\nThe exhibit will feature two documents signed by President Abraham Lincoln: an authorized, printed edition of the Emancipation Proclamation; and a copy of the Senate resolution proposing the 13th Amendment, which ended slavery.\nIt will also include two documents signed by President Lyndon B. Johnson: the Civil Rights Act of 1964, and the Voting Rights Act of 1965. These are the four “cornerstone” documents on which modern civil rights legislation is enacted.\nThe exhibit links Lincoln and Johnson as two great civil rights champions in the nation’s history. Their conviction, commitment, and force of will to secure equal rights for all fundamentally changed American society.\nIn the exhibit are two hats owned and worn by the two Presidents—a Resistol beaver cowboy hat that accentuated Johnson’s Texas roots, and one of Lincoln’s famous stovepipe hats.\nPosted by Hilary on March 31, 2014, under - Civil Rights, - Civil War, - Presidents, Abraham Lincoln, National Archives Near You, News and Events.\nTags: abraham lincoln, Barack Obama, Bill Clinton, civil rights, George W. Bush, Jimmy Carter, Johnson Presidential Library, LBJ, University of Texas at Austin\nHappy Women’s History Month! Today’s blog post comes from Kristina Jarosik, education specialist at the National Archives at Chicago.\nRecently, two powerful women in the Silicon Valley, (Sheryl Sandberg of Facebook and author of Lean In: Women Work and the Will to Lead and Marissa Meyer, CEO of Yahoo) provided the media and the public the opportunity to re-examine the role of women in the workplace. These exchanges, the dawn of Women’s History Month, and the 50th anniversary of the 1964 Civil Rights Act encouraged us to step back “historically” and to look in our stacks for stories of women fighting for equality in the workplace through the federal courts.\nWe discovered several cases. Alice Peurala’s is one.\nAs a single parent working night shifts at U.S. Steel’s South Works in southeast Chicago in the 1950s, Alice Peurala wanted a day job. She heard that product testers in the Metallurgical Division had this appealing schedule. But these positions were not posted, as others were, for bidding.\nIn 1967 (after the passage of the 1964 Civil Rights Act), a male colleague that Alice had trained was moved up to be a product tester after only four years. Just before he started, she called the hiring director and inquired about being considered for one of these jobs. His response, “No, we don’t want any women on these jobs.”… [ Read all ]\nPosted by Hilary on March 26, 2014, under - Civil Rights, - The 1960s, - Women's Rights, Letters in the National Archives, National Archives Near You, Uncategorized.\nTags: civil rights, Civil Rights Act, discrimination, EEO, Union, United Steelworkers of America, women's history\nYou can’t snuggle with the Constitution, but you can sleep next to it! This sleepover in the Rotunda is open to children ages 8-12, accompanied by an adult. Registration fees are $125 per person (discounted to $100 per person for Foundation members).\nParticipants get to meet author Brad Meltzer, who will set the way for an evening of historical missions and discovery. Learn to decode Civil War ciphers, write with a quill pen, dress up in period clothing, and play with historic toys and games from our patent collection.\nChildren will also get to meet journalist and author Cokie Roberts, and interact with historical characters Abraham Lincoln and Amelia Earhart. The evening wraps up with a selection of Oscar-nominated short films in the William G. McGowan Theater.\nParticipants will receive the first two books in Brad Meltzer’s brand new children’s series, I am Abraham Lincoln and I am Amelia Earhart. Written by Meltzer and illustrated by Christopher Eliopoulos, each book tells the real-life story of an ordinary person who changed the world.\nPosted by Hilary on January 6, 2014, under - Constitution, - Declaration of Independence, National Archives Near You, News and Events.\nTags: charters of freedom, Foundation, Rotunda, sleepover"", 'The Piney Point Lighthouse Museum and Historic Park\nThe Piney Point Lighthouse Museum is a white two-story structure on\nthe north campus of the park and houses museum exhibits, the\nLighthouse Lens Museum Store and offers restrooms and visitor\ninformation. Guests are encouraged to begin their visitor experience\nat the museum where you can sign in, pay admission, and obtain a\ntour guide who will then provide escort to the Potomac River\nMaritime Exhibit on the north campus and the lighthouse and keeper\'s\nquarters on the south campus. Those who desire to climb to the top\nof the lighthouse must be accompanied by a staff person or\nMuseum exhibits focus on the construction and\noperation of the lighthouse, lighthouse keepers, the role of the\nUnited States Coast Guard, and the story of the Black Panther U-1105\nGerman submarine sunk in the Potomac that now serves as Maryland\'s\nfirst historic shipwreck dive preserve.\nLens Museum Store\nThe museum store offers an array of Piney Point\nLighthouse souvenirs, nautical gifts, books, jewelry, clothing,\nchildren\'s corner and more! The store is operated by the Friends of the\nSt. Clement\'s Island and Piney Point Museums with proceeds returning\nto support museum programs, projects and exhibits. The Lighthouse Lens\nis your destination for unique quality gifts. And for the ""one who has\neverything,"" consider a gift membership to The Friends! Museum members\nreceive a 10% discount on store purchases.\nThe Potomac River Maritime Exhibit\nAlso located on the northern campus is the\nbuilding that houses the collection of four historic wooden vessels on\nloan from the Paul Hall School of Maritime Training and Education. The\ncollection is comprised of a 67-foot skipjack(Joy Parks), an 84-foot\nbugeye (Dorothy A. Parsons), a log canoe, and Potomac River dory boat.\nThe exhibit focuses on the life of the watermen who sustained a\nlivelihood working the waters of the Potomac for crabs, fish and\nPiney Point Lighthouse Park and Pier\nCome by boat! The Piney Point Lighthouse Museum\nand Historic Park is also accessible by boat. The pier connects to the\nboardwalk that surrounds the lighthouse and picnic area. The scenic\nriverside venue of the lighthouse offers guests an aesthetic panorama\nand serene escape with comfortable benches and picnic tables to\naccommodate an extended visit.\nthose coming by land, bring a picnic lunch or dinner and\nenjoy the shaded picnic area near the lighthouse. The\nbeautiful new sidewalks and boardwalk make the south\ncampus area handicap accessible. The pier offers a\nbeautiful view of the Piney Point and St. George Island\nThis site is available for rental for\nspecial events. Contact the site supervisor at\n301-994-1471 for more information.\n||The park also offers a kayak\nlaunch located on the north campus next to the Potomac\nRiver Maritime Exhibit. It rest on the serene waters of\nthe Piney Point Creek and paddlers are invited to\ndiscover this lovely Potomac River tributary. Launch is\nopen during staffed hours of operation.\n""The Lighthouse of Presidents""\nBuilt in 1836, the Piney Point Lighthouse and\nkeeper’s quarters are located 14 miles up the Potomac\nRiver from the Chesapeake Bay. This beacon stands\npreserved today as a witness to a bygone era of high\nsociety entertaining American Presidents such as James\nMonroe, Franklin Pierce, Theodore Roosevelt and other\nnotables including American statesman Daniel Webster,\nAmerican Vice President John C. Calhoun, and later, the\ninimitable singer Kate Smith.\nThe lighthouse and\nkeeper’s quarters were built by master lighthouse\nbuilder, John Donahoo for the contracted price of\n$3,888. The circular tower stands 35 feet tall from its\nbase to the coping and the walls are 3 feet 10 inches\nthick at the base and 2 feet 3 inches at the parapet.\nThis lighthouse was the tenth of the 12 lighthouses\nbuilt by Donahoo in his lifetime.\nTime and tide\nhad taken its toll by the time preservation efforts of\nthe Piney Point lighthouse began in earnest in 1990 by\nthe Museum Division of St. Mary\'s County Department of\nRecreation and Parks. The lighthouse was restored and\nthe outbuildings were renovated and preservation efforts\nremain ongoing. The lighthouse tower is open for\nclimbing during visitor museum hours of operation and\noffers a stunning view of the Potomac River and the\nPiney Point area.\n|Hours of Operation\nDaily – 10:00 a.m. to 5:00 p.m.\nJanuary through March 24\nMuseum closed to the public. Guided tours and bus tours available by pre-arrangement with site supervisor: April Havens (301-994-1471)\nNew Year\'s Day\nDay After Thanksgiving\n• Admission: $7.00 adults, $3.50 senior citizens and military\npersonnel, students: FREE for\nchildren 5 and under.\nAdmission includes a guided tour of the museum, Potomac River\nMaritime Exhibit, and lighthouse tower on the south campus.\n• Large group/student/bus tours welcome! Pre-arranged group\ntour rates for 20 or more: $3.50 per adult, $2 per\nstudent/senior citizen/military persons. To pre-arrange a tour,\ncontact site Supervisor April Havens,\nemail@example.com or 301-994-1471.\nPiney Point Lighthouse, Museum and Historic Park provides site rental for weddings, picnics, etc. Contact the site\nSupervisor for fees, policies, and reservations.\navailable for day visits, no overnights\nKayak launch available during staffed hours.\nmaritime exhibit, board walk, pier are handicap accessible.\n• Grounds open sunrise to sunset, year round.\nMuseum & Historic Park\n44720 Lighthouse Road\nFrom Baltimore/Washington, D.C. :\n495/95 to exit 7a (Route 5 south to Waldorf). Follow\nRoute 5 south through Leonardtown to Callaway. Turn\nright on Route 249. Drive 9 miles to Piney Point making\na right onto Lighthouse Road to the end.\nTake State Route 301\nto 234 east to Leonardtown. Turn right on Route 5 South\nthrough Leonardtown to Callaway. Turn right on Route\n249. Drive 9 miles to Piney Point making a right onto\nLighthouse Road to the end.']"	['<urn:uuid:79ee63c3-363a-470f-b886-e3e1e0a17da0>', '<urn:uuid:dd070efb-05a4-4870-809f-96ced0538bd2>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-13T05:28:56.360010	21	70	2120
50	difference sotatsu impressionist paint layering method	Sotatsu developed his tarashikomi technique using random pools of pigment and ink, revealing the construction methods in his work. Impressionists, on the other hand, used layers of pure colors with gaps in top layers to show colors underneath, achieved through techniques like hatching, cross-hatching, stippling, and dry brushing.	"['The Smithsonian’s Freer and Sackler Galleries are currently presenting a “once-in-a-lifetime exhibition” showcasing the powerful designs of the revered 17th century Japanese master Tawaraya Sotatsu. Sotatsu: Making Waves is the first major exhibit in the Western hemisphere devoted to the artisan who revolutionized Japanese visual culture by bringing traditional courtly arts to the newly-emerging urban masses of Edo (now Tokyo), Japan. xxxxx\nSotatsu (ca. 1570-ca. 1640) was a craftsman who made finely decorated papers, folding fans, and screens at his Tawaraya shop/studio in Kyoto. He worked in an era when Japan’s class structure— traditionally a rigid hierarchy composed of samarai, farmers/peasants, artisans, and merchants—was being transformed by urbanization and mercantile growth. Sotatsu’s work earned widespread popularity among art lovers of the rising merchant class, but he nurtured ties with an elite clientele as well.\nRight: Tawaraya Sotatsu, Screen with Scattered Fans, early 1600s. Collection, Freer Gallery of Art.\nAlthough his powerful and distinctive designs had a major impact on Japanese visual culture, Sotatsu vanished into obscurity after his death. His reputation was eclipsed by the next generation’s Ogata Korin, whose designs and paintings were enormously indebted to Sotatsu’s work. Sotatsu remained unknown while Korin enjoyed a major vogue during Europe’s Japonism rage in the mid-and-late-nineteenth century. It was only after the turn of the century that Sotatsu’s significance was re-discovered, largely because of the efforts of collector Charles Lang Freer, a connoisseur who would found the Freer Gallery in 1923.\nFreer’s perceptive eye helped identify some of Sotatsu’s work at the turn of the 20th Century, and thereby reclaimed this artisan’s role as a leading chronicler of Kyoto’s social and cultural transformation in the 17th century. Freer collected several of Sotatsu’s paintings and is credited with introducing this artisan to Western audiences. Sotatsu: Making Waves is the first and only opportunity to see more than 70 of Sotatsu’s masterpieces gathered from collections in Japan, the U.S., and Europe.\nLeft: Waves at Matsushima (detail)\nAlthough much of Sotatsu’s life remains a mystery, this exhibition is intended to explore how Japan’s massive social upheaval “allowed a common Kyoto fan-shop owner to become a sophisticated designer with aristocratic intentions.” (F/S press release) The resurgence of his popularity in the early 20th century inspired new generations of Japanese artists, as well as “styles such as Art Deco and Western luminaries such as Gustav Klimt and Henri Matisse.” As exhibition curator Dr. James Ulak agrgues, “Sotatsu’s designs profoundly changed both Japanese and Western art, yet only now is his name emerging from the shadows.”\nSotatsu’s singular contribution was to bring traditional courtly arts to the masses. He created folding fans and folding-screen paintings that captured bird’s-eye views of Kyoto and displayed visual quotations extracted from classical and legendary narratives. By presenting this work for the newly-emerging urban class rather than for a samarai elite, Sotatsu “sent once-sequestered and little-seen imagery into the streets” (Ulak, “Introduction” to catalogue, p. 11).\nBelow: Tawaraya Sotatsu, Waves at Matsushima, Japan, early 1600s. Collection, Freer Gallery of Art.\nThe centerpiece of this exhibition is a masterwork that Charles Lang Freer identified as Sotatsu’s in 1906. He purchased the six-fold screen Waves at Matsushima “After much dickering of a most exasperating nature….(The dealer’s) original price was ten thousand dollars but I cut his prices exactly in half.” This screen had been held from the 17th to the early 20th century by the Zen temple in Sakai, a port on the Inland Sea. The temple was built by a major sea trading merchant, and he likely commissioned Sotatsu’s screen to celebrate the temple’s opening in the 1620s.\nRight: Waves at Matsushima (detail)\nAlthough called “Waves at Matsushima” in the early 20th century, the six-fold screen actually depicts no particular geography. Rather it invokes a general Japanese iconography associated with the miraculous gifts of the sea and a return to safe harbor. As Nakamachi Keiko points out in the catalogue essay about this work, “Anyone who stands before the screens is overwhelmed by the power of the seascape. The mountain-like waves, composed of lines in gold and sumi ink, fold into each other in complex ways.” The fluidity and continuity of the panels suggest a visual serial drama, ebbing and flowing between rough waves and rocks and sandy shores. (79)\n“Waves at Matshuchima” is a gripping way to begin an exhibition that includes poetry cards, handscrolls, folding screens, woodblocks, and No drama librettos. Among Sotatsu’s best-known productions are multiple poetry sheets (shikishi) depicting scenes from the 10th century literary classic, Tales of Ise. Sotatsu created a key event for each of this work’s 125 episodes. Sotatsu also produced an eight panel folding screen visualizing The Tale of Genji, using ink, color, and gold on gilded paper.\nBelow: Tawareya Sotatsu, The Barrier Gate and Channel Buoys from the Tale of Genji (Side 1 of 2-sided screen), Japan, 1631. Seikado Bunko Art Museum.\nSotatsu’s shop/studio in Kyoto was called the Tawaraya, and it specialized in designing, producing, and repairing beautiful paper used by calligraphers. Tawaraya became famous for its finely-decorated folding fans. Folding fans were highly popular in urban Kyoto: people could easily carry them on the street, tucking them into sleeves when not in use.\nIn addition to making his work available to a wider public, Sotatsu developed a technique that was itself revolutionary. Traditionally, the application of the tarashikomi technique involved random pools of pigment and ink, but Sotasu “subtly subverted the tradition. He used ink to depict the ordinary subjects of everyday life—puppies and plants—in a style that took on an allusive ambiguity.” Instead of the solid, opaque colors traditionally associated with narrative painting, Sotatsu “used pooled ink and randomly changing color fields to indicate the underlying Buddhist theme: the unreliability of the visual world” (exhibition text, “Inkwork” section).\nRight: Tawaraya Sotatsu, Dragon and Clouds (detail), Japan, early 1600s. Gift of Charles Lang Freer, Freer Gallery of Art.\nOne of the fascinating themes curator James Ulak has conveyed throughout this exhibition is that Sotatsu celebrated his position as an artisan/craftsman: he dealt with refined subject matter such as ancient aristocratic poetry and illustrated court and religious narratives, but “he sought to reveal the construction and inner workings of an image rather than polish the ‘building blocks’ until they were invisible.” He was “first and foremost a craftsman,” and indeed “he apparently wanted his audiences to appreciate his work” by seeing the media he had used to create them. He had no interest in contending that his work was “art,” a pursuit in which the “building blocks” become subsumed by a higher creative intent. (exhibit catalogue, “The Visibility of Craft”)\nI thought about the relevance of Sotatsu’s celebration of “building blocks” as I walked through the Renwick Gallery’s Wonder exhibition in Washington. Each room is filled with large constructs that proudly display their artistic “bones,” including sculptural stacks of index cards by Tara Donavan and giant bird’s nests by Patrick Dougherty.\nAs we begin 2016, it’s perhaps well-worth celebrating the long lineage of building blocks that fill us with “wonder.”\nBy Amy Henderson, Contributing Writer\nSotatsu: Making Waves is co-organized by the Freer and Sackler galleries and the Japan Foundation; it will be on display until January 31, 2016. There is an exhibition catalogue edited by Yuko Lippit and James T. Ulak, Sotatsu (Arthur M. Sackler Gallery, Smithsonian Institution: Washington, D.C., 2015).\nAmy Henderson is a cultural critic and Historian Emerita, National Portrait Gallery\nSee Elaine King’s, Renwick Gallery, Washington, D.C., review here:', ""Impressionist Painting Techniques For Beginners\nImpressionists unequivocally stressed the impacts of light in their paintings. They utilized short, thick strokes of paint to catch the embodiment of the article instead of the subject's subtleties. Immediately applied brushstrokes give the painterly dream of development and suddenness. A thick impasto utilization of paint implies that even reflections on the water's surface show up as considerable as an article in a scene. The Impressionists helped their palettes to incorporate unadulterated, extreme hues.\nWhat techniques are used in Impressionism?\nIntegral hues were utilized for their lively complexities and shared improvement when compared. Impressionists maintained a strategic distance from hard edges by working wet into wet. The outside of an Impressionist painting is misty. Impressionists didn't utilize slight paint movies and coatings that were promoted by Renaissance craftsmen. Impressionists frequently painted during an era of the day when there were long shadows. This method of painting outside helped impressionists better delineate the impacts of light and stress the liveliness of hues. They utilized Optical Mixing instead of blending on the palette. Broken shading alludes with the impact of mixing hues optically instead of on the palette, disposing of immaculate inclusion and easily mixed advances. The Impressionist painters utilized layers of hues, leaving holes in the top layers to uncover the hues underneath. The system is accomplished through bringing forth, cross-incubating, stippling, dry brushing, and sgraffito (scratching into the paint). A blending of more splendid hues is done legitimately on the canvas to help in making the messed up shading impact and just darker hues are blended on the palette.\nWhat makes a painting impressionist?\nImpressionist art is a style where the artist catches the picture of an item through someone's eyes if they just got a look at it. They paint the photos with a ton of shading and the majority of their photos are open-air scenes. Their photos are brilliant and energetic. The artists like to catch their pictures without detail however with intense hues. applying paint in little contacts of unadulterated shading as opposed to more extensive strokes, and painting out of ways to find a particular transitory impression of shading and light. The outcome was to underline the artist's view of the topic as much as the subject itself. The Impressionists fused new logical examination into the material science of shading to accomplish an increasingly precise portrayal of shading and tone.\nWhat colors did impressionists use?\nWorking from the then-moderately new hypothesis of correlative hues, the consistent shading to utilize was violet, being the reciprocal of yellow, the shade of daylight. Monet said: “Color owes its brightness to force of contrast rather than to its inherent qualities … primary colors look brightest when they are brought into contrast with their complementaries.” The Impressionists made violet by coating cobalt blue or ultramarine with red, or by utilizing new cobalt and manganese violet shades that had turned out to be accessible to artists. The Impressionists were natural in their painting instead of scholarly. They examined nature, and any logical conditions were utilized to suit their comfort in their taking a stab at radiance. Notwithstanding, the laws of science gave ethical help to the Impressionist painter. He presently set out to do what Delacroix would not have done. The researcher concentrated light to examine its characteristics or to look for artificial methods for delivering it, however, the painter attempted to express its lovely quality. The negative blend of hues on the palette diminished their immaculateness, so the artist attempted to accomplish the positive blend gotten by the eye, where hues held their virtue and seemed to vibrate. At the point when seen from a reasonable separation the compared unadulterated hues seemed to blend on the outside of the canvas to give this positive blend which replicated the presence of nature.\nWhat are the three traits of Impressionism?\nCatching the occasion\nThe painting ought to have the option to catch the occasion, the moment demonstrated by light and movement.\nThe artists favored painting outside, which permitted the catch of the shading varieties in Nature. The favored topics were those around Nature itself, particularly scenes.\nColors and tones ought not to be acquired by blending paint on the palette. They should be unadulterated and separated on the canvases with free strokes. The development of the tones and colors turns out to be on a very basic level a round of optics.\nWhat are the characteristics of the Impressionist movement?\nThe impressionism establishes a break of the scholastic groups, restricting them with open-air painting and the catch of the varieties of light and the colors of Nature. The artists try to break free of evenness and geometric ideas and now comply with their very own tactile recognition right now of creation. The Impressionist movement is too, or more all, the assertion of the opportunity of the individual articulation of the artist.\nWhat was emphasized in Impressionist painting?\n- Impressionists emphatically underscored the impacts of light in their paintings.\n- They utilized short, thick strokes of paint to catch the quintessence of the article as opposed to the subject's subtleties.\n- Immediately applied brushstrokes give the painterly fantasy of movement and suddenness.\n- A thick impasto utilization of paint implies that even reflections on the water's surface show up as considerable as any item in a scene.\n- The Impressionists helped their palettes to incorporate unadulterated, extreme colors.\n- Reciprocal colors were utilized for their energetic differences and common improvement when compared.\n- Impressionists maintained a strategic distance from hard edges by working wet into wet.\n- The outside of an Impressionist painting is murky. Impressionists didn't utilize flimsy paint movies and coatings that were advanced by Renaissance artists.\n- Impressionists regularly painted during the day when there were long shadows. This strategy of painting outside helped impressionists better delineate the impacts of light and accentuate the dynamic quality of colors.\n- They utilized Optical Mixing as opposed to blending on the palette.\n- Broken shading alludes with the impact of mixing colors optically as opposed to on the palette, killing impeccable inclusion and easily mixed advances.\n- The Impressionist painters utilized layers of colors, leaving holes in the top layers to uncover the colors underneath. The strategy is accomplished through bringing forth, cross-bring forth, stippling, dry brushing, and sgraffito (scratching into the paint). A blending of more brilliant colors is done straightforwardly on the canvas to help in making the wrecked shading impact and just darker colors are blended on the palette.\nWatercolor painting impressionist style\nWatercolor painting in an impressionist style is a well-known strategy that can be adjusted for fledglings or even kids. Utilize broken brushwork when painting the scene. A snappy brush movement that leaves the surface, as opposed to a smooth completed look, is the objective. Intend to complete the fine art in one sitting and with relative speed. Impressionists considered the painting completed when the light and shadows modified the scene they were painting.\nImpressionism Artists List\n- Claude Monet\n- Pierre-Auguste Renoir\n- Edgar Degas\n- Édouard Manet\n- Mary Cassatt\n- Paul Cézanne\n- Alfred Sisley\n- Camille Pissarro\n- Henri Matisse\n- Berthe Morisot""]"	['<urn:uuid:d4d90154-980e-415f-a184-bd00aee34214>', '<urn:uuid:0bbe850a-d960-46f5-8679-3e3e92fadc58>']	factoid	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-13T05:28:56.360010	6	48	2434
51	soundtrack music role scary movies black comedy violence compare	'Seven Psychopaths' uses a diverse soundtrack of 18 commercial songs to accompany its violent scenes, including songs like 'Angel of Death' at the beginning and various other tracks during scenes of violence and character interactions. This contrasts with traditional horror movies, which typically rely on spooky music and sound effects like banging doors and sudden screams to create fear. For example, horror films like 'Suspiria' use specific musical scores to heighten terror, while 'Seven Psychopaths' often employs contrasting upbeat or popular music against its dark scenes, emphasizing its black comedy nature.	"[""Note:Below is a complete playlist of all 18 songs that can be heard in the black comedy “Seven Psychopaths”. Some of these commercial songs are not included on the official soundtrack album, but are used in the movie.\nAll 18 songs featured in “Seven Psychopaths”:\nAngel of Death - Hank Williams Timestamp:0:00 | Scene: First song. The film begins.\nThe First Cut Is the Deepest - P.P. Arnold Timestamp:0:03 | Scene: After Psycho #1 (Jack of Diamonds) shot the two guys on the bridge.\nMr. Gold and Mr. Mud - Townes van Zandt Timestamp:0:05 | Scene: The two characters Billy (Rockwell) and Hans (Walken) are introduced and are the first time in the picture.\nYou Don’t Know - Helen Shapiro Timestamp:0:18 | Scene: Billy talks to the mirror. He's getting ready.\nStranded - The Walkmen Timestamp:0:19 | Scene: Marty tells his story at the party.\nDifferent Drum - The Stone Poneys Timestamp:0:21 | Scene: Marty wakes up and a dog greets him.\nThe Trumpton Riots - Half Man Half Biscuit Timestamp:0:24 | Scene: Marty reads the ad in the newspaper. The one Billy launched.\nThe Unknown Immortal - Joe Strummer Timestamp:0:28 | Scene: Billy, Marty and Hans go to the cafe and discuss what to do about the murders.\nBerlioz: Strophes ‚Premiers Transports Que Nui N’oblie‘ [Roméo] - The Monteverdi Choir, Orchestre Révolutionnaire et Romantique, Catherine Robbin, Jean-Paul Fouchécourt & John Eliot Gardiner Timestamp:0:33 | Scene: Montage: Zachariach and Maggie on a killing spree.\nDon’t Start Loving Me (If You’re Gonna Stop) - Veda Brown Timestamp:0:45 | Scene: Billy tries to have sex with Ange.\nAngela Surf City - The Walkmen Timestamp:0:49 | Scene: After Billy shot Ange. He drives back to Hans and Marty.\nThe Comancheros - Claude King Timestamp:0:53 | Scene: They go to the bar after the wife of Hans was killed. Marty talks to Hans about the story...\nDallas - The Felice Brothers Timestamp:0:54 | Scene: Still in the bar. Billy tries to keep Marty from telling the story.\nDirty Dishes - Deer Tick Timestamp:0:59 | Scene: Hans, Marty and Billy drive into the desert.\nCountry Dumb - Josh T. Pearson Timestamp:1:02 | Scene: Song can be heard in the desert.\nDer Monde: Ach, Da Hängt Ja Der Mond! (Klein Kind/Chor) - Teresa Holloway, Philharmonia Chorus, Philharmonia Orchestra & Wolfgang Sawallisch Timestamp:1:04 | Scene: Billy tells his version of the story.\nThe First Cut Is the Deepest - P.P. Arnold Timestamp:1:44 | Scene: To be heard in the credits.\nInside a Silent Tear - Blossom Dearie\nWhat’s the movie about?\nMarty (Colin Farrell) is an unsuccessful Hollywood scriptwriter who one day gets more inspiration for his new script than he would like: his best friend Billy (Sam Rockwell) is mixing up the criminal scene in Los Angeles by stealing a Shih Tzu. His bad luck: It’s the lapdog of the crazy gangster Charlie (Woody Harrelson). Before Marty knows it, he’s in the middle of his own script and wants only one thing: Survival! And write his story to the end…\nIs the song you're looking for not listed here? Try asking in the comments below."", 'Horror Movies History and Vocabulary\nIf you like scary movies about ghosts and monsters and crazy killers, you\'re a fan of horror movies. Many early horror movies were based on old stories about scary creatures like vampires. Classic horror movies like 1922\'s Nosferatu and 1931\'s Dracula were based on Bram Stoker\'s vampire novel Dracula. Like all vampires, Count Dracula terrifies his victims by biting their necks and drinking their blood. We see him again in Francis Ford Coppola\'s 1992 film Bram Stoker\'s Dracula, but in 1994\'s Interview With the Vampire we see stylish vampires played by Tom Cruise and Brad Pitt who are more handsome and youthful than Dracula, but just as deadly.\nSome of the scariest creatures in horror movies are flesh-eating corpses called zombies. One of the first zombie films was 1932\'s White Zombie, but it was George A. Romero\'s zombie classics Night of the Living Dead and Dawn of the Dead that set the pattern for later zombie films like 2003\'s 28 Days Later and for Frank Darabont\'s popular TV series The Walking Dead.\nMovies about scary monsters have always been popular and one of the first was 1931\'s Frankenstein about a scientist who tries to make a human body from the body parts of corpses. Since the early 80s most monster movies have featured CGI monsters like those in sci-fi horror movies like John Carpenter\'s The Thing, David Cronenberg\'s The Fly and James Cameron\'s Aliens. In Frank Darabont\'s 2007 movie The Mist terrifying monsters drive people crazy with fear and in 2008\'s Cloverfield giant monsters tear down skyscrapers and huge spiders run through subways.\nHorror movies about nature\'s scary creatures can also be terrifying. If you\'re afraid of sharks you shouldn\'t watch Steven Spielberg\'s Jaws, if you have a fear of spiders you shouldn\'t see Arachnophobia, and if you\'re afraid of being attacked by flocks of angry birds you\'d better not watch Alfred Hitchcock\'s The Birds either. If you see 2005\'s The Descent you might have nightmares about being trapped underground in a cave full of scary creatures, and if you see the classic Australian horror movie Long Weekend you might never go camping again.\nFilms about supernatural beings like ghosts and spirits and movies about spooky places like haunted houses can be very scary as well. The classic horror movie The Haunting is an early example, and more recent examples include The Babadook and 1408 in which a writer of books about haunted houses is driven crazy by a hotel room\'s evil power. These movies use sound effects like banging doors and sudden screams to scare us, and they often use spooky music as well. Some of the finest music in a horror movie score can be heard in 1977\'s Suspiria in which young dancers are terrorized by evil witches who run a ballet school.\nIn paranormal horror movies we see forces and powers that can\'t be explained by science. In Tobe Hooper\'s Poltergeist we see objects being moved by invisible forces and in Brian De Palma\'s Carrie we see a teenage girl using her telekinetic powers to control objects with her mind whenever she\'s angry. In David Cronenberg\'s The Dead Zone a teacher wakes from a coma with the power to see into the past and the future, and in M. Night Shyamalan\'s The Sixth Sense we see a boy with special powers talking with spirits until a twist at the end changes the story.\nSupernatural movies with religious themes are often the scariest of them all, and The Exorcist is widely-regarded as the most terrifying movie ever made. It begins with a girl becoming possessed by Christianity\'s most powerful demon and ends with a Catholic priest performing an exorcism. In Roman Polanski\'s Rosemary\'s Baby we see witches arranging the birth of a very special demon, and in The Omen we see a powerful demon possess the body of a young boy.\n2002\'s Frailty is about a mentally-ill Christian who ""talks to God"" and kills neighbours because they\'re ""demons"". It has a religious theme but it\'s a psychological horror movie because the supernatural elements arise from a mental illness. The 1962 classic What Ever Happened to Baby Jane? is about a mentally-ill woman who mistreats her disabled sister, and Stanley Kubrick\'s 1980 masterpiece The Shining is about a mentally-ill writer who sees supernatural beings as his illness worsens. The Shining is based on a horror story by the American writer Steven King, as is 1990\'s Misery in which a mentally-ill nurse terrorizes one of her favourite writers after finding him injured in a car accident.\nPeople who do terrible things without feeling pity or shame are called psychopaths. Not many psychopaths are as bad as the polite and pretty little eight-year-old girl in 1956\'s The Bad Seed, or as cruel and crazy as the teenage psychopath who terrorizes his family in 2011\'s We Need to Talk About Kevin. But the most famous film about a psychopath is Alfred Hitchcock\'s 1960 horror masterpiece Psycho. Hitchcock is widely-regarded as one of cinema\'s greatest directors, and studying the shots, the editing, the sound effects and the music in the shower scene from Psycho can help us understand why.\nAt first we hear the gentle sound of running water as a woman takes a shower in a motel bathroom. Then a shot filmed from in front of the woman shows a shadowy figure sneaking up behind her - a classic horror movie angle. Suddenly we see the curtain torn open and hear violins screaming as the killer attacks her with a knife. Over thirty quick close-ups draw us into the scene; close-ups of the knife cut to close-ups of the woman as she fights her killer off, and close-ups of her hand pulling the shower curtain down are edited together with shots of her slowly sliding down the wall as she dies. The music stops and we hear the sound of running water once again as her blood drains away and the camera pans back from a lifeless eye.\nThe psychopath in Psycho is a serial killer who murdered many victims before attacking the woman in the shower. Famous movies about serial killers include 1990\'s The Silence of the Lambs starring Jodie Foster and Anthony Hopkins and 1995\'s Seven starring Brad Pitt and Morgan Freeman. In 2004\'s Saw we see a clever psychopath playing deadly games with his terrified victims, and in American Psycho we see a crazy young New York banker killing his victims ""for fun"".\nMany horror movie fans love slasher films in which extremely violent psychotic killers use weapons like axes, knives, hammers and chainsaws. One of the first was Tobe Hooper\'s low-budget 1974 film The Texas Chainsaw Massacre and others include John Carpenter\'s Halloween and Wes Craven\'s slasher classics A Nightmare on Elm Street and Scream. There are countless sequels and remakes of these films, but the originals are often much better.\nMovies in which people are terrorized for money or for fun are often called horror dramas. In The Night of The Hunter we see two young children being hunted for money hidden by their father before he went to jail, and in the 2010 film Buried we spend ninety terrifying minutes with a man who\'s been kidnapped and buried alive. In Funny Games and Eden Lake we see teenage bullies terrorizing people for fun, and in 2011\'s You\'re Next we see a wealthy family being terrorized by a gang of hired killers in animal masks.\nThe makers of the low-budget 1999 horror movie The Blair Witch Project wanted moviegoers to think it was made with video footage they\'d found in a haunted house. The Blair Witch Project was so cheap to make and so successful that it started a new subgenre called found footage horror movies. Some of the scariest are Paranormal Activity in which video cameras record paranormal events while a young couple sleeps, and the Spanish films [REC] and [REC] 2 in which a TV crew records terrifying footage while they\'re stuck inside a quarantined apartment building.\nIn recent years many of the scariest horror movies have been made in Asia. They include paranormal thrillers like Ringu and Pulse in which evil forces travel through television signals and telephone lines, supernatural thrillers like Thailand\'s Shutter and Japan\'s Ju-On: The Grudge, and shockingly violent movies about psychopaths and serial killers like Japan\'s Audition and Confessions and South Korea\'s A Tale Of Two Sisters and I Saw the Devil. If you\'re brave enough to watch these movies, at least make sure you\'re not alone...\nRecommended horror movies\n- Classic: Nosferatu (1922), Dracula (1931), Frankenstein (1931), Vampyr (1932), Cat People (1942), The Haunting (1963)\n- Vampires: Horror of Dracula (1958), Bram Stoker\'s Dracula (1992), Interview with the Vampire (1994), Let Me In (2010)\n- Zombies: White Zombie (1932), Night of the Living Dead (1966), Dawn of the Dead (1978), 28 Days Later (2003)\n- Monsters: The Thing (1982), The Fly (1986), Aliens (1986), The Host (2006), The Mist (2007), Cloverfield (2008)\n- Creatures: The Birds (1963), Jaws (1975), Long Weekend (1979), Arachnophobia (1990), The Descent (2005)\n- Supernatural: Suspiria (1977), The Changeling (1980), 1408 (2007), Insidious (2011), The Babadook (2014)\n- Paranormal: Carrie (1976), Poltergeist (1982), The Dead Zone (1983), The Sixth Sense (1999), The Conjuring (2013)\n- Religious: Rosemary\'s Baby (1968), The Wicker Man (1973), The Exorcist (1973), The Omen (1976), Frailty (2002)\n- Psychological: What Ever Happened to Baby Jane? (1962), The Shining (1980), Misery (1990), Black Swan (2010)\n- Psychopaths: The Bad Seed (1956), Psycho (1960), The Vanishing (1988), We Need to Talk About Kevin (2012)\n- Serial Killers: The Silence of the Lambs (1990), Seven (1995), American Psycho (2000), Saw (2004), Zodiac (2007)\n- Slasher: The Texas Chainsaw Massacre (1974), Halloween (1978), A Nightmare on Elm Street (1984), Scream (1996)\n- Horror Drama: The Night of the Hunter (1955), Funny Games (1997), Eden Lake (2008), Buried (2010), You\'re Next (2011)\n- Found Footage: The Blair Witch Project (1999), Paranormal Activity (2007), [REC] (2007), The Sacrament (2013)\n- Asian: Ringu (1998), Audition (1999), Ju-On: The Grudge (2002), A Tale of Two Sisters (2003), I Saw the Devil (2011)\nangle (noun): the position of the camera in a shot - The angle allows the viewer to see something the victim can\'t.\nclose-up (noun): a shot taken from very close to the subject - There\'s a close-up of her eyes while she\'s screaming.\ncorpse (noun): the body of a dead person - That scene of a corpse rising from the grave is really scary!\ncreature (noun): anything that\'s alive except for people and plants - The scariest creatures I\'ve ever seen are bird-eating spiders.\ndemon (noun): a powerful evil spirit - A demon was crawling along the ceiling, snarling and hissing like a snake.\nedit (verb): to combine different shots when making a movie - I love the way they edited that scene.\nevil (adjective): extremely bad or wicked - They think there\'s something evil in the house.\nexorcism (noun): a ritual that forces a spirit to leave a possessed person - Priests don\'t still perform exorcisms, do they?\nfound footage (noun): footage of real events, or footage that looks real - We\'re making a found footage horror movie.\nghost (noun): the spirit of a dead person - Have you ever seen a ghost?\nhaunted (adjective): occupied by ghosts or evil spirits - The house could be haunted, you know.\nhorror movie (noun): a movie that frightens and shocks people - You\'ll have nightmares if you watch too many horror movies.\nmentally-ill (adjective): having an illness that affects a person\'s mind - Her mentally-ill mother did some terrible things to her.\nparanormal (adjective): strange and unexplained by science - It\'s about people who investigate paranormal events.\npossessed (adjective): controlled by an evil spirit - They don\'t really think she\'s possessed, do they?\npsychological (adjective): related to or affecting the mind - His problem\'s psychological, so he needs to see a therapist.\npsychopath (noun): a person with a mental illness that makes them violent and cruel - I think my boss is a psychopath.\nscary (adjective): frightening or causing fear - We sat around the campfire telling scary stories.\nserial killer (noun): a psychopath who often kills people - On the news it said that the serial killer has already murdered four young business women.\nshot (noun): a view of something in a movie - The films opens with a shot of children playing in a park.\nslasher film (noun): a film about a very violent psychopath - I’m terrified of chainsaws because of a slasher film I watched as a child.\nsound effects (noun): recorded sounds used in films, TV shows, etc - Spooky sound effects like footsteps and creaking doors are often used in horror movies.\nspirit (noun): a supernatural being without a physical body - In Thailand they build little houses for spirits to live in.\nspooky (adjective): makes you think of scary things like ghosts - He told us a scary story about a spooky old house.\nsupernatural (adjective): related to an invisible world of ghosts and spirits - They think he has supernatural powers.\nterrify (verb): to make someone feel very frightened - The thought of being buried alive terrifies me.\nterrorize (verb): to use threats or violence to keep someone scared - The kidnapper terrorized his victims, so they didn’t try to escape.\ntwist (noun): an event that changes a story\'s meaning - Don\'t tell us what the twist is. You\'ll spoil the movie!\nvampire (noun): a scary creature that bites necks and drinks blood - It\'s about a vampire who lives in an old castle.\nwitch (noun): a person who uses magic or supernatural powers - There was a coven of witches living in the apartment building.\nzombie (noun): a dead body that comes alive - After a week of studying for exams with no sleep, we all looked like zombies.']"	['<urn:uuid:ab019ac7-cb9b-4c03-99d0-704b8a32815a>', '<urn:uuid:e7be3b55-f0c4-4fd1-98ef-36663bdb038b>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T05:28:56.360010	9	91	2834
52	I am interested in taking up a new hobby in Japan and I noticed many people practice flower arranging - what is ikebana and how did it start?	Ikebana means 'to arrange flowers' and is a traditional Japanese art form of flower arrangement. It originated as a practice of arranging floral offerings on altars and was introduced to Japan from China along with Buddhism. Today, ikebana is practiced widely as a hobby, with various schools teaching different techniques. These techniques consider aspects like the color, form, and position of the flowers. While some people practice it casually as a hobby, others develop it into a refined artform. It is also known as 'kado' or 'way of flowers'.	['30 Japanese Arts & Crafts You Need To Know\nOne of the great pleasures to any visit to Japan is enjoying the rich and beautiful world of its arts. Outside of the Japan, the country is best known for its most refined arts and cultural achievements however just as captivating, the country has a rich and varied tradition of crafts and folk arts which capture the spirit and essence of the Japanese experience.\nOn this page, we list arts and crafts you are likely to encounter while here, and provide a quick summary of what it’s all about. The list is varied and ranges from the traditional and classical arts to the more contemporary and every day including artistic techniques, practices and specific objects you will encounter on your travels – with a focus on items that make great gifts and souvenirs that are easy to carry for international visitors.\nBy no means exhaustive, we hope the following helps get you started with your exploration of arts and crafts in Japan:\nABURAE / 油絵\nOne of many forms of Japanese painting, ‘aburae’ refers to oil painting, in contrast to other styles. Traditional Japanese painting techniques have their origins in China, however aburae reflects the increasing influence of the West from the 16th century onwards, through the Edo Period and Japan’s opening to the world from the later-19th century onward. Much like in the West, oil painting is a broad artform encompassing all manners of schools, techniques and subjects.\nANIME / アニメ\nDerived from the English word ‘animation’, ‘anime’ encompasses both hand-drawn and computer-generated animation. Dating from the early-20th century, Japan produces a huge amount of anime each year, with countless studios catering to all tastes and storylines. Whereas animation is often considered purely for children in Western cultures, anime in Japan is loved by people of all ages, with film studios including Studio Ghibli and its founder, Miyazaki Hayao, considered national treasures and core part of the country’s social fabric.\nBONSAI / 盆栽\nInstantly familiar to most international visitors, ‘bonsai’ involves the studied cultivation of plants into miniature forms, trained and sculpted to mimic a fully-grown tree. Requiring years of application to master, bonsai can be practiced by anyone with space for a miniature tree of a shelf and as such, is a hugely popular past-time both inside and outside Japan.\nBUTSUZO / 仏像\nWhile Shintoism is the official religion of Japan, Buddhism plays an equally important role with the majority of Japanese identifying with both religions. Imported from Korea and China from around the 5th century, Japanese Buddhism is now a distinct branch of the religion with many denominations existing within that general umbrella. Visitors to temples will be struck by the deep beauty and tradition of artistic expression, of which, Buddhist sculpture or ‘butsuzo’ is perhaps the most striking. Highly complex and quite confusing at first, appreciating the beauty and importance of the sculptures doesn’t require expert knowledge but instead, just simple curiousity about what you are looking at and what it means. Many visitors will also be drawn to and captivated by statues of the buddhas. Often inside temples but also outdoors, buddha statues depict not just the historical Buddha – the man whose teachings Buddhism is based on – but also the many buddhas with the religion. Yet again highly complex and captivating, sculptures range hugely in size, style and if you’re interested in buying one, price.\nCHADO / 茶道\nTranslating as ‘way of tea’, ‘chado’ or ‘tea ceremony’ is perhaps the quintessential Japanese practice and experience – a ceremonial and meditative performance, in which green tea is prepared for a guest. Practiced as a hobby by some Japanese, international visitors can experience chado at many locations throughout the country, with authenticity, expertise and cost varying greatly.\nDARUMA / 達磨\nAny visitor to Japan will surely have noticed the ubiquitous ‘daruma’ dolls decorating restaurants, businesses and homes throughout the country. Often made from papier mache and then hand-painted, the dolls are named after an Indian monk who transmitted Zen Buddhist practices to China. The legends about daruma are many and his dolls are bought to then make a wish or set a goal by colouring in one eye. The doll then remains as such until the wish or goal if fulfilled, at which time the other eye is coloured-in. Coming in all sizes and colours – with colours also carrying specific meaning – daruma make a meaningful gift or souvenir while in Japan.\nHANGA / 版画\nThe general term of ‘hanga’ refers to the traditional art of woodblock prints as used to produce ‘ukioy-e’ images – see below for details. Visitors might also hear the word ‘shin-hanga’. Shin-hanga refers to traditional methods applied to create a ‘new-hanga’, from the early 20th century onward. The movement involved some changes to traditional techniques with Western methods and motifs distinguishing the art from earlier iterations.\nIKEBANA / 生け花\nMeaning ‘to arrange flowers’, ‘ikebana’ is a popular hobby for many Japanese with some practitioners developing their skill to an artform. Also referred to as ‘kado’ or ‘way of flowers’, this ancient artform was originally used in arrangement of floral offerings on altars, introduced to Japan from China with Buddhism. Today, ikebana is practiced by many people as a hobby with others mastering the art with numerous schools of ikebana teaching different techniques considering the colour, form, a position of the flowers.\nKIMONO / 着物\nWhile ‘kimono’ are a type of traditional clothing and as such, not an artform in their own right, like many other practical items in Japan, the skill involved in producing them has so great and that many are not considered pieces of artistic work in their own right. Throughout Japan, you will encounter kimono museums displaying some truly exquisite pieces, never to be worn again or first created as art pieces demonstrating the stunning skill of this most Japanese of arts.\nMANDALA / 曼荼羅\nHindu in origin, ‘mandala’ play an important role in Buddhism including Japanese Buddhism. Made popular in Japan from the 9th century onward, mandala are geometric configuration of symbols depicting Buddhist entities and conceptualisations of the universe radiating-out from the centre. Varying greatly in form and appearance, the production of mandala is considered a means of entering a meditative state and a point of contemplation. Playing an important role in both Hinduism and Buddhism, mandala have been appropriated by ‘new age’ movements and contemporary artists in new interpretations of very old techniques and themes.\nMANEKI-NEKO / 招き猫\nA common sight throughout Japan, ‘maneki-neko’ or ‘beckoning cats’ are believed to bring good luck to their owners. Coming in all sizes and colours, the cat has one paw lifted beckoning fortune and good luck toward it. Often made of ceramic or plastic, some are animated with their lifted paw moving back and forth. Lots of stories tell of the origins of the popular cats, which make a cute and meaningful gift or souvenir of your time in Japan.\nMANGA / 漫画\nEven if you’re not a fan, chances are you have some idea that Japan loves comics. Known as ‘manga’, Japanese comics have their roots in much older art forms and are heavily read by many people through their lives. As such, stories span the spectrum of human experience, from the joyful and childish, to dark and twisted and everything in between. Having first emerged in the 18th century, manga exploded in popularity through the second half of the 20th century and is now a dominant art form in Japan.\nMIZUHIKI / 水引\nGift-giving plays an essential role in Japanese culture as a sign of appreciation and respect. The presentation of gifts can be considered tantamount in importance to the gift itself, a cultural propensity which leads to Japan’s reputation for beautiful design and packaging for even the most simple of items. The ancient art of ‘mizuhiki’ involves the tying of beautiful, colored cords around gifts as a form of decoration. This tradition played an important role during the Edo Period, when gifts presented by and for samurai became increasingly more elaborate in design – with the color and number of cords all holding meaning. Today, mizuhiki continues to be practiced in decorating gifts and artform in its own right.\nNIHONGA / 日本画\nLiterally translating as ‘Japanese painting’, ‘nihonga’ is a general term used to distinguish it from Western-style oil painting. As much as the motifs depicted, nihonga is defined by the materials used including paper, silk, wood or plaster as the base on which ‘sumi’ ink, mineral and other pigments are applied, often with gold leaf and other metals used in abundance.\nNINGYO / 人形\nDolls or ‘ningyo’ play an important role in Japanese culture. From ornaments used for ceremonies to toys and gifts, ningyo range in style and use with many past through the generations as cherished family heirlooms. Wooden ‘kokeshi’ dolls are particularly popular. Seemingly simple, these wooden dolls depict young girls, often simply but beautifully painted – popular with many international visitors as gifts and mementos of their time in Japan.\nO-KAORI & KODO / お香 & 香道\nKnown as ‘kaori’ or more politely, ‘o-kaori’ in Japanese, incense is a ubiquitous and seductive part of local culture. In use at temples, shrines, businesses, homes and other venues throughout Japan, incense imbues life here with serenity and calm. One of the more abstract artforms, ‘kodo’ or ‘way of fragrance’ is the art of appreciating Japanese incense. Considered one of the three classical arts of refinement – along with ‘ikebana’ and ‘chado’ – you don’t have to be a master to engage in kodo. Indeed for international visitors, purchasing incense while in Japan is a fantastic memento of your time here as upon lighting it at home, the beautiful and subtle fragrance will transport you back to your time here and suddenly, you are practicing kodo.\nO-MAMORI / お守り or 御守\nVisitors to temples and shrines will certainly come across ‘omamori’ during their travels. As amulets sold at temples and shrines throughout the country, omamori are said to provide luck and protection for those who carry them. They provide specific protection and luck for all-manner if events – from travel to study, child-birth, protection from accidents, wealth… the list is endless – with many sought-out from famous locations and auspicious temples and shrines. A small and meaningful gift or souvenir for travelers in Japan.\nO-MEN / 能面\nReferred to generally as ‘o-men’ in Japanese, masks play an important role in many aspects of society. With their origins in legend, folkore and classical arts of ‘kabuki’ threatre, ‘noh’ masks can convey all range of emotion and capture Japan’s pantheon of colourful, sometimes menacing characters including ‘oni’, ‘hannya’, ‘tengu’, ‘kitsune’, ‘hyottoko’ and more. Modern interpretations of these masks can be seen in the characters of manga and anime, which have their own plastic masks now often worn by children at festivals.\nORIGAMI / 折り紙\nOf all the many forms of traditional Japanese arts and crafts, ‘origami’ or ‘paper-folding’ is likely the best-known to international visitors. From the first simple folds of paper, this ageless craft transcends to an art form and more, as the most intricate and astounding creations become possible. Enjoyed by Japanese from a young age, origami is often practiced throughout life with some practitioners mastering an artform that can produce astoundingly beautiful and complex creations.\nSHICHIFUKUJIN / 七福神\nReferred to as ‘shichifukujin’ in Japanese, the ‘Seven Lucky Gods’ are popular throughout the country, often seen together, in pairs of by themselves. Interestingly only one of the seven – Ebisu – is of Japanese origin, with the other including Daikokuten, Bishamonten, and Benzaiten originating in India and Jurojin, Fukurokuju and Hotei coming from China. Each god has specific associations including wealth, health, arts, and good harvest among others, with many temples and shrines across Japan depicted to one or another. With their origins lying deep in history, figures and statues of the Seven Lucky Gods vary greatly in appearance and size – from the cute to traditional – and make a fun gift or souvenir for travelers in Japan.\nSHIKKI / 漆器\nKnown as ‘shikki’ in Japan, lacquerware as another broad-ranging artistic practice, the manifestations of which play a hugely important role in daily life and ceremonial events. Also referred to as ‘nurimono’ (coated thing) and ‘urushi-nuri’ (lacquer coating), lacquerware encompasses numerous styles and applications. Almost every Japanese home would have lacquered bowls in the kitchen, with many also cherishing ornamental pieces or heirlooms used on special occasions.\nSHODO / 書道\nUsing brushes to create highly stylised ‘kanji’ (written characters) that flow effortlessly across the paper, the ancient art of ‘shodo’ (calligraphy) is one of the most widely practiced in Japan. The form of the characters can be so stylised that even for Japanese, they may be difficult to recognise and highlighting that the form and flow of the brush is just as important as the meaning, making it an art form accessible to audiences that may have no understanding of the characters.\nSUIBOKUGA / 水墨画\nA similar practiced to ‘suisaiga’ – as described below – ‘suibokuga’ uses only black ‘sumi’ ink applied by brush onto paper, to produce images that are apparently simple in their execution but highly detailed and evocative. Also referred to as ‘sumi-e’, the technique is highly revered and often used to decorate ‘fusuma’, sliding doors that separate rooms with traditional landscapes and\nSUISAIGA / 水彩画\nReferred to as ‘suisaiga’ in Japanese, watercolour painting is a popular artform adopted from the West. Practiced by both amateur and professional artists, this highly adaptable form of painting is used to depict all manner of subjects from traditional Japanese landscapes, contemporary portraits and abstract paintings.\nTAKEZAIKU / 竹細工\nBamboo weaving is a traditional craft still practiced today to produce both functional and highly elaborate forms. Many are intended for daily use while others are woven for their aesthetic beauty and between those two purposes, many occupy both spaces – both functional and intentionally beautiful. One of the world’s most durable and versatile natural products, bamboo is shaped into an amazing array of products and countless purposes, with the work of many takezaiku artists fetching extremely high prices.\nTEMARI / 手まり\nIntroduced to Japan from China, ‘temari’ are colourful fabric and embroidered balls originally given to children as toys at New Year. Often crafted by their grandmother, temari are cherished by children and past down through generations as family heirlooms. In-keeping with any craft in Japan, some temari are exceptionally skillful and elaborate, produced purely for their artistic value rather than a toy.\nTENGUI / 手拭い\nTypically made from cotton, ‘tengui’ are thin fabrics used as multi-purpose towels. Usually measuring 35 by 90 centimetres, tengui are intended as a decorative covering for everyday items or as decorations in their own right. Hugely popular as souvenirs, tengui come in all-manner of styles and decorative patterns.\nTOGEI / 陶芸\nOne of the oldest crafts in Japan, ‘togei’ refers to Japanese pottery and porcelain – a practice that dates back to the Neolithic including Japan’s celebrated ‘Jomon’ pottery. Also referred to as ‘tojiki’ (陶磁器) and ‘yakimono’ (焼きもの), this is another hugely broad and complex artform that is impossible to describe succinctly. From the earliest earthernware to highly technical porcelain and glazing, Japanese ceramics are highly sought after and are ever present in daily life.\nUKIYO-E / 浮世絵\nInternational visitors are likely to have encountered ‘ukiyo-e’ long before they reach Japan. A style of art the flourished during the Edo Period (1603-1868), ukiyo-e translates as pictures of ‘the floating world’. Produced as woodblock prints – or ‘hanga’ – and paintings, ukiyo-e depicted the world of ‘kabuki’ actors, ‘geisha’ ‘sumo’ and natural landscapes – considered a fleeting, ephemeral world. Japan’s most celebrated artist, Katsushika Hokusai, is considered the master of ukiyo-e, of which his woodblock print ‘The Great Wave Off Kanagawa’ is instantly recongisable and one of the most reprinted images in human history.\nWASHI / 和紙\n‘Washi’ refers to traditional Japanese paper and applies to paper produced using local fibres and traditional methods. The medium onto which or through which many of the artforms on this list are expressed, washi is appreciated in its own right and considered an artform that plays a fundamental role in many aspects of Japanese culture.\nEXPLORE JAPAN’S BEST MUSEUMS & GALLERIES\nJapan boasts some outstanding museums and galleries, showcasing the breadth and beauty of creative pursuits throughout the country. Naturally, the greatest concentration of art spaces is in Tokyo with some truly excellent public institutions, private collections, and pop-up galleries exhibiting everything from Neolithic ceramics, to treasures of the Edo Period or the most modern, abstract expressions in creativity.\nOur ’40 Recommended Museums & Galleries In & Around Tokyo’ page introduces some of our favourites in the capital before leading you outside, first to Hakone and Mount Fuji, and then onto Nagano and deep into Central Japan. We do so in recognition that Japan’s creative energy isn’t limited to the capital, with some of the best museums and galleries lying outside of Tokyo, enticing you to travel and explore the entire country – something we like to call, Art Space Japan.']	['<urn:uuid:af4a32b3-4512-4541-ba71-da604bec5a79>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T05:28:56.360010	28	89	2860
53	I keep hearing about nuclear power in the Southeast US, and I'd like to know how important this industry is for our region. What states are involved?	The nuclear industry has a significant presence in the Southeastern United States, specifically across five states: Georgia, North Carolina, South Carolina, Tennessee, and Virginia. This region is considered pivotal in the global nuclear industry, according to a comprehensive study called 'The Economic Impact of the Nuclear Industry in the Southeast United States' conducted by E4 Carolinas and released by the Southeast Nuclear Advisory Council.	"['CHARLOTTE, N.C. – A groundbreaking study, The Economic Impact of the Nuclear Industry in the Southeast United States, released by the Southeast Nuclear Advisory Council and conducted by E4 Carolinas, reveals the Southeastern United States as a pivotal region in the global nuclear industry. The study presents a comprehensive analysis of the economic impacts of the nuclear energy sector in the five-state region consisting of Georgia, North Carolina, South Carolina, Tennessee, and Virginia.\nThe U.S. Nuclear Industry Council (USNIC) has elected Jeff Merrifield to serve as Chair of its Board of Directors. Merrifield is the Nuclear Energy practice leader at Pillsbury.\nan Estonian company pioneering the use of Small Modular Reactor (SMR) technology to support energy security and a net-zero energy market for Estonia, today announced that it has selected Deep Isolation’s technology as its solution for storage and disposal of SMR spent fuel. The two companies today have entered into a Memorandum of Understanding (MOU) to jointly drive forward the development of SMRs in Estonia supported by a safe and scalable solution for the resulting spent fuel.\nNANO Nuclear Energy Inc. (""NANO Nuclear""), an emerging microreactor and advanced nuclear technology company, led by a world-class nuclear engineering team developing proprietary, portable, and clean energy solutions, announced that Idaho National Laboratory (INL) has completed a pre-conceptual design review of NANO Nuclear’s “ODIN” low-pressure coolant microreactor design.\nGE Vernova’s Nuclear business, GE Hitachi Nuclear Energy (GEH), today welcomed confirmation it has been awarded a £33.6 million UK Future Nuclear Enabling Fund (FNEF) grant from the Department for Energy Security & Net Zero (DESNZ). The UK Government has ambitions for 24 GW of nuclear by 2050 to help in providing energy security for the UK and for meeting net zero.\nToday, TerraPower announced that the TerraPower Isotopes team has distributed the first samples of Actinium-225 to two pharmaceutical companies. Actinium-225 from TerraPower will be used as a starting material by these pharmaceutical companies for research and development of advanced, targeted cancer treatments and in drug trials involving targeted alpha therapy in multiple disease areas.\nX-Energy Reactor Company, LLC (“X-energy” or the “Company”), a leading developer of advanced small modular nuclear reactors and fuel technology for clean energy generation, announced that it has successfully completed milestones in the Canadian Nuclear Safety Commission’s (“CNSC”) pre-licensing Vendor Design Review (“VDR”). The Company submitted the design of its Xe-100 advanced small modular reactor to CNSC for a combined Phase 1 and 2 VDR process. Upon completion of these phases, CNSC concluded there are no fundamental barriers to licensing the Xe-100, an outcome that increases confidence in proceeding with formal license applications in Canada.\nNAC International Inc. (NAC) has received approval from the U.S. Nuclear Regulatory Commission (NRC) for its patented OPTIMUS®-L packaging system (U.S. Pat. No. 11,373,773) to contain and transport High-Assay Low-Enriched Uranium (HALEU) TRISO fuel, which will be used in advanced reactors. This is the first time the NRC has licensed high-capacity packagings (>500 lb payload) of HALEU TRISO fuel in the U.S. and lays the foundation for future licensing of NAC’s OPTIMUS systems to carry other HALEU contents.\nPelican Energy Partners Base Zero LP (""Pelican"") is pleased to announce the acquisition of Container Technologies Industries, LLC from a group of private shareholders. CTI is based in Helenwood, Tennessee and is a manufacturer of containment solutions for the nuclear industry. CTI is a certified HUBZone small-business whose customers include the U.S. Department of Energy, the U.S. Department of Defense and the commercial-nuclear industry.\nThe U.S. Nuclear Regulatory Commission (NRC) has voted to issue a construction permit to Kairos Power for the Hermes demonstration reactor to be built at the Heritage Center Industrial Park in Oak Ridge, Tennessee. A critical step on Kairos Power’s iterative pathway to commercializing its advanced reactor technology, the Hermes reactor will demonstrate the company’s ability to deliver clean, safe, and affordable nuclear heat.\nCurio, a trailblazer in nuclear technology solutions, and Deep Isolation, an innovator in nuclear waste disposal, have entered into a Memorandum of Understanding (MOU) to collectively drive forward the development of advanced technologies for efficient and secure disposal of high-level nuclear waste (HLW).\nFramatome Inc. (Framatome) and Ultra Safe Nuclear Corporation (USNC) today signed an agreement to establish a joint venture (JV) at the World Nuclear Exhibition in Paris, France. This JV will provide nuclear fuel for the fourth generation Micro-Modular™ Reactor (MMR®) and other advanced reactor designs. The fuel supply will include commercial quantities of Tri-structural Isotropic (TRISO) particles and USNC’s proprietary Fully Ceramic Microencapsulated (FCM®) fuel.\nPelican Energy Partners Base Zero LP (""Pelican"") is pleased to announce that it has acquired Springs ATG, LLC (""Springs ATG"") from Machine Build Technologies. In conjunction with the transaction, Springs ATG will be rebranded as Advanced Technology Group (""ATG"" or the ""Company""). ATG is based in Westminster, Colorado and is a manufacturer of containment solutions, including gloveboxes, for the nuclear industry. The Company\'s containment solutions are used in multiple applications to facilitate life extension of nuclear power plants across North America, as well as enabling the nascent SMR/Advanced Reactor industry.\nToday, Centrus Energy Corp. announced that it has made its first delivery of High-Assay, Low-Enriched Uranium (HALEU) to the U.S. Department of Energy, completing Phase One of its contract with the Department by successfully demonstrating its HALEU production process. Centrus will now move on to Phase Two of the contract – requiring a full year of HALEU production at the rate of 900 kilograms per year at its American Centrifuge Plant in Piketon, Ohio.\nNASA is leading an effort, working with the Department of Energy (DOE), to advance space nuclear technologies. The government team has selected three reactor design concept proposals for a nuclear thermal propulsion system. The reactor is a critical component of a nuclear thermal engine, which would utilize high-assay low-enriched uranium fuel.\nUltra Safe Nuclear Corporation (USNC), a leader in nuclear energy innovation, is pleased to announce that it has been awarded a contract by NASA to develop and mature Nuclear Thermal Propulsion (NTP) systems for the advancement of America’s civil science and cislunar capabilities. This contract, which will see NTP move from the paper phase into hardware, is a testament to USNC’s capabilities on the leading edge of advanced reactor design and manufacturing.\nAmerican Centrifuge Operating (ACO), a subsidiary of Centrus Energy Corp, started enrichment operations for the first time at the U.S. Department of Energy’s (DOE) enrichment facility in Piketon, Ohio.\nDenetek’s plan could fortify energy security and decarbonization in Ukraine and regionally SMRs will be deployed on a number of current DTEK thermal power plant sites Denetek is in advanced talks with SMR vendors to select the most advantageous technology for safe, cost-effective, and rapid deployment Denetek is leveraging industry research and expertise from EPRI to develop tailored technology solutions.']"	['<urn:uuid:7ad06483-c3cc-4b59-ab92-f40b4ea1109a>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T05:28:56.360010	27	64	1127
54	comparison between milk producing cell types and salivary gland cell composition need details	Milk-producing cells include mammary epithelial cells that secrete milk components and myoepithelial cells responsible for milk ejection. Similarly, salivary glands comprise multiple cell types including serous and mucous acinar cells that produce proteins and mucous respectively, along with ductal cells that modify and transport saliva. Both tissues contain specialized secretory cells embedded in stroma with supporting cells like fibroblasts and blood vessels.	"['as well as liquid droplets\nAfter ovulation, Thimios Mitsiadis,” said lead author Prof, These cells are secreted in milk during the normal course of milking and are used as an index for estimating mammary health and milk quality of dairy animals worldwide, the more\nMilk from Teeth: Dental Stem Cells Can Generate Milk-Producing Cells The ability of adult stem cells to generate various tissue-specific cell populations is of great interest in\nMilk-producing cells are, which is responsible for their eventual movement out of the cell in the form of vesicles, and breed of an animal.\nExtruded into the milk are rare mammary epithelial cells and the plasma membrane-bound lipid droplets referred to as milk-fat globules, By 7-10 days postpartum, The mammary ducts and alveoli are embedded in a stroma that contains fibroblasts, The components are then passed along to the Golgi apparatus, We have already mentioned the myoepithelial cells responsible milk ejection from the breast, somatic cell count (SCC, and are able to generate all mammary cell populations and, Milk is produced during and between feedings.\nMacrophages (55–60%) and neutrophils (30–40%) dominate over lymphocytes (5–10%) ( 10 ), revealed a new study.\n, even more strikingly, the estimated slope from a linear regression of 24-h\nThe alveoli (milk-producing cells) These are made up of milk gland cells around the end of a milk duct, which form acini.\nFor the year 2009, adipocytes, Milk SC is influenced by cow productivity, The milk travels down ducts to the nipples, and it is also possible that key surface molecules on these cells could remain antigenically intact in the gut.\n[PDF]intimately involved in milk production, stage of lactation (days in milk, which then travels down to the front of the breast, the milk is ejected into the milk duct, progesterone from the corpus luteum, the percentage of macrophages then increases to 80-90% at a concentration of 10 4 -10 5 human milk macrophages per milliliter of milk.\nThe milk components are synthesized within the cells, adipocytes, The mammary ducts and alveoli are embedded in a stroma that contains fibroblasts, mainly by the endoplasmic reticulum (ER) and its attached ribosomes, according to a Swiss study, health, Breastfeeding success has nothing to do with the size of your breasts or nipples, Breast size is an inherited trait and determined by the number of fat cells you have, The gland cells produce milk, In stage 1, parity, individual cow test-day production records from 2, Blood flow is greatly expanded during lactation to\nDental stem cells can generate milk-producing cells\nAccording to a new study from researchers at the University of Zurich, The modeling was completed in 2 stages, cannibalized by other cells following the period of breastfeeding, Both vesicles containing aqueous non-fat components,835 Ontario dairy herds were examined, ×103 cells/mL) and parity, Breastfeeding is a supply-and-demand process, Therefore, for each animal in the study, with the transition from colostrum to mature milk, DIM), dental epithelial stem cells from mice can generate mammary ducts and even milk\n“The results show that the dental stem cells contribute to mammary gland regeneration, Blood flow is greatly expanded during lactation to\nDental epithelial stem cells from mice can generate mammary ducts and even milk-producing cells when transplanted into mammary glands, Viable leukocytes from milk have been isolated in feces from infants fed human milk ( 11 ), This could be used for post-surgery tissue regeneration in breast cancer patients.\nThe milk is produced in small clusters of cells called alveoli, Each record consisted of 24-h milk and component yields, The energy for the ER is supplied by the mitochondria, plasma cells and blood vessels,[PDF]intimately involved in milk production, an organ that develops in the ovary each time an ovum has been shed and has the function of preparing the uterus for receiving the developing embryo, The breasts will enlarge with pregnancy and breastfeeding, We have already mentioned the myoepithelial cells responsible milk ejection from the breast, in effect, plasma cells and blood vessels, and with the help of myoepithelial cells, milk-producing cells, lactation stage, from the Institute of Oral Biology at the Centre for Dental Medicine of the University of Zurich.\n[PDF]Milk somatic cells (SCs) are a mixture of milk-producing cells and immune cells, causes the terminal ductal cells to differentiate into the milk-producing cells', ""|Isolation of mouse salivary gland stem cells.|\n|Jump to Full Text|\n|PMID: 21339725 Owner: NLM Status: MEDLINE|\n|Mature salivary glands of both human and mouse origin comprise a minimum of five cell types, each of which facilitates the production and excretion of saliva into the oral cavity. Serous and mucous acinar cells are the protein and mucous producing factories of the gland respectively, and represent the origin of saliva production. Once synthesised, the various enzymatic and other proteinaceous components of saliva are secreted through a series of ductal cells bearing epithelial-type morphology, until the eventual expulsion of the saliva through one major duct into the cavity of the mouth. The composition of saliva is also modified by the ductal cells during this process. In the manifestation of diseases such as Sjögren's syndrome, and in some clinical situations such as radiotherapy treatment for head and neck cancers, saliva production by the glands is dramatically reduced. The resulting xerostomia, a subjective feeling of dry mouth, affects not only the ability of the patient to swallow and speak, but also encourages the development of dental caries and can be socially debilitating for the sufferer. The restoration of saliva production in the above-mentioned clinical conditions therefore represents an unmet clinical need, and as such several studies have demonstrated the regenerative capacity of the salivary glands. Further to the isolation of stem cell-like populations of cells from various tissues within the mouse and human bodies, we have shown using the described method that stem cells isolated from mouse salivary glands can be used to rescue saliva production in irradiated salivary glands. This discovery paves the way for the development of stem cell-based therapies for the treatment of xerostomic conditions in humans, and also for the exploration of the salivary gland as a microenvironment containing cells with multipotent self-renewing capabilities.|\n|Sarah Pringle; Lalitha S Y Nanduri; Marianne van der Zwaag; Ronald van Os; Rob P Coppes|\nRelated Documents :\n|15528335 - The role of b cell-mediated t cell costimulation in the efficacy of the t cell retarget...\n3242625 - Comparative histochemistry of the mucoproteic cells of the hypophysis from rhamdia hila...\n16155905 - Urokinase (u-pa) is produced by collecting duct principal cells and is post-transcripti...\n25225415 - Antibody-membrane switch (ams) technology for facile cell line development.\n19351715 - Homophilic and heterophilic polycystin 1 interactions regulate e-cadherin recruitment a...\n8971175 - Expression of membrane-type matrix metalloproteinase 1 (mt1-mmp) in tumor cells enhance...\n|Type: Journal Article; Video-Audio Media Date: 2011-02-08|\n|Title: Journal of visualized experiments : JoVE Volume: - ISSN: 1940-087X ISO Abbreviation: J Vis Exp Publication Date: 2011|\n|Created Date: 2011-02-22 Completed Date: 2011-04-18 Revised Date: 2013-06-30|\nMedline Journal Info:\n|Nlm Unique ID: 101313252 Medline TA: J Vis Exp Country: United States|\n|Languages: eng Pagination: - Citation Subset: IM|\n|Department of Cell Biology, University Medical Center Groningen, University of Groningen.|\n|APA/MLA Format Download EndNote Download BibTex|\nCytological Techniques / methods*\nSalivary Glands / cytology*\nStem Cells / cytology*\nJournal ID (nlm-ta): J Vis Exp\nJournal ID (publisher-id): JoVE\nPublisher: MyJove Corporation\nCopyright © 2011, Journal of Visualized Experiments\ncollection publication date: Year: 2011\nElectronic publication date: Day: 8 Month: 2 Year: 2011\npmc-release publication date: Day: 8 Month: 2 Year: 2011\nE-location ID: 2484\nPubMed Id: 21339725\nPublisher Id: 2484\n|Isolation of Mouse Salivary Gland Stem Cells|\n|Lalitha S. Y. NanduriI3736D2022|\n|van der Zwaag MarianneI3736D2022|\n|van Os RonaldI3736D2022I3736D2023|\n|Rob P. CoppesI3736D2022I3736D2023|\n|Department of Cell Biology, University Medical Center Groningen, University of Groningen\n|Department of Radiation Oncology, University Medical Center Groningen, University of Groningen\n|Correspondence to: Rob P. Coppes at firstname.lastname@example.org\n- Buffer: 1 % (w/v) bovine serum albumin in Hank's balanced salt solution\n- Reconstitute enzymes. Hyaluronidase enzyme: 40mg / mL, dissolved in buffer. Collagenase II: 23mg / mL, dissolved in buffer. Use freshly prepared enzyme solutions fresh for each isolation. When dissolved, store at 4 °C until use for digestion.\n- 50 mM calcium chloride in distilled water. Filter sterilize through a 0.2 uM pore size filter.\n- Mouse salivary gland (MSG) culture medium: DMEM:F12 with penicillin (100 I.U. / mL), streptomycin (100 μg / mL), glutamax (2 mM), epidermal growth factor-2 (20 ng / mL), fibroblast growth factor-2 (20 ng / mL), N2 supplement (1 %), insulin (10 μg / mL) and dexamethasone (1 μM).\n- Weigh the dissected salivary glands.\n- Chop glands into a homogenous pulp using sterile curved dissection scissors in a small petri dish.\n- Collect minced tissue in 14 mL tubes, using 1mL of buffer per 80mg submandibular tissue. Rinse the petri dishes clean of tissue using some of the buffer.\n- Add another 1 mL of buffer per 80 mg tissue, followed by 25 μL collagenase II enzyme solution, 25 μL hyaluronidase enzyme solution and 250 μL calcium chloride solution per 80 mg tissue. If working with large amounts of tissue, steps 2.4 - 2.9 can be performed in T25 tissue culture flasks for convenience.\n- Incubate in a shaking waterbath set at 37 °C for 20 minutes. Remove tubes and triturate by pipette to mix enzyme thoroughly through tissue again.\n- Replace in waterbath for another 20 mins.\n- Collect tissue by centrifugation at 400 x g, for 8 minutes. Discard supernatant.\n- Resuspend in 2 mL buffer for each 80 mg tissue, and repeat enzyme and calcium chloride addition as above. Incubate 20 minutes in shaking water bath. Remove tubes and triturate by pipette to mix enzymes thoroughly.\n- Incubate for final 20 min in shaking water bath. Collect cells by centrifugation as above, discard supernatant.\n- Resuspend each 80 mg of tissue in 2 mL buffer and pipette to wash tissue free of enzymes.\n- Centrifuge as previously to collect. Discard supernatant.\n- Repeat wash using 1 mL buffer per 80 mg tissue. Centrifuge to collect, discard supernatant.\n- Resuspend tissue solution in 1 mL buffer per 80 mg tissue.\n- Add solution to 100 μm pore-size filter placed over 50 mL Falcon tube. Do not apply more than 3 mLs of minced tissue solution per column, as filters may become blocked. Allow to seep through. Remove filtered material hanging on underside of filter by pipette, and add to filtrate.\n- Use syringe with 26 gauge needle to take filtrate from 50mL tubes and apply to 50 μm pore size filters on 5 mL tubes. Allow to filter through, assisting by loosening lids if necessary.\n- Centrifuge tubes as previously to collect. Discard supernatant.\n- Combine all pellets into one volume. Count using automated cell counter or haemocytometer.\n- Plate cells at density of 0.4 x 106 cells per well of 12-well plate, or 2.67 x 106 cells per T25 tissue culture flask. Add 1 mL MSG medium to each well or 6 mL to each T25 flask.\n- Incubate at 37 °C. Spheres should be clearly visible by day 2.\nAfter two to three days in culture, small aggregates of cells (salispheres) will be apparent in the cultures. Salispheres will continue to grow in size over a period of ten days in culture. Representative phase contrast microscopy images of salispheres are shown in Figure 1. Proliferating cells expressing stem cell-associated marker proteins can be isolated from these spheres, optimally between days 3-5 post isolation, and are capable of differentiation into functional, saliva producing acinar cells.\nFigure 1. Salisphere formation in vitro. Following mechanical and enzymatic digestion using the present protocol, spheres of increasing size can be found in the floating cultures. Panels are representative phase contrast microscopy images of spheres from days 0 (A), 4 (B), 7 (C) and 10 (D). Scale bar = 50 μm.\nThe tissue culture method described here represents a reproducible protocol for the isolation of stem cell-containing salispheres from the salivary glands of mice. Studies using cells isolated in this manner have highlighted the regenerative capacity of salivary gland stem cells 9. Transplantation of one hundred of c-Kit+ cells derived from the salispheres induced functional recovery of irradiated mouse salivary glands. These data are exciting and provide a starting point for the investigation of stem-cell based therapy for xerostomia. Many avenues remain to be explored however, including the full marker protein expression profile of the stem cells, the ability of submandibular glands to rescue the function of irradiated parotid salivary glands and vice versa, and the characterisation of the putative in vivo stem cell niche of the cells. Ultimately, the translation of this protocol to human tissue samples and the subsequent potential for the therapy of xerostomia in human patients using the isolated cells is the most exciting application of the described method.\nNo conflicts of interest declared.\n|Vissink A. Oral Sequelae of Head and Neck RadiotherapyCrit Rev Oral Biol MedYear: 20031419921212799323|\n|Napeñ as,J J,Brennan MT,Fox PC. Diagnosis and treatment of xerostomia (dry mouth)OdontologyYear: 200997768319639449|\n|Denny PC. Parenchymal cell proliferation and mechanisms for maintenance of granular duct and acinar cell populations in adult male mouse submandibular glandYear: 2005235475485|\n|Man YG. Persistence of a perinatal cellular phenotype in submandibular glands of adult ratJ. Histochem. CytochemYear: 199543120312158537636|\n|Cotroneo E,Proctor GB,Carpenter GH. Regeneration of acinar cells following ligation of rat submandibular gland retraces the embryonic-perinatal pathway of cytodifferentiationDifferentiationYear: 20107912013020056310|\n|Eirew P. A method for quantifying normal human mammary epithelial stem cells with in vivo regenerative abilityNat MedYear: 2008141384138919029987|\n|Gorjup E. Glandular tissue from human pancreas and salivary gland yields similar stem cell populationsEuropean Journal of Cell BiologyYear: 20098840942119410331|\n|Alonso L,Fuchs E. Stem cells of the skin epitheliumProc Natl Acad Sci U S AYear: 2003100Suppl 1118301183512913119|\n|Lombaert IM. Rescue of salivary gland function after stem cell transplantation in irradiated glandsPlos OneYear: 20083e2063e206318446241|\n|Coppes RP,Goot Avander,Lombaert IMA. Stem Cell Therapy to Reduce Radiation-Induced Normal Tissue DamageSeminars in Radiation OncologyYear: 20091911212119249649|\n[Figure ID: Fig_2484]\nClick here for additional data file\nPrevious Document: Recapitulation of an ion channel IV curve using frequency components.\nNext Document: Automated interactive video playback for studies of animal communication.""]"	['<urn:uuid:de9f46b9-24d3-4588-88a8-84d8094c58de>', '<urn:uuid:5958de83-4859-404b-ae01-431950f7c5b1>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T05:28:56.360010	13	62	2351
55	best foods to eat and exercise habits prevent cancer maintain healthy weight	To prevent cancer, the best foods to eat include pulses, wholegrain foods, fruits and vegetables - particularly foods containing fiber. It's important to minimize processed and red meats as they can increase bowel cancer risk. For exercise, experts recommend 150 minutes of moderate exercise per week, which can include walking, swimming, dancing, or even housework and gardening. Being active and maintaining a healthy weight can reduce the risk of 13 different types of cancer.	"[""What are the best ways to prevent cancer?\n04th Feb, 2019\nCancer is the second highest leading cause of death worldwide. It seems it’s everywhere you look, and with one in five men and one in six women developing the disease, it’s no wonder.\nHowever, steps can be made to prevent cancer. A key example is that 27% of all cancers relate to tobacco and alcohol use - no more than 10% of diagnoses come from genetic mutations.\nWhat is cancer?\nCancer occurs when a body’s cell or cells become abnormal. This abnormal cell will then divide and make more abnormal cells. These cells can evolve, mutate and weaken the immune system, which is why it is unlikely, if not impossible to create one treatment that cures all cancers.\nThe theme for World Cancer Day 2019 (and the next three years) is ‘I am and I Will’ - a call to action asking people, whether they’re a cancer survivor, healthcare worker or business leader, to make a personal commitment to help reduce the impact of cancer.\nFor those who don’t work in oncology or care for cancer patients, the greatest physical step you can take in cancer prevention is caring for yourself! Experts estimate that lifestyle changes could stop roughly 40% of all cancer cases.\nHere’s how you can do just that…\nHow to prevent cancer\nWe all love having a summer glow, but for skin cancer prevention, it’s crucial you know how to enjoy the sun safely.\nStay safe in the sun by:\n- Using a minimum of SPF15 with a rating of at least four stars - remembering to re-apply regularly, especially if you’re going in and out of water\n- Covering up and wearing sunglasses\n- Giving your skin breaks and spending time in the shade (as much as you can)\nDiet and foods that prevent cancer\nWhat you put into your body can have a massive impact on reducing your cancer risk. It helps you maintain a healthy weight and care for your internal organs, preventing their chance of becoming diseased.\nThe best foods to eat to prevent cancer include pulses, wholegrain foods, fruits and vegetables - essentially foods containing fibre.\nProcessed and red meats can increase the risk of bowel cancer, so it is best to keep these to a minimum within your diet.\nExercise goes hand in hand with diet, as keeping a healthy body weight can really lower your cancer risk. In fact, being active and maintaining a healthy weight can reduce the risk of 13 different types of cancer.\nWhat counts as being active?\nExperts recommend that adults aim for 150 minutes of moderate exercise per week – this could be walking, swimming, dancing or even doing housework and gardening!\nAlcohol can cause seven different types of cancer so cutting down is another way of preventing your risk. All forms of alcohol are equally as hazardous because it is alcohol itself that causes harm. Ultimately, the less you drink, the better your chances of remaining cancer-free.\nSmoking is the world’s leading cause of cancer. According to Cancer Research UK, it forms at least 15 different types! It’s responsible for seven in ten of the UK’s lung cancer cases and is the most common cause of death by the disease.\nWhat else can you do as a healthcare professional?\nThe Union for International Cancer Control (UICC), the leaders of World Cancer Day, asks that healthcare professionals learn the signs and symptoms of cancer to avoid misdiagnosis, and understand, as well as encourage, the value of early detection in their patients.\nSend your CV straight to the right person\nWho is the best nursing agency to work for?\n22 January 2020\nLooking to join a nursing recruitment agency? Here’s everything you need to know.\nby Suzie Lemons\nCervical Cancer Prevention Week\n20 January 2020\nHere is everything you need to know about Cervical Cancer; its causes, symptoms, prevention and most importantly, the future of cancer worldwide.\nby Suzie Lemons\nHow to become a Practice Nurse\n08 January 2020\nHave you ever wondered what it takes to become a general Practice Nurse? Find out here.\nby Suzie Lemons\nA merry New Year's to you all\n30 December 2019\nLast-minute New Year’s plans? Don't worry. Here's a list of places you can go and experience the best firework displays in the UK.\nby Adesh Panwar""]"	['<urn:uuid:67f2b1aa-bc84-4bf2-aa19-dd7aa260c838>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T05:28:56.360010	12	74	729
56	Having experience with both model organism databases and genomic analysis tools, I wonder how ZFIN and goSTAG differ in their approaches to supporting cross-species comparisons?	ZFIN and goSTAG take different approaches to cross-species comparisons. ZFIN facilitates cross-species comparisons through extensive data integration with other databases like NCBI/Entrez Gene, Vega, and Ensembl, while also using standardized ontologies like PATO for phenotype annotations that enable comparison across species. In contrast, goSTAG focuses on identifying biological themes within single-species data through GO term clustering and subtree construction, without explicit cross-species comparison functionality.	"[""|Home | About | Journals | Submit | Contact Us | Français|\nThe Zebrafish Information Network (ZFIN, http://zfin.org), the model organism database for zebrafish, provides the central location for curated zebrafish genetic, genomic and developmental data. Extensive data integration of mutant phenotypes, genes, expression patterns, sequences, genetic markers, morpholinos, map positions, publications and community resources facilitates the use of the zebrafish as a model for studying gene function, development, behavior and disease. Access to ZFIN data is provided via web-based query forms and through bulk data files. ZFIN is the definitive source for zebrafish gene and allele nomenclature, the zebrafish anatomical ontology (AO) and for zebrafish gene ontology (GO) annotations. ZFIN plays an active role in the development of cross-species ontologies such as the phenotypic quality ontology (PATO) and the gene ontology (GO). Recent enhancements to ZFIN include (i) a new home page and navigation bar, (ii) expanded support for genotypes and phenotypes, (iii) comprehensive phenotype annotations based on anatomical, phenotypic quality and gene ontologies, (iv) a BLAST server tightly integrated with the ZFIN database via ZFIN-specific datasets, (v) a global site search and (vi) help with hands-on resources.\nThe zebrafish is an established model for studies in vertebrate biology. Significant contributions are made to the identification and characterization of genes and pathways involved in development, organ function, behavior and disease. As the zebrafish model organism database, ZFIN facilitates research by providing a central database and a uniform interface to integrate and view the large amount of diverse data (1). ZFIN data derive from expert manual curation of scientific literature, collaboration with major resource centers and submissions from individual investigators. Curation is an ongoing effort with daily updates and enhancements. All data are attributed to their primary source. Annotations using controlled vocabulary terms from biomedical ontologies are a part of our routine curation efforts. These annotations provide a standardized data representation that enables cross-species comparisons. Extensive links to outside resources from ZFIN data pages further enhance cross-species comparisons. Data curated at ZFIN are also available through centers such as NCBI/Entrez Gene (2), Vertebrate Genome Annotations (Vega) (3) and the Ensembl (4) and UCSC (5) genome browsers.\nZFIN's new home page serves as a hub for online zebrafish resources. Major enhancements include: improved grouping and prioritization of ZFIN's most popular resources based on community input and page usage statistics; prominent access to the Zebrafish International Resource Center (ZIRC); expanded genome resource links and a ‘News’ section. The three-tab navigation bar available on all ZFIN pages provides convenient traversal of ZFIN and ZIRC pages.\nZFIN welcomes contributions to our ‘News’ section. Please email to: zfinadmn/at/zfin.org with your submissions.\nSupport for mutant and transgenic fish has been expanded to include complex genotypes containing multiple mutations, zygosity and transgenic constructs. Major enhancements include:\nThis query interface (http://zfin.org/cgi-bin/webdriver?MIval=aa-fishselect.apg&line_type=mutant) provides the most comprehensive access to genotype and phenotype data. Mutant/Transgenic queries may include gene or allele name, chromosome, mutagen, mutation type and anatomical structures that are affected. Queries may be performed on both mutants and transgenics or constrained to either characterized mutants or transgenics. The query results page provides summary data and relevant links including allele name, parental zygosity, mutation type, affected gene, mapping position and curated phenotypes.\nThis new page is the hub for genotype-specific data for the more than 5000 genotypes described in ZFIN (Figure 1). Genetic background, parental genotypes, affected genes and current sources for fish with this genotype are listed. Links to all phenotype and gene expression data associated with the genotype are included. A ‘Genotype Details’ section lists affected genes, zygosity, parental zygosity, mutation type, mutagen protocol, lab of origin and mapping information for each allele. Links to construct pages are provided for transgenic genotypes. Mutant/Transgenics query results, phenotype and gene expression details pages and gene pages link to the genotype page.\nThis new page provides information about transgenic constructs including regulatory regions, coding sequences, associated lines and related publications.\nA ‘Mutants and Targeted Knockdowns’ section has been added to the gene page. This section contains links to all associated genotypes and phenotypes. Anatomical ontology (AO) (1) and gene ontology (GO)(6) terms describing the phenotype are summarized. Knockdown reagents are listed.\nThumbnail images, genotype details, parental zygosity, supporting publications and links to affected structures and processes that characterize the phenotype can be accessed from this new page (Figure 2). Detailed phenotype annotations can be viewed by following the thumbnail or figure links. This page can be accessed from gene, morpholino and genotype pages.\nLinks are available from anatomy pages to mutant and transgenic genotypes with related phenotypes.\nA consistent and informative system for naming genotypes and transgenic constructs has been developed in consultation with the Zebrafish Nomenclature Committee. These conventions are described at: http://zfin.org/zf_info/nomen.html.\nPhenotype data from mutant screens and morpholino studies make the zebrafish a powerful model for elucidating gene function in development and disease. The correlation of zebrafish phenotype data with human genes and genetic syndromes is a major goal of zebrafish research. Our approach will facilitate cross-species comparisons by providing a comprehensive and consistent framework for phenotype annotation and queries.\nOntologies, hierarchical controlled vocabularies, provide the structure required for consistent annotations and subsequent comparisons. Ontologically derived annotations are successfully used by model organism databases to enhance computational and manual data analyses. GO annotations are an integral part of ZFIN's gene page and contribute to cross-species comparisons at AmiGO, the official GO query tool provided by the gene ontology consortium. AO annotations facilitate queries of gene expression data (http://zfin.org/cgi-bin/webdriver?MIval=aa-xpatselect.apg).\nThe AO and GO annotations can also be used for the robust classification of phenotypes, together with a cross-species structured vocabulary for describing phenotypes. The phenotypic quality ontology (PATO, http://www.bioontology.org/wiki/index.php/PATO:Main_Page) is being developed in collaboration with the model organism community and the National Center for Biomedical Ontology (NCBO)(http://www.bioontology.org/).\nPhenotype annotations are based on a bipartite syntax composed of an entity and a quality that may be used to describe an observable structure or process. The entity, a GO or AO term, represents the part of the phenotype being described. The quality, a PATO term, characterizes how the entity is affected. Examples of PATO qualities are ectopic, fused, small, edematous and arrested. Special modifiers, or tags, are included in PATO to distinguish normal from abnormal phenotypes.\nWe now capture detailed phenotype data from the scientific literature as part of our daily curation. Our first 7 months of phenotype curation using PATO yielded nearly 5000 annotations from ~200 publications. Phenotype descriptions cataloged at ZFIN in previous years and ongoing collaborations with laboratories conducting large-scale mutant screens provided an additional 9600 phenotype annotations for ~3000 genotypes.\nPhenotypes are tightly integrated into many parts of ZFIN and can be found on gene, genotype and anatomy pages. Comparisons of curated phenotypes at ZFIN are supported through the specification of ‘affected anatomy’ on the Mutants/Transgenics query form (http://zfin.org/cgi-bin/webdriver?MIval=aa-fishselect.apg&line_type=mutant). Phenotype annotations are associated with individual published figures and displayed on phenotype figure pages (Figure 3). Images and figure captions are included when consistent with journal copyright permissions. Ultimately, the use of PATO and other ontologies will provide a means for more complex queries at ZFIN and for comparisons with other organisms, specifically for the determination of animal models of human disease.\nWe encourage the use of PATO by laboratories annotating phenotype data and provide tools for this purpose. Please email us at: zfinadmn/at/zfin.org for additional information.\nBasic Local Alignment Search Tool, BLAST, is a powerful tool for performing comparisons of primary biological sequence data. ZFIN BLAST (http://zfin.org/cgi-bin/webdriver?MIval=aa-blast.apg) tightly integrates with the ZFIN database via ZFIN-specific data sets. Sequences or sequence accession identifiers (IDs) can be used to identify zebrafish genes annotated with phenotype, expression or GO data. Available data sets include, but are not limited to, ZFIN GenBank sequences, ZFIN cDNA sequences, ZFIN genes with expression, ZFIN morpholino sequences, ZFIN microRNA sequences and ZFIN Vega transcripts. ZFIN BLAST displays sequence alignment results with direct links to ZFIN gene and clone pages. Genes with associated expression, phenotype and gene ontology data are annotated with E, P and G icons, respectively. A camera icon indicates that representative figures are available at ZFIN.\nZFIN BLAST utilizes the WU-BLAST program (http://blast.wustl.edu/). Because multiple BLAST searches can require significant system resources, some reasonable constraints have been implemented to optimize overall performance. The sequence query length is limited to 50 000 letters. A graphical display is available for the first 50 alignments only. The zebrafish trace archive may be searched by single queries. Small to medium datasets accommodate batch queries of up to 100 sequences.\nThe entire ZFIN site can be searched quickly using the ‘site search’ feature located at the top right of all ZFIN pages. A results table narrows the search by categorizing matching records by data type. The number of matching pages is displayed for each category. Links to exact matches are provided when available. All individual matching records are listed. This broad view of ZFIN data complements the more comprehensive data type-specific query forms.\nZFIN plays an important role in the identification and procurement of probes, clones and fish lines used in research. Clones, probes and knockdown reagents are listed on gene pages. Fish lines are found on genotype pages. Sources and ‘Order this’ links are provided to resource centers and laboratories that can provide the resource.\nZFIN works closely with ZIRC to create reciprocal links between ZFIN data pages and ZIRC inventory pages. Recent changes to ZFIN's home page and a new tab navigation system provide easy access to ZIRC.\nZFIN anatomy pages provide researchers with an easy means for identifying probes for anatomical structures. Probes, used in large-scale in situ hybridization screens (7–9), are ranked by intensity, specificity and contrast using a five-star system.\nWe will soon expand ZFIN to incorporate detailed information on antibodies that recognize zebrafish anatomical structures and gene products. Sequence support will be enhanced to include transcripts and repeats. Expression patterns and phenotypes will be associated with specific transcripts and transcript annotations. GBrowse, the generic genome browser provided by GMOD (10), will provide an interactive view of the zebrafish genome integrated with ZFIN's rich biological data. Expanded use of ontologies will accommodate more robust queries of ZFIN data.\nFunds for the development of the Zebrafish Information Network are provided by the NIH HG002659. Funding to pay the Open Access publication charges for this article was provided by NIH HG002659. Funds to ZFIN through the NCBO to further development of tools and curation of phenotype annotation are provided by the NIH HG004028.\nConflict of interest statement. None declared."", '- Open Access\ngoSTAG: gene ontology subtrees to tag and annotate genes within a set\nSource Code for Biology and Medicine volume 12, Article number: 6 (2017)\nOver-representation analysis (ORA) detects enrichment of genes within biological categories. Gene Ontology (GO) domains are commonly used for gene/gene-product annotation. When ORA is employed, often times there are hundreds of statistically significant GO terms per gene set. Comparing enriched categories between a large number of analyses and identifying the term within the GO hierarchy with the most connections is challenging. Furthermore, ascertaining biological themes representative of the samples can be highly subjective from the interpretation of the enriched categories.\nWe developed goSTAG for utilizing GO Subtrees to Tag and Annotate Genes that are part of a set. Given gene lists from microarray, RNA sequencing (RNA-Seq) or other genomic high-throughput technologies, goSTAG performs GO enrichment analysis and clusters the GO terms based on the p-values from the significance tests. GO subtrees are constructed for each cluster, and the term that has the most paths to the root within the subtree is used to tag and annotate the cluster as the biological theme. We tested goSTAG on a microarray gene expression data set of samples acquired from the bone marrow of rats exposed to cancer therapeutic drugs to determine whether the combination or the order of administration influenced bone marrow toxicity at the level of gene expression. Several clusters were labeled with GO biological processes (BPs) from the subtrees that are indicative of some of the prominent pathways modulated in bone marrow from animals treated with an oxaliplatin/topotecan combination. In particular, negative regulation of MAP kinase activity was the biological theme exclusively in the cluster associated with enrichment at 6 h after treatment with oxaliplatin followed by control. However, nucleoside triphosphate catabolic process was the GO BP labeled exclusively at 6 h after treatment with topotecan followed by control.\ngoSTAG converts gene lists from genomic analyses into biological themes by enriching biological categories and constructing GO subtrees from over-represented terms in the clusters. The terms with the most paths to the root in the subtree are used to represent the biological themes. goSTAG is developed in R as a Bioconductor package and is available at https://bioconductor.org/packages/goSTAG\nGene lists derived from the results of genomic analyses are rich in biological information [1, 2]. For instance, differentially expressed genes (DEGs) from a microarray or RNA-Seq analysis are related functionally in terms of their response to a treatment or condition . Gene lists can vary in size, up to several thousand genes, depending on the robustness of the perturbations or how widely different the conditions are biologically . Having a way to associate biological relatedness between hundreds or thousands of genes systematically is impractical by manually curating the annotation and function of each gene.\nOver-representation analysis (ORA) of genes was developed to identify biological themes . Given a Gene Ontology (GO) [6, 7] and an annotation of genes that indicate the categories each one fits into, significance of the over-representation of the genes within the ontological categories is determined by a Fisher’s exact test or modeling according to a hypergeometric distribution . Comparing a small number of enriched biological categories for a few samples is manageable using Venn diagrams or other means of assessing overlaps. However, with hundreds of enriched categories and many samples, the comparisons are laborious. Furthermore, if there are enriched categories that are shared between samples, trying to represent a common theme across them is highly subjective. We developed a tool called goSTAG to use GO Subtrees to Tag and Annotate Genes within a set. goSTAG visualizes the similarities between over-representations by clustering the p-values from the statistical tests and labels clusters with the GO term that has the most paths to the root within the subtree generated from all the GO terms in the cluster.\nThe goSTAG package contains seven functions:\nloadGeneLists: loads sets of gene symbols for ORA that are in gene matrix transposed (GMT) format or text files in a directory\nloadGOTerms: provides the assignment of genes to GO terms\nperformGOEnrichment: performs the ORA of the genes enriched within the GO categories and computes p-values for the significance based on a hypergeometric distribution\nperformHierarchicalClustering: clusters the enrichment matrix\ngroupClusters: partitions clusters of GO terms according to a distance/dissimilarity threshold of where to cut the dendorgram\nannotateClusters: creates subtrees from the GO terms in the clusters and labels the clusters according to the GO terms with the most paths back to the root\nplotHeatmap: generates a figure within the active graphic device illustrating the results of the clustering with the annotated labels and a heat map with colors representative of the extent of enrichment\nSee the goSTAG vignette for details of the functions, arguments, default settings and for optional user-defined analysis parameters.\nThe workflow for goSTAG proceeds as follows: First, gene lists are loaded from analyses performed within or outside of R. For convenience, a function is provided for loading gene lists generated outside of R. Then, GO terms are loaded from the biomRt package. Users can specify a particular species (human, mouse, or rat) and a GO subontology (molecular function [MF], biological process [BP], or cellular component [CC]). GO terms that have less than the predefined number of genes associated with them are removed. Next, GO enrichment is performed and p-values are calculated. Enriched GO terms are filtered by p-value or a method for multiple comparisons such as false discovery rate (FDR) , with only the union of all significant GO terms remaining. An enrichment matrix is assembled from the –log10 p-values for these remaining GO terms. goSTAG performs hierarchical clustering on the matrix using a choice of distance/dissimilarity measures, grouping algorithms and matrix dimension. Based on clusters with a minimum number of GO terms, goSTAG builds a GO subtree for each cluster. The structure of the GO parent/child relationships is obtained from the GO.db package. The GO term with the largest number of paths to the root of the subtree is selected as the representative GO term for that cluster. Finally, goSTAG creates a figure in the active graphic device of R that contains a heatmap representation of the enrichment and the hierarchical clustering dendrogram, with clusters containing at least the predefined number of GO terms labeled with the name of its representative GO term.\ngene_lists < − loadGeneLists (""gene_lists.gmt"")\ngo_terms < − loadGOTerms ()\nenrichment_matrix < − performGOEnrichment (gene_lists, go_terms)\nhclust_results < − performHierarchicalClustering (enrichment_matrix)\nclusters < − groupClusters (hclust_results)\ncluster_labels < − annotateClusters (clusters)\nplotHeatmap (enrichment_matrix, hclust_results, clusters, cluster_labels)\nTo demonstrate the utility of goSTAG, we analyzed the DEGs from gene expression analysis (Affymetrix GeneChip Rat Genome 230 2.0 arrays) of samples acquired from the bone marrow of rats exposed to cancer therapeutic drugs (topotecan in combination with oxaliplatin) for 1, 6, or 24 h in order to determine whether the combination or the order of administration influenced bone marrow toxicity at the level of gene expression. Details of the analysis are as previously described . The data are available in the Gene Expression Omnibus (GEO) [11, 12] under accession number GSE63902. The DEG lists (Additional file 1), along with the GO terms from Bioconductor GO.db package v3.4.0 and GO gene associations based on biomaRt package v2.31.4, were fed into goSTAG using default parameters except for the rat species, the distance threshold set at < 0.3 and the minimum number of GO terms in a cluster set at > = 15. The defaults include only considering BP GO terms and requiring at least 5 genes within a GO category. There were 762 BPs significant from the union of all the lists. As shown in Fig. 1, the more red the intensity of the heat map, the more significant the enrichment of the GO BPs. Fifteen clusters of GO BPs are labeled with the term with the largest number of paths to the root in each. Negative regulation of MAP kinase activity (GO:0043407) was the GO BP labeled exclusively in the cluster associated with enrichment at 6 h after treatment with oxaliplatin followed by control. However, nucleoside triphosphate catabolic process (GO:0009143) was the GO BP labeled exclusively in the cluster associated with enrichment at 6 h after treatment with topotecan followed by control.\ngoSTAG performs ORA on gene lists from genomic analyses, clusters the enriched biological categories and constructs GO subtrees from over-represented terms in the clusters revealing biological themes representative of the underlying biology. Using goSTAG on microarray gene expression data from the bone marrow of rats exposed to a combination of cancer therapeutics, we were able to elucidate biological themes that were in common or differed according to the treatment conditions. goSTAG is developed in R (open source) as an easy to use Bioconductor package and is publicly available at https://bioconductor.org/packages/goSTAG.\nAvailability and requirements\nProject Name: goSTAG\nProject Home Page: The R Bioconductor package goSTAG is open source and available at https://bioconductor.org/packages/goSTAG\nOperating System: Platform independent\nProgramming Language: R version ≥ 3.4.0\nDifferentially expressed genes\nFalse discovery rate\nGene Expression Omnibus\nGene matrix transposed\nGO subtrees to tag and annotate genes\nEfron B, Tibshirani R. On testing the significance of sets of genes. Ann Appl Stat. 2007;1:107–29.\nSubramanian A, Tamayo P, Mootha VK, Mukherjee S, Ebert BL, Gillette MA, Paulovich A, Pomeroy SL, Golub TR, Lander ES, Mesirov JP. Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. Proc Natl Acad Sci U S A. 2005;102:15545–50.\nQuackenbush J. Genomics. Microarrays—guilt by association. Science. 2003;302:240–1.\nWang C, Gong B, Bushel PR, Thierry-Mieg J, Thierry-Mieg D, Xu J, Fang H, Hong H, Shen J, Su Z, et al. The concordance between RNA-seq and microarray data depends on chemical treatment and transcript abundance. Nat Biotechnol. 2014;32:926–32.\nHosack DA, Dennis Jr G, Sherman BT, Lane HC, Lempicki RA. Identifying biological themes within lists of genes with EASE. Genome Biol. 2003;4:R70.\nAshburner M, Ball CA, Blake JA, Botstein D, Butler H, Cherry JM, Davis AP, Dolinski K, Dwight SS, Eppig JT, et al. Gene ontology: tool for the unification of biology. The Gene Ontology Consortium. Nat Genet. 2000;25:25–9.\nGene Ontology C. Gene ontology consortium: going forward. Nucleic Acids Res. 2015;43:D1049–56.\nRao PV. Statistical research methods in the life sciences. Pacific Grove: Duxbury Press; 1998.\nBenjamini Y, Hochberg Y. Controlling the False Discovery Rate—a Practical and Powerful Approach to Multiple Testing. J R Stat Soc Ser B Methodol. 1995;57:289–300.\nDavis M, Li J, Knight E, Eldridge SR, Daniels KK, Bushel PR. Toxicogenomics profiling of bone marrow from rats treated with topotecan in combination with oxaliplatin: a mechanistic strategy to inform combination toxicity. Front Genet. 2015;6:14.\nBarrett T, Wilhite SE, Ledoux P, Evangelista C, Kim IF, Tomashevsky M, Marshall KA, Phillippy KH, Sherman PM, Holko M, et al. NCBI GEO: archive for functional genomics data sets—update. Nucleic Acids Res. 2013;41:D991–5.\nEdgar R, Domrachev M, Lash AE. Gene Expression Omnibus: NCBI gene expression and hybridization array data repository. Nucleic Acids Res. 2002;30:207–10.\nThe authors thank Dr. Myrtle Davis and Dr. Elaine Knight for the study design and microarray analyses. We greatly appreciate Dr. Maria Shatz and Dr. Christopher Duncan for their critical review of the manuscript. We thank Drs. Michael Resnick, Thuy-Ai Nguyen, Daniel Menendez, Julie Lowe and Maria Shatz for study designs that motivated the development and application of goSTAG. This research was supported, in part, by the Intramural Research Program of the National Institutes of Health (NIH), National Institute of Environmental Health Sciences (NIEHS).\nThis research was supported [in part] by the Intramural Research Program of the National Institute of Environmental Health Sciences, NIH.\nAvailability of data and materials\nThe microarray gene expression data used as an example for goSTAG is available in GEO under accession number GSE63902.\nPRB conceived the methodology, directed the development of the software and contributed to writing the paper. BDB designed the software, implemented the R code for the software and contributed to writing the paper. Both authors read and approved the final manuscript.\nThe authors declare no competing interest.\nConsent for publication\nCage size and animal care conformed to the guidelines of the Guide for the Care and Use of Laboratory Animals (National Research Council, 2011) and the U.S. Department of Agriculture through the Animal Welfare Act (Public Law 99–198).\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nAbout this article\nCite this article\nBennett, B.D., Bushel, P.R. goSTAG: gene ontology subtrees to tag and annotate genes within a set. Source Code Biol Med 12, 6 (2017) doi:10.1186/s13029-017-0066-1\n- Gene expression\n- Gene Ontology\n- Biological themes\n- Over-representation analysis\n- Functional enrichment\n- Pathway analysis']"	['<urn:uuid:902454e2-8b21-454b-a636-bc24f61a82a0>', '<urn:uuid:e0e50672-dddd-45aa-986e-20dd440ef21b>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-13T05:28:56.360010	25	64	3842
57	how to repot orchid into semi hydro container after preparing media	After preparing the container with media, thoroughly wash your hands with soap and water. Remove the plant from its old pot and carefully clean old media from the roots while keeping them intact. Place the plant in the new pot ensuring lowest leaves are level with pot top and longest roots don't extend below drain holes. Fill remaining space with media, tapping pot occasionally to settle media around roots. Finally, fill with dilute fertilizer solution until liquid runs out of drain holes. Maintain water level even with bottom of drain holes.	"['Here is a little guide with step by step instructions and pictures:\nBy Louis Aszod. Copyright River Valley Orchidworks.\n1. Gather your materials.\nYou will need: an appropriate plastic container, suitable media, a drill, and a plant to repot. For media, we are using fired clay nuggets similar to PrimeAgra. These are roughly 3/4 to 1 inch in all dimensions. Since the principle behind Semi Hydro culture is the same as the principle that rots your orchid\'s roots when you leave the pot standing in water, choice and size of media is important. As a general rule of thumb, the smaller\nthe media, the finer\nthe porosity, the more\nwater will be wicked up by capillary action and retained. Conversely, the larger\nthe media, the coarser\nthe porosity, the less\nwater will be wicked up and retained. So it\'s important that you choose a size and type appropriate for the moisture needs of the plant you are growing.\nFor a container, anything plastic that\'s deep enough and handy (from the laundry room or kitchen) will do. In this case, we are using a piece of Tupperware I snagged when from the kitchen.\n2. Rinse the media to remove any dust and fines and soak it for several hours. At the end of that time period, water the plant thoroughly.\n3. Mark the plastic pot approximately 2 inches up from the bottom and drill holes around the circumference. The holes should be large enough to allow excess water to freely drain, yet small enough to keep the media from spilling out:\n4. Drain the water from the soaked media, and pour enough media into your S/H ""pot"" until it covers the holes. You will need to adjust this initial level depending on the root length of the plant you intend to repot.\n5. WASH YOUR HANDS THOROUGHLY WITH SOAP AND WATER\n, then unpot your plant. This is a Phal\n. seedling we have been growing in Sphagnum, and the roots are in pretty good shape:\n6. Carefully remove the old media from around the roots. Repotting is a ""shock"" to the plant, so be as careful as possible to keep all of the roots intact:\n7. Place the plant into your pot. The plant\'s lowest leaves should be level with the top of the pot, and its longest roots should not extend below the drain holes you drilled in Step 3. Then, fill the pot to the top with the remaining media, rapping the pot against your workbench every so often to settle the media around the roots. The newly re-potted plant should look something like this:\n8. Using a dilute fertilizer solution, fill the pot until the liquid runs out of the drain-holes.\n9. Tada! You\'re done. Your plant has now been happily re-potted into S/H, and you too can now say you\'re a really cool S/H orchid grower.\nAdd water / fertilizer solution as necessary to keep the water level even with the bottom of the drain holes.\nLearn More about Semi-Hydro orchid growing at the OrchidTalk Orchid Forums']"	['<urn:uuid:76976e32-51cd-4e48-a8c8-2083ba82ba52>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T05:28:56.360010	11	91	511
58	time duration precambrian period earth formation cambrian beginning how many million years	The Precambrian period lasted approximately 4000 million years, spanning from the formation of the earth until the beginning of the Cambrian period around 570 million years ago. This represents about 90% of geological time.	['The time between the formation of the earth and the beginning of the Cambrian(about 570mya) is a 4000 my long period known as the Precambrian, this includes approximately 90% of geological time of which we know very little about as pre-Cambrian rocks are poorly exposed, many have been eroded or metamorphosed and fossils are seldom found.\nThe Precambrian has been divided into 3 Eons: 1. Hadean (4600-3800 mya of which there is no rock record) 2. Archean 3800-2500 mya) 3. Proterozoic 2500-570 mya. The present atmosphere is greatly depleted in Ne, Xe and Kr which are inert gases that should be preserved in the atmosphere.\nThis suggests that the earth’s initial atmosphere was lost early on either by boiling away during the magma ocean event or by being carried away by intense solar wind in the early solar system. At the end of the Hadean the present atmosphere and hydrosphere began to develop from volcanic emissions. It was during the proterozoic that a critical change occurred in the atmosphere, when it changed from a trace oxygen content of the Archean atmosphere to above 15% oxygen by 1800 mya.\nIt is widely believed that this change was brought about by the emergence of cyanobacteria which had adapted to create energy from the sun by photosynthesis(probably due to a shortage of raw materials for energy), as a result they had began to poison the earlier anaerobic bacteria or archea with their waste product; oxygen. This essay will focus on the evolution of the atmosphere and its relation to the banded iron formations of the late Precambrian. Banded Iron Formations\nCloud (1968) calls Banded Iron Formations, rhythmically banded chemical sediments of large, open water bodies that take different aspects but most characteristically consists of alternating layers of iron- rich and iron-poor silica. It is present in some of the oldest volcanic sequences (greenstone belts ca 2800 mya) and is a common sediment type formed until approximately 1800 mya. Although some younger formations with similar structure can be found there is great distinctions between them and the BIF’s of the Precambrian.\nThe BIF’s can occur in sequences which range from 15 ferric iron. Some people believe the iron to be of volcanic origin, weathered and transported into the oceans or exhaled from fumeroles Relation to the atmosphere The link between Banded Iron Formations (or BIF’s) and this change in oxygen levels is a close one, as BIF’s appeared about 2600 mya and continued until about 1800 mya. These deposits of marine hematite and quartz represent a precipitation of dissolved iron from sea water as the dissolved oxygen content of the water increased.\nAfter 1800 mya, Banded iron formations are rare, but terrestrial red beds are common for the first time suggesting that iron is being oxidised and precipitated in soils and rocks on land in the source area of sediments instead of being dissolved and carried into the oceans in its unoxidised form. Thus, it has been reasoned that the BIF-red bed transition marks the rise of atmospheric oxygen. Complimentary information comes from detrital uraninite in Archean and earliest Proterozoic alluvial rocks.\nBecause this uranium mineral can survive prolonged transport only in media containing little or no oxygen, the lack of detrital uraninite deposits younger than 2300 ma also points towards a significant environmental transition (Roscoe,1969). Not all scientists have accepted the validity of these observations or of their interpretation, Dimroth and Kimberley(1976) argued that at least some red beds antedate the end of BIF deposition, that Archean granites have paleoweathering profiles indicative of oxic environments, and that oxidised sulphur minerals (sulphates) occur in some of the oldest known sedimentary successions.\nAll of these observations are correct, and we must ask whether they preclude the interpretation of Archean and earliest Proterozoic environments as oxygen poor. Holland (1984) believed the answer to be no. As the formation of red beds and oxidised weathering profiles on granitic substrates requires oxygen, but only in minute quantities- considerably less than is needed for aerobic metabolism.\nAlso, marine sulphate does not require free oxygen as all as H2S can be photooxidized anaerobically to SO42- by photosynthetic bacteria, while the photochemical oxidation of volcanogenic S and SO2 to sulphate was probably a steady source of oxidised sulphur in the Archean oceans(Walker, 1983). Towe (1990) has specifically argued for the development of aerobic respiration early in the Archean and, therefor, for the presence of 1 to 2% PAL (present atmospheric level) O2 in the atmosphere since that time.\nThe possibility that oxygen levels reached this physiologically important threshold so early is not contradicted by the sparse geochemical data available for early Archean rocks, Towe’s model however suffers from the absence of Archean O2 sinks other than Fe2+. Some believe that the neglect of volcanic gases in his model casts significant doubts on the validity of his analysis. Knoll(1979) believed increases in atmospheric oxygen were probably occasioned by increases in primary productivity and/or decreased rates of oxygen consumption.\nHe believes the increase from very low O2 levels to 1 to 2% PAL may have been related to productivity increases associated with rapid growth and stabilising of the continents during the late Archean and earliest Proterozoic, in contrast, Cameron(1983) stated that ‘the later increase to 15% PAL does not seem to be related to a major tectonic event. The high oxygen level in today’s atmosphere must be related to the role of PO2 in the maintaining of redox balance of the atmosphere-biosphere-ocean-lithosphere system’. The nature of the connection is still in dispute.\nIt is believed that atmospheric PO2 determines the concentration of O2 in surface ocean water, but the influence of the O2 concentration in seawater on the burial efficiency of organic matter within marine sediments seems to be slight. Nutrients are a more likely link between PO2 and the burial rate of organic matter, and hence between PO2 and rates of long term O2 generation(Betts and Holland,1991). Holland constructed a plausible argument that links the marine geochemistry of PO43- to that of iron and hence to the O2 content of the atmosphere today.\nHe believes that if this argument is true then the history of atmospheric O2 may have been controlled by a complicated feedback system involving the marine geochemistry of iron and phosphorus. This would explain the rapid increase in PO2 around 2100 mya as marking the passage of the system across a threshold from one steady state to another. Conclusion The complexities and conflicting arguments involved in relation to the evolution of the atmosphere and its links to banded iron formation are hard to over emphasise.\nIt is a topic which still has a long way to go and one which may never be conclusively understood due to the lack of evidence from the Precambrian rocks (or lack of rock record altogether). However the general consensus seems to be that for a period of over a 1000 my until 1800 mya, conditions where favourable for chemical deposition of iron, there was distinct changes occurring in the atmosphere with rapid increase in free O2 coupled with a fundamental change in the evolution of early life from anaerobic bacteria to aerobic cyanobacteria.']	['<urn:uuid:122a711b-4236-48e4-a0f6-4c141b62b100>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T05:28:56.360010	12	34	1190
59	hunting rabbits alberta regulations and tularemia disease safety measures	In Alberta, rabbits and hares can be hunted year-round without a special license or limit. However, hunters must take precautions against tularemia ('rabbit fever'). These include wearing impervious gloves when skinning animals, cooking meat thoroughly to 165°F, and avoiding handling sick or dead animals. The disease can be transmitted through handling infected animals, insect bites, or exposure to contaminated soil.	"[""Rabbits and hares are fun to hunt and one more way to enjoy Alberta’s great outdoors, year-round. They are meaty animals and make delicious table fare. Plus they provide consistent opportunity, a nice bonus during or after bird and big game seasons.\nWho can hunt rabbits?\nIn Alberta, you can harvest rabbits and hares at any time of year, without a special licence requirement or limit.\nWhat kind of rabbits can I hunt?\nAlberta has snowshoe hares, Nuttall's cottontail, and white-tailed jackrabbits. All are good-eating small game animals.\nWhere can I find them?\nCheck out the foothills, parkland and boreal regions of Alberta—all common hare hangouts. Cottontails are a prairie species, and jackrabbits know what’s good, staying within prairie and parkland regions near agriculture. Looking for a place to hunt? Find a Conservation Site near you at www.albertadiscoverguide.com or download the Alberta Outdoor Adventure Guide app.\nWhat is their hunting season?\nAll year! Rabbits and hares can be hunted in any season across Alberta.\nHow do I hunt them?\nIt’s best to venture out on foot (snowshoes in the winter!) to track down hares in larger tracts of habitat. A dog can help too, flushing them from cover. Without a dog, snow can be your best friend, as it quickly reveals whether you're looking in the right spot or not. Hares tend to use the same travel trails and finding tracks means they are close by.\nTry this: have a friend follow tracks and trails while you stand still and watch the backtrail. These cagey critters often make a big circle to avoid the hunter, and the person standing still usually sees the most.\nFinding a hare in dense cover is tricky. You must watch for movement or listen for the sound, as a rabbit on the move can disappear in seconds. Ideally, you want to find a hare in its day nest, which usually provides cover from above. A hare will tuck into or under a tangle of fallen trees or limbs. Scan carefully!\nHares blend into their surroundings in winter, so watch for eyes and ear tips. Once you locate your first hidden hare, the next is a little easier as you know exactly what you’re looking for.\nTry this: set up logs and limbs to create cover along active game trails. The hares are quick to try out the new hiding spots, making them much easier to pinpoint.\nRabbits and hares are most active in low light conditions. To increase your odds of finding them out of cover, hunt in the early and late hours of the day.\nA shotgun is often first choice for hunting rabbit or hare – a .22 rifle also works well. You can skin and field dress your rabbits in the field to cool them quickly. The meat along the backbone and back legs are easy to remove. The front legs can be taken off with bone in. Once you get your rabbit or hare meat home, you can stew, bake, roast, fry, or grind it to make almost any dish! We recommend Mark’s Orelton Stew!"", 'Public Health Officials in Pueblo confirmed a wild rabbit tested positive for tularemia in Pueblo. The rabbit appeared to have no contact with people and was collected from the Liberty Point area in Pueblo West, Colorado.\n“Pueblo residents are advised that tularemia-causing bacteria may be present in some of the mammals – especially rabbits, rodents and hares and on the ground where these animals may be active,” stated Vicki Carlton, program manager in the environmental health division at the Pueblo City-County Health Department. She added, “Although there are no human cases of tularemia identified in Pueblo so far this year, Colorado has experienced human tularemia cases in people who have been exposed to contaminated soil, drinking contaminated water or inhaling bacteria.”\nPublic health has been monitoring rabbit die-offs in Pueblo West over the past month. This recent rabbit tested by the health department confirms tularemia is present in Pueblo County. Public Health specialists continue to monitor tularemia activity.\n“Because tularemia is known to be in Pueblo County, precautions to prevent tularemia infection should always be taken,” emphasized Ms. Carlton.\nTularemia, “rabbit fever,” is a bacterial infection most commonly transmitted to humans by the handling of sick or dead animals infected with tularemia. Infection can also occur from the bite of infected insects (most commonly ticks and deer flies) as well as exposure to soil and vegetation. Hunters who skin animals without gloves and are exposed to infected blood through an open wound are also at risk.\nTypical signs of infection in humans include fever, chills, headache, muscle aches, chest pain, and coughing. Tularemia can be effectively treated with antibiotics; therefore should you have any of these early signs, contact your medical provider.\nDogs and cats also get tularemia by eating infected rabbits or other rodents and through tick and deer fly bites. If your pet shows symptoms of illness including fever, nasal and eye discharge, and skin sores, take it to a veterinarian promptly. Tularemia is easily treated if diagnosed early in dogs and cats.\nRecommended precautions include:\n- Avoid handling wild animals.\n- When outdoors near places where wild rabbits or rodents are present, wear insect repellent containing DEET.\n- Use a dust mask when mowing or doing yard work. Do not mow over animal carcasses.\n- Leash your pets when outdoors and keep them away from dead animals.\n- Routinely use a tick and flea prevention treatment on pets.\n- If a dead animal must be moved, avoid direct contact with it. Wear insect repellent to protect yourself from its fleas or ticks, and use a long-handled shovel to scoop it up. Place it in a garbage bag and dispose in an outdoor trash receptacle. Wash your hands with soap and water afterwards.\n- Wear proper footwear outdoors where dead animals have been found.\n- Do not go barefoot or wear sandals while gardening, mowing or landscaping.\n- Wear gloves while gardening or landscaping, and wash your hands after these activities.\n- Do not drink unpurified water from streams or lakes or allow your pets to drink surface waters.\nIf you hunt, trap or skin animals, take additional steps:\n- Use impervious gloves when skinning or handling animals, especially rabbits.\n- Cook the meat of wild rabbits thoroughly to a temperature of 165°F or higher.']"	['<urn:uuid:a6b583f9-0b4e-472a-92fc-f4480a2df3e1>', '<urn:uuid:964c9f42-94b7-4454-8d46-f8bcc6bdff72>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T05:28:56.360010	9	60	1061
60	garden design professional seeking spade cut edging benefits maintenance requirements seasonal care	Spade-cut edging is the simplest and least expensive border type, created by digging a narrow trench around the bed. It provides an elegant look historically used in English gardens and works well on any slope or area size. For installation, use a flat spade with a straight cutting edge to dig a 3-4 inch deep trench, keeping the lawn edge vertical. Maintenance requirements include regular weed removal, keeping soil out of the trench, and recutting the edge once or twice per season to maintain freshness. The spade's cutting blade should be kept sharp for crisper edges with less effort.	"[""Well-defined gardens and orderly blooms give a lovely, landscaped appearance to any yard. With the use of decorative edging, it is easier than ever to create solid boundaries between your lawn and garden, and also maintain a freshly groomed look at all times. In addition to creating a neat appearance, edging prevents soil and mulch from spilling onto your lawn. It also keeps driveway gravel out of your flower beds and prevents grass from growing into garden areas, reducing the amount of time you need to spend weeding.\nIf you are looking to add both definition and design to your landscape, edging is available in a variety of materials, styles and colors. Edging will not only improve how your yard looks, it will also reduce the time you spend maintaining it. Before you buy edging, consider the following questions:\n• Is it important that the edging be easy to install?\n• What is your estimated budget for edging?\n• How do you install different types of edging?\n• Do you live in an area that experiences temperature extremes?\n• Are you trying to create a straight or curved border?\nSpade-Cut, Strip, Masonry and Wood EdgingThere are several different types of edging available. The four main types include spade-cut, strip edging, masonry and wood edging. Look around your yard and choose a material that aesthetically matches the design and style of your outdoor space. Each edging material has advantages and some varieties are better suited to certain climates.\nThe main difference between each material, however, is budget and appearance. The least expensive, most commonly used material is plastic edging. One of the most attractive and most expensive types is rock or stone. Each type also varies slightly in how it is installed.\nSpade-Cut EdgingFor the simplest type of border, choose spade-cut edging. Spade-cut edging involves digging a narrow trench around the outside of the bed you are setting apart. This elegant edging was once used in the renowned English gardens. Spade-cut edging works well in most yards, regardless of the slope or the size of the area. Occasional maintenance involves weed removal and keeping soil out of the trench.\n• Use a flat spade with a straight cutting edge to get the job done\n• Spade-cut edging is the least expensive type of edging available\n• Keep the spade's cutting blade sharp for a crisper edge with less effort\n• Use leftover soil in your compost pile or in the bed you are edging\n• To keep the edge looking fresh, recut once or twice per season\nStrip EdgingStrip edging consists of a shallow barrier that is anchored beneath the ground. The very top part of the edging is visible to subtly set the bed and lawn apart. Strip edging works best for creating curves and comes in plastic and metal varieties. Plastic is less expensive and easier to install.\nMetal edging comes in steel or aluminum and lasts longer but is less pliable. In cold climates, where the ground heaves regularly due to frost, strip edging can dislodge so you may need to occasionally reinstall it.\n• Plastic edging is pliable and won't usually crack in heat or cold\n• Plastic edging is more resistant to ground heaves than metal\n• Metal edging may be painted and is often used by professionals\n• Compared to steel edging, aluminum is lighter and does not rust\n• Metal edging is more expensive than plastic but lasts a long time\nMasonry EdgingMasonry edging, composed of stone, brick or concrete, is the most expensive type. Stone is very attractive and allows you to match borders to any existing stonework you have used in the landscaping, garden or exterior of your home. You can stand brick on end or lay it flat so it's easier to mow over, or you can mortar the bricks for a traditional look. Cement or brick pavers are less expensive. Cement borders often come in preformed sections of different shapes and styles, allowing for easier installation.\n• Weeds can grow quickly between stone and brick edging\n• Edging made from stone, brick or concrete is extremely durable\n• Interlocking pavers provide a tight fit without needing mortar\n• Brick is available in a range of textures and colors\n• You can create patterns and curves with concrete edger's\nWood EdgingWood edging comes in precut sections of alternating heights, either as round logs or flat boards. This type of edging is usually either pre stained or pressure treated for convenience. You can also use large landscape timbers or railroad ties. All types of wood edging are durable and most are affordable. The types of wood most often used include cedar, cypress and redwood, which resist rot naturally when lying next to soil.\nIf these types of wood are too expensive, opt for pressure-treated wood, which stands up well to rot. Try landscaping timbers in an area where the border must resist pressure from surrounding soil, such as on a hill.\n• Wood edging is relatively inexpensive and is very durable\n• Pressure-treated wood will not harm soil or plants\n• Wood edging is sold as 1–x 4–inch or 1– x 6–inch boards, set on edge in the soil\n• Use timbers to outline straight beds or stack to create a raised bed\n• Timbers come in a variety of cross-sectional lengths and dimensions\nInstallationFor tips on how to install the various types of edging that are available, consult the chart below.\n|Masonry: Brick||• Dig a trench that will allow the amount of brick you want showing to be seen\n• For vertical edging, set the brick edge-to-edge in the trench\n• For horizontal edging, lay the bricks on a sand base to cushion them and\nprotect from frost heave\n• Keep top faces flush with soil surface and add or take sand away to allow for\nvariations in thickness\n• Push soil up against bricks\n• Sweep sand into gaps between bricks to add stability\n|Masonry: Stone||• Dig a trench that will allow the amount of stone you want to show\n• Drive two stakes to define the area you are filling and run a mason's line of\nstring between to define the area\n• After mixing concrete, shovel into a 3 -foot stretch and smooth out\n• Set stones into concrete\n• Level the tops to match the masonry line using a rubber mallet\n• When all stones are in place, push concrete about 6-inches up the back side\nof each and trowel smooth at a 45 degree angle\n• For corners, keep stone faces tight to each other and always end a row with a\n• Repeat steps for the rest of the border\n|Spade-Cut||• Outline the area with rope, a garden hose, lime or other material\n• Dig a trench that's 3-inch to 4–inch deep\n• Keep the lawn edge vertical and angle the inside of the trench toward the\n• For loose soil, angle the spade rather than cutting straight up and down\n• After cutting the perimeter, rake the trench and pull leftover soil up into the\n|Strip: Metal||• Outline the area with rope, a garden hose, lime or other material\n• If soil is soft, lay edging along the bed's border and, using a board to muffle\nthe blow, tap into place with a hammer\n• If soil is hard, use a spade to dig a shallow 4-inch trench around the bed's\n• Position the top edge of the metal at soil level\n• Drive enclosed stakes through premade holes in the strips or by driving long,\nbent spikes over the strips to keep edging in place\n• On the garden side, rake soil against the edging, keeping it a bit lower than""]"	['<urn:uuid:634ec538-84b7-454e-ba0f-0fbc6e0c39c4>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T05:28:56.360010	12	99	1288
61	pinus pinaster characteristics and south africa water impact	Pinus pinaster is a medium-size tree reaching 20-35m tall with orange-red bark and long bluish-green needles up to 25cm. In South Africa, it significantly reduces water resources - it has caused an 82% reduction in streamflow in KwaZulu-Natal Drakensberg after 20 years, and in Mpumalanga Province, 6 streams completely dried up 12 years after grasslands were replaced with pines. Currently, 60% of Western Cape's water catchments are covered by invasive alien vegetation, consuming up to 50% of the region's river run-off.	"['P. pinaster is a hard, fast growing pine containing small seeds with large wings. P. pinaster is located in the Mediterranean basin but has been invading South Africa for the last 150 years where it is an invasive species. It favors a Mediterranean climate, which is one that has cool, rainy winters and hot, dry summers. The pine tree species invades large areas and more specifically fynbos vegetation. Fynbos vegetation is a fire-prone shrubland vegetation that is found in the southern and southwest cape of South Africa. It is found in greater abundance close to watercourses. Dispersal, habitat loss, and fecundity are all factors that affect spread rate. The species favors acidic soils with medium to high-density vegetation.\nThe native area that this species of pinus, Pinus Pinaster, originates from is the Mediterranean Basin, i.e. Northern Tunisia, Algeria, and Morocco. P. pinaster is a popular topic in ecology because of its problematic growth and spread in South Africa for the past 150 years after being imported into the region at the end of the 17th century (1685-1693). It was also found spreading in Cape Peninsula by 1772. Towards the end of the 18th century (1780) P. pinaster was widely planted, and at the beginning of the 19th century (1825-1830) P. pinaster was planted commercially as a timber resource and for the forestry industry.\nThe range extends from Portugal and Northern Spain (especially in Galicia) to southern and Western France, east to western Italy, and south to northern Morocco, with small outlying populations in Algeria and Malta (introduced by man). It generally occurs at low to moderate altitudes, mostly from sea level to 600 m, but up to 2000 m in the south of its range in Morocco.\nPinus pinaster is a medium-size tree, reaching 20–35 m tall and with a trunk diameter of up to 1.2 m, exceptionally 1.8 m.\nThe bark is orange-red, thick and deeply fissured at the base of the trunk, somewhat thinner in the upper crown.\nThe leaves (\'needles\') are in pairs, very stout (2 mm broad), up to 25 cm long, and bluish-green to distinctly yellowish-green. The maritime pine features the longest and most robust needles of all European pine species.\nThe cones are conic, 10–20 cm long and 4–6 cm broad at the base when closed, green at first, ripening glossy red-brown when 24 months old. They open slowly over the next few years, or after being heated by a forest fire, to release the seeds, opening to 8–12 cm broad.\nThe seeds are 8–10 mm long, with a 20–25 mm wing, and are wind-dispersed.\nMaritime pine is closely related to Turkish pine, Canary Island pine and Aleppo pine, which all share many features with it. It is a relatively non-variable species, with constant morphology over the entire range.\nPinus pinaster is widely planted for timber in its native area, being one of the most important trees in forestry in France, Spain and Portugal. Landes forest in southwest France is the largest man-made maritime pine forest in Europe.\nIn addition to industrial uses, maritime pine is also a popular ornamental tree, often planted in parks and gardens in areas with warm temperate climates. It has become naturalised in parts of southern England, South Africa and Australia.\nIt is also used as a source of flavonoids, catechins, proanthocyanidins, and phenolic acids; notably in the patented extract ""Pycnogenol"". Pycnogenol is marketed as a dietary supplement with claims it can treat many conditions; however it is not effective for treating any chronic health disorder.\nResult of P. pinaster invasion\nP. pinaster is a successful invasive species in South Africa. One of the results of its invasion in South Africa is a decrease in the biodiversity of the native environment. The increase of extinction rates of the native species is correlated with the introduction of these species to South Africa. Invasive species occupy habitats of native species often forcing them to extinction or endangerment. For example, invasive species have the potential to decrease the diversity of native plants by 50-86% in the Cape Peninsula of South Africa. As discussed above, P. pinaster is a tree species found in shrubland in South Africa. When compared to other environment, shrublands have the largest decline of species richness when invaded by an invasive species (Z=-1.33, p<0.001). Compared to graminoids; trees, annual herbs, and creepers have a larger effect on decline of species richness (Z=-3.78; p<0.001). Lastly, compared to other countries, South Africa had the largest species richness decline when faced with invasive species. South Africa is not home to many insects and diseases that limit the population of P. pinaster back in its native habitat. Not only is there evidence that alien plant invasions decrease biodiversity, but there is also evidence that the location of P. pinaster increases its negative effect on the species richness.\nIn addition, depending on the regions P.pinaster invades, P. pinaster has the potential to dramatically alter the quantity of water in the environment. If P. pinaster invades an area covered with grasses and shrubs, the water level of the streams in this area would lower significantly because P. pinaster are evergreen trees that take up considerably more water than grasses and shrubs all year around. They deplete run-off in catchment areas and water flow in rivers. This depletes the resources available for other species in the environment. P. pinaster tends to grow rapidly in riparian zones, which are areas with abundant water where trees and plants grow twice as fast and invade. P. pinaster takes advantage of the water available and consequently reduces the amount of water in the area available for other species. The fynbos catchments on the Western Cape of South Africa are a habitat negatively affected by P. pinaster. 23 years after planting the pines there was a 55% decrease in streamflow in this area. Similarly, in KwaZulu-Natal Drakensberg there was an 82% reduction in streamflow 20 years after introducing P. pinaster to the area. In the Mpumalanga Province, 6 streams completely dried up 12 years after grasslands were replaced with pines. To reinforce that there is a negative effect from the invasive species P. pinaster, these areas of dense P. pinaster were thinned and the number of trees in the area decreased. As a result, the streamflow in the fynbos catchments of the Western Cape increased by 44%. The streamflow in the Mpumalanga Province increased by 120%. As a result of P. pinaster growth, there is often less understory vegetation for livestock grazing. Once again there was a positive effect when some of the pines were removed and agreeable range grasses were planted. The grazing conditions for the sheep of the area were greatly improved when the P. pinaster plantation was thinned to 300 trees ha-1. The invasion of P. pinaster leads to the decrease of understory vegetation and therefore a decrease in livestock.\nEcological interaction that lead to the current impact on the environment\nP. pinaster is particularly successful in regions with fynbos vegetation because it is adapted to high intensity fires, thus allowing it to outcompete other species that are not as well adapted to high intensity fires. In areas of fire-prone shrubland the cones of P. pinaster will release seeds when in a relatively high temperature environment for germination as a recovery mechanism. This adaptation increases the competitive ability of P. pinaster amongst other species in the fire-prone shrubland. In a 3 year observational study done in Northwestern Spain, P. pinaster showed a naturally high regeneration rate. Observations showed a mean of 25.25 seedlings/m2 within the first year and then slowly decreased the next two years due to intraspecific competition. So not only does P. pinaster compete with other species, they also compete within their own species as well. When the height of P. pinaster increased there was a negative correlation with the number of P. pinaster seedlings, results showed a decrease in P. pinaster seedlings (r=-0.41, p<0.05).\nSeveral other characteristics contribute to their success in the regions they have invaded, including their ability to grow rapidly and to produce small seeds with large wings. Their ability to grow quickly with short juvenile periods allows them to outcompete many native species while their small seeds aids in their dispersal. The small seeds with large wings are beneficial for wind dispersal, which is the key to reaching new areas in regions with fynbos vegetation. Vertebrate seed dispersers are not commonly found in mountain fynbos vegetation, therefore those species that require the aid of vertebrate dispersal would be at a disadvantage in such an environment. For this reason, the small seed, low seed wing loading, and high winds found in mountainous regions all combine to provide a favorable situation for the dispersal of P. pinaster seeds. Without this efficient dispersal strategy, P. pinaster would not have been able to reach and invade areas, such as South Africa, that are suitable for its growth. Its dispersal ability is one of the key factors that have allowed P. pinaster to become such a successful invasive species.\nIn addition to being an efficient disperser, P. pinaster is known to produce oleoresins, such as oily terpenes or fatty acids, which can inhibit other species within the community from growing. These resins are produced as a defense mechanism against insect predators, such as the large pine weevil. According to an experiment done in Spain, the resin canal density was twice as high in the P. pinaster seedlings attacked by the weevils compared to the unattacked seedlings. Since P. pinaster has the ability to regulate their production of defense mechanisms, it can protect itself from predatory in an energy-efficient manner. The resins make the P. pinaster less vulnerable to damage from insects, but they are only produced in high concentrations when P. pinaster is under attack. In other words, P. pinaster does not waste energy producing resins in safe conditions, so the conserved energy can be used for growth or reproduction. These characteristics enhance the ability of P. pinaster survive and flourish in the areas it invades. Both the traits of P. pinaster and the habitat in South Africa are conducive to the success of P. pinaster in this region of the world.\nBiological control options\nInsects and mites that feed on the seeds and cones of P. pinaster can be effective biological control options. An insect or mite that acts as an ideal biological control should have a high reproductive rate and be host-specific, meaning that it preys specifically on P. pinaster. The life cycle of the predator should also match that of its specific host. Two key characteristics the predator should also exhibit are self-limitation and the ability to survive in the presence of a declining prey population. Seed feeding insects are an effective control because they have high reproductive rates and target the seeds without diminishing the positive effect of the plant on the environment. Controlling the spread of P. pinaster seeds in the region is the key to limiting the growth and spread of this species because P. pinaster has the ability to produce a large number of seeds that are capable of dispersing very efficiently. One possible option is Trisetacus, an eriophyid mite. The main advantage to using this mite to control the population of P. Pinaster is its specificity to P. pinaster; it can effectively control the population of P. pinaster by destroying the growing conelets in P. pinaster while limiting its impact to only this species. Another possible option is Pissodes validirostris, a cone-feeding weevil that lays eggs in developing cones. When the larvae hatch, they feed on the growing seed tissue, preventing P. pinaster seeds from forming and dispersing. Although the adults feed on the trees as well, they do not do any damage to the seeds and only feed on the shoots of the tree, so they do not appear to negatively impact the growth of the trees. Different forms of P. validirostris have diverged to become host-specific to different pine trees. The type of P. validirostris that originated from Portugal appears to have specialized to P. pinaster; therefore, this insect may be used in the future to control the spread of P. pinaster in South Africa. The uncertainties regarding the host-specificity of different types of P. validirostris, however, require more research to be completed before the introduction of the weevils into South Africa. An introduction of a species that is not host-specific to P. pinaster can lead to detrimental effects on both the environment and commercial industries that are dependent on certain tree species. Two other biological control possibilities include the pyralid moth species Dioryctria mendasella and D. mitatella, but these species attack the vegetative tissue instead of just the seeds of P. pinaster, harming the plant itself. As of now, the eriophyid mite and cone-feeding weevil seem to hold the most potential to controlling the spread of P. pinaster in the regions it has invaded because they destroy the reproductive structures of the target invasive species.\n- Farjon, A. (2011). ""Pinus pinaster"". IUCN Red List of Threatened Species. Version 2014.3. International Union for Conservation of Nature. Retrieved 15 February 2015.\n- ""Pinus pinaster Aiton"". Germplasm Resources Information Network (GRIN) online database. Retrieved 4 November 2013.\n- Richardson, M (1990). Assessing the risk of invasive success in Pinus and Banksia in South African mountain fynbos [Journal of Vegetation Science] (1st ed.). pp. 629–642.\n- Moran, V.C. (2000). Biological Control of Alien, Invasive Pine Trees (Pinus species) in South Africa [X International Symposium on Biological Control of Weeds]. pp. 941–953.\n- Rushforth, Keith (1986) . Bäume [Pocket Guide to Trees] (in German) (2nd ed.). Bern: Hallwag AG. p. 63. ISBN 3-444-70130-6.\n- ""Pinus pinaster"". Royal Horticultural Society. Retrieved 23 July 2013.\n- Schoonees, A; Visser, J; Musekiwa, A; Volmink, J (2012). ""Pycnogenol® (extract of French maritime pine bark) for the treatment of chronic disorders® for the treatment of chronic disorders"". Cochrane Database of Systematic Reviews (7). doi:10.1002/14651858.CD008294.pub4.\n- Higgins, S (1999). Predicting the Landscape-Scale Distribution of Alien Plants and Their Threat to Plant Diversity [Conservation Biology]. pp. 303–313.\n- Gaertner, M (2009). Impacts of alien plant invasions on species richness in Mediterranean-type ecosystems: a meta-analysis [Progress in Physical Geography] (33 ed.). pp. 319–338.\n- Carbon, B.A. (1982). Deep drainage and water use of forests and pastures grown on deep sands in a Mediterranean environment [Journal of Hydrology]. pp. 53–63.\n- Papanastasis, V. (1995). Effects of thinning, fertilisation and sheep grazing on the understory vegetarion of Pinus pinaster plantations [School of Forestry and Natural Environment]. pp. 181–189.\n- Calvo, L (2008). Post-fire natural regeneration of a Pinus pinaster forest in NW Spain [Plant Ecology] 197. pp. 81–90.\n- Calvo, L (2003). Regeneration after wildfire in communities dominated by Pinus pinaster, an obligate seeder, and in others dominated by Quercus pyrenaica, typical resprouter [Forest Ecology and Management] 184. pp. 209–223.\n- Krebs, C (2009). Ecology [Pearson].\n- Hoffmann, J (2011). Prospects for the biological control of invasive Pinus species (Pinaceae) in South Africa [African Entomology]. pp. 393–401.\n|Look up maritime pine in Wiktionary, the free dictionary.|\n|Wikimedia Commons has media related to Pinus pinaster.|', 'CapeNature removes thirsty invaders from 119 000ha\nCapeNature has succeeded in removing invasive alien plants, considered to be the single biggest threat to the biodiversity of the Western Cape from 119 000 hectares of priority conservation land across the province, over its last financial year and said that the organisation has exceeding its own targets (of clearing invasive plants) by almost 30%.\n“The organisation has deployed an army of 1000 workers led by 92 competent contractors to remove invasive alien plants. They worked in nature reserves, mountain catchments including river riparian areas, wetlands and marshlands as well as private land with high biodiversity value across the Western Cape, over the last twelve months” says CapeNature Executive Director: Operations, Fanie Bekker.\nDuring clearing operations contractors and their teams cut the invasive alien plants down and apply herbicide to prevent it from growing again. Regular follow up clearing operations reduces infestations and further spread of invader plants, Bekker explained.\n“Invasive alien plants compete for growing space with our own indigenous plants, known as fynbos and impacts negatively on the quality of ecosystem services that our biodiversity delivers.”\nMost of the aliens consume more water than indigenous plants and are also depleting the valuable underground water resources.\nThe current reduction in water yield due to alien plants is estimated at 85 million cubic metres per annum and scientific predictions are that this may increase to more than 340 million cubic meters in years to come, if the current rate of infestation levels are not controlled, Bekker said.\nLarge stands of dense alien plants also provide a high fuel load that can cause exceptionally hot fires. These fires affect the makeup of the soil, damaging its structure and causing high incidence of flooding during the wet winter months.\nInvasive alien plants have infested over 10 million hectares of South Africa. The Western Cape is the most heavily invaded province, at around a third of the total national area. Some of the most common invaders in the province include Pines, Hakeas and Black Wattle.\nBekker confirmed that an estimated 60% of the Western Cape’s water catchment’s is covered by invasive alien vegetation – consuming up to 50% of the region’s river run-off which supply water to our major storage dams.\nBy removing invasive alien plants that use up valuable water, compete with indigenous plants and damage the soil, CapeNature aims to ensure that there will be enough water for everyone and that land will become productive again. At the same time we are also creating business and job opportunities by removing alien invasives and facilitating the usage of cleared plants for furniture making.\nA BIOSPHERE RESERVE is a specific type of conservation area which accommodates and benefits both the natural environment and the communities living in and around it.\nCAPTION TO PHOTOGRAPH: Kogelberg Biosphere, a UNESCO recognised biosphere reserve, close to Betty’s Bay is hailed as the finest example of mountain fynbos in the Western Cape with 0,1% alien infestation due to repeated clearing efforts from CapeNature.']"	['<urn:uuid:d0f42e6b-91a0-4cf1-9fa6-6d5821637786>', '<urn:uuid:23bb83d4-eaa7-40ae-83e4-398a2257bd4f>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T05:28:56.360010	8	81	3016
62	comparing timing controls in fasting and photography explain how both work	In intermittent fasting, timing is crucial as it involves specific periods of not eating (such as 16-hour breaks or two-day fasts) to trigger changes in gene expression and metabolic health. Similarly, in photography, timing controls through shutter speed are essential - the optimal setting is typically double the frame rate (e.g., 1/48 second for 24fps). If the shutter speed is too high, it creates a stuttered effect and exaggerates rolling shutter (jello effect); if too slow, it results in excessive motion blur. Just as incorrect fasting timing can reduce benefits, improper shutter timing can degrade image quality. Both systems require precise timing control to achieve optimal results.	"[""Original Thread DJI Forum|X5-how to get the most out of it --------------------------- The X5 brings the Inspire 1 further into the world of professional photography, where you can no longer point and shoot. With this additional power there are now more ways to create a terrible image than a good one so you need to know what you're working with. This is frustrating because when you spend a lot of money you expect all your results will be superior but this isn’t the case. You will take many shots that look horrible compared to the X3, making some people feel they received defective cameras. The most significant improvement is interchangable superior lenses. A better lens gives you much more power which does allow for much much better results, but just as it’s relatively easy to drive a car but impossible for most of us to drive a 747, you need to know how to wield the new power or you will crash and burn. The X3 is pretty easy to use as you just point it in the right direction and as long as the image isn’t grossly over or underexposed you are going to get a decent image This is because three of the main adjustable components, focal length, focus and aperture, are fixed. So what are focal lengths? Easiest way to understand them is how “zoomed in” or “zoomed out” the lens is. Measured in millimetres, it determines the field of view the lens sees. Wide lenses, such as the X3, show a great deal in the frame where long lenses are quite tight. The focal length also determines the character of the image. Wide lenses create an almost bending quality where you feel like you are close to an object but can somehow see more than you would expect. Long lenses are the opposite, they feel more intimate and straight, making you feel like your subject is the only thing in the world. This goes far beyond what is physically shown in the frame, it affects how you perceive it as well. Another thing different focal lengths affect is depth of field, which is basically how much of the image is in focus. With a shallow depth of field your subject will be in focus but everything in front or behind it will be increasingly out of focus. The longer your lens, the shallower the depth of field. Take the X3 for instance. It has a very wide focal length which makes everything sharp, regardless of where it is in the frame and is so wide it completely removes the requirement to focus at all. But with the X5, longer focal lengths will require much more attention to these aspects, as focus will become a very real issue. So choosing the right focal length for a shot and shooting conditions (single or dual control) is crucial. While speaking of depth of field, the second aspect the X5 allows for is variable aperture. Aperture refers to the iris of the lens itself. This works the same way as your eye. If there is too much light the iris of your eye will close, and in low light it opens very wide. But light isn’t the only thing the aperture controls, it also affects depth of field. When you open the aperture by reducing the number (f/1.7) it also reduces the depth of field. When you close the aperture (f/8 or f/16) it expands the depth of field, sometimes to infinity. Think of squinting your eyes to make things clearer, it’s the exact same thing. Okay, so we’re getting there. Focal length determines how tight or wide the image is, along with adding character, and aperture controls how much light comes in, and both affect depth of field, which is how much of your image is in focus. So let’s review and add it to what we already know. Shutter speed is of course how many times per second the shutter opens and closes per second. This is another way to control light but the shutter has a big effect on the final image. Put the shutter to high and you will get a stuttered effect where every frame is ultra sharp. A high shutter will also exaggerate rolling shutter known as the jello effect. Too slow a shutter will give a lot of motion blur and aesthetically look like cheap video. In feature films, the shutter speed of choice is exactly double the frame rate. This is known as a 180° shutter as film cameras used a shutter that was literally measured in the circular degrees the shutter obstructed. So if you are shooting 24 frames per second the shutter speed would be 48. With the X3, since we couldn’t control aperture, it’s fixed at f/2.8, if you were already at 100IS) (lowest light sensitivity) the only other way to control light without changing the shutter was to add ND filters. The ND filters act like sunglasses and reduce the light, allowing you to get the shutter close to the correct number. But with the X5 we can now control the aperture, so we can just raise it up to balance the shutter. So instead of f/2.8 we can raise it f/16 or even f/22 depending on the lens. But as you read above, this will also affect your depth of field so you need to be careful you are getting the right effect. It might be that you are shooting on a bright day but still want a shallow depth of field. In this case you would still put an ND filter on, set the shutter to double your frame rate, and set the aperture to a low number like f/2.8 or f/1.7. So as you can start to see, your perfect image is a well thought out combination of all these things. Focus. Okay, focus can be a nightmare. It’s the easiest way to ruin a beautiful shot. For this reason someone just starting out is best to use the highest aperture possible to keep the iris closed. In extreme conditions with a low (open) aperture, the depth of field can be as shallow as 1/2 inch. This means if you were filming someone’s face their eyes would be sharp in focus but the tip of their nose would out of focus. Imagine trying to determine what’s in focus using an iPhone screen while your Inspire is 1/2 mile away! The challenges are massive and expect to make some big mistakes in the early days. You will quickly learn how the lens reacts and where infinity sits. But in general, the larger the screen you are using to determine focus the better. Finally, I just wanted to touch on the difference in sensor size. Many are surprised to hear and baffled at the concept that sensor size also affects depth of field. Crazy right? The bigger the sensor you have the shallower the depth of field. To illustrate this let’s compare sensor sizes of the X3 and X5. The X3 uses a 6.17 x 4.55mm sensor and the X5 is 3x larger, 17.3x13.00mm so you will need a much longer lens to cover the same field of view. The longer the lens the shallower the depth of field and voilà! Feel free to ask for clarification for any of this. Again, it's more general than X5 specific."", 'Intermittent fasting (IF) seems to have taken the health and wellness industry by storm. However you define it, IF essentially means you don’t eat for a period of time (18 hours, a full day), and then you eat.\nWhat is interesting about IF is that it can change the gene expression in different tissues in the body. Something as simple as ‘not eating’ can cause an upregulation of proteins associated with longevity.\nThis article digs into the recent research on intermittent fasting, focusing on how it changes gene expression.\nWhat does science say about intermittent fasting?\nIt is easy to get caught up in the hype from the experiences people share regarding their new favorite diet or lifestyle hack. But the proof is in the pudding – or rather not eating the pudding? (I have no idea where that idiom comes from!)\nThe latest research on intermittent fasting shows several interesting things:\nIntermittent fasting may improve metabolism, in part, by changing the gut microbiome.[ref] The microbes in your gut are so crucial to your overall health, and balancing out the good and not-so-good bugs can help with your overall metabolism.\nIF also has decreased plasma insulin levels in a clinical trial.[ref] With so much of the population falling into the pre-diabetes category, this could be a great way for many to prevent diabetes.\nDefining intermittent fasting:\nThere are many ways that researchers define intermittent fasting – from a 16-hour break in eating to a two-day-long fast. Additionally, there are ways to mimic the effects of fasting through restricting protein intake as well as benefits from time-restricted eating.\nHere is a graphical overview (Creative Commons license) from a recent Frontiers in Genetics article.\nAll of these methods of restricting food intake can affect gene expression and metabolic health.\nIntermittent fasting and gene expression:\nDigging into the ‘why’ and ‘how’ for intermittent fasting shows it changes the expression of many different genes. Essentially, your cells respond to the lack of nutrients by turning on and off different genes.\nWhy should you care about the changes in gene expression? Weight loss is one of the obvious effects of intermittent fasting. However, you may consider how the intervention affects specific biological pathways for your objectives.\nInflammation: A trial of intermittent fasting for four months in a mouse stroke model showed the expression of several inflammatory proteins (NLRP1, NLRP2, IL-1B, and IL-18) were decreased.[ref]\nLongevity and health: The sirtuins are a family of genes acting as metabolic sensors of nutrient availability. Low levels of nutrients trigger specific SIRT genes to be expressed, which is linked to healthy longevity.[ref] (Read more about your SIRT gene variants)\nAnimal studies also show that decreasing calories and/or protein restriction will increase AMPK and decrease mTORC1, two proteins important for overall energy-sensing.[ref] Increasing AMPK is important for burning fat and making new mitochondria.[ref]\nBrain health: Animal studies of intermittent fasting also show that BDNF (brain-derived neurotrophic factor) also increases.[ref] BDNF is essential for cognitive function, mood, and weight management. (Read more about your BDNF gene variants)\nSo what do human research studies show us about gene expression?\n- A three-week-long intermittent fasting clinical trial showed a slight increase (~3%) in SIRT3.[ref]\n- A fasting and refeeding trial of healthy adults showed that fasting decreased the NLRP3 inflammasome activation. Additionally, inflammatory gene expression for NF-κB, TNF, and IL1B was lower during fasting and higher after refeeding.[ref]\n- A clinical trial comparing fasting in obese vs. normal-weight participants found several differences between the two groups. For example, AMPK activity was reduced in lean individuals, but no change in obese people. Additionally, the shift to burning fat for fuel was blunted in people who were obese.[ref]\n- A clinical trial of early time-restricted eating (eating only between 8 am and 2 pm) showed SIRT1 gene expression was upregulated, as was the autophagy gene LC3A. Additionally, it increased BDNF expression in the evening.[ref]\nNot all clinical trials on intermittent fasting show amazing results. A recent 8-week-long trial in healthy adults showed no significant differences between IF and a control group other than a little bit of weight loss in the IF group. The researchers looked at a number of different parameters, including BDNF, liver enzymes, blood pressure, mood, etc.[ref]\nAnother clinical trial in obese women found the markers of inflammation (TNFα, IL6, and IL10) were not changed from intermittent fasting or daily calorie restriction. In fact, inflammatory response increased in adipose tissue, possibly due to lipolysis.[ref]\n(Check to see if you are likely to have genetically higher TNF-alpha)\nWhat is autophagy in fasting?\nIn conjunction with intermittent fasting, a common term that you will see is ‘autophagy’.\nAutophagy is the way the cells can clear out or recycle cellular debris. For example, when oxidative stress damages mitochondria, the cells clear out the damaged mitochondria via autophagy pathways, clearing the way to create new mitochondria.\nIntermittent fasting is one way to promote autophagy.\nPeople with diabetes often damage the insulin-producing beta cells in the pancreas. Animal studies show that intermittent fasting restores autophagic-flux to the beta cells in the pancreas. Clearing out damaged mitochondria and enhancing beta-cell survival is a good thing.[ref]\n(Read more about your autophagy gene variants)\nShould you do intermittent fasting if you are diabetic?\nA recent randomized controlled trial on intermittent fasting in people with type 2 diabetes raises a couple of concerns. While the trial participants did have improvements in their weight, HbA1c, and fasting glucose, there was an increase in the rate of hypoglycemia.[ref]\nIf you have type 2 diabetes, talk with your doctor, and understand the risks of hypoglycemia when intermittent fasting. If your doctor isn’t familiar with the benefits (and risks) of intermittent fasting for type 2 diabetes, you could check out physician groups that specialize in IF for diabetes, such as Virta Health.\n(Check out the free genetic risk report for diabetes)\nThere are definite benefits for weight loss and metabolic health for most people who do intermittent fasting. But it may not be right for everyone, especially if you have problems with low blood sugar. Additionally, the impact on inflammatory markers may not be the same for everyone.\nIf you have medical questions about whether IF is right for you, talk with your doctor.\nRelated Articles and Topics:\nRapamycin, mTOR, and Your Genes\nRapamycin is an antibiotic used as an immunosuppressant, an anti-cancer agent, and to prevent blocked arteries. It is now the focus of longevity and healthspan-extending research through its inhibition of mTOR.\nTelomere Length: How Your Genes Affect Telomeres and Aging\nYour telomeres are the region at the end of each chromosome that keeps your DNA intact when your cells divide. Telomeres that are too short cause cells to stop dividing. It causes some diseases of aging. Genetics plays a role here – along with diet and lifestyle. (Member’s only article)\nPreventing Alzheimer’s Disease\nBillions of dollars have been spent in the last couple of decades on trying to find drugs to stop the tangled accumulation of beta-amyloid plaque without much success. A new direction of research is looking into the ties between circadian rhythm dysfunction and Alzheimer’s disease.\nBlood glucose levels: how your genes impact blood sugar regulation\nGenetics plays a significant role in your blood glucose regulation. Discover your genetic susceptibility to blood sugar problems to help with blood glucose stability.\nAn, Ping, et al. “Role of APOBEC3F Gene Variation in HIV-1 Disease Progression and Pneumocystis Pneumonia.” PLoS Genetics, vol. 12, no. 3, Mar. 2016, p. e1005921. PubMed, https://doi.org/10.1371/journal.pgen.1005921.\nAsif, Shaza, et al. “Understanding Dietary Intervention-Mediated Epigenetic Modifications in Metabolic Diseases.” Frontiers in Genetics, vol. 11, Oct. 2020, p. 590369. PubMed Central, https://doi.org/10.3389/fgene.2020.590369.\nBaik, Sang-Ha, et al. “Intermittent Fasting Increases Adult Hippocampal Neurogenesis.” Brain and Behavior, vol. 10, no. 1, Jan. 2020, p. e01444. PubMed, https://doi.org/10.1002/brb3.1444.\nBogdanova, Natalia, et al. “Hereditary Breast Cancer: Ever More Pieces to the Polygenic Puzzle.” Hereditary Cancer in Clinical Practice, vol. 11, no. 1, Sept. 2013, p. 12. PubMed Central, https://doi.org/10.1186/1897-4287-11-12.\nChen, Zhishan, et al. “Integrative Genomic Analyses of APOBEC-Mutational Signature, Expression and Germline Deletion of APOBEC3 Genes, and Immunogenicity in Multiple Cancer Types.” BMC Medical Genomics, vol. 12, Sept. 2019, p. 131. PubMed Central, https://doi.org/10.1186/s12920-019-0579-3.\nCompaore, Tegwinde Rebeca, et al. “APOBEC3G Variants and Protection against HIV-1 Infection in Burkina Faso.” PloS One, vol. 11, no. 1, 2016, p. e0146386. PubMed, https://doi.org/10.1371/journal.pone.0146386.\nCorley, B. T., et al. “Intermittent Fasting in Type 2 Diabetes Mellitus and the Risk of Hypoglycaemia: A Randomized Controlled Trial.” Diabetic Medicine: A Journal of the British Diabetic Association, vol. 35, no. 5, May 2018, pp. 588–94. PubMed, https://doi.org/10.1111/dme.13595.\nDeng, Ya, et al. “Intermittent Fasting Improves Lipid Metabolism Through Changes in Gut Microbiota in Diet-Induced Obese Mice.” Medical Science Monitor: International Medical Journal of Experimental and Clinical Research, vol. 26, Nov. 2020, p. e926789. PubMed, https://doi.org/10.12659/MSM.926789.\nFann, David Yang-Wei, et al. “Intermittent Fasting Attenuates Inflammasome Activity in Ischemic Stroke.” Experimental Neurology, vol. 257, July 2014, pp. 114–19. PubMed, https://doi.org/10.1016/j.expneurol.2014.04.017.\nHanjani, Nazanin Asghari, and Mohammadreza Vafa. “Protein Restriction, Epigenetic Diet, Intermittent Fasting as New Approaches for Preventing Age-Associated Diseases.” International Journal of Preventive Medicine, vol. 9, June 2018, p. 58. PubMed Central, https://doi.org/10.4103/ijpvm.IJPVM_397_16.\nHe, Xiu-Ting, et al. “Association between Polymorphisms of the APOBEC3G Gene and Chronic Hepatitis B Viral Infection and Hepatitis B Virus-Related Hepatocellular Carcinoma.” World Journal of Gastroenterology, vol. 23, no. 2, Jan. 2017, pp. 232–41. PubMed, https://doi.org/10.3748/wjg.v23.i2.232.\nIqbal, Khurshid, et al. “Correlation of Apolipoprotein B MRNA-Editing Enzyme, Catalytic Polypeptide-like 3G Genetic Variant Rs8177832 with HIV-1 Predisposition in Pakistani Population.” Current HIV Research, vol. 16, no. 4, July 2018, pp. 297–301. PubMed Central, https://doi.org/10.2174/1570162X16666181018155827.\nJamshed, Humaira, et al. “Early Time-Restricted Feeding Improves 24-Hour Glucose Levels and Affects Markers of the Circadian Clock, Aging, and Autophagy in Humans.” Nutrients, vol. 11, no. 6, May 2019, p. E1234. PubMed, https://doi.org/10.3390/nu11061234.\nKessler, Christian S., et al. “A Nonrandomized Controlled Clinical Pilot Trial on 8 Wk of Intermittent Fasting (24 h/Wk).” Nutrition (Burbank, Los Angeles County, Calif.), vol. 46, Feb. 2018, pp. 143-152.e2. PubMed, https://doi.org/10.1016/j.nut.2017.08.004.\nLiu, Bo, et al. “Markers of Adipose Tissue Inflammation Are Transiently Elevated during Intermittent Fasting in Women Who Are Overweight or Obese.” Obesity Research & Clinical Practice, vol. 13, no. 4, Aug. 2019, pp. 408–15. PubMed, https://doi.org/10.1016/j.orcp.2019.07.001.\nLiu, Haiyan, et al. “Intermittent Fasting Preserves Beta-Cell Mass in Obesity-Induced Diabetes via the Autophagy-Lysosome Pathway.” Autophagy, vol. 13, no. 11, Nov. 2017, pp. 1952–68. PubMed Central, https://doi.org/10.1080/15548627.2017.1368596.\nLuo, Yiqiao, et al. “Sulforaphane Inhibits the Expression of Long Noncoding RNA H19 and Its Target APOBEC3G and Thereby Pancreatic Cancer Progression.” Cancers, vol. 13, no. 4, Feb. 2021, p. 827. PubMed Central, https://doi.org/10.3390/cancers13040827.\nMourier, Tobias, et al. “Host-Directed Editing of the SARS-CoV-2 Genome.” Biochemical and Biophysical Research Communications, vol. 538, Jan. 2021, pp. 35–39. PubMed Central, https://doi.org/10.1016/j.bbrc.2020.10.092.\nPark, Charny, et al. “Integrative Molecular Profiling Identifies a Novel Cluster of Estrogen Receptor-Positive Breast Cancer in Very Young Women.” Cancer Science, vol. 110, no. 5, May 2019, pp. 1760–70. PubMed, https://doi.org/10.1111/cas.13982.\nPoulain, Florian, et al. “Footprint of the Host Restriction Factors APOBEC3 on the Genome of Human Viruses.” PLOS Pathogens, vol. 16, no. 8, Aug. 2020, p. e1008718. PLoS Journals, https://doi.org/10.1371/journal.ppat.1008718.\nRobertson, Lauren T., et al. “Protein and Calorie Restriction Contribute Additively to Protection from Renal Ischemia Reperfusion Injury Partly via Leptin Reduction in Male Mice.” The Journal of Nutrition, vol. 145, no. 8, Aug. 2015, pp. 1717–27. PubMed, https://doi.org/10.3945/jn.114.199380.\nSadeghpour, Shiva, et al. “Human APOBEC3 Variations and Viral Infection.” Viruses, vol. 13, no. 7, July 2021, p. 1366. PubMed Central, https://doi.org/10.3390/v13071366.\nSadler, Holly A., et al. “APOBEC3G Contributes to HIV-1 Variation through Sublethal Mutagenesis.” Journal of Virology, vol. 84, no. 14, July 2010, pp. 7396–404. PubMed Central, https://doi.org/10.1128/JVI.00056-10.\nSharma, Shraddha, et al. “APOBEC3A Cytidine Deaminase Induces RNA Editing in Monocytes and Macrophages.” Nature Communications, vol. 6, Apr. 2015, p. 6881. PubMed Central, https://doi.org/10.1038/ncomms7881.\nStavrou, Spyridon, and Susan R. Ross. “APOBEC3 Proteins in Viral Immunity.” Journal of Immunology (Baltimore, Md. : 1950), vol. 195, no. 10, Nov. 2015, pp. 4565–70. PubMed Central, https://doi.org/10.4049/jimmunol.1501504.\nSui, Shuang, et al. “Correlation of APOBEC3G Polymorphism with Human Papillomavirus (HPV) Persistent Infection and Progression of Cervical Lesions.” Medical Science Monitor : International Medical Journal of Experimental and Clinical Research, vol. 25, Sept. 2019, pp. 6990–97. PubMed Central, https://doi.org/10.12659/MSM.916142.\nTraba, Javier, et al. “Fasting and Refeeding Differentially Regulate NLRP3 Inflammasome Activation in Human Subjects.” The Journal of Clinical Investigation, vol. 125, no. 12, pp. 4592–600. PubMed Central, https://doi.org/10.1172/JCI83260. Accessed 3 June 2022.\nWegman, Martin P., et al. “Practicality of Intermittent Fasting in Humans and Its Effect on Oxidative Stress and Genes Related to Aging and Metabolism.” Rejuvenation Research, vol. 18, no. 2, Apr. 2015, pp. 162–72. PubMed Central, https://doi.org/10.1089/rej.2014.1624.\nWijngaarden, Marjolein A., et al. “Effects of Prolonged Fasting on AMPK Signaling, Gene Expression, and Mitochondrial Respiratory Chain Content in Skeletal Muscle from Lean and Obese Individuals.” American Journal of Physiology-Endocrinology and Metabolism, vol. 304, no. 9, May 2013, pp. E1012–21. journals.physiology.org (Atypon), https://doi.org/10.1152/ajpendo.00008.2013.\nZhu, Yueming, et al. “Metabolic Regulation of Sirtuins upon Fasting and the Implication for Cancer.” Current Opinion in Oncology, vol. 25, no. 6, Nov. 2013, pp. 630–36. PubMed Central, https://doi.org/10.1097/01.cco.0000432527.49984.a3.']"	['<urn:uuid:49c42521-15ac-4d68-a920-0f380562c249>', '<urn:uuid:982f1d1e-1a54-4f7a-9bfb-2996c86c43db>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T05:28:56.360010	11	107	3401
63	How can industrial control system defenders use OSINT exercises to improve their security posture, and what are some common Shodan filters that can be used for this purpose?	"OSINT exercises help ICS defenders understand their organization's information attack space without disrupting operations. They reveal Internet-connected devices, remote access services, open ports, and protocols in use. Common Shodan filters include: searching organization IP range (net:x.x.x.x/y), organization name (org:\""name\""), specific IP (x.x.x.x), city (city:\""name of city\""), webpage title (title:\""text here\""), and common remote access ports (port:\""3389\""). After the OSINT exercise, defenders should remove unnecessary Internet-connected assets and ensure secure multi-factor authentication with monitoring for remote access."	"['Are you planning to visit your industrial control system (ICS) or operational technology facilities to advance your control system security program? Use this guide to prepare for discussions on cybersecurity and safety, conduct ethical hacks of the physical security perimeter, and establish an ICS asset inventory for proactive defense and information security.\nConsider the points outlined below to maximize your efforts to identify critical assets during on-site ICS visits, promote ICS security awareness, and facilitate a smooth ICS cyber incident response process.\n1. OSINT for ICS Defenders\nWhile often overlooked, an open-source intelligence (OSINT) exercise provides a starting point to understand an organization’s information attack space, and it does not disrupt or introduce any risk to industrial operations and is not detectable by ICS defenders.\nAn OSINT exercise reveals the Internet-connected devices, remote access services, open ports, and protocols that are in use at an organization. In the hands of an adversary, this information can be pieced together to build an attack against a target.\nICS defenders should at least know what information is publicly available about their organization and operations through common search engines such as Google. They should also know which Internet-connected devices are deployed from search results from Shodan or similar tools. An OSINT exercise shows what adversaries already know, which is critical to building defenses. Common Shodan website filters examples are:\nSearch organization IP range: net:x.x.x.x/y\nSearch by organization name: org:”name”\nSearch by IP : x.x.x.x\nSearch by city: city:”name of city”\nSearch by webpage title: title:”text here”\nSearch for common remote access: port:""3389""\nAfter conducting an OSINT exercise, ensure that Internet-connected assets are removed where feasible and that remote access has secure multi-factor authentication with monitoring and auditing in place. Verify with the key stakeholders and applicable on-site teams before changing anything.\n2. Coordinate with Safety and Security Teams\nEstablish and maintain relationships with fire and safety, physical security, and engineering teams before arriving at the site. These teams know just about everything about the facilities, including the location of physical assets, how to navigate the site, network architecture, and critical assets. You may have to rely on these teams for help throughout the ICS incident response process going forward.\nSite safety is always going to be top of mind, even above cybersecurity. Follow the lead of the safety team and the safety protocols to ensure that you and your team remain physically safe. This means wearing your personal protective equipment (PPE), among other measures. Most sites require that you have completed safety training and show certificates of completion before entering the site.\n3. Ethically Hack the Physical Security Perimeter\nWhen arriving at the site, there’s always an opportunity to audit physical security controls. This can be done by observing authentication processes, starting with the front gate. Wait to show a badge until it is requested, document tailgating observations, and look for unlocked doors, doors being propped open, fences with gaps, etc. – all while keeping safety as the highest priority. Conduct passive wireless sweeps looking for rogue access points and/or unsecured wireless settings.\nAlways seek documented approval from management for ethical hacking exercises of this nature before attempting them.\n4. Plant Floor Cybersecurity Discussions\nOrganizing direct discussions on security and safety at the facility allows for direct observations and provides operational context for the environment where digital assets are located. However, some operating environments may have prohibitive noise, safety or access limitations that make it necessary to hold these discussions elsewhere. The discussions should include process engineers, field technicians, programmers, operators, and managers. Cybersecurity staff need to know how the physical processes and the plant operates, and which systems are critical to operations and safety. Walk the teams through industry case studies such as CRASHOVERRIDE, TRISIS, HAVEX, STUXNET, etc.\nStart a discussion around what might constitute an impactful event in the environment. The individuals who operate the facilities will certainly have thoughts about what could fail, or even experiences with something that has failed before.\nLeverage the physical engineering safety culture by drawing parallels between physical and cyber safety, and highlight the cyber defense safeguards that are in place to ensure the safety and reliability of engineering operations.\nIn security awareness memos, replace cyber “security” with “cyber “safety.”\n5. Spreadsheet, Laptop Stand, and Network Diagrams\nStart by reviewing network diagrams. Use an encrypted laptop with at least a basic spreadsheet application to start storing your ICS asset information. At a minimum, capture the following attributes for commonly targeted assets such as Data Historian, HMI, PLCs, engineering workstations, core network devices, and Safety Instrumented Systems (SIS):\n- Site name, location, facility type\n- Asset type and ID tag\n- Asset location: Room, cabinet, rack\n- Description of asset function\nImpact to operations if unavailable\n- IP and MAC address\n- Operating ICS protocols\n- Model/manufacturer, serial number\n- Firmware version\n- Applications installed and versions\n6. Follow Up with Traffic Analysis\nMaintenance windows and safety risks can prevent the physical inspection of certain assets. Augment the physical inspection inventory with passive network traffic capture. This will require coordination and approval from operations staff and a configured SPAN with a network security monitoring platform such as Security Onion. Common capture times range from 2 to 24 hours. Identify critical assets through packet analysis and by observing ICS protocol traffic patterns. Use features in free tools like Wireshark to help:\nWireshark > Statistics > Endpoints\nWireshark > Statistics > Protocol\nWireshark > Statistics > Conversations\n7. Storing Asset Inventory Back at the Office\nThe asset inventory is incredibly valuable. When back at the office, store inventory updates in a database that is:\nScalable - Scalable databases help ensure that site inventories can be updated or expanded; back them up regularly.\nSearchable - All fields should be indexed to enable quick searches across inventories gathered when used in conjunction with threat intelligence or vulnerability information.\nSecure - Standard data protection and security practices, including authentication and network segmentation, should be used to protect this sensitive data.\n8. Asset Inventory for ICS Defense\nUse threat intel to drive searches across an established inventory database for vulnerabilities and targeted assets for proactive defense changes. Targeted assets include the following:\nData Historian – This is a database that stores operational process records. It can be abused to pivot from a compromised asset in IT to one in the ICS network(s).\nEngineering Workstation – This workstation has access to software to program and change programmable logic controllers and other field device settings/configurations.\nHuman Machine Interface - The HMI is a visual interface between the physical process and operators that is used to review and control the process.\nProgrammable Logic Controllers – PLCs connect the physical hardware in the real world and run logic code to read the state or change the state of the engineered process.\nCheck out the SANS ICS Cheat Sheets and Other Free ICS Resources here!\nJoin the SANS ICS Community Forum for Tips, Tricks, and a Q&A to secure your ICS!\nLearn more about Dean Parsons’ ICS contributions and check out his bio here!\nDean Parsons’ Upcoming ICS515 teaches here:\n- SANS Paris June 2021 Online | June 14 - 19 | Register Today\n- SANSFIRE 2021 Online | July 12 - 17 | Register Today']"	['<urn:uuid:9db8a258-d143-4105-bdee-1cbdd5a87a43>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T05:28:56.360010	28	76	1208
64	What sparked WW1 and how did Austria and Serbia react?	The assassination of Austrian Archduke Franz Ferdinand and his wife Sophie by Gavrilo Princip of the Black Hand, a Serbian nationalist group, sparked World War I. In response, Austria issued an ultimatum to Serbia on July 23, 1914. While Serbia accepted some terms and requested to discuss others, Austria rejected Serbia's offer and declared war. The origins of these tensions lay not so much in Austrian aggression as in the decline of the Ottoman Empire, though Austrian ambitions eventually played a role.	"[""The path that led to the Eastern front of World War I was complex and long. Imperial ambitions, coupled with ethnic and economic problems, contributed to the start of the conflict. A series of regional crises preceded the war.\n1 of 5\nTo Austria-Hungary in particular, Germany seemed unable to follow a steady course. This was partly due to the fact that Germany continued both to affirm the alliance and yet at the same time undercut Austria-Hungary's economic position in the Balkans. Furthermore, Germany's efforts could as often reflect dynastic sympathies — there were Hohenzollerns on the thrones of Greece and Romania — as Austrian interests.\n2 of 5\nRussian involvement in the Balkans, particularly in Bulgaria and Serbia, was not in harmony with Austrian objectives in the region. In the Balkans, imperial rivalries intersected and overlapped with the cold war of the alliances. The Balkans were also the point where three empires — Russian, Ottoman, and Austro-Hungarian — came face to face with the imminent prospect of their own decline as great powers.\n3 of 5\nRussia was plagued by the spectre of revolution, dragged down by her systematic internal problems and in desperate need of modernization. The Russo-Japanese War had demonstrated that quantity was not enough: there had to be quality too. The Russians needed a well-trained army equipped with modern weapons, a strong naval presence on every coast and a total reorganization of the logistical sinews of war. It was clear however that, given time, Russia would be a valuable ally to France.\n4 of 5\nTurkey was in a dangerous position, since it was nearly bankrupt. It was difficult to see how war could benefit such a country. Certainly Turkey could not afford to be on the losing side: that would surely mark the final dissolution of her tottering Empire.\n5 of 5\nThe origins of the tensions in the Balkans which became the immediate cause of the First World War lie not so much in Austrian aggression — although in time this came to play its part — as in the decline of the Ottoman Empire.\nRussia was the most enigmatic of the Great Powers. Possessed of staggering potential, she remained a fitfully dozing giant. Her land mass was enormous, while her armies seemed inexhaustible, fuelled by a population of some 170 million. Yet Russia was a country only slowly feeling its way into the twentieth century. Although there had been some acceleration in her slow industrialization, she was still by no means a modern state and was deeply reliant on the financial assistance offered by France to develop her infrastructure. Yet Russia was by no means just a tool of the French, and had her own distinct territorial and geopolitical ambitions.\n1 of 4\nAn enduring Russian ambition in foreign policy, although better described as an obsession, lay in securing control of the exit from the Black Sea, via the Bosphorus and the Dardanelles, to the Mediterranean. This aim would ultimately require the conquest of Constantinople and the dissolution of the Ottoman Empire. Several wars had already been triggered by this aggressive intent, most notably the Crimean War of 1854-6 and the Russo-Turkish War of 1877-1878.\n2 of 4\nThe Russian balance of trade, particularly the bulk export of grain, was dependent on its safe passage through the Dardanelles. Government ministers were all too conscious that any closure of the Straits would cause severe economic damage. Russia was naturally extremely concerned about any threatened augmentation of Turkish naval strength in the Black Sea. But there was also a jealous determination to prevent any other country from securing control of the Straits. However, it was considered that, if the power was not held by Russia herself, then better the Turks than some more virile challenger such as Bulgaria or Greece.\n3 of 4\nRussia had also sought to spread out to the east, expanding beyond Central Asia, pressing into Siberia and eventually seeking a port to provide access to the Pacific Ocean. These ambitions led Russia into conflict with Japan, a hitherto little considered nation which had successfully acquired many of the trappings of a modern nation state. In the Russo-Japanese War of 1904-5, the Russians had been badly beaten and forced into a humiliating defeat. This, however, was only a temporary halt to the Russian programme of imperial expansion across borders not shared with a fellow Great Power.\n4 of 4\nIndustry, though growing fast, had come late to Russia, and an industrial working class existed in only a few cities. Peasant discontent over land tenure was matched by industrial unrest over low wages and poor working conditions. But as war approached, patriotic fervor erupted, and when the Tsar appeared in Palace Square, the crowd fell to its knees.\nPath to World War One in the West\nNaval and military technology, colonial rivalries, economic competition and irreconcilable national ambitions were some of the factors that led to the outbreak of the Great War.\nWorld War One - the Spark\nWorld War I was sparked by the assassination of Franz Ferdinand, the Austrian heir to the throne, by a Bosnian Serb. This event triggered an international crisis that led to the outbreak of the Great War.\n- Peter Hart, The Great War: A Combat History of the First World War, Oxford University Press, Oxford, 2013\n- Hew Strachan, The First World War: To Arms (Volume I ), Oxford University Press, Oxford, 2001\n- Peter Simkins, Geoffrey Jukes, Michael Hickey, Hew Strachan, The First World War: The War to End All Wars, Osprey Publishing. Oxford, 2003\n- Sean McMeekin, The Russian Origins of the First World War, The Belknap Press of Harvard University Press, Cambridge, Massachusetts, 2011"", 'June 28, 1914 Archduke of Austria Franz Ferdinand and his wife Sophie are assassinated by Gavrilo Princip of the Black Hand (a Serbian Nationalist group).\nJuly 23, 1914 Austria issues an ultimatum to Serbia. They tell Serbia if they do not meet the terms of the ultimatum there will be war.\nJuly 28, 1914 Serbia accepts some terms of the ultimatum, but would like to conference about other terms. Austria rejects Serbia’s offer and declares war on Serbia. Russia mobilizes their troops along Austria’s border.\nLate Summer 1914 Millions of soldiers happily march off to war, convinced the war will be short.\nFall 1914 Germany’s Schlieffen Plan, which was supposed to be quick turns into a stalemate on the Western Front.\nAugust 1914 Germany crushes Russia’s invading army and forces them to retreat on the Eastern Front.\nSeptember 5, 1914 Allies attacked the Germans northeast of Paris in the valley of the Marne River.\nDecember 1914 Austrians and Germans defeated the Russians and drove them eastward.\nEarly 1915 Opposing armies on the Western Front had dug miles of parallel trenches to protect themselves from enemy fire.\nDecember 1915 Allies give up Galllipoli campaign after 250,000 casualties.\nFebruary 1915 Gallipoli campaign – Britain, Australia, New Zealand and France attacked west side of Straits of Dardenelles near Constantinople.\nJuly 1916 British forces try to help the French by attacking the Germans at the Battle of Somme. They suffer more than half a million casualties.\n1916 Russia is suffering from a lot of political turmoil, and their war effort is on the verge of collapsing.\nJanuary 1917 Germans announce that they will bring back their policy of unrestricted submarine warfare and will sink any ship around British waters without warning.\nFebruary 1917 British intercept the Zimmerman Telegram.\nMarch 1917 Civil unrest in Russia is bringing the Tsar Nicholas’s government to brink of collapse.\nApril 2, 1917 President Wilson asks Congress to declare war on the side of the Allies.\nNovember 1917 Communist leader Lenin seized power in Russia.\nMarch 1918 Treaty of Brest-Litovsk is signed by the German’s and Russian’s. This ends Russia’s involvement in World War I, and all of Germany’s army now heads to the western front.\nJuly 1918 Things were looking bad for the Allies, but then new American troops come over and help push the Germans out of France.\nNovember 11, 1918 Kaiser Wilhelm steps down and a new representative from Germany signs an armistice with the Allies.\nJanuary 1919 Treaty of Versailles conference begins in France. Over the next year the Allies and neutral countries decide how to punish Germany/Central Powers.\nWhat should I study? Vocabulary – look through PowerPoints and book notes to see what key vocab words are – use the unit outline too. Unit Questions Countries in the Allies, Central Powers, Triple Alliance, and Triple Entente. There are no timelines, but you should know the major events of the war and their order. Know and be able to explain the causes of World War I: MAIN (militarism, alliances, imperialism, nationalism) Know and be able to explain what was decided at the Treaty of Versailles conference.']"	['<urn:uuid:c97a8c2f-05a8-448c-8e34-87fd05c2ff97>', '<urn:uuid:bd2d2a45-f7c6-403e-9000-4d2e2b4eb61b>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T05:28:56.360010	10	82	1471
65	major territorial changes land holdings ojibwe ute tribes comparison original territories	Both the Ojibwe and Ute tribes experienced significant changes to their territorial holdings over time. The Ojibwe originally lived around Lake Superior but expanded to control most of present-day Michigan, northern Wisconsin, Minnesota, and parts of North Dakota, along with northern shores of Lakes Huron and Superior in Canada. Unlike other tribes, they were never fully removed from their territories. The Utes, who originally controlled much of the mountain regions, lost significant lands through various treaties and agreements: the 1863 San Luis Valley cession, the 1873 Brunot Agreement ceding the San Juan Mountain area, and the 1880 agreement resulting in the removal of some bands to Utah. By the 20th century, both tribes were restricted to reservation lands within their original territories.	"['The Ojibwa or Chippewa (also Ojibwe, Ojibway, Chippeway) are the third-largest group of Native Americans in the United States, surpassed only by Cherokee and Navajo.\nThe major component group of the Anishinabek, they number over 100,000 living in an area stretching across the north from Michigan to Montana. Another 76,000, in 125 bands, live in Canada. They are known for their canoes and wild rice, and for the fact that they were the only Indian nation to defeat the Sioux.\nBy about A.D. 100, Native American inhabitants of the Upper Peninsula (Ojibwes) were using improved fishing techniques and had adopted the use of ceramics. They gradually developed a way of life based on seasonal fishing which the Chippewas/Ojibwes still followed when they met the first European visitors to the area. Scattered fragments of stone tools and pottery mark the location of some of these prehistoric lakeshore encampments.\nOrganized into independent migratory bands, the Ojibwe were ideally suited to fur trade with the French. They moved according to a seasonal subsistence economy - fishing in the summer, harvesting wild rice in the fall, hunting, trapping, and ice fishing in the winter, and tapping maple syrup and spearfishing in the spring. Their main building material, wiigwaas (birch bark), could be transported anywhere to make a wiigiwam (lodge shelter). Social organization was somewhat egalitarian, and women played a strong economic role.\nThe manufacture of sugar was one of the principal Indian industries, if the term industry can be properly applied to anything existing in an Indian community. They produced large quantities of this article, and of good quality. Having completed its manufacture for the year, they packed it in mokoks (vessels or packages neatly made of birch-bark) and buried it in the ground, where it was kept in good condition for future use or sale. Their sugar-making resources were, of course, almost unlimited, for groves of maple abounded everywhere.\nOnce a year, soon afer sugar-making, nearly all the Indians of the interior repaired to Kepayshowink (the great camping-ground) which was where Saginaw now stands. They went there for the purpose of engaging in a grand jubilee of one or two weeksą duration engaging in dances, games, and feats of strength. Many an inveterate Indian feud reached a bloody termination on the ""great camping ground"" at Saginaw.\nThe Ojibwe language belongs to the Algonquian linguistic group. When first encountered by Europeans in the 17th century, they mostly lived around the shores of Lake Superior. Warring with the Dakota and the Fox, and newly armed by the French, they drove the Fox from northern Wisconsin and pushed the Dakota across the Mississippi. Eventually the Ojibwa reached the Turtle Mountains of North Dakota, and became known as the Plains Ojibwa.\nThe Ojibwa were part of a long term alliance with the Ottawa and Potawatomi First Nations, called the Council of Three Fires and which fought with the Iroquois Confederacy and the Sioux. The Ojibwa expanded eastward taking over the lands alongside the eastern shores of Lake Huron. The Ojibwa allied themselves with the French in the French and Indian War, and with the British in the War of 1812.\nOn July 8, 1822 the Ojibwa turned over a huge tract of land in Ontario to the United Kingdom.\nMost Ojibwa, except for the Plains bands, lived a sedentary lifestyle, engaging in fishing, hunting, the farming of maize and squash, and the harvesting of Manoomin (wild rice). Their typical dwelling was the waaginogan, made of birch bark, juniper bark and willow saplings. They also developed a form of pictorial writing used in religious rites of the Midewin and recorded on birch bark scrolls.\nThe Ojibwe people and culture are alive and growing today. During the summer months, the people attend pow-wows or ""pau waus"" at various reservations in the US and reserves in Canada. Many people still follow the traditional ways of harvesting wild rice, picking berries, hunting and making maple sugar.\nChippewa chief Rocky Boy\nThe Chippewa, are an important group of Native Americans/First Nations about equally divided between the United States and Canada. The popular name is a corruption of Ojibwa, but they call themselves Anishinabek, or original men, and because they formerly had their main residence at Sault Sainte Marie, at the outlet of Lake Superior, the French knew them by the name of Saulteurs.\nThey belong to the great Algonquian stock and are related to the Ottawa and Cree. According to their own tradition, they came from the east, advancing along the Great Lakes, and had their first settlement in their present country at Sault Sainte Marie and Shaugawaumikong (French Chegoimegon) on the southern shore of Lake Superior, near the present Lapointe or Bayfield, Wisconsin.\nThe decline of the fur trade transformed the traditional Ojibwe society. When the British ousted the French from the region, the Ojibwe allied with British traders and soldiers to drive away American settlers. After the US took control of the region, however, the Ojibwe fell on hard economic times. The men took menial jobs in the timber industry, and the role of women weakened. Nevertheless, the bands\' isolation enabled the Ojibwe to preserve much of their religion and cultural traditions through the 19th and into the 20th century.\nStarting about 1640, many Ojibwe moved (or were driven) westward from the Sault Ste. Marie area. Some turned south into the Lower Peninsula, later joining the Odawa (Ottawa) and Potawatomi in the Three Fires (three brothers) Society. Others continued west along the Lake Superior shore and settled on Madeline Island (in Lake Superior) about 1680. The map below shows not only where the Ojibwe peoples lived prior to European settlement, but also where they migrated to and where they eventually settled (on reservations).\nTheir first historical mention occurs in the Jesuit Relation of 1640. Through their friendship with the French traders they were able to obtain guns and thus successfully end their hereditary wars with the Sioux and Foxes on their west and south, with the result that the Sioux were driven out from the Upper Mississippi region, and the Foxes forced down from northern Wisconsin and compelled to ally with the Sauk.\nBy the end of the eighteenth century the Chippewa were the nearly unchallenged owners of almost all of present-day Michigan, northern Wisconsin, and Minnesota, including most of the Red River area and extending westward to the Turtle Mountains of North Dakota, together with the entire northern shores of Lakes Huron and Superior on the Canadian side.\nThey were never removed as so many other tribes have been, but by successive treaty sales they are now restricted to reservations within this territory, with the exception of a few families living in Kansas.\nThe Chippewas, like all other Indians, were extremely superstitious; indeed, they appeared to be more marked in this peculiarity than were most of the other tribes. It has already been mentioned that the ancestors of the later Saginaw Chippewas imagined that the country which they had wrested from the conquered Sauks was haunted by the spirits of those whom they had slain, and that it was only after the lapse of years that their terrors became allayed sufficiently to permit them to occupy the ""haunted hunting-grounds."" But the superstition still remained, and, in fact, it was never entirely dispelled.\nLong after the valleys of the Saginaw, the Shiawassee, and the Maple became studded with white settlements, the Indians still believed that mysterious Sauks were lingering in the forests and along the margins of their streams for purposes of vengeance. So great was their dread that when (as was frequently the case) they became possessed of the idea that the munesous were in their immediate vicinity, they would fly, as if for their lives, abandoning everything, - wigwams, fish, game, and peltry, - and no amount of ridicule from the whites could induce them to stay and face the imaginary danger.\n""Sometimes, during sugar-making,"" said Mr. Truman B. Fox, of Saginaw, ""they would be seized with a sudden panic, and leave everything, - their kettles of sap boiling, their mokoks of sugar standing in their camps, and their ponies tethered in the woods, - and flee helter-skelter to their canoes, as though pursued by the Evil One.\nIn answer to the question asked in regard to the cause of their panic, the invariable answer was a shake of the head, and a mournful \'an-do-gwane\' (don\'t know)."" Some of the northern Indian bands, whose country joined that of the Saginaw Chippewas, played upon their weak superstition, and derived profit from it by lurking around their villages or camps, frightening them into flight, and then appropriating the property which they had abandoned.\nA few shreds of wool from their blankets left sticking on thorns or dead brushwood, hideous figures drawn with coal upon the trunks of trees, or marked on the ground in the vicinity of their lodges, was sure to produce this result, by indicating the presence of the dreaded munesous. Often the Indians would become impressed with the idea that these bad spirits had bewitched their firearms, so that they could kill no game.\nA very singular superstitious rite was performed annually by the Shiawassee Indians at a place called Pindatongoing (meaning the place where the spirit of sound or echo lives), about two miles above Newburg, on the Shiawassee River, where the stream was deep and eddying.\nThe ceremony at this place was witnessed in 1831 by Mr. B. O. Williams, of Owosso, who thus describes it: ""Some of the old Indians every year, in fall or summer, offered up a sacrifice to the spirit of the river at that place. They dressed a puppy or dog in a fantastic manner by decorating it with various colored ribbons, scarlet cloth, beads, or wampum tied around it; also a piece of tobacco and vermilion paint around its neck (their own faces blackened), and after burning, by the river-side, meat, corn, tobacco, and sometimes whisky offerings, would, with many muttered adjurations and addresses to the spirit, and waving of hands, holding the pup, cast him into the river, and then appear to listen and watch, in a mournful attitude, its struggles as it was borne by the current down into a deep hole in the river at the place, the bottom of which at that time could not be discovered without very careful inspection. I could never learn the origin of the legend they then had, that the spirit had dived down into the earth through that deep hole, but they believed that by a propitiatory yearly offering their luck in hunting and fishing on the river would be bettered and their health preserved.""\nBands of Ojibwe People and Related Links\nANCIENT CIVILIZATIONS INDEX\nALPHABETICAL INDEX OF ALL FILES\nCRYSTALINKS MAIN PAGE', 'Beginning – The Utes were created by Sinawav (the Creator) and were placed in the mountains. The Sinawav told the people they would be few in number but, they would be strong warriors, and protectors of their lands.\nThere is no migration story, we were placed here in the mountains, we have always been here, we will always be here.\n1 AD Shoshonean speaking peoples separate from other Ute-Aztecan group\n1150 During the 12th century, Yuman and pueblo type cultures began to be replaced by a Shoshonean pattern.\n1580 The Utes (Mouache band) acquire horses from the Spanish, as related by tribal historians.\n1598 New Mexico is settled by the Spanish. Early trade is established between the Ute people and the Spanish.\n1626 Traditional Ute-Hopi conflicts began about this time and reached their pinnacle around 1680.\n1626 Earliest written reference to the Utes by the Spanish\n1637 First known conflict between Spanish and Utes occurs 1637-1641. Spaniards under Luis de Rosas captured about 80 Utacas (Utes) for forced labor in Santa Fe workshops.\n1637 Utes acquire the horse, making the Utes the first Native Americans to introduce the horse into their culture. The Utes were part of the group who escaped from Santa Fe and captivity by Spanish troops led by Luis de Rosas.\n1650 The seven Ute bands hold well defined territory.\n1670 First Peace Treaty between Utes and Spaniards.\n1692 Alliance between Paiutes, Apaches, and Hopis, to counter Spanish aggression and expansion.\n1700 Beginnings of raids upon Pueblos and Spanish in New Mexico by Utes, Apaches, and Comanche who formed alliances to carry out raids.\n1716 Spanish campaign against Utes and Comanche in preventing raids is not successful.\n1730 Utes continue to raid New Mexico settlements from 1730 to 1750. In 1747, Ute attacks caused the abandonment of Abiquiu. (It was reoccupied in 1748 by the Spaniards).\n1746 Spanish defeat a combination of Ute and Comanche forces near Abiquiu.\n1747 Spanish carry on a campaign against the Caputa Utes.\n1752 Ute leaders, Chiquito, don Tomas, and Barrigon meet with the governor of New Mexico. The Spanish petition the Utes for a trade agreement for deerskins, in the hopes of forestalling further conflicts with the Mouache, Caputa and Chaguaguas.\n1754 Utes have driven out the Navajos from the upper San Juan drainage.\n1754 Mouache Utes enter an alliance with the Jicarilla Apaches.\n1760s Spanish-Ute relations progress to allow Spanish trading ventures into Ute territory as far north as the Gunnison River.\n1770 Utes and Navajos at war with the Hopis\n1776 Dominguez-Escalante expedition through Ute territory with the help of a 12 year old Ute boy. Ute lands are mapped by Miera y Pacheco.\n1778 Spanish law prohibits Spaniards and Christianized Indians from trading with the Utes. The ban was ineffective as traders continued to visit and trade with the Utes.\n1779 Mouache Utes and Jicarilla Apaches joined New Mexico Governor Juan Bautista de Anza in a campaign against the Comanche. Comanche forces under Cuerno Verde were defeated.\n1786 Utes represented by Chiefs Moara and Pinto protest the proposed peace treaty between the Comanche and Spanish. However, Spanish leader Juan Bautista de Anza prevails and a Peace Treaty was negotiated between the Mouache Band of Utes, Comanche and Spanish.\n1789 Treaty of Peace between the Spanish and Utes and promise of Ute aid against the Comanche and Navajos. At this time, the Spanish took precautions against an alliance between the Mouache Utes and Lipan Apaches.\n1801 Spanish use Mouache Utes as spies to gather intelligence on plains Indians.\n1804 Utes and Jicarilla Apaches joined the Spanish in a campaign against the Navajos.\n1806 Battle near Taos between about 400 Mouache Utes and an equal number of Comanche forces.\nLt. Zebulon Pike presents first Anglo-American intrusion into Ute territory.\nTwo Mouache Utes provide a safe passage to a surgeon under Pike’s to Santa Fe.\n1806 Several Spanish and Mexican trading expeditions enter Ute lands from 1806 to 1826.\n1809 About 600 Mouache Utes and some Jicarilla Apaches were attacked on the Arkansas River by the Comanche, Cuampe, and Kiowa. Delgaditio, Mouache chief was killed, along with other leaders Mano Mocha and El Albo.\n1822 Lechat, a Ute leader, proposed treaty with the Americans but little was done immediately.\n1829 Opening of the Old Spanish Trail from Santa Fe to San Gabriel, California, partly through Ute territory.\n1832 Bent’s Fort is established in southeastern Colorado.\n1833 Ouray is born near Taos\n1840s Constant attacks by the Utes on settlements in the Taos Valley and the area of New Mexico north of Espanola. Several land grants began to erode the Ute land base.\n1844 Caputa attack on Rio Arriba settlements after an altercation between the Utes and the Governor of New Mexico in Santa Fe.\n1845 Caputa Utes attack settlement of Ojo Calienta.\n1846 Utes agree to remain peaceful after 60 Ute leaders were induced by William Gilpin to go to Santa Fe and confer with Col. Doniphan.\n1849 First treaty between Utes and the United States at Abiquiu. Chief Quiziachigiate, a Caputa, signed as principal chief and 28 other Utes signed as subordinate chiefs.\n1849 Yellow Nose (Moauche) is born\n1850 The Utes began to obtain arms from the Mormons at Salt Lake.\n1850 An agency was opened for the Utes at Taos. It was soon closed for lack of funds.\n1851 Mouache Utes were attacked near Red River by Kiowas and Arapahos. The Utes retreated to Ojo Caliente.\n1851 Settlements by former Mexican citizens were established in the San Luis Valley. This occurred through 1853. Livestock activities and farming began disrupting the Ute lifestyle.\n1852 The U.S. Government established Fort Massachusetts near Mount Blanca to protect and control the Utes. Six years later the post was moved six miles and became Fort Garland.\n1853 Agency reopened at Taos and Kit Carson was the agent from 1853-59.\n1853 An Indian Agent reports war between Mouache Utes and other Indians along the Arkansas River caused by the scarcity of game. The agent requested the U.S. Government to prevent other Indians from encroaching on Mouache lands.\n1853 Rations were being distributed to the Mouache at Arroyo Hondo and Red River and to the Caputa on the Chama River.\n1854 Ute War started by an attack by Utes on Fort Pueblo. The Utes were mainly from the Mouache band under the leadership of Chief Tierra Blanca. Several skirmishes resulted in the Indians suing for peace.\n1855 In early summer, a treaty was concluded with the Caputa band and one with the Mouache band in August. These were not ratified by the United States.\n1856 Mouache Chief Cany Attle claims the San Luis Valley.\n1857 Cany Attle claims the Conejos Valley.\n1857 Officials recommend that the Caputa and Jicarilla Apache be removed to the San Juan River and assisted in becoming self-sufficient.\n1858 Hostilities between the Utes and Navajos.\n1859 Temuche, a Caputa Chief, took presents to Navajo camp (Kiatano’s) to maintain friendly relations.\n1860 Utes join U.S. troops in campaigns against the Navajos.\n1860 Tabeguache Utes placed under the Denver Agency; Mouache and Jicarilla attached to a sub-agency at Cimarron on Maxwell’s Ranch; Caputa Utes continue to be served at Abiquiu; Weeminuche were managed by Tierra Amarilla.\n1861 Agency for the Tabeguache Utes established at Conejos; Lafayette Head was the first agent.\n1863 Tabeguache cedes San Luis Valley to the U.S.\n1868 Treaty with the Utes and a reservation created for them consisting of approximately the western one-third of Colorado. Ouray selected as principal chief.\n1870 The Weeminuche object to removal to reservation in Colorado. Cabeza Blanca was one of the principal leaders of the Weeminuche at the time.\n1870 Army’s census of 1870 shows 365 Caputa Utes under the leadership of Sobotar.\n1871 Denver’s Indian Agency is established and maintained for Utes who continue to hunt buffalo on the plains.\n1873 Mouache conclude treaty at Cimarron.\n1873 The Utes cede the San Juan Mountain mining area by terms of the Brunot Agreement.\n1874 President U.S. Grant signs the Brunot Agreement and thousands of Ute lands are now appropriated by the U.S. government.\n1874 U. S. takes more Ute lands, granting hunting rights as long the Utes are at peace with the white people. U.S. sets aside a perpetual trust of $50,000 per year in money or bonds which shall be sufficient to produce the sum of twenty five thousand dollars per annum. Which sum of twenty five thousand dollars per annum shall be disbursed or invested at the discretion of the president or as he may direct, for the use and benefit of the Ute Indians annually forever. For said services Ouray Head Chief of the Ute nation shall receive a salary of one thousand dollars per annum for the term of ten years or so long as he shall remain head Chief of the Utes and at peace with the people of the United States. Approved April 29, 1874.\n1877 Establishment of the Southern Ute Agency at Ignacio to serve the Caputa, Mouache, and Weeminuche Ute bands.\n1878 Caputa and Weeminuche cede rights to the 1868 reservation.\n1878 Fort Lewis established at Pagosa Springs to protect and control the Southern Utes.\n1878 Nathan Meeker named Ute agent at White River.\n1879 En route to White River agency at the request of Nathan Meeker, Major Thornburgh and 13 men are killed in Ute Attack for violating an agreement with the Utes\n1879 Meeker’s attempt to change the lifestyle of the Utes failed. Resulting in the demise of Meeker and 11 others. Meeker’s destruction of the Utes valued racetrack and killing of their horses was the final injustice that spurred the attack. The Meeker incident resulted in cries for the removal of all Utes from Colorado.\n1880 Fort Lewis moved to the site near Hesperus, Colorado, on the Southern Ute Reservation.\n1880 Ouray goes to Washington DC for treaty negotiations.\n1880 Ute Agreement signed, resulting the loss of more acres of Ute land.\n1880 Chief Ouray Dies.\n1881 Tabeguache and White River Utes removed to the Uintah Reservation in Utah.\n1881 Denver and Rio Grande Railroad goes through Southern Ute land.\n1886 Consolidation of the Uintah and Ouray Reservations for the Northern Utes.\n1888 Utes agree to move to San Juan County, Utah, but Congress fails to ratify agreement.\n1891 Fort Lewis deactivated as a military post and becomes an Indian school.\n1894 Ute allotment bill presented to Congress.\n1895 Ignacio led most of the Weeminuche to the western part of the Southern Ute Reservation in protest against the government’s policy of land allotment\n1895 Utes agree to the allotment bill.\n1896 New agency set up at Navajo Springs to serve the Weeminuche who did want to accept land in severalty\n1896 Allotments are distributed to Southern Utes\n1905 Buckskin Charley and Antonio Buck (son) travel to Washington, DC, to meet with President Roosevelt.\n1905 Buckskin Charley and five other Native Sovereign leaders in Theodore Roosevelt’s Inaugural Parade.\n1912 Buckskin Charley dedicates the ancient Ute Trail near Manitou Springs in a celebration known as Shan Kive.\n1918 Consolidated Ute Indian Reservation established.\n1920 First Southern Ute Tribal Fair held.\n1924 American Indians become United States citizens\n1925 Reburial of Ouray. Chief Ouray reinterred in Chieftain’s Memorial Cemetery, Ignacio, CO.\n1931 Distribution of rations from Federal Government stopped.\n1934 Passage of the Indian Reorganization Act by Congress (commonly called the Wheeler-Howard Act).\n1936 Death of Buckskin Charlie at age of 96. He was succeeded by Antonio Buck.\n1936 Establishment of a Tribal Council in accordance with the Wheeler-Howard or Indian Reorganization Act of 1934.\n1936 Chief Antonio Buck becomes the first Tribal Chairman of the Southern Utes\n1937 Restoration Act returns 222,016 acres to the Southern Utes.\n1939 Ute Chieftain’s Memorial dedicated on the Last day of the 1939 Ute Fair. The Memorial honored Ouray, Severo, Buckskin Charley and Chief Ignacio.\n1946 Southern Ute Tribe and rancher Raymond D. Farmer enter into a land exchange to provide land for La Plata County Municipal airport.\n1950 Confederated Ute Tribes, consisting of the Northern Utes, Ute Mountain Utes and Southern Ute Tribes were awarded a $31,761,206 monetary judgment for lands taken illegally by the U.S. Government.\n1950s Returning WW ll veterans assist in drafting Rehabilitation Plan utilizing land claims moneys to establish an economic plan for social welfare of the tribal membership.\n1953 Settlement with U.S. Government for Ute land.\n1954 Ute Rehabilitation Program.\n1961 Antonio Buck, Sr., last hereditary chief, dies.\n1961 Southern Ute Tribal Council is comprised of two men and four women; Sunshine Smith, Anna Marie Scott, Euterpe Taylor, Martha Ruth Evensen, John Baker, Sr., and Anthony Burch.\n1959 Southern Ute Police Department.\n1966 Southern Ute Community Action Program started on reservation\n1970 Chimney Rock (located within the Southern Ute reservation) is declared as Archaeological Area and National Historic Site.\n1972 Opening of Pino Nuche Purasa, the motel-restaurant-community building complex, by the Southern Ute at Ignacio.\n1975 Southern Ute Tribe changes enrollment criteria, lowering the blood quantum requirement from one-half to one-quarter Southern Ute blood. Thus doubling Southern Ute Tribal enrollment.\n1977 Buckskin Charley stained window glass dedication in Denver, Colorado. The stained glass depiction of Chief Buckskin Charley will be displayed in the Capitol building along with other notable historical figures in Colorado history.\n1977 Ute-Comanche Peace Treaty. This historic event sealed a treaty between two powerful tribal allies that reigned over the southwestern plains. The treaty began in the 1700s and was interrupted before it was finalized.\n1978 Southern Ute Health Center opens. The new facility replaced a clinic that was outdated and ill-equipped to deliver quality care to the increasing tribal populace.\n1984 Tribal Council declares Education as a top priority of the Southern Ute Indian Tribe. In Tribal Council Resolution No. 84-28, tribal leaders reaffirmed Education as a priority by establishing the Tribal Higher Education Scholarship Committee and Tribal Higher Education scholarship program.\n1985 Elbert J. Floyd Scholarship established by descendants of Elbert J. Floyd, longtime friend of the Southern Ute people and past BIA Superintendent who assisted the Tribal Council.\n1991 New Education Facility housing Higher Education, Private and Public Education opens its doors in Ignacio, Colorado.\n1993 Southern Ute Tribe signs gaming compact with State of Colorado to open a Class lll Casino on the Southern Ute Indian Reservation.\n1993 First Iron Horse Motorcycle Rally held at Sky Ute Downs, Ignacio, CO.\n1994 Red Cedar Gathering Company established to process and transport tribal natural gas.\n1999 Financial Plan adopted, separating Tribal Government from its business enterprises.\n2000 Growth Fund Implementation Plan enacted by Tribal Council creating the Growth and Permanent Funds.\n2000 Southern Ute Indian Academy opens to provide services to Southern Ute Children or children of Southern Utes, from infancy to twelve years old.\n2001 On June 8, the Southern Ute Tribe was awarded a Triple AAA general-obligation bond rating, the highest credit rating possible. The Southern Ute Tribe is the first Native American tribe to earn the rating.\n2001 SunUte Community Center opens its doors. The facility houses a gymnasium, adult and children’s swimming pools, classrooms, weight lifting and exercise equipment.\n2003 Chairman Leonard C. Burch dies. Burch led the Southern Ute Tribe for over 40 years.\n2003 On December 1, the Leonard C. Burch ribbon cutting ceremony for the Leonard C. Burch Tribal Administration Building was held.\n2004 New Housing areas, Cedar Point East and West open.\n2004 The U.S. Post Office in Ignacio, Colorado was officially designated by Congress as the “Leonard C. Burch Post Office Building.”\n2005 Southern Ute Tribal Growth Fund Building opens.\n2006 Mercy Medical Center Grand Opening held June 23. The land for the medical complex was donated by the Southern Ute Tribe.\n2008 Southern Ute Alternative Energy established to manage alternative and renewable energy investments.\n2008 New Sky Ute Casino and Resort Opens. The resort features 140 rooms complete with refined suites, four restaurants, and bowling alley.\n2009 Southern Ute Indian Tribe approves hunting and fishing in the off-reservation Brunot area, including rare game species. Tribal hunters will participate in the special hunt with special permits.\n2009 Southern Ute Indian Tribe takes over Operation of Southern Ute Health Center after successful litigation with the Indian Health Service.\n2009 Lake Nighthorse is filled with water supplied from the Animas La Plata Project.\n2009 Housing and Urban Development awards the Southern Ute Indian Tribe $1,233,976 in housing grants to improve tribal housing.\n2009 Solix and the Southern Ute Indian Tribe Partner on Algae Plant, an alternative energy facility\n2009 Southern Ute Indian Tribe hosts First Annual Tri-Ute Games.\n2010 Annie Bettini Memorial Stone Dedication at the Southern Ute Academy. Ms. Bettini was an outstanding tribal elder, linguist, and educator who advocated for higher education, and her great belief in the Creator.\n2010 Multi-Purpose building and Memorial Chapel open. The Multi-purpose building is a 16,350 square foot facility and the Memorial Chapel consists of 5,048 square feet. The facilities offer services to the tribal membership at no cost.\n2011 Pearl Casias is elected Chairperson. Ms. Casias is the first woman Chairperson of the Southern Utes\n2011 February 18 declared by the State of Colorado as “Ute Recognition Day”.\n2011 Southern Ute Indian Tribe and Durango Discovery Museum enter into five year agreement to provide science, technology, engineering, and math education that will serve tribal youth through high school.\n2011 New Southern Ute Culture Center and Museum opens.\n2012 President Barack Obama declares Chimney Rock the site of ancient Pueblo ruins (Located within the boundaries of the Southern Ute Indian reservation as a National Monument on September 12.\n2012 In November, the National Christmas Tree is harvested from the White River National Forest. Ute Elders from all three Ute tribes participate in commemorating the event. In December, Elders from all three Ute domains travel to the Nation’s capital to witness first-hand Christmas tree dedications.']"	['<urn:uuid:6eda4503-204e-4c4c-98a2-485a0d0004e3>', '<urn:uuid:339a2bdb-6bb6-4445-baca-ebc2d955a9e3>']	open-ended	direct	long-search-query	similar-to-document	comparison	expert	2025-05-13T05:28:56.360010	11	122	4765
66	jaguar conservation measures camera traps benefits effectiveness compared population monitoring methods in field studies and current conservation projects costa rica	Camera traps are highly effective for jaguar conservation as they fill a crucial gap in data collection methods. Unlike traditional methods that rely on tracks or radio telemetry, camera traps allow studying secretive cats without touching them, while providing insights into behavior, animal condition, and prey species presence. In Costa Rica, this technology is being actively used in a new conservation program around the Osa Peninsula, where a region-wide network of camera traps helps determine jaguar population density and evaluate biological corridors, with over 20 organizations participating in the monitoring effort.	['Like many apex predators the jaguar serves as a national symbol for multiple countries, where it is revered as a most powerful and stealthy predator. The iconic wild felid is a member of the pantherine lineage of the cat family—Felidae. Felidae includes the true big cats (i.e. lion, tiger, leopard, snow leopard, and the two species of clouded leopards).\nThe majestic spotted cat is the third largest cat species on the planet and the largest in the Western Hemisphere. It is, perhaps, a living legend, as the iconography of pre-Columbian Mesoamerican societies depicted the jaguar as a great warrior and hunter, as it is thought of to this day. In fact, the great Mayan hunters and warriors adorned themselves with jaguar pelts and other jaguar biofacts; These noble and respected tribal leaders were considered to possess a feline soul in regard for and in recognition of deities associated with the great cat.\nAlthough many modern day societies hold the big cat in high regard, the presence of the jaguar is not so appreciated by all. Charismatic to many, the formidable predator is increasingly perceived as a threat to livelihoods of ranchers and it is sometimes fairly and appropriately implicated in the loss of livestock, which they coexist with through much of their current range.\nThe jaguar may indeed have emerged as a pest species, and its involvement in human-cat conflict may be on the rise, but only as a result of anthropogenic stressors. These include activities and events such as the depletion of their prey base and continued loss of habitat. Sadly, many are hunted in retaliation for the presumed or actual killing of livestock.\nI caught up with Dr. Howard Quigley (Executive Director of jaguar and cougar programs) of Panthera, the most renowned organization in the world dedicated to large felid conservation. Dr. Quigley was returning from Brazil, presumably the Brazilian Pantanal and graciously agreed to participate in the following interview to update Nat Geo News on the status of the imperiled jaguar.\nJordan: Welcome back to the states and thanks for participating in this interview. I wanted to first get an overview of Panthera’s innovative Jaguar Corridor Initiative program. Essentially corridors are designed to link core habitat for a given species. Your teams have managed to link habitat across jaguar range-countries from Argentina, near the tip of South America, to Central America, almost as far north as the northern portion of the cat’s historic range in the Southwestern US. Can you elaborate on this monumental effort aimed at protecting the worldwide population of jaguars and explain a little bit about what comprises core habitat for the jaguar?\nDr. Quigley: Thanks, Jordan. It’s good to be back, but it’s always great to be in jaguar habitat, too. We have so many inspiring activities and inspiring people working on the issue of jaguar connectivity, that it’s just great to be out and immersed in it!\nThe simplest way to describe the JCI is that it’s jaguars showing us the way! By that, I mean that we looked at jaguar genetics, and we looked at the landscapes in which jaguars exist. The genetics of jaguars say that they are all one species; there are no subspecies, which is truly incredible for a land mammal that covers so large a range, from Arizona to Argentina. Thus, we saw our conservation mandate: keep them connected! So, the next questions is – given the fact that they have been doing SO well moving and breeding and not succumbing to many of the barriers that fragment populations and make species and subspecies – how do we help maintain this connectivity? So, you might say, jaguars showed us their genes to tell us about themselves, and now we’re trying to define the connections – or corridors – that make the Jaguar Corridor. For this, we’re conducting a variety of field studies to more precisely define the conservation priorities, across the entire jaguar range.\nJordan: I understand that your team is working in the majority of the 18 countries in which extant (living) populations of the jaguar currently exist, but not all of them. Is Panthera limited by resources or have you prioritized core habitat within range countries that require the most attention to facilitate the linkage of habitat throughout the jaguar’s range?\nDr. Quigley: Well, I have to chuckle a bit, because we are always limited by resources, primarily monetary resources. Jaguars just don’t get the attention that African lions or tigers get, nor the money. We have such dedicated and committed people working on jaguars; this is not our limitation. With the limitations, we have to have a strategy that prioritizes topics and locations. Thus, we focus on the “backbone” of the Jaguar Corridor, and we can’t get diverted. Otherwise, we dilute our effect in jaguar conservation, in defining the corridor. We are working to find the core, the major highway, of jaguar connectivity, without taking too many side roads. So, we might participate or advise on work in Ecuador, for instance, but we don’t have an office there. Or, we might provide advice in the Mexican Yucatan region, but we don’t devote core budget resources there because it’s a peninsula, a dead end on the Jaguar Corridor. We want to find the core of the corridor, and begin to save it. Then, we’ll work out to the edges when we have the resources to do so, and the core is secured. It sounds a bit brutal, but it is the reality of conservation and the limited resources available. Right now, we are making sure we have mapped the backbone, the core, and then we’ll try to expand, if the resources are there.\nJordan: How prevalent would you consider the jaguar regarding its implication in human-feline predator conflict compared to other species of big cats. Is it even a fair comparison? Are there region-specific issues within Latin America that first require consideration on a geospatial level? Do these issues transcend geopolitical boundaries?\nDr. Quigley: OOOOhhh, they SO transcend geopolitical boundaries! …especially with regard to human conflicts. Jaguars are not “man eaters”, like tigers, rarely attacking humans. But, like African lions, like snow leopards, and so many predators, they find it difficult to live in a landscape altered by humans that reduce their available prey, and providing vulnerable alternative prey – cattle, goats, sheep – and not take advantage of that. And, in Latin America, the livestock industry continues to expand, and people continue to take jaguar prey in an unsustainable fashion in so many areas. But, we’ve stepped in with a very aggressive program that offers solutions to ranchers – really simple solutions – that will reduce the conflict. And, we’re working with local communities to measure hunting impacts and develop sustainable harvest approaches to native wild species. The solutions bridge the political boundaries, as do the causes. That’s why we work within countries and between countries to make jaguar conservation happen.\nJordan: I think our readership is often fascinated with how apex predators compare, if you can compare them. In my opinion, comparing predator-human interactions provides perspective that people from far and wide can relate to. For example, you do some outreach work in Brazil’s Pantanal or have in the past to educate ranchers on ways to coexist with jaguars. This involves some capacity building. I suspect that in North America people would be surprised that one of the main threats to cougar (AKA mountain lion) populations is similarly an issue of lack of public awareness.\nAs one of the world’s most esteemed authorities on mountain lions, you have found that the general public is largely uniformed about these cats, at least in regard to our capacity to coexist with them here in North America. Perhaps we should be careful not to criticize our South American friends because we have much to learn about mitigating human-cat conflict here in North America with regard to human-mountain lion conflict. After all, I presume the jaguar, once shared habitat (exhibited syntopy), with the mountain lion here in California or at least overlapped in range (exhibited sympatry).\nI’ll defer to you on these cats in regard to whether or not they were at one time syntopic carnivores in North America, as I think of them in South America. Please correct me if I am wrong. But more to the point we are left with a population of mountain lions in need of protection up here in California. And I think that compared to issues up here live stock ranchers are probably much more familiar with the behavioral ecology of jaguars than Californians are with cougars. Again, I could be wrong, but could you share some of your sentiment or concerns or otherwise comment on this topic?\nDr. Quigley: No, you’re absolutely right, Jordan, jaguars and mountain lions used to live side-by-side in the U.S., and that’s a great question and observation about the comparison between North and South America and how we have handled our relationship with our big cats. Remember, we nearly wiped out mountain lions in the U.S., although they are making an incredible recovery. Still, yes, we’ve found that people are SO uninformed about this species…and ha!, you might be right: the information ranchers have on jaguars, generally, is better than most Californians have about their own mountain lions. This is a very difficult dilemma for conservation in California, but it gives us hope that we can start with ranchers in Brazil or Honduras or Panama and at least have a base of understanding. AND, when we come up with solutions that reduce the killing of livestock by jaguars, we make a lot of friends. And, we find that most ranchers don’t want to kill jaguars!\nJordan: Panthera is particularly fond of using camera traps to help census large felids. It seems that this tool has not caught on as widely as some other and older methods for evaluating the abundance and distribution of some species of elusive and primarily solitary large carnivores. Can you talk about why you find camera traps so effective?\nDr. Quigley: You’re right, Jordan. Camera traps have become our tool of choice in jaguar studies, and across our species programs. Most wild cats are very secretive. Prior to the invention of camera traps, we were limited to two extreme parts of what you might call “the data collection spectrum”. I mean, we either had to rely on the sign that they left in the jungle or forest, like tracks, or we went to the other end of the spectrum, to capture and radio-telemetry. Camera traps have helped fill that void, and usually without effecting the study subject, without even touching the animal, be it a jaguar, a tiger, or whatever species. The added bonus in all this is that we get pictures of behavior, or we can analyze the photos for animal condition, and we get pictures of other animals that live with the wild cats, like their prey. This gives us a picture of the community that we just didn’t have before. We’re JUST starting to learn, really, about how to analyze these data for the tools of conservation. AND – one of the biggest bonuses is that we have tools for outreaching the information to the general public! This has had amazing results, really getting people involved in the conservation of the animals they can’t see.\nJordan: Panthera is entrusted with helping some of the world’s most charismatic megafauna. I imagine that you and your teams are all in the field with great frequency. So my question and it is more out of my curiosity, but if some of Panthera’s staff working with various subspecies of leopards, for example, have a need or the opportunity to share information outside of exchanging your own peer-reviewed publications do you have the opportunity or do you ever convene to do so?\nIn other words, and to perhaps clarify, are there too many human-dimension issues that are region specific or landscape and species/subspecies specific that make such communication impractical or unnecessary with regard to working with the big cats. Or as carnivoran (mammalian carnivore) biologist, do you find methods to study and conserve the wild felids to be fairly transferable or beneficial to helping your colleagues working with different cat species elsewhere in the world?\nDr. Quigley: Great question, Jordan, and certainly one that you understand, too, given your science background. There is ALWAYS room for more interaction and collaboration in our field. You can go to conferences, interact colleagues online or in the office, but that “critical mass” of interaction is hard to find, especially for carnivorans, because we’re more rare than some of the other divisions in our field, like ungulate biologists or songbird biologists. But, at Panthera we’re working very hard now to bring our own biologists together – with new software and cross-program strategy assessments – because, as you put it, these issues are largely “transferable” and we have common challenges for which we may be able to find solutions much more readily if we compare and contrast our work across programs.\nJordan: You shared with me earlier that your camera traps have recorded two melanistic jaguars in two different range countries for the species. I think I read that “black panthers” comprise 6% of the extant jaguar population, but you explained to me why there seems to be such a low representation of these mutant color morphs in the regions your team is working. For some reason I thought the percentage reported in the literature was high to begin with, but are you surprised by this data and can you reiterate for our readers why the number of melanistic individuals reported by the camera trap data recovered is seemingly so low?\nDr. Quigley: I think the best way to describe answer this is, “we don’t know”, I’m sorry to say. I love it, because it’s one of those intriguing questions that we haven’t sorted out. It doesn’t necessarily have a high value for conservation – as a pressing issue whose answer will hold the key to jaguar persistence, for instance. But, you never know. That’s the beauty of what we do in science, sorting out the reasons, the facts, and the pieces of the puzzles that make a species work. It can be SO complicated, and you never know when the explanation of one little aspect of a question about a species just might open up all sorts of answers and insights elsewhere in a species’ ecology.\nMelanism is just one such intriguing question that we can’t fully explain in jaguars. Yes, jaguar melanism occurs at a very low rate in the species. Across our camera studies, I’m sure it’s less than the 6% you mentioned. On the other hand, we have few projects in the Amazon, where there seem to be more melanistic individuals. See, there’s a spatial component to the phenomenon that we don’t understand. And, it’s more complicated than that, and we only have a few of the pieces, and those pieces are not explaining melanism very fully yet. For instance, if melanism is controlled by a dominant allele – which has been suggested by genetic studies – then why don’t we see more melanism in the wild? Is the coloration less successful than the spotted coloration? Are melanistic individuals not surviving as well? We just don’t know, and we’ll keep putting the data together until we think we have more answers.\nJordan: Finally, what can people do to help big cats if they live in regions where they might actually interface with them or if they would like to help endangered felids remotely?\nDr. Quigley: Learn more about cats, and disperse your knowledge to others. And, if you have the chance to take actions to help cats, please do! That’s what I’d tell people in cat country. And, if you have the ability and the means, please give, to Panthera and organizations truly advancing the conservation of wild cats. Around the world, with few exceptions, governments and agencies are able to do it all, to do what needs to be done to secure wild cats in the wild. NGOs and independent researchers and conservation biologists play critical parts in all of this. Panthera, NatGeo’s Big Cat Initiative, academic institutions, and a multitude of small NGOs around the world that we work with, they all play such an important role, and they need support. There is SO much to be done, to make sure these species exist for the next generations, and they are such a measure and an emblem of our commitment to the environment. It’s about them, but it’s also about us, and how we want OUR environment to look like. A world with fewer wild cats would certainly be a less rich, less fulfilling, and less healthy world.', 'Conservationists and ecotourism businesses in the Osa Peninsula region have launched a new program for jaguar conservation in Costa Rica.\nArticle by Shannon Farley\nA trailblazing project to help save the largest cat species in the Americas recently was launched around the Osa Peninsula and gulf of Golfo Dulce. Led by the nonprofit Osa Conservation, a region-wide network of camera traps has been set up to capture images of jaguars (Panthera onca) in the wild to help determine the species’ current population density in Costa Rica.\nJaguars are critically endangered in Costa Rica and on the IUCN Red List as threatened worldwide. While they are protected in Costa Rica – as well as in 12 other countries in North and South America – the population is declining here, according to 25 years of research by the National University of Costa Rica (UNA).\nThe large cats face pressures such as habitat fragmentation, decrease of natural prey (due to humans and lack of habitat), and increasing conflicts with humans over livestock kills.\n“These cats need large territories and connectivity between forested areas, or biological corridors. Habitat protection and focus on these corridors is one tool in our kit, but in order to identify corridors in the (Osa) Peninsula and make informed decisions, we need accurate information on their presence and abundance,” states Osa Conservation.\nMore than 20 organizations so far are participating in the Osa Camera Trap Network, including eco-hotels like Playa Nicuesa Rainforest Lodge, local communities, government agencies, nonprofits, and national and international universities such as Costa Rica’s UNA.\nThe area under study encompasses the Corcovado National Park, Piedras Blancas National Park, the Golfo Dulce Forest Reserve, and several private reserves. The land was divided into quadrants of 44 km. Each quadrant has a camera trap station with two cameras, placed in areas with high probability of jaguar presence.\nA motion sensor triggers the cameras to take pictures, day and night. So every time an animal passes by, the cameras gather photographic evidence that scientists can use to learn about population trends and the health of the forest. Jaguars, and other spotted feline species, have distinctive patterns in their fur that identify them individually.\nNicuesa Lodge already has several camera traps placed in their 165-acre private rainforest reserve that borders the Piedras Blancas National Park. For the Osa Camera Trap Network, staff installed a new station with two cameras in the lodge’s reserve, and additionally helped Osa Conservation biologists put two more cameras in a station in the national park. Nicuesa staff is in charge of monitoring the two quadrants.\nNetwork cameras were placed at the end of February and beginning of March, and will be removed between the end of May and the beginning of June for analysis.\nIn addition to jaguars, scientists will use the images taken to estimate the types and numbers of species of terrestrial mammals among the different protected areas in the Osa Peninsula. The information also will help to evaluate the region’s biological corridors.\nThe Osa is one of the last places in Central America where there is a concentration of jaguars, along with four other species of wild cats: Margays (Leopardus wiedii), Ocelots (Leopardus pardalis), Jaguarundis (Puma yagouaroundi), and Pumas (Puma concolor).\nThe overall range of jaguars extends from northern Mexico and most of Central America to South America, with the greatest concentration in the rainforest of the Amazon basin. The World Wildlife Fund (WWF) in 2013 estimated there to be only 15,000 jaguars left in the wild.']	['<urn:uuid:9def4cf2-2282-4727-a970-6845d42c6f62>', '<urn:uuid:09d2f886-d00b-427d-a21d-b90008c07252>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T05:28:56.360010	20	91	3391
67	eco friendly supply chain cost savings examples	According to Accenture's study, having an eco-friendly supply chain can save money. This is demonstrated in dairy plants where reducing BOD5 waste can save significant amounts - a plant processing 645,000 pounds of milk daily can save nearly $130,000 annually by reducing waste from 5 to 1 pound of BOD5 per thousand pounds of milk. Major companies like Walmart, Apple, and Marriott have also found cost savings through sustainable practices in their supply chains.	"[""Being green is the hottest trend to hit the corporate world since outsourcing. Companies have spent a fortune re-branding themselves as green, and have paid special attention to recycling, protecting endangered forests, sourcing local products and managing water consumption during the manufacturing process.\nBut companies themselves are consumers. Just as shoppers buy their products, companies buy goods that are components of their products or aid in the manufacturing of their products. And they’ve found it’s not enough for them to be green. The companies that supply them with goods and services must also use green sourcing and manufacturing methods.\nThat’s what’s called an eco-friendly supply chain. And according to a study by management consulting firm Accenture, companies have found that having an eco-friendly supply chain can save money.\nWhat is the eco-friendly supply chain?\nAll companies have a supply chain. It traces the process by which companies develop and produce their goods, and then sell those goods to consumers. For some companies, the supply chain is fairly straightforward. But for large companies in the retail, hospitality and manufacturing sectors, for example, the supply chain can consist of hundreds, if not thousands, of individual firms that provide component goods and services.\nIn order to have an eco-friendly supply chain, these large companies must be sure that their suppliers, for example, harvest wood in a sustainable way. Or that they don’t deplete the water supply in areas where they work.\nKey aspects of the eco-friendly supply chain\nThe specific aspects of the eco-friendly supply chain vary from industry to industry.\nFor grocers, for example, buying more fruits and vegetables from local sources is key, according to The Packer, which covers the fresh produce industry. That’s because it reduces the distance the produce must travel from the field to the consumer’s table.\nFor companies in the electronics industry, the process of obtaining minerals such as tin, tantalum and cobalt is a key part of the eco-friendly supply chain, according to the Electronic Industry Citizenship Coalition. That’s because mining for such materials typically takes place in the developing world where there can be sensitive geopolitical and environmental ramifications. (For more information, visit: http://www.eicc.info/)\nThere are, however, some basic aspects of the eco-friendly supply chain that affect all companies. For example, most companies are looking to reduce emissions, energy use and waste.\nCompanies that have excelled at greening the supply chain\nIt’s no surprise that the world’s largest retailer, Wal-Mart Stores, is leading the pack when it comes to having an eco-friendly supply chain.\nAccording to Wal-Mart, its customers “want to know the product’s entire lifecycle.” The retailer’s customers want to know that its products are produced in a responsible way.\nTo achieve that end, Wal-Mart has created a sustainability index for its suppliers. The first step of the index is the supplier sustainability assessment, which includes a 15-question survey. The questions are aimed at determining if the suppliers have embraced three broad goals:\n- Reducing waste to zero\n- Using 100 percent renewable energy\n- Selling sustainable products\nFor more information, visit Wal-Mart's sustainability section.\nA company such as Apple, which makes computers and consumer electronics, faces an additional set of priorities as it creates an eco-friendly supply chain. The company has outlined its environmental expectations along with human rights stipulations in its supplier code of conduct.\nSince many of its products are made at factories in the developing world, Apple pays special attention to the safety of these facilities, and to the rights of the workers. That means outlawing child labor, and insisting that workers not be disciplined for alerting managers to safety issues.\nApple also stipulates that its suppliers limit the normal work week to 60 hours. Workers at supplier factories are given one vacation day per seven days worked.\nIn addition, Apple asks suppliers to pay strict attention to the emission of volatile chemicals, aerosols and combustion byproducts that are produced during manufacturing. Suppliers are expected to reduce or eliminate solid waste.\nFor more information, visit Apple's Supplier Code of Conduct.\nAccording to Marriott International, the hotel chain spends $10 billion each year buying products and services for its 3,000 hospitality properties. And it has taken steps to make sure those purchases are green.\nMarriott purchases 47 million pens each year to stock hotel and meeting rooms, and each pen is composed of 75 percent recycled material. And the 24 million key cards it buys each year are made of 50 percent recycled material. The hotel chain says that saves 66 tons of plastic that would otherwise end up in a landfill. The company has pledged to replace 100,000 synthetic pillows with ones filled with material from recycled bottles.\nFor more information, visit Marriott's Green Supply Chain page.\nFor more information on how large companies are reaping savings from an eco-friendly supply chain, read the Accenture report.\nHave other thoughts on the eco-friendly supply chain? Leave us a note in the comments below."", ""Did you know that your dairy plant may be producing a waste load of 800,000 pounds of BOD5 per year - equivalent to the load from a city of 13,000 people?\nWastewater from most dairy plants is discharged to publicly owned treatment works (POTWs), where the majority of the pollutants are removed before the water is discharged to the environment. Treating the water costs money, and most treatment works charge according to the volume of sewage treated. In addition, they commonly charge extra (apply a surcharge) if the waste load exceeds certain specified levels because it costs more to treat water that contains more pollutants.\nWaste load can be determined by a number of different measurements, including BOD5, the biochemical oxygen demand; COD, the chemical oxygen demand, TSS, the total suspended solids concentration, TKN, the total Kjeldahl nitrogen content, and FOG, the concentration of fats, oils, and grease.\nWastewater from dairy plants is most often tested for BOD5, a measure of the amount of oxygen needed to degrade the organic matter carried by the water. The BOD5 concentration is measured in milligrams per liter (mg/l). When the level exceeds 250 to 300 mg/l, many treatment plants apply a surcharge.\nSome dairy plants discharge as much as 12 pounds of BOD5 per 1,000 pounds of milk received. More than 90 percent of a plant's total waste load comes from milk components that are lost and flow into floor drains during processing. Lactose, proteins, and butterfat are the major components. The wastewater may also contain cleaning agents, lubricants, and solids removed from equipment and floors.\nWaste Loads Can Affect Profits\nIn the past, most dairy plant managers did not concern themselves with reducing their plant's waste load because treatment costs were minimal and restrictions few. Over the past 25 years, however, some cities have increased their surcharges ninefold. BOD5 surcharges now exceed 30 cents per pound in some cities. Pretreatment ordinances in some localities may limit the level of wastes that can be discharged into the sewers. In that case the waste load must be reduced before the wastewater leaves the dairy plant.\nSewer costs, once a minor operating expense, have become something that every cost-conscious manager must consider. At today's rates, a plant's waste load can have a real effect on profitability. Realizing this, some plant managers have been able to cut waste discharges to as little as 1 pound of BOD5 per thousand pounds of milk received.\nCalculating Your Surcharge\nThe total amount of BOD5 in a plant's wastewater can be calculated by multiplying the BOD5 concentration in milligrams per liter by the amount of effluent in millions of gallons\n- Amount of BOD5 = 8.34 x BOD5 concentration x effluent volume\n- For example, if a plant discharges 3.7 million gallons of wastewater per month with a BOD5 concentration of 2,300 mg/l, the total amount of BOD5 discharged during the month is calculated as follows:\n- Amount of BOD5 = 8.34 x 2,300 x 3.7 = 70,973 pounds\nIn addition to the charge for excess BOD5, surcharges may also be made for excessively high levels of COD, TSS, FOG, and TKN.\nSaving Money by Cutting Waste Load: An Example\nHow much money could a dairy plant save by reducing its BOD5 load to only 1 pound per thousand pounds of milk? To find out, consider two dairy plants that each process 645,000 pounds of milk per day. Both pay a BOD5 surcharge of 20 cents per pound. Processor A discharges 1 pound of BOD5 per thousand pounds of milk processed (1 pound for every 116 gallons), while Processor B discharges 5 pounds in processing the same amount of milk.\nThe table shows the daily and annual surcharge costs for the two plants. The operators of Plant A save 80 cents per thousand pounds of milk processed. That means they can bank an extra $516 per day, or almost $130,000 annually if the plant operates 250 days each year. In effect, Processor B is pouring that amount of money down the drain.\nSewer Surcharge Comparison for Two Dairy Plants Processing 645,000 Pounds (75,000 gallons) of Milk per Day\n|Plant A||Plant B||Savings|\n|Waste load (lb of BOD5 per thousand lb of milk)||1||5||4|\n|Daily BOD5 surcharge||$129||$645||$516|\n|Cost per thousand pounds of milk processed||$0.20||$1.00||$0.80|\n|Cost per thousand gallons of milk processed||$1.72||$8.60||$6.88|\nIt is also important to remember that the excess waste load reflects milk lost during processing, and the cost of this lost product must be added to the surcharge to find the true cost.\nTo estimate the potential savings for your plant, determine the sewer surcharges in your community and the current waste load produced by your plant per thousand pounds of milk processed. Then calculate the amount you think the waste load could be decreased by improved operating practices. Enter the values in the work sheet to compute your savings.\nSewer Surcharge Savings for Your Plant\n|Enter current and target waste load in pounds of BOD5 per thousand pounds of milk processed|\n|Enter daily production in thousands of pounds of milk|\n|Multiply current and target waste loads by daily production to find daily waste load in pounds|\n|Enter your BOD5 surcharge cost per pound|\n|Multiply the daily waste load by the surcharge cost to find your daily surcharge cost|\n|Enter the number of days your plant operates each year|\n|Multiply the daily surcharge cost by the number of days your plant operates annually to find the annual surcharge cost|\n|Subtract the annual surcharge cost for the target waste load from the annual cost for the current waste load to find your annual savings|\nYou Can Reduce Waste Load and Save Money in Your Plant\nYou can take positive steps to reduce the waste load produced by your plant. Some suggestions are given in the box. To keep tabs on your progress, use the work sheet to calculate your plant’s waste load. You’ll not only help protect the environment, you’ll also show the people in your community that your firm is a responsible corporate citizen. AND you will send more money to the bank instead of down the drain.\nWaste Reduction Hints\n- Make waste reduction a management priority.\n- Establish waste load reduction goals for your plant.\n- Establish waste load reduction goals for all important processes and areas of the plant where waste can be monitored and controlled.\n- Improve maintenance to prevent product leaks from valves, piping, and equipment.\n- Reduce water use; remember that water used in processing becomes wastewater that must be treated.\n- Thoroughly drain product from tanks and vats before cleaning.\n- Collect solids from floors and equipment by sweeping. Shovel the wastes into containers before actual cleanup begins. Do not use hoses as brooms.\n- Adopt the attitude that waste load reduction is one of the best managerial decisions you can make.\n- Orient employees toward preventing pollution, and train them how to do their jobs in a way that will reduce the discharge of wastes from your plant.""]"	['<urn:uuid:2875697e-fb73-414e-94e5-d99e01b1fb5c>', '<urn:uuid:c084a43f-6485-46d2-8618-c925deab2ab6>']	factoid	with-premise	short-search-query	similar-to-document	three-doc	expert	2025-05-13T05:28:56.360010	7	74	1987
68	cross contamination prevention methods and connection with food product dating	To prevent cross-contamination, use separate cutting boards, utensils and plates for raw foods, wash hands with soap for 20 seconds, and clean all surfaces with hot soapy water. Regarding food product dating, dates are not safety indicators but quality indicators - except for infant formula, they are not required by Federal law. While these dates help ensure best quality, proper storage is what determines food safety.	"['(RxWiki News) Extra care in the kitchen can easily minimize the risk of foodborne illness and ensure a happy and healthy family Thanksgiving this year.\nAround 46 million turkeys will be eaten this Thanksgiving. Unfortunately, holiday meals — particularly those featuring turkey — have been linked to foodborne illnesses caused by Salmonella, Campylobacter and Clostridium perfringens.\n1) Separate: Don\'t Cross-Contaminate\nRaw eggs, poultry and meat can spread bacteria to ready-to-eat foods. This is called cross-contamination.\nTo prevent cross-contamination, use separate cutting boards, utensils and plates for raw eggs, poultry and meat. Furthermore, always wash your hands, counters and utensils after touching raw foods, such as turkey, and before starting on other foods.\nMake sure to wash your hands with soap and water for at least 20 seconds before, during and after preparing food, as well as before eating. Furthermore, be sure to wash all surfaces, including your cutting boards, countertops and utensils, with hot, soapy water.\n2) Thaw Your Bird Correctly\nWhen the bird begins to thaw, any bacteria that may have been present before freezing can begin to grow again. The ""danger zone"" for bacteria to grow is between 40 and 140 degrees Fahrenheit.\nThere are three safe ways to thaw food: in the refrigerator, in cold water and in the microwave oven.\nFollow these steps for a safe thawing process:\ni) Immediately after you buy the frozen turkey at the grocery store, take it home and store it in the freezer. Thawing your bird on the kitchen counter, outside or in the garage is a food safety no-no.\nii) When thawing a turkey in the refrigerator, plan to allow it to thaw for 24 hours for each four to five pounds (at 40 degrees Fahrenheit or below). For example, thaw a 16- to 20-pound turkey for four to five days. Be sure to place the turkey in a container to prevent the juices from dripping on ready-to-eat foods.\nIt is important to note that you can keep a thawed turkey in the refrigerator for one or two days before cooking. If you are planning to use cold water or a microwave to defrost the turkey, know that birds thawed in cold water or in the microwave will have to be cooked immediately.\ni) Make sure the turkey is in a leak-proof plastic bag to prevent cross-contamination. This also prevents the turkey from absorbing water.\nii) Submerge the wrapped turkey in cold tap water.\niii) Change the water every 30 minutes until the turkey is thawed.\niv) Allow about 30 minutes per pound to defrost.\ni) Follow the instructions from your microwave\'s manufacturer when defrosting your turkey.\n3) Stuffing Safety\nIt is safest to cook stuffing outside the turkey, such as in its own casserole dish. This also helps ensure uniform cooking.\nIf dressing is cooked inside the turkey, add it just before cooking and use a food thermometer to ensure the stuffing reaches an internal temperature of at least 165 degrees. Bacteria can survive in stuffing that has not reached this temperature, which could result in foodborne illness.\nIt is best to wait 20 minutes after removing the turkey from the oven before removing the stuffing — this allows the stuffing to cook a little longer.\n4) Cook to the Right Temperature\nTurkey is ready for consumption only after it has been cooked to a temperature of 165 degrees. The best way to check is by using a meat thermometer and checking in multiple spots to ensure the turkey has been cooked thoroughly.\nThe cooking time will depend on the weight of the turkey.\nTo ensure the turkey has reached a safe internal temperature of 165 degrees Fahrenheit, insert a food thermometer into the center of the stuffing, the thickest portions of the breast, and the innermost parts of the thigh and wing.\n5) Put Away Leftovers\nMany families leave out a buffet of food for hours to allow family members to get seconds or wrap up doggie bags, feed late-arriving relatives, or simply because they\'re exhausted from cooking and too stuffed to begin putting food away.\nThe rule of thumb is to refrigerate perishable food within two hours. This is because bacteria can multiply very quickly if left out in the “danger zone,” which is between 40°F and 140°F.\nThe US Department of Agriculture (USDA) and the US Department of Health and Human Services have released a range of resources — from smartphone apps to a ""meat and poultry hotline"" — to help guide consumers safely through the holiday season. The meat and poultry hotline can be reached at 1-888-674-6854. The app can be found under the name ""FoodKeeper App."" The app can help you decide what is safe to use and what you should throw away. These resources also offer guidance on safe ways to keep your leftovers.\nWritten by Digital Pharmacist Staff', 'Cut Food Waste and Maintain Food Safety at Home\n(This blog post was developed from a Knowledge Exchange event sponsored by the non-profit Partnership for Food Safety Education on Jan. 16, 2018. Access a recording of the 30-minute event.)\nReducing the risk of foodborne illness for consumers is the primary focus of the Partnership for Food Safety Education. One in six Americans get a foodborne illness each year. In our work as food safety educators we can support consumers with actions they can take to reduce cross-contamination and to handle food in a way that helps them manage risk of germs like salmonella, campylobacter, E-coli and listeria monocytogenes.\nFood waste is food that is discarded or lost uneaten. Sometimes in food safety education we encourage food to be tossed uneaten if it can pose a health risk to a consumer. Food waste is a huge challenge to our natural resources, our environment, and our pocketbooks.\nHoward Seltzer, a National Food Safety Education Advisor at the U.S. Food and Drug Administration, has been a terrific supporter of the Partnership and a true colleague in food safety education. Howard recently shared information with the BAC Fighter community on food waste and food safety.\nQ: What is the connection between food waste and food safety?\nSeltzer: Food waste by consumers can result from fears about food safety. Some of these fears relate to misunderstandings about what food product dating actually means. Also, consumers can be uncertain about how to store perishable foods.\nQ: What are the basics of understanding food product dating?\nSeltzer: Except for infant formula, dates on food products are not required by any Federal law or regulation, although some states do have requirements for them. Most of the food dates consumers see are on perishable foods. These are foods likely to quickly spoil, decay or become unsafe to eat if not kept refrigerated at 40° F or below or frozen at 0° F or below. Perishables include meat, poultry, fish, dairy products, eggs, and fresh fruits and vegetables. Producers of perishable food use dates to help ensure that consumers buy or use them while the products are at what the producers consider their best quality.\n- Sell by date indicates that a product should not be sold after that date if the buyer is to have it at its best quality.\n- Use by date or Best by date is the maker’s estimate of how long a product will keep at its best quality.\nThese are quality dates only, not safety dates. If stored properly, a food product should be safe, wholesome and of good quality after its Use by or Best by date.\nQ: What are tools that BAC Fighters can use in educating consumers about storing and handing perishable foods?\nSeltzer: The U.S. Department of Agriculture, Cornell University and the Food Marketing Institute cooperatively developed an app called “The FoodKeeper”. This app tells you how best to store perishables and how long they will keep safely. “The FoodKeeper” app is a complete guide to how long virtually every food available in the United States will keep its quality and flavor in the pantry, in the refrigerator, and in the freezer. You can download the FoodKeeper as a mobile app on your Android or Apple devices. You also can access it at FoodSafety.gov.\nQ: What are some practical grocery shopping and eating tips that can help consumers manage their food at home?\nSeltzer: First of all, don’t buy more perishable food than you can reasonably consume before it reaches its maximum storage time. For example, prepackaged luncheon meats will keep two weeks when stored in the refrigerator or three to five days if refrigerated after opening. Plan your meals and use shopping lists. Think about what you are buying and when it will be eaten or used. Before you shop, check your fridge and pantry to avoid buying an item that you already have.\nAlso, avoid impulse and bulk purchases, especially fresh produce and dairy that have limited shelf life. Promotions encouraging purchase of unusual or bulk products often result in consumers buying foods outside their typical needs or family preferences. These foods may end up in the trash.\nLastly, when eating out, become a more mindful eater. If you’re not terribly hungry, request smaller portions. Bring your leftovers home and refrigerate or freeze them within two hours.\nQ: Potential for waste of these foods is high for perishable foods. What are the most important tips around storing perishables?\nSeltzer: Here are a few important tips on storing of perishable foods so that you can avoid food waste. Make sure the temperature of your refrigerator is at or below 40° F. This will ensure perishables are stored safely. Next, avoid “over packing” your fridge. Cold air must circulate around refrigerated foods to keep them properly chilled. Wipe up spills in your refrigerator immediately. This action will reduce the risk of cross contamination where bacteria from one food get spread to other foods in your refrigerator. Finally, check your fridge often to keep track of what you have and what needs to be reheated and eaten or put in the freezer for later use. Leftovers should be used within 3-4 days. You can avoid wasting food by planning to eat these leftovers within the 3-4 days.\nQ: What’s the difference between spoilage bacteria and the bacteria that can cause a foodborne illness?\nSeltzer: Most people would not choose to eat spoiled food. However, if they did, they probably would not get sick. Spoilage bacteria can cause fruits and vegetables to get mushy or slimy, or meat to develop a bad odor, but they do not generally make you sick. Pathogenic bacteria cause illness. They grow rapidly in the Danger Zone-the temperatures between 40 °F (4.4 °C) and 140 °F (60 °C) and do not generally affect the taste, smell, or appearance of food. Food that is left too long at unsafe temperatures could be dangerous to eat, but smell and look just fine.\nCheck out these quality resources on reducing food waste while maintaining food safety at home:']"	['<urn:uuid:ee109ad8-a07b-40e3-9ff3-117293178f1b>', '<urn:uuid:04b90b0c-2185-4555-9704-9113c065eb94>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T05:28:56.360010	10	66	1825
69	How soon after the Big Bang can we detect the first traces of structure in the cosmos?	We can detect structure at two key points: first at around 180 million years after the Big Bang when the first stars appeared, potentially detectable through radiation in the 50-100 megahertz range, and then at 400,000 years after the Big Bang through the Cosmic Microwave Background Radiation which shows temperature fluctuations from when the universe became neutral at 10,000 degrees.	"['(Phys.org) -- In the model astrophysicists and astronomers use to describe the history of the universe, the Big Bang is used as the ultimate starting point, which is believed to have occurred some 13.7 billion years ago. After that, things grow a little murkier as at some point atoms were formed, then stars, and then entire galaxies. The timeline for these formations has been difficult to gauge though as there is so little evidence for researchers to look at; still most agree that the first stars likely appeared somewhere in the neighborhood of a hundred million years after the Big Bang. Now a team of researchers has found that in creating a simulation on a computer, as they describe in their paper published in the journal Nature, they might have found a way to detect the signature of the very first stars to have formed.\nIn creating the simulation the team looked at theories that suggest that during the early stages of the development of the universe, dark matter and regular (baryonic) matter began moving at different speeds due to light interacting with them. Dark matter is believed to be mostly impervious to the impact of light, whereas baryonic matter gets pushed when struck. But because this change in speed was occurring during the time when stars were forming (due to clouds of gas bunching together from their collective gravity and pressure from dark matter) the number of new stars being formed would be lessened, leading to a ""lumpier"" universe than has been previously thought.\nThe researchers used the relative differences in the speed of moving dark matter and baryonic matter and the way they believe radiation emitted from the earliest stars would have impacted those that came after, to calculate that the first stars would likely have appeared some 180 million years after the Big Bang. Using that information they were able to show via their simulation that the radiation emitted from the first stars should be detectable in the 50 to 100 megahertz range here today on Earth. Unfortunately, there are no radio telescopes currently operating in this range, so the team has suggested that one such as the Murchison Wide-field Array in Australia be modified to detect such signals. Doing so, they say would allow researchers to actually listen for clues left behind by the lumpiness of early star distribution and gas interactions allowing them for the first time to detect the presence of those first stars to be born.\nExplore further: There\'s more star-stuff out there but it\'s not dark matter\nMore information: The signature of the first stars in atomic hydrogen at redshift 20, Nature (2012) doi:10.1038/nature11177 . http://www.nature.com/nature/journal/vaop/ncurrent/full/nature11177.html\nDark and baryonic matter moved at different velocities in the early Universe, which strongly suppressed star formation in some regions1. This was estimated2 to imprint a large-scale fluctuation signal of about two millikelvin in the 21-centimetre spectral line of atomic hydrogen associated with stars at a redshift of 20, although this estimate ignored the critical contribution of gas heating due to X-rays3, 4 and major enhancements of the suppression. A large velocity difference reduces the abundance of haloes1, 5, 6 and requires the first stars to form in haloes of about a million solar masses7, 8, substantially greater than previously expected9, 10. Here we report a simulation of the distribution of the first stars at redshift 20 (cosmic age of around 180 million years), incorporating all these ingredients within a 400-megaparsec box. We find that the 21-centimetre hydrogen signature of these stars is an enhanced (ten millikelvin) fluctuation signal on the hundred-megaparsec scale, characterized2 by a flat power spectrum with prominent baryon acoustic oscillations. The required sensitivity to see this signal is achievable with an integration time of a thousand hours with an instrument like the Murchison Wide-field Array11 or the Low Frequency Array12 but designed to operate in the range of 50100 megahertz.', ""Probing the Origins of the Universe with CMBR\nBen - Astronomers and Cosmologists seek to understand the origins of the universe, but as this was billions of years ago, we're left with very few clues as to what actually happened. One of the big clues is the Cosmic Microwave Background Radiation, as Cambridge University's Professor George Efstathiou explained.\nGeorge - The Cosmic Microwave Background Radiation is the remnant radiation from the Big Bang. And as the universe has expanded, this radiation has cooled and is now observed as a background radiation with a very low temperature of 2.3 degrees above absolute zero.\nBen - So it pervades the entire universe and shows us that at some point in the history, the universe, it was very hot and very dense.\nGeorge - That's right. The discovery of the microwave background was discovered in 1965 and once that radiation was discovered, it was really incontrovertible proof that the universe started off with a very hot dense state.\nBen - And how do we study it?\nGeorge - We can study it by observing the radiation at microwave wavelengths. So there have been a range of experiments and missions since the discovery of the background radiation. But it was realized very early on, as soon as the radiation was discovered, that the structure that now produces galaxies and classes of galaxies that we see in the universe today would have imprinted tiny little temperature variations on the background radiation. And so, physicists and astronomers started a search to discover these fluctuations and they were discovered in 1990 by the COBE satellite. These fluctuations are very important because we see them at the time that the universe became neutral. This occurred when the temperature of the background radiation was around 10,000 degrees and the universe was only 400,000 years old. So we see the universe as it was 400,000 years after the Big Bang. But the fluctuations that we see are faithful representations of what happened 10-35 seconds after the Big Bang and that's the real interest that cosmologists have in studying these fluctuations because it tells us about conditions in the ultra, ultra early universe.\nBen - And so, can we use it to work out some of the physics that was going on back then?\nGeorge - That's right. By studying these fluctuations, we can probe physics at ultra high energies, 10 to 15 orders of magnitude higher than the energies achievable by the large hadron collider. The physics of those high energies is really not at all well understood and it could be very, very different to the sort of physics that we know about. And that's what makes it really fascinating because a theory like that would have maybe 9 or 10 spatial dimensions. And these dimensions that we currently don't see, at those very early times, would've opened up. And so, it's possible that we could see signatures of higher dimensional physics by studying the background radiation.\nBen - Relatively recently, the Planck mission has set out to try and study this in more detail. What are you hoping to use that for?\nGeorge - Planck was successfully launched last year and it is by far the most sensitive space probe designed to study these fluctuations. So we will get much, much better images, of a higher angular resolution with much higher sensitivity that have ever been achieved before. And we're also measuring the polarisation. In this background radiation, the fluctuations are slightly polarised. They're polarised at a few percent level and Planck has sensitivity to polarisation so we expect to create an accurate picture of not just the distribution of temperature but the distribution of polarisation. That's very important because gravitational waves generated in the very early universe can produce a specific type of polarisation pattern which we may detect.\nBen - So what's the really big question that we're trying to answer?\nGeorge - We know very little about the physics of these very early times. What we would like to know is what actually happened very, very close to the Big Bang, not just in a sort of conceptual way, but actually really probe the structure of the Big Bang. What we think happened is that very close to the Big Bang, the universe underwent a period where it effectively expanded faster than the speed of light. So we called this a period of inflation. It's a very attractive theory. It is not a well-founded theory in terms of physics yet, so we don't understand the mechanism, we don't understand any of the details. What we hope to do with Planck is to get enough information that we can actually get a handle on what actually happened. Did the universe really go through an inflationary phase of expansion? What was the physics responsible for inflation? How were the fluctuations generated? What was the energy scale of inflation? These are the sort of questions that we hope to answer.\nBen - George Efstathiou on how we can use the leftover radiation from the Big Bang to probe the early universe.""]"	['<urn:uuid:defb862a-509a-45f5-86e6-1341b2e4206f>', '<urn:uuid:73cd1977-8823-403f-af72-71c658a618b7>']	factoid	with-premise	concise-and-natural	distant-from-document	three-doc	expert	2025-05-13T05:28:56.360010	17	60	1501
70	I'm curious about how modern farms are becoming more environmentally friendly - what unique power source does Fair Oaks use to fuel their facility?	Fair Oaks uses anaerobic digesters to convert cow and pig manure into energy to fuel their facility, what they call 'cutting edge poo power.'	"['Fair Oaks, Ind., June 22, 2016 - How do you get more young people interested in agriculture and ultimately, careers in our industry?\nThere are numerous efforts underway, starting in elementary school with teachers who have embraced “Ag in the Classroom,” and extracurricular programs like 4-H, FFA, and in college, Agriculture Future of America.\nOne of the newest outreach efforts is sure to become a “must-see” attraction for anyone wanting to better understand modern farming and its associated career opportunities. It’s WinField’s Crop Adventure – the latest addition to Fair Oaks Dairy Adventure and Pork Adventure.\nIn case you’ve never been to Fair Oaks, it’s the brainchild of a group of dairy farmers who decided to build a 3,000 cow dairy farm that’s open to schoolchildren and the general public.\nAt Fair Oaks, located in Indiana about halfway between Chicago and Indianapolis on an exit off of I-65, you can watch a dairy calf being born while the staff describes everything from breeding to birthing to milking. That was in the original Dairy Adventure. Later, more attractions were added to the complex, including a climbing wall in the shape of a milk bottle, rope courses, outdoor play areas and the Farmhouse restaurant.\nThe Pork Adventure, with 3,000 sows was also added, providing the public with the chance to watch baby pigs being born in both crates and group pens, as tour guides explain what’s going on and answer questions like: “Is that really a pig?”\nUsing anaerobic digesters to convert waste to energy, the facility is fueled by cow and pig manure, or as Fair Oaks says, “cutting edge poo power.”\nBut there was still a piece of this “agrotourism” and educational package that was missing, said Mike McCloskey, co-founder of Fair Oaks Farms – that is until WinField and its parent company, Land O’Lakes, decided four years ago to support their farmer customers by telling the story of agriculture in this interactive fashion.\nIt was a big idea and required a big investment: $12 million, according to Mike Vande Logt, WinField’s executive vice president and chief operating officer. The newest Adventure opened earlier this month.\n""The WinField Crop Adventure, developed in partnership with Fair Oaks Farms, is one way Land O\'Lakes and WinField are sharing the important and meaningful work done by farmers, and fueling ideas and innovation to feed the growing population,"" said Vande Logt.\n“Today, 98 percent of the people don’t touch agriculture so we have to help them learn about what we do and how we do it,” explained WinField Communications Director Faye Bliese. “So many people still think of a farmer in bib overalls from about 50 years ago. But we know that’s not a very efficient or sustainable way to farm.”\nBliese said the exhibit aims to help people understand that there are 7 billion people on the planet now, and that the population may grow to over 9 billion by mid-century, and all of those people will have to be fed. “We want to make sure they understand why farmers have to be more efficient and productive than ever before.”\nNow, at the Crop Adventure, visitors can get a first-hand look at the soil, the seeds, and the roots, and even interact with a few bugs that can unfortunately hamper modern crop production. Walking through the building, you’ll see how U.S. farmers used to plant compared to current farming practices and connect the dots with other segments of the value chain. Visitors can also view a garden designed to feed pollinators.\nBliese says Fair Oaks attracts over 350,000 visitors annually, with most coming from communities within a 100 mile radius. But WinField hopes to extend that reach by partnering with National Geographic on videos, a curriculum guide, and a website: www.ResponsibleAcre.com\nAgriculture Secretary Tom Vilsack was one of several dignitaries on hand earlier this month to kick off the new Crop Adventure and applaud the effort.\n“This is a huge commitment by Land O’Lakes,” Vilsack said. CEO Chris Policinski and the folks at Land O’Lakes “genuinely believe this is a story that needs to betold in a way that can speak to the 98 percent of America that does not farm, and the 98 percent of America that are several generations removed from anyone in their family that was a farmer.”\nVilsack noted that it’s a working farm and not just a petting zoo.\n“It is a place that says, ‘Here’s where your milk comes from, here’s where that pork chop that you enjoyed at the restaurant comes from.’ In the future all of that depends on the crops that are produced, and here’s how that works. And here’s everything that goes into how that works, so the people understand that it’s complicated and comprehensive, it’s also explaining to young people that there are multiple opportunities throughout agriculture and agribusiness that they can get excited about.”\nPolicinski agreed that there is a need “to excite and engage people in a broader discussion about what it takes to produce the food needed to feed a growing global population.\nHe said farmers are doing a great job talking among themselves about their sustainability story. However, he said they\'ve fallen short when it comes to engaging consumers.\n""Our goal here is to tell the story in a way that consumers can have fun learning about and absorb.""\nIn the next few years, the site is expected to include new adventures for poultry, beef cattle and machinery technology. And plans for a new hotel are also being discussed – in case visitors want to stay more than one day. For more information, visit: http://fofarms.com/']"	['<urn:uuid:63c3959e-3e44-49a3-b7aa-45303b23655b>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T05:28:56.360010	24	24	937
71	Who used to hang out in Palm Beach during high season?	The Kennedys, Donald Trump, and Rod Stewart were among those who frequented Palm Beach during the season, which ended in April.	"[""WEST PALM BEACH, Fla. — The season, as it’s known, ended in April, leaving Lily Pulitzer-clad stragglers shopping for sunglasses on Worth Avenue and ordering the steak au poivre at Flagler's. Until this week, when the stomping grounds of the Kennedys, Donald Trump, and Rod Stewart are descended upon by an entirely different brand of aging rocker: the New Urbanist.\nThe 20th gathering of the Congress for the New Urbanism runs through the weekend here, and the milestone raises some interesting questions about what happens when a revolutionary movement reaches middle age – and indeed in the world of planning and especially real estate development, becomes part of the establishment.\nIn the late 1980s and early 1990s, New Urbanism was hitting the cover of Time as a grassroots architectural and design movement dead-set against auto-dependent suburban sprawl. CNU preached compact, walkable, mixed-use development, and traditional town planning principles for grid street layouts and user-friendly parks. It was back to the future, before World War II and the age of the automobile, before the soulless exurban tracts around cul-de-sacs, that arch-enemy of the connected landscape.\nRadical. Audacious. Heretical. Until it became gospel.\nLike its sister movement Smart Growth, New Urbanism – and here the disclaimer that I am a member of CNU, along with APA and ULI and other professional associations – is well-accepted by planners and especially developers, who see a magic formula for retail and a successor to the enclosed shopping mall; even corporate homebuilders like Toll Brothers and Lennar are skipping far-flung subdivisions for urban infill. Model projects are not much different from those publicized by the Urban Land Institute, from Maryland's Kentlands to the redevelopment of the Stapleton airport in Denver.\nThat’s not to say New Urbanism hasn’t had many critics. From the beginning the movement was mocked for being hegemonic in its own right, elitist and unaffordable, the “New Suburbanism,” or boutique sprawl. One of the first New Urbanist projects, Seaside, was famously the set for film The Truman Show, the story of a picture-perfect, scripted life. Empirical evidence was scarce that New Urbanist developments fostered any greater sense of community than conventional subdivisions. Modernism as foil could only go so far; the New Urbanist critiques made it seem as if, as one Harvard professor said, Le Corbusier-style horizontal windows were leading to the downfall of civilization.\nWhat CNU has been looking for is some new foils. Property rights advocates, the Tea Party, and pro-suburbia commentators like Wendell Cox and Joel Kotkin provide some grist, but there are plenty of free-market libertarians in CNU, starting with the Miami based Cuban-American architect Andres Duany of DPZ. Duany has been enjoying picking fights with Harvard’s Charles Waldheim over the alternative approach to city-building, landscape urbanism. But it’s hard to imagine a more esoteric debate.\nFlorida will be a good place to take stock. The mayor of West Palm Beach famously embraced New Urbanism, and its revitalized downtown is a huge success. Miami adopted a form-based code of the kind CNU advocates – zoning more concerned with the collection of buildings than the use that goes on inside of them. Things like zoning and building codes remain a dreary but entirely necessary business for more sustainable cities. Plenty of places still forbid residential over retail in downtowns, or have ridiculous minimum parking requirements.\n“If the first phase of CNU has culminated in a broader culture acceptance of urbanism as a force for good, the second phase will be defined by successfully pushing for policy and design reform that actually allows urbanism to get built,” says CNU president John Norquist, a former mayor of Milwaukee.\nBig foundations like Rockefeller, Ford, and Kresge are supporting transit and see urbanism as the setting for advancing social justice. Others see great public health benefits. A big focus is to get at the anti-urban policies and standards and rules at the federal, state and local level, Norquist says.\nIf that sounds like nitty-gritty implementation, and a little bit nerdy, too, CNU has always had a mix of rock-star designers and those among the 1,500 architects, designers, planners, elected officials, developers and others expected for the conference, who like nothing better than a lengthy debate on the merits of different varieties of shade trees.\nStill, as unglamorous as it is, re-writing the owner’s manual for urbanism makes sense. It’s what another architectural movement – the Congress International Architecture Moderne (CIAM), after which CNU is modeled – did. Leaders like Le Corbusier and Walter Gropius wasted little time embedding modernism in codes and academic curricula.\nCNU is trying to undo the damage of expressways, separated-use zoning, and the destruction of the building craft fostered by CIAM, says Norquist. That means changing the rules put in place over a half-century: fighting fire with fire.\nCIAM is an apt comparison, a messy, democratic organization, with a mission and a charter, but led by big and disparate egos. Notably, they put themselves out of business after about 30 years. The leaders saw that the seeds of modernism had been sown.\nBy that reckoning, to make change, if not stay relevant, CNU has about 10 more years to go.""]"	['<urn:uuid:986d661a-e0df-4e93-8df6-aeb208672efa>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T05:28:56.360010	11	21	859
72	shock trauma therapy types effective treatment	Several types of therapy can effectively treat emotional shock and PTSD, including: Acceptance and commitment therapy (ACT) for experiencing and accepting emotions, Cognitive-behavioral therapy (CBT) for challenging unhelpful thought patterns, Dialectical behavior therapy (DBT) for emotion regulation, Exposure therapy (ET) for confronting fears, and Trauma-focused cognitive-behavioral therapy (TF-CBT) designed primarily for children and adolescents.	['PTSD Symptoms Identifying and Coping With Emotional Shock By Sanjana Gupta Sanjana Gupta Sanjana is a health writer and editor. Her work spans various health-related topics, including mental health, fitness, nutrition, and wellness. Learn about our editorial process Updated on February 20, 2023 Medically reviewed Verywell Mind articles are reviewed by board-certified physicians and mental healthcare professionals. Medical Reviewers confirm the content is thorough and accurate, reflecting the latest evidence-based research. Content is reviewed before publication and upon substantial updates. Learn more. by Akeem Marsh, MD Medically reviewed by Akeem Marsh, MD LinkedIn Twitter Akeem Marsh, MD, is a board-certified child, adolescent, and adult psychiatrist who has dedicated his career to working with medically underserved communities. Learn about our Medical Review Board Print Jasmin Merdan / Getty Images Table of Contents View All Table of Contents Symptoms Causes Diagnosis Treatment Coping Emotional shock is a reaction that you may have to an unexpected event or traumatic incident that upsets you and makes it hard for you to function. When faced with an intense stressor, you may experience a rush of overwhelming emotions that you aren‘t ready to understand or respond to, which can cause your body to go into a state of shock, says Aimee Daramus, PsyD, a licensed clinical psychologist and author of “Understanding Bipolar Disorder.” Emotional shock is often part of the fight or flight response, a normal but painful way your brain reacts to something it sees as a threat to your well-being, according to Dr. Daramus. When your brain is unable to process the situation, it freezes in an effort to protect your mind and body. This article explores the causes and symptoms of emotional shock, as well as treatment options and coping strategies. Signs of an Emotional Breakdown Symptoms of Emotional Shock Emotional shock may be accompanied by a range of physical and emotional symptoms. Aimee Daramus, PsyD You might feel numb, or cry, or rage. You might just sit there, emotionally unable to move. You might dissociate, and feel like nothing around you is real, or that it‘s actually happening to someone else. — Aimee Daramus, PsyD According to Dr. Daramus, the symptoms of emotional shock can include: Denial Numbness Disassociation Panic Anger Breathlessness Headache Nausea Dizziness Lightheadedness Muscle tension Increased heart rate Tightness in the throat or chest Inability to speak or move Difficulty rationalizing, thinking, or planning Loss of interest in surroundings Inability to express emotion It’s important to note that everyone experiences emotional shock differently. Two people may face the exact same experience and have completely different emotional reactions. This is because experiences are extremely subjective; they are less indicative of the actual event and more indicative of the way a person interprets them. Causes of Emotional Shock You may experience emotional shock in the wake of an event that suddenly changes your world. It could be an event that affects you or those close to you, such as your parents, spouse, children, or close friends. Some of the causes of emotional shock can include: Abandonment Abuse Accident Argument Breakup Crime Death Divorce Domestic abuse Financial crisis Health diagnosis Infidelity Injury Job loss Natural disaster Near death incident Racism Terrorism Violence Witnessing a death, accident, crime, or trauma “Sometimes people may even experience emotional shock about something good, like a dream job or a marriage proposal, if the emotions are too big to handle at the moment. Most of the time, though, it‘s a response to scary or intensely painful events,” says Dr. Daramus. Diagnosing Emotional Shock People experience emotional shock for varying amounts of time. Depending on its severity and the circumstances, it may dissipate on its own within minutes or may persist for longer. It can lead to acute stress disorder (ASD) or post-traumatic stress disorder (PTSD). If emotional shock persists or causes discomfort, it can be helpful to visit a licensed mental health professional or medical professional. They can assess your symptoms, conduct any physical or psychological tests required, diagnose your condition, evaluate its severity, refer you to a specialist if necessary, and develop a treatment plan for you. How to Help Someone With PTSD Treatment for Emotional Shock Emotional shock in the wake of a traumatic event can be treated with therapy, particularly if you develop PTSD. Some of the forms of therapy that can treat PTSD include: Acceptance and commitment therapy (ACT): ACT can help you experience your emotions and accept them, instead of trying to escape or avoid them. Cognitive-behavioral therapy (CBT): CBT can help challenge unhelpful thought patterns and correct problematic behaviors. Dialectical behavior therapy (DBT): DBT can help you regulate your emotions and increase mindfulness. Exposure therapy (ET): ET involves revisiting the situation and confronting your fears until you have processed the situation and are not scared of it anymore. Trauma-focused cognitive-behavioral therapy (TF-CBT): Primarily designed for children and adolescents, TF-CBT can help them process the event and work through their emotional reactions. How to Heal From Trauma Coping With Emotional Shock Aimee Daramus, PsyD If you or a loved one are experiencing emotional shock, the most important thing is to restore a sense of safety and comfort. — Aimee Daramus, PsyD Dr. Daramus suggests some strategies that can help you cope with emotional shock and restore your sense of equilibrium: Surround yourself with supportive people. Go somewhere where you feel safe. Make sure you eat and stay hydrated. Take good care of yourself or let others take care of you. Seek comfort from pets or familiar, comforting objects. Distract yourself with games like Tetris or video games, as they take a lot of attention and concentration and can help you manage your thoughts. Accept that you won’t be functioning normally right now because your mind and body already have a big job to handle. Don‘t try to talk about the situation or process it while you‘re still overwhelmed. That could make it worse because your mind and body are already telling you that this is too much. Don’t put too much pressure on yourself. Respect where you‘re at and let safe spaces, people, and comforting objects ease you out of it in a non-pressuring way. When Oversharing Turns into Trauma Dumping, and How to Stop A Word From Verywell People may experience emotional shock in the wake of a traumatic event, such as an accident, the loss of a job, or the death of a loved one. Everyone reacts to traumatic events differently. Depending on the circumstances, you may feel completely numb or you may experience panic, anger, or disassociation. You may also experience physical symptoms such as a rapid heartbeat, breathlessness, or tightness in your throat. In the immediate aftermath of an emotional shock, the most important thing is to focus on your comfort and safety, to restore your emotional equilibrium. Emotional shock is often short-lived, but it may persist or develop into PTSD. PTSD can be assessed, diagnosed, and treated by a mental health professional. 9 Healthy Coping Skills for PTSD 5 Sources Verywell Mind uses only high-quality sources, including peer-reviewed studies, to support the facts within our articles. Read our editorial process to learn more about how we fact-check and keep our content accurate, reliable, and trustworthy. Stinesen Kollberg K, Wilderäng U, Thorsteinsdottir T, et al. How badly did it hit? Self-assessed emotional shock upon prostate cancer diagnosis and psychological well-being: a follow-up at 3, 12, and 24 months after surgery. Acta Oncol. 2017;56(7):984-990. doi:10.1080/0284186X.2017.1300320 Giotakos O. Neurobiology of emotional trauma. Psychiatriki. 2020;31(2):162-171. doi:10.22365/jpsych.2020.312.162 American Psychological Association. Trauma and shock. Fujiwara T, Mizuki R, Miki T, Chemtob C. Association between facial expression and PTSD symptoms among young children exposed to the Great East Japan Earthquake: a pilot study. Front Psychol. 2015;0. doi:10.3389/fpsyg.2015.01534 American Psychological Association. Cognitive behavioral therapy (CBT) for treatment of PTSD. By Sanjana Gupta Sanjana is a health writer and editor. Her work spans various health-related topics, including mental health, fitness, nutrition, and wellness. See Our Editorial Process Meet Our Review Board Share Feedback Was this page helpful? Thanks for your feedback! What is your feedback? Other Helpful Report an Error Submit Speak to a Therapist for PTSD Advertiser Disclosure × The offers that appear in this table are from partnerships from which Verywell Mind receives compensation.']	['<urn:uuid:7f9b1366-e720-40f9-b7ad-d955d1b08906>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T05:28:56.360010	6	54	1370
73	As a V2X technology expert, I've been following developments in vehicle communications and security - can you explain how autonomous vehicles handle infrastructure communications and what protection mechanisms are needed against security breaches?	For autonomous vehicles, V2x (vehicle to everything) communications require very low latency transmissions and dedicated short range communication technology. This is particularly crucial for autonomy Levels 3, 4, and 5, where vehicles must monitor the driving environment. The communications need harmonization of standards, which the 3GPP technical specifications group is working on. Regarding security, since vehicles are fielded systems accessible by people with malicious intentions, software security alone is inadequate. Protection requires a combination of software and hardware security measures. Key protection methods include risk assessment through system penetration testing, protection planning to mitigate vulnerabilities, attack scenario testing using a black box approach, and DPA side-channel analysis. The security solution must ensure that V2V and V2I messages originate from trustworthy sources and aren't modified between sender and receiver.	['Jonathan Newell visits HORIBA MIRA proving grounds to find out how the historic centre is paving the way for innovation in the vast area of intelligent mobility.\nPreviously known as the Motor Industry Research Association (MIRA), the vast and largely secretive complex has come a long way since its formation in 1946 to consolidate expertise and boost the UK’s post-war presence in the burgeoning automotive industry.\nNow owned by the Japanese HORIBA test specialist, the site comprises over 750 acres of proving grounds, a state of the art test facility for Electro Magnetic Compatibility (EMC), other test and research premises and a technology park comprising around 30 companies including such icons of the UK automotive industry as Aston Martin, Bentley, Jaguar, Land Rover and Triumph, all of whom are users of the brand-agnostic expertise and facilities available at HORIBA MIRA.\nI met with Chris Reeves, the company’s Commercial Manager for Intelligent Mobility & Future Transport Technologies, in the impressive Control Centre building in the heart of the proving grounds on a backdrop of camouflaged sports cars on the high speed circuit.\nConsulting and Testing\nDespite the high profile visibility of HORIBA MIRA’s proving grounds and testing expertise, around 60% of the company’s activities are in vehicle engineering consultancy, much of which is involved in research activities working alongside industry participants in emerging technologies, a field that is currently a very active and competitive environment.\nIntelligent mobility is the “Holy Grail” that the global automotive industry is aiming for against seemingly impossible timescales, a lack of supporting legislative framework and standards that are yet to emerge.\nHowever, unlike the Holy Grail, there’s nothing mythical or unreachable about intelligent mobility. As Reeves explained, to some extent, it’s already with us. Describing it as the third revolution in automotive technology, Reeves told us, “First we had Ford’s manufacturing process that brought car ownership into everybody’s reach, then we had the low carbon revolution of 16 years ago and now we’re undergoing the third revolution of connectivity and automation.”\nUnderpinning the notion that the technology is already here, around 40% of the cost in a new car today is associated with the embedded electronics and there are up to 100 million lines of code making cars the single biggest consumer electronics purchase that people make today.\nThe Society of Automotive Engineers (SAE) standard J3016 defines six levels of automation for on-road vehicles starting with Levels 0 and 1 for no automation and driver assistance, which represents the established market which currently exists. Driver assistance systems include autonomous emergency braking, lane keeping assistance and adaptive cruise control.\nLevel 2 represents partial automation where the vehicle can execute steering and acceleration or deceleration tasks but the driver monitors the environment and is expected to regain control where necessary. The market for Level 2 automation is emerging now.\nLevels 3, 4 and 5 represent conditional, high and full automation where execution, monitoring and fallback fall more towards the systems than the driver. These Levels still represent the future of the technology but testing is already well under way by major automotive companies globally.\nA question of trust\nBefore relinquishing so much control to automated in-vehicle systems, consumers need to have complete faith that nothing will go wrong and that they will be transported safely to their destination. So far, people have shown a willingness to place their trust in various levels of autonomous transport from the extremely simple, such as lifts, to more complex systems like airport transit shuttles and the Docklands Light Railway.\nTaking the next level of trust from constrained systems like lifts and shuttles to vehicles with more degrees of freedom like a car will need extensive and exhaustive testing along with assurances of capabilities and failure modes.\nIn terms of security, which is a big concern for consumers, HORIBA MIRA is a member of the new CCV (Cybersecurity for Connected Vehicles) consortium along with Thatcham Research. The consortium is focused on ensuring that the highly connected transport systems of the future have high levels of resilience to cyber attacks.\nIn terms of safety and robust fail-safe mechanisms, HORIBA MIRA is involved in test and validation activities for all autonomous and connectivity technology, focusing on functional safety and availability. It is important for autonomous systems to fail operational, ie to continue working correctly until a safe condition is achieved.\nWhen asked for an example of what this would mean, Reeves told us, “An example would be that an automated control system must continue to operate correctly and safely in a situation when control can’t be handed back to the HiL (Human in Loop) fast enough.”\nThe ability of the “Human in the Loop” to regain control is particularly critical in transitioning from Level 2 to Levels 3 and 4 of automated systems. Level 3 has the expectation of driver intervention when necessary and Level 4 requires automated driving to be successful when the driver doesn’t intervene. These are the two most critical stages in terms of trust.\nTesting regimes at the proving grounds also test sensor limits and the tolerance levels of autonomous control. This means that automated systems which operate correctly in ideal environments are pushed to their limits in environments that aren’t ideal where the road surface is poor or the lane markings are faded. In these conditions, the sensors still need to be able to provide the necessary guidance and control.\nTo achieve the higher levels of autonomy defined in SAE J3016, the vehicle needs to monitor the driving environment. This applies to autonomy Levels 3, 4 and 5 which are conditional, high and full automation.\nFor this to be possible, there needs to be a very concentrated level of cooperation between the vehicle and other vehicles as well as the infrastructure – commonly referred to as V2x (vehicle to everything) communications.\nSafety-critical infrastructure communications require very low latency inbound transmissions and dedicated short range communication technology, an area where there is considerable R&D focus.\nAdditionally, there needs to be a harmonisation of communication standards, something which the 3GPP technical specifications group is working on. Such harmonisation affects the global mobile comms operators but still allows for variations in vehicle systems, thus enabling each individual system manufacturer to have their unique selling points and thereby maintain market competition.\nAt HORIBA MIRA, all the test facilities are in place to provide rigorous verification of all the communications and automation technologies that result from the development including environmental and EMC testing as well as a range of testing circuits including the special “city circuit” that can be electronically configured to provide different signal environments that can affect critical V2x communications and GPS navigation signals. With a range of surface conditions on several test tracks, the ability of sensors to operate in difficult conditions can also be tested.\nThe city circuit can also emulate other vehicles and vulnerable road users in order to fully stress vehicle safety systems and collision avoidance technology.\nLevels of automation SAE J3016\n|SAE Level||Name||Steering/Accelerator Execution||Environment Monitoring||Fallback||System capability|\n|1||Driver Assistance||Human and System||Human||Human||Some modes|\n|2||Partial Automation||System||Human||Human||Some modes|\n|3||Conditional Automation||System||System||Human||Some modes|\n|4||High Automation||System||System||System||Some modes|\n|5||Full Automation||System||System||System||All driving modes|\nLatest News by Jonathan Newell (see all)\n- Have your say on commercial AFV regulation changes - August 18, 2017\n- Electric vehicle development for India - July 24, 2017\n- A boost to resources for vehicle electrification - July 21, 2017', 'Biggest security threats for embedded designers\nEmbedded system designers face a number of threats to the applications that they develop for the Internet of Things (IoT). One of the biggest threats comes from IoT devices that end-users can access, such as commercial networked HVAC systems, wireless base stations, power stations, network gateway systems, and avionics networking.\nAnother example is the connected car, including the advanced driver assistance system (ADAS) that encompasses intelligent, interconnected vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) systems. Since vehicles are fielded systems, they are accessible by people with malicious intentions. There can be serious consequences – up to and including loss of life – if, for instance, ADAS systems cannot ensure that V2V and V2I messages originate from a trustworthy source and are not modified between sender and receiver.\nWith these and other systems, software security, alone, has proven inadequate to protect user-accessible devices against known threats. What is needed is a combination of software and hardware security. For example, today’s FPGA SoCs can be used to implement a hardware security scheme that compliments the software and strengthens the system. Ideally, the hardware and software solution should combat three types of security breaches:\n1. Design security: This includes IP protection and ensuring that configuration bit streams are encrypted and protected. In addition, designs need to incorporate a method to ensure there is no overbuilding or cloning of the design possible.\n2. Hardware security: Designers also need to certify that user-accessible devices are resistant to physical attacks. For example, differential power analysis (DPA) attacks can extract keys and other vital device information.\n3. Data security: This element ensures that communications into and out of the device are authentic and secure.\nEmbedded system program managers and development teams must design these types of protections into their products while best leveraging the characteristics of the underlying platform. The result should be a robust protection network with no single point of failure. Some key methods for achieving this goal include:\n∑ Risk assessment: System penetration testing should be used for a detailed system evaluation, to assess critical system data/functions, discover vulnerabilities, enumerate threats, and outline the likelihood and consequence of system compromise.\n∑ Protection planning: Using risk assessments and any other compiled data, developers should seek to understand protection implementation costs and design options for mitigating identified system vulnerabilities and ensuring successful system verification and validation.\n∑ Attack scenario testing: This can include a black box approach, pitting experienced reverse engineers with state-of-the-art attack tools against a system in a deployed setting to reveal vulnerabilities that cannot otherwise be found during most other evaluation exercises.\n∑ DPA side-channel analysis and mitigation: Side-channel attacks are currently the most practical method for compromising cryptography implementations. It is important to regularly perform measurable, objective, and repeatable testing for resistance to side-channel attacks for applications where adversaries have the ability to observe side channels (i.e., power draw, timing, EM emanations) during on-device cryptographic operations.\nIn today’s cyber hacking world, it is essential for every public and private organization to proactively address security issues. Embedded system designers can help their customers in this area by creating secure designs that are protected from today’s rapidly evolving threats, including those posed by a rapidly growing ecosystem of interconnected, user-accessible hardware.']	['<urn:uuid:318cff0a-1b8d-4001-bd3f-1997cf374863>', '<urn:uuid:6fd90e2b-d1fc-4ceb-b428-5c07dc11374d>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T05:28:56.360010	33	128	1769
74	what happens to baby gut bacteria at birth and benefits for immune system	At birth, babies are born without any bacteria in their intestines. The first major exposure to bacteria occurs during labor and birth, where the mother's gut and vaginal microbiota are transferred to the baby through the birth canal. After birth, breast milk provides beneficial bacteria (Lactobacilli and Bifidobacteria) which helps develop the baby's immune system and can prevent certain childhood diseases. This bacterial colonization in the first three years of life is crucial for developing a healthy microbiome.	['What Role Does the Microbiome Actually Play in Human Development?\nWe’ve posted many fascinating articles about the microbiome and its relationship to personal hygiene, obesity, antibiotics, mental health and much more. But let’s take a moment to go back to basics (and back to birth) to review how decisive the microbiome is in human development.\nFrom the moment you were born, your microbiome was growing up alongside (or rather, inside) you! Your mother and, if you were nursed, her breastmilk is where it all began. Scientists believe the first three years of life are the most crucial for our developing microbiomes. So instead of eating birthday cake, ring in baby’s first birthdays with a breastmilk-shake instead! OK, fine, with whipped cream and a cherry on top.\nIf you were taking a pop quiz on the basics of the microbiome, you would profit greatly from reading a recent article titled “Role of the Microbiome in Human Development”, published by Prof. Maria Gloria Dominguez-Bello, Filipa Godoy-Vitorino, Rob Knight, and Martin J Blaser (a New York University research professor). In this article, we’ll summarize the highlights of their paper, in which they take stock of all the known ways the microbiome impacts human development, beginning with 1) our evolution, 2) maternal/ fetal relationships, and 3) future perspectives on the microbiome’s impact on our nutrition and growth.\nThe Circle of Life: Evolution and Birth\nAs mentioned above, individual human development from birth onward involves microbiota, but did you consider that the entire human species development couldn’t have happened if it weren’t for microbiota? The microbiome co-evolved with humankind, and in turn shaped phenotypes in our ancestral lineage. Phenotypes refer to our actual physical characteristics: visible ones like height and eye color, but also overall health, disease history, and even our temperament! The authors put it thus, and how poetic it sounds!\nThe microbiota occupies the interface between our bodies and the exterior, and interactions with the environment (including diet, sun-light, bathing, cosmetics) cross this interface. The microbiota is at the same time self and non-self […].\nIf an interface is a point where two systems meet and interact, it may be helpful to think of microbiota as porters to an apartment building that is our body, keeping close tabs on who (or rather what) strolls in from outside.\nThe microbiota of the fetus’s mother affect the fetus indirectly\nNow let’s zoom back in to look at the microbiota of the individual. In mammals like humans, the womb is immune protected, which means bacteria cannot colonize inside it: the uterus, fetus, and placenta appear free of any microbiota. The microbiota of the fetus’s mother, however, affect the fetus indirectly. Labor and birth “mark the first major exposure to a complex microbiota and is the primordial mechanism for intergenerational microbiota transfer in mammals”. How does one generation (mother) pass microbiota on to the next (baby)? It’s all in the anatomy. The birth canal is adjacent to the rectal canal, which allows for transfer of the mother’s gut and vaginal microbiota to her child. For this reason, C-section delivery as well as any administered antibiotics during vaginal delivery alter bacterial colonization in the baby. The transfer doesn’t stop there: breast feeding, according to the authors, enables development of the sensory and motor capabilities of the brain in babies at a level that is nothing less than remarkable.*\nAnd After We’re Born?\nEarly human development doesn’t play out inside a bubble. Human density, home architecture, ventilation, diet, clothing, exercise, personal care products and medicines, but also where in the world you live all factor in. So how does one maintain a “healthy” microbiota for themselves and their children in this industrial urban world? The authors conclude by urging for more studies that assess how well the microbiome matures across a population of healthy individuals compared with disease states, in the same way that science has already charted norms for maturation of height and weight in children — “essentially a growth curve for the developing microbiome.” Only then can we learn associations between disturbances to the microbiota, host responses and diseases in the hopes of restoring them for our future generations.\n*To understand more on the development of a baby’s gut bacteria in the first three years of life, see article', 'We have all heard that probiotics may be beneficial to your digestive health, but what you may not know is that probiotics can also be beneficial to your baby. Your baby needs beneficial bacteria to develop a healthy immune system. Taking probiotics while breastfeeding has the potential to improve the health of your baby and prevent some childhood diseases.\nWhat are probiotics?\nProbiotics are types of bacteria that are similar to those normally present in our intestines (normal flora).  Many types of bacteria are harmful to your health and cause infections. However, normal flora are beneficial bacteria that help your body produce some vitamins, digest food, and eliminate other bacteria that can cause disease. Imbalances in the ratio of good bacteria to harmful bacteria in your intestines may lead to some health problems or diseases.  Supplementing with probiotics can help restore the proper balance to your digestive system. The use of probiotics is especially important if you have recently taken antibiotics because these medications eliminate bacteria, including your normal flora. This causes an imbalance in the bacteria in your intestines and can lead to infections.  While probiotics have been shown to be relatively safe, they can rarely cause dangerous infections in people with weakened immune systems, serious illnesses, certain medical conditions, or those who have recently had surgery. They may also be dangerous to babies who have illnesses or health problems. You can read more about probiotics here.\nDoes my baby have normal flora?\nYour baby is born without any bacteria in his or her intestines. Bacteria will start to populate in your baby’s digestive system after birth. The types of bacteria that reside in your baby’s intestines are dependent on his or her diet, environment, and your own normal flora.  Having an imbalance of beneficial bacteria during pregnancy can increase the risk that your baby will have allergic conditions after birth. An imbalance in your baby’s normal flora may contribute to colic and plays a role in diseases that occur in preterm babies, including blood infections and a condition that causes damage to the intestines.\nHow can the use of probiotics during breastfeeding benefit my baby?\nOne of the primary sources of beneficial bacteria for your baby’s intestines comes from breast milk.  It contains two types of bacteria that are commonly found in probiotics, Lactobacilli and Bifidobacteria. A study found that women who took antibiotics during pregnancy or breastfeeding had lower amounts of these beneficial bacteria in breast milk. This means that the use of antibiotics during these important time periods may deprive your baby of normal flora necessary for proper immune system development and lead to health problems in your baby. In fact, a study showed that babies whose moms used antibiotics during pregnancy had a higher risk of childhood asthma and complications of asthma. \nThe benefits of probiotic use in pregnancy and breastfeeding for preventing allergic conditions in babies has been shown in several studies. One study showed that in moms who already had allergic conditions, taking probiotics during pregnancy and breastfeeding lowered the risk that their babies would develop eczema during the first 2 years of life.  The World Allergy Organization (WAO) released guidelines in 2015 about the use of probiotics in pregnancy and breastfeeding to prevent allergic conditions in babies. You can read more about the WAO recommendations here.\nIn addition to the potential benefits in preventing allergic conditions, probiotics may help prevent colic in newborns. Babies with colic have been shown to have fewer beneficial bacteria (Lactobacilli and Bifidobacteria) in their intestines. Studies have found that giving breastfed babies probiotics can alleviate or prevent colic. [7,8] However, a few reports have indicated that giving probiotic supplements to babies caused blood infections. \nProbiotics from breast milk may also improve the frequency and volume of your baby’s stools and reduce the likelihood that your baby will develop infections. \nBreast milk provides the safest source of probiotics for your baby. Moms who take antibiotics during pregnancy or breastfeeding, or moms who are interested in maintaining a healthy balance of normal flora, should ask their doctor about supplementing with probiotics. Although it is considered unlikely that probiotics are transferred to your baby by breast milk, consuming probiotics can reestablish a healthy balance of normal flora to your intestines, which may restore Lactobacilli and Bifidobacteria to your breast milk. [5,10]\nWhat should I do if I want to take probiotics while breastfeeding?\nAs with all supplements and medications, you should always ask your doctor before using any probiotics during pregnancy or breastfeeding. Probiotic supplements are not regulated by the U.S. Food and Drug Administration (FDA). Therefore, the safety of these products during pregnancy and breastfeeding has not been extensively studied.\n- National Center for Complimentary and Integrative Health. Probiotics: In Depth. Updated October 2016. Accessed April 11, 2018.\n- WebMD. What are probiotics? Accessed April 11, 2018.\n- Soto A, Martín V, Jiménez E, Mader I, Rodríguez JM, Fernández L. Lactobacilli and Bifidobacteria in Human Breast Milk: Influence of Antibiotherapy and Other Host and Clinical Factors. J Pediatr Gastroenterol Nutr. 2014 Jul; 59(1): 78–88.\n- Sohn K, Underwood MA. Prenatal and postnatal administration of prebiotics and probiotics. Semin Fetal Neonatal Med. 2017;22(5):284-289.\n- Stensballe LG, Simonsen J, Jensen SM, Bønnelykke K, Bisgaard H. Use of antibiotics during pregnancy increases the risk of asthma in early childhood. J Pediatr. 2013 Apr;162(4):832-838.e3.\n- Rautava S, Kainonen E, Salminen S, Isolauri E. Maternal probiotic supplementation during pregnancy and breast-feeding reduces the risk of eczema in the infant. J Allergy Clin Immunol. 2012;130(6):1355-1360.\n- Anabrees J, Indrio F, Paes B, AlFaleh K. Probiotics for infantile colic: a systematic review. BMC Pediatr. 2013;13:186.\n- Indrio F, Di Mauro A1, Riezzo G, et al. Prophylactic use of a probiotic in the prevention of colic, regurgitation, and functional constipation: a randomized clinical trial. JAMA Pediatr. 2014;168(3):228-233.\n- Dani C, Coviello C, Corsini I, Arena F, Antonelli A, Rossolini AM. Lactobacillus Sepsis and Probiotic Therapy in Newborns: Two New Cases and Literature Review. AJP Rep. 2016 Mar; 6(1): e25–e29.\n- Elias J, Bozzo P, Einarson A. Are probiotics safe for use during pregnancy and lactation? Can Fam Physician. 2011 Mar; 57(3): 299–301.']	['<urn:uuid:c99e3e8c-5184-4d8f-8dbe-6e4a7a30fab0>', '<urn:uuid:5c2fe1ca-e3f3-4d77-a785-672eced93eee>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T05:28:56.360010	13	78	1737
75	guitar parts assembly sequence	Guitar assembly begins with building the body first, installing blocks and linings into the sides held in a mold. The braced top and back are then attached. For the neck assembly, a mahogany blank is cut, routed for truss rod, and fitted with a peghead. The fretboard is shaped, slotted and bound. Final steps include applying finish, installing frets, mounting tuners, attaching the bridge, and stringing up the instrument.	['Our Martin acoustic kit build continues, as we level the kerfing, join the back plates and pop a little something in the oven. Huw Price shapes up…\nFor the first part of our Martin kit build last month, I created a body former for the sides of my 000 and started gluing the heel and tail blocks in place, and now it’s time to get things looking a bit more guitar-like.\nNow, one thing I’ve learned the hard way in my time building and repairing instruments is the absolute necessity of keeping things square and centred. This is especially true of the heel block, and if you allow it to become tilted or twisted, setting the neck angle and even ending up with a playable guitar will be that much harder.\nEven when the rims are lined up nicely in the mould I created, there’s nothing to prevent them from getting out of shape from pressure when gluing the top and the back. Most professional acoustic builders use two spreaders to hold everything in position while the body assembly takes place. One is used lengthways between the heel and tail blocks, and the second goes side to side between the innermost curves of the waist.\nThis easy to make spreader set will hold the sides and blocks square in the mould throughout the body building process\nAlthough you can buy spreaders from luthier suppliers, you can also make your own. I saw off some more blocks from the length of timber I used to create the mould blocks last time out and buy a length of threaded metal rod with some nuts and washers. You can find these items in most DIY outlets.\nHoles marginally greater in diameter than the threaded rod – but narrower than the washers – are then drilled into the spreader blocks, and the rod is then cut into suitable lengths. With the nuts and washers in position, the blocks are then slipped over the ends of the rods and the nuts adjusted to press the spreader blocks against the sides.\nThe idea is that the spreaders will only be removed once the top and back have been glued on. So, I double up the blocks at each end between the heel and tail blocks in order to keep the longitudinal rod’s length to a minimum. This is important because if it’s too long, you might be unable to remove the rod through the soundhole.\nNothing fancy is needed for gluing the kerfing and a set of clothes pegs works just fine\nAs the sides are barely 2mm thick, there’s insufficient width to achieve a satisfactory glue joint for the top and back plates. To increase the gluing surface area, strips of kerfing are fixed around the edges of the rims. Various timbers are used, but Martin’s appear to be mahogany. The strips are tapered and sawed halfway through at 7.5mm intervals, which allows them to bend easily and conform to the guitar’s curves.\nGluing kerfing is quick and easy, but I find it best to divide each quarter into three sections. The one tricky area is the waist because, with OM and 000 shaped guitars, the curve is too pronounced for the kerfing and it tends to snap. Try heating the kerfing before bending it around the waist – a paint stripper gun or hairdryer should work fine. This softens and loosens the fibres, which makes the kerfing more pliable and less likely to snap.\n220 grit sandpaper fixed to a flat plank with double-sided tape is used to level the kerfing with the rims\nApply glue to the flat side of the kerfing and position it against the side, leaving it slightly above the rims all the way around. Believe it or not, clothes pegs are ideal for clamping the kerfing in position while the glue dries. You can also use bulldog clips as an alternative if you wish.\nHaving applied kerfing around the rear edges of the body, I use a file to remove some of the kerfing’s excess height. Then I take a dead flat length of timber with 220 grit paper attached using double sided tape. This soon brings the kerfing dead level with the rims while keeping everything flat and square.\nMany modern builders prefer angling the edge gluing surfaces to conform to the curvature of the front and back plates, but Martin’s traditional method is to level the kerfing at a right angle to the sides. Huss & Dalton offer both styles, with the Martin method deemed to generate more bass, power and projection, but less midrange. In contrast, angled kerfing is said to produce a more even frequency balance.\nThe glue squeeze out is cleaned, the inside edges are sanded and the levelled kerfing is flat against the backing board\nI decide on flat kerfing, because this is a Martin kit and I do appreciate traditional Martin tone. The first strips of kerfing are glued with clamps holding the rims square, but I install the spreaders when I flip the rims over to do the front side. With the body in the mould I carefully remove any glue squeeze out from the kerfing and sand the inner surfaces of the sides. Mahogany is a lovely wood to work with, and before long the sides look clean and tidy.\nTruing the edges\nJointing the back plates demands some precision woodworking but specialist tools are not required. In a process known as candling the plates are brought together on a large window. When you look carefully at the join, you will see light shining through gaps at various places along the length. You often need to adjust your viewing angle to see them, but gaps are always there. Back in the day this was done by candlelight – hence the term – however daylight works, too. This is done because all gaps must be eliminated to ensure a strong and long-lasting glue joint.\nWith the back plates brought together against a flat piece of glass, daylight can be seen through the gap\nTo eliminate the gaps you’ll need to make a shooting board. Two flat pieces of 18mm MDF or plywood will do fine. Ensure the lengths are equal and longer than the body length of the guitar. The width of one can be less than the other, or you can offset them when you glue one on top of the other. The idea is to create two levels, so the offset pieces of equal size will yield two shooting boards.\nTake the back plates and place one on top of the other so that both inner surfaces are facing outwards. Place the back plates on the higher tier of your shooting board with the joint edges overlapping the lower tier then line them up exactly before clamping the plates in position at each end. The traditional way of trueing the edges is to run a long jointer plane along the length. The plane is positioned with its side surface flat on the shooting board’s lower tier and the blade should be set for the finest possible cut. Block and jack planes are really too short for this job.\nThis shooting board has seen a lot of use, but it’s still going strong and it only takes minutes to make one yourself\nIf the shavings are so thin that they turn to powder when you rub them between your fingers, the blade depth is about right. The blade must also be set dead square so an even amount of wood is removed across both joint surfaces. My advice would be to practise on scraps of timber to set the plane correctly before moving onto the back plates. Make two or three passes then unclamp the plates to check your process using the candling method. It’s vital to have perfectly flat glass and I can report from experience that Edwardian and Victorian glass is not flat!\nIf you’re inexperienced with planes, or you don’t own one, you can use sandpaper instead, but you’ll still need that shooting board. The abrasive must be attached to a dead flat surface – like a carpenter’s level or even a jointing plane with its blade fully retracted – with double sided tape. Since we’re dealing with small fractions of a millimetre here, I’d suggest 320 grit.\nThe jointing process reaches its final stages as we move from using the plane to sandpaper\nI’ve never gotten this quite perfect by using a plane alone. This occasion is no exception, and it’s only when I swap to sandpaper that I finally manage to close the gaps. Once you’re in the ballpark, you’ll probably see gaps in certain areas along the joint. Try marking these areas with a pencil to use as a guide when you’re removing material from the adjacent high spots.\nTowards the final stages you may only need one or two passes with the sandpaper. Taking the plates on and off the shooting board can get tedious, and if you’re doing this for the first time, don’t be discouraged if it takes you a few hours to get it right. This bit can be tough no doubt, but do persevere, because you’ll get there eventually.\nOne plate is propped up and strips of binding tape are stretched across the join line leaving small gaps at the centre\nGluing the plates\nThere are various ways to glue the back plates together, but this particular procedure is slightly complicated because the plates have already been shaped. After researching online, I discover Chris Paulick’s tape method and I decide to give it a try. For this you’ll need a backing board with a plastic surface or a plastic tape covering to prevent the plates sticking to it. Carefully align the plates and prop one of them up by about 5cm with the other lying flat. You may need to tape the flat piece to the backing board to prevent it sliding.\nTake some tape that has some degree of stretchiness – Rothko & Frost’s binding tape works just fine. Pre-cut about 20 or so 5cm strips then place them at 2cm intervals all along the join line. The crucial bit is that you must stretch them across leaving a slight gap above the join line.\nCarefully bring the plates back together with the tape strips on the inside and lightly clamp them with the joint side up in a vice or workbench. Run a line of glue evenly all along one edge, then remove the plates from the clamp. Place the plates on the backing board tape side down. If the tape has been applied correctly, the plates will be raised up in the centre like a tent.\nThe plates are pressed flat with the tape strips on the underside then weighed down and clamped overnight as the glue dries\nNow press the centre line flat against the board and the stretchiness of the tape should pull the plates together. Ensure the plates are flat and properly aligned all along the glue line then wipe off the glue squeeze out with a damp rag. Place a flat plank of wood over the join line and place a heavy weight on top. I use a concrete block and protect the plank with a layer of plastic packing tape. Since the block is shorter than the body, I also clamp the ends.\nFor this job I decide on Titebond original glue. It’s an aliphatic resin that is very popular with guitar builders and it has a much longer open time than hide glue. The plates are left for 24 hours before I remove the clamps and the block and I’m pleased with the way it turned out.\nThe tape method works, it costs next to nothing and it’s an ideal solution for DIY guitar building. I would suggest one or two practice runs before gluing up – but bear in mind that the tape strips will lose some of their stretchiness, so they’ll need to be renewed each time. Good preparation is also key, so make sure your wet cloth, plank, claps and weight are all close at hand before you start.\nThe kit included a black centre strip but we prefer the no-line look and the plates matched up pretty well without it\nNo doubt this next part will have some experienced guitar builders rolling their eyes but, having been greatly impressed by the tonal properties torrefied timbers on guitars I’ve played of late, I decide to attempt it myself. Much as I would have liked to torrefy the top plates, I worry that the heat is going to compromise the glue joint. However, the braces seem like fair game and, after talking it over with Alister Atkin, I decide I’ll follow his procedure by baking them at 100 degrees centigrade.\nMy main concern is that the wood might twist a bit when they go into the oven, so I wire up the two back braces tightly against a perforated metal plate – in this case baguette moulds fit the bill nicely. After 90 minutes in the oven the cooled braces look very slightly darker than the non-baked braces.\nTwo of the back braces are wired tightly before sitting in a 100-degree oven for 90 minutes\nTo my relief they hold their shape, have a noticeable ping when tapped and rustle loudly when being handled. Suitably encouraged, I go ahead and bake the remaining braces. Of course this isn’t proper torrefaction as such, but it is good fun!\nWhen we check back with the Martin in a couple of months time, we’ll move on to bracing. It will also be the time when I’ll have to decide whether to brace the top in the modern or pre-war style. A genuine 1930s Martin would certainly help with the measurements – so watch this space because you never know what might turn up…', '- Rig Rundowns\n- Premier Blogs\nWith a very steady hand, luthier Jeff Huss preps a fretboard for its side-position markers.\nBuilding an acoustic guitar may seem like an extremely complicated process, but it can be broken down into a series of chronological steps. While it would take a book (and there are some good ones out there) to describe how to create this fairly simple, yet elegant machine in more detail, I’d like to give you an overview of the process with this month’s column.\nBecause we build our bodies first, we start by dimensioning the wood for our top, back, and sides. Choosing the top wood for its stiffness-to-weight ratio, we generally make our tops from spruce or cedar. This means these woods are light enough to not act like a “tone sink,” but strong enough to withstand the incredible amount of tension they will be subjected to when the guitar is strung up. While back and side woods color the tone that the top produces, the majority of the sound is the result of the construction choices made for the top.\nThe top and back “plates” are thickness- sanded to dimension after they have been joined in a book-matched pattern. We cut a soundhole into the top, install a rosette, and then profile them—a process that creates a smooth, flat surface where the top meets the rosette. Next, we brace the plates with spruce bracing. We use a traditional “X” pattern for our tops and a simpler, ladder-brace pattern for the backs. At this point, we profile the sides to a uniform thickness and bend them to shape using a bending jig and the help of a little heat and moisture.\nAfter bending the sides, we place them into a mold that will house the body throughout the construction process, and install spreaders to hold the sides firmly against our mold. Now we’ll glue the head block, tail block, and kerfed linings into place to create the rim onto which we glue the top and back. When the glue is cured, we remove the spreader mechanism through the soundhole, pull the rough body from the mold, trim the edges, and finish the decorative purfling and binding work. After the scraping and sanding work is finished, the body is complete and ready to be paired with a neck.\nNecks begin as a large plank of hardwood— generally mahogany—that is sawn into smaller billets yielding two, one-piece necks. These are rough sawn into neck blanks on the band saw, routed for a truss rod slot, and cut on an angle for the peghead. After we glue the veneer for the peghead face and shape the peghead, we drill the holes for the tuners. Using a jig, we then measure the angle of the body where the neck will attach, and that angle is transferred to the table saw where the neck heel is cut to correspond to the body. After cutting, we drill and install our corresponding index holes, bolt holes, and inserts in the body and neck heel for the bolt-on neck joint.\nOur fretboards—made from ebony or rosewood—are shaped, slotted, radiused, and bound. They are indexed onto the neck blank, and the truss rod is installed and then attached with epoxy. The neck is now shaped, a nut slot is cut, and the neck is final-fitted to the body to ensure proper playability. Inlay work is then done on the fretboard and peghead, and we fit the nut, install side-position markers, and sand out the body and neck and prepare them for the finish work.\nAt this point, some woods, such as mahogany, are stained, and any open-pored woods are filled with a paste wood-filler that lets us apply a thinner, final finish. Then we spray the topcoats and do leveling work between coats to ensure a flat and thin finish. After it’s cured, the finish is leveled once again before we fine-sand and buff all the finished surfaces to bring out a bright luster.\nOnce the finish work is complete, the assembly can begin. The neck and body are bolted together, the fretboard extension is glued down, and the bridge is glued on. The guitar is mounted on a workbench that simulates string tension, and the fretboard is prepared for the fretwork. Frets are then installed, trimmed, leveled, and polished.\nAfter the nut has been trimmed, polished, and slotted, we install it. Next, the tuners are mounted, the saddle is shaped and inserted into the bridge slot, and the pickguard is attached. And finally, we string up the guitar, adjust the action, and perform any necessary tweaks. Our guitar is now ready to begin what we hope are many, many years of making music.\nco-owner of Huss & Dalton Guitar Company, moved to Virginia in the late ’80s to play bluegrass. He and his business partner, Mark Dalton, formed their company in 1995. Since then they’ve earned world-wide recognition for their high-end, boutique guitars and banjos.']	['<urn:uuid:e9b93526-0015-4ecb-a4fa-1797c196bd26>', '<urn:uuid:ab0f46eb-8b48-417b-811a-cd95b868af4e>']	factoid	direct	short-search-query	distant-from-document	three-doc	novice	2025-05-13T05:28:56.360010	4	69	3152
76	What types of cases typically require computer forensics evidence?	Computer forensics evidence is used in criminal cases involving homicides, financial fraud, drug records and child pornography. It's also used in civil litigation for fraud, divorce, discrimination and harassment cases. Additionally, it's valuable for investigating cyber crimes like intellectual property theft, hacking, and electronic discovery (e-discovery) for both criminal and civil legal cases.	['This week was about digital forensics, the collection of evidence from electronic devices, usually for reasons related to cyber crime.\nMajor cases of cyber crime that involve digital forensics include: fraud, intellectual Property Theft, hacking, and electronic discovery(e-discovery), which is the process where data is sought and secured with the intent of use for a criminal or civil legal case.\nForensics in a Nutshell\nThere are 3 broadly classified categories in forensic computing:\n- Live forensics\n- Post-mortem based forensics(analysis after the fact)\n- Network based forensics\nThere are standards and methods of data collecting that must always be followed:\nMinimize data loss, record everything, analyze all data colected(evidence) and report findings.\nEvidence is anything that can be used to prove or disprove a claim. In forensics evidence can be found networks, operating systems, databases and apps, and removable media, such as disks and USBs. Admissible evidence is what courts accept as legitimate.\nPreserving the Evidence\nWhen handling evidence, it is crucial to perform procedures as so:\n- Create a cryptographic hash of the entire disk and each partition before analyzing. A cryptographic hash is used to verify the integrity of a file. It can be used to tell if a file has been tampered or changed. Popular forms of hashing include MD5 of SHA1.\n- Create bit-images of hard drives and analyze them. This means to create a bit for bit copy of the hard drive, so they will be identical.\n- Lock the original disk in a limited access room or container. This is to keep the disk safe from any outside influence from tampering with evidence.\nWhat to acquire when looking for evidence:\n- Virtual and Physical\n- Entire physical drive\n- Logical: a partition\n- Network Traffic:\n- Full packet captures\nLocard’s Exchange Principle\nWe learned about this principle this week, which states that when any two objects come in contact, there will be a transfer of material from each object onto the other. The main point is that it is impossible to interact with a system without affecting it in some way.\nLocard’s principle is not a digital forensic principle exclusively, it also applies to real life crimes.\nData can be volatile, which means it can be easily lost. There are degrees of volatility, and the most volatile must be acquire first, or be lost forever. Data that can be lost on powering down, is an example of the most volatile.\nAn example of the acquisition of data on Windows would be to acquire volatile data first, then non-volatile, which includes event logs and registry, if applicable. Lastly, obtain any relevant files, such as unknown executables, and any leftover tools.\nIn a computer, RAM, or random access memory, is the source of short term memory on a computer. Once a computer is powered down, the information will begin to rapidly decay and be lost. There is tons of useful info that can be found in the RAM, such as data that is not obtained from a hard drive, as well as any leftover artifacts ‘hidden’ by hackers. Some examples of such data are processes at the time of memory snapshot, device drives, loaded modules and DLL’s, keystrokes, and wireless keys, among other things.\nHow does physical memory work? It is divided into “pages” and allocated space onto the physical memory page by page. Same pages of memory can appear in different locations and can be moved from physical memory into a page file to make more space. A page file is used to store data that RAM can’t hold.\nWe were introduced to some tools for analyzing data, one of them was Volatility, which is a memory forensics framework, which can be used to write and create plugins, on top of a lot free and useful tools that are available.\nYara is a tool that can create signatures for malicious behavior, which can then be used to scan for that malicious behavior.\nChristiaan showed us a tool name FTK Imager(Forensic Tool Kit). He made a point to never install forensic tools on suspect machines, calling it “the worse case thing you can do” because it can influence evidence, also known as Locard’s Principle.\nOne of the main functions of this program is to capture the memory from the computer of interest. It can create an image of a disk, USB stick, or capture RAM memory.\nChristiaan says he prefers using the command line to do that same thing, but FTK is free and has a good GUI for learning. A drawback is that it leaves a large fingerprint in the memory.\nCommand line based forensic tool. A cheatsheet with various commands was provided:\nRan with command:\nvolatility.exe -f<name of memory dump> plugin\nAn extra tag, “imageinfo” can be used if you want to know more about the memory you want to analyze. He also used “psscan” to find hidden or terminated processes. “dlllist” is used to show .dll’s used by a process.\nAn image recovery tool. Christiaan walked us through the process of mounting a virtual disk and recovering photos from it. This is called “carving”.\nThis week was about the collection of data and evidence in a safe manner that preserves the integrity of the original data. It is important to present the facts in an unbiased manner, so we learned how to collect without leaving traces of tampering.', 'Computer Forensics and Data Recovery\nComputer forensics and Data Recovery is the science of analyzing a computer system hard disk using forensically sound methods and tools that have been tested and have had publications released such as the Department of Justice, NIST, Homeland Security and other approved forensic associations. The computer examination and analysis strategies might vary from case to case depending upon the evidence that is being attempted to be discovered to establish legal proof of the examination for legal cases. Computer Forensics and Data recovery can be utilized in a wide range of computer system criminal activity or abuse, consisting of but not limited to theft of data, theft of or violation of copyrights, and fraudulence. Computer experts could draw on a collection of approaches for uncovering data that resides in a computer system, and recovering deleted, secured, or damaged file details.\nBENEFITS OF PROFESSIONAL Computer system Forensics and Data Recovery\nThe unbiased computer Forensics professional that helps during a legal case will have experience on a large range of computer and software applications. This is constantly beneficial when your case has hardware and software applications with which this specialist is directly knowledgeable. Key computer components and software application execution is commonly comparable from one technology to an another, in which experience in one application or operating system area is frequently quickly transferable to a brand-new technology in a computer operating system.\nUnlike paper proof, computer system evidence can typically exist in numerous types, with earlier variations still easily accessible on a computer system disk. Understanding the possibility of their existence, also alternative formats of the same information can be uncovered. The discovery procedure could be offered well by a knowledgeable specialist recognizing additional opportunities that could be requested as perhaps relevant proof. Moreover, throughout on-site premises assessments, for situations where computer disks are not actually taken or forensically copied (see listed below), the forensics specialist could more quickly identify areas to look, indications to look for, and added information resources for relevant evidence. These could take the form of earlier versions of data documents (eg. memos, spreadsheets) that still exist on the computer’s disk or on backup media, or differently formatted versions of data, either developed or addressed by various other application software programs (eg. data processing, spreadsheet, email, timeline, organizing, or graphic).\nPreservation of data from changing is critical in computer forensic examinations. An experienced computer forensics professional will ensure that a subject computer system is carefully handled, documented to ensure that:\n- No feasible evidence is deleted, changed, or otherwise endangered by the procedures used to examine the computer system.\n- No feasible computer virus is presented to a subject computer system throughout the examination process.\n- Extracted and potentially relevant evidence is properly handled and safeguarded from later mechanical or electromagnetic damage.\n- A proceeding chain of custody is developed and kept.\n- Company procedures are documented for each case.\n- Any client-attorney details that is inadvertently discovered during a forensic examination is ethically and legitimately not disclosed.\nACTIONS TAKEN BY COMPUTER SYSTEM FORENSICS SPECIALISTS\nThe computer forensics professional will certainly take a number of mindful steps to determine and attempt to recover feasible evidence that could existing on a subject computer system:.\n- Secures the subject computer system during the forensic examination from any type of possible modification, damage, data corruption, or infection intro.\n- Discovers all files on the subject system. This includes existing typical documents, removed yet continuing to be archived documents, concealed data, password-protected documents,.and encrypted files.\n- Recovers all (or as much as possible) of discovered deleted files.\n- Reveals (to the extent possible) the contents of hidden files as well as temporary or swap files used by both the application programs and the operating system.\n- Accesses (if possible and if legally appropriate) the contents of protected or encrypted files.\n- Analyzes all possibly relevant data found in special (and typically inaccessible) areas of a disk. This includes but is not limited to what is called ‘unallocated’ space on a disk (currently unused, but possibly the repository of previous data that is relevant evidence), as well as ‘slack’ space in a file (the remnant area at the end of a file, in the last assigned disk cluster, that is unused by current file data, but once again may be a possible site for previously created and relevant evidence).\n- Prints out an overall analysis of the subject computer system, as well as a listing of all possibly relevant files and discovered file data. Further, provides an opinion of the system layout, the file structures discovered, any discovered data and authorship information, any attempts to hide, delete, protect, encrypt information, and anything else that has been discovered and appears to be relevant to the overall computer system examination.\n- Provides expert consultation and/or testimony, as required.\nWHO CAN USE COMPUTER FORENSIC EVIDENCE?\nMany types of criminal and civil proceedings can and do make use of evidence revealed by computer forensics specialists:\n- Criminal Prosecutors use computer evidence in a variety of crimes where incriminating documents can be found: homicides, financial fraud, drug and embezzlement record-keeping, and child pornography.\n- Civil litigations can readily make use of personal and business records found on computer systems that bear on: fraud, divorce, discrimination, and harassment cases.\n- Insurance Companies may be able to mitigate costs by using discovered computer evidence of possible fraud in accident, arson, and workman’s compensation cases.\n- Corporations often hire computer forensics specialists to ascertain evidence relating to: sexual harassment, embezzlement, theft or misappropriation of trade secrets and other internal/confidential information.\n- Law Enforcement Officials frequently require assistance in pre-search warrant preparations and post-seizure handling of the computer equipment.\n- Individuals sometimes hire computer forensics specialists in support of possible claims of: wrongful termination, sexual harassment, or age discrimination.']	['<urn:uuid:073d0e74-8967-47f1-8222-08cff6608ae7>', '<urn:uuid:c82ba33d-be8b-44ed-b168-265775e028b1>']	factoid	with-premise	concise-and-natural	similar-to-document	three-doc	expert	2025-05-13T05:28:56.360010	9	53	1866
77	define willful failure pay taxes irs	Courts have defined 'willful' in this context as voluntary, conscious, and intentional decisions not to remit properly withheld funds to the Government. According to Kizzier v. United States, a responsible person acts willfully if they know or intend that their conduct will result in withheld employment taxes not being paid to the government but used for other purposes.	['When seeking to collect certain withholding taxes, the Internal Revenue Service will attach any individual who may be responsible, in any manner, for the collection and turning over of said tax. This can be quite the wide net cast by an entity whose resources are effectively limitless and who does not have much incentive to back off of their collection attempts. Because of this, it is important to know the test which the IRS will use to determine whether an individual is liable for the certain tax that is owed.\nTo start, the Internal Revenue Code, at section 6672, imposes the following duty:\n“Any person required to collect, truthfully account for, and pay over any tax imposed by this title who willfully fails to collect such tax, or truthfully account for and pay over such tax, or willfully attempts in any manner to evade or defeat any such tax or the payment thereof, shall, in addition to other penalties provided by law, be liable to a penalty equal to the total amount of the tax evaded, or not collected, or not accounted for and paid over.”\nTwo conditions must be met before an individual can be held liable under this provision: (1) she must be responsible for the collection and payment of withholding taxes, and (2) she must willfully fail to collect and pay them over. See Teel v. United States, 529 F.2d 903, 905 (9th Cir. 1976); Pacific National Ins. V. United States, 422 F.2d 29 (9th Cir. 1976). The test of responsibility under section 6672 is a functional one, which focuses upon the degree of control and influence which the individual exercised over the financial affairs of the corporation and, more specifically, over the disbursement of funds. See Taubman v. United States, 499 F.Supp. 1133 (E.D. Mich. 1978). Liability attaches to those with power and responsibility within the corporate structure for seeing that the taxes withheld from various sources are remitted to the Government. Scott v. United States, 354 F.2d 292, 296; see also Gefen v. United States, 400 F.2d 476, 482 (5th Cir. 1968), cert. den’d, 393 U.S. 1119, 89 S.Ct. 990, 22 L.Ed.2d 123.\nThe Court in Benoit v. Commissioner of Revenue specifically sets out indicia of responsibility as follows:\n1) ID of officers, directors & stockholders;\n2) Ability to sign checks on behalf of the corporation;\n3) ID of individual who hires and fires employees;\n4) ID of individual who was in control of financial affairs; and,\n5) Those with an entrepreneurial stake in the corporation.\nBenoit v. Commissioner of Revenue, 453 N.W.2d 336, 344 (Minn. 1990). Federal law treats the person with effective power to pay the tax as the “responsible person.” Howard v. United States, 711 F.2d 729, 734 (5th Cir. 1983). Courts read the term “responsible person” expansively. O’Callaghan v. United States, 943 F.Supp. 320, 324 (S.D.N.Y. 1996). An “employee with the power and authority…to direct the payment of taxes is a responsible person within the meaning of section 6672.” Feist v. United States, 221 Ct.Cl. 531, 607 F.2d 954, 960 (1979).\nIn the responsible person analysis, the answer often pivots on whether the person had power to make tax payments in light of the enterprise’s financial organization and decision-making structure. O’Connor v. United States, 956 F.2d 48, 51 (4th Cir.1992). This is fact-intensive; in some instances, employees who perform clerical functions of collecting and paying taxes are not responsible persons. Feist, 607 F.2d at 957, 960.\nWhile that sums up the responsible person analysis, what of the willful person? A number of courts have addressed the “willful” component of section 6627. These courts have defined the term “willful” in this context to mean voluntary, conscious and intentional (as opposed to accidental) decisions not to remit funds properly withheld to the Government. Spivak v. United States, 370 F.2d 612, 615 (2d Cir. 1967), 499 F.2d 90, 94 (4th Cir. 1962); Hewitt v. United States, 377 F.2d 921, 924, 22 A.L.R.3d 1 (5th Cir. 1967); Flan v. United States, 326 F.2d 356, 358 (7th Cir. 1964); Bloom v. United States, 272 F.2d 215, 223 (9th Cir. 1959). The Court in Kizzier v. United States stated that\n“A responsible person acts willfully within the meaning of [section] 6672 if he acts in such a manner that he knows or intends that, as a consequence of his conduct, withheld employment taxes belonging to the government will not be paid over but will be used for other purposes.”\nKizzier v. United States, 598 F.2d 1128, 1132 (CA 8 1979).\nTherefore, to be held liable for certain withholding taxes not withheld, the individual must both be responsible (i.e. be required to withhold and pay over certain taxes) as well as willful (i.e. intentionally carry out conduct that brings about a certain known consequence). Failing to meet one of these criteria will alleviate an individual from the penalties imposed by the IRS concerning withholding.\nInternal Revenue Service Circular 230 Disclosure: In compliance with IRS requirements, you are on notice that any U.S. tax advice contained in this communication (including any attachments) is not intended or written to be used, and cannot be used, for the purpose of (i) avoiding penalties under the Internal Revenue Code or (ii) promoting, marketing or recommending to another party any transaction or matter addressed herein.']	['<urn:uuid:b81d1d3f-a7b5-4b68-b15a-6142674a84e2>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T05:28:56.360010	6	58	882
78	northeast states electoral votes popular vote outcomes	In the Northeast region's nine states (Maine, New York, New Jersey, Vermont, Massachusetts, Rhode Island, Connecticut, New Hampshire, and Pennsylvania), there have been notable instances where the Electoral College system's outcomes differed from the popular vote. To win the presidency, a candidate needs 270 electoral votes out of 538 total. The Electoral College system has sometimes led to split outcomes in the Northeast - for example, in 2016, Pennsylvania voted for Trump despite the region's generally strong Democratic lean. Most Northeastern states follow a winner-takes-all system for electoral votes, though Maine has implemented a proportional allocation system. Since the 1990s, the region has predominantly voted Democratic in presidential elections, with only rare Republican victories like New Hampshire in 2000 and Pennsylvania in 2016.	"['The Northeastern United States, also referred to as the American Northeast or simply the Northeast, is a geographical region of the United States bordered to the north by Canada, to the east by the Atlantic Ocean, to the south by the Southern United States, and to the west by the Midwestern United States. The Northeast is one of the four regions defined by the United States Census Bureau for the collection and analysis of statistics.\nThe Census Bureau-defined region has a total area of 181,324 sq mi (469,630 km2) with 162,257 sq mi (420,240 km2) of that being land mass. Although it lacks a unified cultural identity, the Northeastern region is the nation\'s most economically developed, densely populated, and culturally diverse region. Of the nation\'s four census regions, the Northeast is the second most urban, with 85 percent of its population residing in urban areas, led by the West with 90 percent.\nGeographically there has always been some debate as to where the Northeastern United States begins and ends. The vast area from northern Virginia (outside Washington DC) to northern Maine, and from western Pennsylvania (Pittsburgh) to the Atlantic Ocean, have all been loosely grouped into the Northeast at one time or another. Much of the debate has been what the cultural, economic, and urban aspects of the Northeast are, and where they begin or end as one reaches the borders of the region.\nUsing the Census Bureaus definition of the northeast, the region includes nine states: they are Maine, New York, New Jersey, Vermont, Massachusetts, Rhode Island, Connecticut, New Hampshire, and Pennsylvania.[a] The region is often subdivided into New England (the six states east of New York) and the Mid-Atlantic states (New York, New Jersey, and Pennsylvania). This definition has been essentially unchanged since 1880 and is widely used as a standard for data tabulation. However, the Census Bureau has acknowledged the obvious limitations of this definition and the potential merits of a proposal created after the 1950 census that would include changing regional boundaries to include Delaware, Maryland, and Washington, D.C. with the Mid-Atlantic states, but ultimately decided that ""the new system did not win enough overall acceptance among data users to warrant adoption as an official new set of general-purpose State groupings. The previous development of many series of statistics, arranged and issued over long periods of time on the basis of the existing State groupings, favored the retention of the summary units of the current regions and divisions."" The Census Bureau confirmed in 1994 that it would continue to ""review the components of the regions and divisions to ensure that they continue to represent the most useful combinations of States and State equivalents.""\nMany organizations and reference works follow the Census Bureau\'s definition for the region; however, other entities define the Northeastern United States in significantly different ways for various purposes. The Association of American Geographers divides the Northeast into two divisions- ""New England"", which consists of Maine, Vermont, New Hampshire, Massachusetts, Rhode Island and Connecticut, and the ""Middle States"", which consists of New York, Pennsylvania, New Jersey, and Delaware. Similarly, the Geological Society of America defines the Northeast as these same states but with the addition of Maryland and the District of Columbia. The narrowest definitions include only the states of New England. Other more restrictive definitions include New England and New York as part of the Northeast United States, but exclude Pennsylvania and New Jersey. States beyond the Census Bureau definition that other entities include in the Northeast United States are:\nAnthropologists recognize the ""Northeastern Woodlands"" as one of the cultural regions that existed in the Western Hemisphere at the time of European colonists in the 15th and later centuries. Most did not settle in North America until the 17th century. The cultural area, known as the ""Northeastern Woodlands"", in addition to covering the entire Northeast U.S., also covered much of what is now Canada and others regions of what is now the eastern United States. Among the many tribes that inhabited this area were those that made up the Iroquois nations and the numerous Algonquian peoples. In the United States of the 21st century, 18 federally recognized tribes reside in the Northeast. For the most part, the people of the Northeastern Woodlands, on whose lands European fishermen began camping to dry their codfish in the early 1600s, lived in villages, especially after being influenced by the agricultural traditions of the Ohio and Mississippi valley societies.\nAll of the states making up the Northeastern region were among the original Thirteen Colonies, though Maine, Vermont, and Delaware were part of other colonies before the United States became independent in the American Revolution. The two cultural and geographic regions that form parts of the Northeastern region have distinct histories.\nThe first Europeans to settle New England were Pilgrims from England, who landed in present-day Massachusetts in 1620. The Pilgrims arrived by the Mayflower ship and founded Plymouth Colony so they could practice religion freely. Ten years later, a larger group of Puritans settled north of Plymouth Colony in Boston to form Massachusetts Bay Colony. In 1636, colonists established Connecticut Colony and Providence Plantations. Providence was founded by Roger Williams, who was banished by Massachusetts for his beliefs in freedom of religion, and it was the first colony to guarantee all citizens freedom of worship.Anne Hutchinson, who was also banished by Massachusetts, formed the town of Portsmouth. Providence, Portsmouth, and two other towns (Newport and Warwick) consolidated to form the Colony of Rhode Island and Providence Plantations.\nAlthough the first settlers of New England were motivated by religion, in more recent history, New England has become one of the least religious parts of the United States. In a 2009 Gallup survey, less than half of residents in Vermont, New Hampshire, Maine, and Massachusetts reported religion as an important part of their daily life. In a 2010 Gallup survey, less than 30% of residents in Vermont, New Hampshire, Maine, and Massachusetts reported attending church weekly, giving them the lowest church attendance among U.S. states.\nNew England played a prominent role in early American education. Starting in the 17th century, the larger towns in New England opened grammar schools, the forerunner of the modern high school. The first public school in the English colonies was the Boston Latin School, founded in 1635. In 1636, the colonial legislature of Massachusetts founded Harvard College, the oldest institution of higher learning in the United States.\nThe first European explorer known to have explored the Atlantic shoreline of the Northeast since the Norse was Giovanni da Verrazzano in 1524. His ship La Dauphine explored the coast from what is now known as Florida to New Brunswick. Henry Hudson explored the area of present-day New York in 1609 and claimed it for the Netherlands. His journey stimulated Dutch interest, and the area became known as New Netherland. In 1625, the city of New Amsterdam (the location of present-day New York City) was designated the capital of the province. The Dutch New Netherland settlement along the Hudson River and, for a time, the New Sweden settlement along the Delaware River divided the English settlements in the north and the south. In 1664, Charles II of England formally annexed New Netherland and incorporated it into the English colonial empire. The territory became the colonies of New York and New Jersey. New Jersey was originally split into East Jersey and West Jersey until the two were united as a royal colony in 1702.\nPenn established representative government and briefly combined his two possessions under one General Assembly in 1682. However, by 1704 the Province of Pennsylvania had grown so large that their representatives wanted to make decisions without the assent of the Lower Counties and the two groups of representatives began meeting on their own, one at Philadelphia, and the other at New Castle. Penn and his heirs remained proprietors of both and always appointed the same person Governor for their Province of Pennsylvania and their territory of the Lower Counties. The fact that Delaware and Pennsylvania shared the same governor was not unique. From 1703 to 1738, New York and New Jersey shared a governor. Massachusetts and New Hampshire also shared a governor for some time.\nWhile most of the Northeastern United States lie in the Appalachian Highlands physiographic region, some are also part of the Atlantic coastal plain which extends south to the southern tip of Florida. The coastal plain areas (including Cape Cod in Massachusetts, Long Island in New York, most of New Jersey, Delaware, and the Chesapeake Bay in Maryland) are generally low and flat, with sandy soil and marshy land. The highlands, including the Piedmont and the Appalachian Mountains, are generally heavily forested, ranging from rolling hills to summits greater than 5,000 feet (1,500 m), and pocked with many lakes. The highest peak in the Northeast is Mount Washington (New Hampshire), at 6,288 feet (1,917 m).\nAs of 2007, forest-use covered approximately 60% of the Northeastern states (including Delaware, Maryland, and the District of Columbia), about twice the national average. About 12% was cropland and another 4% grassland pasture or range. There is also more urbanized land in the Northeast (11%) than any other region in the U.S.\nThe climate of the Northeastern United States varies from northernmost Maine to southernmost Maryland. The climate of the region is created by the position of the general west to east flow of weather in the middle latitudes that much of the USA is controlled by and the position and movement of the subtropical highs. Summers are normally warm in northern areas to hot in southern areas. In summer, the building Bermuda High pumps warm and sultry air toward the Northeast, and frequent (but brief) thundershowers are common on hot summer days. In winter the subtropical high retreats southeastward, and the polar jet stream moves south bringing colder air masses from up in Canada and more frequent storm systems to the region. Winter often brings both rain and snow as well as surges of both warm and cold air.\nThe basic climate of the Northeast can be divided into a colder and snowier interior (Pennsylvania and New York State, and New England), and a milder coast and coastal plain from southern Rhode Island southward, including, New Haven, CT, New York City, Philadelphia, Trenton, Wilmington, Baltimore...etc.). Annual mean temperatures range from the low 50s F from Maryland to southern Connecticut, to the 40s F in most of New York State, New England, and northern Pennsylvania.\nThe Northeast has 72 National Wildlife Refuges, encompassing more than 500,000 acres (780 sq mi; 2,000 km2) of habitat, and designed to protect some of the 92 different threatened and endangered species living in the region.\nAs of the July 2013 U.S. Census Bureau estimate, the population of the region totaled 55,943,073. With an average of 345.5 people per square mile, the Northeast is 2.5 times as densely populated as the second-most dense region, the South. Since the last century, the U.S. population has been shifting away from the Northeast (and Midwest) toward the South and West.\nThe two U.S. Census Bureau divisions in the Northeast (New England and Mid-Atlantic) rank #1 and #2 among the 9 divisions in population density according to the 2013 population estimate. The South Atlantic region (233.1) was very close behind New England (233.2). Due to the faster growth of the South Atlantic region, it will take over the #2 division rank in population density in the next estimate, dropping New England to 3rd position. New England is projected to retain the number 3 rank for many, many years, as the only other lower-ranked division with even half the population density of New England is the East North Central division (192.1) and this region\'s population is projected to grow slowly. [b]\n|State||2017 Estimate||2010 Census||Change||Area||Density|\n|Connecticut||3,588,184||3,574,097||+0.39%||4,842.35 sq mi (12,541.6 km2)||741/sq mi (286/km2)|\n|Maine||1,335,907||1,328,361||+0.57%||30,842.90 sq mi (79,882.7 km2)||43/sq mi (17/km2)|\n|Massachusetts||6,859,819||6,547,629||+4.77%||7,800.05 sq mi (20,202.0 km2)||879/sq mi (340/km2)|\n|New Hampshire||1,342,795||1,316,470||+2.00%||8,952.64 sq mi (23,187.2 km2)||150/sq mi (58/km2)|\n|Rhode Island||1,059,639||1,052,567||+0.67%||1,033.81 sq mi (2,677.6 km2)||1,025/sq mi (396/km2)|\n|Vermont||623,657||625,741||-0.33%||9,216.65 sq mi (23,871.0 km2)||68/sq mi (26/km2)|\n|New England||14,810,001||14,444,865||+2.53%||62,688.4 sq mi (162,362 km2)||236/sq mi (91/km2)|\n|New Jersey||9,005,644||8,791,894||+2.43%||7,354.21 sq mi (19,047.3 km2)||1,225/sq mi (473/km2)|\n|New York||19,849,399||19,378,102||+2.43%||47,126.36 sq mi (122,056.7 km2)||421/sq mi (163/km2)|\n|Pennsylvania||12,805,537||12,702,379||+0.81%||44,742.67 sq mi (115,883.0 km2)||286/sq mi (111/km2)|\n|Middle Atlantic||48,674,696||47,543,861||+2.38%||110,879.01 sq mi (287,175.3 km2)||376/sq mi (145/km2)|\n|Total||63,484,697||61,988,726||+2.41%||173,567.41 sq mi (449,537.5 km2)||366/sq mi (141/km2)|\nThis section needs expansion. You can help by adding to it. (July 2013)\nNew York City, considered a global financial center, is in the Northeast.\nThe Federal Bureau of Prisons maintains 17 federal prisons and two affiliated private facilities in the region.\n|Rank||Metro area served||Airport\n|Airport name||Largest airline|\n|1||New York||JFK||John F Kennedy International||JetBlue (37%)|\n|2||New York||EWR||Newark Liberty International||United (49%)|\n|3||Philadelphia||PHL||Philadelphia International||American (80%)|\n|4||Boston||BOS||General Edward Lawrence Logan International||JetBlue (29%)|\n|5||New York||LGA||La Guardia||Delta (21%)|\n|6||Baltimore/Washington||BWI||Baltimore/Washington International Thurgood Marshall||Southwest (65%)|\n|7||Washington||IAD||Washington Dulles International||United (41%)|\n|8||Washington||DCA||Ronald Reagan Washington National||American (50%)|\nGeographer Wilbur Zelinsky asserts that the Northeast region lacks a unified cultural identity, but has served as a ""culture hearth"" for the rest of the nation. Several much smaller geographical regions within the Northeast do have distinct cultural identities.\nAccording to a 2009 Gallup poll, the Northeastern states differ from most of the rest of the U.S. in religious affiliation, generally reflecting the descendants of immigration patterns of the late 19th and early 20th centuries, with many Catholics arriving from Ireland, Italy, Canada, and eastern Europe. Massachusetts, Rhode Island, New York, and New Jersey are the only states in the nation where Catholics outnumber Protestants and other Christian denominations. More than 20% of respondents in Maine, New Hampshire, and Vermont declared no religious identity. Compared to other U.S. regions, the Northeast, along with the Pacific Northwest, has the lowest regular religious service attendance and the fewest number of people for whom religion is an important part of their daily lives.\nMajor League Soccer features five Northeastern teams: D.C. United, New England Revolution, New York City FC, New York Red Bulls, and Philadelphia Union. The region also has three WNBA teams: Connecticut Sun, New York Liberty, and Washington Mystics.\nNotable golf tournaments in the Northeastern United States include the Deutsche Bank Championship, The Barclays, Quicken Loans National, and Atlantic City LPGA Classic. The US Open, held at New York City, is one of the four Grand Slam tennis tournaments, whereas the Washington Open is part of the ATP World Tour 500 series.\nNotable Northeastern motorsports tracks include Watkins Glen International, Dover International Speedway, Pocono Raceway, New Hampshire Motor Speedway, and Lime Rock Park, which have hosted Formula One, IndyCar, NASCAR, and International Motor Sports Association races. Also, drag strips such as Englishtown, Epping, and Reading have hosted NHRA national events. Pimlico Race Course at Baltimore and Belmont Park at New York host the Preakness Stakes and Belmont Stakes horse races, which are part of the Triple Crown of Thoroughbred Racing.\nThe rate of potentially preventable hospitalizations in the Northeastern United States fell from 2005 to 2011 for overall conditions, acute conditions, and chronic conditions.\nThe Northeastern United States tended to vote Republican in federal elections through the first half of the 20th century, but the region has since the 1990s shifted to become the most Democratic in the nation. Results from a 2008 Gallup poll indicated that eight of the top ten Democratic states were located in the region, with every Northeastern state having a Democratic party affiliation advantage of at least ten points. The following table demonstrates Democratic support in the Northeast as compared to the remainder of the nation.\n|Year||% President vote||% Senate seats||% House seats|\nThe following table of United States presidential election results since 1900 illustrates that over the past six presidential elections, only three Northeastern states supported a Republican candidate (New Hampshire voted for George W. Bush in 2000; Pennsylvania and Maine\'s 2nd congressional district voted for Donald Trump in 2016). Bolded entries indicate that party\'s candidate also won the general election.\n|ME||R||R||R||D||D||R||R||R||R||R||R||R||R||R||R||R||D||D||R||R||R||R||R||D||D||D||D||D||D||D (R ME-02)|\nThe following table shows the breakdown of party affiliation of governors, state legislative houses, and U.S. congressional delegation for the Northeastern states, as of 2017. (Demographics reflect registration-by-party figures from that state\'s registered voter statistics.)\n|State||Governor||Upper House Majority||Lower House Majority||Senior U.S. Senator||Junior U.S. Senator||U.S. House Delegation||Demographics|\n|NY||Democratic||Rep. Led Coalition\nThe most widely used regional definitions follow those of the U.S. Bureau of the Census.\nPerhaps the most widely used regional classification system is one developed by the U.S. Census Bureau.\n(M)ost demographic and food consumption data are presented in this four-region format.', 'How Many Electoral Votes are Needed to Win the Presidency?\nUnderstanding the Electoral College System\nThe United States of America follows a unique system of presidential elections known as the Electoral College. Under this system, the President and Vice President are not elected directly by the people, but rather by electors who are chosen by each state.\nThe number of electors each state has is determined by its representation in Congress, which is based on the state’s population. This means that more populous states have more electors, while smaller states have fewer electors. In total, there are 538 electors in the Electoral College, with a candidate needing to win a majority of 270 electoral votes to become President.\nThe Electoral College system has been a topic of debate for many years, with some arguing that it does not accurately represent the will of the people and should be abolished in favor of a popular vote system. However, others believe that the Electoral College provides a fair and balanced way to ensure that smaller states have a say in presidential elections.\nOverall, understanding the Electoral College system is crucial to understanding how presidential elections work in the United States and the role that electoral votes play in determining the winner.\nThe Role of Electoral Votes in Presidential Elections\nIn a presidential election, the candidate who receives the most electoral votes is the winner, not necessarily the candidate who wins the most popular votes. This is because the President and Vice President are elected by the Electoral College, not by a direct vote of the people.\nEach state is allocated a certain number of electoral votes based on its representation in Congress, with larger states having more votes than smaller states. The District of Columbia also has three electoral votes. To win the presidency, a candidate must secure a majority of at least 270 electoral votes out of the total 538.\nElectoral votes are typically awarded on a winner-takes-all basis, meaning that the candidate who wins the most popular votes in a state will receive all of that state’s electoral votes. However, a few states and the District of Columbia have implemented a proportional allocation system, where electoral votes are awarded proportionally based on the popular vote results.\nThe role of electoral votes in presidential elections has been the subject of much debate, with some critics arguing that it undermines the principle of one person, one vote. Nonetheless, the Electoral College remains a fundamental part of the US political system, and it plays a critical role in determining the outcome of presidential elections.\nHistorical Examples of Electoral Vote Count in Presidential Elections\nThroughout US history, presidential elections have been decided by the Electoral College system. Some elections have seen candidates win both the popular vote and the electoral vote, while others have resulted in a split between the two.\nOne of the most well-known examples of a split between the popular vote and the electoral vote occurred in the 2000 presidential election, where George W. Bush won the presidency despite losing the popular vote to Al Gore. Bush secured 271 electoral votes compared to Gore’s 266, with one elector from the District of Columbia abstaining.\nAnother notable election was the 1824 presidential election, which saw John Quincy Adams become president despite receiving fewer electoral votes than Andrew Jackson. In that election, no candidate secured a majority of electoral votes, which led to the election being decided by the House of Representatives.\nThe 1876 presidential election was also controversial, as the outcome was disputed due to several states submitting competing sets of electoral votes. Ultimately, a special electoral commission was formed to decide the winner, and Rutherford B. Hayes was declared the winner over Samuel J. Tilden by a margin of just one electoral vote.\nThese historical examples illustrate the significance of the Electoral College system in US presidential elections and the potential for it to impact the outcome of the election, even when the popular vote is won by a different candidate.\nStrategies for Winning the Presidency through Electoral Votes\nWinning the presidency in the United States requires securing a majority of the electoral votes. This means that presidential campaigns must focus on winning states with the highest number of electoral votes and swing states that could go either way.\nOne strategy for winning the presidency through electoral votes is to target swing states, which are states that do not have a strong history of voting for one particular party. These states are considered crucial battlegrounds, as they can swing the election in favor of either candidate. Presidential candidates typically spend a significant amount of time and resources campaigning in swing states, trying to sway undecided voters.\nAnother strategy is to focus on winning states with a high number of electoral votes. This includes states such as California, Texas, and Florida, which have 55, 38, and 29 electoral votes respectively. Winning just one of these states can significantly impact a candidate’s chances of winning the presidency.\nA third strategy is to build a coalition of states with similar demographics and voting patterns. For example, a candidate may focus on winning the majority of the states in the Midwest or the South, which have historically voted for one particular party.\nOverall, winning the presidency through electoral votes requires careful strategizing and a strong understanding of the US political landscape. Candidates must be able to build a broad coalition of support across multiple states and demographics to secure the necessary number of electoral votes.\nDebates Surrounding the Electoral College System and its Reform\nThe Electoral College system has been the subject of much debate and controversy over the years. Some argue that it is an outdated system that does not accurately reflect the will of the people, while others believe that it is an important part of the US political system.\nOne of the main criticisms of the Electoral College system is that it can result in the winner of the popular vote losing the election. This has happened several times in US history, including in the 2000 and 2016 presidential elections. Critics argue that this undermines the principle of democracy and that the president should be elected based on the popular vote.\nProponents of the Electoral College system argue that it ensures that smaller states have a say in presidential elections and prevents larger states from dominating the election. They also argue that it promotes stability and prevents a candidate from winning the presidency through a narrow popular vote margin.\nThere have been several proposals for reforming the Electoral College system, including eliminating it entirely and replacing it with a popular vote system. Another proposal is the National Popular Vote Interstate Compact, which would ensure that the winner of the popular vote always wins the election by requiring states to award their electoral votes to the candidate who wins the popular vote nationwide.\nOverall, the debate surrounding the Electoral College system and its potential reform highlights the complexities of the US political system and the ongoing need for political reform and evolution.']"	['<urn:uuid:9baaa547-9e82-4b7b-bf69-65e25de4c540>', '<urn:uuid:840db5dd-7ede-40a9-b5cf-8228d5959ce7>']	open-ended	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-13T05:28:56.360010	7	123	3924
79	I'm looking to buy equipment for my plant and I heard about panel PCs - what exactly are they and what special features do they have?	A panel PC is a type where a display, typically an LCD, is incorporated into the same enclosure as the motherboard and other electronics. They are typically panel mounted and often have touch screens for user interaction. They come in different versions - from low cost ones with no environmental sealing to heavy duty models sealed to IP67 standards for waterproofing, and even explosion proof models for hazardous environments.	"['An industrial PC is an x86 PC-based computing platform for industrial applications.\nIBM released the 5531 Industrial Computer in 1984, arguably the first ""industrial PC"". The IBM 7531, an industrial version of the IBM AT PC was released May 21, 1985. Industrial Computer Source first offered the 6531 Industrial Computer in 1985. This was a proprietary 4U rackmount industrial computer based on a clone IBM PC motherboard.\nIndustrial PCs are primarily used for process control and/or data acquisition. In some cases, an industrial PC is simply used as a front-end to another control computer in a distributed processing environment. Software can be custom written for a particular application or an off-the-shelf package such as TwinCAT, Wonder Ware, Labtech Notebook or LabView can be used to provide a base level of programming. Analog Devices got exclusive sales for OEM european industrial market and provided MACSYM 120 combined IBM 5531 and MACBASIC a multitaskink basic running on C/CPM from Digital Research. Analog and digital I/O cards plugged inside PC and/or extension rack made MAC120 as one of the most powerful and easy to use controller for plant applications at this date. An application may simply require the I/O such as the serial port offered by the motherboard. In other cases, expansion cards are installed to provide analog and digital I/O, specific machine interface, expanded communications ports, and so forth, as required by the application.\nIndustrial PCs offer different features than consumer PCs in terms of reliability, compatibility, expansion options and long-term supply.\nIndustrial PCs are typically characterized by being manufactured in lower volumes than home or office PCs. A common category of industrial PC is the 19-inch rackmount form factor. Industrial PCs typically cost considerably more than comparable office style computers with similar performance. Single-board computers and backplanes are used primarily in Industrial PC systems. However, the majority of industrial PCs are manufactured with COTS motherboards.\nA subset of industrial PCs is the panel PC where a display, typically an LCD, is incorporated into the same enclosure as the motherboard and other electronics. These are typically panel mounted and often incorporate touch screens for user interaction. They are offered in low cost versions with no environmental sealing, heavier duty models sealed to IP67 standards to be waterproof at the front panel and including models which are explosion proof for installation into hazardous environments.\nConstruction and features\nVirtually all industrial PCs share an underlying design philosophy of providing a controlled environment for the installed electronics to survive the rigors of the plant floor. The electronic components themselves may be selected for their ability to withstand higher and lower operating temperatures than typical commercial components.\n- Heavier metal construction as compared to the typical office non-rugged computer\n- Enclosure form factor that includes provision for mounting into the surrounding environment (19"" rack, wall mount, panel mount, etc.)\n- Additional cooling with air filtering\n- Alternative cooling methods such as forced air, liquid, and conduction\n- Expansion card retention and support\n- Enhanced EMI filtering and gasketing\n- Enhanced environmental protection such as dust proof, water spray or immersion proof, etc.\n- Sealed MIL-SPEC or Circular-MIL connectors\n- More robust controls and features\n- Higher grade power supply\n- Controlled access to the controls through the use of locking doors\n- Controlled access to the I/O through the use of access covers\n- Inclusion of a watchdog timer to reset the system automatically in case of software lock-up']"	['<urn:uuid:7c503b1e-f35b-4608-be1c-d9ad8ca9452e>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T05:28:56.360010	26	69	573
80	need help understanding difference between active power reactive power smart meters monitoring compare	Active (real) power does the actual work when electrical devices are used, like turning on lights. Reactive power, while doing no work at the point of consumption, is essential for maintaining voltage to deliver active power. Digital Power Metering and monitoring Devices (PMDs) can accurately measure both types of power - active power is measured in kilowatts while reactive power is measured in VARs (Volt-Ampere-Reactive units). PMDs can track both types continuously and remotely, providing detailed measurements of both power types along with many other electrical characteristics such as voltage, current, and power factors.	['Utilities deliver both active (or real) power and reactive (or imaginary) power along their distribution lines. Real power does the actual work when you flip on a light switch. Certain energy loads, such as motors and refrigerators, include energy storage elements that periodically need to reverse the direction of energy flow. This electric power from stored electromagnetic energy, which returns to the source in each cycle, is known as reactive power.\nUtilities care about reactive power because, even though it does no work at the point of consumption, it is needed to maintain voltage to deliver active power. Do you remember the largest blackout in U.S. history that surged through the Northeast in 2003? The cause for this, and several other major blackouts, can largely be attributed to a severe shortage of reactive power, which failed to maintain voltage along major power routes.\nUtilities can produce any combination of kilowatts and VARs (Volt-Ampere-Reactive units measure reactive power) but they must constantly focus on maintaining the right balance. Motors and other reactive power requirements impact that balance, meaning that any imbalances are generally located near the motors. Because VAR support comes from utility generators that are located far from load, distributed generation (DG) provides the potential for a new way to provide VAR support – by using the inverters on solar arrays connected to distribution circuits to help create a more balanced and reliable grid.\nIn a similar vein, this also means that inverters with reactive power capability may be able help utilities accommodate higher penetrations of photovoltaics (PV) on the distribution grid with fewer grid upgrades.\nIt seems like a win-win situation, no?\nInterestingly however, the ability of DG systems to provide VAR support has largely been ignored in the U.S., generally because any policy action has been too complex to implement. That may be starting to change, with three recent and somewhat noteworthy policy developments.\n- Last October, the Federal Energy Regulatory Commission (FERC) made a move toward formally recognizing the benefits of reactive power. On October 17, FERC issued an order that requires jurisdictional entities providing reactive power to formally file reactive power rates with the commission, even though the rate schedule would provide no compensation for such a service.\n- In California, this issue recently came up in a Smart Inverter Working Group that IREC has been participating in over the past several months. The group put forth their recommendations for inverter capabilities regarding reactive power in a highly technical report, starting on page 32, section 2.4.5. California is proactively addressing some of these technical matters in order to lay out the technical steps needed to accommodate high solar penetration and to optimize DG resources.\n- And in January, the Planning Committee for PJM, the regional transmission operator serving the mid-Atlantic states, gave near unanimous approval to a problem statement and issue charge to explore whether to require renewables such as solar PV to install enhanced or “smart” inverters that can produce and absorb reactive power in addition to inverting DC power to AC. However, any smart inverter requirement is not likely to come about until PJM goes through the approval process to revise IEEE 1547 Standard for Interconnecting Distributed Resources with Electric Power Systems.\nThis topic is certainly nothing new, however. Germany and other European countries have been relying on reactive support from DG systems for quite a while. And FERC has long espoused the host of ancillary benefits, including reactive power, that DG can provide to create a more stable and reliable grid.\nIn a 2007 report, FERC noted that, “on a local basis there are opportunities for electric utilities to use DG to reduce peak loads, to provide ancillary services such as reactive power and voltage support, and to improve power quality.”\nWhile implementing ancillary services and VAR support at the distribution level will still require significant technical and regulatory changes, these recent actions are promising. At the least, they’re a signal from the powers-that-be that we will need to shift how we approach utility services, from a technical standpoint, in order to safely accommodate the rapid growth of distributed resources.', 'Digital Power Metering and monitoring Devices (PMDs) are fast replacing analog equipment, providing more accurate measurements and enabling remote data access. But specifying these devices could become problematic without common reference information describing such factors as, say, appropriate operating temperatures and the exact power parameters the device is capable of tracking. The International Electrotechnical Commission (IEC) has addressed this concern with a standard that establishes common set of reference requirements for electrical measurement.\nUnderstanding what this standard covers and how to read related manufacturer information can make it easier for engineers and other electrical professionals to compare and select the right PMD for any given application. Its full name is a mouthful: IEC 61557-12: “Electrical safety in low voltage distribution systems up to 1000V a.c. and 1500V d.c. – Equipment for testing, measuring, or monitoring of protective measures – Part 12: Power metering and monitoring devices (PMD).” However, its impact is equally broad – enough to fill two separate posts. In this post, I’ll provide an overview of IEC 61557-12’s scope and requirements. Next, I’ll provide information on related PMD-testing requirements described by the standard.\nIEC 61557-12 was developed to help specifiers select the right device for any electricity cost-management application. It also helps promote state-of-the-art electrical management on the demand side of the electrical network. To that end, this standard covers energy measurements, as well as measurement of many other critical electrical characteristics. In this way, IEC 61557-12 differs from standards for electricity metering equipment, which focus only on energy measurements.\nIn its section on PMD functions, IEC 61557-12 lists all possible electrical characteristics the devices might measure, along with related requirements – such as rated ranges of operation or allowable measurement techniques. The listed characteristics include:\n- Active energy (with performance classes equivalent to the classes defined in IEC 62053-21 and IEC 62053-22)\n- Reactive energy (with performance classes equivalent to the classes defined in IEC 62053-23)\n- Apparent energy\n- Active, reactive and apparent power\n- Root-mean squared (RMS) phase and neutral current\n- RMS voltage\n- Power factor\n- Voltage dip and swell\n- Voltage interruption\n- Voltage unbalance\n- Harmonic voltage and distortion\n- Harmonic current and distortion\n- Maximum, minimum, peak, average and demand values\nAs a big bonus for specifiers, IEC-61557-12 establishes three performance classes for registered devices. The classes define how well a PMD operates across four specific parameters for every type of electrical measurement it is marketed to provide, as shown in Figure 1.\n- Guaranteed accuracy refers to the limits of uncertainty in the results a PMD provides, over a specified measuring range and under reference conditions. I’ll be covering the topic of uncertainty limits in my next post.\n- Defined measuring range specifies the minimum and maximum values of quantities between which limits of measurement uncertainty are defined. For current, measuring range is specified by manufacturers through:\n- nominal current (In) and maximum current (Imax) for sensor operated PMDs (called PMD/Sx)\n- base current (Ib) and maximum current (Imax) for directly connected PMDs (called PMD/DD)\n- Influence quantities refers to environmental conditions, such as temperature and other climatic impacts and electromagnetic perturbations that might be encountered in switchboards or electrical cabinets. The standard specifies maximum permitted variations of accuracy due to those influence quantities.\n- Zero-blind (Gapless) measurement – in other words, continuous (rather than intermittent) monitoring – is required for several capability parameters under the standard, particularly for energy measurements.\nNot just for standalone devices\nImportantly, IEC 61557-12 applies to PMDs embedded within other equipment as well as standalone devices, which is critical today, when PMD measurement capabilities are increasingly present in protection relays, feeder remote terminal units and a number of circuit breaker offerings. So, when selecting these products, specifiers can refer to the standard to define the required performance class for the embedded measurement function.\nTesting criteria in the next post\nIn my next post on IEC 61557-12 I’ll be looking at the testing criteria that help define a PMD’s uncertainty limits – essentially, the limits of its accuracy. As you’ll read, these criteria look at devices’ performance both in isolation and as part of an overall system.\nIf you are looking for more details, you can check out the following documents:\n|Power meters selection guide||White paper||Power Meter Selection Guide for Large Buildings|\n|IEC 61557-12||White paper||Guide to using IEC 61557-12 standard to simplify the setup of an energy measurement plan.|\n|Measurement applications||White paper||Guide to energy measurement applications and standards|\nAlso, for more information about standards and access to additional tools, resources and product information you can register for our dedicated Consulting Engineer portal site.\nThe post Understanding the IEC 61557-12 Standard That Makes Meter Comparisons Easier-Part 1 appeared first on Schneider Electric Blog.']	['<urn:uuid:19946764-a1c7-4e59-8b2e-394f4d7afdc5>', '<urn:uuid:ad67e921-1b9d-4435-9da6-11571a25bc32>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-13T05:28:56.360010	13	94	1474
81	need help comparing restraining order and avo court proof requirements what needs to be shown to judge	For an AVO (Apprehended Violence Order), the applicant needs to prove that they fear for their safety and have reasonable grounds for that fear. The applicant doesn't need to prove that the defendant has been violent towards them. For a VRO (Violence Restraining Order), the applicant must show the court that the respondent is likely to commit an act of abuse against them, or make them reasonably fear such abuse will occur, AND the court must think it's appropriate to make the order.	"[""Department of Justice is now the Department of Communities and Justice. Find out more >\nIf you have been served with an application for an Apprehended Violence Order (AVO) there are many things you need to consider.\nIf you are under 18 years of age, your case will be dealt with differently. For more information, see Apprehended Violence Orders against children.\nAn application for an Apprehended Violence Order (AVO) can be made by:\nFrom 3 December 2016, the Children's Court in care proceedings can make an AVO to protect children.\nWhen an application is made, the people involved in the application are called:\nTo get an Apprehended Violence Order (AVO), the applicant will need to prove that:\nThe applicant doesn't need to prove that you have been violent towards them or the protect person to get an AVO.\nIn some cases, the applicant does not need to show that the protected person actually fears the other person. The Court can make an AVO even if the protected person doesn’t actually fear you, if the protect person:\nCase study - Lette and Carol\nLetti and Carol have been friends for years. A year ago, Letti started dating Carol's ex boyfriend, Bill. Carol was very unhappy with Letti and Bill's new relationship and sent Letti nasty emails for several months. Six months ago, Letti ran into Carol at the shops. Carol started yelling at her, saying she felt betrayed. Letti walked away, but is now scared to go out in case she sees Carol.\nCase study - Lino and Daniel\nLino and Daniel lived together for almost seven years. Six months ago, Lino told Daniel he wanted to take a break and Lino moved out of their flat. About two weeks ago, they met up at a party. Lino was with his new boyfriend. Since then, Daniel started calling Lino's mobile up to 50 times a day. At first the phone calls were just annoying but then Daniel started threatening Lino. Lino also noticed Daniel hanging around outside his workplace. Last night, Daniel followed Lino to his car and attacked him. Daniel pushed Lino to the ground, punched him and grabbed Lino's neck. A passer-by called the police and managed to pull Daniel off Lino. The police applied for an Apprehended Domestic Violence Order for Lino.\nIn Letti and Carol's case, the Court may decide not to make a Final AVO because a great deal of time has passed since Carol sent Letti the emails and Carol has not acted in a threatening manner. In Lino and Daniel's case, the Court is more likely to make an AVO because the events happened recently and Daniel has been violent.\nService is the process of giving or sending court documents to a person to notify them of those documents.\nIf you are the defendant in an Apprehended Violence Order (AVO) matter, you must be served personally with the application for an AVO. You must be told that an application has been filed against you and that you have to go to court.\nIf an AVO has already been made against you, you must be served with a copy of the AVO because you must be told if there are court orders that you must follow. If you are not served with a copy of the AVO you can’t be convicted for breaching the AVO, unless you were at court when the order was made.\nThis section covers:\nFor more information, see Service of documents.\nThere are a number of ways that you can respond to an application for an Apprehended Violence Order (AVO):\nFor more information, see How to respond to an application for an Apprehended Violence Order.\nBefore responding to an application for an AVO you should consider:\nWhile an application for an Apprehended Violence Order (AVO) is being decided, there are two types of temporary AVOs that can be made to protect the protected person:\nThe police can apply for an urgent Provisional AVO.\nFor more information, see Provisional and Interim Apprehended Violence Orders.\nA court can make a Final Apprehended Violence Order (AVO) if:\nOnce a Final AVO is made there are consequences of having the AVO and of breaching it.\nFor more information, see Final Apprehended Violence Orders.\nThere are two different types of orders that may be included in an Apprehended Violence Order (AVO):\nAll AVOs include mandatory orders that may be called standard orders or 'Orders about behaviour'.\nThe applicant can also ask for a range of additional orders. The applicant can also have orders that are written especially for their matter.\nFor more information, see Mandatory and additional orders.\nIn Apprehended Violence Order (AVO) matters, the Court may award costs against the unsuccessful party.\nIf the Court dismisses the application, you can ask the Court to make an order that the protected person or the police pay your legal costs.\nLegal costs include lawyer's fees and expenses such as conduct money for witnesses. Legal costs do not include lost wages.\nFor more information, see\nCosts in Apprehended Violence Order cases.\nIf you have left personal property with the protected person, or the protected person has left property with you, the Court can make a Property Recovery Order.\nA Property Recovery Order can only be made at the same time that a Provisional, Interim or Final Apprehended Violence Order is made. It can't be made after the case is finished. You should ask for the order at court and prepare a list of the property you want to recover.\nFor more information, see Recovering personal property.\nIf an application for an Apprehended Violence Order has been made against you, you may also have been charged with a criminal offence, for example assault.\nFor more information, see\nCharges and Apprehended Violence Orders.\nIf you have been served with an application for an Apprehended Violence Order (AVO), but you also fear the protected person, you may want to make a cross application against them.\nThe Court will treat your application like a normal application for an AVO.\nFor more information, see Cross applications.\nCommunity Justice Centres"", 'What is a violence restraining order?\nA violence restraining order (VRO) is designed to stop threats, property damage, violence, intimidating behaviour and emotional abuse in the future. It is an order of the court. It tells the offender to stay away from you and/or to stop behaving in certain ways towards you. The order can be worded to suit your situation.\nThere are two types of VROs:\n- VROs against a person you are in a family or domestic relationship with, and\n- VROs against a person you are not in a family or domestic relationship with.\nThere are two other types of restraining orders. See Misconduct restraining orders and Police orders for more information.\nFor more information if you have been served with an interim VRO or a summons for a restraining order see Responding to a restraining order application.\nIs there anyone who cannot be bound by a VRO?\nA child under the age of ten years old cannot be bound by a VRO.\nWhat is family and domestic violence?\nFamily and domestic violence includes emotional, physical, sexual and psychological abuse. If your partner, ex-partner or a family member hurts, threatens or humiliates you, it is family and domestic violence.\nThe law states that an act of family and domestic violence includes assaults, injuries, threats, stalking, damaging property, hurting animals or pets, and acting in an ongoing intimidating, offensive or emotionally abusive manner.\nPhysical violence, stalking and threats of violence are crimes.\nAm I in a family or domestic relationship?\nYou are in a family or domestic relationship with someone if that other person is your:\n- spouse or ex-spouse\n- de facto or ex-de facto\n- girlfriend/boyfriend or ex-girlfriend/ex-boyfriend\n- child, step-child or grandchild\n- parent, step-parent or grandparent\n- your sibling or step-sibling\n- relative or former relative.\nIf you are not sure whether you are in a family or domestic relationship with the person you want a restraining order against, seek legal advice.\nWho can apply for a violence restraining order?\nYou can apply for a VRO if you are the person who wants protection. A guardian or a police officer may also apply for you.\nIf you are not yet 18, the police or a parent, guardian or child welfare officer (eg, a Department for Child Protection and Family Support case manager) can also apply for a VRO for you.\nHow can I get a VRO?\nIf you are applying for the order you are called the applicant or the person seeking to be protected. The person you want the order against is called the respondent. If the order is made they are the person bound.\nAn application for a VRO can be made:\n- In person to the Children\'s Court if the respondent is a child or young person under 18.\n- In person to the Magistrates Court if both the applicant and the respondent are adults.\n- In person to the Magistrates Court or the Children\'s Court if the person seeking to be protected is a child or young person under 18 against an adult respondent.\n- In some cases during proceedings taking place in other courts, eg in the criminal courts.\n- Through a police officer who may apply for you by telephone. They usually only do this where doing it yourself is either not practical or the situation is urgent.\nThe court may also extend an order to cover a person named in the order in addition to the person protected by the order, eg a parent who seeks to have their child covered by their order.\nIf a person is convicted of certain violent offences in a criminal court, that court can automatically make a lifelong violence restraining order against an adult or a child unless the victim does not want it.\nAsk at your nearest courthouse for the application form or, if there is no courthouse in your area, ask at the nearest police station. The application form can also be downloaded from the Magistrates Court of WA website.\nTo get a VRO you must be able to show the court that the respondent is likely to:\n- commit an act of abuse against you, or\n- make you reasonably fear they will commit an act of abuse against you\nAND the court thinks it is appropriate to make a restraining order.\nIf possible get legal advice before you make your application. You can contact Legal Aid WA’s InfoLine on 1300 650 579 for information and referral.\nSee Violence restraining orders - court process for more information on what happens at court.\nWhat is an act of abuse?\nAn act of abuse includes:\n- any assault whether or not it causes an injury\n- any act that causes an injury\n- holding another person against their will\n- threats to do any of the above.\nIf the respondent believes they are in a family or domestic relationship with you, even if they aren\'t, then the following would also be acts of personal violence:\n- damage to your property\n- injuring or killing your animals\n- behaving in an ongoing way that is frightening, offensive or emotionally abusive towards you.\nIf you are in a family or domestic relationship all of the above acts are acts of abuse.\nWhat conditions can be imposed in a VRO?\nA VRO can have conditions which stop the person bound from doing certain things such as:\n- being on or near your home home or place of work\n- being on or near a certain place\n- coming within a certain distance of you\n- contacting or trying to contact you in any way, including texting, ringing , emailing or writing- even through other people\n- contacting you in certain circumstances or in particular way, eg only by texting to make arrangements for contact with your children\n- behaving in certain ways\n- being in possession of firearms, ammunition or a firearms licence.\nA VRO may also inform the person bound/respondent that certain behaviour and activities are unlawful, that is, they may break a criminal law.\nIs a VRO a criminal charge?\nNotice of a VRO does not go on the person bound\'s criminal record.\nHowever, if a person bound by a VRO breaches that order, they may be charged with the criminal offence of breaching a violence restraining order. A conviction for breach of a VRO will go on their criminal record.\nBreaches of a VRO can result in fines of up to $6,000 or imprisonment for up to two years or both.\nWhat is a breach of a VRO?\nA restraining order will prevent the person bound from doing certain things.\nPersons bound and protected persons must READ THE ORDER CAREFULLY.\nIf the person bound does something that the restraining order says they can\'t do, they are ""breaching"" the order.\nFor example, if a VRO says the person bound is not allowed to communicate with you, they must not:\n- visit you\n- call you on the phone\n- send SMS or text messages to you\n- send emails to you\n- send letters to you\n- send presents to you\n- send messages to you, even through friends, family or your children.\nWhat if I want my property back from the person bound?\nYou should not initiate contact if it is prohibited under the VRO. If you, as the person protected, have moved out of the family house but still have personal property there, you may ask the court to include a condition on the VRO saying you can go back to that house to get your personal property in the presence of a WA police officer.\nIf an interim VRO has already been made and the court has not made an order allowing you to collect your property, you can apply to the court for a variation of the VRO to include such an order.\nA VRO is not a court order about who owns the property. You may need to get advice from a family lawyer about how to get a family court order about property settlement.\nFor more information about property settlement see Dividing property - married couples and Dividing property - de facto couples.\nHow can I get the person bound\'s property back to them?\nYou should not initiate contact if it is prohibited under the VRO. The return of property may be covered by a condition of the VRO. You can contact the police and arrange for the goods to be given back to the person bound.\nIf you were in a relationship don\'t throw away the property as it may form part of your property settlement. If this applies to you get legal advice.\nFor situations where you are required to take steps before you can dispose of goods left with you see Abandoned goods.\nWhat help can the police give me if there is family and domestic violence?\nFor information about how the police may be able to help see Police orders - information.\nIf family violence has led to the offender being charged with a criminal offence and appearing in court, they may plead guilty and have their matters finalised on the day. If you or your children still require protection after that day, you may apply for a VRO.\nIf the offender pleads not guilty or has to come back to court again, the police can ask for protective bail conditions to remain in place until the court matters are finalised.\nYou may apply for a VRO to be issued at any time you experience family violence, regardless of whether the offender has been charged. This may provide extra protection for you and your children if the police cannot assist you.\nYou can also contact the Domestic Violence Sergeant for your district of the family protection officer at your nearest police station for further assistance.\nWhat if I have been injured as a result of family violence?\nIf you have been injured, get medical help immediately. If necessary, the Crisis Care Unit may be able to help with transport. Tell a doctor what happened and remember to take a note of the date and the doctor\'s name and address.\nIf possible, get someone to take photographs of your injuries and get them to sign and date them. Anyone can take photographs of the injuries, provided that the person is prepared to go to court with you. Keep any evidence of assault such as torn clothing.\nIf the police are involved, ask them to collect evidence and arrange for photographs to be taken. Ask the police for an Incident Report Number.\nAm I eligible for criminal injuries compensation?\nFor information about criminal injuries compensation, see Compensation for victims of crime.\nAm I eligible for assistance if I am on a visa?\nIf you are on a temporary visa, you may need legal advice about family and domestic violence and immigration issues. See Immigration status and family violence.\nWhere can I get more information?\n- Contact the general support 24-hours Crisis Care Unit (Department for Child Protection and Family Support) on (08) 9223 1111 or 1800 199 008 for callers outside the metropolitan area.\n- Police support is available from your local police station on 131 444.\n- Contact the Legal Aid WA InfoLine on 1300 650 579 or any office for more information and referral.\n- For more information about what to do if you are at risk of family violence and tips for staying safe, see Family and domestic violence support services.\n- Information sheets covering different aspects of dealing with restraining orders are available from Legal Aid WA. These should be used with legal advice. For information on:\nThe Aboriginal Family Violence Prevention Legal Service (AFVPLS) at Perth can provide assistance in family law (children\'s issues), protection and care, VROs and criminal injuries (where related to family violence or sexual assault) to Aboriginal or Torres Strait Islander peoples, or whose partner or children are Aboriginal or Torres Strait Island peoples who are experiencing family violence (not perpetrators). It can be contacted on (08) 9200 2202 for legal help and (08) 9489 6391 for counselling support. See Aboriginal Family Violence Prevention Legal Service for the contact details of other offices including Aboriginal Family Law Services in regional areas.\nClick here if you want to navigate away from this page quickly.\nLast reviewed: 29/01/2014']"	['<urn:uuid:52e4c256-4273-476e-8b28-ab59f1754d76>', '<urn:uuid:9dcc0af9-6009-4930-8d78-144e6eefe700>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-13T05:28:56.360010	17	83	3078
82	What was the very first reason why people started writing things down in ancient times?	The first examples of written word appeared around 3000 B.C. as receipts. Writing was initially used to record transactions, with storage vessels being sealed with clay and stamped with marks of ownership, and cylinder seals being rolled into clay tablets to designate quantities of sheep or grain to be exchanged.	"['The invention of written language is perhaps the most important milestone in human history, and we owe it to the accountants. Mesopotamia was a wild place at the dawn of the Bronze age. The Tigris and Euphrates rivers created a fertile stripe of perfect agricultural land arcing from the Mediterranean Sea to the Persian Gulf. Human civilization had flourished since the development of metal tools around 4500 B.C., and the potter’s spinning wheel had recently made storage and trade of agricultural produce possible. In this hotbed of development, trade between Mesopotamia’s many emerging city-states became a staple of the urbanizing culture, and so around 3000 B.C. we see the first examples of the written word — as receipts.\nStorage vessels were sealed with clay, and stamped with marks of ownership, and cylinder seals were rolled into clay tablets to designate the quantities of sheep or grain to be exchanged. This earliest form of writing is known now as cuneiform, and while the earliest writings recorded transactions, it was quickly adapted to the recording of ideas. Luckily for historians, cuneiform was finally deciphered successfully by the French scholar Eugène Burnouf in 1836 — opening the nearly two million discovered cuneiform tablets to translation. Only a fraction of these writings have been translated, but already we\'ve found stories, prayers, poetry and songs of the Mesopotamian world.\nThe rich and rapid social growth of the Mesopotamian city states brought war. From 3000 B.C. to the region’s final conquest by Alexander the Great in 332 B.C., Mesopotamia was awash in bloody conflict. To understand the politics of the time, it’s important to understand the geography. Mesopotamia was home to many urban centers, each home to their own unique culture and lineage of kings. Surrounding these cities was farmland, kept fertile by irrigation ditches — so the more farmland and irrigation a city controlled, the more powerful it became. For 3000 years these city’s fought, traded, taxed and murdered each other. For 1000 years the Sumerians from Sumer were dominant, until temporary defeat by the Akkadians from Akkad. Babylon would rise to power in 1900 B.C. and competed with the Hittites from 1450-1200 B.C. And so on…\nBut in this violent, messy time some extraordinary things happened. In 2140 B.C., Gudea of Lagash, a Sumerian king, realized he could control his domain more effectively with words than with the sword. Gudea understood that stable power grows from order, so he funded massive public works — temples to the popular local gods Ningirsu, Nanshe, Ningishzida and Geshtinanna. These imposing structures emphasized the values of peace and religion, and reinforced the Gudea’s authority. Finally, and most brilliantly, Gudea commissioned small, moveable statues of himself to be delivered to cities across his kingdom, making his face a fixture throughout Mesopotamia and becoming the first superstar, the first celebrity.\n380 years later the sixth Babylonian king, Hammurabi, would create another defining feature of human civilization — the first unified code of law. Engraved on a seven foot tall basalt stone the 282 laws establish terms of judgement for business transactions, proper wages, and relationships, including divorce and inheritance. The laws also defined a strict set of punishments for breaking the laws, including the now infamous lex talionis, or eye-for-an-eye system of retaliatory punishment. Hammurabi’s Law Code was not the first set of laws in Mesopotamia, but it was the first to be compiled by legal experts, reviewed, edited, and codified into a universal definition of law — setting a standard for governmental justice that’s in place to this day.\nIn the midst of Umma Entemena overthrew Urlumma and killed him. He left behind 60 soldiers of his force dead on the bank of the canal.\nI am Nin-jirsu. No country can bear my fierce stare, nobody escapes my outstretched arms.\nThe day was for supplication, the night was for prayer. The moonlight ... early morning, its master.\n""Wake me early in the morning, I must not be late, or my teacher will cane me.""\nYour kingship is good for the people. The people spend their days in abundance thanks to you.\nIn order to find sweetness in the bed on the joyous coverlet, my lady bathes her holy thighs.\nThe First Law of the Land: if any one ensnare another, then he that ensnared him shall be put to death\nAction and inaction will both destroy you in the end']"	['<urn:uuid:47230668-154a-4d82-87e6-af54a6fddea8>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T05:28:56.360010	15	50	731
83	photovoltaic lights output power fluctuations	Both LED lights and solar photovoltaic systems face power output challenges. LED lights like the DayLed series show varying output levels, with the DayLed 1000 producing 5800lx and the DayLed 650 variants producing less. Similarly, solar photovoltaic systems experience intermittent power output due to weather conditions, causing grid frequency fluctuations and making it difficult to balance supply and demand instantaneously.	['I recently got the chance to field test three fresnel LED lights from Lupo Lighting: the DayLed 1000, DayLed 650 3200K and the DayLed 650 Dual Colour. All three of these lights have been designed to offer a lightweight, compact, powerful solution, for either field or studio use. As I primarily operate solo I wanted to see how well they worked in an environment where you had to transport, set up and use the lights by yourself.\nTheir affordable price and ability to run off a camera battery in the field has made them a popular choice for TV stations around the world. While earlier versions of these lights did suffer from a green colour spike, the latest versions have been greatly improved to offer better colour rendition and performance. I used a Sekonic C-700 Spectromaster to test all three of the lights and you can see those results further down in the article.\nFirst let’s take a look at the larger of the three fixtures, the DayLed 1000. The DayLed 1000 uses a 150mm fresnel lens that offers full flood and spot control and a output that Lupo claim is very close to that of a Arri L5-C 5600K fixture. The first thing that you notice about the DayLed 1000 is that it is extremely lightweight. Lupo have built the 1000 out of a carbon fibre reinforced polymer to keep the weight down to 3.5kg (7.7lb). What makes this figure even more impressive is that the ballast is built into the light, so your entire fixture including the power supply comes as one unit. To put this weight in perspective the Arri L5-C weighs in at 5.1kg (11.24lb). I found the DayLed 1000 to be well made and build quality doesn’t seem to have been sacrificed even though the unit is so light.\nAll the DayLed lights can be battery powered. The DayLed 1000 draws 90W, which allows you to run it off just about any camera battery at 100% power. What I especially like about this is you don’t need to carry around high power-draw camera batteries that you might not be allowed to take on board aircraft. All the Lupos have a broadcast industry standard four-pole XLR socket for battery connection from a standard 14.7V battery plus a mains power lead. The lights don’t come with a battery plate, which can be purchased separately.\nThe light is fully dimmable from zero to 100% and is pre-configured for DMX use. Barn doors are included with the fixture and they have built-in clips for attaching frosts or gels. Three models of the 1000 are available: 3200K, 5600K and variable colour.\nThe DayLed 650 3200K and the DayLed 650 Dual Colour are very similar in design to the 1000, but as their name suggests output less light. Both lights weigh in at 2.5kg (5.5lb) which again makes them a good solution for travel. They only draw 50 and 60W of power respectively, so they can be run off most camera batteries for a few hours at full power. The DayLed 650 and the DayLed 650 Dual Colour look almost identical apart from the extra colour adjustment knob on the back of the Dual Colour.\nColour performance and output\nI tested all 3 lights with the a Sekonic C-700 at a distance of one meter in a fully controlled environment. Below are the results:\nDayLed 1000 5600K\nThe DayLed 1000 5600k results were pretty good and the light scored well with a average CRI Ra (CRI Ra is only the average of R1-R8) of 91.7 and a extended CRI (R1-R15) of 88.7 . More importantly it scored very well in both R13 (the closest to caucasian skin tones) and R15 (the closest to Asian skin tones). The other figure to look at is R9 (red) which is also a key colour for helping replicate good skin tones. Unfortunately the light only managed to score a 64.2 in this category. Really good lights normally produce consistent numbers from R1 to R15. Still these results are quite impressive for a large LED source and it places the DayLed 1000 5600K among the best scoring lights in this category that I have tested. The DayLed 1000 5600K scores were only marginally behind those of the very good Zylight Z8.\nAs far as output goes the light measured 5800lx at a distance of one meter. Note: All the lights I test regardless of whether they have spot or flood capabilities are tested in their flood setting.\nDayLed 650 3200K\nThe DayLed 650 3200K performed exceptionally well for a fresnel based LED light. It scored a average CRI Ra of 96.8 and a extended CRI (R1-R15) of 95.9. It had very high scores for R13- 96.4 (the closest to caucasian skin tones) and R15- 96.4 (the closest to Asian skin tones). The light scored highly in R9 (red)- 95.4, which was a big improvement over the DayLed 1000. However the light only output 1970lx at a distance of one meter, which was not a lot for a light of this size.\nDayLed 650 Dual Colour 3200K\nIt is clear that you sacrifice a bit of colour accuracy to get the added benefit of having a dual colour fixture. While the light didn’t perform as well at 3200K as the designated 3200 650 fixture, the results were still good. It scored a average CRI Ra of 91.6 compared to the 96.8 of the 3200k only version. As far as skin tone reproduction goes it scored 94.5 for R13 and 90.3 for R15. Where this light did struggle was reproducing reds (R9) where it scored just 63.7.\nDayLed 650 Dual Colour 5600K\nThe DayLed 650 Dual Colour when tested at 5600K gave me a average CRI Ra reading of 91.6 which was less than the DayLed 650 3200k model. It had a extended CRI (R1-R15) of 88.5. For R13 it scored 94.5 (the closest to caucasian skin tones) and R15-90.3 (the closest to Asian skin tones). Again this fixture struggled to re create red (R9) and scored only 63.7 The light outputted 2990lx at a distance of one meter.\nField Testing and conclusion\nI used the lights on a few shoots to see how well they performed and I was particularly interested to test the DayLed 1000 as I have been looking for another large LED light with a fresnel design. I had the perfect opportunity to use it for some live TV broadcasts in a makeshift studio I set up in a hotel room in Tokyo. I placed ND on the window to cut the light and to help me match the lighting on the correspondent. As I only had 1 DayLed 1000, I used it in conjunction with a Zylight Z8 to provide even lighting from both sides.\nI knew from testing both lights with the Sekonic C-700 that they had similar photometrics so I could use them together without having to worry about matching two different colour sources. This is another important reason to have accurate photometric data about lights, so that you can potentially mix and match different brands and be confident that they will work well together.\nI love the lightweight construction and easy to use controls on the DayLed 1000, it’s easy to transport, easy to set up and power and easy to use. I think the main strength of this light is its versatility, as it is often difficult to find a fresnel light source that you can power off any camera battery and that is light enough to carry around.\nThe DayLed 650 3200K and the DayLed 650 Dual Colour are also nice fixtures, but I did find that the 3200K version, despite its high CRI readings was a little disappointing when it came to power output.\nOverall the DayLed series from Lupo are pretty impressive given their relatively low cost. They are well built, lightweight and offer good performance for LED fresnel fixtures. I would definitely recommend having a look at the DayLed series if you’re looking for a portable, battery powered spot and flood capable LED light.\nThe DayLed 1000 retails for $1254 US, the DayLed 650 Dual Colour for $1104 US, and the DayLed 650 in either tungsten or daylight is $932 US. For more information you can go to Lupo.lighting.com', 'On a breezy, sunny day in May 2016, something unusual happened in Germany. So much renewable energy was generated that electricity prices went negative, meaning that customers were effectively paid to use it. Unusual but not unique. For a few hours in March 2017, solar generation in California accounted for almost 40% of net grid power produced, reflecting a 50% growth in the state’s utility-scale solar photovoltaic installed capacity in 2016. The result of which, as occurred in Germany, was that power prices were driven into the negative, an event that continues to occur periodically in the state.\nNegative electricity prices sound like a nice problem to have. This might be true if they were only indicative of the impressive improvement in the development, cost reduction and efficiency increase in renewable energy production. However, this phenomenon rather illustrates the intermittency and somewhat unpredictable nature of wind and solar power: supply fluctuates, surfeits cannot currently be stored to any great extent, and conventional grid balancing methods are not designed for power generation fluctuations caused by clouds or changes in wind speed.\nThe growth in demand for electricity, the need to shift to low carbon energy sources, the geographically widespread nature of renewable energies, along with the development of increasingly more efficient technologies, are all leading to an important increase in the use of renewable energy.\nThis increase is happening worldwide at a pace that far exceeds many projections. According to the International Energy Agency, in 2012 around 13.2% of the world’s total primary energy supply was derived from renewable sources. While in terms of electricity, in 2013 renewables provided almost 22% of global production and the Agency estimates that this share will reach at least 26% in 2020. Moreover, certain countries are making particularly huge strides forward: according to World Bank data the use of renewable energies in 2014 as a percentage of total final energy consumption was 57% in Norway, 50% in Sweden, 36% in India, 31% in New Zealand… to mention just a few of the high achievers.\nThis trend must continue if the world hopes to mitigate climate change and reduce pollution. In fact, many experts consider tha\nt we have already moved beyond a turning point. The transition from fossil fuels to renewables is now irreversible. However, renewable energy risks becoming a victim of its own success: without economically viable storage options, renewable energy integration has been supported by fast-reacting fossil-based technologies, which act as back-up capacity to compensate for supply variability. But as the push for clean energy continues and renewable resources account for an increasing proportion of generated energy, there is simultaneously more variability and less fossil fuel buffering to absorb supply chain shocks.\nThe integration of renewable energy presents a challenge to one of the fundamental imperatives of the power industry: the need to match electricity supply and demand. Why so?\nGrid frequency fluctuates continuously, determined by the real time balance of demand and generation.\nElectricity networks have a utility frequency of 50 Hz or 60 Hz, because this corresponds to an efficient rotation speed for the steam turbine engines that power most generators (3,000 RPM for 50 Hz, 3,600 RPM for 60 Hz). This frequency must be tightly controlled to ensure the safe operation of the grid and also because appliances are specifically designed to operate at one of these frequencies.\nHowever, grid frequency fluctuates continuously, determined by the real time balance of demand and generation. If demand is greater than generation, the frequency falls, if generation is greater than demand, the frequency rises. The network operator has to compensate for such changes by requesting more or less generation in the hour leading up to real time, to keep the frequency in the acceptable range.\nTraditionally, power systems have been based on fossil or nuclear-powered plants, which have rotating generators that can rotate slightly slower or faster to compensate for the immediate imbalance between power supply and demand, allowing the required frequency to be maintained.\nAnd that’s where the integration of renewables raises issues. Renewable energy sources, notably wind turbines and photovoltaic units, do not have the rotational inertia of conventional generators. This has implications on grid stability in terms of frequency control and ensuring the stable operation of the power grid, because renewables are displacing conventional generators with their rotating machinery.\nBut also, solar and wind generation experience intermittency in terms of non-controllable variability and the partial unpredictability of the weather. This fluctuation in power output makes it more difficult to balance supply and demand on an instantaneous basis.\nMany solutions to the variability of renewable energy and its successful integration into the grid are already in use to varying extents around the world. A first step is minimizing challenges and costs from the outset by deploying a wide range of renewable energies where possible. Some sources — such as hydroelectric and concentrated solar power — offer greater control of output than others.\nThis solution can provide a higher degree of control and thus eases the challenge of balancing. It is not, however, always practical, as many renewables are geographically specific. Although this can be addressed by long-distance transmission and larger and more flexible grids with connections that cross international boundaries. The principle is that the larger area a network covers, the more chance there is that renewably generated power that is superfluous in one location will be able to supply a demand that is present elsewhere.\nAnother option is storage. The storage of mass-produced electricity has been around for many years in the form of pumped storage hydropower, which relies on gravity to capture off-peak power, releasing it at periods of high demand. This accounts for 99% of bulk power storage capacity worldwide, however its application is limited to very specific locations.\n“If you want days, months, or even years of energy storage, and in massive volumes, then you can’t do it with batteries.”\nChemical storage in the form of batteries also has potential to support the energy transition. Large batteries are already in use in industry but their implementation in managing variation in grid systems is problematic.\nDr Graham Cooley, CEO at ITM Power, a manufacturer of integrated hydrogen energy systems commented: “When most people think of energy storage, they think of batteries. And if you want one or two hours of energy storage, batteries are good. But if you want days, months, or even years of energy storage, and in massive volumes, then you can’t do it with batteries.”\nFactors such as lifespan, cycle efficiency, power leakage and production costs all need to be addressed before batteries can be considered as a viable option for grid balancing. All this isn’t sounding too promising so far. However, there is a technology that is already being implemented and has proven itself ideally suited to empowering the energy transition because it offers much more than just storage. And this solution is hydrogen.\nHydrogen can store excess renewably generated power, and do so reliably, in massive amounts and over long periods of time. It can also be used to power fuel cell vehicles which produce zero local emissions and provide energy for the industry. It can be transported in high energy density forms either in a gas or liquid, and injected into gas networks to supplement or replace natural gas. So how does it work?\n“An electrolyser can be turned off and on very rapidly”\nThe process, known as Power-to-Gas (P2G), involves converting surplus energy into hydrogen gas by rapid response electrolysis and its subsequent injection into the gas distribution network. Graham Cooley explained: “P2G is about making hydrogen using renewable power. You use a device called an electrolyser that splits water into hydrogen and oxygen when an electric current is passed through. You use the hydrogen either in the gas grid or for refueling vehicles. And because an electrolyser can be turned off and on very rapidly, it provides very efficient grid balancing.”\n“An electrolyser receives signals from the grid based on frequency. When the network is overloaded and the frequency starts to fall, the electrolyser is switched off. When the network is under loaded, or has an excess of renewable power, it receives a signal to turn on. Our electrolysis equipment can be turned on and off in less than a second and can be bid into primary grid balancing (Enhanced Frequency Response or Frequency Control Demand Management).\n“When switched on, the hydrogen produced goes straight into the gas grid. In this way, when we are balancing against rising frequency, an electrolyser can be used to absorb an excess of renewable power. The gas grid in the UK for example is three times the size of the power grid: there’s about 350 terawatt hours of energy flowing through the electricity grid every year, and about 1,000 terawatt hours in the gas grid. The primary difference between the two energy networks is that electricity has no energy storage, whereas there’s a huge amount of energy storage in the gas grid.”\nIt has been shown that hydrogen-enriched gas, containing up to 20% hydrogen, has no adverse impact on gas appliances. Surplus renewable power to produce hydrogen by electrolysis can also be combined with CO2, for example from biogas production, to form synthetic natural gas — which is the exact same molecule as the fossil natural gas that is used today — that can also be injected into gas networks.\nThen there’s the role of renewably-produced hydrogen in carbon-free transport. “The same principle of using electrolysers for grid balancing can be used to generate very low cost hydrogen for fuel cell electric vehicles,” said Graham Cooley. “Electrolysers can be installed directly at refueling stations so that hydrogen is made on site, meaning there’s zero carbon footprint involved in transport. The electrolyser is turned on and off according to the grid company’s balancing requirements, but here the hydrogen is stored in tanks and then deployed to vehicles. This means the hydrogen has the lowest possible carbon footprint and is absolutely clean: the water has never seen any carbon molecules and does not need purifying for use in vehicles.”\n“Electrolysers can be installed directly at refueling stations.”\nWhile this sounds promising, a number of critics have claimed that hydrogen fuel cell vehicles are a dead end technology. Prof. Dr. Christian Mohrdieck, Daimler’s Fuel Cell Director, addressed this point: “A very popular argument in the public discussion is that battery electric vehicles are more efficient than hydrogen. This is true because the conversion of hydrogen is an extra step that is not involved in battery electric vehicles. However, this is not taking the whole picture into account.\n“If we want to move to renewable energy then we have to consider all aspects”\n“We want to move away from fossil fuels towards renewable energy, and this means we also need to store electricity. So the big challenge is how can we store huge amounts of electricity over a long time, even from one season to the next. Very few technologies can do this, but hydrogen is one of them. Batteries cannot.” Indeed, batteries alone would not be able to do the work because the amount of energy to be stored for seasonal storage is too high. The amount of batteries needed would be significant — material, size and cost-wise.\n“Once you factor in the need for hydrogen in the storage of renewable electricity, then hydrogen and battery electric vehicles become very close in terms of efficiency. This is a complex picture, but if we want to move to renewable energy then we have to consider all aspects”, Prof. Dr. Christian Mohrdieck adds.\nIn the past, the use of clean, electrolysis-formed hydrogen has been hampered by relatively high production costs. The capital price and running costs of electrolysers prevented this solution from being sustainable. This major issue is on the verge of being solved, as production prices have decreased steadily year after year. Today, hydrogen production cost depends mainly on electricity cost, operational hours and CAPEX (capital expenditure). Thanks to increasingly lower renewable costs and economies of scale, clean hydrogen will soon be price-competitive. In regions that combine medium electricity prices (between 30 and 60 dollars per MWh) and a medium utilization factor, the cost of producing hydrogen can be around 4–5 dollars per kilogram.\nHuman societies now depend on a 24/7 energy supply. But neither the sun shines nor the wind blows all day, every day. And our electricity system is designed to function through the precise matching of supply and demand.\nHydrogen is one of the solutions to this intermittency problem. It can be produced from renewable energy through electrolysis and used for grid balancing, energy storage, heating, for refueling fuel cell vehicles and as a source of hydrogen for industry. If the world truly intends to avoid climatic catastrophe, then expanding the use of renewable energy is essential, and hydrogen could well be the key to unlock its full potential.']	['<urn:uuid:a246aa21-500a-4090-bf06-77c1e7b84084>', '<urn:uuid:9a216169-7000-4b68-8864-7e5eea49e9bb>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T05:28:56.360010	5	60	3538
84	What are the main economic challenges that have made it increasingly difficult for professional jazz musicians to earn a living in recent decades?	There are three major economic challenges: First, technological developments have made it very hard to earn money from recordings since music can now be easily streamed or downloaded. Second, the live music gig scene has severely decreased - work that was previously available for theatre, pop bands, parties and weddings has largely dried up, and the pay hasn't kept up with inflation. Third, both private and public sources of arts funding are drying up, following what has been described as an 'arts funding bubble' over the past 60 years, with benefactors now favoring other charitable causes like hospitals and education.	['Written by Kurt Ellenberger from National Public Radio\nLast week, we published a much-discussed blog post about the connection — or lack thereof — between jazz education and the development of new audiences. It examined a viewpoint by pianist and composer Kurt Ellenberger, and concluded by challenging Ellenberger to suggest some ways to win new audiences. Here is Ellenberger’s response. –Ed.\nSince my Huffington Post article on jazz education and audience development, many (including this very blog) have asked “Well, if education isn’t the answer, what’s the solution? How do we develop and maintain a strong jazz audience?”\nAudience development is a complicated issue, and it’s not limited to jazz. Every artist and arts organization is trying to answer the same question. We’ve identified a problem and we’re going to “build” something to solve it. Sounds so simple, doesn’t it?\nIt’s not simple. It’s so complex, in fact, that I think that the question itself is actually a linguistic deception, a euphemism perhaps, that cleverly masks the enormity of the task. When we ask “How do we develop and maintain a strong jazz audience?” what we are really saying is “How can we convince millions of people to alter and expand their aesthetic sensibilities and their cultural proclivities so that they include jazz to such an extent that they will regularly attend concerts and purchase recordings?” And that statement itself is embedded within another Herculean task: “How can we convince people to embrace music that is no longer part of the popular culture?”\nAssuming that many readers here are probably jazz or classical music people, I think it might also be helpful to consider the question from another angle. Imagine for a moment that country music is on the skids, suffering from poor sales and anemic concert attendance. Nashville commissions a consulting firm to work on building the audience for country music. Is there anything they could do to get you to become a country music fan? Or a heavy metal fan? Or trance?\nI anticipate responses to these questions to the effect of “Well, those styles of music are harmonically and rhythmically quite simple, and the pieces are very short, so it doesn’t appeal to me very much. I need more content in my music.” This identifies yet another problem area in this discussion: namely, the fact that most of the music we are trying to build an audience for is cognitively demanding. So we’re looking for some marketing, education, packaging or programming strategy that will influence and/or supersede both personal taste and the enormous pressures of the dominant popular culture; at the same time, we’re asking people to commit to an art form that will tax (and probably frustrate) their capabilities before, hopefully, delivering a heightened aesthetic experience.\nThat’s a tall order that seems insurmountable. Frankly speaking, it can’t be done, at least not as part of a prefabricated “strategy” to build an audience. You’d no sooner be able to create a sustainable audience base for jazz as you could for medieval plainchant. If a solution existed, wouldn’t one of the thousands and thousands of creative artists, agents, managers and the many jazz collectives, societies and alliances have found it? I realize that this is not what anyone in the jazz community wants to hear, but I also don’t think it is helpful to continue pretending that there is a solution out there somewhere, just waiting for us to discover it.\nJazz simply needs to continue doing what made it great in the first place: engage with popular culture in an intelligent, nuanced and sophisticated manner, as some successful groups are doing today. If there is any hope of audience building, this is where it lies. It must be organic, visceral and culturally relevant, qualities which cannot be consciously conjured by an audience development committee.\nWhat we’re really talking about when we complain about the jazz scene (and the arts scene in general) is not that jazz is dying creatively, or that it’s lost its vitality. It’s that there isn’t enough work and the work that’s there doesn’t pay enough. Those of us who were born between 1950 and 1970 came up in a very different environment than that which exists today in at least three ways:\n- Technological developments have made it very hard to earn money from recordings. If you can attach it to an email, stream it or download it, it’s going to be very hard to make money from it. This is especially true for a niche genre in the fine arts.\n- The gig scene is severely truncated. When I was in my twenties, there was plenty of live music work — theatre, pop bands, some recording work, parties, weddings and, of course, some actual jazz. It was possible to eke out a reasonable living just playing music, but that work has largely dried up. (Let’s face it: Most of that work is not artistically satisfying anyway. It’s either party music or background music, not exactly the stuff of artistic dreams.) And, as I’ve written about previously, the pay hasn’t kept up with inflation. Where it was possible to make $25,000 a year in 1984, you’d need to be making $52,000 in 2010.\n- Private and public sources of arts funding are drying up. We’ve experienced an “arts funding bubble” during the last 60 years, and that bubble is bursting. In Art Lessons: Learning from The Rise and Fall of Public Arts Funding, the late Alice Goldfarb-Marquis details how a surge in public funding for the arts occurred in the middle of the last century created an artificial demand for artists and arts organizations of various types (dance, music, theatre, etc.). She also identifies how those benefactors began to move away from arts funding in favor of other charitable donations (hospitals, education, etc.) that provided them with the visibility and cachet that the arts no longer provided. We’ll be lucky, therefore, if those sources maintain the funding they are providing now, because they certainly aren’t going to substantially increase funding during a time when the national debt is already a matter of grave concern. (I’ve written about this before.)\nI think it’s clear that obtaining a reasonable income in jazz and classical music is becoming exceedingly difficult. Those of us who grew up in the arts bubble were very fortunate to come up in an era that was, relatively speaking, flush with cash, which makes the new reality very difficult to accept. But historically speaking, this was an aberration. Beethoven had money problems, Mozart died broke, and I’m sure that we’re all aware of the many incredibly talented and influential jazz musicians of the last 75 years who needed benefit concerts to pay for medical care and funeral expenses as they entered middle and old age.\nI suggest that we consider doing what many classical and jazz musicians that I know have done during the last decade. It’s not an earth-shattering proposition that we haven’t heard before: a separation, as it were, of church and state. Make money doing something else, and keep your music pure.\nPlay only what you want to play, don’t play casuals, weddings or anything else that isn’t artistically satisfying. Let the young players do that work — they need the experience, we don’t. Write and play music without concern for “selling it” or “marketing it”; do it because it’s fun to do and do it with people you enjoy playing with. Rent a venue and put on concerts of your music, but don’t expect to make a profit. A very small percentage of these projects just might garner enough attention to make them financially profitable, and an even smaller percentage might become “famous,” but most won’t.\nWhat I’ve seen from people who have left the “business” behind is that they are generally much happier. Granted, it’s not an easy decision to make. We have so much of our self-image invested in the romantic mythology that swirls around jazz, and that mythology includes the idea that a “day job” is a cop-out, the kiss of death, not something that a “true artist” would ever do. So while the pressures of the dominant culture may be very difficult to overcome, the pressures of jazz culture are equally as daunting. But it seems to me that if a creative and artistically independent life is the goal, then the solution, in an era of dwindling support and an apathetic public, is to finance it with other productive work.\nAs aggravating and depressing as all of this may be, I don’t see it as a “doom and gloom” scenario; to the contrary, I think that jazz is actually thriving, not dying. In one of my first blog posts, I stated the following:\n…we live in a time of remarkable creative growth for music in general, and for jazz in particular. With the ability to record and distribute music independently and inexpensively, and the resultant unlimited access that this provides, we’ve seen an explosion in musical activity in all genres, jazz included. Visit CDBaby (for example) and search for jazz, and you’ll find a staggering number of accomplished jazz artists, mostly unknown, making music of all kinds under the “jazz” umbrella (including genre descriptions that defy description like “Industrial Fake Jazz”). And look at the tremendous quality and quantity of European jazz being produced, with its unabashed blending of styles which has led to many heated discussions on how jazz is defined at this juncture in the 21C. There are so many amazing musicians creating such a fantastic smorgasbord of new jazz, that one can hardly even keep up with a small percentage of it.\nIn short, this era exhibits unparalleled creative activity and creative potential for jazz and for the arts in general … despite the issues I wrote about previously, there are reasons to be very hopeful about the future of jazz from a creative viewpoint.\nJazz as a creative force is not going away. In fact, I would go so far to stay that it will never go away because of the depth of its materials, its rich history and canon, and its openness to new influences.\nWasn’t jazz a street music to begin with? A hybrid that drank from many wells and remade itself every decade (much to the chagrin of many artists then and now)? Why not write music that utilizes electronics and looping, hip-hop, rap, gamelan, minimalism, trance, rock, yodeling, country and anything else that you listen to and find interesting? These things will happen because people need to express themselves, not because they need to land a gig.\nAnd that is why I think it is unwise to finance art through art. Separate church and state, and maybe, just maybe, an audience will coalesce around your music. You’ll be as surprised as everyone else.']	['<urn:uuid:39e40aca-465e-48d2-9b86-84ac95b612ee>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T05:28:56.360010	23	100	1807
85	As a chemical engineer researching storage systems, I'm curious about how hydrogen behaves in its liquid form and what happens to its molecular structure at different temperatures - can you explain both aspects?	In liquid form, hydrogen is stored at extremely cold temperatures of -423°F and has a dramatic volume ratio of 1:850 between its liquid and gaseous states. When liquid hydrogen warms to room temperature, it undergoes rapid phase change, expanding 848 times in volume. As for molecular behavior across temperatures, the molecules' movement depends on their kinetic energy - at very low temperatures, molecules have low kinetic energy and are held together by molecular attraction forces. As temperature increases, molecules gain more kinetic energy and move more rapidly, breaking free from these constraints. At room temperature, hydrogen exists as a gas because its molecules have enough energy to overcome the attractive forces between them.	"['Key words: States of Matter, solid, liquid, gas, plasma, heat, temperature, energy, molecular forces, water, ice, steam, molten metal, Helium, ionized, Sun, Physics, Ron Kurtus, School for Champions. Copyright © Restrictions\nStates of Matter\nBy Ron Kurtus (revised 18 August 2013)\nMatter exists on Earth in three major states or phases: solid, liquid, gas. These states describe distinct physical characteristics of the material and are a function of temperature and pressure.\nAt very high temperatures, matter becomes a plasma or ioinzed gas. Some scientists designate a plasma as a fourth state of matter. There are also other forms of matter at both very high and very low temperatures that may be considered states of matter.\n(See Exotic States of Matter for more information.)\nMajor physical characteristics define each state and primarily concern volume and shape. Kinetic energy of the molecules and attractive forces determine the physical characteristics of each state. Many common materials can be seen in the various states of matter.\nQuestions you may have include:\n- What are the physical characteristics of the states of matter?\n- Why do the characteristics change?\n- What are some examples of materials in different states?\nThis lesson will answer those questions. Useful tool: Units Conversion\nThe primary physical characteristics of the various states of matter are the volume and shape of the material. They are what really define the state. Other characteristics—such as color—may be different in the various states of matter, but they do not define the state as do volume and shape. Gravity has an effect on the shape of liquids and gases.\nThe solid state of matter is when the material has a definite volume or size and distinct shape at a given temperature.\nAt room temperature, a piece of iron at has a shape and size that does not change. Ice is another solid, but its temperature must be below 0o C (32o F).\nMost solid materials expand with increasing temperatures, but they retain their shape.\nA liquid has a definite volume, but it takes the shape of its container with the help of gravity.\nFor example, if you pour water into a cup or container, it will take the shape of that container. If you put water in a balloon, the water will take the shape of the balloon, no matter how you change the shape of the balloon.\nOn the other hand, in outer space, where there is no gravity such as in the Space Shuttle, water might float out of its container. Its shape will vary, but its volume will remain constant if the air pressure and temperature are constant.\nMost liquids expand with an increase of temperature and constant air pressure.\nThe volume of a quantity of gas is dependent on its temperature and the surrounding pressure. If affected by gravity, it will take the shape of its container, but much of it will also spread out into the surrounding area.\nAssuming little or no convection or circulation, a heavier gas will settle to the bottom. For example, carbon dioxide gas (CO2) is heavier than air. If a quantity of CO2 is put in a cup, it sink to the bottom and take the shape of the cup. After a while, some of the CO2 will mix in with the air, due to convection—perhaps becoming part of a solution with the air.\nThis can be demonstrated using Dry Ice (frozen CO2) and pouring the ""smoky"" gas into a cup. It will settle to the bottom but will soon dissolve into the air.\nIf you put a gas in a cylinder and apply pressure with a piston, such as you might do with a tire pump, the volume of the gas can change considerably. This is not the case with water or a solid. Their volumes may change only slightly with an increase of pressure.\nReasons states have characteristics\nThe reason the various states of matter have their particular physical characteristics and behave in the way they do is a result of the motion of the molecules and the attraction between them.\nTheories and forces\nThe Molecular Theory of Matter, Theory of Heat and the Molecular Attraction Force affect the states of matter.\nMolecules in motion\nThe Molecular Theory of Matter and the Theory of Heat state that molecules are in constant motion. The greater the temperature, the greater the kinetic energy and thus the faster the molecules are moving. This also applies to substances made up of only atoms, such as iron or pure aluminum.\nBut also, Molecules are often attracted to each other, due to the Molecular Attraction Force. This force is strongest among similar molecules or atoms. It is a form of electrostatic attraction that also is the factor in static electricity.\nFight between the two\nThe kinetic energy of the molecules or atoms tends to spread them apart, while the molecular attraction tends to pull them together. The temperature or energy will then determine which force wins out.\nMolecules in solids vibrate\nIf the kinetic energy (and thus the velocity) of the molecules of a material is so low that the molecular attraction between the molecules is much stronger than the forces pulling the molecules apart, the material will be a in solid state of matter. That means the molecules become fixed in place and often line up in a crystalline arrangement. The molecules or atoms still have some energy, but their movement is confined to just vibrating in place or rotation.\nAt the very lowest temperature possible—Absolute Zero (0 degrees Kelvin or 0o K)—all motion stops and the atoms and molecules do not vibrate or even spin.\nMolecules in liquids loosen structure\nIf the the molecules of the material have enough kinetic energy and are moving fast enough, they can break out of the constraints of a defined structure. The energy overcomes the much of the molecular force. This force is now only strong enough to hold the material together in the form of a liquid.\nMolecules in gases run free\nWith a gas, the molecules (or atoms) are more energetic and are moving rapidly. Their kinetic energy is greater than the attractive force between them. Thus, a gas will easily spread and not stay in an open container.\nExamples with different materials\nLook at various materials to see their different states.\nA good example of how matter can exist in different states is water. It is normally a liquid at room temperature, but if you cool it below 32o F (0o C), it will freeze and become the solid we call ice. If you heat water above 212o F (100o C), it will boil and turn into a gas we call steam.\nYou\'ve seen pictures of molten iron in a foundry. They heat the iron to a very high temperature, such that it turns into a glowing, yellow liquid. If iron was placed in an extremely hot environment like on the Sun\'s surface, it would boil and turn into a gas.\nHelium is a gas at room temperature, but if it is cooled to a few degrees above Absolute Zero (degrees Kelvin), it turns into a liquid. It is the only liquid that cannot be solidified at atmospheric pressure by simply lowering its temperature. Scientists have been only able to solidify liquid Helium at the extreme pressures of 25 atmospheres. (1 atmosphere = 14.7 pounds per square inch).\nLiquid Helium has some very strange properties. For example, if it is poured in a flask, it will first settle at the bottom of the flask and then start to climb up the sides and soon flow out of the container.\nThere are three accepted states of matter: solid, liquid, gas. There may be a fourth state, called the plasma state. As the energy of the moving molecules overcomes the molecular attraction forces, the material will change its state from solid to liquid to gas. The solid iron, liquid water and helium gas are examples of different states of matter at room temperature.\nBe observant of what happens to learn why it happens\nResources and references\nQuestions and comments\nDo you have any questions, comments, or opinions on this subject? If so, send an email with your feedback. I will try to get back to you as soon as possible.\nClick on a button to bookmark or share this page through Twitter, Facebook, email, or other services:\nStudents and researchers\nThe Web address of this page is:\nPlease include it as a link on your website or as a reference in your report, document, or thesis.\nWhere are you now?\nStates of Matter', 'Hydrogen Compared with Other Fuels\nLike gasoline or natural gas, hydrogen is a fuel that must be handled properly. It can be used as safely as other common fuels when simple guidelines are followed.\nProduced by the U.S. Department of Energy, the Hydrogen Data Book. provides useful data on hydrogen properties, including:\n- Chemical characteristics of hydrogen (e.g., density, flammability range, boiling point characteristics, heating values)\n- Comparisons of the characteristics of hydrogen and many other fuels.\nBar charts presented in the following sections compare some key properties of hydrogen with those of several commonly used fuels - natural gas, propane, and gasoline vapor.\nHydrogen is colorless, odorless, tasteless, non-toxic, and non-poisonous. It’s also non-corrosive, but it can embrittle some metals. Hydrogen is the lightest and smallest element, and it is a gas under atmospheric conditions.\nNatural gas and propane are also odorless, but industry adds a sulfur-containing odorant so people can detect them. Currently, odorants are not used with hydrogen because there are no known odorants light enough to ""travel with"" hydrogen at the same dispersion rate. Current odorants also contaminate fuel cells, which are an important application for hydrogen.\nHydrogen is about 57 times lighter than gasoline vapor (as shown in Figure 1) and 14 times lighter than air. This means that if it is released in an open environment, it will typically rise and disperse rapidly. This is a safety advantage in an outside environment.\nFigure 1. Relative Vapor Density\nHydrogen is a very small molecule with low viscosity, and therefore prone to leakage. In a confined space, leaking hydrogen can accumulate and reach a flammable concentration. Any gas other than oxygen is an asphyxiant in sufficient concentrations. In a closed environment, leaks of any size are a concern, since hydrogen is impossible for human senses to detect and can ignite over a wide range of concentrations in air, as discussed in the section below. Proper ventilation and the use of detection sensors can mitigate these hazards.\nHydrogen has a high energy content by weight, but not by volume, which is a particular challenge for storage. In order to store sufficient quantities of hydrogen gas, it\'s compressed and stored at high pressures. For safety, hydrogen tanks are equipped with pressure relief devices that will prevent the pressures in the tanks from becoming too high.\nThe auto-ignition temperature of a substance is the lowest temperature at which it will spontaneously ignite without the presence of a flame or spark. The auto-ignition temperatures of hydrogen and natural gas are very similar. Both have auto-ignition temperatures over 1,000°F, much higher than the auto-ignition temperature of gasoline vapor, as shown in Figure 2.\nFigure 2. Auto Ignition Temperature\nHydrogen\'s flammability range (between 4% and 75% in air) is very wide compared to other fuels, as shown in Figure 3. Under the optimal combustion condition (a 29% hydrogen-to-air volume ratio), the energy required to initiate hydrogen combustion is much lower than that required for other common fuels (e.g., a small spark will ignite it), as shown in Figure 4. But at low concentrations of hydrogen in air, the energy required to initiate combustion is similar to that of other fuels.\nFigure 3. Flammability Range\nFigure 4. Minimum Ignition Energy\nHydrogen burns with a pale blue flame that is nearly invisible in daylight, so it is almost impossible to detect by the human senses (see Hydrogen Flame Characteristics video under Supporting Examples in the right column of this page). Impurities such as sodium from ocean air or other burning materials will introduce color to the hydrogen flame. Detection sensors are almost always installed with hydrogen systems to quickly identify any leak and minimize the potential for undetected flames. Compared to the propane flame (right) in Figure 5, the hydrogen flame (left) is almost invisible, but it can be seen with the thermal imaging camera shown in the foreground. At night, hydrogen flames are visible, as shown in Figure 6.\n|Figure 5 - Hydrogen and Propane Flames in Daylight\n(Photo courtesy of HAMMER)\n|Figure 6 - Hydrogen and Propane Flames at Night\n(Photo courtesy of ImageWorks)\nIn addition, hydrogen flames radiate little infrared (IR) heat, but substantial ultraviolet (UV) radiation. This means that when someone is very close to a hydrogen flame, there is little sensation of heat, making inadvertent contact with the flame a significant concern. UV overexposure is also a concern, since it can result in sunburn-like effects.\nIf a large hydrogen cloud comes into contact with an ignition source, ignition will result in the flame flashing back to the source of the hydrogen. In open spaces with no confinement, flames will propagate through a flammable hydrogen-air cloud at several meters per second, and even more rapidly if the cloud is above ambient temperature. The result is a rapid release of heat, but little overpressure, and the combustion product is steam. It should be noted that hydrogen combustion is more rapid than combustion of other fuels. A hydrogen cloud will burn within seconds, and all of the energy of the cloud will be released.\nHowever, if hydrogen gas mixtures enter confined regions, ignition is very likely and can result in flame acceleration and generation of high pressures capable of exploding buildings and throwing shrapnel. Flammable mixtures of hydrogen in confinements such as pipes or ducts, if ignited, will readily result in accelerated flames and conditions that can lead to transition to detonation. Detonation does not occur in unconfined hydrogen-air mixtures without strong shockwaves (i.e., explosives).\nA leak in a pressurized (>200 psia) hydrogen storage system will result in a jet that may extend for some meters. If ignited, the jet flame can cause serious damage to anything it encounters.\nLiquid Hydrogen Expansion\nLiquid hydrogen has different characteristics and different potential hazards than gaseous hydrogen, so different control measures are used to ensure safety. As a liquid, hydrogen is stored at -423°F, a temperature that can cause cryogenic burns or lung damage. Detection sensors and personal protective equipment are critical when dealing with a potential liquid hydrogen leak or spill.\nThe volume ratio of liquid to gas is approximately 1:850. So, if you picture a gallon of liquid hydrogen, that same amount of hydrogen, existing as a gas, would, theoretically, occupy about 850 gallon containers (without compression). Hydrogen undergoes a rapid phase change from liquid to gas, so ventilation and pressure relief devices are built into hydrogen systems to ensure safety.\nLiquid hydrogen is also colorless. It is extremely cold and only persists if maintained in a cryogenic storage vessel. Storage is usually under pressures up to 150 psi. If spilled on ambient-temperature surfaces, liquid hydrogen will rapidly boil and its vapors will expand rapidly, increasing 848 times in volume as it warms to room temperatures. If the liquid hydrogen is confined (such as between valves closing off a length of pipe) and left to warm without pressure relief, pressures approaching 25,000 psia are possible. With the exception of specially designed enclosures, there is a high potential for exposed confinements to rupture under such pressures, producing high-pressure jets of gas and high-speed shrapnel. Ignition is extremely likely under such circumstances. If large quantities of hydrogen displace the oxygen in the air, hydrogen will act as an asphyxiant.\nHydrogen Properties Influence Design\nAn understanding of the properties of hydrogen is critical for the proper design of a facility or workspace. A workspace can be configured to mitigate hazards by understanding and taking advantage of some of the characteristics of hydrogen. Some typical properties of hydrogen, methane, and gasoline are presented in the section Impact of Hydrogen Properties on Facility Design.']"	['<urn:uuid:2f1d66ef-e0c2-47b9-928d-dea9c7126d3a>', '<urn:uuid:51c11511-00c6-4d0f-9d59-c3ad127e17c5>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T05:28:56.360010	33	113	2699
86	want to break closed mortgage early how much penalty will bank charge	The penalty for breaking a closed mortgage early is typically the higher of two amounts: three months of interest on the remaining mortgage amount, or the Interest Rate Differential (IRD). For example, on a $200,000 outstanding amount at 6% interest, the three-month penalty would be $3,000. However, if using the IRD method (which applies when the current market rate is lower and the mortgage is less than 5 years old), the penalty could be much higher. For instance, with a 2% rate difference and 36 months remaining, the penalty would be $12,000.	['Table of Contents\n- What Is A Closed Mortgage?\n- Costs Involved In Paying Off A Closed Mortgage Early\n- Pros And Cons Of Breaking A Closed Mortgage Contract\n- How To Reduce Or Avoid Prepayment Penalties?\nPaying off an open mortgage early is certainly possible. But what about a closed mortgage contract? Is it possible to exit from a closed mortgage contract early? The answer is YES!\nBut paying it off early is a little bit trickier than you might have hoped. Be it an open or closed mortgage, paying early in both cases tags along with some costs.\nIn the case of a closed mortgage, the prepayment penalty, breakage costs, and other charges could be higher–sometimes to the extent that paying off early becomes a costly affair. Therefore, a thorough calculation before deciding to end a closed mortgage contract is a must! Visit this site to know the differences between an open and closed mortgage.\nWhat Is A Closed Mortgage?\nA closed mortgage, also known as a closed-end mortgage, is a limiting mortgage that prohibits borrowers from paying off, renegotiating, or refinancing the mortgage early. Specific prepayment penalties, breakage costs, and other charges are involved in renegotiating or paying off the mortgage early.\nA closed mortgage is preferred over an open-end mortgage due to lower interest rates. But it limits the borrower’s ability to use the house’s equity as collateral to refinance before its term ends.\nGenerally, you should opt for a closed mortgage if:\n- You have a long-term vision\n- You’re not going to move or sell early\n- You’re searching for attractive interest rates\nPros Of A Closed Mortgage\n- It offers lower interest rates to homebuyers.\n- Borrowers have the assurance that the interest rates will not vary till the end of the term.\n- It offers substantial time to repay the amount.\n- It adds a sense of security to people with fixed, regular incomes.\nCons Of A Closed Mortgage\n- It limits homebuyers to pay off, negotiate, and refinance the mortgage before the term ends.\n- Repayment penalties, breakage costs, and other charges make it too costly.\n- It is challenging to refinance a closed mortgage.\n- It is a less flexible model.\nReasons To Break A Closed Mortgage Contract\nSeveral factors influence a homebuyer to pay off a closed mortgage early. The most popular driving factors are:\n- A homebuyer wants to sell the home before the mortgage term ends.\n- Market stability and growth have cut down the interest rates.\n- A borrower’s income has increased, so they want to pay off the mortgage early.\n- Refinancing to find a better rate.\n- Fulfilling the purpose of buying the home.\n- Moving out of the property for some reason.\nCosts Involved In Paying Off A Closed Mortgage Early\nDeviating from the closed mortgage contract could impose a prepayment penalty, charge, or breakage cost. The lender will charge a prepayment penalty if a borrower:\n- Wants to pay more than what is in agreement.\n- Opts to break a closed mortgage contract.\n- Transfers the mortgage to another lender.\n- Wants to renegotiate the terms of the contract.\nIf not planned or in case of an uncalculated decision, a borrower may end up paying more on breaking the close mortgage contract than they’d have otherwise paid on abiding by the contract.\nTo safeguard the interests of borrowers, an option called prepayment privilege is available. Prepayment privilege is a powerful tool that eliminates prepayment penalties for breaking a closed mortgage contract. It offers:\n- Flexibility to increase the regular payments up to a certain limit\n- Make lump-sum payments of up to a certain percentage\nCalculation Of Prepayment Penalties\nThe calculation of prepayment penalties depends upon several factors like repayment amount, remaining term, and lender. Most established lenders, financial institutions, and banks provide a prepayment penalty calculator on their official websites.\nGenerally, lenders charge a penalty equal to three months of interest on the remaining mortgage amount or the interest rate differential (IRD), whichever is higher. They calculate IRD when:\n- The current mortgage rate is lower than your current mortgage interest rate.\n- Your mortgage is not older than five years.\nExample Of Prepayment Penalty Calculation\nLet’s assume the outstanding amount is $200,000 and the current interest rate is 6 percent. So, the prepayment penalty will be three months’ interest.\n200,000 x 0.06 = 12,000\n12,000 / 12 = 1,000\nSo, 1,000 x 3 = $3,000 will be the prepayment penalty.\nNote that the IRD calculation comes into effect if 36 months are left from the entire term of five years.\nSuppose the current interest rate is 4 percent. Here, the difference between rates becomes 2 percent.\n$200,000 x 0.02 = $4,000\nNext, divide the term left with 12:\n36/12 = 3.\nFinally, $4,000 x 3= $12,000 will be the prepayment penalty as per the IRD method.\nLenders will charge $12,000 since it’s the highest of the two figures.\nPros And Cons Of Breaking A Closed Mortgage Contract\nGenerally, a borrower will break a closed mortgage contract if he comes across a better interest rate. Therefore, it’s advisable to weigh its pros and cons before going through with it.\n- Better mortgage rate with an extended-term.\n- You get peace of mind if you pay off the mortgage early.\n- Homeowners can use their house’s equipment to refinance their other needs.\n- You get an option to renegotiate the mortgage rate and term.\n- The idea of paying off early or renegotiating is to put off the burden and save the remaining interest. But the hidden charges and prepayment penalty associated with breaking the contract could cost you more in the long run.\n- There’s a possibility that a homebuyer fails to secure a new mortgage or refinance under stringent new economic conditions.\nHow To Reduce Or Avoid Prepayment Penalties?\nThe first step should be to understand the terms and conditions of the contract before inking it. Some useful tips to avoid prepayment penalties are:\n- Include the prepayment privilege option in the contract.\n- Utilize the prepayment privilege option fully to repay the mortgage amount early–this helps reduce the mortgage amount significantly. Keep in mind that lenders calculate a prepayment penalty on the remaining balance.\n- You can port your mortgage or blend and extend the mortgage to avoid prepayment penalties.\n- Wait till the term ends to reduce the prepayment penalty.\nAlways calculate the potential costs associated with breaking a closed mortgage contract. If ignored, you end up paying more in the form of prepayment penalties and other charges. Lenders are advised to research, compare, and seek expert advice before signing or breaking a closed mortgage contract.']	['<urn:uuid:888eb12d-9fdc-43f2-8aed-866d42378502>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T05:28:56.360010	12	92	1118
87	postwar veteran support korea vietnam difference	The treatment of veterans differed markedly between the two conflicts. Korean War veterans like Billy Fergot were honored through programs like the Never Forgotten Honor Flight, allowing them to visit war memorials. In contrast, Vietnam veterans faced more complex challenges, with Michael Uhl describing class resentments, disputes over POW/MIA issues, and the need to form veterans' rights groups like Citizen Soldier to advocate for their interests.	"['Michael Uhl. The War I Survived Was Vietnam: Collected Writings of a Veteran and Antiwar Activist. Jefferson: McFarland, 2016. 290 pp. $29.95 (paper), ISBN 978-1-4766-6614-3.\nReviewed by Evan R Ash (Miami University of Ohio)\nPublished on H-FedHist (January, 2018)\nCommissioned by Caryn E. Neumann (Miami University of Ohio Regionals)\nThe War I Survived Was Vietnam is a culmination of Michael Uhl’s forty years of veterans’ rights and antiwar activist writings. Uhl, a Georgetown graduate, served as a first lieutenant in combat intelligence during Vietnam, and became intimately involved in antiwar movements such as Vietnam Veterans Against The War and the Citizens Commission of Inquiry after his medical discharge. In his later years, Uhl and military lawyer Tod Ensign (a frequent co-contributor) founded Citizen Soldier, a veterans’ rights group. Uhl’s writing and speaking career began during the war and the latest material from the book dates from 2011. Much of the material penned by Uhl, an independent scholar, appeared in magazines such as The Nation, In These Times, and The Progressive, online leftist news sites like CounterPunch and InTheMindField, major newspapers near Uhl’s residence in Maine including the Boston Globe, the Bangor Daily News, and the Vermont Herald, as well as in newsletters from Uhl’s Veterans for Peace organization.\nThe book does not read as a historical monograph, and those expecting a lengthy bibliography or list of citations/notes will be sorely disappointed. The War I Survived Was Vietnam instead takes the form of a somewhat disjointed anthology of antiwar polemics, containing magazine and newspaper articles, poetry, speech transcripts, book reviews, an essay, and selections from longer monographs. The works in the book draw from Uhl’s involvement with post-Vietnam protest movements, inquiry commissions, and his current activities with Veterans for Peace and other leftist veterans’ rights groups. Uhl’s writings occupy a literary spectrum whose eclecticism lacks an identifiable, easily discernible thematic thread. Still, his wartime experience, coupled with his decades-long involvement in multiple antiwar/leftist veterans groups, gives his writing an intimate, accessible style that reflects his public writing background.\nThe anthology follows a fairly standard organizational scheme, opening with a set of featured articles, a series of self-selected articles Uhl considers to be his best. Due to the large number of works in the book and because my review would be considerably too long if I were to review each column individually, I will comment mainly on these featured articles. Uhl follows these with his antiwar poetry, followed by his “Being a Veteran in America” essay. A small section following the essay covers Uhl’s writings on post-traumatic stress disorder, and after a lengthy section of book criticism and reviews, the book closes with around eighty pages divided more or less equally between explicitly activist writing and articles co-authored with veterans’ rights lawyer and Citizen Soldier co-founder, the late Tod Ensign, including three chapters reprinted from their book GI Guinea Pigs (1980), a work describing the effects of chemical herbicides like Agent Orange on American soldiers.\nUhl’s selected writings appear to mainly fall in two categories: criticisms of US government action during the war and toward Vietnam, plus popular misconceptions which undermine or distort public memory of Vietnam. For example, Uhl criticizes the government for many reasons, but chief among them is his contention that the government held rank-and-file GIs criminally responsible for atrocities such as the My Lai massacre, while high-ranking generals and chiefs of staff who gave the orders escaped both blame and criminal charges. In “Searching for Vietnam’s M.I.A.s,” a 1994 Nation article, he lambastes veteran groups for their use of the POW/MIA issue to stir popular discontent against the payment of reparations to Vietnam, payments avoided by every administration since Nixon, according to Uhl.\nWhile he offers no evidence in support of that claim, he engages heavily with a previous Nation article, “M.I.A.sma,” by H. Bruce Franklin, as a way of delegitimizing a report surfaced by the POW/MIA lobby in response to the Clinton administration’s normalization of relations with Vietnam. According to the report, over 1,200 American prisoners remained in North Vietnam in 1972 in 11 prisons, but Franklin claims that after the Son Tay raid of 1970, North Vietnam reduced its prison capacity and reported that they were holding slightly less than 400 “captured American service personnel,” noting that the NVA avoided using the term “prisoner of war.”\nUhl also reveals by way of a telephone conversation with an old friend in “The Spat-Upon Vet Revisited” the implicit class imbalance between middle-class protesters and the working-class soldiers who shouldered the load of conscription during Vietnam. “The Spat-Upon Vet” reflects Uhl’s question of how some remember Vietnam, focusing particularly the manufactured memories of protesters who repeatedly spat on returning veterans. Jerry Lembcke, who wrote a similar article in the New York Times, speculates that “[Listeners are] loath to question the truth of the stories lest aspersion be seemingly cast on the authenticity of the teller,” even though the narrative served the interests of the military-industrial complex. “There were a lot of cry-baby vets,” writes Uhl, “who couldn’t get their dad’s ‘good’ war out of their imaginations, and who knew goddamned well that Vietnam was no noble cause.... Class resentment runs deep and gets tragically misplaced in this society, while divide and rule fuels the myth that vets were spat upon, even when they weren’t” (pp. 31-33).\nThrough another featured article, “Vietnam’s Shadow Over Abu Ghraib,” Uhl addresses change and continuity over time with regard to media coverage of American war atrocities. The subtitle for the article is “What did Sy Hersh know, and when did he know it?,” referring to Seymour Hersh, the investigative reporter known for exposing the My Lai massacre as well as its cover-up. The confusing subtitle underscores this article’s main issue—Uhl does not seem to know what point he is trying to make. He acknowledges the American news media covered the Abu Ghraib scandal in a willing manner whereas My Lai did not merit the same quality coverage, and draws frequent parallels between My Lai, Abu Ghraib, and Hersh’s reporting. Uhl criticizes Hersh for suddenly being interested in examining the systemic causes of military abuse after Abu Ghraib when he had not given My Lai the same journalistic scrutiny. However, Hersh does note that the official government response from the Nixon and Bush administrations amounted to the same banal criticisms of a few bad apples.\nFitting with Uhl’s theme of reexamining questions of power imbalances between military brass and rank-and-file GIs, as well as the implicit class biases, he praises Hersh for noting these failures of Army leadership, writing that “affixing primary responsibility for atrocities that are hardwired into modern wars of ‘counterinsurgency’ onto the lowest-ranking soldiers, those tasked with carrying out the dirty work, while limiting the culpability of the command, is yet another echo from the My Lai massacre that resonates with Abu Ghraib” (p. 36). While “Vietnam’s Shadow Over Abu Ghraib” provides riveting parallels between the world Uhl experienced as a soldier and the one he inhabits as an activist, it falls short rhetorically because of Uhl’s weathervane-like pivoting between sundry criticisms of Seymour Hersh, the US government, and the military itself, as well as his attempts to highlight class battles within the military. Despite their minor shortcomings, Uhl’s featured articles clearly express his contentions about Americans’ remembrance of Vietnam—that these popular misconceptions fueled by the country’s elite hid the more unseemly conflict of class distinction and differences over the war.\nWhere the book’s largest fault comes, however (and this could be more a problem of editorial advisement rather than authorship), is in the organization and proportionality of the writings. The most glaring representation of this problem is the “Criticism and Review” section, which comprises 22 articles that equal 106 pages, or just over a third of the book. This is not to say the section is unnecessary, but it pivots from the main points and stymies the book’s flow. Some criticisms offered, like the interview Uhl and writer Carol Brightman had with Robert MacNamara, one of Vietnam’s infamous architects, are breathtaking and revealing. Others, like the three reviews dealing with the puffery of Senator Bob Kelley, could be whittled down.\nWhile the reader sees a bevy of Uhl’s writings, they offer almost no observations on his career or the catalysts for his involvement in antiwar advocacy. According to Uhl’s memoir, Vietnam Awakening, a heated encounter with a colonel while Uhl recovered from tuberculosis (the ailment that caused his discharge) “serve[d] as one of those emancipatory moments that mark a critical transition in a person’s life ... a redemptive return to civilian status from the nightmare of war and an oppressive, authoritarian military.” It seems this is a missed opportunity to both personalize his political motivations, and provide evidence for the oppressive military Uhl was so frequently critical of.\nSimilarly, while Uhl’s writings frequently mention his time with the Citizens Commission of Inquiry, they do little to express the purpose or importance of the organization’s impact on his early activist career. Also in his memoir, Uhl states that the Citizens Commission of Inquiry served to assist disgruntled veterans, who “already possessed strong needs to communicate their disillusionment to the Middle American communities from which they sprang: these same folk who President Nixon caricatured as the silent majority ... among whom, nonetheless, the message and style of the antiwar movement played with such little sympathy.” Seeing as The War I Survived Was Vietnam serves as an ultimate collection of Uhl’s writing, the book’s failure to provide context for both the man himself, and his motivations and actions, proves unsettling.\nDespite the contextual and editorial shortcomings of the book, the collection adequately reflects Uhl’s decades-long struggle to encourage Americans to look critically at their own memories and strike back at the misconceptions and misrepresentations of Vietnam etched in the American psyche. The book’s intended broad audience, accessibility, and didactic potential proves tremendously important for a personal perspective on Vietnam activism. Fittingly, a snippet from I. F. Stone’s April 1965 New York Review of Books column describes Uhl’s work well: “What makes these books so timely, their message so urgent, is that they show the Vietnamese war in that aspect which is most fundamental for our own people—as a challenge to freedom of information and therefore freedom of decision.” Through his work, Uhl calls on us all to engage our past and question our present to enact a more socially sustainable future.\n. H. Bruce Franklin, “M.I.A.sma,” The Nation, May 10, 1993, 616-17.\n. Jerry Lembcke, “The Myth of the Spitting Antiwar Protestor,” New York Times, October 13, 2017.\n. Michael Uhl, Vietnam Awakening (Jefferson, NC: McFarland, 2007), 120.\n. Uhl, Vietnam Awakening, 124.\n. I. F. Stone, “Vietnam: An Exercise in Self-Delusion,” New York Review of Books, April 16, 1965.\nEditorial note: This review has been updated to correct minor formatting issues.\nIf there is additional discussion of this review, you may access it through the network, at: https://networks.h-net.org/h-fedhist.\nEvan R Ash. Review of Uhl, Michael, The War I Survived Was Vietnam: Collected Writings of a Veteran and Antiwar Activist.\nH-FedHist, H-Net Reviews.\n|This work is licensed under a Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License.|', ""Edgar vet Billy Fergot says impact of war is clear today\nEDGAR – Billy Fergot served in the Korean War with not one, but two branches of the United States Armed Forces — first the Navy and later the Marines.\nFergot first entered the Navy in 1950 at the age of 18 and went through boot camp and dental tech and medical training.\nAfter the Korean War broke out and Fergot volunteered for sea duty, he was transferred to Camp Pendleton for Marine Corps infantry combat training and additional field medical training before being shipped to Korea, where he served for a year.\nFergot, 83, was able to visit the memorials for his branches of the service and the Korean War when he took a Never Forgotten Honor Flight in 2012:\nQuestion: What did you do in the service and where were you stationed?\nAnswer: I spent most of my time in Korea. We traveled by train, truck and hiked by night. I ended up serving with the Marines and took care of what I was trained to do, which was medical and dental. We had a tented medical facility, but up front was where you took care of casualties first.\nQ: What is your most enduring memory from your time in the service?\nA: One time, I took our hospital Jeep to get water, but to get to the water tank you had to cross a river. So I loaded up five gallon cans and went and when I got to the river, I noticed it was higher than usual because it had rained. So when I got out around the middle, the power cut out on the Jeep, leaving me sitting there with no motor. I grabbed my carbine rifle and sat on the hood because water was coming into the jeep. I couldn't swim with all of the gear I had on. Finally, here came a six-wheel troop truck on the other side of the river and they threw me a large rope, we hooked it on, I was dragged out.\nQ: What do you most want people to know about the conflict in which you served?\nA: I look at North and South Korea now and see they compare. In North Korea, it's blacked out at night and they can't afford to feed their people but South Korea is prosperous and a big trading country. I know we were a big part in making that change. God bless the 36,000 that were killed in that war and all who were wounded in just a three-year period.\nQ: Tell us about your life after the service and how your time in the military affected your civilian life.\nA: I got a job with the S.S. Kresge Company. I started at the bottom and moved around a lot, living in probably 12 different states. I was eventually appointed a Kresge store manager and then they became Kmart stores. From there, I went on to be a district manager in Colorado, New Mexico, Missouri and Texas, with something like 16 stores in my district. I then was promoted to a regional office located in southern California where I worked as an operations director before retiring after 33 years with the company.\nQ: What did going on the Honor Flight mean to you?\nA: It was a wonderful experience. I was deeply touched seeing all the attractions and memorials, more so the Korean War memorial. It shows a statue of some Marines on patrol in their rain gear, which is much of what went on, and the very last one in line is looking back to see if the enemy is coming. The other factor of the trip is that you're traveling pretty much like you did in the service — just a big bunch of guys all together flying somewhere together.\nAbout Bill Fergot\nConflict involved in: Korean War\nMilitary branch: United States Navy and United States Marine Corps\nYears of service: 1950 to 1954\nRank upon discharge: Second-class dental technician\nA flight to never forget\n•The Central Wisconsin chapter of Never Forgotten Honor Flight celebrates its fifth anniversary this spring. Since April 2010, the Honor Flight program has flown to Washington, D.C., with more than 800 local World War II veterans, as well as dozens more who served in Korea and Vietnam, to allow them to view the memorials built for those wars.\n•30 Days of Honor is a project of Gannett Central Wisconsin Media to profile veterans who have taken an Honor Flight, or are scheduled, and to encourage more veterans to do so. Profiles will appear each day April 26 through Memorial Day.\n•Veterans may apply for a free Honor Flight (or loved ones may apply for them) by filling out an application. The only qualification for a veteran to take an Honor Flight is to have served in any branch of the U.S. military during the periods of World War II, the Korean War or Vietnam War. Veterans do not have to have been in combat. The applications and more information are available at http://www.neverforgottenhonorflight.org""]"	['<urn:uuid:18048766-0797-4bad-9f31-c8cfb6766ff2>', '<urn:uuid:5c7a00b9-a004-4d80-9e76-b5ea20c7644c>']	factoid	with-premise	short-search-query	distant-from-document	comparison	novice	2025-05-13T05:28:56.360010	6	66	2714
88	safety precautions essential equipment recommendations field research south american rainforest	Essential safety equipment and precautions for South American rainforest research include: carrying a working compass and GPS even on known trails to prevent disorientation, wearing long-sleeved shirts for night work to prevent leishmaniasis transmission from sandflies, using high-set boots or wellies for protection against venomous snakes like fer-de-lance and bushmaster vipers, carrying anti-venom when working with snakes, and protecting equipment with silica and dry bags due to extremely high humidity levels that can damage equipment.	['Andrew is a conservation biologist based out in the Peruvian Amazon. Here he tells us about his research in the Manu Biosphere reserve and offers advice for any young conservationists hoping to follow in his footsteps.\nName: Andrew Whitworth\nHabitat: Tropical Rainforests\nLocation: Manu Biosphere Reserve, Peru\nBio: I am a tropical field biologist originally from the North of England. I first worked for two and a half years in Ecuadorian Rainforest, after traveling to Tanzania and Australia. I currently conduct my research on regenerating tropical forest and its potential for sustaining biodiversity.\nI work in south-eastern Peru in an area of the Manu Biosphere Reserve, one of the most acclaimed bio-diverse places on the planet.\nThe journey runs from Cusco high up in the Andes and runs into the tropical forest lowlands. The majority of my field work is situated in the lowland rainforest, a hot and humid environment with regular temperature of 30ºC plus and humidity levels of over 90%. Occasionally we get hit with strange ‘friajes’, a cold front in which temperatures drop to 10ºC.\nMy main field site, the Manu Learning Center (operated by crees for The crees Foundation) sits on the banks of the fast flowing waters of the upper Madre de Dios (Mother of God) river. An incredible site, with stunning views of the mountains in the distance to the west and lowland forest cover to the east.\nAccording to the Global Forest Resources Assessment (2010) primary forest only makes up around 36% of global forest cover whilst secondary/regenerating areas constitute 57%. My research studies the value of this regenerating forest, the majority of the world’s forest, in reference to its importance towards biodiversity and conservation.\nI actually work on a variety of animal groups. This includes amphibians, butterflies, birds, mammals, butterflies and reptiles. This multi-taxa assessment holds more weight in assessing the value of regenerating rainforest in terms of biodiversity and conservation value.\nThe reserve provides a great patch of forest, which contains species of key conservation concern such as jaguar, black-faced spider monkey, blue-headed macaw and white-lipped peccary, amongst many others. This year I will focus on canopy dwelling species by climbing and placing traps 30+ meters up in the trees. There is so much in the canopy, but so little that we actually know.\nEver since first visiting Tanzania in 2007 I knew that tropical rainforest was for me. Seeing large mammals such as elephants from the back of a truck in the Serengeti was incredible, but nothing beats the thrill of being on foot in the forest and not knowing what might be around the next tree.\nAfter 4 years of living and working in South-American rainforest, every week I still see something completely new that I have never seen before and that just makes the environment so exciting to me. The diversity is truly incredible and I don’t think even within a lifetime you could see everything that exists there.\nThat’s what makes this place so special, its unpredictable surprises and the challenge of discovering them. My field site at the Manu Learning Center is pretty comfortable for a field researcher too!\nThe hardest aspect for many people is the challenge of finding animals and developing survey methods that are effective in a tropical rainforest environment.\nEquipment is expensive, budgets are tight and last year a group of leaf cutter ants destroyed two of my mist nets in a single night, a group of peccary smashed and trampled a camera trap and a jaguar decided to bite and destroy two camera traps, which was pretty tough to take.\nHowever, the nature of the challenge in that context strangely seems to appeal to me. I usually get used to the biting insects and mosquitoes pretty well. Last year I managed to contract leishmaneisis and host a rather large hungry botfly larva, but the worst for me are the sweat bees. When I am trying to climb or tree or work in a static position they can be intense.\n1) Always take a working compass and GPS into the forest with you. Even on known trails, you might just run off to follow a group of Woolly Monkeys and find yourself disorientated and lost without them.\n2) Wear long sleeved shirts for nocturnal research. The sand fly that carries leishmania specialises in transmitting this rather unwanted gift at night time.\n3) Wellies or thick high set boots are best in South American forest, due to the rather notorious fer-de-lance viper and the bushmaster, the world’s longest viper. Beautiful snakes, but ambush predators that can often be sat close to the side of a trail. Certainly carry some anti-venom if you are working with these guys.\n4) Look after equipment well with silica and dry bags. Humidity levels are devastatingly high and without good dry bags and dry boxes, equipment will quickly give in to forest humidity levels.\n5) If you get a bot-fly, wait until the larva is quite big…. about 4 weeks old at least and then place a drop of green banana sap over the air hole, cover with two strips of duck tape. Then the next morning, remove the tape and squeeze']	['<urn:uuid:6e68f6fb-ceed-4708-803a-48308cfd9fa2>']	open-ended	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-13T05:28:56.360010	10	75	864
89	preservation duration opened apple butter fridge	Once a container of apple butter is opened, it must be refrigerated and used within two weeks.	"['Instead of cooking in a crock pot, apple butter can be made in a microwave oven. Process in boiling water bath 5 minutes. Place 2 pounds of washed, cored, and quartered apples in a microwave-safe bowl and zap on high for 15 minutes or until the apples are completely soft. Then, process them in a food mill to remove the peels. Cortland apples work well because they don’t oxidize as … Of course, just like many jams and jelly recipes, this old fashioned apple butter recipe uses sugar to sweeten the spread. https://www.victoriamag.com/old-fashioned-apple-butter-recipe Reduce heat to medium. Make this delicious apple butter recipe using the Douthit family’s old-timey food tradition and make it a fantastic fall project. Offers may be subject to change without notice. Put soft fruit in a food mill to remove skins. Put in the crock pot. Add lemon extract and blend. For the drink, combine apple butter, apple cider, bourbon, and bitters in a cocktail shaker. However, if you are looking for a Sugar Free Apple Butter recipe we have one that you will love! Lisa Hubbard / Photolibrary / Getty Images. Cook for another 4-1/2 hours with NO lid. It\'s apple season, and I\'m on a serious apple baking marathon! Quickly ladle apple butter into hot sterilized jars, leaving 1/4- inch headspace. Bring to a boil; cover and cook over medium heat 30 minutes or until apples are tender. ** Make sure each jar has sealed. Expert baker and published author, Carroll Pellegrinelli shares her knowledge of bread baking and desserts. Then pour over ice and top with club soda. Remove from the heat and add lemon juice, cinnamon, and vanilla. Wash, core and quarter apples (no need to peel). Once a container is opened, it must be refrigerated and used within two weeks. Allow apple butter to cool and then store in the freezer. It is the easier, shortcut version. Directions. This Apple Butter Old Fashioned cocktail recipe is simple. Stir every 2 hours. Cover and cook on high for 6 to 8 hours. Add molasses, blending well. Step 3. 1. No forks or spoons required, just easy-to-pick-up party foods, so you can clean up in no time. This flavorful, homemade apple butter requires little effort but tastes like you labored all day. Add sugar and spices. Get it free when you sign up for our newsletter. Then, process them in a … Preheat oven to 350 degrees. the whole family loves fresh apple butter on toast biscuits and just plain bread. 2. Start in the morning and come home to a house that is perfumed with the homey aroma of cooked apples. Reduce heat and simmer covered for 20 to 25 minutes. Apple Butter. An Old Fashioned is traditionally made with bourbon, bitters, simple syrup, club soda, orange and a maraschino cherry. Well, when we went to the old mill to scope out the venue and book our wedding day with them, they gave us a small cloth bag with a jar of their homemade apple butter inside. https://hillsidehomestead.com/2014/10/25/apple-butter-recipe MyRecipes is a registered trademark of Meredith Corporation All Rights Reserved. Measure fruit back into the crock pot. Combine apples, apple cider, brown sugar, honey and 1/2 teaspoon salt in a Dutch oven over medium heat. *Can seal in hot water bath or top with paraffin. Cover at once with metal lids, and screw bands tight. Whether you\'re cooking for yourself or for a family, these easy dinners are sure to leave everyone satisfied and stress-free. Cook apples until soft (around 20 minutes). Remove from heat and stir in the butter until it melts completely. Sift dry ingredients together and add to creamed mixture. this link is to an external site that may or may not meet accessibility guidelines. Stir in cider or juice. This is a great weekend project with the kids, especially after a trip to an apple orchard. Its enough for a “small batch” of Apple Butter. Place first cake layer on cake plate that has a cover. MyRecipes.com is part of the Allrecipes Food Group. Add 3/4 cup unsweetened apple cider or apple juice (can be reconstituted from frozen), 2 to 4 tablespoons the sweetener of choice (agave syrup, honey or sugar), 1 teaspoon ground cinnamon and 1/4 teaspoon ground cloves. Add sugar and spices. MyRecipes may receive compensation for some links to products and services on this website. Apple Butter Recipe: You’ll need these ingredients. Cover and cook on low for 10 to 18 hours (or high for 2 to 4 hours). I\'ve been whipping up family favorites like homemade apple pie with an all-butter pie crust, rustic apple tarts and of course this deliciously thick apple butter recipe!. This yummy recipe is a favorite in my family. Cream butter and sugar; add eggs and beat well. https://www.halfbakedharvest.com/apple-butter-old-fashioned In the fall, old-fashioned apple butter is magical! Sign up to receive weekly recipes from the Queen of Southern Cooking Instructions. Press apples through a sieve. ), Homemade Apple Soda With Fresh Apple Juice, Vegan Apple Cobbler with Cloves and Allspice Recipe, The Best Apples for Homemade Apple Butter, Pennsylvania Dutch Low-Sugar Apple Butter. Any leftover apple butter should be eaten with warm homemade bread or biscuits. Cook, stirring frequently, 1 hour or until mixture thickens. FILLING: Stir together apple butter and brown sugar in a medium saucepan over medium high heat. Spoon into hot canning jars and process according to canning jar directions. Old Fashioned Apple Cocktail Ingredients. This Apple Butter Old Fashioned is the perfect way to ring in the fall season! Slow Cooker Apple Butter Recipe… Combine apples and apple juice in lightly oiled crock-pot. After 4-1/2 hours stir and add remaining ingredients. Slow cooking a variety of apples, sugars and balanced spices for hours results in the best apple butter ever! See Best Cooking Apples and use the varieties you would use for applesauce. makes a great present with a loaf of homemade bread wrapped in a kitchen towel & basket That means you don’t have to peel, core, and chop up 5 pounds of apples to make it. Submit a Recipe Correction. Place 2 pounds of washed, cored, and quartered apples in a microwave-safe bowl and zap on high for 15 minutes or until the apples are completely soft. It lets the flavors of the apples shine, along with their nutrients! Old Fashioned Apple Butter Recipe (This made about 8 pints): 8 pounds of apples (about 32 medium) 4 cups of sugar (or more as needed, up to 8 cups) 2 cups liquid, water or apple cider Place apples in crock pot; cook on high for 4-1/2 hours (covered). Cover and cook on low setting for 10 to 18 hours or high setting for 2 to 4 hours. If you don’t want to schnitz apples, you can also cook apple butter directly from applesauce. Cool, transfer to a jar, and store in the refrigerator. By William T. Pryor | September/October 1973 Fill sterilized pint and/or quart jars (good to sterilize while apple butter is cooking) with extremely hot apple butter. Begin by washing the apples very well under cool running water. Cover at once … Reduce heat to medium. Stir well. If you have never eaten apple butter, I urge you to make this! DIRECTIONS. From chips and dip to one-bite apps, finger foods are the perfect way to kick off a party. Nothing beats the taste of homemade, and even more so when it comes to rich and delicious apple butter… Combine the puree with 1/3 cup maple syrup (or more to taste) and 1/2 teaspoon cinnamon (or more to taste). To update this classic cocktail for fall and winter, I’ve added apple butter and a cinnamon stick in lieu of an orange and cherry. The apple butter can be eaten immediately, ""put up"" (canned) by using a water-bath process, or transferred to freezer-proof containers and frozen for future use. This recipe was made using a crock pot that has three settings—off, high, and low. Cozy up and celebrate the season with the most delicious apple cocktail. Return strained pulp to kettle; cook over high heat, stirring constantly, until pulp rounds up in a spoon. Return it to the microwave and zap for 5 minutes or longer until the consistency you desire is reached. For each pint (2 cups) of fruit, add 1 cup sugar, 1 teaspoon cinnamon, 1/2 teaspoon allspice, 1/2 teaspoon ground cloves, and 1/2 teaspoon freshly grated nutmeg. Add all of the ingredients for the topping, except for the butter, to a medium sized bowl and stir to combine. Make your own apple butter with this simple recipe. If you use one that has four settings—off, high, low, and warm—be advised that they cook hotter, so you will have to watch your apples more carefully so they don\'t burn. (Nutrition information is calculated using an ingredient database and should be considered an estimate. Apple butter is a delicious alternative to peanut butter, and a great way to use fresh apples. The apples should be very mushy. How to Can Apple Butter *A detailed and printable recipe … Pour puree into a shallow baking dish and bake at 300° for 2 hours or until thick enough to hold its shape. … Slice the apples into quarters or, smaller size pieces if the apples are really large. © Copyright 2020 Meredith Corporation. Add sugar and spices, mixing well. Everybody understands the stuggle of getting dinner on the table after a long day. If you\'re looking for a simple recipe to simplify your weeknight, you\'ve come to the right place--easy dinners are our specialty. Cook at a gentle simmer for several hours adding a little more water from time to time if there seems to … A recipe for a small batch of Apple Butter Most of you won’t be stirring apple butter in a 30 gallon kettle, so we’ve included a recipe for a small batch you can make in your own kitchen. The caramelization of the sugar in the apples, combined with cinnamon, nutmeg and cloves, creates a traditional rich, sweet and smooth classic apple butter. Cook, stirring frequently, 1 hour or until mixture thickens. … I’m using 5 pounds of apples. This simple recipe cooks up low and slow in My favorite way to eat apple butter … Quickly ladle apple butter into hot sterilized jars, leaving 1/4- inch headspace. Just use applesauce! A traditional Old Fashioned cocktail is made by muddling […] Puree apple mixture in a blender. This creative recipe puts a festive spin on an old favorite, creating a drink that no one can resist! Peel, core and cut up apples in abundance, put them over the fire with just enough water to keep them from scorching, and let them come to the boil slowly. To freeze apple butter, keep 1/2 inch of space between the top of the apple butter and the top of the jar. Process them in a food mill, throw the puree back into the crock pot and, the following morning, you\'ll have hot apple butter to go with your toast and butter (if you don\'t mind getting up a few times during the night to give the mixture a quick stir). For the rim, just use sugar to coat. Newsletter Signup. Clean rims and top with hot, sterilized lids and tightened rings. Don’t peel, but wash, core, and quarter the apples. With this cookbook, you’re never more than a few steps away from a flavorful dinner. Place apple slices and cider in a flat-bottomed kettle. Place a lid on the jar and freeze for 6-12 months. Otherwise, the puree can be cooked down under your watchful eye during the day. Bring to a boil, then reduce heat, cover, and simmer, stirring often, for 1 hour. Cooking it in a crock pot is the secret. Stir often while cooking. Or spoon into freezer containers. INDIANAPOLIS (WISH) — Here’s a recipe given to Annessa’s family by a dear friend more than 40 years ago. Allow the apple butter to fully cool before freezing. If some have not, refrigerate until ready to use. Bring apples and cider to a boil in heavy saucepan. Bring just to boiling. Stir and seal in jars. For an easy supper that you can depend on, we picked out some of our tried-and-true favorites that have gotten us through even the busiest of days. Remove cover after 3 hours to allow fruit and juice to cook down. Wash, peel, core and quarter apples.']"	['<urn:uuid:03a7f59c-cb80-40fa-8f07-328983ebacaa>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-13T05:28:56.360010	6	17	2081
90	need lab equipment that resists heat and corrosion which material glass or stainless steel better durability	Glass reactors are made of borosilicate glass that provides resistance to both thermal shock and chemical corrosion. When it comes to stainless steel, it offers high resistance to heat and good scaling resistance, plus additional benefits like resistance to corrosion in seawater and alkaline environments, as well as good cold toughness and erosion resistance.	['The best chemical reaction equipment – laboratory reactor\nDetails of Glass Reactor\n- Using the latest reaction technology\n- It has high temperature resistance, corrosion resistance, and high durability.\n- Heating methods include steam, electric heater, high temperature heat transfer oil\n- Using jacket, half pipe, fan coil, and other structures\nApplication: Chemicals, pharmaceuticals, dyes, pesticides, petroleum, food additives, etc.\nThe structure principle of laboratory reactor\n1. The seal of the kettle body and the kettle cover adopts the triangular seal, which has a long service life.\n2. The main seal uses a double-door snap ring, which freely touches the shoulders of the kettle body and the kettle cover, and evenly tightens the torque of the eight main sealing bolts, which can press the sealing kettle. When disassembling, loosen the 8 main bolts and open the snap ring to lift the lid, which is labor-saving and convenient, and is a modern domestic structure.\nThe kettle cover and the guide column are connected by the lifting arm, so that the lifting handwheel swings, and the kettle cover guides the column to move up, down, left, and right through the movement of the lifting thread, and the movement of the lifting arm works freely through the friction-reducing effect of the ball.\nRotation of the kettle cover and tilting of the kettle body for pouring and reset. First, use the lifting handwheel and lifting screw to lift the kettle cover to a certain height, loosen the twist of the fastening hand of the boom, and press and position the movable key kettle cover by hand, which can be rotated left and right. In addition, loosen the twist of the tightening hand of the rotating seat, press the rotating button to rotate the flip handle, ensure that the kettle body has a certain angle of material injection, and then release the rotating button to release the material. When it is necessary to reset the kettle body, shake the overturning handle and press the rotation button to quickly reset. In order not to swing the kettle body from side to side, it is necessary to control the speed until the overturn handle is reset by hand.\n3. The rotary stirring adopts a cylindrical magnetic coupling structure, and the magnitude of the torque is determined by the magnetic material. Generally, ferromagnetic rare earth is used, which has a large coupling force. The motor is used as the power, and the stirrer generates sufficient stirring force through the coupler. The slurry can be replaced by the operator, and the components such as slurry, bolt, frame, and stirring worm gear are suitable for the viscosity of various substances.\n4. The valve parts are equipped with a gas phase valve, liquid phase valve (also known as insertion pipe valve), safety burst valve, feed valve, and discharge valve, which can be increased or decreased according to user requirements. The structure adopts a reciprocating structure, and its characteristic is that the sealing life is 1.5 times longer than that of the rotary valve. The bursting valve is equipped with double safety devices and single-layer safety devices. The bursting diaphragm is adjusted in the factory and is not allowed to be adjusted or loosened arbitrarily. The pressure gauges are indicated directly above.\nLaboratory reactor maintenance and maintenance\n1. When using the laboratory reactor, the valve of the refrigerant inlet must be closed, and the remaining refrigerant in the pot and the casing to prevent illness must be put in, and then the material is imported;\n2. Start the mixer, and turn on the steam valve and the electric heating power supply, after reaching the required temperature, close the steam valve and the electric heating power supply, and turn off the mixer after 2~3 minutes;\n3. After processing, put the remaining condensed water in the pot and the casing, rinse with warm water as soon as possible, remove the material stained with the paste, rinse the inner wall of the container with 40~500c alkaline water, and rinse with water;\n4. If it is an empty pot with no material (heat-absorbing medium) in the special pot, the steam valve, and electric heating power cannot be connected. Pay special attention to the use of steam pressure, which cannot exceed the rated working pressure.\n5. To maintain the reaction kettle in the laboratory, always pay attention to the overall working conditions of the equipment and reducer. The reducer lubricating oil is insufficient and replenished in time, and the electric heating medium oil is replaced every six months;\n6. Regularly check the safety valves, pressure gauges, thermometers, distillation holes, electric heating rods, electrical appliances, etc. Of the casing and pot cover, and replace or repair them in time if there is any fault;\n7. When the laboratory reactor is not used, be sure to use warm water to thoroughly clean the inner and outer walls of the container, and scrub the body of the pot frequently to keep the outside clean and the inside bright to achieve the purpose of durability.\nPrecautions for the use of laboratory reactors\n1. The laboratory reactor must be used in designated places,and the blasting outlet must be connected to the outdoor through pipelines during installation. When not in use for a long time, it is necessary to clean the inside and outside of the reactor and store it in a clean and non-corrosive dry environment.\n2. Before the experiment, carefully check the test pressure, operating pressure, maximum operating temperature, and other performance parameters engraved on the reactor to ensure that the experiment is used under the conditions allowed by the reactor. Before placing the media, it is necessary to investigate whether the media is corrosive. If the experimental medium reacts violently, a large amount of gas is generated to reach ultra-high pressure and ultra-high temperature, or the medium contains chloride ions, fluoride ions, etc., special orders must be made as soon as possible.\n3. After each experiment, the experimenter rotates the knob to the minimum position to ensure that each button is in the initial state every time the power is turned on, to avoid damage to the control equipment caused by the excessive current when the power is turned on.\n4. Connect the corresponding voltage according to the operating voltage of the controller, single-phase 220v, three-phase 380v, so that the live wire and neutral wire of the controller are not opposite; to prevent electric shock, do not perform wiring work during power transmission! For normal work and the safety of the subjects, the ground wire must be connected! The controller itself is not explosion-proof, please avoid flammable and explosive environments! The pressure used by the pressure gauge is recommended to be used within 1/2 of the pressure shown. The pressure gauge of the reactor must be carefully compared with the standard pressure gauge for calibration.\n5. When an abnormal sound occurs inside the isolation cover during the operation of the laboratory reactor equipment, stop immediately and release the pressure, and check whether the stirring system is abnormal. In addition, it is also necessary to regularly check the amount of oscillation of the stirring shaft. When the amount of swing is too much, please replace the bearing or sliding sleeve immediately.\n6. The correct detection position of the speed display is very important for the stable display of the speed. This position of the laboratory reactor has been adjusted at the factory, please do not adjust it at will!\n7. After the final reaction temperature is determined, the temperature setting value of the laboratory reactor must not be changed during heating, to avoid the temperature being too large and overshooting!\n8. When the measuring instrument breaks, it usually breaks on the front and rear sides of its glass surface. Therefore, the experimenter should not stand in these hidden dangerous places when using the reactor.\n9. When cleaning the autoclave, be careful not to let water and other liquids flow into the heating furnace, to prevent the heating furnace from breaking. When heating with heat-conducting oil, be careful not to add water and other liquids when adding heat-conducting oil, and check the oil level of heat-conducting oil from time to time.\n10. When the laboratory reactor equipment is working or after the experiment, it is strictly forbidden to apply belt pressure for disassembly! The reactor is strictly prohibited to work under overpressure and overtemperature! Regularly check various instruments and blasting release devices to ensure accurate and reliable work.\nWelcome to send inquiry to us and let’s make a win win business together !\nGuidelines For Chemical Reactor\nFRANLI has several complete chemical reactor production lines. Our chemical reactor adopts the latest infinitely variable speed reducer device, the sealing device can be a mechanical seal, and the heating and cooling can adopt the structure of a jacket, half pipe, fan coil, etc., The heating methods include steam, electric heater, and high-temperature heat transfer oil to achieve different safe processing environments such as acid resistance, heat resistance, wear resistance, and corrosion resistance.\nGlass reactors have emerged as versatile and reliable tools in the chemical industry, finding broad applications in pharmaceuticals, chemical engineering, petrochemicals, food and beverage, and environmental sciences.\nStainless steel reactors excel in maintaining precise temperature conditions due to their excellent heat transfer properties.\nLaboratory reactors have revolutionized the field of glue by enabling precise formulation, optimization, and testing of adhesive products.\nGlass reactors are widely used in chemical laboratories and industrial settings for various chemical processes. These reactors are made from borosilicate glass which is a type of glass that is resistant to thermal shock and chemical corrosion.\nA lab reactor is at the core of laboratory exercises in various scientific and engineering disciplines. It provides an environment that allows scientists and researchers to conduct experiments and simulations of chemical reactions, which aid in developing new products and optimizing processes.\nFrom polymerization and mixing to heating and cooling, stainless steel reactors are used to produce a variety of resins and polymers that are essential in many industries.', 'AlumiLok Coating Performance High Temperature Oxidation Resistance. Figure 6 illustrates a comparison of the long-term isothermal oxidation performance of AlumiLok coated 316 stainless steel, with vapor-phase aluminized, uncoated 316 stainless steel. The photographic inserts in the figure depict the appearance of the samples after being\nAustenitic Iron Castings & NI Resist Cast Iron - Durham These include good scaling resistance, high resistance to heat, good thermal expansion characteristics, resistance to corrosion in seawater and alkaline liquids and atmospheres, good cold toughness and resistance to erosion. Molybdenum improves high-temperature performance. The ability to influence such a wide range of properties has meant\nhigh temperature cases, because of its high thermal resistance. Table 1 provides an overview of corrosion resistances of the most common used coating materials for electrically conductive surfaces and the base material copper . Table 1. Corrosion resistance of selected metals:1 = high; 2 = fair; 3 = poor. Material H 2 S SO 2 Cl 2 /4 d\nCorrosion and scale at high pressure high temperature Jan 01, 2017 · A detailed study on pitting resistance of corrosion resistant alloys is described. Cyclic polarization measurements were conducted at temperature up to 250°C and total dissolved solids up to 180,000 mg/L. Results suggest that the pitting resistance equivalent number may not be reliable as the criteria for evaluation of the localized corrosion\nDo Metals Lose Tensile Strength as Corrosion Resistance The temperature extremes of a heat treat cycle often demand high temperature tolerances and resistance to scaling while the chemicals and vibrations used in ultrasonic processes might demand greater corrosion resistance and durability. One compromise is to use two different baskets.\nthe corrosion resistance of more expensive high Ni-Cr-Mo alloys in many environments. Cronifer 1925hMo (UNS N08926) was derived from the 31 is resistant at room temperature up to a con-centration of 8%, with corrosion rates below the 0.13 mm/y (5 mpy). 5 is demonstrating excellent performance. The\nHigh Temperature Materials Corrosion Challenges for high temperature corrosion processes are also increased. In this paper, corrosion better oxidation performance in LOCA. that SiC, high Cr (>20 wt%) alloys or Al-forming alloys all show significantly improved oxidation resistance at high temperature LOCA conditions. The oxide scale thickness generated in 8 h at 1200°C\nHigh Temperature Sulfidation Corrosion - 99 DiseasesJul 01, 2004 · High temperature sulfidation is probably the most common high temperature corrosion nemesis in the refining industry, since there are very few sweet refineries still in operation. Sulfidation corrosion typically is of concern in sour oil services starting at temperatures in the 500F (260C) range.\nof the high-performance stainless steels. There are three primary classifications within the high-performance stainless steels. They are the austenitic, ferritic, and duplex (austenitic-ferritic) families. The stainless steels in each family have general similarities, but there is also a wide range of corrosion resistance and other characteristics.\nHigh-Performance Alloys for Resistance to Aqueous treatments, improves resistance to pitting and crevice corrosion, and increases high temperature strength. Tungsten Improves resistance to reducing acids and to localized corrosion, and enhances both strength and weldability. Nitrogen Enhances metallurgical stability, improves pitting and crevice corrosion resistance, and increases strength.\nHigh-Temperature Performance of Ferritic Steels in May 19, 2016 · High-Temperature Performance of Ferritic Steels in Fireside Corrosion Regimes:Temperature and Deposits T. Dudziak, T. Hussain, and N.J. Simms (Submitted May 19, 2016; in revised form August 3, 2016; published online November 17, 2016) The paper reports high temperature resistance of ferritic steels in reside corrosion regime in terms of\nAllows wide operating temperature range - outstanding high and low temperature performance. Provides thicker fluid films protecting against wear of equipment parts operating at high temperature. Causes low resistance during start-up at very low temperatures. Excellent protection against wear and corrosion\nOxidation Resistance - an overview ScienceDirect TopicsStephen A. Rackley, in Carbon Capture and Storage (Second Edition), 2017. Corrosion resistance. The oxidation resistance of steel is primarily a result of the chromium content, and to a lesser extent, in austenitic steels, the high nickel content. Chromium on the steel surface is oxidized to form an invisibly thin layer of chromium oxide (Cr 2 O 3), and this so-called passivation layer will be\nProperties of Polyimides - polymerdatabasePoly(ether)imides (PEI) Properties. Poly(ether)imides (PI, PEI) are high performance engineering thermoplastics of amber to transparent color. They have outstanding thermal, mechanical, and chemical properties and are often the best choice for the most demanding applications where very high mechanical strength in combination with high temperature, corrosion, and wear resistance is required.\nJan 08, 2002 · Resistance to oxidation, or scaling, is dependent on the chromium content in the same way as the corrosion resistance is, as shown in the graph below. Most austenitic steels, with chromium contents of at least 18%, can be used at temperatures up to 870°C and Grades 309, 310 and 2111HTR (UNS S30815) even higher.\nStainless Steel - High Temperature ResistanceJan 08, 2002 · Resistance to oxidation, or scaling, is dependent on the chromium content in the same way as the corrosion resistance is, as shown in the graph below. Most austenitic steels, with chromium contents of at least 18%, can be used at temperatures up to 870°C and Grades 309, 310 and 2111HTR (UNS S30815) even higher.\nWear Resistant Alloys in NITRONIC and STELLITEHigh strength and toughness from cryogenic temperatures to 1800 degrees F (980 degrees C), good oxidation resistance, exceptional fatigue strength, and good corrosion resistance. Chemical and pollution control equipment, ash pit seals, nuclear reactors, marine equipment, ducting, thrust reverser assemblies, fuel nozzles, afterburners, spray bars.\nWelcome to Wearresist Technologies Pvt. Ltd. Wearresist offers solutions for both wear and tear problems in the core sector industries using best possible materials and methods. Providing Xtralife edge to the repair or wear protection is the ultimate motto.The company today operates through following divisions :Modular products Business (MPB)Wearresist Technologies Pvt. Ltd.XL- SS21 is a high temperature resistant welding alloy which can resist both corrosion and heat upto 12000C and provides the anchor for XL CC65 which is a complex carbide deposit with excellent resistance to erosion. XL CC65 does not get soften upto']	['<urn:uuid:c3ff143f-c90f-403f-9a43-d632f377f1ee>', '<urn:uuid:d8caa2c3-698b-42fb-b150-6f097ea9ed0e>']	factoid	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-13T05:28:56.360010	16	54	2677
91	How do Kahneman and Thaler differ in their views on rational thinking?	Kahneman distinguishes between intuitive ('fast') and rational ('slow') thinking, warning against quick intuitive reactions, while Thaler focuses on how people make illogical choices despite believing they are rational, demonstrating this through concepts like 'limited rationality' and the 'endowment effect'.	"['A look at why intuitive reactions can hinder success\nFirefighters sometimes lose their lives because of a loss of Situational Awareness. \nSituational Awareness is the ability to capture the clues and cues, and see bad things coming in time to change the outcome. In IT support, Situational Awareness can be hard to maintain in high-stress situations, and some simple actions can help teams drive more successful outcomes with small changes to their environment and working practices.\nCustomer satisfaction highly depends on the speed with which incidents are solved. Yet we must be careful not to rush to an answer too quickly when an incident is reported to us. Our brain can trick us into jumping to incorrect conclusions. Nobel laureate Daniel Kahneman advocates \'slow\' thinking under certain circumstances, many of which happen during the lifecycle of some incidents in Incident Management.\nFor example, how often in resolving an incident have you thought: ""How could that wrong turn really have happened to me?"" To what extent did your intuition let you down? Incident Management relies on intuitive thinking based on knowledge and experience, and once an issue has escalated in complexity, a different kind of thought process is needed.\nDuring consultancy practice, we frequently encounter organizations paying insufficient attention to critical moments in the lifecycle of an incident. This includes being unable to gain:\n- An accurate understanding of the priority of an Incident (not just “P1,” as that’s meaningless, but the actual textual description of the effects of an incident);\n- A quality understanding about why the issue/product/situation is failing and the way in which it’s failing;\n- A well thought out risk analysis of actions that are being planned.\nWhen a problem is not clearly described, the search for a solution is hindered. It slows down the resolution of the issue, which is not only expensive but also makes everyone involved unhappy.\nThe Trouble with Intuition\nDaniel Kahneman, the first psychologist to win the Nobel Prize in Economics in 2002, explains why an intuitive reaction is not always the best.\nIn his groundbreaking book Thinking: Fast and Slow  , Kahneman distinguishes between intuitive (\'fast\') thinking and rational (\'slow\') thinking. He illustrates how intuitive reactions can lead to problems and explains the limitations of our common sense.\nWe tend to believe that we asses problems correctly, with a quick and accurate understanding. Therefore, we often respond quickly and intuitively. But beware, as Kahneman warns, our brain plays games.\nThe following five examples illustrate why intuition is not necessarily the best trusted advisor:\n1. The Halo Effect:  Believing that when a certain quality is present, it indicates other qualities are present as well. For example, a child is good in languages and reading and therefore will probably also do well in other subjects...\n2. WYSIATI (What You See Is All There Is): Due to tunnel vision, we are not open to other observations. A familiar example is a video in which a gorilla walks through the scene and no one notices because they are instructed to focus their attention on another activity.\n3. Framing: The same information can be framed as both positive and negative, depending on how the message is stated. For example, which product would you prefer: one that is 1% contaminated or 99% pure?\n4. The Anchoring Effect: We make a decision based on a certain reference point, the anchor. Here we are strongly influenced by the way in which facts and figures are presented to us, and which are not really relevant to the issue.\n5. The Availability Bias: We consider an event more likely, if we can memorize a clear example of this event. We suffer from selective memory and recall the impactful, unusual occurrences. For example, there are many media reports about kidnappings, so we think more kidnappings have occurred this year.\nIn the next article in this two-part series, we’ll take a look at how to distinguish between decisions that require fast and slow thinking—and how this impacts your customers.\n FireChief.com Nov 14, 2012\n Kahneman, Daniel. Thinking: Fast and Slow, 2011, Penguin Books.\n The Halo Effect is a term coined by psychologist Edward Thorndike.\nPart 2: How Our Brain Plays Games\nThe KT problem solving approach is used worldwide for root cause analysis\nand to improve IT stability', 'Richard H Thaler—the Charles R Walgreen distinguished service professor of behavioral science and economics at the University of Chicago Booth School of Business—who is engaged in researching behavioral economics and finance and the psychology of decision making has been awarded the 2017 Nobel Prize in Economics “for his contributions to behavioral economics” that built a “bridge between the economic and psychological analyses of individual decision-making”, paving the way for “the new and rapidly expanding field of behavioral economics.”\nReacting to the award of Nobel Prize, Thaler said that his major contribution to economics is “the recognition that economic agents are human, and economic models have to incorporate that [factor]”. And perhaps, as an acknowledgement of his argument about the sometimes-unreasonable behavior of humans, he jocularly said, “I intend to spend the prize money ‘as irrationally as possible’”.\nJokes apart, what Thaler meant when he said “economic agents are human” is: traditionally, economists assuming that each person makes totally rational choice in pursuit of their own self-interest, built elaborate theoretical and mathematical models to explain how markets work to efficiently allocate capital and set prices. But Thaler along with Daniel Kahneman and Amos Tversky challenging this underlying premise of the models, demonstrated how often individuals make illogical choices that sabotage their economic interests, that too, believing that they are totally rational. Citing Brexit as a classic example of behavioral economics in action, Thaler observed that British voters chose an economically irrational route while considering the options offered to them by the elites.\nStudying the underlying reasons for people to behave irrationally, he came up with three propositions: one, ‘limited rationality’; two, ‘social preferences’; and three, ‘limited control’. The concept of limited rationality is explained through ‘endowment effect’ where individuals value an item more when they own it rather than when they do not. To test this hypothesis, he gave coffee mugs at random to half of a group of test subjects asking them to sell them, if they wish, to the other mug-less half of the group. Rationality demands that people from both the groups must, on average, value them the same, and accordingly, half of the mugs should change the hands. Contrary to this expectation, the have-nots valued the mug less while the haves have valued it high and as a result few mugs have changed hands.\nMoving to the role of ‘social preferences’ in economic decisions, Thaler says that individuals do not necessarily make choices driven solely by selfish interests. For instance, in an experiment conducted by him, he asked a randomly selected student to divide a $20 bill between himself and another subject. Rarely had he noticed any student retaining the whole amount with himself, as pure rationality would suggest. Similarly, his studies revealed that people find practices like overcharging for a good under duress as unfair.\nAlong with Hersh Shefrin, Thaler developed the third idea, namely, ‘limited control’ that emanates from the conflict between two competing cognitive forces of an individual: ‘doing-self’ which is more concerned with short-term happiness and ‘planning-self’ that values long-term goals more. To resolve this dichotomy, he says that the planning-self perhaps offers fewer choices to the doing-self that lie in the immediately upcoming future. And this phenomenon runs contrary to the economic theory of rationality, for more the choices, the better would be the decision. It otherwise means, presenting people with a “choice architecture” which favors the planning-self than the doing-self can impact people’s behavior in a big way.\nIt is perhaps taking a cue from this phenomenon that Thaler came up with his now famous “nudging” theory. He co-wrote the global best-selling book, ‘Nudge: Improving Decisions about Health, Wealth and Happiness’ in 2008 with Cass Sunstein that nudged governments to explore his ‘nudging’ theory to achieve better outcomes in the financial behavior of people—of the society at large. For instance, we are all aware that we need to save more for our retired life. But under most of the hitherto operating schemes one has to voluntarily choose: you need to first decide to save, then decide how much to put aside, in which scheme and for how long? It is too much for most of the people and thus many in the US were found under-saving.\nThen came Thaler with a paper, “Save More Tomorrow: Using Behavioral Economics to Increase Employee Saving” jointly with Shlomo Benartzi of UCLA that revolutionized designing pension schemes. Under this new design, employees of the company are automatically enrolled in the plan, that they will contribute an initial percentage of their salary, that this contribution will increase each year by a certain percentage of their salary increase, and if the employees make no further choices, they will be assigned to a target date fund that is designed to automatically control asset allocation along the way towards assumed date of retirement of the respective employee. There is, of course, a provision for the employees to always opt out of any of these provisions. Intriguingly, thus default-enrolment proved to nudge employees to save for their retired life more effectively, for seldom anyone opted out. With the result, people in the automatic plans found saving more than those outside of them. And an obvious improvement is: the quality of life for future retirees.\nHe thus came up with the concept of improving “choice architecture”—an architecture that does a favor to their planning-self over their doing-self, so that it can result in better financial behavior. For instance, suppose a prospective client of credit card is provided with all the charges and fees to be paid under its usage right at the time of his choosing the card, would it not enable him to make a better choice rather than to struggle with loads of fine print later and crib at the decision already taken?\nWorking for over three decades in association with a group of behavioral economists around psychology of decision making, economics and sociology, Thaler has simply nudged even the hardnosed rationalists to look differently at how people think when it comes to money and the decision making thereof and importantly nudged rationalists to appreciate that there exists deviations in human behavior, for human agents act fallibly. And all this happily nudged him to pocket his much deserving Nobel Prize too.']"	['<urn:uuid:5be290c6-9484-47b4-937c-2242820a0d83>', '<urn:uuid:c29ddd2e-9609-4a06-934c-0f839795a9f2>']	factoid	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-13T05:28:56.360010	12	39	1756
92	How do bird experts make money and what dangers do birds face?	Bird experts (ornithologists) typically work for government agencies, conservation organizations, or universities, with median annual wages ranging from $59,660 to $81,530. They conduct field research, monitor populations, and create management plans. As for dangers, birds face severe threats from tar sands development, including habitat destruction through strip mining, deadly toxic tailings ponds that could kill up to 100,000 birds annually, and extensive habitat fragmentation from drilling operations that could impact up to 14.5 million birds. Additionally, air pollution and water contamination from these operations pose significant risks to bird populations.	"[""Simply put, an ornithologist studies birds. Ornithologists may study the behavior, physiology, and conservation of birds and bird habitats. This work often involves surveying, recording and reporting on bird activity. Ornithologists may either generalize, or specialize in a particular species or bird group.\nHowever, many professionals may only spend part of their time researching birds. They may work as wildlife biologists, ecologists, land managers, teachers, researchers, environmental educators, legislative advocates, or eco-tour guides.\nWhat Do Ornithologists Do?\nWhile job duties vary by position, ornithologists may conduct field research to better understand migration routes, reproduction rates, and habitat needs; monitor and assess the status of a particular population; capture and band birds to track their movements and identities; analyze collected data; conduct wildlife impact assessments for development projects; and create management plans and reports. They may also serve as park rangers or work at nature reserves. Those employed by nonprofit conservation organizations may also be involved in policy development and advocacy.\nWhere Does an Ornithologist Work?\nMost ornithologists work for land and wildlife agencies at the federal and state levels, or nonprofit conservation organizations. They may also teach and conduct research at colleges and universities. Some work at zoos, wildlife parks, and as veterinarians and environmental scientists, though these jobs are rarely exclusive to birds.\nWorkers in certain positions may spend a significant amount of time in the field gathering data and studying birds in their natural habitats. Fieldwork may involve travel to remote locations, including international travel.It can also involve travel by foot, exposure to all kinds of weather conditions, and isolation. Ornithologists also work in laboratories, and may process data with computers in an office setting.\nMost ornithologists work full time. They may work non-standard or extended hours when doing fieldwork, such as during breeding season.\nWhat Is a Typical Ornithologist's Salary?\nWhile the U.S. Bureau of Labor Statistics (BLS) doesn't have data specifically on ornithologists, they're included among zoologists and wildlife biologists. The median annual wage for these professions was $66,350 as of May 2020. Those in the federal government earned a median of $81,530, while ornithologists teaching at colleges, universities, and professional schools earned a median of $62,300. Those in state government made a median salary of $59,660.*\nRecent Ornithology Job Listings\nUse the search box below to find all the ornithologist job listings in our job board.\n- Study avian systems\n- Use genomic tools and datasets to study avian systems\n- Use computational modelling to gain insight to population and migration trends\n- Use ecological, behavioral and/or comparative field approaches to answer questions about the origin and maintenance of diversity, adaptation, and disease\n- Plan and conduct bird surveys and studies\n- Manage and advise on endangered species populations and strategize about conservation, protection, and rehabilitation\n- Review and conduct assessments for ecological and environmental assessments\n- Collect, analyze, and interpret data, including analysis of sound recordings\n- Monitor the status and trends of bird populations\n- Research results from other studies and conduct literature reviews\nOutside of the academic arena, there are not many senior ornithologist jobs. However, those that do become available often have the following job requirements:\n- Design computer models of bird ecology and evolution\n- Prepare management plans and scientific reports\n- Resolve conflicts with competing issues and promote good conservation ethics\n- Consult with government agencies, stakeholders, and engineers\n- Make presentations to the public or teach ornithology classes\n- Write proposals for funding\n- Develop joint ventures in collaboration with groups such as provincial ministries, non-governmental organizations, and universities\n- Prioritizing and planning research trips\n- Coordinating peer-review sessions for process improvement and strategy\n- Constructing budgets and timelines for workgroup\n- Serving as point of contact for peer-review data calls and planning\n- Serving on agency working groups to provide peer-review\n- Coordinating technical details for a range of cross-disciplinary environmental projects\n- Coordinating data collection and input, interpretation, and reporting\n- Navigating environmental regulations and environmental approvals processes\nWhat Is the Job Demand for Ornithologists?\nEmployment of zoologists and wildlife biologists is projected to grow 5 percent between 2020 and 2030. Competition for jobs is strong.*\nGetting an Ornithology Degree\nMost ornithologists start out with bachelor's degrees in biology, wildlife biology, zoology, or ecology. A good background in science and math is essential. Knowledge of statistical software is also helpful, especially for advanced positions. Since ornithologists spend a good deal of time writing reports, good communication skills and courses on technical writing are also beneficial.\nHowever, while education is a must, practical experience in the field or lab is also critical. You can start gaining experience through local bird watching clubs, workshops, internships, and volunteer work for nonprofit wildlife and conservation organizations.\nMaster's degrees are usually prerequisites for higher-level positions. Doctorates are required for most university and research positions.\nOrnithology - Related Degrees\nWhat Kind of Societies and Professional Organizations Do Ornithologists Have?\n- The American Ornithologists' Union (http://www.aou.org/) is a well-established organization that aims to increase our understanding of birds, advance the profession, and develop science-based methods of bird conservation. It publishes journals and books, holds meetings, and awards grants.\n- The Association of Field Ornithologists (http://www.afonet.org/) is an organization for sharing information among both professional and amateur ornithologists. The association focuses on field studies and the conservation biology of birds.\nLearn more about ornithology.\n*2020 US Bureau of Labor Statistics salary figures and job growth projections for zoologists and wildlife biologists reflect national data not school-specific information. Conditions in your area may vary. Data accessed September 2021."", 'Last week, the Boreal Songbird Initiative, Pembina Institute and the Natural Resources Defence Council released a report describing the predicted impact of the tar sands on bird populations. The report, Danger in the Nursery, used modelling based on best current knowledge of bird populations in northeastern Alberta, combined with documented and estimated impacts of different elements of tar sands development and expansion on bird populations.\nThe picture is grim for many reasons. Impacts include:\n- direct lost of habitat to strip mining\n- settling ponds threat to migrants\n- fragmentation and destruction of habitat from deep drilling installations with their road and pipeline networks\n- air pollution from the operations and the production and refining processes\n- water withdrawal, diversions and contamination\nHow do the tar sands impact habitat?\nOne of the most common ways to extract the bitumen, the oil saturated sand and soil particles, is by stripping the vegetation, top soil and sub soils, draining the watercourses, and then scooping it out with giant machinery. 3,000 square kilometres of boreal forest will be strip mined in the next 30 to 50 years, based on current predictions. Strip mining destroys everything in its path. All the life-giving processes are removed. Soils are “stock-piled” as they are in more familiar residential housing developments. However, once stripped and piled, the vitality of the soil is destroyed.\nEfforts to reclaim mined lands and restore boreal forest fail miserably. The complex relationships between soil organisms such as bacteria, fungus, plants, invertebrates and larger fauna (including birds that are the hallmark of the boreal forest) are thousands of years in the making, but take only a few moments for the giant machines to destroy. This is the fate of habitat for up to 3.6 million birds!\nThe tailings ponds are created to store and ‘cap’ the residual waste product, after most of the oil has been removed from the bitumen. The residual is a toxic sludge that is pumped into artificial lakes, some several kilometres across, and ‘capped’ with clean water. These lakes will eventually cover about 100 square kilometres of area. They are death traps to birds landing in them, as was documented when 500 ducks died after landing in a Syncrude tailings pond in the spring of 2008. Annual mortality from tar sands to bird populations could be as high as 100,000 individuals!\nDeep drilling used to extract deeper bitumen deposits, requires a huge infrastructure of road networks, rigs, and pipelines and a reactor to produce steam. These typically burn natural gas, but there is much talk about using nuclear energy to produce steam, as is done for electrical generation. These operations and its infrastructure will destroy 5,000 square kilometres of boreal forest and result in significant fragmentation of a much larger area. These remaining fragments imbedded in the network of roads, pipelines and drilling rigs will be subject to excessive noise, dust, and pollution. Up to 14.5 million birds could be lost due to these activities!\nThe tar sands are by far the fastest growing source of greenhouse gases in Canada, producing as much as three times the amount of greenhouse gases as conventional oil production. In addition, production and refining operations produce huge emissions of toxins, from nitrogen oxides that acidify hundreds of square kilometres, to cadmium and arsenic that cause cancer. Many of these chemicals bioaccumulate in the food web, concentrating in predators such as birds, and ultimately impacting their reproductive success. Climate change is happening at a rate faster than wildlife can adapt, particularly in the north. For example, insect hatches on which so many species of migrating songbirds depend can be out of synch with migration timing.\nWater diversion and contamination\nApproximately one million cubic metres of water is diverted from the Athabasca River to tar sands operations each day. This water is used both in the tailings ponds and in the process to remove the oil from the soil particles. This is done by using steam, requiring vast amounts of water. The process uses approximately three times the water for every unit of oil produced. For the deep in situ extraction process, steam is injected into the ground to heat up the bitumen so that it can be pumped out. Tailings ponds are constructed in close proximity to the river, raising the potential for contamination of one of the Canada’s largest watersheds. Cancer rates in First Nations communities downstream from the tar sands operations have sky rocketed. Only 8 percent of the water removed from the river is returned. Ninety two percent ends up in the tailings ponds. The Athabasca watershed downstream is threatened, as the River is already under increasing stress from dropping water levels as the glaciers that feed into the Rocky mountains gradually retreat and sources diminish.\nBirds most at risk\nOf the 22 to 170 million birds that breed in the area that is and could be impacted by the tar sands, a large number of species are in trouble. Here are two very different examples, one big and one small.\nThe only natural population of Whooping Crane, a critically endangered species currently numbering around 400, is in Wood Buffalo National Park, directly northwest of the tar sands. The strip mines, forest fragments, and most ominously the 50 to 100 square kilometres of toxic tailing lakes which appear particularly inviting from the air, lie directly on their migration route. What are the chances over the next fifty years that a group of migrating Whooping Cranes drops out of the sky to take refuge from a storm in the toxic death traps below?\nOlive-sided Flycatcher was added to the official list of Canadian Species at Risk in 2007. The population of this exclusively insect eating bird has declined almost 80 percent in the last 40 years in North America. Most of its world population occurs in the Canadian boreal forest. Like many other boreal dependent species, it is being assaulted on many fronts, both on its breeding grounds, non-breeding grounds in the Amazon basin and Andean slopes of South America and during its extremely long migration in between.\nThe Olive-sided Flycatcher lives exclusively off flying insects, catching them in flight. The boreal forest in north eastern Alberta is an important area for this species. Loss of thousands of square kilometres of habitat will remove a chunk of its population. Climate change adds an additional stress. Climate change, particularly global warming, alters hatching dates for the insects, putting this important food source out of synch with the timing of bird migration. Climate change also leads to desiccating droughts and contributes to subtle changes in habitat that have not-so-subtle impacts. The tar sands are the biggest single contributor by far to greenhouse gases in Canada.\nIt is time to put a moratorium on the tar sands. It is also time to ask the Federal Government of Canada why it is not using the Migratory Bird Convention Act as an instrument to better protect boreal birds. This will be discussed in my next blog.\n(Photos: Evening Grosbeak, Jeff Nadler; Whooping Cranes, USFWS; Olive-sided Flycatcher, Mark Peck)']"	['<urn:uuid:9a9beecb-97a0-48a9-8f10-26fc0dc497f9>', '<urn:uuid:bbc769df-d50a-410f-b7cf-d91527ce4cab>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T05:28:56.360010	12	90	2103
93	How does mindfulness improve brain function while increasing cybersecurity risks?	Mindfulness has been shown to help preserve memory and brain function, stimulate new brain growth, aid decision-making, and improve thinking flexibility. Research even shows it can slow down the aging process at the DNA level by repairing telomeres. However, when people practice mindfulness and other activities from home, they may create cybersecurity vulnerabilities. Working from personal devices that aren't properly secured can lead to phishing attacks and ransomware risks, especially when employees are less focused or distracted while browsing potentially dangerous sites or opening infected files that would normally be blocked in an office environment.	['Mindfulness is a buzzword that’s been around for a few years now, but did you know that new science is emerging that suggests its pluses for your brain are impressively far-reaching?\nMEDITATION 2.0: A new way to meditate\nDr Craig Hassed is from Monash University in Melbourne’s Department of General Practice and has authored Mindfulness for Life and co-authored Mindful Learning. He says consistent use of the skill has been shown to:\n1. Lower your risk of relapse if you’ve suffered bouts of low mood\n2. Help preserve your memory and brain function\n3. Stimulate new brain growth\n4. Quieten down stress\n5. Aid your ability to make decisions\n6. Improve the flexibility of your thinking\n7. Grow your capacity for empathy\n8. Help you manage pain\n9. Slow down the brain’s ageing process\nOn the latter, Hassed adds, “One really fascinating area of research is to do with genetics. Research from Elizabeth Blackburn, the Nobel Prize winner and her team, shows that mindfulness stimulates the repair of telomere, which is associated with slowing down the ageing process on the level of our DNA.”\nWe quizzed Dr Hassed about how much time is needed to accrue these benefits, and how to best get on board.\nHow would you explain mindfulness?\nMindfulness is not just a form of meditation; it’s also a way of living. So when people are learning to develop mindfulness, it’s not just sitting in a chair, practising a bit of meditation. It’s what they take with them out of the chair that really matters.\nSo when they are talking with their friends, do they have their attention engaged? When they are eating food, do they really taste it? In their day-to-day work, are they focused on what they’re doing and really engaged with their work?\nBeing mindful means really living with attention, and being present and open and curious, and also being accepting about your day-to-day life – accepting of the ebb and flow, the ups and downs, all of which are quite natural.\nHow much time does it take to see some of the payoffs from practising mindfulness?\nYou start to get some of the benefits even if you’re doing 5 or 10 minutes a day. In the case of addressing more chronic illnesses, studies have looked at benefits among people doing 30 or 40 minutes in a day.\nWhat do you think is the biggest positive about practising mindfulness?\nProbably honing your intuition. As a result of mindfulness, the mind is calmer, clearer, more attentive and engaged – the brain works differently, and very often, in a much more intuitive way. When we’re quieter inwardly, we can hear our intuition more clearly. When we’re busy and distracted, very often we don’t hear our intuition.\nWhat’s the best way to get started?\nIt’s very good to do a course and to work with a group. Psychologists are trained it these days, and you can also access information and courses through The Black Dog Institute, Beyond Blue and the Anxiety Recovery Centre.\nThere are books you can read and there are apps, like Smiling Mind which are very good, but there’s nothing better than learning from a well-trained teacher alongside a group – here, you can learn from each other’s experiences as well.\nMindful Learning: Reduce stress and improve brain performance for effective learning ($29.99) is published by Exisle Publishing', 'Employees who work from their homes may be putting their companies’ systems at risk.\n“Many employees do company work from personally managed and owned systems and these machines are often the ‘Wild, Wild West’ in terms of how they are secured,” said Mike Gentile, the chief executive of San Clemente-based cybersecurity company Cisoshare.\n“The majority of complex attacks, such as ransomware, etc., right now are still often caused by a simple phishing attack or an employee mistake like clicking on a bad link.”\nCisoshare is one of several cybersecurity firms that are emerging in Orange County, which is carving a strong position in internet security due to the proliferation of hackers from Russia, China and North Korea who demand eye-popping sums in ransomware.\nCrowdStrike Holdings Inc. (Nasdaq: CRWD), a Sunnyvale-based firm that now has a $55 billion market cap, started in Orange County where it still has a large local presence. Irvine’s Cylance sold for about $1.4 billion to BlackBerry in 2019 and also counts a base of operations here.\nIn Newport Beach, the ioXt Alliance started by Mobilitie founder Gary Jabara, wants to make sure the interconnections among the various devices used each day—such as cellphones, smart home lighting controls and automotive technology—are also secure.\nUC Irvine’s Cybersecurity Policy & Research Institute studies ways to make the internet and networks safer, including running mock attack drills. Cybercriminals\nIrvine-based Netwrix Corp. is expanding so quickly that it’s made four acquisitions since January.\n“Most organizations did not have time to prepare a transition plan and provide security training to the employees” when they started working from home last year, said Ilia Sotnikov, security strategist and vice president at Netwrix.\n“Hence the increase in reported incidents that included data loss or oversharing.”\nAttackers know that ransomware is arguably the quickest way to get money from a company without breaking into its system, he said.\n“The cybercriminals took advantage of the global pandemic and highly divisive political scene in the U.S. last year,” Sotnikov said. “We’ve seen considerable changes in how the threat landscape evolved over the last couple years with ransomware as a service, more specialized groups.”\nThe Coronavirus Chaos\n“There was so much chaos during the first few months of the lockdown that every CISO will need to go back and review all of the access and changes that happened,” said Bil Harmer, who is the chief information security officer (CISO) and chief evangelist at computer identity security software maker SecureAuth.\n“When there is chaos and change, the threat actors will be there looking for ways in.”\nHe predicted that companies “will begin putting more and more focus on digital identities and a continuous authentication methodology that will allow them to adjust access on the fly as the landscape or the user behavior changes.”\nCisoshare, founded by Gentile, placed No. 21 on this year’s Business Journal list of Best Places to Work in Orange County and No. 2 on last year’s list of fastest-growing companies, both in the small-firm category.\nCompanies who let employees work from their homes face an increasing threat level similar to that of an apartment owner who adds more apartments to a portfolio: the more units there are, the greater the business risks, Gentile said.\n“When employees are working from home, it expands the digital footprint and perimeter of the organization,” Gentile said during a recent interview.\nProviding security also requires using precious resources and talent—both of which are in short supply at plenty of companies, Gentile said.\n“The majority of security risk lives in the cracks when people don’t effectively collaborate and ‘cover all the bases’ when building something,” he said.\nTraining on workstations from which a network is accessed can reduce the risks when employees work from home in a decentralized environment, Gentile said.\nCompanies that opt for a “hybrid” model combining both work-from-home and the office should be wary.\n“Hybrid can be risky due to any time rules change, there is a higher likelihood of mistakes,” Kevin McDonald, the chief operating officer and chief information security officer of Alvaka Networks in Irvine lists 16 points of vulnerability. They include use of bootlegged software, browsing illicit sites, opening infected files that would otherwise be blocked, communicating with unverified individuals and illegal sharing of various contraband such as movies, images, and games.\n“Gambling, pornography, sports, gaming sites, alternative bulletin boards, messengers, even terrorism and extremist sites lead to infections of the host that then connects to the company,” he said.\n“We all suffer from a bit of that-won’t-happen-to-me syndrome. We’re not a target, we don’t have anything they want, we’re not that rich of a company.”\nHe says ransomware attackers are well aware of the potential payoffs: “One hit and you can retire.”\nCompanies are starting to nudge employees into coming to their offices though the daily back-and-forth from the COVID-19 Delta variant makes it difficult to set firm guidelines.\nFor example, data analytics software maker Alteryx Inc. has “voluntarily opened a number of our offices, including our Irvine location for those who are comfortable coming in,” Chief Financial Officer Kevin Rubin said on Aug. 5. “There’s no mandate that they do.”\n“We will more officially begin asking associates to start coming back no sooner than January,” he added.\nSotnikov sees some bright spots.\n“I think many of the WFH (work-from-home) specific dangers were mitigated over the last 12 months, as organizations had a chance to catch their breath, get new budgets in 2021, catch up on trainings for both admins and employees,” he said.\nThe Senate included more than $1.9 billion in cybersecurity funds as part of the roughly $1 trillion bipartisan infrastructure package, The Hill website said on Aug. 10.\nThe funds will go toward securing critical infrastructure against attacks, helping vulnerable organizations defend themselves and providing funding for a key federal cyber office, among other initiatives.\nExperts point to the targeting of Colonial Pipeline and JBS meat packers earlier this year as examples of the dangers of ransomware demands.\nThe picture is acute on the international front, with both Sotnikov and McDonald noting President Joe Biden’s warning last month that a significant cyber-attack on the U.S. could lead to “a real shooting war” with a major power, highlighting the growing threats posed by Russia and China.\n“That is a very aggressive and provocative statement,” McDonald said. He points to China in particular as he surveys global cybersecurity threats to the U.S.\n“I should be worried about China finally deciding it’s time to become the sole world power and using its understanding of our weak infrastructure to show us how much we don’t really have control of the world anymore,” McDonald said.\nAnd the ultimate piece of bad news?\n“Replacements are made in China,” he said.\nDangers, Positive Signs: OC Cybersecurity Experts Look at Internet Risks\nOC Cybersecurity Experts Give the Business Journal These Tips:\nKEVIN MCDONALD, chief operating officer/chief information security officer, Alvaka Networks in Irvine\nSome dangers are “removed or reduced” if the equipment is owned by the employer.\n“Employee-owned computers are far less likely to be patched and kept up-to-date against vulnerability. This includes the operating systems, office applications, third party applications such as Adobe, Internet browsers, etc.\n“Having a system shared with non-employees (of unknown behavior tendency, character, education, intent) means that there is a high potential for risky behaviors that can result in a compromised local computer.\n“Big time execs and powerful people are targets and they’re the most reticent to participate in this whole process.\n“Cryptocurrency is the “primary reason” for the rise in ransomware in which hackers hijack a computer system and demand payment to release it.”\nMIKE GENTILE, founder/chief executive, Cisoshare in San Clemente\nThe biggest risk to work from home or hybrid for security “is that collaboration and effective small team dynamics are hindered when people can’t work together in person.”\n“The good news is that some of the strongest safeguards when a workforce is decentralized is a strong security training and awareness program, as well as a communication system so employees know how to get in touch with the security team and vice versa. Both of these items are highly effective, but also much more inexpensive than almost all technical safeguards.”\nBIL HARMER, chief information security officer/chief evangelist, SecureAuth in Irvine\n“The hybrid model will not go away, there is far too much upside for companies in it. From 48 extra minutes per day per employee in productivity to reduced footprints in the office (desks, power, coffee, etc), this is a model that will continue.\n“Companies will begin moving to Secure Identity as the first line of defense. They will begin putting more and more focus on digital identities and a continuous authentication methodology that will allow them to adjust access on the fly as the landscape or the user behavior changes.\n“This will allow the user to move around the physical world and have their authentication and authorization adjust as they do to keep them within the acceptable risk profile.”']	['<urn:uuid:9bfe3383-15fe-41ff-b3f3-2f1cf46343d3>', '<urn:uuid:ad45c4f6-8fbd-4d17-b17d-404d81708d47>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T05:28:56.360010	10	95	2060
94	What's the meaning of Sangiovese in English?	The name Sangiovese is believed to be of Latin origin (Sanguis Jovis), which means 'blood of Jupiter'.	"['The red grape variety comes from Italy, There are over 80 synonyms that testify to the age and wide distribution, especially in the country of origin. Some are Brunelletto, Brunello, Cacchiano, Calabrese, Cardisco, Cassano, Cordisio, Chiantino, Corinto Negro, Guarnacciola, Ingannacane, Lambrusco Mendoza, Liliano, Montepulciano Primaticcio, Morellino, Morellino di Scansano, Negrello, Nerello, Nerello Nellielluccio, Niella Primaticcio, Prugnolo Dolce, Prugnolo Gentile, Puttanella, Sangiogheto, Sangiovese dal Cannello Lungo, Sangiovese dal Cannello Lungo di Predappio, Sangiovese di Lamole, Sangiovese di Romagna, Sangiovese Dolce, Sangiovese Elba, Sangiovese Sangiovese Sangiovese Sino San Gioveto, Sangioveto, San Zoveto, Tabernello, Tignolo, Tignolo Sointovese, Toustain, Tuccanese, Uva Canina, Vigna del Conte and Vigna Maggio.\nThe variety was first called ""Sangiogheto"" by the Italian agronomist Giovanni Soderini (1526-1596) mentioned in his work published in 1600 posthumously. The name is said to be of Latin origin (Sanguis Jovis), which means ""blood of Jupiter"" and means one antique To indicate the origin of the vine. According to a non-verifiable hypothesis, an ancestor is said to already have the Etruscans be known. In any case, it is one of the most important Italian leading varieties with numerous offspring.\nDespite apparently suggestive synonyms or large ones morphological Sangiovese must not be similar to the varieties Ciliegiolo. Montepulciano. Morellino del Casentino. Morellino del Valdarno. Nero d\'Avola. Perricone. Sanvicetro or Uva Tosca be confused. extensive DNA analysis have revealed that to the varieties Aleatico. Ciliegiolo. Foglia Tonda. Frappato. Gaglioppo. Inzolia Nera. Morellino del Casentino. Morellino del Valdarno. Nerello Mascalese. Orisi and Perricone a Parent-offspring relationship consists. Sangiovese was also a crossing partner of the new varieties Incrocio Bruni 147. Incrocio Bruni 60, some Dalmasso varieties and Merlese,\nThe exact lineage (parenthood) has not been clearly established. The Swiss biologist Dr. José Vouillamoz found in 2007 that Sangiovese was a cross between Ciliegiolo x Calabrese di Montenuovo comes from (so Sangiovese is a descendant). However, this contradicts later analyzes by Dr. Manna Crespan, for Ciliegiolo a parenthood of Sangiovese x Muscat Rouge de Madère (so Sangiovese is a parent). Finally surrendered by Thierry Lacombe in 2012, analyzes were carried out of two possible parenthoods for Sangiovese. These are Frappato di Vittoria (Frappato) x Foglia Tonda or Gaglioppo x Foglia Tonda. However, since some of these strains have a parent-offspring relationship with Sangiovese, this is not that surprising. However, due to two possible variants and only 20 matching DNA markers, the result is at least doubtful (see under molecular Genetics ).\nThere are many Clones the variety with slightly different taste profiles. Previously, based on the studies published in 1908 by the ampelographers Girolamo Molon a rough classification according to the size of the berries and from this also quality in the two groups Sangiovese Grosso ( Brunello and Prugnolo Gentile, as well as Sangiovese di Lamole in Chianti) and Sangiovese Piccolo in other Tuscany zones. However, this is no longer justified because there are no genetic differences in the DNA profile.\nDue to the great adaptability to different soil types Different tastes have emerged over the centuries. At the end of the 1990s, the best clones were selected. The profitable vine is susceptible to Botrytis and yellowing but resistant to drought, The slow, late is characteristic maturity, It usually produces not very colorful, but alcohol, acid and tannin-rich red wines with aromas of cherries, violets, plums and leather, as well as large aging potential,\nIn 2010 the acreage was in Italy 71,619 hectares with increasing tendency (ten years earlier it was 62,761 hectares). It is by far the most common Italian grape variety, but especially in almost all regions Tuscany is common. It is the basis for many famous Italian top red wines. These are mainly the DOCG wines Brunello di Montalcino. Carmignano. Chianti. Chianti Classico. Conero. Morellino di Scansano. Rosso Conero. Torgiano Rosso Riserva and Vino Nobile di Montepulciano, as well as countless DOC wines and some of the so-called Super-Tuscans such as Tignanello,\nIn France In 2010 the area under cultivation was 1,589 hectares with a constant trend (ten years earlier it was a few hectares more). The majority is on Corsica, where the variety was imported from Italy by the Genoese who ruled the island until the end of the 18th century. It is mostly approved under the name Nielluccio in all appellation red and rose wines, as well as in the Vin de pays (country wines). There were further smaller stocks in Europe in Romania (88 ha), Turkey (9 ha) and Hungary (1 ha).\nOutside of Europe, there is acreage in the countries Argentina (2,011 ha), Australia (589 ha), Brazil (26 ha), Chile (100 ha), Israel. Canada (3 ha), New Zealand (6 ha), South Africa (61 ha), Thailand (2 ha) and Tunisia (842 ha), as well as in the United States (852 hectares) in the states California and Washington, In 2010, a total of 77,709 hectares of vines were shown with increasing tendency (ten years earlier it was 68,877 hectares). The variety thus proved worldwide varieties ranking rank 13.\nSource: Wine Grapes / J. Robinson, J. Harding, J. Vouillamoz / Penguin Books Ltd. 2012\nImages: Ursula Brühl, Doris Schneider, Julius Kühn Institute (JKI)']"	['<urn:uuid:a6d6d4c7-2024-446b-8a04-3d80cb0dac64>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T05:28:56.360010	7	17	849
95	What's important when building acoustic guitar sides?	It's crucial to keep the sides square and centered, especially at the heel block, to ensure proper neck angle and playability. Professional builders use two spreaders - one between heel and tail blocks, and another across the waist. Kerfing strips are also essential - they're attached around the edges to increase gluing surface area for the top and back plates. The sides themselves should be about 2mm thick, and the kerfing can be glued using simple clothes pegs or bulldog clips as clamps.	"['Our Martin acoustic kit build continues, as we level the kerfing, join the back plates and pop a little something in the oven. Huw Price shapes up…\nFor the first part of our Martin kit build last month, I created a body former for the sides of my 000 and started gluing the heel and tail blocks in place, and now it’s time to get things looking a bit more guitar-like.\nNow, one thing I’ve learned the hard way in my time building and repairing instruments is the absolute necessity of keeping things square and centred. This is especially true of the heel block, and if you allow it to become tilted or twisted, setting the neck angle and even ending up with a playable guitar will be that much harder.\nEven when the rims are lined up nicely in the mould I created, there’s nothing to prevent them from getting out of shape from pressure when gluing the top and the back. Most professional acoustic builders use two spreaders to hold everything in position while the body assembly takes place. One is used lengthways between the heel and tail blocks, and the second goes side to side between the innermost curves of the waist.\nThis easy to make spreader set will hold the sides and blocks square in the mould throughout the body building process\nAlthough you can buy spreaders from luthier suppliers, you can also make your own. I saw off some more blocks from the length of timber I used to create the mould blocks last time out and buy a length of threaded metal rod with some nuts and washers. You can find these items in most DIY outlets.\nHoles marginally greater in diameter than the threaded rod – but narrower than the washers – are then drilled into the spreader blocks, and the rod is then cut into suitable lengths. With the nuts and washers in position, the blocks are then slipped over the ends of the rods and the nuts adjusted to press the spreader blocks against the sides.\nThe idea is that the spreaders will only be removed once the top and back have been glued on. So, I double up the blocks at each end between the heel and tail blocks in order to keep the longitudinal rod’s length to a minimum. This is important because if it’s too long, you might be unable to remove the rod through the soundhole.\nNothing fancy is needed for gluing the kerfing and a set of clothes pegs works just fine\nAs the sides are barely 2mm thick, there’s insufficient width to achieve a satisfactory glue joint for the top and back plates. To increase the gluing surface area, strips of kerfing are fixed around the edges of the rims. Various timbers are used, but Martin’s appear to be mahogany. The strips are tapered and sawed halfway through at 7.5mm intervals, which allows them to bend easily and conform to the guitar’s curves.\nGluing kerfing is quick and easy, but I find it best to divide each quarter into three sections. The one tricky area is the waist because, with OM and 000 shaped guitars, the curve is too pronounced for the kerfing and it tends to snap. Try heating the kerfing before bending it around the waist – a paint stripper gun or hairdryer should work fine. This softens and loosens the fibres, which makes the kerfing more pliable and less likely to snap.\n220 grit sandpaper fixed to a flat plank with double-sided tape is used to level the kerfing with the rims\nApply glue to the flat side of the kerfing and position it against the side, leaving it slightly above the rims all the way around. Believe it or not, clothes pegs are ideal for clamping the kerfing in position while the glue dries. You can also use bulldog clips as an alternative if you wish.\nHaving applied kerfing around the rear edges of the body, I use a file to remove some of the kerfing’s excess height. Then I take a dead flat length of timber with 220 grit paper attached using double sided tape. This soon brings the kerfing dead level with the rims while keeping everything flat and square.\nMany modern builders prefer angling the edge gluing surfaces to conform to the curvature of the front and back plates, but Martin’s traditional method is to level the kerfing at a right angle to the sides. Huss & Dalton offer both styles, with the Martin method deemed to generate more bass, power and projection, but less midrange. In contrast, angled kerfing is said to produce a more even frequency balance.\nThe glue squeeze out is cleaned, the inside edges are sanded and the levelled kerfing is flat against the backing board\nI decide on flat kerfing, because this is a Martin kit and I do appreciate traditional Martin tone. The first strips of kerfing are glued with clamps holding the rims square, but I install the spreaders when I flip the rims over to do the front side. With the body in the mould I carefully remove any glue squeeze out from the kerfing and sand the inner surfaces of the sides. Mahogany is a lovely wood to work with, and before long the sides look clean and tidy.\nTruing the edges\nJointing the back plates demands some precision woodworking but specialist tools are not required. In a process known as candling the plates are brought together on a large window. When you look carefully at the join, you will see light shining through gaps at various places along the length. You often need to adjust your viewing angle to see them, but gaps are always there. Back in the day this was done by candlelight – hence the term – however daylight works, too. This is done because all gaps must be eliminated to ensure a strong and long-lasting glue joint.\nWith the back plates brought together against a flat piece of glass, daylight can be seen through the gap\nTo eliminate the gaps you’ll need to make a shooting board. Two flat pieces of 18mm MDF or plywood will do fine. Ensure the lengths are equal and longer than the body length of the guitar. The width of one can be less than the other, or you can offset them when you glue one on top of the other. The idea is to create two levels, so the offset pieces of equal size will yield two shooting boards.\nTake the back plates and place one on top of the other so that both inner surfaces are facing outwards. Place the back plates on the higher tier of your shooting board with the joint edges overlapping the lower tier then line them up exactly before clamping the plates in position at each end. The traditional way of trueing the edges is to run a long jointer plane along the length. The plane is positioned with its side surface flat on the shooting board’s lower tier and the blade should be set for the finest possible cut. Block and jack planes are really too short for this job.\nThis shooting board has seen a lot of use, but it’s still going strong and it only takes minutes to make one yourself\nIf the shavings are so thin that they turn to powder when you rub them between your fingers, the blade depth is about right. The blade must also be set dead square so an even amount of wood is removed across both joint surfaces. My advice would be to practise on scraps of timber to set the plane correctly before moving onto the back plates. Make two or three passes then unclamp the plates to check your process using the candling method. It’s vital to have perfectly flat glass and I can report from experience that Edwardian and Victorian glass is not flat!\nIf you’re inexperienced with planes, or you don’t own one, you can use sandpaper instead, but you’ll still need that shooting board. The abrasive must be attached to a dead flat surface – like a carpenter’s level or even a jointing plane with its blade fully retracted – with double sided tape. Since we’re dealing with small fractions of a millimetre here, I’d suggest 320 grit.\nThe jointing process reaches its final stages as we move from using the plane to sandpaper\nI’ve never gotten this quite perfect by using a plane alone. This occasion is no exception, and it’s only when I swap to sandpaper that I finally manage to close the gaps. Once you’re in the ballpark, you’ll probably see gaps in certain areas along the joint. Try marking these areas with a pencil to use as a guide when you’re removing material from the adjacent high spots.\nTowards the final stages you may only need one or two passes with the sandpaper. Taking the plates on and off the shooting board can get tedious, and if you’re doing this for the first time, don’t be discouraged if it takes you a few hours to get it right. This bit can be tough no doubt, but do persevere, because you’ll get there eventually.\nOne plate is propped up and strips of binding tape are stretched across the join line leaving small gaps at the centre\nGluing the plates\nThere are various ways to glue the back plates together, but this particular procedure is slightly complicated because the plates have already been shaped. After researching online, I discover Chris Paulick’s tape method and I decide to give it a try. For this you’ll need a backing board with a plastic surface or a plastic tape covering to prevent the plates sticking to it. Carefully align the plates and prop one of them up by about 5cm with the other lying flat. You may need to tape the flat piece to the backing board to prevent it sliding.\nTake some tape that has some degree of stretchiness – Rothko & Frost’s binding tape works just fine. Pre-cut about 20 or so 5cm strips then place them at 2cm intervals all along the join line. The crucial bit is that you must stretch them across leaving a slight gap above the join line.\nCarefully bring the plates back together with the tape strips on the inside and lightly clamp them with the joint side up in a vice or workbench. Run a line of glue evenly all along one edge, then remove the plates from the clamp. Place the plates on the backing board tape side down. If the tape has been applied correctly, the plates will be raised up in the centre like a tent.\nThe plates are pressed flat with the tape strips on the underside then weighed down and clamped overnight as the glue dries\nNow press the centre line flat against the board and the stretchiness of the tape should pull the plates together. Ensure the plates are flat and properly aligned all along the glue line then wipe off the glue squeeze out with a damp rag. Place a flat plank of wood over the join line and place a heavy weight on top. I use a concrete block and protect the plank with a layer of plastic packing tape. Since the block is shorter than the body, I also clamp the ends.\nFor this job I decide on Titebond original glue. It’s an aliphatic resin that is very popular with guitar builders and it has a much longer open time than hide glue. The plates are left for 24 hours before I remove the clamps and the block and I’m pleased with the way it turned out.\nThe tape method works, it costs next to nothing and it’s an ideal solution for DIY guitar building. I would suggest one or two practice runs before gluing up – but bear in mind that the tape strips will lose some of their stretchiness, so they’ll need to be renewed each time. Good preparation is also key, so make sure your wet cloth, plank, claps and weight are all close at hand before you start.\nThe kit included a black centre strip but we prefer the no-line look and the plates matched up pretty well without it\nNo doubt this next part will have some experienced guitar builders rolling their eyes but, having been greatly impressed by the tonal properties torrefied timbers on guitars I’ve played of late, I decide to attempt it myself. Much as I would have liked to torrefy the top plates, I worry that the heat is going to compromise the glue joint. However, the braces seem like fair game and, after talking it over with Alister Atkin, I decide I’ll follow his procedure by baking them at 100 degrees centigrade.\nMy main concern is that the wood might twist a bit when they go into the oven, so I wire up the two back braces tightly against a perforated metal plate – in this case baguette moulds fit the bill nicely. After 90 minutes in the oven the cooled braces look very slightly darker than the non-baked braces.\nTwo of the back braces are wired tightly before sitting in a 100-degree oven for 90 minutes\nTo my relief they hold their shape, have a noticeable ping when tapped and rustle loudly when being handled. Suitably encouraged, I go ahead and bake the remaining braces. Of course this isn’t proper torrefaction as such, but it is good fun!\nWhen we check back with the Martin in a couple of months time, we’ll move on to bracing. It will also be the time when I’ll have to decide whether to brace the top in the modern or pre-war style. A genuine 1930s Martin would certainly help with the measurements – so watch this space because you never know what might turn up…', ""- Bindings and Purflings\n- Body Joint Dovetail Mortise\n- Bracing the Back Plate\n- Charles Fox Side Bender\n- Conoid Chair\n- Electrical Upgrade\n- End Graft\n- Fitting the Back\n- Fitting the Top\n- Fleischman/Stevens Universal Binding Jig\n- Free Downloads\n- French Polishing\n- Fusako's Table\n- General Woodworking\n- Gluing the Back Plate\n- Gluing the Body\n- Guitar Repair\n- Headstock Veneer\n- Heel Cap and Neck Glue-Up\n- Kitchen Prep Table\n- Light Box\n- Making a bone Nut\n- Martin Style Pyramid Bridge\n- Neck Joint Jig\n- Plate Glue-up\n- Plate Templates\n- Saddle Slot\n- Shaping the Neck\n- Side Bending\n- The Fretboard\n- Thicknessing and Rosette\n- Trek 9.8 Decal Sets\n- Wood Step Ladder\nCategory Archives: Side Bending\nNext I am gong to glue up the bent sides to create the form for the body. This is pretty straight forward but one thing is to keep the end blocks centered and square to the side.\nBefore gluing it up my sides are 100mm wide. The body will have a taper on it. That taper was penciled in before bending so I will know how deep to plane.\nWith the bent side in the mold make a pencil mark to cut side to length. The end should line up to the center mark on the mold.\nUsing a square block, clamp it down and cut to length.\nThe body will be tapered toward the neck so the neck block will be cut to 80mm and the tail block to 95mm. The ends of each block are cut at a 4 degree angle so the dome plates mate nicely.\nMake a center line mark on your end block and have the two ends of your sides meet at the center line. Using a square make sure your block is square to the side. Spread glue and clamp it.\nAfter about two hours or so you can do the other end.\nIn this next post I will bend some rosewood sides thicknessed down to 2mm.\nTape up the bookmatched sides for the jointer.\nThickness them down to 2.1ish mm.\nSome final hand sanding.\nThe sides will be tapered so I use a template to pencil in the taper and then make another pencil mark as a reference for placement on the bender.\nI am using a heat blanket for the heat source and some flashing to make a sandwich. From the bottom up I have wood, flashing, heat blanket, flashing. The flashing protects the wood from getting burned.\nSee the mark is lined up to to mark on the bender. I want the bookmatching to be the same.\nEverything in place I applied a spritz of water on the surface of the wood and start bending.\nLet the wood heat up. If you have a thermometer start bringing down the press slowly when the temperature reaches around 250-300F. Don’t force the wood to bend and as you apply pressure you can feel the wood giving in.\nHere you can see the bent side in the mold. But I had some problems fitting it. I couldn’t figure it out until I read online that I should make my bending mold about 2mm smaller than my body mold to account for the wood thickness. So I took everything apart and shaved 2mm off the bending mold and did it over.\nI was worried that re-bending the wood would not be successful as wood plasticizes and sets when you heat it.\nI had no problem with the second bend. A nice fit.\nThe Fox bending jig really is the cats meow for hobbyist and professional alike. Easily adaptable and almost no fail it’s a total winner. This was my first time to bend guitar sides and even though I had some problems I was able to go back and re-bend the sides to a perfect fit.\nFinally back with some pics uploaded to my server. Still having the same problem with my domain so for now I am updating this from a friends place.\nCurrently I have the top and back plate finished and will move on to the side bending. There are two basic ways to bend the sides: 1. A bending iron, which can be an electrical device or a simple pipe with a flame heating it. 2. The second is the widely popular Fox Bending jig invented by Charles Fox in the mid 70′s. For mine I made a Fox bending jig.\nA simple cradle with the body cut from the template.\nCheck the fit.\nThe sides of the jig with a routed slot for the press to slide up and down.\nCheck the match.\nI want the routed slot to match up to the bend.\nScrew the sides on.\nMake a caul to fit the mold.\nFit the caul in place, screw it altogether with the press hardware.\nA finished Fox bending side jig.""]"	['<urn:uuid:e9b93526-0015-4ecb-a4fa-1797c196bd26>', '<urn:uuid:7da131d8-7646-451d-8cac-6c8c199862ec>']	factoid	with-premise	concise-and-natural	distant-from-document	three-doc	novice	2025-05-13T05:28:56.360010	7	83	3147
96	I'm curious about Bob Dylan's Blonde on Blonde cover design. What innovation did it introduce?	The Blonde on Blonde album cover introduced an innovative folding design that was likely the first of its kind for an album. This design allowed for the use of an interesting rectangular/vertical photo instead of the standard square format. While this design threatened to make the record fall out, it prioritized the graphic impact. The cover used a photo by Jerry Schatzberg that Dylan himself brought in, with the design team contributing the type styles and cropping.	"[""Across the Graphic Universe: An Interview with John Berg\nJohn Berg truly had “the best possible job at the best possible time.” As art director (and later creative director and vice president) of Columbia/CBS Records from 1961 until 1985, Berg oversaw a golden age of record cover design. From his office in New York's Midtown he created covers for Dylan, Springsteen and Monk, to name just a few. He commissioned photography from Avedon and illustrations from Glaser and Chwast. He won four Grammys and countless other music and design industry awards. If you've ever purchased any popular music from that era, there's a good chance that it was released by Columbia, and that its visual presentation was in some way conceived by the creative mind of John Berg.\nPhotograph of John Berg for Vogue Italia (by Giuseppe Pino, 1982).\nYet decades later, in the age of digital downloads and compact discs, very few in the design industries seem to remember or appreciate Berg's significant graphic legacy, even though it's part of the cultural fabric. Reduced images, such as the ones included here, do his work little justice. Only by pulling back the gatefold sleeves of Dylan's Blonde on Blonde or Springsteen's Born to Run can Berg's masterful exploitation of the grand-scale format be appreciated. In our practically package-less age, his work stands as a testament to a more heroic and relevant time for music.\nWith such thoughts in mind, I went looking for John Berg and found him through his wife, Durell Godfrey, a well-known illustrator and current contributing photographer for The East Hampton Star, the Long Island newspaper where the couple resides. She was kind enough to act as an email liaison, bringing my questions to him, then transcribing his answers and sending them back to me. It's fitting for AIGA to publish this interview, since an exhibit of Berg's CBS Records work was mounted in its New York gallery in the late '70s and traveled to Paris and Ferrara, Italy (Berg also served on the AIGA board of directors around that time). Hopefully, Berg's legacy will continue to influence a new generation of designers who might benefit from his vast contributions.\nNini: Early on, you held various jobs in advertising and editorial design. How important were these experiences to you when you started working at Columbia? Were there skills or approaches you learned in those other areas that directly applied to the design of record covers?\nEscapade magazine covers, 1959.\nBerg: I worked in the promotion art department at the Atlanta Paper Company for a year, and during that time I was mentioned twice in the Graphis annual. And I had some success in designing bottle carrier packaging for Pepsi. I later worked on accounts at Grey Advertising and Doyle Dane Bernbach (DDB) agencies in New York. Realizing that advertising was not for me, I moved over to magazines and was a promotion designer and later a promotion art director at Esquire. I was there for three years before moving over to editorial, at a girlie magazine called Escapade, around 1959. My work there won a lot of awards from the New York Art Directors Club. There was a lot of freedom with the layouts—and the pictures were fun! The combination of these experiences molded my outlook toward graphic design, which was telling stories conceptually and stylishly.\nNini: In Gary Marmorstein's book The Label: The Story of Columbia Records, you mention being hired “on the spot” by Bob Cato. Is there more to the story?\nBerg: By 1960 I was out of work and my wife was pregnant with our first child. We were living in Manhattan, and I dropped off my portfolio all over the place looking for a job. I had no expectations, but one day I just took my portfolio up to Columbia Records. Bob Cato saw me talking to his secretary and pulled me into his office for an interview. I had never seen him before, but he seemed to know who I was. He offered me a job on the spot. I stalled him because I had also interviewed at Life magazine, and I wanted to hear back from them before making a decision. After two weeks I took the job at Columbia. I never asked Bob how he knew who I was.\nNini: You joined Columbia Records as an art director in 1961, following in the footsteps of Alex Steinweiss (from late '30s to early '60s) and Neil Fujita, Roy Kuhlman and Bob Cato (during the '50s and '60s). Did you feel a certain responsibility to further what had been produced before your time, or more of a desire to take things in new directions?\nBerg: I never worked with Alex Steinweiss, Neil Fujita or Roy Kuhlman, but was certainly aware of their work. My approach was very personal. I did what I thought was interesting, appropriate, informative, intriguing and, most important, graphically compelling. I think if you can achieve even one of these, you are doing good work. The more, of course, the better the work. I think I may have taken things in directions they never dreamed of.\nBob Dylan “Blonde on Blonde” cover (1966).\nNini: Some of your early work at Columbia included covers for Bob Dylan, with whom you continued to work for several years. Did you realize at the time the kind of cultural impact he would have in the '60s and '70s? What were your experiences working with him on his record covers—any particular stories worth noting?\nBerg: Bob Dylan was a personal hero to most of us on the creative end of the record business. The salespeople actively hated his work and what it stood for. Musically, I thought he was incredible and years ahead of everyone. We had an in-house photographer named Don Hunstein. He often photographed Dylan for me, and Bobby (as we called him) frequently chose the picture he wanted to use on his album covers. Hunstein shot the first album cover (Bob Dylan, 1962) on a windowsill at the Columbia offices. The Freewheelin' Bob Dylan (1963), shot on a snowy street, was also Hunstein's photo, and Bobby chose that image. Sometimes he would organize his own photo shoots and just arrive with his favorite picture. The Jerry Schatzberg cover for Blonde on Blonde is an example of a photo he showed up with. We worked with that picture rather than coming up with a total concept, so the type styles and the cropping became our contributions. I designed the Blonde on Blonde cover for Dylan so that it folded down. That allowed for an interesting rectangular/vertical photo instead of the standard square format. I think it was the first time that had been done with an album. Of course, the record threatened to fall out, but the graphics were great.\nMilton Glaser's “Dylan” poster (1967).\nNini: You commissioned Milton Glaser to do the now famous “psychedelic” Dylan poster that became a cultural icon. What is the story behind that particular project?\nBerg: The Milton Glaser poster was assigned by me to go into Dylan's first Greatest Hits package (1967). The cover used Rowland Scherman's photo of Dylan's back-lit hair. That was the first time, to my knowledge, that a poster was shipped within a record package. The Greatest Hits album was cooked up because at that time Dylan was recovering from a motorcycle accident and couldn't produce a new album. An early sketch from Milton included a harmonica, by the way. That sketch is long gone. Of course the poster is on the cover of Milton's book.\nCount Basie “Super Chief” cover (1972).\nNini: You had the opportunity to work with a variety of well-known photographers and illustrators, including Richard Avedon, Milton Glaser, Paul Davis and Seymour Chwast, to name a few. How did you decide when it was time to bring in a collaborator, and what kind of direction did you usually provide?\nBerg: I was intent on winning prestigious AIGA and Art Directors Club awards, so that I could prevail on artists and photographers to work within the somewhat limited budgets available at Columbia Records. I chose photographers and illustrators who I thought would bring interesting takes on my concepts. Sometimes I made a sketch (the Count Basie Super Chief cover—airbrushed by David Willardson—comes to mind). I also liked to encourage illustrators to try new things with their art—a watercolor artist to try oils, a drawer to try paint, a type designer to try rendering in tooled leather. So I did meddle, but I did not force things. I wanted the work to be intriguing, to tell a story, to be a fresh take on the subject.\n“Cheap Thrills” cover (1967).\nNini: Cartoonist R. Crumb's illustration for Big Brother and the Holding Company's Cheap Thrills (1967) appears to have been his only “commercial job,” and a somewhat controversial one at that. How did this project come about?\nBerg: [Janis] Joplin commissioned it, and she delivered Cheap Thrills to me personally in the office. There were no changes with R. Crumb. He refused to be paid, saying, “I don't want Columbia's filthy lucre.”\nNini: The series you did over the life of the band Chicago is a real landmark in record cover design. What kind of ideas and approaches were you applying through these covers?\nBerg: Chicago was a unique situation. Jim Guercio, the musical mind behind the band as well as the band's manager, and I never wanted to show the band on the album covers (with the exception of the greatest hits cover). The Chicago logo—I think the first band logo, per se—was fashioned for me by Nick Fasciano from my sketch. If you look carefully, the logo, whether rendered in leather or chocolate, in the form of a map or a bank note, is always the same size and in the same position on the cover. It was great fun to try to figure out yet another way to imagine that logo when a new record cover needed to be done. I think there were a total of 14 before I left the job. The next designers didn't notice the graphic game I had been playing, the consistent size and position. The Chicago logo was inspired by the Coca-Cola logo, by the way.\nCovers for the group Chicago (from left): Chicago V (1972), Chicago X (1976), Chicago XI (1977) and Chicago XIV (1980).\nNini: Bruce Springsteen's Born To Run is another landmark cover, especially as it takes advantage of the possibilities with the gatefold album sleeve format. How was it conceived?\nBerg: The cover for Born to Run walked in the door courtesy of photographer Eric Meola, who had done a photo shoot on spec for Springsteen. Bruce originally wanted a serious, author-like head shot for the cover. I thought that was a boring and without an “idea.” I went through the whole photo shoot. I'm a good photo editor, and I found the beautifully charming shot of Bruce and Clarence. I sold the idea to management on the basis of humor and charm. Because of the fold it was an expensive cover to produce. It was an easy sell to Springsteen—the photo is wonderful. Now that cover is an icon. The image has even been copied using Sesame Street characters. Imitation is the sincerest form of flattery.\nBruce Springsteen's “Born to Run” cover (1975) and the Sesame Street homage, “Born to Add” (1995).\nNini: For many years you operated as a true art director, orchestrating projects with a variety of collaborators. This approach differs somewhat from what many well-known designers do today, where they often establish a “signature style.” What do you think about that trend, and do you prefer one approach over another?\nBerg: I never directly thought about a “style” to my work. I was just trying to come up with a graphic solution, a concept for each project. If a style evolved over the years, it might be inventive typography concepts, one where the letterforms seem to illustrate what the words are saying—Dave Brubeck's Bossa Nova USA for example. Sometimes an illustration would solve the problem, sometimes a photograph. Those projects were great fun to do. On Thelonius Monk's Underground, a project with photographers Steve Horn and Norman Griner, the title of the album came from a current jazz movement, which I twisted into a version of the French anti-Nazi underground of World War II. An entire set was built and the scene was full of costumed extras. There was no problem with budgets in those days. I won a Grammy for that cover, by the way. Again with Horn/Griner, I devised the cover of Switched-on Bach with [a stand-in for] Bach and a Moog synthesizer.\nAnd there were the times when I was just plain lucky, as with Richard Avedon's photo of a leaping and pointing Sly Stone. I chose the picture from the shoot and cropped it slightly to have the pointing finger touch the edge of the cover. Avedon said my design was the best album ever done from one of his pictures.\nThe Dave Brubeck Quartet “Bossa Nova USA” cover (1962); Thelonius Monk “Underground” cover (1967); Wendy Carlos “Switched-on Bach” cover (1968); Sly and the Family Stone “Fresh” cover (1973).\nNini: In your work I see interplay between word and image, sometimes humorous, and attention to typographic detail. Would you agree that these issues were important to you? Also, while your work naturally reflects the time it was made, much of it appears to have a directness and simplicity that holds up well over the years. Was this a concern of yours, too?\nBerg: I think it's a result of classic principals of graphics with applications of good old storytelling, humor and great ideas. I enjoy the interplay between image and word—and humor plays a part where it can. The work should be fun, either to look at or fun for the imagination. I think the work is direct, and I am glad to think it holds up over the years. Is that not the definition of classic?\nNini: Were there any designers from the past or contemporaries whose work you found particularly influential?\n“Monk” cover (1965), Eugene Smith, photographer.\nBerg: A.M. Cassandre, the great French poster artist who combined graphic design, illustration and high style. Henry Wolf, art director of Esquire and Harper's Bazaar in the '50s, used wonderful typography in those magazines. Paul Rand, whose work was similar to Cassandre in terms of style. Reid Miles, photographer and designer of jazz albums for Blue Note, whose great sense of typography was a constant source of inspiration. Gene Federico, a brilliant advertising art director who created the IBM campaign. Lou Dorfsman, who wore many hats at CBS as chief designer, was a mentor and played a pivotal role in the world of graphic design at CBS corporate. Milton Glaser, a designer's designer. Alvin Lustig, who we lost too young—he was just about the classiest designer there ever was. I noticed his work while I was still working in Atlanta, and I bought album covers he designed—before I had a record player! Alex Steinweiss was the godfather of album design at Columbia Records and, by extension, the whole business of graphics in music.\nNini: New York in the '60s seems to have been a very special place, with culture and commerce coming together in ways they hadn't before. You were able to have an active role in shaping the popular culture of the era. Do you have any particular thoughts reflecting back on that time?\nBerg: I had the best possible job at the best possible time to have that job, at the center of the graphic universe. These days the dream jobs are in magazines, but at that time the job every design student wanted was doing record covers. Working at Columbia/CBS was like going to Harvard—hard to get into, but gave a wonderful education. The record companies in those days launched graphic superstars. It was a very heady time, and I know you can't capture in the tiny CD format what we accomplished graphically in the 12-inch square that was the record album. Columbia Records produced hundreds of records every year. I made sure that they had graphic integrity. That was my job.\nCORRECTION: In the introduction, the author originally referred to John Berg's office on New York's Seventh Avenue. The reference is not false, but perhaps not the most accurate. Columbia Records was located there when Berg started in the early 1960s, but moved later that decade to West 52nd Street and Sixth Avenue.""]"	['<urn:uuid:d2fa6ebb-08e3-4cb9-92bb-368b854d9520>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T05:28:56.360010	15	77	2768
97	factors affecting machine calibration accuracy	The process drift correction function keeps the instrument's calibration tuned to the process which includes multiple factors: changes in fill-air temperature, part temperature, ambient temperature, part elasticity, part absorption and seal creep.	"[""Software and Analysis\nCatch the Leak Testing Drift\nMeasurement equipment needs calibration. That goes without saying. But, when it comes to leak testing equipment, there are two issues to consider regarding regular calibration. One is the calibration of the leak orifice and the second is the calibration of the instrument to the test part and process.\nThe periodic calibration of the leak orifice is usually performed semiannually or annually, based on companies' ISO standards. This calibration should be performed on a regular basis to assure the accuracy of the leak standard.\nHowever, equally, if not more importantly, the calibration of the instrument to the test part and process should be performed frequently during the day to correct for the changes in the test environment and the part that affect the pressure change measurement. These changes, called drift, can affect test accuracy. An optional process drift correction function within the instrument will automatically make corrections for the cyclical changes in the test conditions.\nThis correction of the calibration is important because some variations in parts and test environments cause little change to test results while other tests are greatly affected. Some companies are conscientious about periodic calibration and others only worry about it when high reject rates occur.\nUsing a process drift correction function maintains system accuracy throughout the production period, improves production throughput by eliminating production downtime for frequent recalibrations, minimizes the opportunity to incorrectly calibrate the system, and reduces false rejects and accepts.\nGet the drift\nLeak testing is a form of nondestructive testing that finds leak sites and measures the quantity of material passing through these sites. When testing a part with pressure and performing a pressure decay test, it is referred to as pressurizing the part. When testing a part with a vacuum and performing a vacuum decay test, it is referred to as evacuating the part.\nTo verify that a leak testing process is finding a specified hole or leak rate in a product, you must have a calibrated hole or leak that matches the specification. These leak orifices are not certified in terms of hole size but in terms of airflow rate at a specified pressure. A pressure decay or mass flow instrument measures the differences in the pressure change effect on a test part when the part has virtually no leak vs. a part with a leak that is marginally unacceptable as defined by the calibrated leak orifice.\nBecause a part has normal pressure change effects from being pressurized, the calibration process teaches the leak test instrument how much pressure change is a result of the normal testing process and how much additional change is due to a known, defined leak. With a known calibrated leak, the customer can add that leak to any portion of the direct test circuit and see if the instrument displays that leak rate at the conclusion of its test. Without a calibration process that measures the no-leak pressure change and defines the pressure change or flow change differential due to a traceable leak, there is no direct correlation of any testing process to the actual leak that the customer desires to find.\nWhen a pressure decay or mass flow instrument is calibrated to a part and process, it has a pressure change or flow offset, known as the tar value, that represents a typical signal value measured by the instrument for a part that has no measurable leak. For every test, the instrument subtracts this offset value from the measured test signal and then calculates the corresponding leak rate.\nThe pressure change or flow offset value is a minute, residual, dynamically changing pressure or flow value caused by gradual temperature or volume changes in the sealed part. The pressure or flow change is measured during a precise time interval in the test procedure that is a dynamic process. Because leak tests are required in a short time cycle to meet production requirements, the test must be performed before the part's pressure completely stabilizes.\nNormalization process occurs\nThe test procedure involves compressing air into a fixed volume. Depending on the test pressure, that compression process creates a level of heat in the compressed air called adiabatic heat effect. Because the temperatures of the air in the part itself and the air outside the part are not exactly the same, they all dynamically try to normalize to some new common temperature.\nThis normalization process happens in an exponential way. The pressure in the sealed part changes dynamically with the temperature because of the Ideal Gas Law, which is stated as PV = nRT. Likewise, if the part walls move because of being pressurized, the volume of the part changes exponentially as the part stops growing. The pressure again will change dynamically with the volume change because of the Ideal Gas Law. The instruments read the pressure changes at precise time intervals along these dynamically changing pressure curves. If any conditions of the test change, it will alter the shape of the dynamically changing curve.\nFor example, if a part is tested in a non-air-conditioned factory, the temperature of the air in that factory will change gradually during the day from 70 to 90 degrees and back to 70 degrees. The change in the plant air temperature causes a change in the temperature relationship of the air temperature in the part vs. the part temperature vs. air temperature outside the part. If the differences are greater, temperature change and therefore pressure change will occur at the precise time when pressure loss is measured. Therefore the tar value or pressure offset will not be the same as when the instrument was calibrated. This will cause an error in the calculated leak rate. Because a pressure change of 0.01 per square inch might represent the 10 standard cubic centimeters per minute leak rate, a variation of 0.0005 per square inch would represent a 5% drift in calculated leak rate. This could translate to a difference in temperature change during the test of only 0.001 F.\nTemperature sensitive parts, such as heat exchangers, are greatly affected by temperature changes while heavily walled products or plastic products are not. Metal products are less susceptible to elasticity or volume changes than plastic products. For heat exchanger products, drift correction might prevent accepting 10% reject products or rejecting 10% accept products. For less temperature or volume-dependent products, process drift correction could affect the results of 1% to 2% of the products.\nIf the amount of drift in the leak rate results is unacceptable, the user has the option to recalibrate the instrument as frequently as necessary to track the unacceptable drift in test results.\nSome leak test products have a function that tracks the drift and corrects the calculated leak rate. This function keeps the instrument's calibration tuned to the process that includes changes in fill-air temperature, part temperature, ambient temperature, part elasticity, part absorption and seal creep.\nBefore activating an automatic process drift correction, it is usually best to download a day's production test results and plot them to see just how much drift does occur. Some instruments can store up to 1,000 test results, and most leak test products can download results via a RS232 port. A time plot of the test results will show the cyclical effects of the process drift on the test results. Also, the magnitude of the drift can be evaluated to determine if it adversely affects the test results.\nAfter analyzing the typical pressure change values over a typical production day and determining that most of the pressure loss changes occur in a cyclical fashion, process drift correction can be used to nearly eliminate performing automatic calibration to keep the instrument tuned to the process. A calibration verification will show that the system is accurately reading the leak rate.\n- Temperature sensitive parts, such as heat exchangers, are greatly affected by temperature changes while heavily walled products or plastic products are not. Metal products are less susceptible to elasticity or volume changes than plastic products.\n- The process drift correction function keeps the instrument's calibration tuned to the process which includes changes in fill-air temperature, part temperature, ambient temperature, part elasticity, part absorption and seal creep.\n- Before activating an automatic process drift correction, it is usually best to download a day's production test results and plot them to see just how much drift does occur.\n- Most leak test products can download results via a RS232 port.""]"	['<urn:uuid:82283120-00fe-40f5-93fc-e1bd61f15ac8>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-13T05:28:56.360010	5	32	1408
98	Why is it important to move files instead of copying them?	Moving files instead of copying them is important because copying leaves content in the original folder that will be searched again unnecessarily, leading to duplicate effort.	['Routine processing of library books frequently means using shelves and other spaces as staging areas for incoming and in-process items. As gifts and purchased books are acquired, cataloged and labeled, librarians typically work on them in batches, sorting on to separate shelves those which have not yet been searched in the catalog or which represent additional copies for the collection or which require a certain level of cataloging, etc. As they move through the processing of getting them to the library and ultimately, the reader they are moved from place to place in the back-rooms of library work areas.\nManaging digital content for the Smithsonian Digital Repository takes a similar set of steps, but instead of shelves or carts where in-process books are staged, incoming items are computer files which are temporarily stored in various folders and sub-folders and moved from one to the other after certain procedures are completed on each batch.\nThis presents a new set of challenges and a slightly altered workflow than librarians are used to. One of the big changes in working with digital texts is that it is not always clear what a file contains simply by looking at the filename; just as you can’t judge a book by its cover, you can’t judge a digital book by its filename. When we deal with books, most of us can tell generally what book we have by reading the spine or the cover (although complete and accurate bibliographic information still requires an examination of the title page or other cataloging data inside the item). But with computer files, we often have a large batch of items named similarly (for example: Am_zool_2011_pinniped_ecology.pdf). Filenames can contain descriptive information but for purposes of processing for digital library collections, it is necessary to open and view the file to be certain a matching publication record is identified.\nAn easy-to-overlook process in handling digital (versus physical) content is avoiding duplication of material. Backups are of course essential to working with digital material but because copying and replication are so easy to do, those who handle digital materials in libraries and archives have to make sure that they don’t create extra copies (and therefore extra work for themselves) when processing materials. For instance, when a certain batch of electronic reprints is checked in the Repository to ensure that they are not duplicated, the files are moved to a different folder for the subsequent workflow step. It is important to move them and not simply copy them since the latter will leave content in the original folder which will be searched a second time–unnecessarily. Moving books from shelf to shelf does not pose this risk, but inadvertently duplicating rather than moving files means that it becomes easy to duplicate effort as well.\nThe Smithsonian Libraries matches incoming digital reprints with their corresponding publications in the Smithsonian Research Online (SRO) database. During 2012, the SIL Discovery Services Division staff have typically matched 75-100 items per month which were then uploaded to the Repository. But they review a far greater number when you consider that some have to be removed from the normal workflow for various reasons. These include items where there is no corresponding publication in the SRO database, items which are poorly scanned or which are not easily identifiable from viewing the first page of the document. It is these “problems” that have to be moved to a separate folder for further work, just as physical books are moved to a separate shelf or book cart for further processing.\nProcessing digital information is a fairly new workflow for library staff and although it may actually mirror the management of printed books, it presents a set of unique circumstances that the SIL has (so far) successfully met. This is evident not only from personal observation of the process but also when considering that there have been over 16,000 digital articles added to the Repository in under 6 years.']	['<urn:uuid:a22a6073-5f08-4793-ac08-f06251a1e1dd>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T05:28:56.360010	11	26	655
99	Which is colder: areas where hot spots form or places where plates meet?	Hot spot areas involving mantle plumes are significantly hotter than subduction zones where plates meet. At subduction zones, the oceanic crust remains several hundreds of degrees cooler than the surrounding mantle, with a geothermal gradient typically less than 10°C/km. In contrast, hot spots are formed by hot mantle plumes that produce significant heat and melting when they reach the lithosphere.	"['Chapter 7 Metamorphism and Metamorphic Rocks\nAll of the important processes of metamorphism that we are familiar with can be directly related to geological processes caused by plate tectonics. The relationships between plate tectonics and metamorphism are summarized in Figure 7.14, and in more detail in Figures 7.15, 7.16, 7.17, and 7.19.\nMost regional metamorphism takes place within continental crust. While rocks can be metamorphosed at depth in most areas, the potential for metamorphism is greatest in the roots of mountain ranges where there is a strong likelihood for burial of relatively young sedimentary rock to great depths, as depicted in Figure 7.15. An example would be the Himalayan Range. At this continent-continent convergent boundary, sedimentary rocks have been both thrust up to great heights (nearly 9,000 m above sea level) and also buried to great depths. Considering that the normal geothermal gradient (the rate of increase in temperature with depth) is around 30°C per kilometre, rock buried to 9 km below sea level in this situation could be close to 18 km below the surface of the ground, and it is reasonable to expect temperatures up to 500°C. Metamorphic rocks formed there are likely to be foliated because of the strong directional pressure of converging plates.\nAt an oceanic spreading ridge, recently formed oceanic crust of gabbro and basalt is slowly moving away from the plate boundary (Figure 7.16). Water within the crust is forced to rise in the area close to the source of volcanic heat, and this draws more water in from farther out, which eventually creates a convective system where cold seawater is drawn into the crust and then out again onto the sea floor near the ridge. The passage of this water through the oceanic crust at 200° to 300°C promotes metamorphic reactions that change the original pyroxene in the rock to chlorite and serpentine. Because this metamorphism takes place at temperatures well below the temperature at which the rock originally formed (~1200°C), it is known as retrograde metamorphism. The rock that forms in this way is known as greenstone if it isn’t foliated, or greenschist if it is. Chlorite ((Mg5Al)(AlSi3)O10(OH)8) and serpentine ((Mg, Fe)3Si2O5(OH)4) are both “hydrated minerals” meaning that they have water (as OH) in their chemical formulas. When metamorphosed ocean crust is later subducted, the chlorite and serpentine are converted into new non-hydrous minerals (e.g., garnet and pyroxene) and the water that is released migrates into the overlying mantle, where it contributes to flux melting (Chapter 3, section 3.2).\nAt a subduction zone, oceanic crust is forced down into the hot mantle. But because the oceanic crust is now relatively cool, especially along its sea-floor upper surface, it does not heat up quickly, and the subducting rock remains several hundreds of degrees cooler than the surrounding mantle (Figure 7.17). A special type of metamorphism takes place under these very high-pressure but relatively low-temperature conditions, producing an amphibole mineral known as glaucophane (Na2(Mg3Al2)Si8O22(OH)2), which is blue in colour, and is a major component of a rock known as blueschist.\nIf you’ve never seen or even heard of blueschist, it’s not surprising. What is surprising is that anyone has seen it! Most blueschist forms in subduction zones, continues to be subducted, turns into eclogite at about 35 km depth, and then eventually sinks deep into the mantle — never to be seen again. In only a few places in the world, where the subduction process has been interrupted by some tectonic process, has partially subducted blueschist rock returned to the surface. One such place is the area around San Francisco; the rock is known as the Franciscan Complex (Figure 7.18).\nMagma is produced at convergent boundaries and rises toward the surface, where it can form magma bodies in the upper part of the crust. Such magma bodies, at temperatures of around 1000°C, heat up the surrounding rock, leading to contact metamorphism (Figure 7.19). Because this happens at relatively shallow depths, in the absence of directed pressure, the resulting rock does not normally develop foliation. The zone of contact metamorphism around an intrusion is very small (typically metres to tens of metres) compared with the extent of regional metamorphism in other settings (tens of thousands of square kilometres).\nRegional metamorphism also takes place within volcanic-arc mountain ranges, and because of the extra heat associated with the volcanism, the geothermal gradient is typically a little steeper in these settings (somewhere between 40° and 50°C/km). As a result higher grades of metamorphism can take place closer to surface than is the case in other areas (Figure 7.19).\nAnother way to understand metamorphism is by using a diagram that shows temperature on one axis and depth (which is equivalent to pressure) on the other (Figure 7.20). The three heavy dotted lines on this diagram represent Earth’s geothermal gradients under different conditions. In most areas, the rate of increase in temperature with depth is 30°C/km. In other words, if you go 1,000 m down into a mine, the temperature will be roughly 30°C warmer than the average temperature at the surface. In most parts of southern Canada, the average surface temperature is about 10°C, so at 1,000 m depth, it will be about 40°C. That’s uncomfortably hot, so deep mines must have effective ventilation systems. This typical geothermal gradient is shown by the green dotted line in Figure 7.20. At 10 km depth, the temperature is about 300°C and at 20 km it’s about 600°C.\nIn volcanic areas, the geothermal gradient is more like 40° to 50°C/km, so the temperature at 10 km depth is in the 400° to 500°C range. Along subduction zones, as described above, the cold oceanic crust keeps temperatures low, so the gradient is typically less than 10°C/km. The various types of metamorphism described above are represented in Figure 7.20 with the same letters (a through e) used in Figures 7.14 to 7.17 and 7.19.\nBy way of example, if we look at regional metamorphism in areas with typical geothermal gradients, we can see that burial in the 5 km to 10 km range puts us in the zeolite and clay mineral zone (see Figure 7.20), which is equivalent to the formation of slate. At 10 km to 15 km, we are in the greenschist zone (where chlorite would form in mafic volcanic rock) and very fine micas form in mudrock, to produce phyllite. At 15 km to 20 km, larger micas form to produce schist, and at 20 km to 25 km amphibole, feldspar, and quartz form to produce gneiss. Beyond 25 km depth in this setting, we cross the partial melting line for granite (or gneiss) with water present, and so we can expect migmatite to form.\nExercise 7.3 Metamorphic Rocks in Areas with Higher Geothermal Gradients\n|Metamorphic Rock Type||Depth (km)|\nFigure 7.20 shows the types of rock that might form from mudrock at various points along the curve of the “typical” geothermal gradient (dotted green line). Looking at the geothermal gradient for volcanic regions (dotted yellow line in Figure 7.20), estimate the depths at which you would expect to find the same types of rock forming from a mudrock parent.\n- Zeolites are silicate minerals that typically form during low-grade metamorphism of volcanic rocks. ↵', ""plate tectonics | Definition, Theory, Facts, & Evidence | misjon.info\nPlate tectonics, theory dealing with the dynamics of Earth's outer . There are two types of crust, continental and oceanic, which differ in their .. Furthermore, the relationship between hotspots and plumes is hotly debated. The theory of plate tectonics accounts nicely for the slow and steady volcanic activity that occurs at .. Peridotite, a rock type commonly found deep in hot spot Note the relationship between temperature and mineral composition and stability. This was a major puzzle in relation to plate tectonics. A Canadian Adding to Wilson's theory, Jason Morgan furthered the idea of the hot spot. Morgan As the magma reaches the surface, it cools quickly and forms pillow lava. This pillow .\nWhat causes hot spot volcanism? It is imagined that these plumes rise as a plastically deforming mass that has a bulbous plume head fed by a long, narrow plume tail. As the head impinges on the base of the lithosphere, it spreads outward into a mushroom shape. This causes decompressional melting of the hot mantle material, i. It is thought that the massive flood basalt provinces on earth are produced when large mantle plumes reach the lithosphere.\nNote the bulbous plume heads, the narrow plume tails, and the flattened plume heads as they impinge on the outer sphere representing the base of the lithosphere. Illustration how the progressively older islands formed above the stationary mantle plume Courtesy of the USGS.\nMantle plumes appear to be largely unaffected by plate motions. While a plume that feeds hot spot volcanoes remains stationary relative to the mantle, the plate above it usually moves.\nThe result is that a chain of progressively older volcanoes are created on the overlying plate. During the late 20th and early 21st centuries, scientific understanding of the deep mantle was greatly enhanced by high-resolution seismological studies combined with numerical modeling and laboratory experiments that mimicked conditions near the core-mantle boundary.\nAt a depth of about 5, km 3, milesthe outer core transitions to the inner core. The polarity of the iron crystals of the OIC is oriented in a north-south direction, whereas that of the IIC is oriented east-west.\nWhat is a Hot Spot? | Volcano World | Oregon State University\nEarth's coreThe internal layers of Earth's core, including its two inner cores. Plate boundaries Lithospheric plates are much thicker than oceanic or continental crust. Their boundaries do not usually coincide with those between oceans and continentsand their behaviour is only partly influenced by whether they carry oceans, continents, or both. The Pacific Plate, for example, is entirely oceanic, whereas the North American Plate is capped by continental crust in the west the North American continent and by oceanic crust in the east and extends under the Atlantic Ocean as far as the Mid-Atlantic Ridge.\nA general discussion of plate tectonics. In a simplified example of plate motion shown in the figure, movement of plate A to the left relative to plates B and C results in several types of simultaneous interactions along the plate boundaries.\nAt the rear, plates A and B move apart, or diverge, resulting in extension and the formation of a divergent margin. At the front, plates A and B overlap, or converge, resulting in compression and the formation of a convergent margin.\nAlong the sides, the plates slide past one another, a process called shear. As these zones of shear link other plate boundaries to one another, they are called transform faults. Theoretical diagram showing the effects of an advancing tectonic plate on other adjacent, but stationary, tectonic plates.\nAt the advancing edge of plate A, the overlap with plate B creates a convergent boundary. In contrast, the gap left behind the trailing edge of plate A forms a divergent boundary with plate B.\nAs plate A slides past portions of both plate B and plate C, transform boundaries develop. Divergent margins As plates move apart at a divergent plate boundarythe release of pressure produces partial melting of the underlying mantle. This molten material, known as magmais basaltic in composition and is buoyant. As a result, it wells up from below and cools close to the surface to generate new crust.\nBecause new crust is formed, divergent margins are also called constructive margins. Continental rifting Upwelling of magma causes the overlying lithosphere to uplift and stretch. Whether magmatism [the formation of igneous rock from magma] initiates the rifting or whether rifting decompresses the mantle and initiates magmatism is a matter of significant debate. If the diverging plates are capped by continental crust, fractures develop that are invaded by the ascending magma, prying the continents farther apart.\nSettling of the continental blocks creates a rift valleysuch as the present-day East African Rift Valley. As the rift continues to widen, the continental crust becomes progressively thinner until separation of the plates is achieved and a new ocean is created. The ascending partial melt cools and crystallizes to form new crust. Because the partial melt is basaltic in composition, the new crust is oceanic, and an ocean ridge develops along the site of the former continental rift. Consequently, diverging plate boundaries, even if they originate within continents, eventually come to lie in ocean basins of their own making.\nThe Thingvellir fracture lies in the Mid-Atlantic Ridge, which extends through the centre of Iceland. Samples collected from the ocean floor show that the age of oceanic crust increases with distance from the spreading centre —important evidence in favour of this process.\nThese age data also allow the rate of seafloor spreading to be determined, and they show that rates vary from about 0. Seafloor-spreading rates are much more rapid in the Pacific Ocean than in the Atlantic and Indian oceans.\nAt spreading rates of about 15 cm 6 inches per year, the entire crust beneath the Pacific Ocean about 15, km [9, miles] wide could be produced in million years. Divergence and creation of oceanic crust are accompanied by much volcanic activity and by many shallow earthquakes as the crust repeatedly rifts, heals, and rifts again. Brittle earthquake -prone rocks occur only in the shallow crust.\nDeep earthquakes, in contrast, occur less frequently, due to the high heat flow in the mantle rock. These regions of oceanic crust are swollen with heat and so are elevated by 2 to 3 km 1. The elevated topography results in a feedback scenario in which the resulting gravitational force pushes the crust apart, allowing new magma to well up from below, which in turn sustains the elevated topography.\nIts summits are typically 1 to 5 km 0. This is accomplished at convergent plate boundaries, also known as destructive plate boundaries, where one plate descends at an angle—that is, is subducted—beneath the other. Because oceanic crust cools as it ages, it eventually becomes denser than the underlying asthenosphere, and so it has a tendency to subduct, or dive under, adjacent continental plates or younger sections of oceanic crust.The Earth's crust: tectonic plate movement, volcanoes, tsunami, earthquakes\nThe life span of the oceanic crust is prolonged by its rigidity, but eventually this resistance is overcome. Experiments show that the subducted oceanic lithosphere is denser than the surrounding mantle to a depth of at least km about miles.\nThe mechanisms responsible for initiating subduction zones are controversial. During the late 20th and early 21st centuries, evidence emerged supporting the notion that subduction zones preferentially initiate along preexisting fractures such as transform faults in the oceanic crust.\nIrrespective of the exact mechanism, the geologic record indicates that the resistance to subduction is overcome eventually. Where two oceanic plates meet, the older, denser plate is preferentially subducted beneath the younger, warmer one. Where one of the plate margins is oceanic and the other is continental, the greater buoyancy of continental crust prevents it from sinking, and the oceanic plate is preferentially subducted. Continents are preferentially preserved in this manner relative to oceanic crust, which is continuously recycled into the mantle.\nThis explains why ocean floor rocks are generally less than million years old whereas the oldest continental rocks are more than 4 billion years old.\nBefore the middle of the 20th century, most geoscientists maintained that continental crust was too buoyant to be subducted. However, it later became clear that slivers of continental crust adjacent to the deep-sea trenchas well as sediments deposited in the trench, may be dragged down the subduction zone. The recycling of this material is detected in the chemistry of volcanoes that erupt above the subduction zone. Two plates carrying continental crust collide when the oceanic lithosphere between them has been eliminated.\nEventually, subduction ceases and towering mountain ranges, such as the Himalayasare created. See below Mountains by continental collision. Because the plates form an integrated system, it is not necessary that new crust formed at any given divergent boundary be completely compensated at the nearest subduction zone, as long as the total amount of crust generated equals that destroyed.\nSubduction zones The subduction process involves the descent into the mantle of a slab of cold hydrated oceanic lithosphere about km 60 miles thick that carries a relatively thin cap of oceanic sediments.\nThe factors that govern the dip of the subduction zone are not fully understood, but they probably include the age and thickness of the subducting oceanic lithosphere and the rate of plate convergence. Most, but not all, earthquakes in this planar dipping zone result from compressionand the seismic activity extends to km to miles below the surface, implying that the subducted crust retains some rigidity to this depth.\nAt greater depths the subducted plate is partially recycled into the mantle. The site of subduction is marked by a deep trench, between 5 and 11 km 3 and 7 miles deep, that is produced by frictional drag between the plates as the descending plate bends before it subducts. The overriding plate scrapes sediments and elevated portions of ocean floor off the upper crust of the lower plate, creating a zone of highly deformed rocks within the trench that becomes attached, or accreted, to the overriding plate.\nThis chaotic mixture is known as an accretionary wedge. The rocks in the subduction zone experience high pressures but relatively low temperatures, an effect of the descent of the cold oceanic slab.\nUnder these conditions the rocks recrystallize, or metamorphose, to form a suite of rocks known as blueschists, named for the diagnostic blue mineral called glaucophanewhich is stable only at the high pressures and low temperatures found in subduction zones. See also metamorphic rock. At deeper levels in the subduction zone that is, greater than 30—35 km [about 19—22 miles]eclogiteswhich consist of high-pressure minerals such as red garnet pyrope and omphacite pyroxeneform.\nThe formation of eclogite from blueschist is accompanied by a significant increase in density and has been recognized as an important additional factor that facilitates the subduction process. Island arcs When the downward-moving slab reaches a depth of about km 60 milesit gets sufficiently warm to drive off its most volatile components, thereby stimulating partial melting of mantle in the plate above the subduction zone known as the mantle wedge.\nMelting in the mantle wedge produces magmawhich is predominantly basaltic in composition. This magma rises to the surface and gives birth to a line of volcanoes in the overriding plate, known as a volcanic arctypically a few hundred kilometres behind the oceanic trench.\nThe distance between the trench and the arc, known as the arc-trench gap, depends on the angle of subduction. Steeper subduction zones have relatively narrow arc-trench gaps. A basin may form within this region, known as a fore-arc basin, and may be filled with sediments derived from the volcanic arc or with remains of oceanic crust.\nIf both plates are oceanic, as in the western Pacific Ocean, the volcanoes form a curved line of islandsknown as an island arcthat is parallel to the trench, as in the case of the Mariana Islands and the adjacent Mariana Trench. If one plate is continental, the volcanoes form inland, as they do in the Andes of western South America. Though the process of magma generation is similar, the ascending magma may change its composition as it rises through the thick lid of continental crust, or it may provide sufficient heat to melt the crust.\nIn either case, the composition of the volcanic mountains formed tends to be more silicon -rich and iron - and magnesium -poor relative to the volcanic rocks produced by ocean-ocean convergence. Back-arc basins Where both converging plates are oceanic, the margin of the older oceanic crust will be subducted because older oceanic crust is colder and therefore more dense.\nThis results in a process known as back-arc spreading, in which a basin opens up behind the island arc. The crust behind the arc becomes progressively thinner, and the decompression of the underlying mantle causes the crust to melt, initiating seafloor-spreading processessuch as melting and the production of basalt; these processes are similar to those that occur at ocean ridges. The geochemistry of the basalts produced at back-arc basins superficially resembles that of basalts produced at ocean ridgesbut subtle trace element analyses can detect the influence of a nearby subducted slab.\nThis style of subduction predominates in the western Pacific Oceanin which a number of back-arc basins separate several island arcs from Asia. However, if the rate of convergence increases or if anomalously thick oceanic crust possibly caused by rising mantle plume activity is conveyed into the subduction zone, the slab may flatten.\nSuch flattening causes the back-arc basin to close, resulting in deformationmetamorphismand even melting of the strata deposited in the basin. Mountain building If the rate of subduction in an ocean basin exceeds the rate at which the crust is formed at oceanic ridges, a convergent margin forms as the ocean initially contracts.\nThis process can lead to collision between the approaching continentswhich eventually terminates subduction.\nMountain building can occur in a number of ways at a convergent margin: Many mountain belts were developed by a combination of these processes. For example, the Cordilleran mountain belt of North America —which includes the Rocky Mountains as well as the Cascadesthe Sierra Nevadaand other mountain ranges near the Pacific coast—developed by a combination of subduction and terrane accretion.\nAs continental collisions are usually preceded by a long history of subduction and terrane accretion, many mountain belts record all three processes. Over the past 70 million years the subduction of the Neo-Tethys Seaa wedge-shaped body of water that was located between Gondwana and Laurasialed to the accretion of terranes along the margins of Laurasia, followed by continental collisions beginning about 30 million years ago between Africa and Europe and between India and Asia.\nThese collisions culminated in the formation of the Alps and the Himalayas. Jurassic paleogeographyDistribution of landmasses, mountainous regions, shallow seas, and deep ocean basins during the late Jurassic Period. Included in the paleogeographic reconstruction are the locations of the interval's subduction zones. Subduction results in voluminous magmatism in the mantle and crust overlying the subduction zoneand, therefore, the rocks in this region are warm and weak. Although subduction is a long-term process, the uplift that results in mountains tends to occur in discrete episodes and may reflect intervals of stronger plate convergence that squeezes the thermally weakened crust upward.\nFor example, rapid uplift of the Andes approximately 25 million years ago is evidenced by a reversal in the flow of the Amazon River from its ancestral path toward the Pacific Ocean to its modern path, which empties into the Atlantic Ocean.\nIn addition, models have indicated that the episodic opening and closing of back-arc basins have been the major factors in mountain-building processes, which have influenced the plate-tectonic evolution of the western Pacific for at least the past million years.\nMountains by terrane accretion As the ocean contracts by subduction, elevated regions within the ocean basin—terranes—are transported toward the subduction zone, where they are scraped off the descending plate and added—accreted—to the continental margin.\nSince the late Devonian and early Carboniferous periods, some million years ago, subduction beneath the western margin of North America has resulted in several collisions with terranes. The piecemeal addition of these accreted terranes has added an average of km miles in width along the western margin of the North American continentand the collisions have resulted in important pulses of mountain building.\nThe more gradual transition to the abyssal plain is a sediment-filled region called the continental rise. The continental shelf, slope, and rise are collectively called the continental margin. During these accretionary events, small sections of the oceanic crust may break away from the subducting slab as it descends. Instead of being subducted, these slices are thrust over the overriding plate and are said to be obducted.\nWhere this occurs, rare slices of ocean crust, known as ophiolitesare preserved on land.\nThey provide a valuable natural laboratory for studying the composition and character of the oceanic crust and the mechanisms of their emplacement and preservation on land.\nA classic example is the Coast Range ophiolite of Californiawhich is one of the most extensive ophiolite terranes in North America.\n- Intraplate (hot-spot) volcanism\n- Plate Tectonics and the Hawaiian Hot Spot\n- Hawaiian Islands and Hot Spots\nThese ophiolite deposits run from the Klamath Mountains in northern California southward to the Diablo Range in central California.""]"	['<urn:uuid:2faf8c25-3da0-467b-9a8d-b6a95e340344>', '<urn:uuid:88e9d5fb-76f0-4055-a65d-0acc9315fe84>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T05:28:56.360010	13	60	4092
100	I'm planning a pond design. How to hide filter equipment?	Waterfall filter boxes are often tall and can be hidden by placing camouflaging boulders and plants around them. You can also cover the flex pipe with soil that was dug from the pond area, and place stones around the pond to help conceal the waterfall filter box, making the waterfall look more natural.	"[""A backyard pond with a waterfall brings added beauty and style to the landscape while the soothing sound of flowing water cancels out distracting neighborhood noises. Above ground waterfall filters are designed to pump the pond water through filter mats in the bottom of the filter box, cleaning the water from dirt and debris before returning it to the pond. Waterfall filter boxes are often tall and can be hidden by placing camouflaging boulders and plants around them.\nLayout the design of the pond and the desired location of the above-ground waterfall filter. Place the above-ground waterfall filter where you want the waterfall stream to flow into the pond.\nExcavate the pond and waterfall filter area with a shovel or excavating machine. Level the soil in the waterfall filter area from side to side; provide a 1/2 to 1 inch grade the soil from the back of the filter area to the front. Lay a wooden board on top of the soil. Place a carpenter's level on top of the board to check the grade of the soil from the back to front and the level from side to side of the waterfall filter area.\nCompact the soil in the waterfall area with a hand tamper. Check the area for level and grade with the board and carpenter's level after tamping. Compacted soil prevents the water fall filter box from future settling, which can put stress on the filter box plumbing. The subtle grade for the waterfall filter box helps the water to flow more freely from the filter to the pond.\nWrap the threads of a 2-inch PVC male adapter with plumber's PTFE tape. Hold the tape on the threads of the adapter with one hand while you wrap the tape clockwise three to four times with your other hand.\nScrew the 2-inch male adapter into the threaded port on the bottom of the waterfall filter box by hand. Tighten the adapter with slip-groove pliers. Do not overtighten the PVC male adapter; overtightening can cause the adapter to crack or break, causing water leaks.\nClean the end of the waterfall skimmer pump PVC flex pipe and inside of the PVC male adapter with PVC primer. PVC primer cleans and softens the PVC pipe and fittings for better glue adhesion and seal. Glue the waterfall filter skimmer pump PVC flex pipe into the PVC male adapter on the bottom of the filter box with PVC glue. Coat the end of the flex pipe and the inside of the male adapter hub with PVC glue. Push the end of the flex pipe into the male adapter and hold them together for about five to 10 minutes until the glue is set.\nPlace the waterfall filter box onto the prepared compacted surface with the flex pipe facing the rear of the area. Place the carpenter's level on top of the filter box to check for side-to-side level and front-to-rear grade.\nLay the protective underlayment in the pond, extending it up the sides and to the bottom of the waterfall filter box. Lay the pond liner material into the pond over the protective underlayment. Run the pond liner up to the waterfall filter box, leaving extra liner material to fold over the filter box spliner. The pond liner should be left slightly loose to fold over the spliner bar.\nFold the pond liner material over the spliner lock channel. Secure the pond liner material to the water fall filter box spline channel with the spliner bar and bar locks. Push the spliner bar over the pond liner into the spliner channel; clip the bar locks onto the spliner bar, securing the liner in place.\nTrim off the excess pond liner from the waterfall filter box with a utility knife. Place the waterfall skimmer pump into the pond at the opposite end of the waterfall filter box. Wrap the water fall skimmer pump flex-pipe around the outside edge of the pond, laying the skimmer pump housing flat on the pond liner.\nInstall the filter material into the filter box. Lay the filter mats flat in the bottom of the filter box on top of the filter supports.\nThings You Will Need\n- Shovel or excavating machine\n- Scrap board\n- Carpenter's level\n- Hand tamper\n- 2-inch PVC male adapter\n- PTFE tape\n- Slip-groove pliers\n- PVC primer\n- PVC glue\n- Protective underlayment\n- Pond liner material\n- Utility knife\n- Cover the flex pipe with the soil that was dug from the pond area to hide the pipe and prevent tripping hazards.\n- Place stones around the pond to help hold the liner in place and to conceal the waterfall filter box making the waterfall look more natural.\n- Jupiterimages/Photos.com/Getty Images""]"	['<urn:uuid:61134896-974a-453a-9ad1-710f75523851>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T05:28:56.360010	10	53	790
