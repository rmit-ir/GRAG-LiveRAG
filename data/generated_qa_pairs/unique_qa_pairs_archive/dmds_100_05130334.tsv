qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	dna modification process modern applications	DNA modification involves inserting desired genes through vectors to create recombinant DNA, combining genetic material from different organisms. This technology has diverse applications, from agriculture and animal science to medicine and industry. Modern applications include producing GMOs through recombinant DNA technology and reproductive cloning, with notable achievements like Dolly the sheep in 1996, and the development of whole-genome replacement in bacteria.	"['Learninsta presents the core concepts of Microbiology with high-quality research papers and topical review articles.\nMolecular Biology and Genetic Engineering\nMolecular biology – is the study of the structure, function & makeup of the molecular building blocks of life. It focuses on the interactions between the various system of a cell, including the interrelationship of DNA, RNA & Protein synthesis &how these interaction are regulated. Bioscience, Molecular biology closely interrelate with the fields of Biochemistry, Genetics & Cell biology.\nMolecular biology is a specialised branch of biochemistry, the study of the chemistry of molecules which are specifically connected to living processes. Importance to molecular biology are the nucleicacids (DNA and RNA) and the proteins which are constructed using the genetic instructions encoded in those molecules.\nOther biomolecules, such as carbohydrates and lipids may also be studied for the interactions they have with nucleic acids and proteins. Molecular biology is often separated from the field of cell biology, which concentrates on cellular structures (organelles and the like), molecular pathways within cells and cell life cycles.\nGenetic Engineering is the act of modifying the genetic makeup of an organism. Modification can be generated by methods such as gene therapy, nuclear transplantation, transfection of synthetic chromosome or viral insertion.\nThe manipulation of genetic make up of living cells by inserting desired genes through a DNA vector, is the genetic engineering. The gene is a small piece of DNA that encodes for a specific protein. The gene is inserted into a vector DNA so that a new combination of vector DNA is formed.\nThe DNA formed by joining DNA segments of two different organisms is called recombinant DNA. The organism whose genetic make up is manipulated using recombinant DNA technique, is called genetically manipulated organism (GMO). Genetic engineering has many application in agriculture, animal science, industry and medicines (Figure 1.3).\nGenetically Modified Organism (GMO)\nOrganism genome has been engineered in the laboratory in order to favour the expression of desired physiological traits or the production of desired biological products. In conventional livestock production, crop farming, and even pet breeding, it has long been the practice to breed select individuals of a species in order to produce offspring that have desirable traits.\nIn genetic modification, however, recombinant genetic technologies are employed to produce organisms whose genomes have been precisely altered at the molecular level, usually by the inclusion of genes from unrelated species of organisms that code for traits that would not be obtained easily through conventional selective breeding.\nGMOs are produced through using scientific methods that include recombinant DNA technology and reproductive cloning. In reproductive cloning, a nucleus is extracted from a cell of the individual to be cloned and is inserted into the enucleated cytoplasm of a host egg.\nThe process results in the generation of an offspring that is genetically identical to the donor individual. The first animal produced by means of this cloning technique with a nucleus from an adult donor cell (as opposed to a donor embryo) was a sheep named Dolly, born in 1996.\nSince then a number of other animals, including pigs, horses, and dogs, have been generated by reproductive cloning technology. Recombinant DNA technology, on the other hand, involves the insertion of one or more individual genes from an organism of one species into the DNA (deoxyribonucleic acid) of another.\nWhole-genome replacement, involving the transplantation of one bacterial genome into the “cell body,” or cytoplasm, of another microorganism, has been reported, although this technology is still limited to basic scientific applications.', 'The old fashioned method of modifying plants and animals by mating one member of a species with another to produce offspring with more desirable traits is no longer the standard paradigm. Genetic manipulation is the dominant method of acquiring such traits and, for many plants now, the rule rather than the exception.\nAlthough traditional cross-breeding has given us an enormous variety of foods, many of which our ancestors would barely recognize, the genie was let out of the bottle when we learned where that variety came from. Genetics revealed to us the units that determined traits and how to read the alphabet the units spelled out … and eventually how to swap those genes around to make them perform for us.\nThat alphabet known as the genome (the set of instructions for building the proteins that determine the characteristic traits of all plants and animals) consists of four “letters” strung together in endless combinations. Traditional cross-breeding allowed us to change the combinations of letters within the limits defined by the rules of mating. Very simply put, the organisms in question must be able to interbreed.\nWith the successful laboratory-based manipulation of genes independent of their hosts, those limits have been shattered -- for better or worse.\nCreating genetically modified organisms (GMOs) circumvents the slow and imprecise method of breeding. By transferring sections of DNA — genes — directly from one organism to another, geneticists select as narrowly as possible only the desired traits. Species restrictions don’t matter because the genetic subunits that determine physical traits are the same throughout nature. This frees scientists to select and import genetic traits from virtually any species using highly sophisticated techniques.\nOne result is the Atlantic salmon developed by AquaBounty Technologies. Mix in genes from a faster-growing Pacific Chinook salmon and some from an eel and, voila, you get a premium fish that grows to market size in 16-18 months rather than three years.\nThis results in problems both physical and political. Physically, adverse reactions, from allergies to full-blown anaphylactic shock, occur when a type of foreign molecule — usually a protein — triggers an adverse physical reaction. By creating new organisms, the risk is that one also is creating new antigens that can trigger previously unknown physical reactions.\nPolitically, along with the introduction of GMOs comes the inevitable baggage of proprietary processes and patented life forms.\nYes or no, GMO?\nScientific advance always brings growing pains, but GMOs have opened a Pandora’s Box of complex and controversial topics that includes economic, political, biological and even moral issues, none of which shows any sign of abating.\nThe potential benefits and challenges surrounding GMOs were summed up in November of last year in a review in the Journal of Food Science and Technology, “Genetically modified foods: safety, risks and public concerns.” Authors Amarinder Bawa and Kandangath Anilakumar wrote, “Technologies for genetically modifying foods offer dramatic promise for meeting areas of challenge for the 21st century. Like all new technologies they pose some risk, both known and unknown. Controversies in public concern surrounding GM foods and crops commonly focus on human and environmental safety, labeling and consumer choice, intellectual property rights, ethics, food security, poverty reduction and environmental conservation.”\nGMOs already have produced notable benefits. Insulin for diabetics was once derived and purified from cows and pigs killed for food. This process was expensive and carried with it the dangers of allergic reactions. Today, the human gene for insulin can be grown in GM bacteria that become factories for human insulin.\nOf course the bacteria are held in a controlled environment. Plants growing in a field present a different story, one of potential contamination of non-GMO and organic (presumed to be non-GMO) crops.\nMany in the public are mistrustful of GMOs, worried about the potential safety for humans and for the environment. Others look at food economics and politics and see a powerful process controlled and guided by only a few sources.\nThe USDA recently fielded comments on the coexistence of GMO and non-GMO crops from farmers across 17 states, primarily in the Midwest. Food & Water Watch, in partnership with the Organic Farmers’ Agency for Relationship Marketing (OFARM), released survey results that appeared to confirm what organic farmers have been contending for some time: The risks of GMO contamination have burdened organic and non-GMO farmers with extra work, longer hours and financial insecurity.\nJust how extensive is the public concern? A New York Times poll conducted in July 2013 estimated support for mandatory labeling of GM foods as high as 93 percent. However, two recent ballot initiatives to force mandatory labeling of GMO foods, one held in California in November 2012 and one in Washington state the following November, were narrowly defeated. Similar referenda are being discussed in a number of other states.\nAccording to a recent study on GMO awareness and concern among consumers released by NPD Group, about half of U.S. consumers “express some level of concern” about GMOs, although many are uncertain how to describe them. One-third of primary grocery shoppers are willing to pay more for non-GMO foods — a figure that rises to half of consumers who shop at specialty stores.\nThe public right to know the origin of food seems like a simple and obvious proposition. However, labeling GMOs has become complex. In March 2013, Austin, Texas-based Whole Foods Market, announced that by 2018 all products in its U.S. and Canadian stores must be labeled to indicate whether they contain GMOs. Whole Foods Market is a strong supporter of government-mandated labeling of GMO ingredients which would “enable shoppers, retailers and manufacturers to make purchasing decisions that reflect their beliefs.""\nAlso responding to customer demands, Aldi GmbH’s Trader Joe’s chain, Monrovia, Calif., declared that products under the Trader Joe label are sourced from non-GMO ingredients. However, the company stops short of requiring a non-GMO label for all products it sells. The company explains in communications this is “because there are no clear guidelines from the U.S. governmental agencies covering food and beverage labeling.”\nSensing the public mood, many manufacturers are not waiting for guidelines. General Mills Inc., Minneapolis, reformulated original formula Cheerios with non-GMO ingredients. Oats and wheat have always been non-GMO ingredients, so the reformulation required only finding non-GMO sources of sugar and starch. The company does not have plans to follow with other non-GMO products at this time.\nPost Foods LLC, St. Louis, announced its iconic Grape Nuts are now sourced with non-GMO ingredients. But it is also noted the new formulas for both cereals are lower in certain vitamins — riboflavin for Cheerios and riboflavin, vitamin A and vitamin D for Grape Nuts.\n“Our transition to non-GMO buttery spreads has been in process for several months,” says Stephen Hughes, CEO for Boulder Brands, Paramus, N.J., which makes Glutino, Earth Balance, Level, Evol, Smart Balance and Udi’s Gluten Free. “Consumers are expressing a very strong desire for more transparency about what’s in the foods they’re eating and serving their families. We’re taking this important step to meet that need. We know that, just as important as what is in those foods, is what you take out.\n“Making the necessary changes to our ingredient supply chain and manufacturing processes requires a several months-long transition,"" he continues. ""We plan to transition our entire Smart Balance product portfolio to non-GMO, but this is the start of a journey for us. Our peanut butter is already non-GMO. We know it will take some time, but our goal is to work closely with industry leading partners to identify the required resources and assess the conversion of our entire Smart Balance product line to non-GMO.”\nHughes notes that the Earth Balance line always has been made with non-GMO ingredients. Both Udi’s Granola Clusters and Granola Bars along with 15 Glutino products have been verified by the Non-GMO Project.\nThe Non-GMO Project\n“Because we want to be as transparent and accurate as possible for consumers, our seal is not a GMO-free claim, as that is scientifically indefensible,” says Isabel VanDerslice, outreach coordinator for the Non-GMO Project, Bellingham, Wash. The Non-GMO Project is “North America’s only independent verification for products made according to best practices for GMO avoidance.” Its “Non-GMO Project Verified” label indicates the product has met the organization’s standard for GMO avoidance.\nThat Non-GMO Project Verified standard is consistent with laws in the EU, where any product containing more than 0.9 percent GMO must be labeled. “More than 14,000 products are Non-GMO Project Verified, and about 500 more are added every month,” adds VanDerslice. “The length of the process can vary based on a number of factors, such as number of facilities or complexity of the product. An average product can take 3-6 months to complete the process from start to finish.”\nAnother supporter of the Non-GMO Project is WhiteWave Foods Inc. Broomfield, Colo. The maker of dairy substitute beverages and organic dairy products began as a small tofu company. WhiteWave uses only non-GMO ingredients for all its products. This is somewhat difficult sourcing since the vast majority of soybeans are genetically modified.\nBut many ingredient providers are recognizing the need and so pushing forward with non-GMO offerings. For example, Briess Malt & Ingredients Inc., Chilton, Wis., offers a portfolio of non-GMO natural sweeteners and specialty grain ingredients, all derived from North American sources. Starch giant Ingredion Inc., Westchester, Ill., notes in its corporate communications that it and its affiliated companies “produce non-GMO and other identity-preserved products derived from corn, oats, stevia, and tapioca/yucca/manioc roots.”\nTIC Gums Inc., White Marsh, Md., is expanding its line of non-GMO ingredients. The company promotes ingredients classed as “Non-GMO” or “NGMO” if they meet the 0.9 percent criteria. Suppliers of raw ingredients to TIC Gums must certify that their ingredients also meet the 0.9 percent rule ""or there are no known commercial sources of the products’ raw ingredient constituents derived from genetically modified organisms.” Examples include gum acacia and guar gum.\nSome companies are taking the non-GMO commitment beyond the ingredients of their products. Oakland, Calif.-based Numi Tea Co.’s commitment to non-GMO verification is promoted as “complete.” Numi claims it is the first company to have its tea bags — not merely the tea ingredients — verified by the Non-GMO Project. Tea bags are submerged in hot water before consumption, so, the composition of the bags themselves is important to Numi. The bags are compostable and made from Manila hemp cellulose. They are oxygen whitened and even the tag is made from 100 percent recycled material and soy-based inks.\nIn January of this year, Whole Earth Sweetener Co., Chicago, announced the launch of a new PureVia stevia sweetener produced with non-GMO ingredients. It’s billed as “the first global brand of all-natural sweetener to promote this choice in the U.S.” The reformulation affects both regular PureVia and the PureVia Turbinado Raw Cane Sugar & Stevia Blend. The non-GMO dextrose used is derived from cassava root rather than from corn.\nAt present, opinions regarding genetically modified organisms seem entrenched. Whether or not the government comes down with a ruling on labeling in the near future, a form of labeling already is happening, in the form of the Non-GMO Project. That means, for the foreseeable future, there will be an expanding market for non-GMO foods.']"	['<urn:uuid:ac7cb948-a9e3-4ae9-a727-4f2d3cfaa208>', '<urn:uuid:e3dd4e4e-bde1-43ea-b7e4-b43598471a33>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T03:34:55.284799	5	61	2458
2	What happens if someone defaults on a hypothecation agreement?	When a borrower defaults, the lender can exercise the lien by foreclosing on the property and sell it to recoup losses. If any sale proceeds remain, they go to the former owner.	['Real estate investors look for ways to earn a competitive return on investment while exposing themselves to minimal risk. One way to reduce investor or lender risk is through a hypothecation agreement. In this article, we’ll answer, “What is a hypothecation agreement?”\nThen we show you an example hypothecation agreement form. We also discuss what you must know about hypothecation in real estate and elsewhere. Finally, we’ll discuss rehypothecation and answer a few frequently asked questions.\nWhat Is a Hypothecation Agreement?\nTo answer “What is a hypothecation agreement?”, let’s first define hypothecation. It’s the pledging of collateral to secure a loan without relinquishing collateral ownership rights, possession, or title. A hypothecation agreement or hypothecation letter specifies the terms of the hypothecation arrangement.\nImportantly, it states the legal recourse available to lenders in case of debtor default. Usually, recourse is through a lien on the collateral property. Indeed, if the debtor defaults on the loan, the lender can seize the collateral and sell it to make itself whole.\nHere you can find a sample hypothecation agreement form from the SEC archives.\nWhat Should I Know About Hypothecation?\nThere are many aspects of hypothecation, which we’ll explore now.\nHypothecation in Real Estate\nUsually, hypothecation in real estate appears in a transaction like a mortgage on commercial or residential property. That is, a borrower pledges an asset as collateral to secure a real estate loan.\nIn doing so, the borrower does not give up ownership rights, title, or possession of the property. Nor does the borrower forgo any income that the property generates. On the contrary, the lender has no claim on the property’s cash flows.\nGenerally, a lender uses a hypothecation agreement when the owner of the collateral is not the obligor on the secured obligation. For instance, suppose Tom pledges his home as collateral for his fiancée Mary’s construction loan on her home.\nTom is the owner of the collateral (his home) but isn’t the obligor on the secured obligation (Mary’s home). Hence, the hypothecation agreement specifies that Tom’s home, but not Tom, secures Mary’s construction loan.\nA hypothecation agreement may specify that a tenant cannot hypothecate its interest in a lease or premises without landlord consent. The example below shows this kind of hypothecation agreement.\nThe situation changes if the borrower defaults on the loan. That’s because the borrower provides a lien to the lender as part of the loan agreement. When a borrower defaults, the lender can exercise the lien by foreclosing on the property.\nThen it can sell the property to recoup any losses. If any sale proceeds remain, they go to the former owner of the property.\nA second mortgage complicates the picture. If a lender forecloses and takes possession of the property, it becomes responsible for paying the second mortgage until it sells the property.\nTypically, the first and second lien holders work out an agreement on how to handle this unfortunate occurrence.\nDifferences Between Mortgages and Hypothecation Agreements\nAlthough similar, a mortgage deed and a hypothecation agreement are not the same:\n- Title: In a mortgage, the title of the property passes from owner to lender as collateral for the loan. However, in a hypothecation agreement, title and possession remain with the borrower unless default occurs.\n- Indications: A mortgage indicates that the borrower transfers its interest in the asset to the lender. Instead, a hypothecation agreement indicates that the borrower pledges the property as loan security.\n- Tenure: Mortgage deeds are long-lived. Hypothecation agreements can be long-lived or short-lived, depending on the context.\nInvestment hypothecation occurs when a trader or investor pledges collateral for a margin loan to purchase or short securities. Specifically, broker/dealers (BDs) offer margin accounts that allow traders to borrow up to 50% of the securities’ value. The margin account agreement contains a hypothecation agreement for the collateral.\nTypically, the hypothecation agreement specifies important items:\n- Form of Collateral: Cash collateral is possible, however, in most cases, the trader’s position serves as collateral for the loan. That is, the trader hypothecates the securities in the position to guarantee the margin loan.\n- Credit Payments: Naturally, margin loans charge interest that the trader has to periodically pay to the BD. Failure to meet these credit payments can cause the BD to sell collateral and call for its replacement. Alternatively, the BD can sell some of the trader’s position to collect its margin interest.\n- Margin Call: In a margin call, the trader’s position loses value so that the margin loan covers more than 50% of the position. Accordingly, the BD can demand more collateral in the form of additional prepaid securities or cash to restore the balance. If the trader fails to meet the call in a short period, the BD can liquidate the position.\n- Rehypothecation: The hypothecation agreement states whether or not to allow rehypothecation (see below) and if so, how much.\nThe definition of rehypothecation is when a BD reuses a trader’s pledged collateral as collateral for the BD’s own trades and borrowings. This provides the creditor with leverage since the creditor doesn’t have to tie up its own assets. In the U.S., laws limit the amount of rehypothecation to no more than 140% of the original debit balance.\nInterestingly, the creditor doesn’t carry the non-cash collateral available from rehypothecation on its balance sheet. A trader can indicate that it doesn’t want the BD to rehypothecate the trader’s collateral. The BD must then decide whether to grant a margin account to the trader.\nIn some cases, the trader receives compensation for allowing the BD to rehypothecate the trader’s collateral. That compensation can take the form of lower interest rate on margin loans or a rebate of fees.\nHypothecation, Rehypothecation, and Churning\nWhat Should I Know About Hypothecation?\nRisks of Investment Hypothecation Agreements\nWhen a Broker/Dealer grants a margin account to a trader, it must be able to handle three important risks. Normally, standard reports reveal these risky practices and prevent them from occurring.\n- Sometimes, traders use prepaid securities, such as Treasury bills, to collateralize a margin account. The risk is that the trader may sell this collateral even as the position remains in effect.\n- Traders may “double-dip” on financing margin transactions by hypothecating the same collateral multiple times.\n- If the BD must execute a margin call, it might be costly for the BD to realize the collateral’s full value.\nRepos and Reverse Repos\nRepurchase agreements, or repos, allow a party to sell securities to a second party and buy it back later. The first party pays less than the sale proceeds to buy back the security. The buyback discount is the seller’s source of profit on the repo agreement. Thus, repo agreements are actually loans in which the sold securities act as rehypothecated collateral.\nBig banks and hedge funds use rehypothecated securities from their customers to undertake repo transactions. Most repos have an overnight term in which the buyback occurs the day after the sale. However, participants also use longer-term and open-ended repos.\nA reverse repo is a repo transaction from the point of view of the borrower/buyer rather than the lender/seller.\nHypothecation Agreement Forms\nThe following language is for a real estate hypothecation agreement form and comes from Law Insider:\nHypothecation. Tenant shall not hypothecate, mortgage, or encumber Tenant’s interest in this Lease or in the Premises or otherwise use this Lease as a security device in any manner without the consent of Landlord, which consent Landlord may withhold in its sole and absolute discretion. Consent by Landlord to any such hypothecation or creation of a lien or mortgage shall not constitute consent to an assignment or other transfer of this Lease following foreclosure of any permitted lien or mortgage.\nThere are other types of hypothecation agreements, such as those for investments and repos. We leave it to the curious reader to ferret those out via appropriate Internet searches and their legal counsel.\nIf you’re interested, you can read this real-world example of a hypothecation agreement.\nFrequently Asked Questions\nWhat is hypothecation in commercial real estate?\nHypothecation in commercial real estate is the posting of collateral to secure a loan. Typically, a rental building operates as its own collateral. However, construction loans require other collateral since the borrower hasn’t built the underlying property yet.\nWhy is a mortgage like a hypothecation contract?\nThey aren’t really the same. In a mortgage, the borrower hold’s the property’s title until the borrower repays the loan. In a hypothecation agreement, the borrower keeps title to the property.\nWhat’s the difference between hypothecation and pledges?\nIn a pledge, you intend to transfer the asset to another owner. In hypothecation, your intent is to collateralize the asset to guarantee a loan. Importantly, you plan to maintain title to the hypothecated asset after you repay the loan.\nWhat is a hypothecation letter?\nHypothecation letter is another name for a hypothecation agreement. Sometimes, we call a hypothecation agreement a hypothecation deed. These are all synonyms for the same document that specifies the terms of a hypothecation arrangement.\nWhen is a hypothecation agreement used?\nBroker/dealers routinely use hypothecation agreements when setting up margin accounts. In real estate, a landlord uses a hypothecation agreement to prevent subleasing. Also, lenders use hypothecation in real estate when a different property secures a mortgage or building loan.']	['<urn:uuid:b9964ca6-6fe2-4a61-8f65-c07369c08577>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T03:34:55.284799	9	32	1549
3	compare power levels broadcast strength asheville fm wwvb radio stations transmitters	While Asheville FM is a low power FM community radio station broadcasting in Asheville, NC, WWVB is a much more powerful station that has significantly increased its broadcast strength over time. WWVB started with 5 kW power in 1963, increased to 13 kW, then to 25 kW in 1997, and finally reached approximately 70 kW in its current configuration using two modern transmitters broadcasting simultaneously on 60 kHz. In contrast, Asheville FM operates as a low power FM frequency station broadcasting at 103.3 FM from downtown Asheville.	"[""What is Asheville FM?\nAsheville FM is a volunteer-based, grassroots community radio station brodcasting at 103.3 on the FM dial in Asheville, NC. Created by Friends of Community Radio, a 501(c)3 non-profit entity. The idea for Asheville FM was hatched in early 2009. Radio lovers from Western North Carolina came together to serve pipin’ hot sounds and have fun while doing it.\nOur goal is to add to and reflect the rich stew of arts, culture and community involvement that is Asheville. We want to hear music, news, and the unusual all produced right here in our neck of the woods. We want to hear sounds from around the world, discerned and distilled just for us by our neighbors. We want to help make connections between diverse groups and support the local economy of ideas. We believe people from all parts of our community should have a chance to let their voice be heard.\nAsheville FM began broadcasting as an internet radio station in September of 2009. In late 2013 we filed for a terrestrial Low Power FM frequency with the Federal Communications Commission, and in March 2014, our application was granted! AshevilleFM is now broadcasting at 103.3 FM from downtown Asheville. Sound Good? It’ll sound even better if you get involved and help us bring this vital local programming to our community!\nAFM Station Management & Friends of Community Radio Board\nStation Manager & DJ - scr...scr...scr...scratching!\nScratching vinyl on turntables and digging in the crates makes me happy. Music is one of the best things in the universe. Peas.\n#Vinyltimetravelers #TurntableTuesday #BEATLIFE #2.5 @DJKUTZU\nPresident, Friends of Community Radio\nWhen Kama realized her kids didn't need quite so much raising anymore, she looked into raising other things. Hell? Farm Animals? No... How about a radio station?-- It's a little bit of both! She has a BA in communication and in her spare time enjoys domestic policy, eating, and traveling to exotic thrift stores for old records.\nVice President, Friends of Community Radio\nBefore returning to his native Asheville, Greg was a DJ at WPRB (Princeton, NJ) and the Music Director at WXDU (Durham, NC). He's also lived in Berlin and Zurich, where he suffered withdrawal from freeform radio. Greg is a historian of early modern Europe, an editor, and a teacher of professional writing. He also currently teaches Humanities at UNC-Asheville.\nTreasurer, Friends of Community Radio\nAs co-owner of Harvest Records in West Asheville for over a decade, Mark has ingrained himself in our community not just through this retail outlet, but through concert production & promotion, art shows, live music, tour managing and artist management. He was formerly a Programming Director at WXJM 88.7 (Harrisonburg, VA) and was elected to the Asheville FM Board in October...\nFriends of Community Radio\nBorn and raised in South Carolina, Kimberly moved to Asheville with her husband Nathanael in the fall of 2006. She has a BA in Music, with a concentration in Music Industry, from James Madison University. Kimberly works at The Mothlight, teaches private piano lessons in her home with some of the coolest kids on the planet, and volunteers at AshevilleFM. Having had the pleasure...\nJoining the AFM team as a voice on the newshour I thought it might be fun to do a little music and talk in the morning. I used to play music on a friday afternoon radio show, far away and long ago that is......\n1988-1998: Microsoft Corporation, Redmond, Washington. Technical Writer for Microsoft Word, User Assistance Department Manager for Microsoft Excel, Manager of technical writers and editors for Microsoft Office manuals and on-screen Help content.\nRetired from Microsoft, August 1998; moved to rural south central Colorado. Served as a volunteer for a number of non-profits..."", 'History of WWVB\nWWVB began operation as radio station KK2XEI in July 1956. This experimental station was operated from 1530 to 2000 hours universal time each working day from Boulder, Colorado. The continuous wave 60 kHz signal was not modulated, except for a call sign ID that was sent every 20 minutes. The effective radiated power (ERP) was originally said to be 40 W, but later reduced to 1.4 W. Data recorded in January 1957 showed that the frequency of the broadcast was within a few parts in 1010 of the national standard located in the adjacent Boulder laboratory, proving (as expected) that a LF transmission was far more stable than the signals from WWV and WWVH.\nThe success of the 60 kHz broadcast led to the construction of a very low frequency (VLF) radio station named WWVL, which began operation from Sunset, Colorado in April 1960 using a carrier frequency of 20 kHz. It was originally planned to use the 20 kHz for worldwide coverage, and the 60 kHz broadcast for coverage of the United States. In March 1960, the call sign WWVB was obtained by NBS for the 60 kHz station. The “B” in the call sign probably stands for Boulder, the site of the original transmitter. However, one interesting theory is that the “B” could stand for Brown. W. W. Brown, one of the designers of the Fort Collins station, was employed as a contractor by NBS when the call sign application was submitted. Perhaps not coincidentally, his initials were W. W. B.\nIn 1962, NBS began building a new facility on a site north of Fort Collins, Colorado that would later also become the home of WWV. The site was attractive for several reasons, one being its exceptionally high ground conductivity, which was due to the high alkalinity of the soil. It was also reasonably close to Boulder (about 80 km, 49.3 mi), which made it easy to staff and manage, but much farther away from the mountains. The increased distance from the mountains made it a better choice for broadcasting an omni-directional signal.\nWWVB went on the air on July 5, 1963, broadcasting a 5 kW signal on 60 kHz. This was later increased to 7 kW and then 13 kW, where it remained until December 1997. WWVL began transmitting a 500 W signal (later increased to 2 kW) on 20 kHz the following month. WWVL had a relatively short life span, going off the air in July 1972, but WWVB went on to become a permanent part of the nation’s infrastructure.\nA time code was added to WWVB on July 1, 1965. This made it possible for radio clocks to be designed that could decode the signal, recover the time, and automatically set themselves. The time code format has changed only slightly since 1965; it uses a scheme known as binary coded decimal (BCD), which uses four binary digits (bits) to send one decimal number.\nThe WWVB broadcast continued operations up to the 1990’s with only minor modification to the format or equipment. The number of customers was relatively small, mostly calibration laboratories who operated WWVB disciplined oscillators, devices that utilized the 60 kHz carrier as a frequency reference. Also, the limitations of the aging transmitting equipment at WWVB became increasingly apparent as the years passed. The situation came to a head on February 7, 1994 when a heavy mist froze to the antenna, and the antenna tuning system could not compensate, shutting down the WWVB broadcasts for about 30 hours. After reviewing the available options, it was decided that a redesign of the entire WWVB transmitting system was necessary.\nDuring the discussions about redesigning WWVB, it was decided to substantially raise the power level of the broadcasts. It was obvious that WWVB could play a much larger role and reach far more customers if the signal were easier to receive. In Europe, low cost radio controlled clocks were beginning to appear, designed to synchronize to stations such as MSF in the United Kingdom and DCF77 in Germany. These stations were very similar to WWVB, but had a much smaller coverage area to service. As a result, European customers were able to purchase radio controlled alarm clocks, wall clocks, and wristwatches at reasonable prices. These products lacked the external antennas and high sensitivity of the laboratory receivers, but would undoubtedly work well in the United States if the WWVB signals were made stronger.\nExpert consultants and engineers from the U.S. Navy’s LF/VLF support group were hired by NIST beginning in October 1994 to evaluate the WWVB system and propose changes. Their reports suggested that although the antennas themselves were in reasonably good shape, the transmitters and matching equipment should be completely redesigned and new or upgraded equipment installed. The project progressed in phases over the next several years, as funding and equipment became available. Discussions between agencies at the highest levels resulted in the transfer of modern LF transmitters and other equipment from recently decommissioned Navy facilities to NIST. New station staff members were hired who had previous experience with the new systems and equipment. Contractors normally employed by the Navy for LF work were hired to design a new broadcast control system fully utilizing the assets of the existing station.\nA formal announcement that the WWVB power was to be increased was made during 1996, and a significant number of low cost radio controlled clock products were introduced in the United States shortly after the announcement. By December 1997, an interim stage of the upgrade was completed and the ERP was increased to about 25 kW. By August 5, 1999, the upgrade was complete. The new WWVB configuration used two modern transmitters operating into two antennas that simultaneously broadcast the same 60 kHz signal. This increased the ERP to 50 kW, about four times more power than the pre-upgrade configuration . This power level was later increased to its current level of about 70 kW. The increase in power greatly increased the coverage area, and low cost radio controlled clocks that synchronized to WWVB soon became commonplace throughout the United States.\nQuestions? Send mail to: Michael Lombardi']"	['<urn:uuid:1fd006ce-36c1-412f-92d3-d923269b17c8>', '<urn:uuid:474540c9-6966-4567-bc03-137dcd51b429>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	expert	2025-05-13T03:34:55.284799	11	87	1646
4	How does Jupiter's magnetic field compare to Earth's in terms of strength and auroral behavior, and what makes its polar light displays so distinctive?	Jupiter's magnetic field is much stronger than Earth's - the field at Jupiter's cloud surface is almost ten times stronger than Earth's magnetic field. This intense field is believed to form from dynamic movements of metallic hydrogen within Jupiter. While both planets display auroral phenomena, Jupiter's auroras behave very differently. Unlike Earth, where northern and southern auroras mirror each other due to similar magnetic fields, Jupiter's polar lights act independently. X-ray observations show that Jupiter's south pole produces regular pulses every 11 minutes, while the north pole's emissions are erratic. This asymmetry may be due to Jupiter's tilted magnetic field allowing better visibility of the northern aurora, potentially revealing regions where the magnetic field connects to multiple locations with different travel times.	['Magnetism is an entirely separate force despite similarities and it depends more on particular properties rather than simply mass, such as electrons and can both push and pull. Magnetism is present throughout the universe and we can experience it in many ways; when I am out hiking, my compass explains the pressure of magnetism and direction with the movement of the needle as it is attracted by the force.\nThere are a number of properties and varieties of magnetic forces that explain invisible fields that applies a force that influence objects or material from the magnetism. There are rules that confirm magnetic fields are dipolar and just like earth has both a north and south magnetic pole and the ‘magnetic flux’ explains how the force and attraction between the poles – usually represented by lines as visible in the image below – that can be averaged by the magnetic field and the perpendicular area the field infiltrates. Measurements of the force is determined by the mathematical formula F= qvB (Lorentz Force Law), which is the magnetic force, the charge, the velocity and the magnetic field and the unit of these field are measured in terms of Standard International (SI) units known as tesla.\nEarth’s magnetic field is known as a geomagnetic field and magnetosphere the predominate reason for the magnetic field is the liquid iron core surrounding the solid inner core is the source of this phenomenon, the very ‘magnet’ where the electric currents produced by the flow of iron and other metals including nickel cause convection currents from the inertial force of the Coriolis Effect that ultimately splits the field into a surrounding force that envelops Earth and aligns back into the same direction. The changes in temperature and composition of the liquid core creating the currents that rise or sink matter all play a part in Earths magnetic field, that can be captured visually when solar winds collide with it (usually where the magnetic force is much stronger near the north and south poles) and the charged particles trapped by the magnetic field produce the aurora borealis or the aurora australis.\nThe picture explains the rotational poles but that their alignment geographically differs from our north and south poles on earth, whereby the magnetic south poles resides further north of Antarctic’ South Pole and quite close to the south of Australia while the north magnetic pole is closer to northern Canada and thus south of the North Pole. The magnetic lines explain the streamlined flow of the magnetic field that makes it easier to ascertain the process mathematically. Jupiter has a number of powerful toroidal magnetic fields where the intensity is said to have formed from the dynamic movements of the metallic hydrogen within; the field on the surface of the clouds is almost ten times stronger than earth’s. The Milky Way also has magnetic fields as do galaxies and the universe contains some colossal magnetic fields, where observations of galaxy clusters have found magnetic fields extending millions of light years!\nThe magnetic force is the attraction or the repulsion (as you experience when attempting to connect two magnets with equal poles) occurs from the magnetic field. The properties of electrical fields (pole) with a positive and negative charge differ with that of magnetic fields (dipole) despite a close correlation, because electromagnetism involves a magnetic dipole producing an electric field as it moves and conversely an electric field can produce a magnetic field meaning the difference is an elementary change in the field. A magnet does not have an electric charge as two separate poles, while a dipole interacts as a charge as visualised in the following image. What this means is that the electrical force itself behaves on a charged particle in the direction of the field and does not need motion while a magnetic force requires this motion and acts perpendicular to the magnetic field.\nGravitational fields also acts as force fields for mass and the gravitational force itself depends on the mass and the mass experiences the gravitational force. The gravitational field has a place in every direction and point in space and known by the formula g = F/m where F is the force of gravity.\nMaurizio Gasperini, Theory of Gravitational Interactions, Springer (2016) 115\nStephen Blundell, Magnetism: A Very Short Introduction, Oxford (2012) 106\nAnupam Garg, Classical Electromagnetism in a Nutshell, Princeton University Press (2012) 83', 'New X-ray observations show the auroras on Jupiter behave differently at each pole, making Jupiter puzzling and unlike Saturn (no known auroras) or Earth (where north and south pole auroras mirror one another). Scientists hope to combine Chandra, XMM-Newton, and Juno data to learn more about the source of Jupiter’s auroras.\nJupiter’s intense northern and southern lights, or auroras, behave independently of each other according to a new study using NASA’s Chandra X-ray and ESA’s XMM-Newton observatories.\nUsing XMM-Newton and Chandra X-ray observations from March 2007 and May and June 2016, a team of researchers produced maps of Jupiter’s X-ray emissions (shown in inset) and identified an X-ray hot spot at each pole. Each hot spot can cover an area equal to about half the surface of the Earth.\nThe team found that the hot spots had very different characteristics. The X-ray emission at Jupiter’s south pole consistently pulsed every 11 minutes, but the X-rays seen from the north pole were erratic, increasing and decreasing in brightness — seemingly independent of the emission from the south pole.\nThis makes Jupiter particularly puzzling. X-ray auroras have never been detected from our Solar System’s other gas giants, including Saturn. Jupiter is also unlike Earth, where the auroras on our planet’s north and south poles generally mirror each other because the magnetic fields are similar.\nTo understand how Jupiter produces its X-ray auroras, the team of researchers plans to combine new and upcoming X-ray data from Chandra and XMM-Newton with information from NASA’s Juno mission, which is currently in orbit around the planet. If scientists can connect the X-ray activity with physical changes observed simultaneously with Juno, they may be able to determine the process that generates the Jovian auroras and by association X-ray auroras at other planets.\nOne theory that the X-ray and Juno observations may help to prove or disprove is that Jupiter’s X-ray auroras are caused by interactions at the boundary between Jupiter’s magnetic field, which is generated by electrical currents in the planet’s interior, and the solar wind, a high-speed flow of particles streaming from the Sun. The interactions between the solar wind and Jupiter’s magnetic field can cause the latter to vibrate and produce magnetic waves. Charged particles can surf these waves and gain energy. Collisions of these particles with Jupiter’s atmosphere produce the bright flashes of X-rays observed by Chandra and XMM. Within this theory the 11-minute interval would represent the time for a wave to travel along one of Jupiter’s magnetic field lines.\nThe difference in behavior between the Jovian north and south poles may be caused by the difference in visibility of the two poles. Because the magnetic field of Jupiter is tilted, we are able to see much more of the northern aurora than the southern aurora. Therefore for the north pole we may be able to observe regions where the magnetic field connects to more than one location, with several different travel times, while for the south pole we can only observe regions where the magnetic field connects to one location. This would cause the behavior of the north pole to appear erratic compared to the south pole.\nA larger question is how does Jupiter give the particles in its magnetosphere (the realm controlled by Jupiter’s magnetic field) the huge energies needed to make X-rays? Some of the X-ray emission observed with Chandra can only be produced if Jupiter accelerates oxygen ions to such high energies that when they violently collide with the atmosphere all eight of their electrons are torn off. Scientists hope to determine what impact these particles, which crash into the planet’s poles at thousands of kilometers per second, have on the planet itself. Do these high-energy particles affect the Jovian weather and the chemical composition of its atmosphere? Can they explain the anomalously high temperatures found in certain places in Jupiter’s atmosphere? These are the questions that Chandra, XMM-Newton, and Juno may be able to help answer in the future.\nA paper describing these results appeared in the October 30th issue of Nature Astronomy, led by William Dunn of the University College London. NASA’s Marshall Space Flight Center in Huntsville, Alabama, manages the Chandra program for NASA’s Science Mission Directorate in Washington. The Smithsonian Astrophysical Observatory in Cambridge, Massachusetts, controls Chandra’s science and flight operations.\nPublication: W.R. Dunn, et al., “The independent pulsations of Jupiter’s northern and southern X-ray auroras,” Nature Astronomy 1, 758–764 (2017); doi:10.1038/s41550-017-0262-6\nImage credit: X-ray: NASA/CXC/UCL/W.Dunn et al, Optical: South Pole:Credits: NASA/JPL-Caltech/SwRI/MSSS/Gerald Eichstädt /Seán Doran North Pole Credit:NASA/JPL-Caltech/SwRI/MSSS']	['<urn:uuid:351e8e55-a6a4-4471-98f4-62230ee4963e>', '<urn:uuid:16fbfd44-5551-4caa-b9d2-69993629b3ef>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T03:34:55.284799	24	122	1486
5	Do collision avoidance and flight service specialists have similar roles?	Collision avoidance and flight service specialists have distinct safety roles. Collision avoidance involves both ATC tools/procedures for controllers and crew tools like TCAS and see-and-avoid capabilities. In contrast, flight service specialists focus on providing pilots with pre-flight and in-flight weather information, suggested routes, relay clearances, assist in emergencies, and coordinate searches for missing aircraft - they do not actively manage or separate air traffic.	"['Generally, two types of safety barriers exist – prevention barriers and mitigation barriers. The former are used to minimize the risk of an occurrence happening and the latter are intended to eliminate or reduce the impact of an event after it has happened.\nThere are no specific prevention barriers to address transponder failure since this is usually an equipment failure. This means that the general maintenance rules and procedures apply (as with any other piece of equipment). Therefore, this article focuses on the mitigation barriers, which are divided into “repairable” and “other” categories based on the barrier’s ability to be restored (after being breached), e.g. by a procedure. Furthermore, the mitigation barriers (as well as all safety barriers in general) are divided into several stages each of which is intended to activate if the previous have failed:\nDesign and strategic planning – includes strategic level barriers, e.g. airspace design;\nDemand and capacity balancing – focuses on ensuring sustainable workload levels;\nTraffic planning and synchronisation – includes long and medium term tools and procedures for conflict detection and resolution (e.g. MTCD);\nTactical conflict management – includes various tools and procedures used for ensuring separation by timely conflict detection and resolution;\nATC collision avoidance – includes tools and procedures for collision avoidance used by air traffic controllers (e.g. STCA);\nCrew collision avoidance – includes tools and procedures used by the pilots for collision avoidance (e.g. TCAS, see and avoid, etc.).\nRepairable mitigation barriers\nA mitigation barrier is called “repairable” when a failure has reduced the effectiveness of a barrier in the system, but certain actions may be able to restore its effectiveness. The main repairable mitigation barrier at each stage are:\nDesign and strategic planning\nApplication of transponder validation procedures on first contact - On first radar contact with an aircraft, the ATCO should validate the transponder function, including e.g. operation, Mode A code and Mode C operation. This could include on start-up or departure. The thoroughness of completion of this procedure could be improved for certain sectors or environments.\nTraffic planning and synchronization;\nMore effective flight plan data - This is the improvement of controller prediction tools to give more accurate performance when using only flight plan data (even if designed for “dynamic” updates using track data), for example when manually updated by the ATCO. The ATCO also requires clear procedures and training for manually inputting and updating flight plan data for the most effective use during a loss of track scenario. This also reflects the general mitigation of appropriate use of flight plan data in the event of a loss of track for both ATCO tools and the controller.\nTactical conflict management\nRegular scanning by ATCO - The controller should maintain an effective regular scan (e.g. to be able to detect non-alerted dropped tracks), rather than solely rely on “first contact” procedures. This also applies to detection of incorrect aircraft being given a clearance (due invalid correlation). There is some debate as to the effectiveness of this barrier, since in en-route controlled airspace, strip management is traditionally the primary means of deconfliction.\nUse of primary radar data If available, this can be used to maintain a correlated track to support tactical conflict management in the event of a loss of secondary surveillance information. Note that this may be assisted by cooperation with the military, allowing the sharing of primary radar data.\nCrew detection of transponder failure. Existing alerts are incorporated on most commercial aircraft, but may not be immediately noticeable in flight (e.g. Embraer Legacy-B737 accident in Brazil). Fail-safe indications of transponder failures or malfunctions, if detected, should be given to the flight crew.\nOther mitigation barriers\nThe other existing (or possible new) mitigation barrier at each stage are:\nDesign and strategic planning\nAirspace design gives positive separation – This includes the systematic separation of aircraft using de-conflicted RNAV/RNP based routes. Free-route airspace may reduce the effectiveness of this barrier.\nProcedure design for transponder malfunction - Procedures can be defined and implemented for transponder loss. If primary radar is available, flight plan correlation should be maintained. If not (e.g. in the subsequent sector), procedures may vary, and may include military escort or in extremis refusing the aircraft entry or returning the aircraft to an airfield. Procedures may also include assistance of the supervisor or planner. The aircraft should also be cleared out of RVSM airspace.\nAppropriate ATC system design and calibration – In the case of total loss of a transponder, design and calibration of an effective tool for alerting ATCOs in the event of: a dropped track (across one or more sectors); or a non-correlated track (i.e. without flight plan data); or a track without secondary surveillance information (i.e. primary only, but still correlated).\nDemand and capacity balancing\nSector capacity planning - Ensuring that the number of aircraft the controller can handle if a track is lost is appropriate i.e. ensuring that sector capacity limits are appropriate by “sensitivity analysis” of track drop scenarios.\nTraffic planning and synchronisation\nUse of voice reporting - Use of voice reporting is particularly relevant as a barrier during sector handover, when defined procedures may be followed. If silent handover is used, this barrier may not be applicable. If the ATCO has detected the track drop/loss, it may also be used within a sector for improved situational awareness.\nTactical conflict management\nAlert for change in track status - Any change of track status should be alerted to the controller. This includes the loss of transponder information (i.e. primary only, or flight plan track), or the total loss of a track. Alerting improves the detectability. This is also applicable for multiple sectors, i.e. also alerting the next sector the aircraft is due to enter, and may be used at the planning stage.\nUse of voice reporting remains important to resolve conflicts and separate traffic, as long as the ATCO is aware of the two aircraft.\nATC collision avoidance\nCollision avoidance via procedural control – Use of altitude information acquired through voice reporting to achieve vertical separation.\nCrew collision avoidance\nSee and avoid practiced by aircraft - This could include the executive controller actively encouraging the aircraft to see-and-avoid through informing them of the track loss situation (if detected) and notifying them of proximate aircraft’s approximate or last known position. The effectiveness of see-and-avoid for Commercial Air Transport is not thought to be high, particularly where there is no indication of the other aircraft through other means (e.g. via TCAS display or through party-line situational awareness).', ""The National Airspace System (NAS) is a vast network of people and equipment that ensures the safe operation of commercial and private aircraft. Air traffic controllers work within the NAS to coordinate the movement of air traffic to make certain that planes stay a safe distance apart. Their immediate concern is safety, but controllers also must direct planes efficiently to minimize delays. Some regulate airport traffic through designated airspaces; others regulate airport arrivals and departures.\nTerminal controllers watch over all planes traveling in an airport's airspace. Their main responsibility is to organize the flow of aircraft into and out of the airport. They work in either the control tower or the terminal radar approach control (TRACON) room or building. Relying on visual observation, the tower local controllers sequence arrival aircraft for landing and issue departure clearances for those departing from the airport. Other controllers in the tower control the movement of aircraft on the taxiways, handle flight data, and provide flight plan clearances. Terminal radar controllers manage aircraft departing from or arriving to an airport by monitoring each aircraft’s movement on radar to ensure that a safe distance is maintained between all aircraft under their control. In addition, terminal controllers keep pilots informed about weather and runway conditions.\nMany different controllers are involved in the departure of an airplane. If the plane is flying under instrument flight rule conditions, a flight plan is filed prior to departure. The tower flight data controller receives the flight plan in the form of a flight strip, which is output from a computer, and arranges it in sequence. When an aircraft calls for clearance the clearance delivery controller issues the clearance and moves the strip over to the ground controller who manages the movement of aircraft on the airport surface, except the active runway. When the aircraft arrives at the active runway the strip is moved to the local controller who issues the departure clearance, observes the takeoff and turns the plane over to the departure controller. The TRACON departure controller identifies the plane on radar, climbs it, and directs it on course.\nAfter each plane departs, terminal controllers notify en route controllers, who take charge next. There are 20 air route traffic control centers located around the country, each employing 300 to 700 controllers, with more than 150 on duty during peak hours at the busiest facilities. Airplanes usually fly along designated routes; each center is assigned a certain airspace containing many different routes. En route controllers work either individually or in teams of two, depending on how heavy traffic is; each team is responsible for a sector of the center’s airspace.\nAs the plane proceeds on its flight plan to its destination it is handed off from sector to sector both within the center and to adjoining centers. To prepare for planes about to enter the team’s sector, the radar associate controller organizes flight plans output from a printer into strip bays. If two planes are scheduled to enter the team’s sector in conflict, the controller may arrange with the preceding sector unit for one plane to change its flight path or altitude. As a plane approaches a team’s airspace, the radar controller accepts responsibility for the plane from the previous sector. The controller also delegates responsibility for the plane to the next sector when the plane leaves the team’s airspace.\nWhen the plane is approximately 50 miles from the destination airport, it is handed off to that airport’s terminal radar arrival controller who sequences it with other arrivals, and issues an approach clearance. As the plane nears the runway, the pilot is issued a clearance to contact the tower. The local controller issues the landing clearance. Once the plane has landed, the ground controller directs it along the taxiways to its assigned gate. The local and ground controllers usually work entirely by sight, but may use airport surface radar if visibility is very poor.\nBoth airport tower and en route controllers usually control several planes at a time, often making quick decisions about completely different activities. For example, a controller might direct a plane on its landing approach and at the same time provide pilots entering the airport's airspace with information about conditions at the airport. While instructing these pilots, the controller also might observe other planes in the vicinity, such as those in a holding pattern waiting for permission to land, to ensure that they remain well separated.\nIn addition to airport towers and en route centers, air traffic controllers also work in flight service stations at 17 locations in Alaska. These flight service specialists provide pilots with preflight and in-flight weather information, suggested routes, and other aeronautical information important to the safety of a flight. Flight service specialists relay air traffic control clearances to pilots not in direct communications with a tower or center, assist pilots in emergency situations, and initiate and coordinate searches for missing or overdue aircraft. At certain locations where there is no airport tower or the tower has closed for the day, flight service specialists provide airport advisory services to landing and departing aircraft. However, they are not involved in actively managing and separating air traffic.\nSome air traffic controllers work at the FAA's Air Traffic Control Systems Command Center in Herndon, VA, where they oversee the entire system. They look for situations that will create bottlenecks or other problems in the system and then respond with a management plan for traffic into and out of the troubled sector. The objective is to keep traffic levels in the trouble spots manageable for the controllers working at en route centers.\nDuring busy times, controllers must work rapidly and efficiently. Total concentration is required to keep track of several planes at the same time and to make certain that all pilots receive correct instructions. The mental stress of being responsible for the safety of several aircraft and their passengers can be exhausting. Unlike tower controllers, radar controllers also have the extra stress of having to work in semi-darkness, never seeing the actual aircraft they control except as a small “blip” on the radarscope. Controllers who work in flight service stations work in offices close to the communications and computer equipment.\nControllers work a basic 40-hour week; however, they may work additional hours, for which they receive overtime, or premium pay, or equal time off. Because most control towers and centers operate 24 hours a day, 7 days a week, controllers rotate night and weekend shifts. Contract towers and flight service station working conditions may vary somewhat from the FAA.\nEducation & Training Required\nThere are three main pathways to become an air traffic controller with the FAA. The first is air traffic controllers with prior experience through either the FAA or the Department of Defense as a civilian or veteran. Second are applicants from the general public. These applicants must have 3 years of progressively responsible full-time work experience, have completed a full 4 years of college, or a combination of both. In combining education and experience, 1 year of undergraduate study—30 semester or 45 quarter hours—is equivalent to 9 months of work experience. The third way is for an applicant to have successfully completed an aviation-related program of study through the FAA’s Air Traffic-Collegiate Training Initiative (AT-CTI) program. In 2008, there were 31 schools in the AT-CTI program.\nAT-CTI program schools offer 2–year or 4-year non-engineering degrees that teach basic courses in aviation and air traffic control. In addition to graduation, AT-CTI candidates need a recommendation from their school before being considered for employment as an air traffic controller by the FAA.\nCandidates with prior experience as air traffic controllers are automatically qualified for FAA air traffic controller positions. However, applicants from the general public and the AT-CTI program must pass the FAA-authorized pre-employment test that measures their ability to learn the duties of a controller. The test is administered by computer and takes about 8 hours to complete. To take the test, an applicant must apply under an open advertisement for air traffic control positions and be chosen to take the examination. When there are many more applicants than available testing positions, applicants are selected randomly. However, the FAA guarantees that all AT-CTI students in good standing in their programs will be given the FAA pre-employment test. Those who achieve a qualifying score on the test become eligible for employment as an air traffic controller. Candidates must be granted security and medical clearance and are subject to drug screening. Additionally, applicants must meet other basic qualification requirements in accordance with Federal law. These requirements include United States citizenship and the ability to speak English.\nUpon selection, employees attend the FAA Academy in Oklahoma City, OK, for 12 weeks of training, during which they learn the fundamentals of the airway system, FAA regulations, controller equipment, and aircraft performance characteristics, as well as more specialized tasks. Graduates of the AT-CTI program are eligible to bypass the Air Traffic Basics Course, which is the first 5 weeks of qualification training at the FAA Academy.\nAfter graduation from the FAA Academy in Oklahoma City, candidates are assigned to an air traffic control facility and are classified as “developmental controllers” until they complete all requirements to be certified for all of the air traffic control positions within a defined area of a given facility. Generally, it takes new controllers with only initial controller training between 2 and 4 years, depending on the facility and the availability of facility staff or contractors to provide on-the-job training, to complete all the certification requirements to become certified professional controllers. Individuals who have had prior controller experience normally take less time to become fully certified. Controllers who fail to complete either the academy or the on-the-job portions of the training usually are dismissed. Controllers must pass a physical examination each year and a job performance examination twice each year. Failure to become certified in any position at a facility within a specified time also may result in dismissal. Controllers also are subject to drug screenings as a condition of continuing employment.\nOther Skills Required (Other qualifications)\nAir traffic controllers must be articulate to give pilots directions quickly and clearly. Intelligence and a good memory also are important because controllers constantly receive information that they must immediately grasp, interpret, and remember. Decisiveness also is required because controllers often have to make quick decisions. The ability to concentrate is crucial because controllers must make these decisions in the midst of noise and other distractions.\nAir Traffic Controllers - What They Do - Page 2\nAviation encompasses all the activities relating to airborne devices created by human ingenuity, generally known as aircraft. These activities include the organizations and regulatory bodies as well as the personnel related with the operation of aircraft and the industries involved in airplane manufacture, ...more""]"	['<urn:uuid:69264af6-40c6-472e-8fd4-82d5406f6cc7>', '<urn:uuid:ae69bdd7-ca65-4a51-8032-dfafd5672b55>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T03:34:55.284799	10	64	2882
6	machine learning depression diagnosis privacy concerns	Machine learning algorithms can classify individuals with depression with 75% accuracy using brain MRI scans. However, this medical data faces privacy concerns as HIPAA regulations allow government agencies and employers to access mental health records without patient consent, potentially leading to workplace discrimination.	"['Summary: Researchers are utilizing neuroimaging data to identify brain patterns and predict depression with the help of machine learning.\nSource: University of Texas at Austin.\nUniversity of Texas researchers use Stampede supercomputer to identify patterns in neuroimaging data that are predictive for mental disorders.\nDepression affects more than 15 million American adults, or about 6.7 percent of the U.S. population, each year. It is the leading cause of disability for those between the ages of 15 and 44.\nIs it possible to detect who might be vulnerable to the illness before its onset using brain imaging?\nDavid Schnyer, a cognitive neuroscientist and professor of psychology at The University of Texas at Austin, believes it may be. But identifying its tell-tale signs is no simpler matter. He is using the Stampede supercomputer at the Texas Advanced Computing Center (TACC) to train a machine learning algorithm that can identify commonalities among hundreds of patients using Magnetic Resonance Imaging (MRI) brain scans, genomics data and other relevant factors, to provide accurate predictions of risk for those with depression and anxiety.\nResearchers have long studied mental disorders by examining the relationship between brain function and structure in neuroimaging data.\n“One difficulty with that work is that it’s primarily descriptive. The brain networks may appear to differ between two groups, but it doesn’t tell us about what patterns actually predict which group you will fall into,” Schnyer says. “We’re looking for diagnostic measures that are predictive for outcomes like vulnerability to depression or dementia.”\nIn 2017, Schnyer, working with Peter Clasen (University of Washington School of Medicine), Christopher Gonzalez (University of California, San Diego) and Christopher Beevers (UT Austin), completed their analysis of a proof-of-concept study that used a machine learning approach to classify individuals with major depressive disorder with roughly 75 percent accuracy.\nMachine learning is a subfield of computer science that involves the construction of algorithms that can “learn” by building a model from sample data inputs, and then make independent predictions on new data.\nThe type of machine learning that Schnyer and his team tested is called Support Vector Machine Learning. The researchers provided a set of training examples, each marked as belonging to either healthy individuals or those who have been diagnosed with depression. Schnyer and his team labelled features in their data that were meaningful, and these examples were used to train the system. A computer then scanned the data, found subtle connections between disparate parts, and built a model that assigns new examples to one category or the other.\nIn the study, Schnyer analyzed brain data from 52 treatment-seeking participants with depression, and 45 heathy control participants. To compare the groups, they matched a subset of depressed participants with healthy individuals based on age and gender, bringing the sample size to 50.\nParticipants received diffusion tensor imaging (DTI) MRI scans, which tag water molecules to determine the extent to which those molecules are microscopically diffused in the brain over time. By measuring this diffusion in multiple spatial directions, vectors are generated for each voxel (three-dimensional cubes that represent either structure or neural activity throughout the brain) to quantify the dominant fiber orientation. These measurements are then translated into metrics that indicate the integrity of white matter pathways within the cerebral cortex.\nOne common parameter used to characterize DTI is fractional anisotropy: the extent to which diffusion is highly directional (high fractional anisotropy) or unrestricted (low fractional anisotropy).\nThey compared these fractional anisotropy measurements between the two groups and found statistically significant differences. They then reduced the number of voxels involved to a subset that was most relevant for classification and carried out the classification and prediction using the machine learning approach.\n“We feed in whole brain data or a subset and predict disease classifications or any potential behavioral measure such as measures of negative information bias,” he says.\nThe study revealed that DTI-derived fractional anisotropy maps can accurately classify depressed or vulnerable individuals versus healthy controls. It also showed that predictive information is distributed across brain networks rather than being highly localized.\n“Not only are were learning that we can classify depressed versus non-depressed people using DTI data, we are also learning something about how depression is represented within the brain,” said Beevers, a professor of psychology and director of the Institute for Mental Health Research at UT Austin. “Rather than trying to find the area that is disrupted in depression, we are learning that alterations across a number of networks contribute to the classification of depression.”\nThe scale and complexity of the problem necessitates a machine learning approach. Each brain is represented by roughly 175,000 voxels and detecting complex relationship among such a large number of components by looking at the scans is practically impossible. For that reason, the team uses machine learning to automate the discovery process.\n“This is the wave of the future,” Schnyer says. “We’re seeing increasing numbers of articles and presentations at conference on the application of machine learning to solve difficult problems in neuroscience.”\nThe results are promising, but not yet clear-cut enough to be used as a clinical metric. However, Schnyer believes that by adding more data — related not only to MRI scans but also from genomics and other classifiers — the system can do much better.\n“One of the benefits of machine learning, compared to more traditional approaches, is that machine learning should increase the likelihood that what we observe in our study will apply to new and independent datasets. That is, it should generalize to new data,” Beevers said. “This is a critical question that we are really excited to test in future studies.”\nBeevers and Schnyer will expand their study to include data from several hundred volunteers from the Austin community who have been diagnosed with depression, anxiety or a related condition. Stampede 2 — TACC’s newest supercomputer which will come online later in 2017 and will be twice as powerful as the current system — will provide the increased computer processing power required to incorporate more data and achieve greater accuracy.\n“This approach, and also the movement towards open science and large databases like the human connectome project, mean that facilities like TACC are absolutely essential,” Schnyer says. “You just can’t do this work on desktops. It’s going to become more and more important to have an established relationship with an advanced computing center.”\nAbout this psychology research article\nFunding: National Institutes of Health, National Science Foundation funded this study.\nSource: Aaron Dubrow – University of Texas at Austin Image Source: NeuroscienceNews.com images are credited to David M Schnyer, Peter C. Clasen, Christopher Gonzalez and Christopher G. Beevers. Original Research:Abstract for “Evaluating the diagnostic utility of applying a machine learning algorithm to diffusion tensor MRI measures in individuals with major depressive disorder” by David M Schnyer, Peter C. Clasen, Christopher Gonzalez, and Christopher G. Beevers in Psychiatry Research: Neuroimaging. Published online March 22 2017 doi:10.1016/j.pscychresns.2017.03.003\nCite This NeuroscienceNews.com Article\n[cbtabs][cbtab title=”MLA”]University of Texas at Austin “Enlisting The Help of Machine Learning to Diagnose Depression.” NeuroscienceNews. NeuroscienceNews, 28 March 2017. <https://neurosciencenews.com/depression-machine-learning-6301/>.[/cbtab][cbtab title=”APA”]University of Texas at Austin (2017, March 28). Enlisting The Help of Machine Learning to Diagnose Depression. NeuroscienceNew. Retrieved March 28, 2017 from https://neurosciencenews.com/depression-machine-learning-6301/[/cbtab][cbtab title=”Chicago”]University of Texas at Austin “Enlisting The Help of Machine Learning to Diagnose Depression.” https://neurosciencenews.com/depression-machine-learning-6301/ (accessed March 28, 2017).[/cbtab][/cbtabs]\nEvaluating the diagnostic utility of applying a machine learning algorithm to diffusion tensor MRI measures in individuals with major depressive disorder\nHighlights •Examination of utility of applying support vector machine (SVM) learning to MRI measures of brain white matter in order to classify individuals with major depressive disorder (MDD). •High degree of accuracy when utilizing a feature selection approach. •Distributed information is more diagnostically informative than highly localized.\nAbstract Using MRI to diagnose mental disorders has been a long-term goal. Despite this, the vast majority of prior neuroimaging work has been descriptive rather than predictive. The current study applies support vector machine (SVM) learning to MRI measures of brain white matter to classify adults with Major Depressive Disorder (MDD) and healthy controls. In a precisely matched group of individuals with MDD (n = 25) and healthy controls (n = 25), SVM learning accurately (74%) classified patients and controls across a brain map of white matter fractional anisotropy values (FA). The study revealed three main findings: 1) SVM applied to DTI derived FA maps can accurately classify MDD vs. healthy controls; 2) prediction is strongest when only right hemisphere white matter is examined; and 3) removing FA values from a region identified by univariate contrast as significantly different between MDD and healthy controls does not change the SVM accuracy. These results indicate that SVM learning applied to neuroimaging data can classify the presence versus absence of MDD and that predictive information is distributed across brain networks rather than being highly localized. Finally, MDD group differences revealed through typical univariate contrasts do not necessarily reveal patterns that provide accurate predictive information.\n“Evaluating the diagnostic utility of applying a machine learning algorithm to diffusion tensor MRI measures in individuals with major depressive disorder” by David M Schnyer, Peter C. Clasen, Christopher Gonzalez, and Christopher G. Beevers in Psychiatry Research: Neuroimaging. Published online March 22 2017 doi:10.1016/j.pscychresns.2017.03.003', '|Healthcare Training Institute - Quality Education since 1979CE for Psychologist, Social Worker, Counselor, & MFT!!\nConfidentiality Issues in the time of HIPAA\nRead content below or listen to audio.\nLeft click audio track to Listen; Right click to ""Save..."" mp3\nIn the last section, we discussed three changes in ethical boundaries in regards to the disclosure of raw test data. These three test data boundary changes include: shift in standards; effects of HIPAA; and protecting test security.\nOn this section, we will discuss three controversies created by HIPAA and its possible breach of ethical boundaries. These three HIPAA privacy controversies include: governmentally accessed information; contradictory language; and employer access.\n3 HIPAA Privacy Controversies\n♦ Controversy #1 - Governmentally Accessed Information\nThe first HIPAA privacy controversy is governmentally accessed information. The branch of the government, Health and Human Services, otherwise known as HHS, in addition to taking on the responsibility of writing the legislation, also takes on the responsibility of monitoring it. However, to do this, the HHS demands unlimited access to clients’ records with or without consent of the client. Upon the inception of HIPAA, the HHS facilitated the federal government’s admittance to the records of millions of mental health clients.\nFor the first time in history, an act of legislation makes it legal for the United States government to retrieve medical information about its citizens, often without the knowledge of the citizens themselves. In addition, HHS has created what HIPAA expert Michael Freeny terms ""a holy trinity of insurers, providers, and claims clearinghouses"" who can exchange information freely between each other without the client’s knowledge.\nEssentially, the information that the client only wanted and thought would be shared just between him or herself and the clinician becomes a free-for-all among these HIPAA approved entities. However, clients are never aware of these transactions and believe their information to be perfectly confidential because, as we discussed in section 3, the Notice of Privacy Policies are so convoluted and obscure that they cannot cipher through it all. Therefore the federal invasion of privacy remains undetected unless a client extensively researches HIPAA for him or herself.\nI have found that many clients who have undertaken the laborious task of sifting through HIPAA legislation feel cheated and betrayed by the government and most significantly by their therapist. This puts an unnecessary strain on the client-therapist relationship so that is why I try and make the language of my NPP as clear as possible to my clients from the beginning so they are not surprised by newly discovered and interpreted legislation.\nThink of your clients. Can you think of any clients who may be shocked by the extent to which the government has access to his or her records?\n♦ Controversy #2 - Contradictory Language\nThe second HIPAA privacy controversy is contradictory language. Although the legislation presents all appearances of attempts to protect a client’s privacy, upon closer scrutiny, it becomes apparent that certain passages may contradict each other.\nMost specifically, Michael Freeny points out a specific section within the legislation which guarantees consumers the right to see and copy their health records and request corrections of mistakes that may be contained in those records. However, providers are obligated by HIPAA to inform clients that they are not bound to fulfill such requests. Therefore, the client’s rights may not be recognized by the provider if the provider deems it unnecessary.\nIn addition, the guidelines stipulate that clients maintain the right to restrict the distribution and transmission of their medical information. However, Freeny points out, ""It’s not true because the next sentence says that the provider is under no obligation to give you that information."" Even further, although the client may retain the right to restrict the distribution of their information and the provider consents, there are certain entities to which the provider cannot deny access, specifically the federal government, police, and public health agencies.\nSo although HIPAA may appear to be more restrictive regarding the distribution of records, this restriction only applies to certain individuals not among the HIPAA approved agencies. Many clients and clinicians are not aware of these contradictions due to the convolution of the regulations themselves. Michael Freeny believes that many therapists prefer to do the bare minimum in regards to HIPAA compliance due to the time it would consume to actually understand the guidelines at a deeper level. Do you agree?\n♦ Controversy #3 - Employer Access\nIn addition to governmentally accessed information and contradictory language, the third HIPAA privacy controversy is employer access. As we discussed in section 4, certain authorities may access information illegitimately and use it to the disadvantage of the client. However, certain employers who operate under the Employee Retirement Income Security Act as insurers for their employees have access to their employees\' health records.\nThis information includes symptoms, medication, and even diagnoses. According to HHS, employers are not allowed to use this information in any business transaction. However, there are no consequences implemented to force the employer to take responsibility for his or her action.\nJohnny, age 32, suffered from alcoholism. He had just obtained a job with a packing company who acted on the Employee Retirement Income Security Act and had access to his mental health records. After about a month at this company, he was fired, even after a positive evaluation. His employer stated that Johnny had been a part of a series of cutbacks, although no other employees, even those with less experience and worse evaluations, had been let go. Johnny believes that his supervisor had been told about Johnny’s drinking problems through this act, but there was no way to prove he had been the victim of this discrimination.\nThink of your Johnny. Could he or she run the risk of losing his or her employment due to his or her disorder?\nIn this section, we discussed three controversies created by HIPAA and its possible breach of privacy boundaries. These three HIPAA privacy controversies included: governmentally accessed information; contradictory language; and employer access.\nPeer-Reviewed Journal Article References:\nBenefield, H., Ashkanazi, G., & Rozensky, R. H. (2006). Communication and records: Hippa issues when working in health care settings. Professional Psychology: Research and Practice, 37(3), 273–277.\nCampbell, L. F., & Norcross, J. C. (2018). Do you see what we see? Psychology\'s response to technology in mental health. Clinical Psychology: Science and Practice, 25(2), Article e12237.\nDouglas, S., Jensen-Doss, A., Ordorica, C., & Comer, J. S. (2020). Strategies to enhance communication with telemental health measurement-based care (tMBC). Practice Innovations, 5(2), 143–149.\nGlueckauf, R. L., Maheu, M. M., Drude, K. P., Wells, B. A., Wang, Y., Gustafson, D. J., & Nelson, E.-L. (2018). Survey of psychologists’ telebehavioral health practices: Technology use, ethical issues, and training needs. Professional Psychology: Research and Practice, 49(3), 205–219.\nRichards, M. M. (2009). Electronic medical records: Confidentiality issues in the time of HIPAA. Professional Psychology: Research and Practice, 40(6), 550–556.\nStiles, P. G., & Petrila, J. (2011). Research and confidentiality: Legal issues and risk management strategies. Psychology, Public Policy, and Law, 17(3), 333–356.\nWhat are three controversies created by HIPAA and its possible breach of privacy boundaries? To select and enter your answer go to .']"	['<urn:uuid:d127661c-da97-4115-9abd-a098698f3a07>', '<urn:uuid:e84a5fa8-27a3-4eb0-be19-a8ca48d08943>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T03:34:55.284799	6	43	2723
7	french drain installation steps drainage problems	For French drain installation, first find an exit point and mark the drain path with string or paint. Then dig a trench that slopes away from the problem area (minimum 1% gradient). Line the trench with permeable landscaping fabric, add 1 inch of gravel, lay the perforated drain pipe, cover with 3-4 inches more gravel, fold the fabric over, and backfill with soil. Be cautious of underground utilities when digging. This system effectively solves drainage problems by collecting excess water through pipe perforations and directing it to drier areas.	"['Land drain, French drain, weeping tile, rock drain, filter drain? You might have seen all these phrases floating around on the internet if you’re looking to fix a drainage problem in your home. But what do they all mean?\nEssentially, they are all the same thing. However, French drain more typically refers to the process of removing the surface water by installing a trench and backfilling with gravel. Before there was such a thing as a land drain pipe, this method was called a French drain - without a pipe. Nowadays people still call this method a French drain even though it will most likely incorporate a perforated land drain pipe.\nWhere can you use a French drain?\nMost typically, a French drain is used in and around your home where excess surface water is a problem.\n- This could be in your lawn, artificial or real grass, or back yard area.\n- A basement or garage where damp is a recurring problem.\n- Or it can be used outside the home in outdoor open spaces such as playground areas and golf courses.\nOn a larger scale, for field drainage in agriculture to give the land a greater potential to be farmed more productively, increasing the growing season and crop yeild.\nWhy use a French drain?\nIf you have any drainage problems on your property, installing a French drain is the easiest and most cost-effective method to free your land of the excess water. The land drain pipe allows the water to easily be collected through the perforations and flow through the pipe distributing the water to a more suitable place where flooding won’t be a problem.\nWhere to end a French drain?\nAfter you have planned out your French drain installation, the next problem you may face is knowing where to end the drain, and where the water can go. The drain must end at the lowest point for the water to be able to flow at a downwards slope and work efficiently. The slope needs to be no less than 1% gradient.\nIdeally if installing in your home or garden you want the end of the drain to be relatively close to the start to save costs, but long enough to actively remove the water to a drier area away from the problem area.\nThere are a number of suitable end points for your French drain system:\n- The water can be directed into an existing rain gutter.\n- Into a gravel pit or dry well filled with small gravel stones.\n- You can install a soakaway system such as a RAINBOX® crate which will hold the water and then slowly release back into the soil avoiding flooding.\nBefore discharging from a French Drain system, get approval from your local planning authority.\nDisadvantages of using a French drain system\n- Installation can be tricky. If you are DIY-ing there are things to look out for when digging your trench such as gas lines and underground utilities. You would need to be aware of where they are before starting to remove the ground.\n- Obviously as the land drain pipe is underground it can sometimes be difficult to maintain and clean, using a geotextile material around the pipe and trench should protect the land drain from mud and debris.', 'Although that muddy area of your lawn may seem as though it’s not salvageable, with some work and creativity, you can solve your drainage problems and transform that muddy spot into an attractive area of your garden. Since many possible underlying reasons for the mud exist, you\'ll find no single solution to the problem. However, several methods can improve drainage or work around the soggy conditions.\nEvaluate the soil and drainage in the area. Muddiness may be caused by heavy clay soil, which doesn\'t drain well and tends to stay wet and sticky. It might also be caused by the grading in the rest of your lawn. Walking through the lawn to determine the yard\'s grading is crucial since it may have been graded to drain toward a spot that lacks an outlet, leading to overall muddiness.\nImprove soil drainage. If the soil is heavily compacted, tilling it 2 to 3 feet deep can help to break up clay or hardpan that is preventing the soil from absorbing moisture normally. One of the easiest ways to improve drainage problems caused by poor soil is to amend the soil with organic materials such as compost. University of California Cooperative Extension\'s Master Gardener Suzzanna Walsh recommends mixing in 30 percent organic materials, or about 1 inch of compost for every 3 inches of garden soil.\nInstall a French drain to solve problems with grading and to encourage better drainage away from the muddy lawn area. A French drain is basically a sloped trench that holds a gravel-covered perforated pipe, known as a drain tile. The trench should slope away from the muddy area, and the drain should slope from the muddy area to an exit point that is above grade. The drain can be installed by finding an exit point and laying the drain path out with string or spray paint before digging the trench line with a spade or turf cutter. Lay the sod aside and keep it moist while completing the trench. After digging the trench, line it with permeable landscaping fabric, and add 1 inch of gravel to the bottom before laying the drain tile in the trench. Cover the pipe with 3 to 4 inches of gravel before folding the edges of the landscaping fabric on top of the gravel. Fill the trench with soil, tamp it down and cover it with the sod that you removed when you dug the trench. Cover the pipe’s outlet with large rocks or a grate.\nWork with what you’ve got by selecting moisture-loving plants that won\'t mind the soggy conditions. Some examples include grassy perennials such as Golden dwarf sweet flag (Acorus gramineus ""Ogon""), which grows 6 to 12 inches tall and is hardy in U.S. Department of Agriculture plant hardiness zones 6 through 9. Other options include white skunk cabbage (Lysichiton camtschatcensis), hardy in USDA plant hardiness zones 5 through 9; or Siberian iris (Iris ibirica), hardy in USDA plant hardiness zones 3 through 9.\nThings You Will Need\n- Organic materials\n- Spade or turf cutter\n- Permeable landscape fabric\n- Perforated drain pipe\n- Moisture-loving plants\n- Wood, cement or stepping stones (optional)\n- Create a walkway to prevent getting your feet muddy while walking in the area. Build an elevated pathway out of pressure-treated wood, tropical hardwood, cement blocks or stepping stones.\n- Hemera Technologies/AbleStock.com/Getty Images']"	['<urn:uuid:6ce18c19-a20d-4340-9fa1-17a6d187614f>', '<urn:uuid:13a8a11f-e56d-442e-8ea9-eb00bfb5b1fc>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T03:34:55.284799	6	89	1109
8	How do partnerships support precision medicine development?	Partnerships, particularly with the FDA, are crucial for precision medicine development. The FDA's investment in understanding NGS provides public assurance about NGS-based IVD tests and signals to the pharmaceutical industry and clinicians that the technology is clinic-ready. Additionally, partnerships between diagnostic companies and pharmaceutical manufacturers are essential for enabling clinicians to consider multiple drug options for patients.	['Next-generation sequencing (NGS) now provides many clinicians with a much broader picture of the mutations present in a patient’s tumor. In fact, NGS is among those technologies that has rapidly made the leap from “concept to clinic,” a likely reason it was one of the topics discussed during the 13th Annual Personalized Medicine Conference in Boston.\nAmong those attending and presenting at the Conference was Joydeep Goswami, president, clinical next-generation sequencing, oncology, Thermo Fisher Scientific. He participated in a panel entitled “Models for the Development of Personalized Medicine Diagnostics,” which featured representatives from the pharmaceutical and diagnostics industries discussing the role of partnerships in overcoming the obstacles that have traditionally slowed the development of new diagnostic tools.\nFor his part, Goswami addressed how NGS has rapidly evolved to support patient treatment at the point of care. He pointed to the fact that NGS now enables us to look for multiple biomarkers at once, giving clinicians much more clinically actionable diagnostic information in a concise report. Moreover, NGS allows clinicians to not just understand the tumor itself, but also the immune system’s response to it. Goswami also pointed to emerging approaches to cell-free DNA (cfDNA) analysis using “liquid biopsy,” which allows for ongoing monitoring of patients’ responses to therapeutics. This will transform how clinicians develop and manage treatment plans for their patients.\nRegarding the broader use of biomarkers in the development of treatment plans, fellow panelist Jacob Van Naarden, chief business officer at Loxo Oncology, pointed out that increasing prevalence of therapies based on biomarkers is starting to make NGS-based approaches more indispensable. NGS-based approaches can save valuable time, by no longer requiring multiple tests to screen for different sets of mutations; eliminate the need for multiple invasive biopsies; and help improve a patient’s quality of life significantly, perhaps even extending it, by identifying appropriate treatments earlier.\nThe panelists also addressed what is still preventing more widespread adoption of NGS-based testing. Goswami identified three areas: awareness, a lab’s technical capabilities and reimbursement. “Greater awareness across healthcare will certainly spark greater demand, and that’s important, but labs must prepare for changes too,” said Goswami. He pointed out that among those changes must be newly automated workflows and increased reliance on analytical instruments that deliver sufficient throughput without sacrificing accuracy, turnaround time and test-to-test consistency.\n“And we can’t discount how important reimbursement is to adoption,” added Goswami, a point echoed by Van Naarden. Van Nardeen stressed how important adequate compensation will be to assay manufacturers and labs, otherwise it’s likely we’ll see limited access to tests. This could create an economic barrier that stifles growth and access to the right care for patients. Van Naarden also pointed to the importance of advanced clinician training and support – explaining results to patients will require new skills and additional doctor-patient time, something that has become a luxury in modern medicine.\nFinally, the panelists discussed how partnerships are evolving to support precision medicine. According to Goswami, one obvious but often overlooked partner is the U.S. Food & Drug Administration (FDA). “The FDA invested considerable time and resources to understand NGS, and this should provide assurance to the public that NGS-based IVD tests have been thoroughly vetted,” said Goswami. “FDA rigor also signals to the pharmaceutical industry and clinicians that the technology is robust and clinic-ready, an important consideration as more investment goes into developing new therapies and commercializing them.” Van Nardeen agreed, adding that, ultimately, we want clinicians to be able to use NGS-based tests to consider as many drugs as possible for a patient. He suggested that stronger partnerships between diagnostic companies and pharmaceutical manufacturers will continue to be key to making that possible.']	['<urn:uuid:8fc89365-0388-4220-9403-d4f9b35e21d3>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T03:34:55.284799	7	57	609
9	treatment ukrainian refugees compared discrimination roma refugees	There are stark differences in how Ukrainian refugees and Roma refugees from Ukraine are treated. While there has been overwhelming solidarity and swift action toward Ukrainian refugees, with people offering private homes, converting buildings into shelters, and providing generous support, Roma refugees face discrimination and unequal treatment. Roma refugees have been refused transport and accommodation by private persons and hotels, segregated from other refugees, and brought to remote places. Some politicians and media have started labeling Roma as 'economic migrants' who 'abuse the refugee system.' In countries like the Czech Republic, there are plans to build separate 'tent towns' specifically for Roma refugees, and some authorities deny them access to the same protection systems available to other Ukrainian refugees. This discrimination is particularly problematic since Roma make up a significant portion of Ukraine's population (up to 400,000) and many Roma have joined Ukrainian forces defending the country.	['Bioethics Forum Essay\nEthical Placemaking for Refugees\nMore than 4 million people have fled from war and terror in Ukraine. Numbers increase by the day. Neighboring countries are granting shelter and protection, people are making space in their private homes, in Airbnbs, in hotels. One project funded by New Zealanders is converting Slovak barns that once housed Nazis into refugee accommodations. The amount of help and donated money and goods for people (and their pets) is overwhelming. What was coined as “welcome culture” in Germany in 2015, when many refugees were greeted warmly and welcoming especially by volunteers and private supporters, is repeating itself.\nAnd yet, refugees from other places face the same deterring policies and practices as usual. They must now make space for Ukrainian refugees and are thereby being torn from established structures such as language classes and schools. Conditions in Syria (also partly due to Russian bombardment), the Central African Republic (where Russian-backed mercenaries operate), and in many other countries including Eritrea, Mali, and Yemen are also dire. Desperate people from these places take the perilous journey across the Mediterranean Sea (where already in 2022 an estimated 227 people are dead or missing) in most cases to meet with hostility, barbed wire, and a state of protracted limbo. Some are using the terms first- and second-class refugees: one Swiss newspaper even spoke of “real” refugees when referring to Ukrainian refugees.\nWhat is, once more, becoming brazenly clear is not only the impotence of the existing international apparatus designed to protect the worldwide 70 million forcibly displaced people, but also its asymmetries. Here we refer not only to the Refugee Convention, which has never been enforceable and has run aground in recent years around conceptions of “persecution.” We also include governments and civil societies around the world that have proven xenophobic and willing to flout international law, and some would say even basic decency.\nIf there is moral indignation over what is happening in Ukraine, why is there not similar outrage concerning Syria and other places that are inhospitable? The usual diagnosis of such asymmetries points to the co-morbidities of ethnocentrism and stereotyping, scapegoating, and economic anxiety.\nWhat might we do – what should we do – to engage moral imaginations in order to generate recognition for those relegated to languishing as refugees? How should we overcome the risk of selected, exclusive, short-term (a worry even for Ukrainians in the European Union, whom we should note have been toward the bottom of Europe’s ethnic hierarchies until recent weeks) solidarity? How could we shift from seeing certain people as foreigners to be feared and resisted to seeing our fellow humans struggling to survive, many of whom will need long-term assistance given the trauma suffered?\nAi Weiwei’s inspiring documentary, Human Flow, includes a section of stunning portraits of people fleeing Middle Eastern and African countries. They are standing still, thereby requiring the viewer to stop for close to a minute and regard them as if they are works of art – not in a way that objectifies or pities. It is a breathtakingly beautiful series in form and content. He disrupts the destructive binary between us and them, between deserving and undeserving. This is ethically significant filming. Instead of presenting people usually depicted as dirty, bloodied, and impoverished, it elicits recognition and respect for universal human dignity and an appreciation for our individuality. Weiwei builds solidarity in this way – he sparks the moral imagination to recognize the needs of others and humanitarian action in some form.\nPart of what is poignant about his gesture is that his “subjects” are filmed on a humanitarian ship, against a gleaming white backdrop, excerpted in a sense, extracted from their origins. And, for the most part, all we know of them is that these human beings are headed to some other place. After all, when the bombs start exploding or the water dries up, the first question people ask is, “Where can we go?”\nThe depth of solidarity and empathy and the swiftness of action by the international community toward the Ukrainian refugee crisis has been striking. Without hesitation, people in the region and around the world have recognized the urgent need and rapidly responded. Specifically, they have responded by creating places for refugees. As they should.\nRecognizing the suffering of displaced people in conditions unable to sustain them follows from the most minimal appreciation of them as fellow human beings. Still another feature of Human Flow is its poignant depictions of desertification, oil fields on fire with their spiraling black smoke, and also refugee encampments in their remote, sprawling monotony. Recognizing that no one could live in such places is one way to see our shared humanity, ultimately, our moral equality. This can – and should – generate a solidaristic response.\nUnderstanding the importance of place ties us together. As fellow human beings, we understand that our subjectivity, identity, and relations as ecological subjects are dependent on places. It also emphasizes our interdependence. We have historic and contemporary connections to people around the world and, as a result, have contributed to their plight, often unintentionally and indirectly. Some of the ties that bind us are more evident than others. Connections like ancestry, ethnicity, religion, and geography can seem obvious, whereas economic or historical links (like to trade or colonization) are not always as clear. But these less evident and sometimes circuitous links also ground responsibilities to respond; where these connections create or perpetuate injustice, they generate responsibilities. One starting point for fulfilling these responsibilities – and, indeed, promoting transitional justice – is ethical place-making. We refer here not only to the place needed for emergency shelter (which also warrants further attention), but also a truly ethical place where people can care and be cared for, physically secure, exercise agency and autonomy, build relations and feel rooted over time.\nWe can do better, ethically, if we think of ourselves (all of us) more explicitly as ecological subjects, people who are radically interdependent: we are embodied (which relies on others’ embodiment); embedded socially; and need to be situated in certain atmospheric, geographic, and material environs. Place is critical to our ability to flourish. Interdependence between peoples and places, moreover, grounds responsibility for justice.\nAttention to place is important for our own academic communities in bioethics and especially public health ethics and global health ethics, as a central concern for these disciplines is a just distribution of the ability to flourish. Refugees – many enduring some disability – arrive in hospitals and clinics, doctors’ offices, and all the other places where health care is provided. We must, therefore, argue for responsibilities to support ethical place-making in places of patient care, including finding ways to better respond to the needs of distinct populations. At the societal level, we can also argue for the moral (and practical) necessity of public health policies that support the importance of ethical place-making. We can argue, for example, for deploying mobile units for refugees on the move and establishing care and health resource sites at border crossings. Overcrowded and unsanitary shelters and other forms of accommodation threaten physical and mental health, and catalyze frustration and aggression. In communities where refugees settle, we can advocate for the deployment of embedded care resources, in addition to adequate housing fostering the ability to build relations, exercise agency, feel safe, healthy, and rooted. We can participate in efforts to resist and reduce distrust, stigma, and other barriers to health care and well-being, literally building trust in the ways we design institutions, care settings, interventions, and interactions with attention to place. Finally, we can and should argue for revisiting and realigning the Refugee Convention to better respond to contemporary realities of why people are compelled to flee their homes for the sake of individual, public, and global health. For people everywhere, this requires a sense of place. We should work to ensure it through our international legal instruments and conventions, our public health policies, and the design of our care institutions and encounters.\nWe close by making a much broader call for ending the war in Ukraine, and wars elsewhere, to protect and create safe and healthy places for people where they feel they belong and where they can flourish. The obvious task, also for bioethicists who are interested in ethical place-making, is to do everything we can to prevent and fight against forced displacement in the first place.\nLisa Eckenwiler, PhD, (@lisaaeck) is a professor of philosophy and department chair at George Mason University. She is vice president of the International Association of Bioethics and a Hastings Center fellow. Verina Wild, PhD, (@VeriWil) is a professor of medical ethics at the University of Augsburg in Germany.', 'By Stephan Müller\nRoma in Ukraine are equally affected by the Russian aggression as all citizens of the Ukraine. Thousands of Roma have joined the Ukrainian forces defending the country, other Roma are involved in humanitarian work supporting vulnerable people – Roma and non-Roma alike.\nRoma were forced to leave their homes in the eastern part of Ukraine due to the bombings and Russian atrocities. Many are living now in the western parts of Ukraine as Internally Displaced Persons (IDP), joining the Romani IDP who were forced to leave their homes in Eastern Ukraine already in 2014.\nMany others, probably a few ten thousand, found refuge in Moldova and in Member States of the European Union. Many of those perceived as Roma experience unequal treatment and discrimination by authorities and volunteers.\nIn many countries, civil society organisations have observed cases of discrimination: private persons or hotels refused to take in persons considered Roma; drivers refused to transport persons considered Roma; in other cases, they were segregated from other refugees or brought to remote places.\nIn the first weeks of the war, primarily civil society organisations, individual volunteers or church organisations provided assistance to the refugees, including Romani refugees. With official structures taking over the reception and provision of assistance to refugees, incidents of discrimination have not stopped, but are now in the responsibility of the authorities.\nBased on the available information we cannot talk of a systemic discrimination of persons considered Roma. At the same time, there are indications of antigypsyism and of structural discrimination.\nIn particular, certain politicians and media outlets have begun to single out Roma as purely “economic migrants” who “abuse the refugee system” and break the rules. These irresponsible racist statements apparently aim at paving the ground for getting rid of the Romani refugees again.\nBackground: Roma in Ukraine\nRoma have lived on the territory of what is now Ukraine since the 15th century. During the last census in 2001, 47,600 persons were registered as Roma. However, their actual number is assessed to be up to 400,000.\nThe Roma in Ukraine are a very diverse community. There are several sub-groups based on the main language or dialect of Romanes spoken, traditions, traditional professions, religion, or the region of Ukraine they live in.\nThe largest number can be found in the Transcarpathian region, a region that also houses a large Hungarian community. Many of the Roma in this region speak Hungarian and consider themselves as part of the Hungarian community and its culture.\nIn 2018-2019, several violent attacks were committed against Roma by right-wing extremists in which one young Romani man, David Popp was killed. Overall, discrimination and social exclusion of Roma was as wide-spread as in all other countries in Europe.\nSince the onset of the war, some cases of violence and of discrimination against Roma took place, in particular in western Ukraine. There is only limited information about the situation of the Roma in the occupied areas. According to NGOs active in the area, the representative of the Roma community in Izyum was killed by Russian forces after he refused to cooperate with them.\nAccording to European Roma Right Centre (ERRC) up to 20% of the Roma do not possess IDs which created problems for leaving Ukraine or for being accepted as a refugee under the Temporary Protection System of the European Union.\nDespite these incidents of violence and the general discrimination of persons considered Roma in Ukraine, the overwhelming majority of the Roma are now defending Ukraine.\nInternally Displaced Persons\nA few ten thousand Roma were forced to leave their homes in eastern Ukraine and found temporary refuge in central or western Ukraine. Overall they face similar problems as other IDP, which are compounded by specific problems such as discrimination when they are looking for transport, accommodation or humanitarian assistance. Their plight is further exacerbated by the fact that they cannot afford private accommodation and have to rely on other forms of temporary accommodation.\nRoma from Ukraine as Refugees\nIt is difficult to determine the exact number of Romani refugees from Ukraine, but we can assume that several ten thousand, primarily women and children have left Ukraine.\nIn May 2022, Ukrainian authorities regulated that only persons from war zones or in close proximity to war zones can leave the country without documents.\nIn March 2022, the European Union activated the Temporary Protection Directive that allows for access to accommodation and the labour market to persons from Ukraine. Ukrainian refugees do not have to enter asylum procedures. However, it seems that it depends on the individual country if it extends Temporary Protection also to persons from Ukraine entering without any documents. While Slovakia announced that also persons without documents are eligible for Temporary Protection after an identity check, Germany has not clarified this issue as of yet.\nAnother general problem seems to be that many Roma from the Transcarpathian region have a dual Ukrainian and Hungarian citizenship. In recent years, the Hungarian government has allegedly issued around one million citizenships to persons from neighbouring countries, such as Serbia, Romania or Ukraine. Just like ethnic Hungarians or ethnic Ukrainians, many Roma from Transcarpathia availed themselves of this opportunity to enter the labour market in the European Union by means of a Hungarian passport.\nYet, persons with a dual citizenship, in particular Roma, now face serious problems in countries such as the Czech Republic and Germany, since they are considered as citizens of the European Union and access to both the temporary protection system and asylum procedure are denied to them. A similar situation seems to exist in Austria.\nThe public discussion in the Czech Republic and to a lesser extent in Germany on refugees with dual citizenship focuses on Roma and ignores the fact that they constitute only a part of the Romani refugees from Ukraine.\nThe following sketches give an impression of the situation of the Roma in the Czech Republic, Hungary and Germany:\nPresident Milos Zeman stated that the Roma are not war refugees, but solely economic migrants. When Roma refugees from Ukraine slept rough in front of the train station of Brno, the Deputy Mayor Ms. Marketa Vankova claimed that Roma refugees had applied for financial assistance in several countries but failed to provide proof for her statement when questioned by journalists.\nIn Prague, a few hundred Roma are spending the nights at the central train station. The city government plans to build a “tent town” only for Roma who have fled Ukraine. The mayor specifically referred to Roma with a dual citizenship.\nOn 19 May 2022, the Czech government modified the legislation governing the situation of the refugees from Ukraine. In the light of the ongoing public discussion before, a number of amendments to the law were introduced to target the Roma in particular. It was stipulated that the provision of humanitarian assistance to refugees at the main train station will stop at the end of May. The overwhelming majority of the refugees at the main train station are Roma. The civil society organisations working with the refugees were not consulted when the amendments were discussed.\nThe Czech Minister of Internal Affairs, Vít Rakušan, talked with his Ukrainian counterpart requesting that persons with dual citizenship should not be allowed to travel to the Czech Republic. He also contacted the Hungarian authorities and allegedly reached an agreement that Hungary will shorten the turnaround time for vetting new arrivals to the Czech Republic for a Hungarian citizenship.\nIn Hungary, there exists a considerable discrepancy between the number of Ukrainian refugees who arrived there and the actual number of refugees who registered in order to be eligible for health insurance and material assistance.\nOn May 12th, the Ministry of Internal Affairs stated that ca. 578,531 Ukrainian refugees had arrived in Hungary. Other government data vary between 700,000 and one million persons. These figures also include persons who came from Ukraine to Hungary via Romania. By contrast, only 21,320 persons requested temporary protection and 11.467 persons received the status by May 13th. In Slovakia, by comparison, 75,000 out of 425,000 refugees received temporary protection. In Poland, this was the case for one million persons out 3.6 million.\nWe can assume that a large number of the refugees who entered Hungary did not request protection and travelled on to other countries.\nThere exists conflicting information regarding the situation of the refugees with a dual citizenship. Between 2011 and early 2015, a total of 94,000 Ukrainians received Hungarian passports.\nAccording to the Hungarian Helsinki Committee, all refugees from Ukraine with double citizenship are not eligible for asylum status, but they enjoy the rights granted to refugees from Ukraine under the temporary protection system, including access to temporary accommodation, access to health care, monthly financial support, etc .\nThe website of the National Directorate for Aliens Policing states that persons who have a dual citizenship enjoy the same rights and obligations as Hungarian citizens. Yet, there are reports from Hungary that Romani refugees receive unequal treatment. During the first weeks of the war, members of civil society or churches primarily took care of the arriving refugees and organised transport, accommodation or provision with food or hygienic articles. Due to discriminatory practices, it fell to Romani civil society organisations to arrange transport and accommodation for Romani refugees, to provide food or hygienic articles for them and an to assist them in their communication with the authorities.\nIn Germany, a few cases of discrimination have been reported and several biased media reports on Roma from Ukraine have been published. These reports merely concern the wrong doings of some individuals, but never present the situation of the overwhelming majority of the Roma. There were no reports at all about Romani lawyers and students. There was no mention of women and children arriving in a country whose language they do not speak. These women were discriminated at home and continue to be discriminated now while their husbands and fathers are fighting at the frontline.\nThough being aware of it, the relevant German authorities such as the Ministry of Interior, or the Commissioner for Integration have not clarified the status of refugees from Ukraine who never possessed any documents.\nWith regard to the situation of Roma with a dual Ukrainian and Hungarian citizenship, it seems that the German authorities tend to share the position of the Czech Government and deny them access to temporary protection and asylum. The German authorities have not issued an official statement regarding this question so far.\nFor the German authorities, the Hungarian citizenship takes priority over the Ukrainian citizenship. In some cases, Roma with a dual citizenship were offered train tickets to Hungary, though they might have never lived in Hungary.\nSince Hungary never signed the European Convention on Social and Medical Assistance, refugees considered as Hungarian citizens would also not be eligible for receiving health insurance or any kind of social assistance in Germany.\nPersons considered Roma by the majority population have only received a similar welcome as other refugees from Ukraine in a few cases. It was very often up to the Romani civil society to assist them and we can identify from the very beginning a difference between Ukrainian refugees considered Roma and white Ukrainian refugees.\nIn countries such as the Czech Republic and Germany, the authorities and politicians tend not to consider and accept Roma as “war refugees” on a par with all other refugees from Ukraine. Although Roma constitute only a part of the Ukrainians with a dual Ukrainian-Hungarian citizenship, the public discussion in both countries focuses solely on the Roma. Further, in the Czech Republic, the authorities allegedly only check persons considered as Roma if they have a dual citizenship.\nThis raises the question of what the situation of the Romani refugees will be like in the near future. No one can predict how the Russian war against the Ukraine will develop, but we can assume that for a large part of the Roma a return to their pre-war homes will not be possible in the foreseeable future.\nThe receiving countries cannot exclude all Roma refugees from humanitarian assistance or inclusion measures. The example of attitude of the Czech Republic shows, however, that certain governments try to limit the number of Roma refugees in their country or to exclude them from the kinds of assistance other Ukrainian refugees receive.\nAmong the refugees, there are primarily women with children. Looking at socio-demographic data for Roma in Ukraine, we can assume that many women have only basic school education which creates challenges for their participation in inclusion measures such as accessing the labour market.\nIn addition, persons considered Roma by the majority and the authorities will face more severe obstacles to inclusion, primarily due to antigypsyism and the rejection by the majority population and authorities.\nExperiences from other wars or post-war situations (Bosnia and Herzegovina, Kosovo or Syria) show that persons considered Roma also face serious obstacles or even resistance when they return to their home countries – either to their home towns or to other parts of their home countries.\nDue to non-existing systematic monitoring and the dynamic situation, it is a challenge producing a comprehensive overview of the situation of Romani refugees from Ukraine in other European countries. This document is therefore less an analysis of their situation than a description based on available reports from media, humanitarian organisations and activists.\nSituation in Ukraine\nSituation in Neighbouring Countries\nSituation in Germany']	['<urn:uuid:b8f9f5e2-26d9-4d9b-ab35-de9fbf8e09ea>', '<urn:uuid:771bb3b6-7ea4-4ec4-8b0e-ec027d1cb9f4>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T03:34:55.284799	7	147	3689
10	ways track success email campaigns customer reactions social media	Success of email campaigns and customer reactions can be tracked through multiple methods: 1) Email marketing software provides analytics to track open rates, click-through rates, bounce rates, and conversion rates. 2) For social media reactions, sentiment analysis tools can scan social media platforms to detect positive or negative mentions about brands and products. Companies can monitor these mentions to gauge customer reactions and compare brand performance against competitors.	"[""peshkova - Fotolia\nSentiment analysis tools generate insights into how companies can enhance the customer experience and improve customer service.\nSentiment analysis is an application of natural language processing (NLP) that reveals the emotional states in human speech or text -- in this case, the speech and text that customers generate. Businesses can use machine-learning-based sentiment analysis software to examine this speech and text for positive or negative sentiment about the brand. With this information, companies have an opportunity to respond meaningfully -- and with greater empathy. The aim is to improve the customer relationship and enhance customer loyalty.\nCustomer interactions with organizations aren't the only source of this expressive text. Companies can also obtain customer sentiment by mining social media. Social media monitoring produces significant amounts of data for NLP analysis. Social media sentiment can be just as important in crafting empathy for the customer as direct interaction.\nWhen the organization determines how to detect positive and negative sentiment in customer expressions, it can improve its interactions with the customer. By exploring historical data on customer interaction and experience, the company can predict future customer actions and behaviors, and work toward making those actions and behaviors positive.\nWhy sentiment analysis is necessary\nSentiment analysis can improve customer loyalty and retention through better service outcomes and customer experience.\nFor example, a customer makes a support request through email or chat. The NLP machine learning model generates an algorithm that performs sentiment analysis of the text from the customer's email or chat session. The algorithm detects the customer's emotional state. In this scenario, the customer feels agitation. Business rules related to this emotional state set the customer service agent up for the appropriate response. In this case, immediate upgrade of the support request to highest priority and prompts for a customer service representative to make immediate direct contact. Finally, the service representative's awareness of the customer's emotional state results in a more empathetic response than a standard one, leading to a satisfying resolution of the issue and improvement in the customer relationship.\nThis scenario, simple though it may seem, shows how effectively sentiment analysis can improve customer outcomes. It's an example of augmented intelligence, where the NLP assists human performance. In this case, the customer service representative partners with machine learning software in pursuit of a more empathetic exchange with another person.\nThis scenario is just one of many; and sentiment analysis isn't just a tool that businesses apply to customer interactions. It's also a resource for brand management in social media.\nThe importance of customer sentiment extends to what positive or negative sentiment the customer expresses, not just directly to the organization, but to other customers as well. People commonly share their feelings about a brand's products or services, whether they are positive or negative, on social media. If a customer likes or dislikes a product or service that a brand offers, they may post a comment about it -- and those comments can add up. Such posts amount to a snapshot of customer experience that is, in many ways, more accurate than what a customer survey can obtain.\nThis is where social media monitoring comes in. By mining the comments that customers post about the brand, the sentiment analytics tool can surface social media sentiments for natural language processing, yielding insights. This activity can result in more focused, empathetic responses to customers.\nSentiment analysis tool options\nThere are numerous steps to incorporate sentiment analysis for business success, but the most essential is selecting the right software.\nWhich sentiment analysis software is best for any particular organization depends on how the company will use it. A company that focuses exclusively on doing sentiment analysis to bolster the empathetic responses of customer service personnel might choose one tool, while a company interested in social media monitoring and mining that data for customer sentiment might choose a different one. Another business might be interested in combining this sentiment data to guide future product development, and would choose a different sentiment analysis tool.\nHere are five sentiment analysis tools that demonstrate how different options are better suited for particular application scenarios. These tools are not in a ranked order.\n- Talkwalker. This tool is sensitive to emotional nuance; it can detect sarcasm, for instance. It also can detect trends in external data from a given market, making it possible to compare brand performance against other brands.\n- Clarabridge. Clarabridge integrates a customer's sentiment results with other contextual data, including surveys, online reviews and emails. This feature helps generate predictive insights into customer behavior.\n- MeaningCloud. This tool gathers sentiment data and integrates new data with old data. Use this analysis feature to observe long-term sentiment trends and behavioral shift over time in a customer base.\n- Aylien. This tool's focus is the text found on news websites. Rather than being customer-specific in its sourcing, it surfaces brand mentions in articles, reviews and press releases. Use it to gain insights into brand performance in the marketplace.\n- Mention. Mention scans for mentions of the brand on social media and news websites, as well as in search results. It categorizes the results as positive or negative. Mention also has a competitor comparison feature. Consider Mention if social media sentiment is the organization's only real sentiment analysis interest.\nThe social-media-friendly tools integrate with Facebook and Twitter; but some, such as Aylien, MeaningCloud and the multilingual Rosette Text Analytics, feature APIs that enable companies to pull data from a wide range of sources.\nHow to use sentiment analysis for customer feedback\nA necessary first step for companies is to have the sentiment analysis tools in place and a clear direction for how they aim to use them.\nSentiment analysis tools show the organization what it needs to watch for in customer text, including interactions or social media. This is more than a matter of scanning for positive and negative keywords. Patterns of speech emerge in individual customers over time, and surface within like-minded groups -- such as online consumer forums where people gather to discuss products or services.\nThe organization can choose certain keywords that sentiment analysis software detects, such as:\n- It's great or it's terrible;\n- It's simple or it's difficult; and\n- It's economical or it's expensive.\nHowever, they can also include word groups, such as:\n- I didn't like it;\n- It doesn't work; and\n- It's easy to use.\nSentiment analysis software may also detect emotional descriptors, such as generous, irritating, attractive, annoyed, charming, creative, innovative, confusing, lovely, rewarding, broken, thorough, wonderful, atrocious, clumsy and dangerous. These are just a few examples in a list of words and terms that can run into the thousands.\nSentiment analysis software notifies customer service agents -- and software -- when it detects words on an organization's list. Sometimes, a rule-based system detects the words or phrases, and uses its rules to prioritize the customer message and prompt the agent to modify their response accordingly. The customer's feedback determines the agent's course of action.\nBusinesses can use the sentiment in customer feedback for strategic planning, product design, marketing campaigns and internal process improvement.\nSentiment analysis use cases\nWhen considering sentiment analysis tools, it's helpful to think through the use cases.\nSocial media monitoring and brand management. Companies can scan social media for mentions and collect positive and negative sentiment about the brand and its offerings.\nCustomer service response. Bolstering customer service empathy by detecting the emotional tone of the customer can be the basis for an entire procedural overhaul of how customer service does its job.\nProduct analysis. When a company puts out a new product or service, it's their responsibility to closely monitor how customers react to it. Companies can deploy surveys to assess customer reactions and monitor questions or complaints that the service desk receives.\nCompanies should also monitor social media during product launch to see what kind of first impression the new offering is making. Social media sentiment is often more candid -- and therefore more useful -- than survey responses.\nWhen harvesting social media data, companies should observe what comparisons customers make between the new product or service and its competitors to measure feature-by-feature what makes it better than its peers. This data can become part of the product's evolutionary lifecycle.\nCRM enhancement. Companies can use customer sentiment to alert service representatives when the customer is upset and enable them to reprioritize the issue and respond with empathy, as described in the customer service use case.\nCustomer service platforms integrate with the customer relationship management (CRM) system. This integration enables a customer service agent to have the following information at their fingertips when the sentiment analysis tool flags an issue as high priority.\n- the customer's preferred channel of contact -- especially if it's a different channel than the one they used to make the complaint;\n- the customer's journey stage, such as whether they are a new or old customer; and\n- the customer's frequency of issues in the past.\nThis information can help the agent decide how to respond -- not just with heightened empathy -- but with greater and more focused effectiveness."", 'Email marketing software is an essential tool for businesses of all sizes, offering a powerful and efficient way to send out promotional emails, newsletters, and other forms of communication with customers.\nIt can be used to streamline the process of engaging with existing customers as well as reaching new ones. With the right email marketing platform in place, businesses can enjoy greater reach and better results from their campaigns.\nIn this article, we will look at the features and benefits that email marketing software offers, provide tips on selecting the right platform for your business needs, and explore how it can help you take your customer engagement to the next level.\nWhat Is Email Marketing Software?\nEmail marketing software is a tool that businesses use to create, send, and track bulk emails to their customers and subscribers.\nThe software allows businesses to manage their email lists, design and customize email templates, schedule emails to be sent at a specific time, and track the performance of their email campaigns.\nEmail marketing software also includes features such as email segmentation, which allows businesses to divide their email lists into smaller groups based on certain criteria such as demographics or past purchasing behavior.\nThis helps businesses to send more targeted and personalized messages. Additionally, the software also includes analytics tools to track the success of the email campaign, such as open and click-through rates, bounce rates, and conversion rates.\nWhy Email Marketing Software Is Important?\nEmail marketing software is important for businesses because it allows them to effectively communicate with their customers and target audience in a personalized and automated way. With email marketing software, businesses can:\nReach a large audience\nEmail marketing software allows businesses to reach a large audience quickly and easily, as opposed to traditional methods such as mail or phone.\nEmail marketing software allows businesses to personalize their messages to specific segments of their audience, which can lead to higher engagement and conversions.\nAutomate repetitive tasks\nEmail marketing software allows businesses to automate repetitive tasks such as sending welcome emails, abandoned cart emails, and birthday emails which save time and resources.\nEmail marketing software includes analytics and reporting features that allow businesses to track the success of their campaigns, such as open rates, click-through rates, and conversions.\nEmail marketing is one of the most cost-effective marketing methods, as it requires only a small investment in the software and an internet connection.\nFeatures of Email Marketing Software\nEmail marketing software typically includes the following features:\nContact List Management\nThis feature allows you to create and manage a database of contacts. It enables you to segment your list into different categories, such as current customers and potential leads. You can also use this feature to collect and store data about each contact for more targeted messaging.\nEmail Design And Templates\nThe software includes a number of pre-designed email templates that you can use to create professional-looking emails. This makes it easy to quickly and effectively create messages with your own branding, without needing design or coding skills.\nThis feature enables businesses to schedule emails to be sent at a specific time or set up automated messages that will be triggered by customer interactions or other activities. This saves you time and ensures that your customers receive the right message at the right time.\nAllows businesses to create automated email campaigns, such as welcome emails or abandoned cart emails. This feature allows you to automate routine tasks and processes.\nYou can set up rules for when emails should be sent and how often, as well as create personalized messages based on customer data. Automation also enables you to track the effectiveness of your campaigns and analyze results in real-time.\nAllows businesses to personalize emails with the recipient’s name, location, and other information. You can use this feature to create highly targeted messages that are tailored to each individual customer.\nThis helps to increase engagement and conversions, as well as build relationships with your customers.\nTracking and Reporting\nAllows businesses to track the performance of their email campaigns, such as open and click-through rates, bounce rates, and conversion rates. This helps to measure the success of your campaigns and identify areas for improvement.\nA/B split testing allows businesses to test different subject lines, email designs and calls to action to determine which ones are most effective. This helps you to optimize your campaigns and ensure that your emails are as effective as possible.\nMany email marketing software can integrate with other tools such as CRM, e-commerce platforms, and social media to provide a seamless user experience. This improves efficiency and allows for more personalized customer engagement.\nSpam and Deliverability\nEmail marketing software often includes features to help ensure emails reach the recipient’s inbox and avoid being marked as spam. This helps to ensure that your emails reach their intended audience and are not blocked by email providers.\nEmail marketing software often includes features to help keep the email list clean, and up-to-date and to comply with regulations such as GDPR, CAN-SPAM, and CASL. This helps to ensure that your emails are delivered successfully and that customers’ data is secure.\nBy taking advantage of the features provided by email marketing software, businesses of all sizes can create powerful and effective campaigns to engage with their customers and drive conversions.\nBenefits of Email Marketing Software\nEmail marketing software offers a number of benefits for businesses, including:\nWith email marketing software, you can reach more potential customers than ever before. You can easily create segments and target them with specific campaigns to ensure you’re sending relevant information to the right people at the right time. This significantly increases your chances of acquiring new customers and retaining existing ones.\nEmail marketing software allows you to automate tasks and campaigns so you can focus on other aspects of your business. You can schedule emails in advance, automate welcome emails, create automated follow-up messages, and more. This saves time and effort and helps you stay organized.\nThe best email marketing software platforms provide real-time analytics and reporting that allow you to track the performance of each campaign. This helps you identify areas for improvement and optimize future campaigns for better results.\nEmail marketing is much more cost-effective than traditional advertising methods, such as print or television ads. You can maximize your budget by using email campaigns to reach more customers for less money.\nEmail marketing software lets you create highly targeted campaigns that are tailored to the needs and interests of your customers. This increases engagement and helps you build relationships with them over time.\nEmail marketing software provides the tools to create engaging content that will capture your customers’ attention and encourage them to act. You can use visuals, videos, polls, or surveys to make emails more interactive and drive higher levels of engagement.\nIncreased Brand Awareness\nEmail marketing software is also an effective way to boost your brand awareness and reach new customers. You can create campaigns that are tailored to specific customer segments and increase your presence in the market.\nThe best email marketing platforms provide you with real-time analytics and reporting so that you can measure the performance of each campaign. This helps you identify areas for improvement and optimize future campaigns for better results.\nTips for Choosing the Right Email Marketing Software\nDetermine your needs\nBefore choosing email marketing software, it is important to determine your specific needs and goals. Consider factors such as the size of your email list, the types of emails you want to send, and the features you require.\nConsider the pricing\nCompare pricing plans of different email marketing software to ensure you choose one that fits your budget.\nLook for scalability\nConsider the scalability of the software, as your business may grow and your email list may increase. Choose a software that can accommodate your future needs.\nCheck for integrations\nLook for software that can integrate with other tools you use such as CRM, e-commerce platforms, and social media.\nCheck the deliverability\nCheck the reputation of the software in terms of deliverability, and ensure that the software is able to deliver your emails to inboxes.\nCheck the features\nMake sure that the software has all the features you need, such as automation, personalization, A/B testing, and analytics.\nCheck the support\nMake sure that the software provider offers good customer support, and check if they have a knowledge base or tutorials to help you get started.\nRead reviews and testimonials from other users to see if they have had a positive experience with the software.\nTest the software\nTake advantage of free trials or demos to test the software and see if it is a good fit for your business.\nIncreasing Customer Engagement With Email Marketing Software\nThere are several ways to take your customer engagement to the next level with email marketing software:\nUse personalization features in the software to create targeted and personalized emails for different segments of your email list. This can increase open and click-through rates, and lead to higher conversions.\nUse automation features to create automated email campaigns, such as welcome emails, abandoned cart emails, and follow-up emails. This can save time and increase engagement.\nA/B Split Testing\nUse A/B testing features to test different subject lines, email designs, and call to action to determine which ones are most effective. This can help you improve your open and click-through rates.\nUse interactive content such as polls, quizzes, and surveys to increase engagement and gather valuable customer insights.\nPersonalized subject line\nUse personalized subject lines that include the recipient’s name or other relevant information. This can increase open rates and engagement.\nUse segmentation features to divide your email list into smaller groups based on certain criteria such as demographics or past purchasing behavior. This can help you send more targeted and personalized messages.\nSchedule your email at the right time when your customers are most likely to be active, for example sending a newsletter on a Monday morning may not be the best idea as it’s not the most convenient time for your customers.\nEnsure that your emails are mobile-optimized, as many people check their emails on their mobile devices.\nEngage with your customers\nUse email marketing software to engage with your customers on a regular basis and to encourage them to share their feedback and opinions.\nUse data and analytics\nUse data and analytics to track the success of your email campaigns, and make data-driven decisions to improve engagement and conversions.\nIn conclusion, email marketing software is a powerful tool for businesses looking to improve their customer engagement and boost their sales.\nWith features such as personalization, automation, A/B testing, and analytics, email marketing software can help businesses create targeted and personalized emails that increase engagement and conversions.\nBy determining their specific needs and goals, businesses can choose the right email marketing software for their needs. Additionally, by utilizing the tips provided such as personalization, automation, A/B testing, and segmentation, businesses can take their customer engagement to the next level.\nWith the ability to automate repetitive tasks, schedule emails at the right time, ensure mobile optimization, and track the success of their email campaigns, businesses can save time and resources while improving their customer engagement and increasing their revenue.\nLast Updated on January 28, 2023 by Derren Blio']"	['<urn:uuid:32dd3e4a-62de-4846-a490-f973d6044615>', '<urn:uuid:65d576e1-11f1-4e46-a179-8352cc41fb54>']	factoid	direct	long-search-query	distant-from-document	three-doc	novice	2025-05-13T03:34:55.284799	9	68	3406
11	personal library book tracking methods scholastic wizard alternatives	There are multiple tools for tracking and finding leveled books: Scholastic's Book Wizard provides guided reading level and other metrics, Media Collector with IntelliScanner can scan and store book information, and ARBookFinder allows searching the Accelerated Reader catalog for book levels. Excel spreadsheets can also be used to track book titles, authors, levels, and locations.	"['A Virtual Peek Into My Classroom Library\n- Grades: 1–2, 3–5\nA library is an essential part of any elementary classroom. To run an effective Reading Workshop, it is necessary to stock your classroom library with books of a variety of genres, topics, and levels.\nA library is an essential part of any elementary classroom. To run an effective Reading Workshop, it is necessary to stock your classroom library with books of a variety of genres, topics, and levels. Teachers who use the workshop method know that readers need lots of books in a single year, as they are given time to read self-selected texts independently on a daily basis. For this reason, it\'s important to organize your classroom library in a way that allows students to easily find ""just right"" books that they are interested in reading.\nRead on to watch a video about how I organize my classroom library and how I use it as a tool to help my students evaluate their own reading progress throughout the year. You will also find ideas for collecting more books for your own classroom library, links to download book labels, and additional photos of the library.\nTake a Virtual Tour of My Classroom Library!\nUsing Colored Baskets to Organize My Books\nAll baskets have a unique label that tells a reader what type of books they can find inside. The basket labels vary based on the section of the library in which the basket is located.\nHow I Level My Books\nI do not level my books just so that I can assign students a color code (level) and then make them read only at that level. I make certain that my students are involved in the process in every way. They read books from the classroom library and try to determine what levels seem ""just right"" for them. I meet with each student individually to decide upon a comfortable ""just right"" level so that students can start choosing appropriate books that they can read independently. (Watch my library video for more information about how this process works.) Once a student\'s JR level is determined, he or she can refer to the basket labels as a guide for finding books that are ""just right"" for them. As the school year progresses, students are constantly reevaluating what levels feel ""just right"" for them and reading trial books at a higher level before deciding to regularly read books at that level independently.\nI use Scholastic\'s Book Wizard to level my books. It provides a variety of levels including guided reading level, grade level equivalent, DRA level, lexile level, and interest level. A description of each book is also provided along with its genre, common themes, and topics you will find in the book. The Book Wizard also allows teachers to create, print, and even exchange book lists with other teachers. You can also use Book Wizard to help you find ""just right"" books for your students using the ""Book Alike"" feature.\nCollecting More Books for Your Classroom Library\nIt\'s common knowledge that an effective classroom library has a large variety of books at many different levels, about many different topics, and of many different genres. That sounds great, but where can you get more books?\nOne of my favorite ways to collect additional library books is to ask my current students to donate books from home that they have already read. To provide them with an incentive, the donated books are given a special label with the child\'s name and the date that the book was donated. Students like to know that their book will forever be part of the Newingham library.\nAnother idea to consider is a read-a-thon. Students can collect pledges from family and friends for each book they read in a month (or a certain period of time). Students can count the books they read in class and at home. Not only are students motivated to read lots of books, but the money raised can go to the purchasing of new books for your classroom library. The kids then get to enjoy reading the books they earned for the class.\nKeeping Track of Your Books\nOnce I began collecting a good number of books, it became important to me that I had some sort of inventory of the books I own. This is helpful when choosing books to read aloud, when suggesting ""just right"" books for students, and for keeping track of all my books. Since I was using the computer to look up the levels of my books, it made sense to also add the book title, author, level, and library location to an Excel file that I could access when searching for a book.\nIn the past couple of years, I have been transferring my book collection to Media Collector, a software used with IntelliScanner. An IntelliScanner is a device used to scan the barcodes on your classroom library books. The information is collected and stored on your computer. You can choose to add your own categories to the collected information as well. For example, once a book is added to my collection, I add categories for book level and library location.', 'Your Child’s Reading Level Explained\nAshley Day-Bohn, M.Ed.\nHow is reading level measured?\nThere are several common assessments used to measure a child’s reading level, such as the Developmental Reading Assessment (DRA), which is typically administered one-on-one with a teacher. Another popular leveling system is the The Lexile Reading Framework, which is usually assessed on the computer. Both types of tests will give you a numerical score which identifies your child’s reading fluency (speed and accuracy of decoding the text) and comprehension compared to grade level expectations.\nIf your child attends school and you are not sure of his or her reading level, contact the classroom teacher, who will likely have this information on file. If your child is home-schooled, you can contact a certified teacher/evaluator through Avenue Education to obtain a complete diagnostic reading assessment.\nWhat does reading level mean?\nThe instructional reading level is the highest level of text your child can read without reaching the point of frustration. It is the optimal level for teaching reading concepts, since the text provides just enough challenge to introduce new concepts without overwhelming the student. Accordingly, children may need help with a few words here and there when reading text at the instructional level. This is different from the independent reading level, which involves easier text that can be read without help. The ideas of instructional and independent levels are derived from a theory of learning called the “Zone of Proximal Development” (Vygotsky, 1978). For example, a Lexile score of 500-550 represents a ZPD ranging from independent (500) to instructional (550).\nHow do I use reading level when buying books?\nBooks for your child’s home reading library should fall somewhere in that ZPD range, depending on how you plan to use them. Books that are slightly above your child’s reading level are great for reading aloud, such as for a bedtime story. Shared reading builds vocabulary and comprehension skills. Independent reading should be fun and stress-free. Scholastic.com/teachers has a great tool called the Book Wizard, which can help you find materials within their inventory according to reading level. Another excellent resource is ARBookFinder, which allows you to search the Accellerated Reader catalog to identify the approximate level of a given book you may already have in mind.\nWhat if I don’t know the level of a book?\nIf you are in the library or bookstore with your child, you can use a simple trick teachers like to call, “The Five Finger Rule.” Your child reads one page from the book aloud. If you count more than five errors on one page, the book is too difficult for them to read alone. If your child makes fewer than five errors, this book is probably a good fit for independent reading. If they make no errors, the book may be too easy. This method is so simple, children can start using it to make good selections for themselves. It can teach them responsibility and self-awareness, plus save you time and frustration!\nMcLeod, S. A. (2012). Zone of Proximal Development. Retrieved from www.simplypsychology.org/Zone-of-Proximal-Development.html\nFor more tips on choosing books for your child:\n""3 Simple Tips for Choosing Age-Appropriate Children’s Books""\nLeveled Reading Sets\nNonfiction books support new Common Core Standards. The books below have articles written on four different reading levels! They include comprehension questions for each article.']"	['<urn:uuid:60c6693f-8c4e-4cbf-876e-295c10808960>', '<urn:uuid:f8b1047c-ea96-4f77-be63-7d8e6d13d175>']	factoid	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-13T03:34:55.284799	8	55	1420
12	How do location and lighting affect urban infrastructure development?	Location and lighting play crucial roles in urban infrastructure development. For lighting, as demonstrated by the Veterans' Glass City Skyway in Toledo, strategic implementation of LED fixtures (200 Lumenpulse Lumenfacade fixtures) can create dynamic displays that boost civic pride and celebrate community events. Regarding location considerations, urban developments must carefully consider factors like operational costs and distribution efficiency - as seen in vertical farming where positioning near distribution centers on city outskirts is often more practical than city center locations, despite the initial appeal of urban centrality.	"['Project detailsProject type: Infrastructure\nLocation: Toledo, Ohio, United States\nClient: Ohio Department of Transportation\nLighting Design: HLB Lighting Design\nElectrical Engineer: FET Construction Services\nCommissioning and Programming: Barbizon Lighting Company\nPhotographer: Feinknopf Photography\nAesthetic lighting, public art, and community pride are interwoven to turn the glass-enclosed tower of Veterans\' Glass City Skyway into an iconic symbol of Toledo using Lumenpulse luminaires.\nWe have a long-standing relationship with Lumenpulse and we know they will stand behind their luminaires.Faith E. Baum\nHorton Lees Brogden Lighting Design\nVeterans’ Glass City Skyway\nThe 400-foot-high, glass-enclosed tower of the cable-stayed bridge known as the Veterans\' Glass City Skyway (VGCS) is an incredible achievement and has become a symbol of Toledo itself. Initially opened in 2007, the single, tall central pylon, which helps suspend twin 612-foot road spans, used 348 LED luminaires to light the tower. To update the lighting system and controls, as well as to allow for a more dynamic lighting of the tower, HLB Lighting Design replaced the older system in 2018 with 200 Lumenpulse Lumenfacade fixtures whose RGBW capabilities are capable of producing a full spectrum of colorful displays. ""We have a long-standing relationship with Lumenpulse,"" said Faith E. Baum, Principal at HLB Lighting Design, ""and we know they will stand behind their luminaires.""\nThe public has been excited about the new lighting design, and it has become a centerpiece of countless social media posts. This project is a rarity amongst infrastructure lighting as it involves a unique collaboration between not only the client and the lighting designer, but also local artists. The community-driven initiative sets this bridge\'s lighting apart from so many others. The primary lighting programs are the works of six artists which have been interpreted into kinetic light scenes. These original designs are played out nightly on the bridge\'s column of glass and Lumenpulse Lumenfacades are the linear projectors responsible for bringing these visions to light. The artists created two or more scenarios each, 10 of which are shown on a rotating basis. Between the artist scenarios, and during special occasions, HLB has designed a number of programs to celebrate local sports teams, to commemorate local and national holidays and events, as well as programs to mark seasonal changes, such as a program that mimicks snowfall.\nThere are 45, four-foot long Lumenfacade fixtures located in each of the glass-enclosed quadrants of the pylon and 16 two-foot fixtures in the tower peak. This allows for granular control and a more precise use of color than the previous design. The control system is located in the neighboring Robert Craig Memorial Bridge. The controls are also accessible remotely from Ohio Department of Transportation\'s (ODOT) district office through a custom-designed user interface. It is from here that continuous diagnostic monitoring of each LED fixture\'s performance can also be carried out.\nThe Lumenfacade luminaires have reduced maintenance costs and increased the flexibility and controls of the bridge\'s lighting, while simultaneously enacting dynamic, color-changing lighting programs that have boosted civic pride. Now a symbol synonymous with Toledo, Baum attributes the success of the project to, ""the new lighting system and the incredible collaboration between local artists, ODOT, HLB and the entire design team. It was a unique opportunity that allowed us to think about the lighting of an iconic bridge in a whole new way.""\n184 x Lumenfacade Color-Changing RGBW, 4 ft, 30º x 60º\n16 x Lumenfacade Color-Changing RGBW, 2 ft, 30º x 60º', 'Vertical Farming: Location a Key Factor to Success, Says IDTechEx\nVertical farming, the practice of growing crops indoors on vertically stacked layers, has received no small amount of interest over the last few years. Vertical farms commonly tout impressive numbers, such as using 95% less water and providing crop yields 20-30 times that of conventional agriculture. These claims, among many others, have seen many vertical farming start-ups being founded alongside large amounts of industry funding; funding for the industry reached a record high in 2021, with over US$1 billion being raised across the entire industry. The recent IDTechEx report, ""Vertical Farming 2022-2032"", details the economic and technological factors shaping this rapidly growing industry.\nWith crops being grown indoors under controlled environments, a selling point used by multiple vertical farms is that they can grow crops anywhere – even in the heart of a city. This has led to proponents of the industry envisioning ""smart cities"", where vertical farms in city skyscrapers help feed the urban population. While this is achievable in principle, the truth is that the choice of location for vertical farming is much more involved and intricate than it may appear from these claims alone. Choosing an ideal location can be one of the most important factors in determining the success of a vertical farm.\nSome vertical farms may choose to set up their facilities in pre-existing facilities, such as abandoned warehouses. In these cases, identifying the suitability of the venue is the first point of consideration: vertical farms are very energy intensive, and it is important to ensure the facilities chosen can support these energy loads. In addition, the ergonomics of the facility is also important; should the layout not be given proper consideration, this can impede workers and decrease worker efficiency. As labor costs are typically among the largest sources of expenditure for a vertical farm, improving labor efficiency to reduce these costs is of paramount importance.\nWhile growing crops in the center of a city may seem ideal, the reality is that this may be counterproductive. Obtaining and maintaining such a location is expensive and can contribute significantly to the operating expenditure of a vertical farm while presenting logistical challenges in distributing produce; the ""last mile"" of food distribution is often the hardest. Having a farm right next to the consumers themselves may also be less ideal than instead choosing a location near food distribution centers, as this allows for more efficient delivery of produce. As distribution centers are typically located on the outskirts of cities, the cost of land is also much cheaper. This is the approach chosen by UK-based Jones Food Company, which chose Scunthorpe as a location for its vertical farm – this is a relatively low-cost location located near food distribution centers and a network of motorways that could still reach many consumers in a day, even if it isn\'t right in the middle of the capital city. Vertical farms should carefully consider their place in the supply chain before establishing a base.\nOn a larger scale, vertical farms may prove more profitable in different geographical regions. Vertical farms can reduce water usage significantly over conventional agriculture, and the high degree of control over the growing environment allows them to grow crops in extreme climates – where such crops may not otherwise be able to grow. In return, vertical farms demand more energy to carry out growing operations. To maximize their potential, vertical farms would ideally be located in regions of water scarcity, such as Sub-Saharan Africa and the Middle East, or in areas with extreme climates, such as in Scandinavian countries, where the low amounts of sunlight and high costs of regulating greenhouse environments single out vertical farms as an optimal solution. The amount of agricultural land available is also an important factor – regions looking to increase food security and reduce reliance on imports while facing challenges in acquiring sufficient agricultural land would find vertical farms to be ideal. A particularly prominent example of such a country is Singapore, which has demonstrated much interest in vertical farming over the last few years.\nBeyond the considerations of water scarcity and temperature, the general availability of fresh produce and the distribution networks of given countries should also be considered. Vertical farms use the added freshness and higher quality of their crops as a primary selling point, but these are typically offset by higher prices. Should there already be a large supply of high-quality produce made available at lower costs, vertical farms will find it hard to distinguish their own produce and may struggle to establish a significant market share. The converse would also be true; should a country lack easy access to fresh produce, vertical farms are expected to see much demand for their produce. An example of such a region would be the Middle East: leafy greens typically travel several thousand miles to reach stores, resulting in consumers facing high prices and low-quality products. The high price of conventionally farmed leafy greens, alongside government subsidies, makes it easier for vertically farmed produce to approach price parity while providing much fresher, higher-quality products.\nWhile the choice of location is an important consideration, it is only one of many others that must be given proper thought. Only through proper optimization of growing operations to improve efficiency and reduce costs can vertical farms reach their true potential. In the IDTechEx report, ""Vertical Farming 2022-2032"", many further important factors for consideration are discussed in detail, and the future of vertical farming is evaluated through 10-year market forecasts.\nIDTechEx guides your strategic business decisions through its Research, Subscription and Consultancy products, helping you profit from emerging technologies. For more information, contact research@IDTechEx.com or visit www.IDTechEx.com.\nThis post does not have any comments. Be the first to leave a comment below.\nPost A Comment\nYou must be logged in before you can post a comment. Login now.']"	['<urn:uuid:cde18ab7-040c-443b-9bf7-e5f47b21e837>', '<urn:uuid:98c0eed7-8d91-42cd-b0d1-cc7c105f9a4d>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T03:34:55.284799	9	87	1551
13	What is the current scientific understanding of how the Higgs field interacts with particles to give them their mass according to the standard model of physics?	The Higgs field is present everywhere around us, and all particles move in its presence. Particles interact with this field to different degrees - heavier particles interact strongly with the field while lighter ones interact very weakly with it. This interaction with the Higgs field is what gives particles their mass by either slowing them down more or less strongly.	"['Scientists said Thursday that they believe they have discovered the elusive Higgs boson particle, sometimes called ""the God particle,"" that is theorized to be a crucial building block of the universe.\nIt is an important development not only because it explains why sub-atomic particles have mass but also because it highlights the value of the collaborative spirit that led to its discovery.\nThe announcement is a step forward from July, according to Philippe DiStefano from Queens University in Kingston, Ont., when physicists working at the Large Hadron Collider near the French-Swiss border said they had found a particle that was ""consistent"" with the Higgs boson.\n""They’re sort of pinning down the properties of the particle a bit better than they did initially,"" says DiStefano, an associate professor of physics and astronomy.\nAccording to a release from the European Organization for Nuclear Research, or CERN, determining whether the discovery was in fact the Higgs boson depends on its quantum properties and how it interacts with other particles. After going through their entire data set from 2012, the results ""strongly indicate"" they have found the elusive particle.\n\'Touches every other particle’\nFirst theorized almost 50 years ago by British physicist Peter Higgs and others, the Higgs boson is the only particle yet to be observed in the Standard Model of Physics, which basically explains how the basic building blocks of matter interact.\n""The Higgs field is everywhere around us, and all particles are moving in the presence of this field,"" said William Trischuk, a professor of physics at the University of Toronto. All these particles ""interact more or less strongly with it, and they are either slowed down or not slowed down so much and that\'s what gives them mass.""\n""The heavier ones interact strongly with this field and the light ones interact very weakly with this field.""\nTrischuk is part of the ATLAS collaboration, which is one of two experiments seeking out the Higgs using the Large Hadron Collider.\n""We hate calling it the God particle but the reason it picked that up is because it goes out and touches every other particle and gives them their property, which is their mass,"" Trischuk said.\nStill, while the Higgs boson is fundamental to the universe itself, it\'s unclear what impact this discovery will have on the average person, DiStefano said.\n""I don’t think that anyone can guarantee what type of outcome on any time scale,"" he said. ""This is fundamental physics so it\'s hard to know in advance what, if any, of the outcomes will be.""\nHowever, other scientific breakthroughs have eventually led to tangible impacts on daily life. For example, positrons, which were discovered in the 1930s, are now used in medical scanners, DiStefano pointed out.\nGathering more data\nThere is more work to be done in terms of nailing down the properties of the Higgs boson.\n\'It shows that the theory that we have of particles and forces is basically correct\' —Itay Yavin, McMaster University\nFor instance, Trischuk said, scientists can go further in trying to determine the mass of the particle.\nCurrently physicists have reported its mass to be either 125 or 126 GeV.\n""In five or 10 years one of the interesting things will be, is it 125.1 or 126.3 GeV? And that may have some consequences for the predictions"" throughout the world of physics.\nThe release from CERN also said it remains ""an open question, however, whether this is the Higgs boson of the Standard Model of particle physics, or possibly the lightest of several bosons predicted in some theories that go beyond the Standard Model,"" according to the release from CERN.\nItay Yavin, assistant professor for theoretical particle physics at McMaster University and the Perimeter Institute in Waterloo, said there are a number of particles that are nearly identical to each other except for their mass. For instance, a muon has nearly the same properties as electrons but a different mass.\nIt is possible that there is more than one Higgs boson, with each affecting different particles, although the Standard Model currently only allows for one.\nHowever, the evidence so far suggests that scientists have in fact found the Higgs boson, which in turn bolsters the Standard Model, Yavin said.\n""Now, scientifically, it shows that the theory that we have of particles and forces is basically correct.""\nFortunately for Higgs boson seekers, the Large Hadron Collider is currently being upgraded over the next two years so it can smash atoms together with more energy, which will help gather more data to delve deeper into the mysteries of the Higgs boson.\nThat higher energy allows for the detection of a possible heavier Higgs particle, if such a thing even exists, along with the one that scientists say they have discovered, Yavin said.\nThe discovery of the Higgs boson also points to the value of the $10-billion Large Hadron Collider and the collaborative spirit behind the massive endeavour to uncover a fundamental building block of the universe, Trischuk said.\n""It was the quest for the Higgs that brought us all together over the last 15 years from five continents and [6,000] people to work together,"" said Trischuk.\nThat sentiment was echoed by Yavin.\n""By collaborating, we can unlock one of deepest mysteries in nature and discover a particle that was predicted on paper some 50 or so years ago,"" he said. ""It\'s really a remarkable testimony about everything that is good about the human spirit, to collaborate and to think deeply about the universe.""\nIn terms of finally settling the question of whether scientists have discovered the Higgs boson with absolute confidence, Trischuk said that may be unlikely and that the answer, instead, might come in the form of successive approximations approaching certainty.\n""But I think from here on out we\'re going to stop calling it a Higgs-like boson and we\'ll call it a Higgs boson.""']"	['<urn:uuid:2c876fd6-c702-4048-9dca-6f75482105db>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T03:34:55.284799	26	60	978
14	processing capacity planned output lithium extraction facility cornwall united downs	The Pilot Plant is designed to process 140 cubic meters of deep geothermal water from GEL's United Downs site testing. It is planned to have a small capacity of 10 tonnes of Lithium Carbonate Equivalent per year, and the pilot results will inform the design of a future commercial lithium plant in Cornwall.	['Exploration rig on site of United Downs Deep Geothermal Project, UK (source: GEL)\nFor an intended direct lithium extraction from geothermal pilot plant at the United Downs Deep Geothermal Task, GeoCubed has picked the French start-up GeoLith to supply its exclusive DLE modern technology.\nUK-based GeoCubed, a joint endeavor between Cornish Lithium Limited as well as Geothermal Design Limited (GEL), previously this month announced that French innovation startup GeoLith SAS has actually been selected to give their Li-Capt ® Direct Lithium Removal (“DLE”) modern technology for GeoCubed’s Pilot Plant. GeoLith’s DLE technology was chosen for usage in the Pilot Plant complying with a comprehensive tender process. The Pilot Plant is because of be appointed at United Downs in Cornwall by the end of March 2022.\nThe GBP4 million Pilot Plant is being supported by the Cornwall and Isles of Scilly Resident Venture Collaboration (“LEP”) with GBP2.9 million from the UK Government’s Obtaining Structure Fund.\nDuring the option procedure, GeoCubed and also Cornish Lithium engaged with over 10 providers of DLE innovations to assess their effectiveness on Cornish geothermal waters. Adhering to the conclusion of these examinations, five service providers formally tendered their DLE modern technology for use in the Pilot Plant. GeoLith’s technology was selected due to the quality of their tender, the performance of their technology, as well as their capacity to layout and also supply a Pilot Plant.\nThe Pilot Plant will be made use of to process the 140 metres3of deep geothermal water effectively obtained during GEL’s current testing at its United Downs site, which will certainly confirm that lithium can be created in Cornwall from geothermal brine. The Pilot Plant is planned to have a small capability of 10 tonnes of Lithium Carbonate Equivalent each year and also the results of the pilot are expected to give adequate info to allow the style of a commercial lithium plant in Cornwall.\nJeremy Wrathall, CEO and also Owner of Cornish Lithium and also a Director of GeoCubed, claimed:“We are pleased to announce that GeoCubed has actually selected GeoLith’s Li-Capt ® modern technology for usage in this Pilot Plant. We have developed a great working connection with the team at GeoLith, that provided a demo plant, together with their operations group, to enable us to test their DLE innovation on our superficial geothermal water samples in June. This test job given excellent outcomes as well as we look forward to dealing with them to develop this Pilot Plant, which positions Cornwall at the center of lithium production, as the UK strives to secure a domestic supply of this vital battery steel.”\nRyan Regulation, MD and Creator of GEL and a Supervisor of GeoCubed, said:“The current geothermal tests at our United Downs website have shown that geothermal power can be created in Cornwall. The consultation of GeoLith is additionally a wonderful step forward for geothermal lithium production in the UK.”\nJean-Philippe Gibaud, CEO and also Founder of GeoLith, claimed:“We are honoured to have actually been selected to supply our lithium filter innovation as the “technological enabler” of this tidy lithium mining job, demonstrating the expediency of sustainable mining for the future. We would like to praise GeoCubed on this initial semi-industrial lithium salt water production center in Europe.”\nMark Duddridge, Chair of the Cornwall and also Isles of Scilly LEP said:“This is a considerable advance for GeoCubed as well as its aspirations to open Cornwall’s low carbon possibility, by helping to power the environment-friendly industrial revolution. We wish the group every success.”\nSource: Firm launch']	['<urn:uuid:0f5cf949-901c-42dc-8b0c-8b0ec7fe2661>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-13T03:34:55.284799	10	53	585
15	As someone involved in survival training, I'd like to understand why some rescue attempts fail even in the final moments. What critical mistakes do survivors make during the rescue and recovery phase?	Rescue attempts can fail due to several critical mistakes made by survivors, particularly during the final moments. The main error is when survivors fail to follow instructions carefully from rescue teams, sometimes resulting in death. People often mistakenly think that once they've been spotted, the rescue is straightforward. However, even when as close as a rescue craft's doors, lives have been lost due to survivors' inability to follow instructions, or through panic and impatience. That's why remaining calm despite relief is crucial, and following any instructions to the letter is essential since rescue crews are highly professional and well-trained.	['Rescue and Recovery\nIn most survival and search and rescue situations where it has been reported that people are missing, the situation is usually resolved within 48 hours. This is because of the training, capabilities and the responsiveness of professional search and rescue teams. However, in order that a survival situation can reach a happy outcome in as short a timescale possible, survivors themselves also have to be aware of their responsibilities both before and during a rescue and recovery situation in order that they are fully prepared for a rescue attempt.\nSignalling PreparationSignalling is dealt with in more depth in another article contained on this website but it’s crucial that you know how to signal for help correctly and to do so in the most appropriate location. That means that as soon as you have got the basic priorities of survival covered, you need to evaluate the environment surrounding you and look for possible landing zones, drop zones or suitable places in which boats can moor up or at least get close to you. In fact, once your basic survival needs are met, you may even wish to relocate your position to a nearby but more suitable place for a rescue attempt if it is practical for you to do so, before you start attempting to signal for help. However, you need to weigh this up against any information you have left with people back home as if you have told them your destination and you stray too far from it, your rescuers may not know where to look as they will tend to start searching from your last known position and then work outwards from there.\nHow You Can Tell You’ve Been SpottedUnless you’re the sort of person who comes fully ‘tooled-up’ and you have an emergency radio on you, rescuers are only too aware that most people facing a survival situation have no means of direct communication with their rescuers but there are some signs which they can give to let you know that you’ve been spotted and that help is at hand. A ship might sounds its horn or might be able to use a searchlight or some similar device to send you a signal to let you know they’ve seen you and a light aircraft will usually raise and lower its wings in a kind of rocking motion. This is an indication that they have spotted you and you shouldn’t panic if they then leave the area. This might simply mean that they are low in fuel and need to refuel or it may mean they are summoning help from a helicopter to complete the rescue mission, so you should not give up hope and should simply stay put.\nPreparing for the Rescue AttemptIn the event of a rescue attempt being made by a helicopter or light aircraft, you need to ensure that anything that you have lying on the ground which could get sucked up by the rotor blades or inside an aircraft engine are removed out of harm’s way. This might include tents, tarps, ropes, blankets etc. You also need to keep a safe distance from any landing area if you’ve been told that the aircraft is going to land.\nStaying Calm and the Importance of Following OrdersMany people who have never faced a survival, rescue and recovery situation before often make the fatal error of thinking that once a rescue team has spotted them, then it’s all a matter of ‘plain sailing’. However, you would be foolish to think this and many rescue attempts have had to be aborted due to the inability of those being rescued to follow instructions carefully and, in some cases, not following instructions given by the rescue teams has resulted in death.\nTake a helicopter, for example. It’s not always as simple as the chopper landing on an open patch of ground and then all the survivors simply clambering into it. Depending on the terrain and environment, a helicopter might have to use a horse collar, a winch, a basket or even a rope to lift you from the ground. And, if you don’t follow instructions to the letter which the crew will give you prior to the rescue attempt, it could end up being disastrous. Neither should you panic. Despite your relief, it’s important that you remain as calm as possible and follow any instructions to the letter. After all, the rescue crews do this for a living and are highly professional and well-trained. By ignoring their instructions, you are placing yourself at risk of severe injury or even possibly death. This is as equally as important if a boat or ship is providing the rescue – simply pay heed to the instructions which you might be given. People have even got as close to safety as a rescue craft’s doors but have still been lost due to their inability to follow instructions or through panic or impatience.\nIf some device is being used to bring you into an air or sea craft, do not let go of the device even though you think you might be safe. Always wait for a rescue technician to remove you from the device. A split second too soon can mean the difference between survival and death.\nBy using common sense, understanding the mechanisms of how a rescue and recovery takes place, preparing for the rescue and following instructions from the rescue crew, you’ll be able to complete the final stage of survival - the rescue and recovery stage which is often the most challenging and difficult.']	['<urn:uuid:84b12e47-63af-4965-8b85-56c0dad8a728>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T03:34:55.284799	32	99	927
16	algorithms textbook cormen vs mathematics sampler berlinghoff differences	These are two different textbooks with distinct approaches. Introduction to Algorithms by Cormen focuses specifically on algorithms, their complexity analysis, and technical aspects like dynamic programming and greedy algorithms. In contrast, A Mathematics Sampler presents mathematics more broadly as both science and art, covering diverse topics from number theory to four-dimensional geometry, with unique LINK sections connecting mathematics to humanities.	"['Welcome to my page of solutions to “Introduction to Algorithms” by Cormen, Leiserson, Rivest, and Stein. It was typeset using the LaTeX language, with most . Introduction to Algorithms. by Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. PREFACE · CHAPTER 1: INTRODUCTION. This edition is no longer available. Please see the Second Edition of this title.\n|Published (Last):||22 March 2017|\n|PDF File Size:||6.84 Mb|\n|ePub File Size:||7.61 Mb|\n|Price:||Free* [*Free Regsitration Required]|\nThis biography of a living person needs additional citations for verification.\nIntroduction to Algorithms, Second Edition – PDF Drive\nDirector of the Dartmouth College Writing Program However, there are occasions when outside help can be beneficial. This biographical article relating to a computer ccormen in the United States is a stub.\nReview quote “As an educator and researcher in the field of algorithms for over two decades, I can unequivocally say that the Cormen et al book is the best textbook that I have ever seen on this subject.\nParticipation means coming to class, asking questions, taking part in discussions, not falling asleep, and so on. Between and he directed the Dartmouth College Writing Program. Retrieved from ” https: After 2 days the assignment will be considered a zero.\nThomas H. Cormen\nMany new exercises and problems have been added for this edition. Book ratings by Goodreads. Archived from the original PDF on June 6, Due Thursday, May 2 Change in grading policy: The second edition featured new chapters on the role of algorithms, probabilistic analysis and randomized algorithms, and linear programming.\nContentious material about living persons that is unsourced or poorly sourced must be removed immediatelyespecially if potentially libelous or harmful.\nI strongly recommend that you buy the text rather than borrow it; this is one of dormen two text books that I still use on a regular basis.\nDue Monday, April 22 Homework 7: The final grade will be calculated as a weighted average: This page was last edited on 28 Decemberat However, all homework problems assigned from the book will be referenced from the second edition; it is your responsibility to find a way to look them up.\nAugust Learn how and when to remove this template message. If you can recall the solution from cormeh, you probably understand it.\nIntroduction to Algorithms : Thomas H. Cormen :\nVisit our Beautiful Books page and find lovely books for kids, photography lovers and more. CS Library, Olsson Introduction, administration, time and space complexity.\nSend us anonymous feedback on toolkit. Due Wednesday, February 13 Homework 4: Due Friday, April 12 Homework 6: Change in grading policy: The third edition has been revised and updated throughout. It features improved treatment of dynamic programming xaa greedy algorithms and a new notion of edge-based flow in the material on flow networks.\nYou can help Cormsn by expanding it. Inhe published a new book titled Algorithms Unlocked. It offers an incisive, encyclopedic, and modern treatment of algorithms, and our department will continue to use it for teaching at both the graduate and undergraduate levels, as well as a reliable research reference. Check out the top cormenn of the year on our page Best Books of Goodreads is the world’s largest site for readers with over 50 million reviews.\nIt is an indispensable reference.\nIntroduction to Algorithms\nIntroduction to Algorithms uniquely combines rigor and comprehensiveness. The “CULTURE” topics represent interesting but non-essential material from fields such as computational geometry and computer graphics; they add some variety to the schedule but also give us some slack if we get behind schedule.\nDue Friday, March 9 Some sample code Homework 5: We will also analyze algorithm complexity throughout, and touch on issues of tractibility such as “NP-Completeness”.\nViews Read Edit View history. Retrieved 2 September', 'Ships same day or next business day via UPS (Priority Mail for AK/HI/APO/PO Boxes)! Used sticker and some writing and/or highlighting. Used books may not include working access ...code or dust jacket.Read moreShow Less\nNow in its fifth edition, A Mathematics Sampler presents mathematics as both science and art, focusing on the historical role of mathematics in our culture. It uses selected topics from modern mathematics—including computers, perfect numbers, and four-dimensional geometry—to exemplify the distinctive features of mathematics as an intellectual endeavor, a problem-solving tool, and a way of thinking about the rapidly changing world in which we live. A Mathematics Sampler also includes unique LINK sections throughout the book, each of which connects mathematical concepts with areas of interest throughout the humanities. The original course on which this text is based was cited as an innovative approach to liberal arts mathematics in Lynne Cheney\'s report, ""50 HOURS: A Core Curriculum for College Students"", published by the National Endowment for the Humanities.\nWilliam P. Berlinghoff is visiting professor of mathematics at Colby College. Kerry E. Grant is professor of mathematics at Southern Connecticut State University. Dale Skrien is professor of computer science at Colby College\nChapter 1 Preface Chapter 2 To the Student Part 3 Chapter 1: Problems and Solutions Chapter 4 What Is Mathematics? Chapter 5 Problem Solving Chapter 6 It All Adds Up Chapter 7 The Mathematical Way of Thinking Chapter 8 Topics for Papers Chapter 9 For Further Reading Part 10 Chapter 2: Mathematics of Patterns: Number Theory Chapter 11 What Is Number Theory? Chapter 12 Divisibility Chapter 13 Counting Divisors Chapter 14 Summing Divisors Chapter 15 Proper Divisors Chapter 16 Even Perfect Numbers Chapter 17 Mersenne Primes Chapter 18 LINK: Number Theory and Cryptography Chapter 19 Topics for Papers Chapter 20 For Further Reading Part 21 Chapter 3: Mathematics of Axiom Systems: Geometries Chapter 22 What is Geometry? Chapter 23 Euclidean Geometry Chapter 24 Euclid and Parallel Lines Chapter 25 Axiom Systems and Models Chapter 26 Consistency and Independence Chapter 27 Non-Euclidean Geometries Chapter 28 Axiomatic Geometry and the Real World Chapter 29 LINK: Axiom Systems and Society Chapter 30 Topics for Papers Chapter 31 For Further Reading Part 32 Chapter 4: Mathematics of Chance: Probability and Statistics Chapter 33 The Gamblers Chapter 34 The Language of Sets Chapter 35 What Is Probability? Chapter 36 Counting Processes Chapter 37 LINK: Counting and the Genetic Code Chapter 38 Some Basic Rules of Probability Chapter 39 Conditional Probability Chapter 40 LINK: Probability and Marketing Chapter 41 What Is Statistics? Chapter 42 Central Tendency and Spread Chapter 43 Distributions Chapter 44 Generalization and Prediction Chapter 45 LINK: Statistics in the Psychology of Learning Chapter 46 Topics for Papers Chapter 47 For Further Reading Part 48 Chapter 5: Mathematics of Infinity: Cantor\'s Theory of Sets Chapter 49 What Is Set Theory? Chapter 50 Infinite Sets Chapter 51 The SIze of N Chapter 52 Rational and Irrational Numbers Chapter 53 A Different Size Chapter 54 Cardinal Numbers Chapter 55 Cantor\'s Theorem Chapter 56 The Continuum Hypothesis Chapter 57 The Foundations of Mathematics Chapter 58 LINK: Set Theory and Metaphysics Chapter 59 Topics for Papers Chapter 60 For Further Reading Part 61 Chapter 6: Mathematics of Symmetry: Finite Groups Chapter 62 What Is Group Theory? Chapter 63 Operations Chapter 64 Some Properties of Operations Chapter 65 The Definition of a Group Chapter 66 Some Basic Properties of Groups Chapter 67 Subgroups Chapter 68 Lagrange\'s Theorem Chapter 69 Lagrange\'s Theorem Proved [Optional] Chapter 70 Groups of Symmetries Chapter 71 LINK: Groups in Music and in Chemistry Chapter 72 Topics for Papers Chapter 73 For Further Reading Part 74 Chapter 7: Mathematics of Space and Time: Four-Dimensional Geometry Chapter 75 What Is Four-Dimensional Geometry? Chapter 76 One-Dimensional Space Chapter 77 Two-Dimensional Space Chapter 78 Three-Dimensional Space Chapter 79 Four-Dimensional Space Chapter 80 Cross Sections Chapter 81 Cylinders and Cones [Optional] Chapter 82 LINK: 4-Space in Fiction and in Art Chapter 83 Topics for Papers Chapter 84 For Further Reading Part 85 Chapter 8: Mathematics of Connection : Graph Theory Chapter 86 What Is Graph Theory? Chapter 87 Some Basic Terms Chapter 88 Edge Paths Chapter 89 Vertex Paths Chapter 90 Crossing Curves Chapter 91 Euler\'s Formula Chapter 92 Looking Back Chapter 93 LINK: Diagraphs and Project Management Chapter 94 Topics for Papers Chapter 95 For Further Reading Part 96 Chapter 9: Mathematics of Machines: Computer Algorithms Chapter 97 What Is a Computer? Chapter 98 The Traveling Salesman Problem Chapter 99 The Speed of a Computer Chapter 100 Algorithms and Sorting Chapter 101 Comparing Algorithms Chapter 102 Complexity Analysis Chapter 103 NP-Completeness Chapter 104 Implications of NP-Completeness Chapter 105 LINK: Algorithms, Abstraction, and Strategic Planning Chapter 106 Topics for Papers Chapter 107 For Further Reading Part 108 APPENDICES Part 109 Appendix A: Basic Logic Chapter 110 Statements and Their Negations Chapter 111 Conjunctions and Disjunctions Chapter 112 Conditionals and Deduction Chapter 113 Topics for Papers Chapter 114 For Further Reading Part 115 Appendix B: A Brief History of Mathematics Chapter 116 Preliminary Thoughts Chapter 117 From the Beginning to 600 B.C. Chapter 118 600 B.C. to A.D. 400 Chapter 119 400 to 1400 Chapter 123 The Fifteenth and Sixteenth Centuries Chapter 124 The Seventeenth Century Chapter 125 The Eighteenth Century Chapter 126 The Nineteenth Century Chapter 127 The Twentieth Century Chapter 128 Topics for Papers Chapter 129 For Further Reading Part 130 Appendix C: Literacy in the Language of Mathematics']"	['<urn:uuid:dbf91f6d-75c6-4191-9684-519e283affc6>', '<urn:uuid:363be3d1-b244-4778-bfb1-820a6f4a7443>']	open-ended	direct	short-search-query	similar-to-document	comparison	expert	2025-05-13T03:34:55.284799	8	60	1529
17	How does cryogenic processing affect indium bump formation and metal growth?	For indium bump formation, a cryo-cooled chuck is needed to create small diameter bumps (less than 10 µm) with high aspect ratios, which creates good shear strength and crystal structure. However, this can cause yield loss due to lack of bump adhesion. In terms of metal growth, when ferrous metals are cryogenically processed, they grow in size, which can affect tolerance. This growth is most significant in poorly heat-treated parts and less significant in properly heat-treated parts, but all parts will experience some growth.	"['To form indium bumps, a patterned mask is applied to the wafer and then indium is deposited. The patterned mask has “holes” that allow some indium to deposit directly onto the wafer, while the rest of the indium is deposited onto the mask. After indium bump deposition, the unwanted indium and the mask must be removed. This is known as lift-off.\nIf the indium is not deposited properly, indium bumps could be unintentionally removed. This results in dead pixels or unmade connections on the focal plane array. However, when those connections are ensured, device yield greatly improves.\nBalancing Bump Quality with High Pixel Density\nPatterned coatings are an important requirement for forming interconnects in flip-chip micro arrays. Using indium as the coating material, the indium is deposited on an array of under-bump metallization (UBM) layers, which connect through a sensor device. A similar process is repeated on a controller device. The indium bumps form interconnects between the sensor and the controller to create a single photodetector device.\nThere is increasing demand for improved bump quality with high aspect ratio arrays, to meet requirements for higher pixel densities in newer applications with smaller feature sizes. These tight requirements make it more difficult to form good bumps during deposition. With good bump formation, the lift-off process will reveal high quality bumps that deliver high device yield.\nIndium can be deposited at room temperature, but to create small diameter bumps (e.g., less than 10 µm) with a high aspect ratio (above 1:2 height to width), you need to use a cryo-cooled chuck. The cryo-chuck creates good shear strength and crystal structure, but adding this to your process can introduce new yield loss due to lack of bump adhesion. These missing bumps will result in dead pixels on the photodetector device.\nTo find the right balance between good bump formation and adhesion, you need to optimize your process to improve lift-off yield.\nWhat is the Lift-Off Process?\nThis refers to the process of removing the masking agent (typically a photoresist) as well as the unwanted indium, leaving behind the bump arrays across the photo detector device. This is what ultimately creates the pixels for focal plane arrays, after the indium is actually deposited. Poor adhesion to the under-bump metallization lead to poor lift-off, which negatively affects device yield.\nHow Does Pre-Cleaning Improve Lift-Off?\nPre-cleaning is a thin film deposition process that can greatly improve adhesion and lead to much better lift-off. It is performed before indium bump deposition, and it is done to ensure that the bump formations remain intact after lift-off, even at cryogenic temperatures.\nAs the name implies, during pre-cleaning, the surface of the substrate is thoroughly cleaned to remove any microscopic particles or molecules that might come between the substrate surface and the coating, such as water. At cryogenic temperatures, the water is much more likely to stick to the substrate during deposition, so the pre-cleaning step is imperative to removing these molecules and ensuring good adhesion.\nDenton Vacuum’s research has shown significant improvements in bump formation and device yield when pre-clean is performed before indium bump deposition. Despite using the same photoresist process and being deposited under the same conditions, the data clearly shows that without pre-clean, there were a number of bumps missing from the array after lift-off, and with pre-clean, the bumps stayed intact.\nThere are a number of considerations to take into account before selecting a particular pre-clean method. Denton’s proprietary pre-clean is integrated into our application-specific indium bump system, and provided as part of our integrated indium bump solution. You can read more about attaining good bump formation during indium bump deposition in our infographic.', ""October 25, 2001\nWhile cryogenics has been around for awhile, alot of shops don't know how to use the process to their advantage. Knowing a few basics may help your shop turn that around.\nCryogenics is a branch of physics dealing with subzero, or deep-cold, treatment.\nCryogenic treatment of metals is not a new process. However, its use as a method to improve tool wear characteristics is still considered to be a relatively new engineering field. The cryogenic process produces considerably longer wear life for most heat-treated tools or parts that are subject to wear and abrasion.\nThe key factor for a stamping company or stamping tool expert is that this process should extend tooling life and greatly reduce the need to resharpen dies and downtime.\nThe following are some areas to explore if you are considering using cryogenics.\nFor heat-treated ferrous metals, cryogenics completes the heat-treating process. It continues transforming the metal's grain structure (retained austenite) into a usable, life-extending structure (martensite). Although carbides are not heat-treated ferrous metals, cryogenics affects their bonding materials, causing a tighter grip on the carbides and thus preventing premature carbide losses, called fluffing off.Cryogenics also is a stress reliever. Although often forgotten and not taught in tech schools today, subzero cold is an effective stress reliever for ferrous and nonferrous metals. The process doesn't discolor or cause oxidation on the surface of the metal.\nUsing cryogenics on existing dies is not a good idea. Whenever a ferrous metal is heat-treated and cryogenically processed, it grows in size. The growth can throw stamping dies far enough out of tolerance to make them unusable.\nIf you want to gain the extended wear that cryogenics can offer, cryogenically treat the existing die components after heat-treat and tempering and before final grinding or electrical discharge machining (EDM). Be aware that dowel pinholes, or any features added before processing, may not line up after processing. The amount of growth is not predictable but will be most significant in poorly heat-treated parts and less significant in properly heat-treated parts. But be assured they all will grow.\nOnce tooling has been cryogenically treated, subsequent cryogenic treatment used for nondiscoloring stress relief will not cause growth, unless the component is annealed and reheat-treated for design changes.\nCan higher-cost tool steels be replaced with lower-cost, easier-to-machine tools that have been cryogenically processed?\nThis is entirely possible for some applications. However, it is very hard to change the solid rules of selection.\nEvery application must be evaluated separately. The parameters then can be evaluated to choose the right steel grade for that specific application.\nCareful selection of the metal grade and heat-treatment procedure will produce the optimal metal properties.\nIt is quite common for toolmakers to make a part from, for example, A2 tool steel and then temper the part to 54 to 56 Rockwell hardness C to add toughness. They should instead consider using a shock-resistant steel such as S7 to do the job.\nTool designers and makers need to understand the importance of using the right metal, based on the parameters of the application. It's vital to get all the facts, know about the tool steels, and make educated choices. Don't use a steel because Gramps did it that way or because it's the only steel in stock.\nThe life of a cryogenically processed die is not more predictable than an unprocessed die. A tool's increased wear resistance can't be predicted accurately, either by grade or by application. The controlling factor still depends greatly on how well the heat-treatment process was performed.\nOn occasion, some cryogenically treated tools don't show wear resistance increases because the manufacturers treat their tools after heat treatment. Others use cryogenics to correct bad heat-treatment processes.\nTools will not get brittle after being processed if they are tempered after cryogenics is performed and are temper-soaked long enough to stabilize the fresh martensite formation. The posttemper needed should be only 300 degrees F for two hours for each inch of thickness.\nProcessing companies may stack tools up during processing, causing the tools to be only partially tempered on the outer edges of the stack. Many poorly made processors have onboard heating elements used to temper the load after cryogenic processing. This is the main contributing factor to lack of air space around the parts and uneven tempering of the entire load. Plus, onboard heat often creates oxidation on the surface, which ruins the immediate effect of increased wear resistance until the tool is resharpened.\nYes. Small, easy-to-operate processors are on the market for use in the tooling department or an in-house heat-treat operation. These processors have programmable controllers that run the operation from start to finish, using your own tempering furnace for the final critical step.\nCryogenics is not a cure-all. For heat-treated ferrous metals, cryogenics is simply the continuation of the heat-treat process. It can increase wear resistance by transforming austenite to fine-grained martensite. Plus, if the alloy and carbon content are supportive, a profusion of fine carbides will be precipitated, yielding increased wear.\nCaveat emptor! If you're about to get involved in cryogenics, investigate the company you're considering using.\nIf the process will be done in-house, first view the application. If a stamping die is wearing too fast or breaks often, find out why. It's just like being a detective or a doctor. It's imperative to make an accurate diagnosis before prescribing a treatment. Look at the heat-treatment process that was used. Was it the right recipe? Was the recipe followed, or was a short cut taken?\nSimply put, cyrogenics can improve the life of your tools, but make sure to ask the proper questions before making a choice.\nSTAMPING Journal® is the only industrial publication dedicated solely to serving the needs of the metal stamping market. In 1987 the American Metal Stamping Association broadened its horizons and renamed itself and its publication, known then as Metal Stamping. Print subscriptions are free to qualified stamping professionals in North America.""]"	['<urn:uuid:700b70a8-5180-4ab5-a3df-4187c59d65a4>', '<urn:uuid:e8cabe57-89d7-4d83-b011-b47c3bdb7281>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T03:34:55.284799	11	84	1602
18	nutrient deficiency bone disease symptoms differences	Vitamin D deficiency and related bone diseases present distinct symptom patterns. Vitamin D deficiency typically manifests as fatigue, muscle weakness, joint and muscle pain, depression, and impaired wound healing. When it progresses to osteomalacia, symptoms include bowing legs, dull pain in back/hips/legs/ribs, muscle weakness in legs, and difficulty walking. In contrast, osteoporosis initially has few noticeable symptoms but later presents with back pain, loss of height, hunched posture, and frequent bone fractures. While both conditions can lead to bone breaks, osteomalacia specifically causes bone softening and shape changes, while osteoporosis results in brittle, easily breakable bones.	['Osteoporosis vs Osteomalacia\nWhen you are dealing with new or worsening physical health symptoms, you want answers immediately. At times, the uncertainty surrounding your condition can be more stressful than the actual symptoms. With so many diseases and disorders sharing similar symptoms, it can seem impossible to know what ailment is responsible for your experience.\nThe debate between osteoporosis and osteomalacia is a great example of this struggle to identify the correct condition. These disorders of the bones are frequently mistaken for each other, resulting in misunderstandings and misdiagnoses.\nTo better understand your symptoms, it is valuable to gain an understanding of the conditions that could be affecting your life.\nOsteoporosis is a condition that causes your bones to become weak and brittle. Though they seem static, bones are constantly being broken down and rebuilt within the body. With osteoporosis, the body does not do a good job of replacing and repairing the bones that are broken down.\nWhen bones are affected by osteoporosis, even the most minor bump or fall can lead to a break. The hips, wrists and spine are at the greatest risk for fractures as osteoporosis develops.\nOsteoporosis is tricky to identify early on because the condition begins with few noticeable symptoms. As time goes on, you may experience signs of osteoporosis such as:\n- Back pain\n- Shrinking or loss of height\n- Appears hunched or stooped over\n- Numerous broken bones\nOsteomalacia, on the other hand, is a completely different condition that also affects the bones. In osteoporosis, the bones become brittle, while in osteomalacia, the bones become soft.\nAs the bones soften, they may begin to change shape and bow. This bowing is especially common in larger bones that support weight, like in the legs. In older people, the softening bones of osteomalacia can result in bone breaks, just like in people with osteoporosis.\nThe main cause of this condition is vitamin D deficiency. When there is an extreme lack of vitamin D in the body, the bones cannot sustain the expected strength and durability.\nLike with osteoporosis, people can have osteomalacia for some time without experiencing any obvious symptoms. The most common symptoms of osteomalacia include:\n- Bowing legs\n- A dull pain in the back, hips, legs or ribs\n- Muscle weakness in the legs\n- A changing way of walking that is slower and more difficult\nOsteoporosis vs Osteomalacia Causes\nThese two conditions share many similarities, but the causes of each are quite different. With osteoporosis, the cause is linked to the natural breakdown of bone in the body associated with aging.\nFrom birth until the early 20s, the body is making more bone than it breaks down, leading to thick, strong bones. Then, after age 30, the body begins to break down more than it can renew. Therefore, people who were unable to make sufficient bone in their early life, will have a greater risk of developing osteoporosis.\nOsteomalacia, on the other hand, has more to do with your body’s absorption of essential vitamins and minerals, or your body’s ability to use them effectively. The most common causes of osteomalacia include issues that prevent the body from obtaining the needed levels of vitamin D:\n- Stomach or digestive surgeries: when the normal flow of digestion is interrupted, the body cannot absorb and use the needed level of vitamin D.\n- Celiac disease: having a sensitivity or allergy to gluten damages the walls of the intestine, making it impossible to absorb the vitamin D and calcium required to maintain bone health.\n- Kidney and liver disorders: these organs are key to the body’s ability to properly use vitamin D, so when they are not operating well, the key ingredient for bone health cannot be activated.\n- Certain medications: some medications your doctor may prescribe to treat seizures can disrupt the body’s ability to use vitamin D.\nOsteoporosis vs Osteomalacia Treatments\nOsteomalacia may be a complicated condition, but its treatment is straightforward. A simple vitamin D supplement can often be enough to combat the condition. Other times, your doctor may add calcium and phosphorus to make the vitamin D more effective.\nOsteoporosis treatment is slightly more complex. Depending on your level of bone loss, your treatment team may offer options like:\n- Biophosphonates: the most common and widely used group of medications for osteoporosis that includes drugs like Boniva, Reclast and Fosamax.\n- Monoclonal antibody medications: a drug like Prolia or Xgeva is offered as an injection every six months and can improve bone density.\n- Hormone therapy: bone loss is linked to a decrease in estrogen and testosterone, so starting hormone therapy as levels decrease can help maintain bone health.\n- Other bone-building medications: for people who cannot use other treatments, there is a group of new medications that build bone in novel ways. Be sure to check with your doctor to see if any new medications are right for you.\nOsteoporosis vs osteomalacia: these are two separate conditions, both with the ability to significantly impact your bones and overall well-being. Do your best to understand the disorders and their ability to affect your life, and consult with your doctor to minimize their influence.', 'Vitamin D is an essential nutrient that is crucial for our overall health and well-being. It plays a vital role in maintaining strong bones, boosting immunity, regulating the absorption of calcium and phosphorus, and protecting against diseases. Despite its importance, many people suffer from vitamin D deficiency, which can lead to a range of health problems. In this article, we will discuss the symptoms, causes, and treatments of vitamin D deficiency.\nWhy is Vitamin D so Important?\nVitamin D is a crucial nutrient for maintaining good health and well-being. Despite its importance, many people are deficient in this nutrient, which can lead to a range of health problems. In this article, we will explore why vitamin D is so important and what role it plays in our bodies.\nRegulates Calcium and Phosphorus Absorption\nOne of the most important functions of vitamin D is to regulate the absorption of calcium and phosphorus, two minerals that are essential for strong bones. Vitamin D helps the body absorb these minerals from the gut and distribute them to the bones, where they are stored. If the body doesn’t get enough vitamin D, calcium and phosphorus levels may drop, leading to weak and brittle bones, a condition known as osteomalacia.\nBoosts Immune System\nVitamin D plays a crucial role in boosting the immune system, helping the body fight off infections and diseases. The nutrient activates the immune system’s T-cells, which are responsible for recognizing and destroying harmful pathogens. It also helps regulate the production of cytokines, proteins that help coordinate the immune response.\nReduces Risk of Chronic Diseases\nStudies have shown that vitamin D may help reduce the risk of chronic diseases, such as heart disease, diabetes, and certain types of cancer. The nutrient is believed to play a role in regulating cell growth and division, and may also help protect against oxidative stress and inflammation, two factors that contribute to the development of chronic diseases.\nImproves Mood and Mental Health\nVitamin D is also important for maintaining good mental health and a positive mood. The nutrient is involved in the production of neurotransmitters, such as serotonin, which play a role in regulating mood and emotions. Low levels of vitamin D have been linked to depression, anxiety, and other mental health conditions.\nSymptoms of Vitamin D Deficiency\nVitamin D deficiency can present itself in a number of ways, and the symptoms may not be obvious at first. Some of the most common signs of vitamin D deficiency include:\n- Fatigue and tiredness\n- Muscle weakness\n- Pain in bones and muscles\n- Depression and mood swings\n- Decreased bone density\n- Impaired wound healing\n- Increased risk of infections\nCauses of Vitamin D Deficiency\nThere are several factors that can contribute to vitamin D deficiency, including:\n- Lack of sun exposure: Vitamin D is produced by the body when the skin is exposed to the sun’s UV rays. If you spend most of your time indoors or live in a place with limited sun exposure, you may be at risk of vitamin D deficiency.\n- Dark skin: Melanin, the pigment that gives skin its color, reduces the skin’s ability to produce vitamin D from sun exposure. This means that people with darker skin are at a higher risk of deficiency.\n- Aging: As we age, our skin becomes less efficient at producing vitamin D, which can lead to a deficiency.\n- Obesity: Vitamin D is stored in fat, and obese individuals may have lower levels of vitamin D in their body.\n- Malabsorption: Certain medical conditions, such as Crohn’s disease and celiac disease, can interfere with the body’s ability to absorb vitamin D, leading to a deficiency.\nTreatments for Vitamin D Deficiency\nThe treatment for vitamin D deficiency depends on the severity of the deficiency and the underlying cause. Here are some of the most common treatments:\n- Sun exposure: Spending time in the sun is the best way to get vitamin D. When your skin is exposed to the sun’s UV rays, it produces vitamin D. Aim to spend 10-15 minutes in the sun each day, especially between 10 AM and 3 PM, when the sun’s rays are the strongest.\n- Supplements: If you can’t get enough vitamin D from food and sun exposure, consider taking a supplement. Vitamin D supplements are available in different forms, including tablets, liquids, and gummies. Your doctor can recommend a supplement that’s right for you.\n- Diet: Fatty fish, such as salmon, mackerel, and tuna, are good sources of vitamin D, as are egg yolks and mushrooms. Some dairy products, such as milk and yogurt, are fortified with vitamin D, and can help increase your levels of the nutrient.\n- Fortified foods: Many foods, such as breakfast cereals and dairy products, are fortified with vitamin D, which can help increase your levels of the nutrient.\nIn conclusion, vitamin D deficiency can lead to a range of health problems, including bone weakness, fatigue, and depression. If you suspect you have a deficiency, it’s important to speak with your doctor, who can diagnose and treat the problem. By following a healthy diet, getting regular sun exposure, and taking vitamin D supplements, you can help keep your levels of the nutrient in check and maintain good health.\n- What are the symptoms of Vitamin D deficiency?\nThe symptoms of Vitamin D deficiency can include fatigue, muscle weakness, joint and muscle pain, and depression. In severe cases, Vitamin D deficiency can also lead to osteoporosis and increased risk of fractures.\n- What causes Vitamin D deficiency?\nVitamin D deficiency can be caused by a number of factors, including a lack of exposure to sunlight, a diet that is low in Vitamin D, certain medical conditions that limit the body’s ability to absorb Vitamin D, and certain medications that can interfere with Vitamin D metabolism.\n- How is Vitamin D deficiency treated?\nVitamin D deficiency is usually treated with supplements, either in the form of oral tablets or injections. The recommended daily dose and duration of treatment will depend on the individual’s unique needs and will be determined by their healthcare provider. It is also important for individuals with Vitamin D deficiency to make lifestyle changes, such as increasing their exposure to sunlight and consuming a diet that is rich in Vitamin D, to help prevent future deficiencies.']	['<urn:uuid:87265ca6-f14a-4e42-9524-1fadb2444378>', '<urn:uuid:7b91b940-5deb-488e-a808-a8af0db6f6c0>']	open-ended	direct	short-search-query	distant-from-document	three-doc	expert	2025-05-13T03:34:55.284799	6	96	1919
19	What makes the Santa Lucia Highlands special for growing grapes?	The Santa Lucia Highlands features well-drained southeastern slopes combined with cool, moist winds from the bay, creating conditions that produce wines with complex and subtle textures.	['Wines from the Santa Lucia Highlands\nWinery: First cousins, Christian Tudor and Dan Tudor started Tudor Wines in 2000, from an ancestry including generations of wine grape-growers on the island of Hvar off Croatia’s Dalmatian Coast. The family began growing grapes in California early in the 1900′s and continues to farm premium table grapes in Delano to this day. Fruit is from family owned vineyards and transformed into wine using traditional techniques, including small fermentations mixed by hand and aged in French barrels. 4,000 case annual production.\nWinemaking: Winemaker Dan Tudor begins by using only grapes from the finest sites. Perfect balance of tannins, flavor, alcohol, acid and fruit is the goal. Only the finest three year dried French oak barrels are used to complement the wines without over-powering the elegance and finesse. Wines are de-stemmed, cold-soaked, bottled unfined and unfiltered. Single vineyard wines are selected by individual barrels, and are free-run juice. Larry Brooks was original consulting winemaker. Winery located in Paso Robles.\nVineyards: Tondre Grapefield is the source of most of Tudor wines, located in the center of the SLH appellation, with the Hook Vineyard Pinot Noir from further south in the SLH. The well-drained southeastern slopes of the Santa Lucia Highlands combine with cool, moist winds from the bay to create wines with complex and subtle textures. Each row is farmed to very exact specifications in terms of water, canopy, leaf volume, fruit thinning, and shoot positioning. Individual rows are harvested at peak ripeness, not under-ripe and never over-ripe. 2.5 to 3.5 tons per acre is targeted. Picking is determined by tasting the skins of the berries, examining seed color, grape pulpy-ness, pulp separation from the seeds, the color is released into the juice when bunches are crushed, the firmness of the berries in the morning, the vine stress level, and lastly the pH, TA and Brix of the fruit.\nNacina Off-Dry Riesling, Tondré Grapefield, Santa Lucia Highlands 2016 New Vintage 12.0% alcohol\nWinery Notes: “Astonishingly fresh with intense yet delicate notes of melon, honey, varietally-correct petrol and well-spiced ripe grapefruit. Enormous concentration, plush flavors of ripe apricot and pear, perfectly balanced by acidity and a long finish.” Stainless fermented at low temperature, made to be off-dry at 1.5% RS, in the halbtröcken style. This racy style of Riesling continues to develop with bottle aging. 825 cases made.\nNacina Dry Riesling, Tondré Grapefield, Santa Lucia Highlands 2014 Nearly Sold Out 13.0% alcohol\nWinery Tasting Notes: “This classic Dry German styled Riesling has mineral nose, hints of ripe citrus, pear tart, apple and white plum. The vibrant finish is lush, with creamy notes leading to dried ginger accents on the finish.” Low-temperature, stainless tank- fermented. 200 cases made.\nChardonnay, Highlands Project, Santa Lucia Highlands 2015 New Vintage1\nWinery Notes: “Full-bodied yet elegant with loads of racy mineral, honey, ripe pear, and lemon. The fresh citrus and floral flavors combine with silky, creamy structure to provide incredible complexity.” Boekenoogen Vineyard. Made with No Oak and No Malolactic fermentation. Made in a style that showcases the varietal, unmasked by winemaking flourishes. 150 cases.\nTudor Pinot Noir, Boekenoogen Vineyard, SLH 2015 Library Release 93 Points, Vinous 13.8% alcohol\n93 pts Vinous Media Josh Raynolds: “Ripe red berries, incense, cola and candied flowers on the exotically perfumed, spice-accented nose. Supple in texture, raspberry, cherry-vanilla and spice-cake flavors, maintaining a delicate touch. Impressive energy and spicy thrust on the persistent finish, well-knit tannins and resonating florality.” 100% from the Boekenoogen Vineyard in the SLH. 33% New oak, 33% 1 year-old and 33% 2 year-old French barrels enhance the spice and support the fruity character. 200 cases.\nNacina Riesling Ice Wine,“Wein der Eisbox”Tondré Grapefield, SLH 2014 7.5%alcohol\n92 Points, Vinous: “Intensely perfumed, fleshy alluringly sweet orchard and pit fruit liqueur flavors show impressive clarity for their richness.” Made with a post-harvest freezing process which perfectly simulates nature’s method, hence the Eisbox name. 406 cases of 12 – half bottles.\nNacina Gewürztraminer Ice Wine, “Wein der Eisbox” Danny’s Vyd, Monterey 2014 90 Points 7.5% alcohol\n90 Points, Galloni: “Aromas of apricot nectar, honey and marzipan, candied ginger. Lush, round and very sweet, offering palate- coating pit fruits and honey flavors. A decidedly rich, decadent style that lingers impressively.” Made with a post-harvest freezing process. 256cases,1⁄2bottles.']	['<urn:uuid:39b115ad-c97b-450b-b506-d292bbecb795>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T03:34:55.284799	10	26	710
20	Which specific deities were depicted in the procession of eight Aramean gods on the ancient stone panel, and what were their distinguishing characteristics?	The panel showed several deities including: the storm god Hadad who was depicted larger than the others with a trident-like lightning fork, the goddess Atargatis who was known as the mother figure and Hadad's companion, the moon god Sîn who wore a crown of crescent and full moon, and the sun god Šamaš who had a winged sun on his head.	['While constructing a home in Turkey, looters realized they’d been building atop an ancient chamber. Instead of alerting local authorities, they began to dig themselves—and only after the heritage criminals were apprehended did archaeologists arrive to save the day (and the artifacts within).\nNow, a stone panel uncovered during their emergency excavation sheds new light on the melting pot of cultures that coexisted during the early days of the expansion of Mesopotamia’s Neo-Assyrian Empire nearly 3,000 years ago.\nAn article published this week in Antiquity reveals more about the panel, which likely dates to between the ninth and eighth centuries B.C.E. The symbols inscribed there represent a variety of cultures—the inscriptions are written in Aramaic, while its carvings reflect a Neo-Assyrian style with a local Syro-Anatolian flair.\nThe panel shows a procession of eight Aramean gods. Among them are storm god Hadad, who is depicted with a trident-esque lightning fork and looms larger than the rest. The goddess Atargatis, known as the mother figure and frequently depicted as the companion of Hadad; the moon god Sîn, who wears a crown of a crescent and full moon; and the sun god Šamaš, sporting a winged sun on his head, are also present.\nScholars believe the work was left unfinished, since only the gods’ upper bodies or heads in profile are shown.\nThe panel shows “ … a local cohabitation and symbiosis of the Assyrians and Arameans in a region and period under firm Assyrian imperial control,” the authors write. They call the panel “ … a striking example of regional values in the exercise of imperial power”—a relationship that could have involved a give and take between Assyrians who wanted to impress their new subjects and Arameans eager to please their new overlords.\nThe site was likely uncovered when the home in the southeastern Turkish village Başbük was being built, reports National Geographic’s Tom Metcalfe. Instead of notifying authorities, looters cut a hole into the ground floor of the two-story home. On the other end of that makeshift entryway lay an ancient chamber hewn out of the limestone bedrock that led by staircase to another lower room.\nExcavations went on for two months in 2018 before archaeologists were forced to stop due to the “instability of the site,” according to the study.\nThe rock art is from an active time in Assyrian history. Between 900 and 600 B.C.E., the Neo-Assyrian empire expanded its territory within southeastern Anatolia. The region was once largely ruled by city-state dynasties whose residents spoke Aramean and Luwian. But gradually, Neo-Assyrian King Ashurnasirpal II took control of notable Aramean settlements.\nThis meshing of cultures is evident in not just the language of the inscriptions but the way in which the gods are portrayed. The panel’s dating captures the Assyrian takeover of power “in its early phases,” study author Selim Ferruh Adali, a history professor at the Social Sciences University of Ankara, tells Live Science’s Emily Staniforth.\n“Although some features of the gods are distinctly Assyrian—such as their rigid poses, and the particular style of their hair and beards—many details of the carvings show strong influences from the local Aramaic culture,” Metcalfe writes for National Geographic.\nArchaeologists also found an inscription that might offer insight into the work’s purpose. Aramaic writing on the panel appears to include the name of Mukīn-abūa, an Assyrian official in charge of the province of Tušhan around 811 to 783 B.C.E. It may mean that the complex was created as a way to engender favor with local residents, says Adali to CNN’s Ashley Strickland.\n“The panel was made by local artists serving Assyrian authorities who adapted Neo-Assyrian art in a provincial context,” Adali says. “It was used to carry out rituals overseen by provincial authorities. It may have been abandoned due to a change in provincial authorities and practices or due to an arising political-military conflict.”\nThough it’s unclear what caused artisans to abandon the panel, the art they began is effective, even thousands of years later. Even now, its depictions hold an emotional power over those who cast their eyes on them.\n“I felt as if I was in a ritual,” says lead author Mehmet Önal, archaeologist at Harran University, to National Geographic. “When I was confronted by the very expressive eyes and majestic, serious face of the storm god Hadad, I felt a slight tremor in my body.”']	['<urn:uuid:52f2f7d4-4d0a-49ad-a13a-32f9d27a47a3>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T03:34:55.284799	23	61	727
21	education gender inequality causes poverty benefits girls	Gender inequality in education is a major cause of poverty, particularly affecting ethnic minorities. However, educating girls brings multiple benefits: they marry later, have healthier children, earn more money for their families, take leadership roles in communities, and help drive economic growth. Additionally, more educated mothers tend to have more educated children, especially daughters.	['International Day of the Girl: Girls Speak Out 2016\nBrief summary of presentation of information made\nSecretary-General, Ban Ki-Moon, introduced by the Executive Director and Under-Secretary General of the UNFPA, Dr. Babatunde Osotimehin, as the ‘most gender-sensitive Secretary-General so far… who has achieved a huge amount… introduction of SDGs particularly important.’\nVideo introduction by Secretary-General: remarks that many of the SDGs relate to girls (directly or indirectly), hence theme (‘Girls’ Progress = Goals’ Progress’). ‘Investing in girls is the right things to do – the smart thing to do.’\nStatement by Minister of Status of Women for Canada, the Honourable Patricia A. Hajdu: Canada PM Trudeau has initiated a cabinet with a 50/50 gender split, and has made gender equality a focus for his government which has link to UNICEF (for sanitation and education) and NGOs (working for community programmes).\nExplanation that event would be broken into three section: Marginalised Girls; Girls in Politics; Girls for Girls. (Entire event, with exception of respondents and brief spoken introduction by the Working Group on Girls, presented by girls.)\nGender inequality stated as being a major cause of poverty. Gender inequality in education, particularly felt by ethnic minorities. Childhood and arranged marriages a particular issue: 150,000 children will be in marriage by 2020 if current trends continue. Sexual harassment in schools another issue.\nSpeaker Urooba (Girls Learn International) on Muslim America: Islamaphobia in schools – she has first-hand experience of unequal, negative discussions in the classroom, which she says stems from a lack of knowledge. Her desire is to create and distribute a toolkit to schools to educate.\nSpeaker Eva (Girl Scouts of the USA) on Black Lives Matter Youth: belief that issues BLM works against are systemic, institutionalised and a problem for the whole community. Implications of racist behaviour for all black people including girls. Unequal education: ethnic minorities more likely to be suspended, then later drop out and be incarcerated. Speaker founded I Project website to promote discussion of little-discussed topics through art.\nDeputy Executive Director of UN Women, Lakshimi Puri: Importance of SDGs in this area, particularly promise to leave no girl or woman behind (SDG 5). Marginalisation in many ways: sexual/physical violence in public and private, especially in rural areas, the poor, unpaid workers; child marriage and pregnancy; infanticide and pregnancy termination. Economic barriers exist. Speaker believes measurement crucial – where are their problems, to what extent and why? ‘Let us all be agents’ for change. UN Women prioritising empowerment of females.\nGirls in Politics\nSpeaker Rachel: 74% of girls believe that females must work harder than males to succeed in politics – but ‘girls are interested in politics.’ Ways to tackle include: after-school programmes, school clubs, school governance in order to encourage participation in politics, create an interest and provide an organised path to engagement; speaking out against bias; demanding girls rights.\nSpeaker Sofy (Plan International) on childhood and adolescence: trafficking can come about through pressure from households, a way to help the family (economic reasons). Also a way of preventing stigma (early pregnancy). Speaker: ‘girls are not merchandise – why should girls be compelled in this way? The law is there to protect.’\nSpeaker Hariella on action: the message of the empowerment of girls has not yet touched everyone. Speaker trained as leader at Bella Abzurg Leadership Institute and went on to advocate for social justice; other peers went on to intern for NGOs for gender empowerment. Speaker: ‘make female rights central to policy.’ Importance of advocacy for those with no voice.\nGirls for Girls\nSpeaker Emma: solidarity required to change. Power of events like this one needs to transcend into everyday life. Over 62 million girls not in school globally, and 50% of sexual violence is against girls 16 and under. ‘Be each other’s champions.’\nSpeaker Anna (Alice Paul Institute): Alice Paul has roots in the suffrage movements – visionary. Speaker: API affirmed and inspired her views on women. API girls advisory council – educates on womens’ issues, speak at events, lobby, fundraise.\nSpeaker Johnnie (Plan International) on attitudes: inequality manifests itself in many ways, creates fear. Distressing behaviour has become normalised, but positive behaviour can be too.\n- Successes of summit over five years discussed, as a forum to discuss, inspire and raise awareness.\n- Solidarity encouraged: ‘Speak up for each other’\n- Challenge negative behaviour\nWhat was of particular significance to share with The Salvation Army globally?\n- Affirmation of importance of measurement in working towards the SDGs, as SA is working towards.\n- Gender inequality comes in many forms, both visible (e.g. street harassment) and invisible/subtle (education inequality, fear, attitudes). The SA must recognise it in all forms.\n- A role to play in promoting roles for women and girls in all spheres – leadership, politics, employment\nWeb links for more information\n- http://www.itstheiproject.com/ - created by BLM Youth co-founder to promote discussion of activism through art\n- http://www.alicepaul.org/our-programs/girls-community/ - Alice Paul Girls Advisory Council', '“There is no more valuable investment than in a girls’ education.” — Ban Ki Moon, secretary-general, United Nations.\nEducating a girl is one of the best investments her family, community, and country can make. We know that a good quality education can be life-changing for girls, boys, young women, and men, helping them develop to their full potential and putting them on a path for success in their life. We also know that educating a girl in particular can kick-start a virtuous circle of development. More educated girls, for example, marry later, have healthier children, earn more money that they invest back into their families and communities, and play more active roles in leading their communities and countries.\nOver the last 25 years, there have been large gains in girls’ education, and we as a global community can congratulate ourselves for the real progress that has been made. This demonstrates that with shared goals and collective action—among governments, international organizations, civil society, media, and the private sector—we can change the educational prospects for girls around the world.\nDespite this progress, our research shows that there are hotspots in the world where girls are not getting a quality education. While there certainly are places where boys are behind, we have focused on understanding how and where across the world girls are behind. The message is that many countries have work to do to improve girls’ education, whether related to the gender gap in primary or secondary enrollment or learning.\nThere are about 80 countries where progress on girls’ education has stalled. These countries are not meeting the education Millennium Development Goals. They are stuck in an education bog—still struggling to enroll all girls and boys in primary school and close the gender gaps between boys and girls at both the primary and secondary levels.\nThere are an additional 30 countries that have successfully enrolled girls and boys in primary and secondary education but are trapped in low-quality learning. They are struggling to ensure that girls and boys master foundational skills such as basic literacy, numeracy, and science concepts. Quality learning is important for the future lives of girls and boys, but it is also an especially important ingredient in the virtuous circle of development that comes from girls’ education.\nFinally, there are another 30 countries where children are successfully enrolled and learning. However, girls are behind boys in math. In some ways, we can think of girls in these countries bumping up against an educational glass ceiling.\n• Why do we care about girls’ education?\n• What progress can we build on?\n• What do we face today in the effort to educate girls?\n• Why are girls behind?\n• What is working to address obstacles to girls’ education?\n• What should we do to accelerate progress on girls education?\nSeven main benefits of girls’ education to society\n1. More educated girls and women aspire to become leaders and thus expand a country’s leadership and entrepreneurial talent.\n2. It is the quality of schooling that really counts; economic growth is faster when girls (and boys) learn.\n3. More equal education means greater economic empowerment for women through more equal work opportunities for women and men\n4. More educated girls and young women are healthier—and as adults they have healthier children.\n5. More educated mothers have more educated children, especially daughters.\n6. More educated women are better able to protect themselves and their families from the effects of economic and environmental shocks.\n7. Education is valuable for girls in and of itself. Finally, in the words of Urvashi Sahni, an Indian girls’ education activist, “even without all of the ‘developmental and economic goodies’ that come\nfrom girls’ education, we should care about educating girls because it is inherently valuable to them and is their right” (Sahni, 2015)\nWhat should we do: Taking action on girls’ education\nTaking action on girls’ education should not be confined to the halls of government offices or multilateral institutions. Civil society networks, business leaders, media organizations, academia, social enterprises, philanthropic communities, and individual global champions all have a role to play. With this in mind, we are recommending two focused streams of action.\n• Recommendation 1:\nLean in with girls and women’s leadership. Our first recommendation proposes specific initiatives that are well positioned for engaging diverse actors, including: women’s groups, technology companies, media partners, transparency and EDUCATION NGO’S and government education.\nplanning departments. These initiatives are envisioned as catalytic “quick wins” that, if given sufficient financial and political support, could be scaled up within a short time period. They also represent an attempt to explore relatively new approaches to tackling the decades-long girls’ education problem. They are also recommended with the notion that while not directly confronting violence and early marriage, they will certainly help empower girls to push back against these forces. It is our assessment that all countries could benefit from leaning in on girls and women’s leadership, as it is fundamental to sustainable social change not only for girls’ educational opportunities but for gender equality more broadly.\nThe two initiatives we recommend are:\nº Recommendation 1.1: Build strong girl leaders. We propose a girls’ leadership initiative that simultaneously provides opportunities for girls to develop the soft skills so crucial for their success as well as provides roles models and networks that help shift social perceptions and norms around girls’ education and gender equality. We propose a mentorship model be used with either teachers or recent secondary school girl graduates and that the initiative be scaled up with diverse partners starting in countries where girls’ education is the most behind.\nº Recommendation 1.2: Girl-generated data. We propose a girl-generated big data initiative, which would combine the power of “factivists and feminists” (Drummond, 2015). Girl-generated data has the potential to radically change the power dynamics, with girls themselves generating regular information about their circumstances, needs, and achievements that is translated into digestible and timely insight for policymakers, civil society actors, community leaders, and educators. Transparency and accountability take on whole new meanings in this light and ultimately puts the girls at the center of the process.\nA girl generated big data initiative also can go a long way in helping fill the data gap on girls’ education, both on basic education data that we have seen is often missing in many countries, but also more importantly on sensitive issues such as school-related gender-based violence and child marriage. We propose a model where technology firms would partner with civil society and governments to collect, analyze, and disseminate this girl-generated data to those actors who can make the changes needed to improve girls’ lives.\n• Recommendation 2:\nFocus on systemic reform with a gender lens. Ultimately, the best approach for helping girls get educated is to ensure governments have strong education systems, ones that enable all children to access good schools and quality learning opportunities. Good schools must be in places where girls and boys alike are given the opportunity to thrive and grow. Developing an education system where good schools are a reality, including for marginalized girls, necessitates systemic reform in many of the countries where girls are behind. In support of systemic reform we propose:\nº Recommendation 2.1:\nDesign for education hotspots. We recommend that international donors and multilateral institutions focus increased attention on hotspot countries, in particular in countries stuck in an education bog where girls’ education progress has stalled. This includes both ensuring aid dollars flow to those countries and that the dollars go toward shoring up basic education and gender equality, including in humanitarian contexts. Governments must also do their part and employ strategies for including girls in education progress. This could include defraying costs, supporting great teachers, or improving teaching and learning materials. Teacher organizations also have a role to play. Global capacity can be deployed to help the professional development of teachers across countries where girls are farthest behind.\nº Recommendation 2.2:\nFocus with a gender lens. Countries themselves, and their regional and global partners, must ensure they undertake systemic reforms with a gender lens. This means all decisions around things such as policy, budgets, hiring, and monitoring must be evaluated with the understanding of their differential impacts on girls versus boys. Gender analysis tools should be systematically used in the development of education sector plans. Applying a gender lens to the process of sector plan development—including sector analysis, plan preparation, and plan appraisal—can ensure that that the key tools for national education system reform and associated policies and strategies promote effective actions that advance gender equality.\nWhile made separately and with distinct purposes in mind, these two recommendations are also mutually reinforcing. Improved girls’ and women’s leadership, and boosting the availability of relevant data generated, can provide an important feedback loop for governments either for planning or monitoring purposes. Likewise, government reforms can open up space for girls’ and women’s leadership, serving to both help such leadership flourish and reap its outcomes in terms of improved girls’ education opportunities. Ultimately, we hope that these two recommendations, and the specific initiatives made within each, are translated into action and together with the wide range of other strategies actors are pursuing can make a difference to girls, their learning opportunities, and ultimately their ability to be successful in their lives and livelihoods.\nEducation and gender indicators: Bogs, traps and ceilings EDUCATION INDICATORS\nGender parity education not reached in Africa\nCAMEROON, GHANA, KENYA, MORROCCO, MOZAMBIQUE, ZAMBIA*\nCountries below the enrollment mean in Africa: ANGOLA, BENIN REPUBLIC, BURKINA FASO, BURUNDI, CENTRAL AFRICAN REPUBLIC, CHAD, COTE d’IVOIRE, DEMOCRATIC REPUBLIC OF CONGO*, DJIBOUTI, ETHIOPIA, GUINEA, GUINEA-BISSAU, LIBERIA, MALAWI, MALI, NIGER,**NIGERIA**, SIERRA LEONE, SOUTH SUDAN, TOGO, UGANDA, Equatorial Guinea, Gambia, Lesotho, Madagascar, Mauritania.\nGender parity in net enrollment rates in Africa\nBotswana, Namibia, Sao Tome & Principe, Senegal, Tanzania, Zimbabwe\nCountries below the enrollment mean:\nGender inequality in math achievement\nAlgeria, Rwanda, South Africa, Tunisia.\nfor more reading pls click on the following links:']	['<urn:uuid:ba414dc9-e3d3-4808-8cbe-95f8b4ca4229>', '<urn:uuid:d1bcbe8f-cd93-4788-9066-1e05447dc32b>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T03:34:55.284799	7	54	2479
22	How often do workers in restaurants and food companies need to get trained on keeping food safe, and do they need to keep repeating the training?	Food safety training is a continuous process, not a one-time event. Additional training is required when there are changes in production processes, introduction of new products, or upgraded storage facilities. In some countries, laws require food safety training to be repeated every few years. Regular updates and refresher courses are necessary to maintain compliance.	['Food safety has assumed significance in recent times. Businesses involved with food such as food processing units, manufacturers of food products, hotels, or even a thriving restaurant business have to pay attention to food safety. Food safety involves a set of practices, procedures and processes that need to be followed to ensure safety and hygiene. These steps are crucial at every step from production, procurement, and storage to preparation and presentation.\nFood safety is a complex process and if it has to be followed meticulously, employees have to be trained in the right way so that there are no lapses. Failing to follow food safety procedures can lead to serious consequences for organizations and their reputation. Food safety training is necessary to avoid such repercussions.\nFood safety training can be provided in the classroom for employees; however, it might not be a one-time event because updates have to be provided for which subsequent training sessions have to be organized. Classroom training can work for small restaurants and hotel chains where employees are at one place but what about large hotel chains or food manufactures whose employees are at diverse locations? Holding training sessions every time new employees join or getting employees together whenever there is an update might be impractical.\nThe food industry is fast-paced, and employees need to be highly skilled and well-trained to keep pace with the changes. Classroom training might be impractical and expensive in such circumstances. E-learning is a better alternative. The advantages offered by e-learning are compelling many companies in the industry to adopt this mode of training. Let us explore the reasons for this.\n1. Provides Accessibility\nEmployees in the food industry usually work in shifts and e-learning provides the convenience of accessing safety training any time they want, as per their convenience. This is useful for employees at companies having operations at diverse locations. Moreover e-learning modules can now be accessed through mobile devices and can be made available in multiple languages to cater to a wide variety of learning needs. The courses being self-paced can cater to various skill levels of learners from sanitation workers to food safety inspectors.\n2. Ensures Quick Updates and Better Completion Rates\nAny changes in rules and regulations or updates that employees need to know can be quickly updated in e-learning courses and delivered to employees. Emergency refresher training is easy and can be deployed when required.\nCompletion rates are better when they are delivered as short, microlearning modules because these short and crisp courses can be completed at the learner’s convenience unlike classroom sessions which can be long and tedious.\n3. Promotes Higher Retention\nE-learning results in higher retention of knowledge. The courses are engaging and interactive and require learner participation. Food safety training becomes effective if there are elements that invite their active participation.\nTraining is more effective when learners are not just given the information about a process but are given demonstrations on how to do it. Learners should be given activities that enable virtual practice so that they can apply what they have learned. For this, the best option would be scenario-based learning.\nScenario-based learning helps the learner to examine and explore a particular situation and make choices on the actions to be taken or precautions to be followed. It also allows the learner to make mistakes without any real-world consequences and learn from them. Tools such as Articulate Storyline make it easy to create scenarios for such course. View a sample module we created for a course on food safety.\n4. Helps in Better Tracking and Reporting\nThe food industry is heavily regulated, and organizations need to regularly submit proof of training and certification of their staff on food safety regulations. When you host your course on an LMS, you have better chances of tracking who has taken the course, those who need a refresher course, or employees who need training from scratch. This will help you plan your training schedule. The LMS will help you create reports which will show how many employees have completed the training.\n5. Facilitates Recurrent Training and Performance Support\nFood safety training is a continuous process; it is not a one-time issue. Changes in production processes or the introduction of new products or upgraded storage facilities may call for new training programs or refresher courses. In some countries, laws require that food safety training be repeated every few years. E-learning makes it easy to schedule such training based on employees’ convenience as well as rollout updates in short time. Performance support in the form of mobile learning modules that can be accessed by employees at the point of need is possible with online learning.\nFood safety is a crucial issue in the food industry and e-learning provides the best options to ensure your employees are well-trained and updated on latest requirements while your business stays compliant.']	['<urn:uuid:c8f1c768-6901-41bc-93e7-94c8162d27e5>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T03:34:55.284799	26	54	810
23	What is the relationship between physical exercise and dystonia management, and what are the common causes of neck muscle strain?	Weight lifting exercises should be avoided in cervical dystonia patients as they can worsen the condition by reinforcing dystonic muscles and compromising Botox injection effectiveness. Specifically, exercises like lateral arm raises, shoulder shrugs, and various presses should be avoided as they reinforce the trapezius muscles. Regarding neck muscle strain, common causes include repeated or prolonged neck movements, extended computer use, sudden turning while sleeping, carrying heavy items especially with one arm, poor posture, and cradling phones against the neck without hands.	['Magritte – The False Mirror, 1928\nMental imagery, visualisation and focal dystonia retraining\nMagritte – The False Mirror, 1928\nMental imagery, visualisation and focal dystonia retraining\nMental imagery is already used in high level sports competition, and athletes are trained to rehearse their race in their mind, to increase their performance.\nCould dystonic patients benefit from mental imagery focusing on their motor\nThere are scientific facts, and anecdotal reports from dystonic patients to suggest that it may be very helpful on the long term.\nReal and mentally stimulated movements rely upon largely overlapping networks. In another words, writing or imagining writing involved common networks in our brain. Studies have shown that the mental imagery of a movement in patients with dystonia follows the same difficulties than the execution of the dystonic movement.\nA study by Fioro, 2006 has looked at the mental rotation of body part (hands, feet) in which the subjects imagine moving their body part, from their actual posture into that seen on a picture to recognise which side is belonged in patients with writer’s cramp. Writer’s cramp patients are slower than controls in mentally rotating hands but not feet; it suggests that the mental rotation of body parts reflects the anatomical constraints of real hand movements.\nCould the mind reshape the brain?\nNeuroplasticity is a normal process by which the brain develops new connections at different levels, following interaction with environment, emotions, behavior. Dystonia is understood to be a condition characterized by an excessive neuroplasticity of the brain leading to abnormal learning of motor program. To be able to influence this brain plasticity It’s important to create an environment for positive learning and recovery. Mindfulness can be helpful in patients with focal dystonia to minimise stress and increase well being before embarking for retraining.\nRetrain your brain, not your muscles!\nThe visual and mental imagery training will encourage you to imagine successful, normal execution of tasks, which the dystonia prevents you to do..\nRemember the time when you could perform the movement normally, and how easy it was to do it and the pleasure, which come with it.\nIt will be best performed after the Botox treatment has already controlled the dystonic spasm\nin the affected body part and together with sensory retraining.\nTry daily to visualize yourself free of the dystonia, walking eye opened, turning your head freely looking around you, writing a page with our favorite pen or chewing a delicious meal.\nAlso music and melody can help you to IMAGINE…\nYou may say I’m a dreamer, But I’m not the only one, I hope someday\nyou’ll join us, And the world will live as one (John Lennon)\nMeditation refers to a family of self -regulation practices that focus on training attention and awareness in order to bring mental processes under greater voluntary control and thereby foster general well-being and development and/or specific capacities such as calm, clarity and concentration.…( Walsh and Shapiro, 2006, quoted in Wikipedia on meditation http://en.wikipedia.org/wiki/Meditation\nSeveral recent studies have shown the influence of mindfulness meditation on brain morphology. In particular, Britta et al (Soc Cogn Affect Neurosci. 2010; 5: 11–17) has shown the influence of an 8-week mindfulness-based stress reduction intervention on the changes in amygdaloid gray matter density, which is a part of the brain involved in response to stress.\nPatients with dystonia have all experienced the worsening of their dystonic symptoms with stress and Henry Meige at the beginning of the 20th century adviced his dystonic patients to have a regular, calm life to avoid fluctuations in the severity of the cervical dystonia.\nThe dystonia itself is source of great frustration and stress for the patients, who are limited in their daily tasks (walking, reading, eating, speaking, writing) by the dystonics spasms. In addition, professional and family life can bring a lot of joy , but can be also emotionally challenging.\nCan meditation help patients with dystonia? There is no scientific study on that topic but I want to share with you the testimony of Anne, a lady with blepharospasm who has been involved in teaching meditation to groups and practices mediation regularly for her well being.\nTestimony of Anne\n“I have had Blephoraspasm for 7 years, for me the most difficult things to come to terms with were\nØ Loss of independence\nØ Loss of career\nØ Not being able to drive when I wanted\nØ Learning to use Public Transport alone\nØ Fear of travelling alone\nØ Decrease in social activities\nI went through a period of becoming almost house bound only going out with my son and friends.\nHaving worked as a senior Nurse in Mental Health for over 40 years I was very aware of the symptoms of stress and anxiety but had not fully appreciated just how much these symptoms were affecting my Dystonia, I had spent years as a CPN teaching patients and carers Anxiety Management but had not recognised the tell-tale signs in myself\nFirst I had to accept that I had this disabling disorder and that I had to learn to manage the symptoms, I soon became more and more aware that any sort of negative stress or anxiety made the symptoms a great deal worse. It could be something really trivial and my eyes would start closing.\nTo assist me with relaxation I tried many different complementary therapies some helped some didn’t, we are all individuals, what will help one person won’t another. I also used anxiety management techniques to help, particularly when I went out i.e. shopping.\nI also tried to develop new activities/hobbies that I could do rather than dwell on those that I had lost [not easy]\nI had been interested in Meditation for many years and although I had practised this, it had not been in a very disciplined way, now I aim to sit for 30 minutes once or twice daily.\nThere are many different types of meditation and many definitions.\nMeditation for me is about stilling the mind and looking within, facilitating a sense of peace and calmness\nSimple meditation exercise.\nThe following is a simple method that I use with the aim of reducing stress, identifying tension in various parts of the body and assisting me in controlling negative thoughts and generally aiding relaxation\nPreparation for meditation\nØ Wear comfortable clothing\nØ Use a space for this purpose, a spare room or a corner of a bedroom whatever works for you importantly the space needs to be conducive to sitting quietly with an even temperature\nØ Attempt to sit at the same time each day, this will help to establish a routine that will be easier to adhere to.\n5 to 10minutes is fine to begin with. Don’t beat yourself up if you cannot achieve this.\nSit comfortably on a chair with your back supported, feet flat on the floor [use a cushion for your feet if they don’t reach the floor] hands on your lap.\nClose your eyes if this is comfortable.\nFocus on your breath both the inhalation and exhalation [it is important to breathe normally not too deeply] Some people think of the word Relax as they exhale.\nYou will find that to begin with there are many distracting thoughts going round and round in your mind, this is normal, don’t worry about them let them come and go, just gently bring your attention back to your breath, this may be difficult to begin with but with practise it becomes easier.\nIt is impossible to think about 2 things simultaneously, as you focus on the breath the distracting thought will disappear at least for that moment.\nYou may feel fidgety at first, your body and mind need time to adjust to not worrying and rushing to do all the normal daily tasks, practise will help.\nAs you become more expert in the exercise you will observe which muscles are tenser than others. Gradually you will be able to sit for a longer period leaving you feeling more relaxed. If it is preferable, quiet gentle background music could be played. Most types of meditations start with this type of exercise how long you sit and how deeply you go within yourself is a matter of personal choice\nIt will be interesting to hear the voices of dystonic patients from India for instance, where meditation is part of a long cultural tradition to know if they find it helpful.\nOf course, I am not suggesting that meditation is a treatment of dystonia, or that meditation is good for everybody, but it may be a coping strategy for some dystonic patients when the stress in their life has a negative impact on the severity of their muscle spasms.\nI was invited last week to lecture at the University of Liege ( Belgium ) about the understanding of dystonia by Henry Meige, a French neurologist . I went back to his original publications and found that he advocated in 1907 a psycho-motor retraining (“discipline psycho-motrice”) for the treatment of spasmodic torticollis…\nThe Magic Mirror, Magritte, 1929\nTreatment of spasmodic torticollis by a psycho-motor retraining: A method developed 100 years ago by Henry Meige\nI was invited last week by Professor Gustave Moonen to lecture at the University of Liege (Belgium) about the understanding of dystonia by Henry Meige, a French neurologist.\nI went back to his original publications and found that he advocated in 1907 a psycho-motor retraining (“discipline psycho-motrice”) for the treatment of spasmodic torticollis. It was at the time when there was no routine use of anticholinergics and Botox injection for cervical dystonia was not even a blip on the horizon. I could not resist to give you the translation of this therapeutic approach that I found quite inspiring…\n1-The patient becomes actor of his treatment: “I don’t say the patient is cured, but the patient has cured himself”. This treatment is based on regular immobilizations and movements in front of a mirror; the patient has to be supported by his family and his doctor, as these daily exercises require a lot of effort and determination from the patient. “The goal of the treatment is to correct the abnormal postures, the put at rest the hyperactive muscles and to learn the control of the motor acts.”\n2-The patient needs to have a regular life, going to bed at regular times. The patient is told that the course of the disease will be capricious, that he will have to perseverate and that the exercises will be eventually beneficial.\n3-The patient had to exercise in front of a mirror 3 times a day; the patient is sitting, back non supported and hands flat on a table. The mirror is divided by 1 vertical line going through the middle of the face and 2 horizontal lines, through the alignment of the eyes and through the base of the neck above the shoulders, in order for the patient to be aware of the movement of his head. The patient is asked to focus on the point of crossing of the first 2 lines.\n4-Two types of exercises:\n1-Immobilization: for 5 seconds 10 times with 15 seconds rest in between each immobilizations, increasing of 5 seconds every day the immobilization time.\n2- Movement: slow and smooth movement, without saccade of the head in rotation, lateral flexion, flexion forward and extension; also movements of the shoulders, arms and trunk and exercises of relaxation of the muscles.\nThen also in front of the mirror, exercises of writing, reading, breathing and speaking and daily tasks exercises.\n5- Cervical dystonia has a good prognosis:\nHenry Meige was convinced that over the years the cervical dystonia always settle down, with the dystonic spasms becoming less severe and less frequent, until they disappear; the patient is left with only neck stiffness. He did not give any figure of proportion of patients who improved, but they were patients followed for more than 5 years.\n6-Since the 80’s, physiotherapy for cervical dystonia has been developed by Jean-Pierre Bleton in France, but still remains a French specialty despite individual effort to develop it abroad.\nBotox injections had become the first line treatment for cervical dystonia,, leaving the patient in the passive expectation of his injections every 3 months.\nThe mirror is still a very important tool in the therapeutic approach of neck dystonia as the patient is very inaccurate when assessing the position of his head, thinking that his head is straight when in reality the head posture has a 20 degrees tilt or rotation.\nMay be it’s the time for patients with cervical dystonia to become actor of their treatment and to look for a therapeutic strategy including Botox injections and “psycho-motor retraining “ without forgetting the role of the mirror, like Magritte, a Belgium painter…\nMeige H : Les peripeties d’ un torticolis mental. Histoire clinique et therapeutique.. Nouvelle iconographie de la Salpetriere. 1907, 6:461-480\nPush up and Press up have became part of the life of young adults…\nDr MH Marion explains why weight lifting is not advisable in cervical dystonia patients…\n1-Push up and Press up have became part of the life of young adults. Going to the gym is a healthy and an advisable way of keeping fit in a urban society, which expect us to be sitting all day long in front of a computer and to be in full shape for climbing mountains.\nHowever, it’s more recent that weight lifting to reinforce selectively muscle strength and to modify body shape is part of the routine of ordinary people, who are neither athletes, nor body builders. Weight lifting can quickly be part of the life style, with addictive personal challenges to lift heavier and heavier weights.\n2-Cervical dystonia is a neurological condition, affecting young adults, resulting in an unbalanced activity of the neck muscles. Some muscles are hyperactive, and inhibit their counterpart on the other sides (reciprocal inhibition); for instance a patient with an involuntary rotation of the head to the right (right spasmodic torticollis) will have a large , hyperactive left Sterno-Cleido Mastoid muscle and a thin right Sterno-Cleido-Mastoid muscle. For a better understanding, read the blog “What makes my head turn?”.\nThe treatment of cervical dystonia is based on Botulinum injections, which correct this disequilibrium by relaxing the hyperactive muscles and on physiotherapy by reinforcing the inhibited muscles.\n3- Is weight lifting contra-productive in patients with cervical dystonia?\nI had the opportunity to treat few patients with cervical dystonia who were adept of weight lifting practice. These patients require larger doses of Botulinum toxin, even if the small number of cases doesn’t allow any scientific conclusion. This could explain by the fact that the exercised muscles became larger and stronger.\nBut the question is which exercises have an impact on the neck muscles and are some exercise worst than others, by targeting neck muscles involved in the dystonia?\nI had the opportunity to discuss resistance training exercises with a specialized exercise instructor , Mr Rajah James who gave me a reference book “ Strength training anatomy” from Frederic Delavier. Every resistance training exercise is analysed in terms of functional anatomy with detailed illustrations of which muscles are targeted for each exercise. It’s an amazing book full of details and drawing, that I will strongly advise to anybody interested in exercising against resistance.\n4-. Exercises to avoid at any cost:\nThe neck muscles are involved in the erect posture of the neck, in another words keeping the neck straight; any weight lifting will tense the neck muscles to stabilize the neck during the effort.\nMore specifically, shoulders muscles such as Trapezius and Levator scapulae elevate the shoulder but also are involved in the rotation, lateral flexion and extension of the neck.\nLateral Arm Raises and Shoulder Shrugs (Machine and Dumbbells shrugs), have to be avoided at any cost; they both reinforce the Trapezius in his upper and anterior part and in addition the shrugs reinforce the Levator Scapulae.\nBack Press, Front Press, and Dumbbell Press are reinforcing the Trapezius muscles in its upper part.\nTherefore I advise strongly against any weight lifting exercise in case of cervical dystonia;it can worsen the dystonia by reinforcing the dystonic muscles and increase the muscle unbalamnce and also it can partially compromise the effect of the Botox injections.\n5- Physiotherapy for cervical dystonia\nThe retraining of the cervical muscles, which are becoming less active because of the dystonia, and the stretching of the overactive muscles are a very important part of the treatment. Jean-Pierre Bleton in Paris has written extensively about his original approach of physiotherapy with dystonic patients.\nFrederic Delavier: Strength Training Anatomy\nJean Pierre Bleton : Role of the physiotherapist in the treatment of dystonia', 'A neck spasm caused by the sudden tightening of the neck muscles may cause:\n- severe, sharp pain\n- problems moving the neck and shoulders\n- increased pain when moving the neck and shoulders\n- a headache\n- dizziness or tingling in the base of the neck\nDizziness or tingling in the base back of the neck may occur if neck muscles pull on the scalp when they spasm.\nWhat causes neck spasms?\nNeck spasms may be caused by strain due to exercise.\nNeck spasms have several common causes:\n- repeated or prolonged movements of the neck\n- sitting at a computer for extended periods\n- turning suddenly while sleeping\n- placing too much weight on one shoulder with a bag\n- carrying something heavy, especially if with one arm\n- using a mobile without hands, cradling it against the neck\n- emotional stress\n- strain caused by exercise\n- poor posture\nThere may also be a more serious reason for neck spasms, such as:\n- whiplash or other trauma\n- ankylosing spondylitis (an inflammatory condition that affects the spine)\n- torticollis or cervical dystonia (a condition where neck spasms cause the head to twist to one side)\n- spinal stenosis\n- joint disorders that affect the jaw\n- herniated disk\nTreatment from a specialist such as a chiropractor may be recommended.\nThe following medical treatments may help:\n- anti-inflammatory medication\n- pain relief medication\n- muscle relaxants\n- steroid or anesthetic injections (only if other treatments have proved ineffective)\nA doctor may also recommend physical therapy.\nA 2017 study suggests an over-the-counter expectorant called guaifenesin may prove effective in treating neck spasms. More research is needed before doctors can recommend this treatment.\nWhat are some exercises for neck spasms?\nA good way to treat neck spasms is with stretching exercises. The following may help:\nBasic neck stretches\nTo do a basic neck stretch:\n- place your right hand on the top of your head\n- pull your head down towards the right side of the chest\n- repeat on the left side\n- repeat the exercise three times\nTo do a scalene stretch:\n- stand with both hands behind your back, clasping your right wrist with your left hand\n- use your left hand to pull the right arm and shoulder down\n- meanwhile, tilt your head to the left, stretching the right side of the neck\n- repeat three times on each side\nNeck curl and head lift\nTo do a neck curl and head lift:\n- lie down as if you are about to do a sit up\n- with your hands behind your head, tuck your neck into the chest\n- then lift your head off the floor, keeping your shoulders on the floor\n- repeat five times\nIn addition to anti-inflammatory medication, pain relief, and stretching, the following home remedies may help:\nYoga may help to ease neck pain and relieve stress.\nStress may trigger neck spasms. Taking regular breaks from work and exercising as often as possible may help a person better manage stress.\nDeep breathing, yoga, and meditation may also help.\nIce may help reduce neck spasms. Using an ice pack for 20 minutes a time, every 3 to 4 hours may help relax the neck muscles.\nApplying moist heat to the affected area may also help with recurring neck spasms. Apply moist heat using heating pads or a damp, warm cloth.\nSelf or partner massage may help relieve muscle spasms. Try to use gentle to firm pressure on the neck and move fingers in circular motions.\nA 2014 study found that classic massages could reduce neck pain.\nPreventing neck spasms\nTo prevent neck spams:\n- take regular screen breaks\n- exercise regularly\n- use laptop or computer stands to adjust screen height\n- make sure office chairs promote good posture\n- improve posture with strengthening exercises, such as pilates\n- make sure to stretch at the earliest sign of neck pain\n- using supportive pillows\nWhen to see a doctor\nMeningitis can cause a stiff neck. Meningitis can be extremely dangerous, and anyone who suspects they may have meningitis should contact emergency services. The symptoms of meningitis include:\n- sudden high fever\n- stiff neck\n- a headache\n- purple bruise-like marks on the skin\nMost neck spasms have a common cause. If caused by an injury, it is essential to speak to a doctor straight away.\nA person should also speak to a doctor if their symptoms are very severe or last longer than a week.']	['<urn:uuid:ca7d77c6-84ee-49c5-af8e-87468d775ad1>', '<urn:uuid:df97622a-e923-41ef-b6c3-a296341a0800>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T03:34:55.284799	20	81	3530
24	equality rights males females guaranteed constitutional section	Equality between males and females is guaranteed in two sections of the Charter - Section 15 which protects equality rights without discrimination based on sex, and Section 28 which explicitly states that Charter rights and freedoms are guaranteed equally to male and female persons, notwithstanding anything else in the Charter.	"['Human rights are rights that we all have by virtue of our shared humanity. Depending on the nature of the right, both individuals and groups can assert human rights. The realization of human rights is a constant struggle on the part of people who suffer injustices and who seek redress.\nHuman rights are rights that we all have by virtue of our shared humanity. Depending on the nature of the right, both individuals and groups can assert human rights. The realization of human rights is a constant struggle on the part of people who suffer injustices and who seek redress. Human rights are an important part of the social fabric of Canadian society. Canadians have played a special role in the evolution of human rights on the international stage\nHuman rights are so fundamental that they receive a degree of special protection at three levels: in international law, in national constitutions and in human rights laws.\nAt the international level, the touchstone is the 1948 Universal Declaration of Human Rights. Canada has signed the Declaration and has ratified several international human rights treaties. There are ten core United Nations human rights treaties. By 2014, Canada had ratified seven out of ten.\nAt the constitutional level, the Canadian Charter of Rights and Freedoms (the “Charter”) is now part of the Constitution Act, 1982. Before 1982, there was little constitutional protection against government interference with human rights. Today, the Charter guarantees fundamental freedoms (free expression, religion, association and peaceful assembly), democratic rights (such as participation in elections), mobility rights (which protect certain rights of citizens and permanent residents in Canada), legal rights, equality rights (between men and women, for example), and the right to enforce these rights. Language rights receive distinct constitutional protections as well. Beyond the Charter, the rights of Aboriginal peoples merit unique protections because of their particular history (see Aboriginal Rights). These include individual and collective rights to their cultural and religious traditions, rights to autonomy and self-government, and the right not to be subjected to forced assimilation or to the forcible removal of their children to any other group.\nThe third level of protection is found in anti-discrimination laws. The laws include the federal Canadian Human Rights Act, as well as 10 provincial and three territorial human rights laws. These laws mainly protect equality rights.\nTypes of Human Rights\nPeople use the language of human rights to support all kinds of claims, but not all claims count as human rights in law. Equality rights give people equal protection and benefit of the law. The equality rights section of the Charter, section 15, came into force in 1985. It applies to all levels of government in Canada.\nCivil Liberties are associated with fundamental freedoms and privacy rights. They are usually invoked to oppose state intrusions into those rights, typically on grounds of national security and public order. Civil liberties (equivalent to “civil and political rights” at the international level), can be claimed before the courts on an immediate basis, for example by demanding an immediate end to cruel and unusual treatment in detention. This group of rights is often referred to as “first generation” rights.\nEconomic, social and cultural rights are also fundamental components of human rights. They are sometimes referred to as “second generation” rights. They include education, health, housing and employment. Canada usually implements these rights through government policies and programs. They require progressive or gradual realization: progressive realization recognizes that it is impossible for most governments to achieve these rights fully and immediately for everyone. Human rights legislation in most of Canada recognizes and enforces many economic and social rights, for example by protecting equality rights in the areas of housing, employment and education.\nThere is also a third generation of other rights, including the rights to economic and social development, to participate in cultural heritage, and to intergenerational equity and sustainability. Third generation rights are generally not enforceable, but they are an emerging set of norms that are developing quickly.\nAll human rights are indivisible, interrelated and interdependent. None has automatic precedence over any other. This important principle was established at the end of the Cold War at the Vienna World Conference on Human Rights in 1993.\nHuman Rights in Canada\nHuman rights as we understand them today are a relatively modern concept. Slavery was practised in the British colonies, including Canada, until it was made illegal under the Slavery Abolition Act of 1833. After Confederation, discriminatory laws discouraged the immigration of non-whites (see Immigration Policy). Discrimination and de facto segregation (see Prejudice and Discrimination) were commonplace in many parts of Canada, especially for Chinese, Japanese and Black Canadians.\nDuring the two world wars, and at different points in time, Canada interned people (see Internment) who were perceived to be enemy aliens, including Eastern European, Italian, German and Japanese Canadians.\nIn the early 20th century, the rights of women and children were restricted (see Women and the Law; Murdoch Case). Women were not considered ""persons"" eligible for appointment to the Senate until 1929. Women did not gain the federal vote until the First World War, and did not gain the provincial vote in all provinces until 1940 (see Women’s Suffrage). In Québec, restrictions on the legal capacity of married women were not lifted until 1964.\nLaws denying the franchise to Aboriginal peoples were in place until after the Canadian Bill of Rights in 1960. Aboriginal women continued to experience discrimination. Women who married non-Aboriginal men lost their status under the Indian Act whereas Aboriginal men who married non-Aboriginal women retained their status. This discrimination was challenged under international law, but the Canadian law did not changed until 1985. Aboriginal peoples were also prohibited from filing federal human rights complaints about discrimination in relation to the Indian Act until 2011. Aboriginal issues remain one of Canada’s most serious human rights concerns.\nLegal measures to combat discrimination, racism and hate speech began to emerge in the 1930s, but it was not until the 1940s that they became more widespread. In 1944, Ontario introduced provincial legislation called the Racial Discrimination Act, followed by Saskatchewan\'s Bill of Rights in 1947. Both statutes relied on quasi-criminal prosecutions — as opposed to civil human rights claims, which would emerge two decades later — and were rarely used. Fair accommodation and fair employment practices laws were enacted throughout Canada in the 1950s, followed by equal-pay legislation for women.\nIn the 1960s and 1970s, the provinces and territories started to consolidate these statutes into comprehensive human rights codes, administered and enforced by permanent human rights commissions and tribunals. These laws have “quasi-constitutional status,” meaning that they prevail over other types of laws and they apply not only to government actions, but extend to businesses, the media and non-profit organizations, as well as to individuals in certain circumstances.\nBill of Rights in Canada\nThe 1960 federal Bill of Rights was an important step in the evolution of human rights in Canada. Following two petitions submitted to Parliament by Jehovah’s Witnesses, many of whom were arrested in Québec during the 1940s, the Bill was championed by Prime Minister John Diefenbaker, who had been a strong defender of the Witnesses during the Second World War. The Bill had several weaknesses: it had no constitutional force and could be amended in Parliament like any other law. It was restricted to federal matters, and did not affect the provinces in any way. The Bill of Rights made little legal impact, but its shortcomings demonstrated the importance of a constitutional bill of rights and paved the way for the Charter.\nDevelopments in International Law\nIn 1948, the United Nations General Assembly proclaimed the Universal Declaration of Human Rights as the ""common standard of achievement for all peoples… to promote respect for these rights and freedoms, and by progressive measures national and international, to secure their universal and effective recognition and observance."" The Declaration was accepted by a unanimous vote, with the six members of the Soviet bloc, Saudi Arabia, and the Union of South Africa abstaining.\nBy 1966, the international scene had evolved in important ways. The 1948 Universal Declaration of Human Rights was supplemented by the International Covenant on Civil and Political Rights (ICCPR) and the International Covenant on Economic, Social and Cultural Rights. Canada acceded to both in 1976 with the unanimous consent of the provinces. Both covenants are now binding upon Canada in International Law. Canada also ratified the Optional Protocol to the Civil and Political Rights Covenant: anyone in Canada can now file a complaint (called a “communication”) to the UN Human Rights Committee that oversees the ICCPR if the Canadian government fails to meet its obligations.\nCanada has ratified or signed other core human rights treaties and submitted itself to oversight by their respective treaty bodies. The treaties include the Convention on the Rights of the Child, the Convention on the Elimination of all Forms of Discrimination against Women, and the Convention on the Rights of Persons with Disabilities. Each has its own oversight committee at the United Nations.\nImpact on Canadian Law\nThe development of international human rights law generated pressure to strengthen our human rights laws in Canada, leading to the enactment of the Canadian Human Rights Act and the entrenchment of the Canadian Charter of Rights and Freedoms.\nThe Charter is a bill of rights but is also a political compromise. Section 33 of the Charter permits any government to enact laws ""notwithstanding the Charter,"" and thus a majority government may infringe the Charter through this ""notwithstanding"" power. Such actions will, in most cases, constitute a clear violation of international law. Public interest groups frequently oppose such actions in democratic societies through the actions of individuals, political parties, the media and members of the legislature.\nBy the late 1970s, human rights laws had been enacted at the federal level and in every province. All three territories now have their own human rights systems as well. Alberta, Saskatchewan, Québec and Yukon also have special protections in their human rights laws to protect certain civil and political rights. The Québec Charter of human rights and freedoms (1975) goes further and protects economic, social and cultural rights.\nHuman Rights in Practice\nThe scope and meaning of human rights are always evolving. Canadian courts have said that the constitution is a “living tree,” meaning that it can grow and evolve with time. There are several important examples of this ongoing evolution.\nThe right to equality on the ground of sexual orientation has now been “read into” the Charter, meaning that although sexual orientation is not mentioned in section 15 (the equality rights section of the Charter), it is considered as an analogous ground and is therefore protected.\nCourts often have to balance different rights and freedoms or decide whether limits on rights are justifiable in free and democratic societies. For example, the Supreme Court of Canada decided in 1992 that the crime of spreading false news was an infringement of free speech and therefore unconstitutional in the Holocaust-denial case of R. v. Zündel. On the other hand, the Court has held that criminal and human rights laws prohibiting hate speech against minorities such as Jews (see Anti-Semitism), people of colour, and gays and lesbians (see Homosexuality), are reasonable limits on free speech in the cases of Taylor (1990), Keegstra (1990), and Whatcott (2013).\nSuch cases confirm the principle that there is no hierarchy of rights and that no right or category of rights has automatic precedence over any other type of right.\nEvolving areas of human rights law include the interaction between Aboriginal rights and human rights laws, and the operation of equality rights in rapidly changing areas such as family status and disability. The right to equality on the grounds of gender expression or gender identity for trans people has been introduced in several Canadian jurisdictions (see Lesbian, Gay, Bisexual and Transgender Rights in Canada).\nIn the aftermath of the terrorist attacks of 11 September 2001, there has been a growing tension between human rights and national security, reigniting debates about torture, religious freedoms, immigration and refugee policy, and national surveillance. Racial profiling issues have also come to the fore. Since 2008, Québec has had a particularly divisive debate about the accommodation of religious minorities and the role of secularism in the state (see Québec Values Charter).\nThe renaissance of public political protest and the growing number of restrictions on civil society organizations around the world (non-governmental organizations, human rights defenders, and international development groups, and environmental groups, for example) have reignited questions about reasonable limits on freedom of association and peaceful assembly in Canada and around the world.\nDisability rights are another key area of development for human rights law in Canada. Canada lags behind the standards contained in the Convention on the Rights of Persons with Disabilities in terms of many basic rights, such as access to services, public transit, employment, and education. It has not fully implemented the Covenant on Economic, Social and Cultural Rights. Although the Optional Protocol to the International Covenant on Economic, Social and Cultural Rights came into force in 2010, Canada has declined to become a State Party. Thus, individual Canadians currently do not have the right to claim violations of their economic, social and cultural rights unless the violations of those rights are discriminatory and can be addressed under human rights laws.\nThese are all important and ongoing areas for future development. The fight for human rights and the nature of Canada’s role in the development of human rights is an important part of our national heritage and our ongoing and shared national task of respecting, protecting and fulfilling human rights. As part of this journey, the Canadian Museum for Human Rights, which opened in 2014 in Winnipeg, is the first museum in the world solely dedicated to the narrative of Canada’s human rights journey, its evolution, celebration and future.\nIrving Abella and Harold Troper, None Is Too Many: Canada and the Jews of Europe 1933–1948 (Toronto: Lester and Orpen Dennys, 1982).\nConstance Backhouse, Colour-Coded: A Legal History of Racism in Canada, 1900–1950 (Toronto: University of Toronto Press, 1999).\nA. Alan Borovoy, When Freedoms Collide: The Case for Our Civil Liberties (Toronto: Lester & Orpen Dennys, 1988).\nAnne Bayefsky and Mary Eberts, eds, Equality Rights and the Canadian Charter of Rights and Freedoms (Carswell, 1985).\nDominique Clément, Canada\'s Rights Revolution: Social Movements and Social Change, 1937-82 (Vancouver: UBC Press, 2008).\nClinton Timothy Curle, Humanité: John Humphrey’s Alternative Account of Human Rights (Toronto: University of Toronto, 2007).\nShelagh Day, Lucie Lamarche and Ken Norman, eds, 14 Arguments in Favour of Human Rights Institutions (Toronto: Irwin Law, 2014).\nPearl Eliadis, Speaking Out on Human Rights: Debating Canada’s Human Rights System (Montreal: Kingston: McGill-Queen’s University Press, 2014).\nPeter Hogg, Constitutional Law of Canada, 2013 Student Edition (Carswell, 2013).\nPeter Kulchyski, Aboriginal Rights Are Not Human Rights: In Defence of Indigenous Struggles (Winnipeg: ARP Books, 2014).\nWilliam Pentney, Discrimination and the Law, loose-leaf edition (Carswell, 1990).\nPatricia E. Roy, A White Man’s Province: British Columbia Politicians and Chinese and Japanese Immigrants, 1858–1914 (Vancouver: University of British Columbia Press, 1989).\nMaxwell Yalden, Transforming Rights: Reflections from the Front Lines (Toronto: University of Toronto Press, 2009).\nRobin W. Winks, The Blacks in Canada: A History, 2d ed. (Montreal: McGill-Queen’s University Press, 1997).', ""(Logo by Heather Chase of Chase Designs).\nThe Witches' Voice Inc.|\nWe at the Witches' Voice are continually compiling a important reference documents, form letters that you can use in your local fight for YOUR freedom... Note: The Witches' Voice Inc. does not offer legal advice nor are we qualified to do so. This document does not constitute legal advice but is intended to be used in conjunction with the legal services of an attorney licensed to practice in your state. This document can be copied and distributed to your lawyer should you decide that you need the services of one.\n||Chapter Page Views: 1,746,670\nCanadian Charter of Rights & Freedoms|\nNOTE: Below are the portions of the Canadian Charter of Rights and Freedoms which directly pertain to religious issues in Canada:\nBeing Part I of the Constitution Act, 1982 [Enacted by the Canada Act 1982 [U.K.] c.11; proclaimed in force April 17, 1982. Amended by the Constitution Amendment Proclamation, 1983, SI/84-102, effective June 21, 1984. Amended by the Constitution Amendment, 1993 [New Brunswick], SI/93-54, Can. Gaz. Part II, April 7, 1993, effective March 12, 1993.]\nWhereas Canada is founded upon principles that recognize the supremacy of God and the rule of law:\nRIGHTS AND FREEDOMS IN CANADA.\n1. The Canadian Charter of Rights and Freedoms guarantees the rights and freedoms set out in it subject only to such reasonable limits prescribed by law as can be demonstrably justified in a free and democratic society.\n2. Everyone has the following fundamental freedoms:\nDEMOCRATIC RIGHTS OF CITIZENS.\n(a) freedom of conscience and religion;\n(b) freedom of thought, belief, opinion and expression, including\nfreedom of the press and other media of communication;\n(c) freedom of peaceful assembly; and\n(d) freedom of association.\n3. Every citizen of Canada has the right to vote in an election of members of the House of Commons or of a legislative assembly and to be qualified for membership therein.\nLIFE, LIBERTY AND SECURITY OF PERSON.\n7. Everyone has the right to life, liberty and security of the person and the right not to be deprived thereof except in accordance with the principles of fundamental justice.\nSEARCH OR SEIZURE.\n8. Everyone has the right to be secure against unreasonable search or\nDETENTION OR IMPRISONMENT.\n9. Everyone has the right not to be arbitrarily detained or imprisoned.\nARREST OR DETENTION.\n10. Everyone has the right on arrest or detention:\nPROCEEDINGS IN CRIMINAL AND PENAL MATTERS.\n(a) to be informed promptly of the reasons therefor;\n(b) to retain and instruct counsel without delay and to be informed of that right; and\n(c) to have the validity of the detention determined by way of habeas corpus and to be released if the detention is not lawful.\n11. Any person charged with an offence has the right\nTREATMENT OR PUNISHMENT.\n(a) to be informed without unreasonable delay of the specific offence;\n(b) to be tried within a reasonable time;\n(c) not to be compelled to be a witness in proceedings against that person in respect of the offence;\n(d) to be presumed innocent until proven guilty according to law in a fair and public hearing by an independent and impartial tribunal;\n(e) not to be denied reasonable bail without just cause;\n(f) except in the case of an offence under military law tried before a military tribunal, to the benefit of trial by jury where the maximum punishment for the offence is imprisonment for five years or a more severe punishment;\n(g) not to be found guilty on account of any act or omission unless, at Canadian or international law or was criminal according to the general principles of law recognized by the community of nations;\n(h) if finally acquitted of the offence, not to be tried for it again and, if finally found guilty and punished for the offence, not to be tried or punished for it again; and\n(i) if found guilty of the offence and if the punishment for the offence has been varied between the time of commission and the time of sentencing, to the benefit of the lesser punishment.\n12. Everyone has the right not to be subjected to any cruel and unusual treatment or punishment.\n13. A witness who testifies in any proceedings has the right not to have any incriminating evidence so given used to incriminate that witness in any other proceedings, except in a prosecution for perjury or for the giving of contradictory evidence.\nEQUALITY BEFORE AND UNDER LAW AND EQUAL PROTECTION AND BENEFIT OF LAW / Affirmative action programs.\n15. (1) Every individual is equal before and under the law and has the right to the equal protection and equal benefit of the law without discrimination and, in particular, without discrimination based on race, national or ethnic origin, colour, religion, sex, age or mental or physical disability.\nENFORCEMENT OF GUARANTEED RIGHTS AND FREEDOMS / Exclusion of evidence bringing administration of justice into disrepute.\n(2) Subsection (1) does not preclude any law, program or activity that has as its object the amelioration of conditions of disadvantaged individuals or groups including those that are disadvantaged because of race, national or ethnic origin, colour, religion, sex, age or mental or physical disability.\n24. (1) Anyone whose rights or freedoms, as guaranteed by this Charter, have been infringed or denied may apply to a court of competent jurisdiction to obtain such remedy as the court considers appropriate and just in the circumstances.\n(2) Where, in proceedings under subsection (1), a court concludes that evidence was obtained in a manner that infringed or denied any rights or freedoms guaranteed by this Charter, the evidence shall be excluded if it is established that, having regard to all the circumstances, the admission of it in the proceedings would bring the administration of justice into disrepute.\nABORIGINAL RIGHTS AND FREEDOMS NOT AFFECTED BY CHARTER.\n25. The guarantee in this Charter of certain rights and freedoms shall not be construed so as to abrogate or derogate from any aboriginal, treaty or other rights or freedoms that pertain to the aboriginal people of Canada including:\nOTHER RIGHTS AND FREEDOMS NOT AFFECTED BY CHARTER.\n(a) any rights or freedoms that have been recognized by the Royal Proclamation of October 7, 1763; and\n(b) any rights or freedoms that now exist by way of land claims agreements or may be so acquired.\n26. The guarantee in this Charter of certain rights and freedoms shall not be construed as denying the existence of any other rights or freedoms that exist in Canada.\n27. This Charter shall be interpreted in a manner consistent with the preservation and enhancement of the multicultural heritage of Canadians.\nRIGHTS GUARANTEED EQUALLY TO SEXES.\n28. Notwithstanding anything in this Charter, the rights and freedoms referred to in it are guaranteed equally to male and female persons.\nRIGHTS RESPECTING CERTAIN SCHOOLS PRESERVED.\n29. Nothing in this Charter abrogates or derogates from any rights or\nprivileges guaranteed by or under the Constitution of Canada in respect of denominational, separate or dissentient schools.\n|Important Note: The Witches' Voice Inc. does not offer legal advice nor are we qualified to do so. This document does not constitute legal advice but is intended to be used in conjunction with the legal services of an attorney licensed to practice in your state. This document can be copied and distributed to your lawyer should you decide that you need the services of one.|\nWeb Site Content (including: text - graphics - html - look & feel)\nCopyright 1997-2017 The Witches' Voice Inc. All rights reserved\nNote: Authors & Artists retain the copyright for their work(s) on this website.\nUnauthorized reproduction without prior permission is a violation of copyright laws.\nWebsite structure, evolution and php coding by Fritz Jung on a Macintosh G5.\nAny and all personal political opinions expressed in the public listing sections (including, but not restricted to, personals, events, groups, shops, Wren’s Nest, etc.) are solely those of the author(s) and do not reflect the opinion of The Witches’ Voice, Inc. TWV is a nonprofit, nonpartisan educational organization.\nSponsorship: Visit the Witches' Voice Sponsor Page for info on how you\ncan help support this Community Resource. Donations ARE Tax Deductible.\nThe Witches' Voice carries a 501(c)(3) certificate and a Federal Tax ID.\nMail Us: The Witches' Voice Inc., P.O. Box 341018, Tampa, Florida 33694-1018 U.S.A.""]"	['<urn:uuid:6d1a81f3-e337-4dcd-a41a-d7a2d6d9602a>', '<urn:uuid:459ea5c6-c334-46f2-ab58-1c3817449a0d>']	factoid	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-13T03:34:55.284799	7	50	3938
25	I'm researching humane practices for a commercial lobster facility. Could you explain the correct technique for splitting lobsters to ensure a quick and ethical death?	Splitting must only be performed after the lobster is properly stunned and showing signs of insensibility. The process involves rapidly cutting through the centre-line of the head, thorax, and abdomen with a large, sharp knife along the longitudinal midline. This is necessary because lobsters have multiple nerve centres (ganglia) running down their central length, and all nerve centres must be destroyed for humane killing. Splitting is not suitable for crabs, which require different techniques.	"[""Search: Advanced search\nPlease enter a keyword or ID\nWhat is the most humane way to kill crustaceans for human consumption?\nThis article is also available for download as a PDF - RSPCA Humane killing of crustaceans\nCrustaceans show responses consistent with signs of pain and distress.1-6 They also have the cognitive capacity to remember, and learn to avoid, unpleasant stimuli.7-9 As a result, RSPCA Australia considers that crustaceans should be captured, handled, transported, stored and killed humanely. This applies to all crustaceans, including crayfish, lobsters, crabs, Moreton Bay bugs and yabbies, whether the animal is to be eaten raw (sashimi) or cooked.\nKilling involves loss of sensibility (ability to feel pain), followed by death. For killing to be humane, either:\nInsensibility should persist until death intervenes.\nA variety of methods are used to capture, hold, kill and process crustaceans. The methods used depend on the species involved, the scale of the processing operation (commercial or noncommercial) and the end product. In each case, crustaceans should be killed by the most humane method.\nThe legal status of crustaceans in Australia varies between different states and territories. In New South Wales, Victoria, the Northern Territory and the Australian Capital Territory, crustaceans are protected under the relevant animal welfare legislation (in some states, this only applies to crustaceans intended for human consumption). Penalties may apply if crustaceans are not treated humanely.\nSkills and experience required\nRSPCA Australia does not recommend that live crustaceans for human consumption are made available for purchase by the general public. Instead, they should be humanely killed by trained and competent personnel before purchase.\nTraining should include how to:\nSigns of insensibility\nSigns of insensibility vary from species to species but generally include:10\nSigns of stress\nSigns of stress include:\nAcceptable stunning and killing methods\nThis advice is based on the available scientific evidence. However, further research is required before definitive conclusions can be drawn about the humaneness of stunning and killing methods for crustaceans.\nCrustaceans must not be subjected to mechanical killing without first being rendered insensible using one of the following methods.\nWith sufficient electrical current, crustaceans can be rendered insensible within 1 second of current being applied – that is, an immediate loss of sensibility.11\nOnly purpose-built electrical stunning equipment (the Crustastun) should be used, in accordance with the manufacturer’s instructions.10 Failure to adequately electrically stun may have serious welfare consequences, including a high rate of autotomy.\nCrustaceans are cold-blooded animals and reportedly enter a state of torpor at air temperatures of 4 °C or below. They are rendered insensible when their body temperature is sufficiently reduced by chilling.3\nScientific proof of the association between chilling and absence of discomfort, stress or pain is limited. However, this process is commonly considered to be effective, as crustaceans subjected to chilling do not show the behavioural signs of stress that occur when some other killing methods (such as boiling) are used.12 Further research is needed to fully understand the effects of different chilling methods on crustacean welfare.\nOne major benefit of chilling is that it reduces mobility. This makes crustaceans easier to handle and humanely kill, and also prevents individuals from injuring each other.13\nChilling in an ice slurry\nTropical species of crustaceans and temperate species that are susceptible to cold temperatures may be stunned by chilling in an ice slurry.14 Insensibility occurs more quickly in an ice slurry than in air at similar temperatures because water absorbs heat much faster than air.3,15-16\nA saltwater ice slurry must be used for all marine species. Marine crustaceans should never be placed in a freshwater ice slurry because this is likely to induce osmotic shock.\nFreshwater crustaceans should never be placed in a saltwater ice slurry.\nChilling in an ice slurry is not recommended for temperate marine species that are adapted to colder temperatures.3,12 When a saltwater ice slurry is used, the salinity of the water in the slurry decreases as the ice melts, potentially causing osmotic shock if the animal is left in the slurry for too long. For cold-adapted species, this may occur before insensibility has been reached, unless the salinity of the slurry is maintained. Monitoring and proper control of salinity of the slurry may help to overcome this potential welfare problem.3\nProcedure: chilling in an ice slurry\nChilling in air\nLarge crustaceans that are adapted to very cold temperatures may be stunned by chilling in air. Chilling in air takes longer than chilling in an ice slurry because of the slower rate of heat transfer to air than to water.3,14,17\nProcedure: chilling in air\nMechanical killing methods\nOnce crustaceans are stunned and are showing signs of insensibility, they should be mechanically killed as soon as possible to ensure that they do not recover.16\nCrustaceans have multiple nerve centres (ganglia). Humane killing requires rapid destruction of all the nerve centres. It is not possible to kill crustaceans quickly by destroying just a single central location (unlike in vertebrates).\nSplitting is suitable for lobsters and similarly shaped species. Lobsters have a chain of nerve centres running down their central length (ventral longitudinal midline) (Figure 1). All the nerve centres are beneath the longitudinal midline on the animal’s undersurface, except the first nerve centre, the supraoesophageal ganglion, which is located at the top end of the chain and is reached through the head rather than the undersurface.10,14\nFigure 1 Cross-section view of lobster, showing internal ganglia\nSplitting involves rapidly cutting through the centre-line of the head, thorax (chest) and abdomen with a large, sharp knife. Cutting must occur along the longitudinal midline (lengthways) to destroy all the nerve centres (Figure 2).\nFigure 2 View of lobster from above (or below), showing line of cut for lobster splitting\nSpiking is suitable for crabs. Crabs have two main nerve centres. One is located at the front of the animal, under a shallow depression. The other lies towards the rear of the animal and may have a small hole positioned over it (Figures 3 and 4).10,14\nFigure 3 Topside of crab, showing internal ganglia (nerve centres)\nFigure 4 Underside of crab, showing reference points for spiking\nCrabs can be killed by rapid destruction of both nerve centres by piercing both ganglia from the underside of the crab with a pointed spike (e.g. a thick, pointed pithing instrument, an awl or a sharp-pointed knife).\nSpiking must not be performed on lobsters because they have a long chain of nerve centres.\nFigure 5 Cross-section view of crab, showing angles for spiking\nUnacceptable killing methods\nAnaesthetic agents (AQUI-S and clove oil) are not included as acceptable methods for the humane killing of crustaceans because it is not yet known whether they are safe for human consumption.\n1 Elwood RW (2012). Evidence for pain in decapod crustaceans. Animal Welfare 21(S2):23–27.\n2 Barr S, Laming PR, Dick JTA & Elwood RW (2008). Nociception or pain in a decapod crustacean? Animal Behaviour 75:745–751.\n3 European Food Safety Authority (2005). Opinion on the ‘Aspects of the biology and welfare of animals used for experimental and other scientific purposes’. EFSA Journal 292:1–46.\n4 Broom DM (2007). Cognitive ability and sentience: which aquatic animals should be protected? Diseases of Aquatic Organisms 75:99–108.\n5 Appel M & Elwood RW (2009). Motivational trade-offs and potential pain experience in hermit crabs. Applied Animal Behaviour Science 119(1–2):120–124.\n6 Appel M & Elwood RW (2009). Gender differences, responsiveness and memory of a potentially painful event in hermit crabs. Animal Behaviour 78(6):1373–1379.\n7 Elwood RW & Appel M (2009). Pain experience in hermit crabs? Animal Behaviour 77:1243–1246.\n8 Elwood RW, Barr S & Patterson L (2009). Pain and stress in crustaceans? Applied Animal Behaviour Science 118:128–136.\n9 Magee B & Elwood RW (2012). Shock avoidance by discrimination learning in the shore crab (Carcinus maenas) is consistent with a key criterion for pain. Journal of Experimental Biology 216:353–358.\n11 Roth B & Øines S (2010). Stunning and killing of edible crabs (Cancer pagurus). Animal Welfare 19(3):287–294.\n12 Yue S (2008). The welfare of crustaceans at slaughter, Humane Society of the United States.\n13 Davidson GW & Hosking WW (2004). Development of a method for alleviating leg loss during post-harvest handling of rock lobsters, project 2000/251, Fisheries Research and Development Corporation & Geraldton Fishermen’s Cooperative, Geraldton.\n14 NSW Department of Primary Industries. Humane harvesting of fish and crustaceans. www.dpi.nsw.gov.au/agriculture/livestock/animal-welfare/general/fish/shellfish\n15 Tseng Y, Xiong YL, Webster CD et al. (2002). Quality changes in Australian red claw crayfish, Cherax quadricarinatus, stored at 0 °C. Journal of Applied Aquaculture 12(4):53–66.\n16 Beatty SJ, Morgan DL & Gill HS (2004). Biology of a translocated population of the large freshwater crayfish, Cherax Cainii. Austin & Ryan, 2002 in a Western Australian river. Crustaceana, 77 (11):1329–1351.\n17 Queensland Department of Agriculture and Fisheries (2015). Freshwater crayfish for lab examination.\nThis website provides general information which must not be relied upon or regarded as a substitute for specific professional advice, including veterinary advice. We make no warranties that the website is accurate or suitable for a person's unique circumstances and provide the website on the basis that all persons accessing the website responsibly assess the relevance and accuracy of its content.""]"	['<urn:uuid:fe33a97f-a38b-4463-b79c-497d1c2b692a>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T03:34:55.284799	25	74	1516
26	digital artist looking for apps realistic paint textures effects compare artrage ibispaint features	ArtRage specializes in realistic textures and effects, tracking paint application and wetness, with tools for smudging and smearing. It's particularly good for watercolor and oil paint mediums, offering unlimited layers and Photoshop blending modes. IbisPaint, on the other hand, offers 140 different brushes including dip pens, felt tip pens and paint brushes with adjustable opacity, angle and thickness. It also supports unlimited layers and includes features like eraser, smudge, blur, ruler, blender, bucket fill, zoom and image stabilizer.	['Adobe Illustrator Draw\nAdobe Illustrator Draw is a part of Adobe’s Creator applications and offers a variety of features like layers, 5 different pen tips with various customisation features and a zoom in up to x64 to work on details. You can change the opacity of the canvas background and add more shapes by installing third-party shape libraries. Adobe Illustrator Draw is free with in-app purchases available, that may cater to professionals. Professionals can easily create and manage projects and a project can have as many drawings as you want! You can import designs from Adobe Capture CC and export to different platforms, like Behance, copy your image to the Adobe Creator Cloud, or even export to your desktop for use on other Adobe products like Photoshop CC, Illustrator CC, Capture CC and Photoshop\nWith more GIFs supported in different social media platforms, more apps are available to generate animated GIFs of your drawing. There are features to add text to photos, change the text size, font and opacity before adding it to the image. ArtBoard also offers several brushes and 4 unique blending modes. The brushes are divided into two categories: basic and artistic and you can customise the size, strength, hardness and spacing. You can create up to 15 layers and have a colour mixer and palette tool and even the option to insert an image in the canvas. The best part is, apart from making GIFs, you can create a drawing and record it in the form of videos. It is a drawing and animation app in one!\nArtecture is an excellent app for beginners because they offer users practical tools like pencil, marker pen, oil brush, oil blend brush and real oil brush. It allows users to learn the basics of drawing symmetries by having vertical, horizontal and concentric ruler guidelines. After you sketch, draw and paint, your creations can be extracted in formats like jpg, png and bmp.\nCredit: Artecture Google Play\nArtFlow is one of the in-depth drawing apps that has a smooth UI; perfect for beginners and professionals alike. When you start drawing, the app hides its navigation bars to make sure you have enough space to draw on the canvas. Features include up to 70 different brushes, smudge and other tools. They also have layers and a layer blending functionality, and offers up to 6 shape tools. You can flip the canvas horizontally or vertically and also frame the canvas!\nWith more time-lapse drawing tutorials popping out on social media, ArtFlow makes sure you can achieve this seamlessly as it can record a time-lapse video of your drawing. It can also export to jpeg, png, and PSD (to import to Photoshop later), but if you want a hard copy of your creation, it also has an inbuilt tool to print your photo.\nArtRage is perfect for those who want the impression of real textures and effects that you can get from real paint—it tracks how much paint you have applied and even how wet the paints are. You can smudge and smear with various tools and is perfect for users who want to indulge in watercolour or oil paints as their medium. There are unlimited layers and photoshop blending modes, which are very forgiving features for beginners and kids. There is also unlimited undo and redo and opacity controls.\nAutodesk is one of the most popular apps out there, suitable for both beginners and professionals alike. The app can be used on both Android and iOS. Features include a variety of drawing tools and up to 10 customizable brushes, the ability to create and manage layers and a full-screen drawing mode. There are 6 blending modes and up to 2500% zoom to enable you to add minute details to your work. It also offers a simulated pressure sensitivity to make sure you can emulate real life strokes in your drawing.\nThere is an eraser, image cropper, pinch to zoom feature, blender utility, colour picker, ruler tool and a gallery organiser feature embedded as well. For vector images, users can easily insert a line, square or circle in a picture. When done, drawings can be shared on different social network platforms.\nCredit: www.sketchbook.com; www.filehorse.com\nOur games and cartoon animations started off with pixel art but it is an art form long forgotten. A lot of newer games have been picking it up again recently too! (Think, Minecraft) Dotpict allows you to do pixel art by offering up a grid, and you can zoom in and create little scenes or people by filling up the pixel boxes. No need to worry about losing your work as it comes with an auto-saving feature, and an undo, redo and export function to secure your job when it’s done.\nCredit: DotPict Google Play\nFresco Paint Pro\nThis app offers 12 different brushes, up to 4 layers and 21 filters including blur and vignette. Your creations can also be exported to Photoshop. In our opinion, this is best suited for beginner drawers.\nIbisPaint has one of the most comprehensive features in a drawing app with 140 different brushes including dip pens, felt tip pens and actual paint brushes, where you can change the brush opacity, angle and thickness. It has layer support where you can have unlimited layers, can modify layer parameters and offers a palette tool that contains 30 colours. It also allows users to replace colour on the palette with one they use most frequently. How convenient is that? Several filters are in place, like the eraser, smudge, blur, ruler, blender, bucket fill, zoom and image stabiliser. You can also quickly change the orientation of the canvas. IbisPaint also caters explicitly to those who want to create specific types of drawings like mangas and animes.\nInfinite Painter comes with more than 80 brushes and also the option to create your brush and brush settings! The brushes can emulate a realistic interaction with digital paper texture and create the impression that you are working on paper instead of a tablet. You can also create as many layers with their Photoshop blending modes, perspective guides selection masks and clipping masks. Adding details is also easy with their 2500% zoom feature. Its pro version includes more brush sets like a synthetic and smudge brush, more layers and gradient fill tools.\nLearn How to Draw\nThe Learn How to Draw app, as its name suggests, offers a variety of tutorials for beginner to advanced drawers. Mainly focusing on comic book style drawing, there are new tutorials to follow almost every week. The advantage of using this app is that you can directly practice on the app itself, saving you the time of switching between apps if you were to follow a tutorial on YouTube.\nCredit: Learn How to Draw Google Play\nOne of the best features of MediBang paint is its cross-platform support and cloud saving feature. You can download this app from mobile, Mac or even Windows. You can also keep your work in one spot and move it to another platform, making your creation accessible anywhere at any time. They have a decent number of brush tools that are suitable for free drawing and comic drawing.\nPaperOne’s edge is in its ability to emulate your creations to real life as closely as it can. There are different brush types with different opacity depending on the pen pressure sensitivity and this enables users to emulate actual life strokes and lines. There is also a tracing feature where you can import a picture and set it to the transparent mode and users can trace the original photograph. It helps beginners get a feel for face proportions or different drawing styles.\nThis app allows you to create animations by drawing it frame by frame and string it together at the end to develop little cartoons. You can control the frame rate and resolution and finished projects can be exported as GIFs, a Quicktime video or as an image sequence. Most free messaging apps now support the sending of GIFs, so make your customised GIFs and share the fun with your friends!\nSketch allows you to set a background colour for your canvas and offers 15 unique brushes that are customizable and stickers that you can impose on your images. You can create new paintings, add text to pictures and rotate the canvas. You can also glam up your previously taken pictures with sticker packs and crop them. Just like a photo editing app, you can also change the brightness and contrast of images. The app comes with a ruler tool and colour palette utility and redo and undo function. It also allows you to backup or sync your creations to multiple devices.']	['<urn:uuid:a8aa1ac2-e5c0-4b73-93c3-2f0e54786f54>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T03:34:55.284799	13	78	1451
27	I've noticed some fascinating patterns in bird flocks and have been wondering how large groups of birds manage to fly so well together without a leader. Could you explain the underlying mechanisms behind this coordinated movement?	This is an example of self-organization in complex systems, where globally coherent patterns emerge from local interactions. In the case of flocking birds, the coordination is achieved through three simple local rules: 'separation' (avoiding crowding through short-range repulsion), 'alignment' (steering towards the average heading of neighboring birds), and 'cohesion' (steering towards the average position of neighbors through long-range attraction). These rules result in well-organized flock patterns even without central command. Similar self-organizing behavior can be observed in other natural systems like shoals of fish, swarms of insects, bacteria colonies, and herding land animals.	"[""'Complexity' is a technical term. It is not the same thing as complicatedness. What is a complex system? In general terms, a complex system consists of a number of interacting 'members', 'elements' or 'agents', which have the potential to generate qualitatively new collective behaviour. Manifestations of this new behaviour are the spontaneous creation of new spatial, temporal, or functional structures.\nComplex systems can self-organize, which means that globally coherent patterns can emerge in them out of local interactions. Flocking behaviour of birds is an example of this. Simple local rules like 'separation' (avoidance of crowding, or short-range repulsion), 'alignment' (steering towards the average heading of neighbours), and 'cohesion' (steering towards the average position of neighbours, or long-range attraction), result in well-organized flock patterns, even when nobody is in command.\nMany other such examples of self-organization can be seen in Nature: shoals of fish; swarms of insects; bacteria colonies; herding behaviour of land animals. Experiments carried out on humans showed something similar: When 5% of the 'flock' changed direction, others followed suit.\nPer Bak, who made seminal contributions to complexity science, gave the following definition: Complex systems are systems with large spatial and/or temporal variability. In the context of this definition, some counter-examples are a gas and a crystal. Both are epitomes of uniformity or sameness, with hardly any variability; all portions are the same.\nWe can formally define complexity as something we associate with a complex system.\nAs explained in Part 30, a characteristic feature of complex systems is the emergence of unpredictable and therefore unexpected properties or behaviour. The emergence of life out of nonlife was one such property.\nWe humans and our interactions with one another, and with our biosphere, are among the most complex imaginable systems. What is our future going to be like? Although we cannot make definite predictions, even probabilistic statements about the more likely scenarios can have a salutary effect on how we conduct our affairs (e.g. regarding the management of climate change) to achieve high levels of sustainability.\nWhy do complex systems self-organize? The answer to such questions has to do, as usual, with the second law of thermodynamics for open systems. Imagine a bathtub filled with water. Suppose you suddenly pull out the stop. An interesting vortex structure develops soon, as the water drains out. The vortex is an ordered dynamic structure, which appears to have emerged 'spontaneously' from stagnant water. So this is self-organization. Strictly speaking there is really nothing spontaneous about it, because there is a driving force, namely gravity. We still call it 'spontaneous' because it has happened even when no design work by anybody went into it.\nUnder the continual action of the driving force (gravity), the system is in a far-from-equilibrium condition, in which, although there is an overall increase of disorder (entropy) as the water is accelerated in an irreversible manner down the drain, there is creation of order locally.\nThe whirlpool is an energy-dissipating structure. It is also an example of energy-driven organization, popularly known as just self-organization, resulting in 'emergent' or unexpected properties or patterns.\nNiele (2005) made a distinction between driving forces and shaping forces in the emergence and evolution of complex dissipative structures. In the whirlpool, the driving force sustaining the energy-dissipating structure results from an energy gradient, whereas the shaping forces come from interactions within the whirlpool and with the surrounding tub etc. For example, the shape of the tub and the shape of the drainage hole influence the shape of the vortex. The shaping forces within the whirlpool are encoded in the structure of water molecules and in the interactions among them (predominantly 'hydrogen-bond' interactions). No water molecule has any embedded information or instructions about how to construct the vortex. Yet ‘strings of synchronized interactions’ among the water molecules do the shaping of the complex vortex structure.\nIn the context of complexity, the important point is that it is impossible to go backwards from the observed whirlpool structure, and work out in a reductionistic fashion the details of the positions and velocities of all the molecules and the interactions among them that have given rise to the observed complex behaviour. This is generally true of all dissipative systems. And most real-life systems are dissipative systems.\nSimilarly, except in a broad macroscopic or hydrodynamic sense, the observed complexity cannot be predicted in detail in a constructionistic fashion from the underlying simplicity of the shapes of the molecules and the interactions among them.\nThe gross features of order and pattern in the whirlpool are on a scale millions of times larger than the features of the interactions causing them. In any case, one cannot perform computations at infinite speed, and a system is said to be computationally irreducible if the simplicity underlying it cannot be worked out or computed in reasonable time. Both reductionism and constructionism stand discounted.\nA principle of self-organization was enunciated by the British cybernetician W. Ross. According to it, an open dynamical system tends to move towards the nearest attractor in phase space. What is the mechanism of self-organization by movement towards the nearest attractor? It is the either deterministic or probabilistic ('stochastic') variations that occur in any dynamical system, enabling it to explore different regions in phase space until it reaches an attractor. Entering the attractor stops further variation outside the basin of the attractor, and thus restricts the freedom of the components of the system to behave independently. There is an increase of coherence, or decrease of local entropy.\nMy book Complexity Science discusses such things in substantial detail.""]"	['<urn:uuid:e9e66271-50ab-4fa1-968b-16cfee1d0736>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T03:34:55.284799	36	93	923
28	Do Tuscan and regular tomatoes need different temperatures to grow?	Both types of tomatoes need similar growing conditions. All tomatoes need soil temperatures around 60°F and night temperatures shouldn't fall below 55°F. The Tuscan tomatoes specifically benefit from the mild climate of Tuscany, while regular tomatoes need at least three months of warm weather to produce a good crop.	"['Features of the Tuscan\nThe cultivation is 99% transplanted, cultivating the seedlings from the month of April to the first / second week of June. The growing cycle varies between 90 days for early varieties and 120 days for the late varieties. The industrial tomato crops need to be irrigated frequently, because the tomato is composed of water for the 94% of its weight: Tuscan companies associated to ASPORT all use the techniques of drip irrigation (micro irrigation) through the hose (driplines) and this technique is combined to the fertigation, namely the administration of nutrients via the above-mentioned equipment.\nThe water used for irrigation comes from very deep aquifers through the use of artesian wells: the water samples are analyzed every 6 months resulting perfectly drinkable. The same agricultural techniques are also used for BIO / organic crops, obviously with the obligation to use only products approved by the European regulations. The plants of industrial tomatoes are excellent for crop procession, thus integrating perfectly into the agricultural landscape; besides, the tomato has a highly developed root system, of about 50 cm, and is perfectly suited to the mild and windy climate of Tuscany in different climatic conditions.\nTuscan terrain\'s features\nAgricultural terrains in the Val di Cornia, Val di Chiana, Grosseto, Maremma, Siena’s and Pisa’s countrysides enjoy the benefits from their proximity to the sea and to the mountains. They are rich in minerals, such as phosphorus and potassium, and can boast the use of the deepest aquifers for the irrigation of the fields, thanks to the artesian wells. These areas have clay-sandy soils with high permeability, ideal for preventing water stagnations and, consequently, the proliferation of mold.\nA – Campagne Pisane. Along the Arno river are rich and loamy soils\nB – Val di Cornia. Mineral wealth for our production\nC – Val di Chiana e Campagne Senesi. a nature wonder accompanied by the river Chiani\nD – Maremma e Grosseto. The proximity to the sea makes unique our tomato crops\nASPORT is an organization of fruit and vegetable producers established in 1971 with an operating area that stretches across the Tuscan territory and also in other regions of Italy. For over 40 years a leading association in the cultivation of industrial tomatoes, now it has more than 600 farms including 7 cooperatives. The ASPORT ensures jobs and income for member companies that produce respecting the environment and the consumers’ health, thanks to a careful variety selection aimed at obtaining good production, in terms of quantity and quality, and the application of rules for integrated and organic production.\nRosso Toscano Project\nThe project “Rosso Toscano” (Tuscan Red) is the first network contract born in Italy to enhance the supply chain of canned and preserved tomatoes, thus concluding the process begun in 2012 with the Region of Tuscany and the president Enrico Rossi to close the production chain. Jointly with ASPORT (Association of Tuscan fruit and vegetable producers) and CTT (Consorzio Trasporti Toscano) Petti created a project for the support, development and innovation of the entire supply chain of the Tuscan tomatoes. The network contract, Rosso Toscano, is another tangible sign and support that Petti is providing to the territory on a daily basis.\nThe goal is clear: organizing all the processes of transformation of the raw material together with the agricultural part, the ASPORT, that has been providing industrial tomatoes of excellent quality for more than thirty years to our plant in Terme Venturina (LI). A raw material now certified and guaranteed through the Agriqualità certification mark issued by the Tuscany Region. The purpose is also to share facilities and resources for a modern integrated logistics with CTT, the transport company that feeds the plant during the summer season with fresh tomatoes from all farms distributed in Tuscany.\nWhen reaching the right degree of ripeness the tomato is gathered in the fields through self-propelled harvesting machines equipped with optical switch. From there they are loaded on trucks dedicated to the transportation to the Petti plant. Within 6 hours from the harvest the tomato is processed in the plant of Venturina Terme, in the province of Livorno. Here follow various stages of production, in particular the tomato is processed at low temperature. Thanks to this unique method the products of “Petti – Il Pomodoro al centro” keep intact the bright red color and the taste of freshly harvested tomato.\nThe products branded “Petti – Il pomodoro al centro” are certified by the Tuscany Region through the ""Agriqualità"" brand, identifying and promoting agri-food products cultivated with integrated farming techniques, or with agricultural production methods compatible with environmental protection requirements and with the maintenance of the countryside. This certification therefore certifies and guarantees clearly and unequivocally that the Petti products are made with 100% Tuscan tomato, as the mark of the winged Pegasus, coat of arms of the Tuscany Region, is reminding on each label. The line ""Petti – Il Pomodoro al centro"" (Tomato comes first) is the only line among the preserved products in the large scale Retail in Italy that can boast a certification mark of the entire production chain issued by the Tuscany Region, which stands out for its uncontaminated territories and for its agri-food production of high quality recognized worldwide. This label appears on all product packaging, a further guarantee for consumers on the traceability of the entire supply chain.', 'Tomatoes generally take between seventy and ninety days to grow and ripen, depending on the variety you decide to grow and the climate where you live. It’s recommended that you plant your tomato seed in the spring at the beginning of the growing season. This will give your plants time to grow and produce a crop of fruit throughout the summer.\nTomatoes are a warm-season crop, are relatively easy to grow, and are tasty and versatile. Let’s look at the best time of year to plant tomatoes to achieve a bountiful harvest.\nWhen is the best time of year to plant tomatoes?\nThe simple answer is that tomatoes should be planted in the early spring after the last frost. However, to determine the perfect time, you’ll also need to consider factors such as your location and its climate and temperature. The type of tomato plant you plan to grow, how long it will take to mature, and the fruit to ripen.\nA tomato plant needs at least three months of warm weather to produce a great crop, as a general rule. It’s, therefore, a good idea to plant your tomatoes in the spring so that they can grow throughout the summer months. You may need to plant your seeds inside before the last frost so that they are ready to go out in your yard when the weather begins to heat up.\nWhen growing tomatoes, you have two choices, you can either plant seeds, or you can buy ready-grown seedlings. If you’re planting seeds, you can grow these indoors in March or April, depending on which state you live in. If you live in USDA hardiness zones eight or nine, you can start growing your tomato seeds indoors as early as mid-January.\nFactors that determine the best time of year to plant tomatoes\nWhen deciding when to plant tomatoes, there are many things to consider, such as temperature, variety of seeds, and how long the tomatoes take to grow. You can also consult the seed packet for more advice about when to plant. Here are some things you may like to consider:\nFor tomatoes to establish themselves, thrive, and continue to thrive and produce fruit, the temperature at night shouldn’t fall below 55℉. If you plant seeds outside at a lower temperature, they won’t be able to germinate and sprout.\nDon’t move seedlings that have been grown inside, outdoors until temperatures start to rise. Seedlings need to acclimatize before they are moved outside full time and left out at night. You can put your seedlings outside for a couple of hours during the warmest part of the day and bring them inside at night. Don’t leave them outside full time until the night temperature is around 60℉.\nAs well as air temperature, it’s also a wise idea to think about soil temperature. The best soil temperature for growing tomatoes is 60℉. You can use a soil thermometer to test the temperature of the soil. Some gardeners can tell whether the soil is warm enough or not by sticking their finger about two inches into the ground. This knowledge will come with experience. If the soil feels too cold to keep your finger there for over a minute, then it’s not yet warm enough to grow tomatoes.\nSome states are warm year-round and have multiple summer-like growing seasons. For example, many places in California have a growing season that starts in the fall around October and another growing season that begins in January or February.\nExposure to the sun\nTomatoes like to grow in sunny conditions and should be exposed to full sun for at least six hours a day. If you don’t get at least half a day of sun in the spring when you’re planning to plant your tomatoes, it may be best to start the seeds off indoors or wait a couple of weeks until the weather gets better.\nTomato type and variety\nThere are two types of tomatoes available; determinate tomatoes or indeterminate tomatoes. Each kind also has many different varieties. It’s a good idea to think carefully about which kind you want to grow as the choice will determine the number of tomatoes you’re likely to produce as well as the duration of the harvest. The type of tomatoes you choose will have varying growing requirements, and you’ll need to plant them at different times of the year.\nDeterminate tomatoes include varieties that are smaller and bushier. These types of plants yield fruit over a shorter period of time. They produce lots of tomatoes over a spec of between two or three weeks, after which the plant deteriorates.\nIndeterminate tomato plants are longer lasting and will continue to produce fruit throughout the season.\nRegardless of the variety of tomato, there can be challenges growing them. Black worms are a common problem, for instance. Slugs and snails like to eat tomatoes, so make sure to be on the lookout for them.\nHow to determine whether you still have time to plant tomatoes\nTomatoes can be planted later in the growing season as long as they have enough time to mature and produce fruit. If you haven’t planted your tomatoes in the spring, all is not lost; you may still have time to plant them. Each tomato variety has a certain number of days to maturity. This figure can be used to work out whether you have enough time to grow tomatoes this season.\nYou can use a simple math formula to determine whether it’s a good time to plant tomatoes. First, determine when the first frost in your area is likely to be. Then count how many days you have left before that date. You can then compare this with the number of days it’s likely to take the tomato plant to reach maturity.\nIf the number you have left until the first frost in your area is more than it will take your tomato plants to mature, you can plant your tomato seeds. Different tomato varieties have varying growing season lengths. Tomatoes are generally categorized into either early season varieties, mid-season varieties, or late-season varieties.\nEarly season tomatoes\nEarly season tomatoes generally take between 42 and 65 days to reach maturity and produce fruit. Popular varieties include bush beefsteak tomatoes which take 62 days to mature, cold set and cherry tomatoes which take 65 days; and subarctic tomatoes, which take just 42 days to mature.\nMid-season tomatoes grow slightly slower than early season varieties and take between 70 to 80 days to mature. Popular mid-season varieties include Abraham Lincoln tomatoes, Atkinson, Beefsteak, and Brandywine.\nLate season tomatoes\nLate-season tomatoes take at least 80 days to reach maturity, with some such as Bull’s Heart taking 90 days. Some of the most common late-season tomatoes include Amana Orange, Cherokee Purple, Hugh’s, and Hillbilly, which all take around 85 days.\nThe best time of year to plant tomatoes will depend on various factors, including location, climate, and variety of tomatoes. Many tomatoes will grow throughout the summer, while others have a shorter time to produce fruit, so you may need to stagger your planting if you want to eat home grow tomatoes all growing season.']"	['<urn:uuid:f1a1f8b3-cf9c-47b6-966a-517a51ce9a2d>', '<urn:uuid:dcd25440-0c65-429a-81c2-d4b30e41128c>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T03:34:55.284799	10	49	2088
29	federal direct loan versus graduated repayment plan first 2 years interest payment comparison	During the first two years, there is a key difference. For Direct Unsubsidized Loans, the student is responsible for paying the interest during all periods, and if they choose not to pay it, the interest will accumulate and be added to the loan amount. In contrast, under the Graduated Repayment Plan, the monthly payments during the first two years only comprise the interest charged on the principal amount, which must be at least 25% of the standard payment or the interest charged, whichever is higher.	['Types of Financial Aid\nThere are many different types of financial aid that may be available to you. It’s important to understand each type of aid so you make the best financial decisions possible for your future.\nOn this page:\nBasic Financial Aid Requirements\nSMCC offers financial aid to full-time and part-time students who:\n- Are accepted into an SMCC degree or certificate program. For more information, visit our Apply webpage.\n- Have completed a Free Application for Federal Student Aid (FAFSA). To complete yours, visit the FAFSA website.\n- Meet SMCC’s Satisfactory Academic Progress (SAP) standards for financial aid recipients. To learn more, download the SMCC SAP handout.\n- Meet the Basic Eligibility Criteria established by the U.S. Department of Education. To learn more, visit the Federal Student Aid website or download our Financial Aid Basic Eligibility Criteria handout.\n- Are considered ‘gift aid’ and, unlike loans, are not required to be repaid.\n- Are awarded based on financial need and other eligibility criteria as specified by the Department of Education or governing agency.\n- May be available for those who qualify while enrolled full-time or part-time.\nFederal Pell Grants\n- Federal Pell Grants are awarded to undergraduate students with financial need who have not yet earned a bachelor’s degree.\n- Students apply for a Pell Grant by completing the FAFSA.\n- The maximum Pell Grant award changes from year to year and is determined by the student’s financial need and whether the student is attending full-time or part-time.\n- You cannot receive a Federal Pell Grant for more than 12 full-time semesters.\nFederal Supplemental Educational Opportunity Grants (FSEOG)\n- FSEOG funds are awarded to students with exceptional financial need and are need-based grants awarded to students who also qualify for the maximum Federal Pell grant.\n- Students apply for FSEOG funds by completing a FAFSA.\n- Students who enroll for less than 6 credits will lose their eligibility for this grant.\nState of Maine Grant\n- Are available to students who:\n- Are Maine residents for at least one year prior to the beginning of the school year.\n- Complete a FAFSA by May 1 every year.\n- Meet a certain level of financial need as specified each year by the Finance Authority of Maine.\n- Do not have to be repaid to the state of Maine.\n- For more information about the State of Maine Grant and other financial aid programs offered by the State of Maine, visit the Finance Authority of Maine’s website.\nFederal Direct Loans\n- Are a low-interest loan option for students to help cover the cost of their education..\n- Must be repaid, with interest, and should be considered the last funding option for students and families to use.\n- The lender is the U.S. Department of Education, rather than a bank or other financial institution.\n- Your SMCC Financial Aid award notice will show the amount of Direct Loans you could borrow for the academic year.\n- To view an explanation of the yearly and lifetime maximum Direct Loans you may receive, visit the U.S. Department of Education’s website.\n- To learn more about the terms and requirements of these loans, Download our Direct Subsidized and Unsubsidized Loans handout.\n- Loan origination fees are deducted from each loan disbursement; for current interest rates and fees, visit the US Department of Education’s website.\n- There are limits to the total amount of Direct Subsidized Loans that one can receive. For more information,\n- To learn more about the terms and requirements of Federal Direct Subsidized Loans,\nDirect Subsidized Loans\n- Subsidized loans are based on financial need, as determined by the information provided on your FAFSA.\n- The federal government pays the interest while you are enrolled at least half-time (6 or more credits per semester).\n- Repayment of both interest and principal starts after your six-month grace period, which begins once you graduate or drop below half-time in a semester.\n- Subsidized loans disbursed after July 1, 2014, will begin accruing interest once you enter repayment. Subsidized loans disbursed between July 1, 2012, and July 1, 2014, begin accruing interest once you drop below half-time enrollment or enter your grace period.\nDirect Unsubsidized Loans\n- Eligibility is NOT based on financial need.\n- The student is responsible for paying the interest on Direct Unsubsidized Loans during all periods.\n- If you choose not to pay the interest while in school or during grace or deferment periods, your interest will accumulate and be added to the amount you borrow.\n- The amount you can borrow is determined by SMCC and may not exceed your cost of attendance minus any other aid received.\nFederal Direct PLUS Loans\nDid you know that if you’re a parent* of a dependent student attending SMCC, you may be eligible to borrow a Federal Direct Parent PLUS Loan to help pay for your child’s educational expenses?\n- The parent borrower must be credit-worthy and complete a PLUS loan application (available at the Financial Aid Office) and a PLUS Master Promissory Note.\n- Repayment begins 60 days after the loan is fully disbursed, unless deferment arrangements are made with the servicing agency.\n- A federal loan origination fee is deducted from each disbursement of the loan. For current interest rates and origination fees, visit the US Department of Education’s website.\n- PLUS loan eligibility is not listed on your financial aid award notice; contact the Financial Aid Office to determine your PLUS loan eligibility.\nDisbursement of Financial Aid Funds\nFinancial aid funds, including federal student loans and grants, are paid to student accounts approximately 2 weeks after the end of the add/drop period each semester. Funds received by the Student Accounts Office are applied to tuition and fees (and room & board, if you live in the residence hall) first. If financial aid exceeds charges, the excess is considered a credit and will be refunded to the student (or parent if a PLUS loan created the credit). The first refunds of the semester are typically available by the end of the 5th week of the semester.\nDirect Loan Disbursement Exceptions\nThere are a variety of disbursement exceptions that may apply to a student in a specific situation. To learn more about exceptions to the disbursement information outlined above, download the Direct Loan Disbursement Exceptions handout.\nStudent Loan Repayment\nAll student loans that you borrow must be repaid; therefore, it is important to understand when repayment starts, how to make your payments, repayment plan options, and what to do if you have trouble making payments. For a great starting point, visit the US Department of Education’s website.\nTo help keep your loan in good standing, we recommend the following:\n- Always inform the servicer of your loan of any changes to your name, mailing address or telephone number. To find out who your loan servicer is, visit the National Student Loan Data System (NLDS) website.\n- Review and keep copies of all documents you receive pertaining to your loans. If you do not understand the information, contact your loan servicer and ask for an explanation.\n- If you do not feel that you can afford to make your minimum loan payments, contact your loan servicer to discuss options for postponing payments through deferment and forbearance options or by setting up an alternate repayment plan.\n- If you have questions about your loans and need assistance figuring things out, we encourage you to contact the Financial Aid Office.\nFor more information, download our Direct Loan Repayment Tips & Default Information handout.\nStudent employment opportunities are available to all students who wish to work on campus and meet the following conditions:\n- Have filed a FAFSA for the current academic year (international students are exempt from having to file a FAFSA).\n- Are accepted into an SMCC degree or certificate program.\n- Maintain enrollment of at least 3 credits per semester and maintain satisfactory academic progress.\n- Are eligible to work in the US.\n- Complete and submit to the SMCC Payroll Office ALL of the required employment forms.\nOnce you’ve been accepted to SMCC you can browse currently available positions, view your work-study award and access student employment paperwork by visiting the MySMCC online student portal and selecting the student employment link.\nStudents who obtain a job on campus will be paid from one of the following funds:\nFederal Work Study\nFederal work-study is a federally funded program that provides you the ability to work part-time on-campus at SMCC to help fund the cost of your College education. There are a variety of positions on campus including those in administration, academics and facilities that allow you to gain experience in your field of interest or participate in work that serves the community.\nCollege Works for ME or Student Labor Funding\nStudents who do not meet the eligibility requirements for Federal Work Study will be paid from either College Works for ME or SMCC Student Labor funds. Eligibility is determined once the student has been hired for a position on campus and has completed the required student employment paperwork.', 'Selecting a loan repayment plan is an important decision, and it is necessary that you make this selection after knowing the pros and cons of all repayment plans. The government of US allows the borrowers of student loans to repay the borrowed amount in small monthly payments over several years. The amount of monthly payments and the repayment term depends on the type of repayment plan.\nHere is a list of repayment plans one can choose from to pay back their federal student loan.\n- Standard repayment plan\n- Pay-as-you-earn plan\n- Income-sensitive plan\n- Income-driven plan\n- Graduated repayment plan\n- Extended repayment plan\nGraduated Repayment Plan\nGraduated repayment plan offer you the flexibility to start paying back the principal amount in small monthly payments over two years. After two years of timely monthly payments, the amount is increased depending on the terms of repayment plan. In graduated repayment plan, the repayment term may extend over 10 to 30 years depending on the total amount borrowed and the type of student loan.\nWhat Type of Student Loans are Eligible for Graduated Repayment Plan?\nOne can use a graduated repayment plan for the following types of student loans:\n- FFEL PLUS loans\n- Direct PLUS loans\n- FFEL and Direct Consolidation loans\n- Direct loans, both subsidized and unsubsidized\n- Federal Stafford loans, both subsidized and unsubsidized\nHow does the Repayment Work?\nWith graduated repayment plan, repayment of the student loan starts with small monthly payments. For the first two years of repayment term, the monthly payments comprise of the interest charged on the principal amount only. After which, the amount of monthly payments is revised and the borrower pays an amount equal to standard monthly payments. These standard monthly payments contain a portion of principal amount borrowed and the interest charged on it.\nFor the first two years of repayment term, the monthly payment is equal to at least 25 percent of the standard payment or the interest charged on it, whichever is higher. When standard monthly payments start, the borrower cannot pay less than 50 percent or more than 150 percent of standard monthly payments.\nThe repayment term may extend over 10 to 30 years, depending on the type of loan. All federal direct student loans, except consolidation loans, are paid back over a period of 10 years. FFEL and direct consolidation loans have a repayment term of 30 years under graduated repayment plan.\nWhat are the Benefits of Graduated Repayment Plan?\nThe graduated repayment plan is highly flexible. It allows you start loan repayment in very affordable, small monthly payments. It’s best suited for fresh graduated who don’t have a steady source of income. In addition to this, individuals who are facing financial difficulties but who anticipate a rise in their income in the coming years may also opt for this plan.\nHowever, it is important to take this point into consideration that you are required to pay a higher interest amount under this plan. For the first two years, the monthly payments consist only of interest charged and therefore, are not counted towards loan repayment.']	['<urn:uuid:41871068-1013-4c1e-9a33-441cf9638100>', '<urn:uuid:503c87ab-0f9f-4ccf-9368-f2ed6056240d>']	factoid	with-premise	long-search-query	distant-from-document	comparison	expert	2025-05-13T03:34:55.284799	13	85	2035
30	differences relational database management systems security vulnerabilities exploitation methods	RDBMS systems have specific properties and vulnerabilities. On the property side, they must store all types of data, relate tables through mappings, isolate data from applications, prevent data duplication, support multiple users simultaneously, and provide different access views based on user roles. Regarding vulnerabilities, SQL injection attacks can exploit these systems in several ways: through in-band error-based injection (exploiting visible error results), UNION-based injection (combining multiple SELECT statements), blind attacks (reconstructing database structure through server behavior), and out-of-band attacks (using separate channels for attack and data collection). These vulnerabilities arise primarily from unsanitized inputs and dynamic SQL queries.	['1) What is SQL or Structured Query Language?\nSQL is a Structured Query Language used to communicate with database. A database is the one which will have logical data storage structure called tables. In order to retrieve data from it and store/update data in it, we need some language which can do the needful. This is provided by SQL. It supports operations like insertion, updation, retrieval and deletion.\n2) Explain Relational Database Management System (RDBMS)?\nRDBMS is a relational database management system where datas are managed in tables. Each tables in DB are inter-related by means of relationship or mappings. It is established using primary and foreign key constraints.\nRDBMS is a program that guides us how to create and maintain a database. It tells us how to divide related information into different tables and inter-relate them so that we can select/insert/update/delete all the related data easily and efficiently.\n3) Explain the properties of a relational table?\nBelow are the properties of RDBMS:\n- Should be able to store all kinds of data that exists in this real world. Since we need to work with all kinds of data and requirements, database should be strong enough to store all kinds of data that is present around us.\n- Should be able to relate the entities / tables in the database by means of a relation. i.e.; any two tables should be related. Let us say, an employee works for a department. This implies that Employee is related to a particular department. We should be able to define such a relationship between any two entities in the database. There should not be any table lying without any mapping.\n- Data and application should be isolated. Because database is a system which gives the platform to store the data, and the data is the one which allows the database to work. Hence there should be clear differentiation between them.\n- There should not be any duplication of data in the database. Data should be stored in such a way that it should not be repeated in multiple tables. If repeated, it would be unnecessary waste of DB space and maintaining such data becomes chaos.\n- DBMS has a strong query language. Once the database is designed, this helps the user to retrieve and manipulate the data. If a particular user wants to see any specific data, he can apply as many filtering conditions that he wants and pull the data that he needs.\n- Multiple users should be able to access the same database, without affecting the other user. i.e.; if teachers want to update a student’s marks in Results table at the same time, then they should be allowed to update the marks for their subjects, without modifying other subject marks. A good database should support this feature.\n- It supports multiple views to the user, depending on his role. In a school database, Students will able to see only their reports and their access would be read only. At the same time teachers will have access to all the students with the modification rights. But the database is the same. Hence a single database provides different views to different users.\n- Database should also provide security, i.e.; when there are multiple users are accessing the database, each user will have their own levels of rights to see the database. Some of them will be allowed to see whole database, and some will have only partial rights. For example, instructor who is teaching Physics will have access to see and update marks of his subject. He will not have access for other subjects. But the HOD will have full access on all the subjects.\nDatabase should also support ACID property. i.e.; while performing any transactions like insert, update and delete, database makes sure that the real purpose of the data is not lost.\n4) What is ACID mean in Sql Server?\nACID is used for evaluating application and database architecture. Below are the ACID properties\n- Atomicity : This property states that each transaction must be considered as a single unit and must be completed fully or not completed at all. No transaction in the database is left half completed. Database should be in a state either before the transaction execution or after the transaction execution. It should not be in a state ‘executing’.\n- Consistency : Any transaction should not inject any incorrect or unwanted data into the database. It should maintain the consistency of the database.\n- Isolation : If there are multiple transactions executing simultaneously, then all the transaction should be processed as if they are single transaction. But individual transaction in it should not alter or affect the other transaction. That means each transaction should be executed as if they are independent.\n- Durability : The database should be strong enough to handle any system failure. It should not be working for single transaction alone. It should be able to handle multiple transactions too. If there is any set of insert /update, then it should be able to handle and commit to the database. If there is any failure, the database should be able to recover it to the consistent state.\n5) What are the difference between “Where” and “Having” clause in Sql Server?\n“Where” clause is used to filter the rows based on condition. Even though “Having” clause used with SELECT clause, this is mainly used with GROUP BY clause. If GROUP BY clause not used then “HAVING” clause works like a “WHERE” clause.\n6) Explain primary key in Sql Server?\nThis constraint is another type of UNIQUE constraint. This constraint forces the column to have unique value and using which, we can uniquely determine each row. This column cannot be NULL.\n7) Explain unique key in Sql Server?\nIt is a constraint which enforces only the unique value in the column of the table. Unlike primary key, it will allow one row column to be NULL. But not more than one record’s column value can be NULL.\n8) Explain foreign key in Sql Server?\nForeign key is used to establish a relationship between the columns of other table. Foreign key is the one to be created between two tables by referencing primary key of one table as foreign key in another table.\n9) What is the use of “JOIN” in Sql Server?\nIt is the method of retrieving related data from more than one table using their parent –child keys. That means related tables can be joined using primary-foreign keys to get the meaningful result.\n10) Explain the types of JOINS in Sql Server?\nBelow is the list of JOINS in Sql Server –\n- Inner Join : This is the most common type of join and is similar to AND operation. It combines the results of one or more tables and displays the results when all the filter conditions are met. There are two ways to represent the same\n- Right Outer Join : This join is opposite of left outer join. It retrieves all the records from the table which is on the RHS of ‘RIGHT OUTER JOIN’ clause and only the matching records from LHS.\n- Left Outer Join : This join retrieves all the records from the table which is on the LHS of ‘LEFT OUTER JOIN’ clause and only the matching records from RHS.\n- Full Outer Join : This is combination of both left and outer join. It displays all the matching columns from both the tables, and if it does not find any matching row, it displays NULL.\n11) In which TCP/IP port does Sql Server run?\nBy default it runs on port – 1433 and it can be changed from “Network Utility TCP/IP” properties.', 'How to Prevent SQL Injection\nSQL injection attacks are one of the top threats to the security of websites and web applications. In fact, the SQL injection threat has been number one on OWASP’s Top 10 list since it was publicly disclosed more than 10 years ago.\nSQL injection attacks have caused historic havoc. They have been used to breach the World Health Organization (WTO) database and steal data from internal staff, and also to attack U.S. government agencies, including the U.S. Army and NASA. SQL injection has also been used to steal the personal data of close to 157,000 TalkTalk customers and this is only the tip of the iceberg.\nIn the same way that SQL injection attacks are relatively straightforward and anyone with some SQL knowledge or access to tools could launch them, their prevention and protection strategies are also straightforward.\nIn this post, we’ll go through the process of an SQL injection, the types of SQL injections and why they are exploitable. We’ll also highlight the prevention and protection mechanisms for all types of SQL injections.\nThe SQL Injection: What Is It and How Does It Work?\nAn SQL (Structured Query Language) injection is a type of cyberattack that aims to substitute a legitimate website’s input, such as a username, password, or URL, with an SQL statement that injects code.\nHackers perform an SQL injection, by entering fake SQL statements into a website input form view bits and pieces of information or alter a database table. In other words, hackers use unauthorized SQL code to “piggyback” on an authorized path (an input form) to reach and interfere with a site’s SQL database.\nWhere Is the Vulnerability?\nA site with an SQL injection vulnerability has forms of entry, such as input application forms or URL strings, that are unable to differentiate between a user’s input and the native commands in the site’s code.\nText fields, such as the username or password areas on login pages that allows a user or hacker to interact with the backend SQL database are at risk.\nFor example, when a user inputs a username and password, the information is compared with the website’s database using SQL statements.\nAs shown in the screenshot below, a SELECT statement is the one used to select data from a database and return results to a result table. This statement could be exploited to steal data.\nRefer to the screenshot above. When someone types a username in the text input, the SELECT statement will search in the database’s table users by utilizing this input from the user which according to the example code above, relies on a variable (“ + txtUserId;).\nIf there is no protection or way to prevent a user from entering invalid inputs, the user could enter calculated SQL statements. For example, an unknown visitor could enter a “crafted” input in the username field, such as “105 OR 1=1”.\nWith this crafted input, the SQL statement would change, but still be valid.\nNormally, the variable (+ txtUserId, as stated above) would be replaced with a normal UserId, such as “Diego”.\nHowever, with this crafted input, something else happens. The invalid input “105 OR 1=1” was designed by the hacker to intentionally create a brand new query. For example, OR 1=1 means “always TRUE”. It makes the database return all row values from the Users table.\nAlthough calculated statements like these can be dangerous, there are far sneakier ways hackers can steal, gain control, and even destroy entire databases.\nFor example, by entering something like “or 1=1–” in the password field, a new SQL statement is created that could return all password values for a Users table.\nWhy Certain Types of SQL Injections Are Exploitable\nAlthough SQL injections are generally easy to execute, hackers tend to be really persistent, especially if they want to get something out of an SQL injection.\nSQL injections can be classified based on the server’s response and the channel extraction method. Based on the server’s response, an SQL injection can be either performed by error or blind (inferential), and based on the communication channel, an SQL injection can be either in-band or out-of-band.\nA. In-band Error-based SQL injection\nAn in-band SQL injection occurs when a hacker uses the same communication channel to launch the attack and collect results. In the in-band error-based SQL injection, the hacker attempts to find and exploit visible error results given by the database server, to retrieve information, such as table names and contents. This intention behind this type of SQLi attack is used to learn as much as possible about the structure of the database.\nAn error-based SQL injection is exploitable due to:\n- User-supplied data (input) is not filtered by the web application.\n- Dynamic SQL queries (non-parameterized calls).\nUnsanitized inputs can lead to error-based feedback that provide too many useful clues for the attacker.\nB. In-band UNION-based SQL injection\nAn in-band UNION-based SQL injection is another type of in-band SQL injection in which a hacker uses the same communication channel to attack and gather results. This SQLi is based on the UNION SQL operator, which extends the results provided by the original query. For example, it could combine the results of two (or more) SELECT statements into a single result.\nA UNION-based SQLi is exploitable due to:\n- User-supplied data (input) is not sanitized by the web application.\n- Dynamic SQL queries (non-parameterized calls).\nC. Blind Attacks (Inferential SQL injection)\nAs the name suggests, hackers who perform blind SQL injection attacks have no idea what is on the other end. The application or website is still vulnerable to SQL injection, but the HTTP response does not come with clues about the SQL query or database error. The hacker can launch this type of attack by blindly reconstructing the database structure with the response and behavior of the server. A blind SQL injection can be Boolean or Time-based.\nA blind SQLi is exploitable due to:\n- Lack of parameterized queries or prepared statements.\n- Dynamic SQL queries using their procedures.\nD. Out-of-Band (OOB) SQLi\nThis type of SQLi occurs when an attacker exploits two different channels, one for launching the attack and the other for collecting data. This attack uses an OOB channel, such as DNS or HTTP protocol to exfiltrate data.\nAn OOB SQLi is exploitable due to:\n- Lack of sanitized or validated inputs from web applications.\n- Lack of (or poor) network security measures that allow the listening database server to initiate outbound requests (HTTP or DNS) to the public.\n- The right permissions to run the necessary function that starts the outbound request.\nHow to Prevent an SQL Injection\nAn SQL injection is one of the easiest to perform and most effective forms of cyber attacks. On the other hand, it is also relatively easy to defend against an SQL injection. The best way to prevent an SQL injection attack is to separate data from queries and commands using prepared statements.\nHow to defend against an SQL injection\n- User Prepared Statements\n- Validate User Inputs\n- Enforce Least Privilege\n- Perform Penetration Testing\n- Set Up a a WAF (Web Application Firewall)\nA. Use Prepared Statements with Parameterized Queries\nPrepared statements guarantee that user inputs passed to SQL statements are safe. The idea is that a user’s input cannot directly influence the SQL commands that control and access an SQL database. The database will treat all inputs (including malicious SQL statements) as ordinary inputted data and not as commands.\nIn a prepared statement, the variables in a query are always separated from the rest of the query. In other words, when a developer defines the code for SQL queries (prepared statements) the user input and code are separated.\n- Parameterize stored procedures.\n- Avoid dynamic SQL or at least parameterize them.\n- Prepared statements also help escape to evade user-supplied inputs. Configure user inputs to a specific function to ensure that characters, such as the single quote (‘), are not passed along to an SQL query as instructions.\nB. Validate User Inputs\nAnother way to defend a website’s front line against SQL injections is to validate user inputs. With this method, whatever a user inputs to the web application, there is always a validation (filtering) process allowing/disallowing inputs.\nTo validate user inputs, identify and define the essential SQL statements. Establish an SQL statement whitelist that helps filter out all unvalidated statements and accepts valid inputs.\nAdditionally, user data input can also be defined by context. For example input fields can be filtered for email addresses by allowing only the characters used in the email, such as the @ symbol or phone number inputs, which can be filtered by only allowing numbers.\nC. Enforce Least Privilege\nAs a second line of defense, the level of access and user’s privileges to a database should be limited. It is important to restrict access rights to the specific users, accounts, or computer processes that need to perform a specific activity on the database. For example, if a website or application only needs to use SELECT statements for the database, there should not be any user privileges to use INSERT or DELETE.\nEnforcing the least privileged account strategy will help reduce the level of damage in case a user account gets compromised. If a hacker steals a user’s account information, they will also have limited access to the database.\nD. Perform Penetration Testing\nA great way to prevent an SQL injection is to know the website’s flaws and to perform regular assessments of the website’s defenses by auditing suspicious activities, user privileges, and using pen-testing tools.\nSQL injection hackers and even script kiddies are beginning to use automation for time-consuming and repetitive tasks. Examples are Havij, an automated SQL Injection tool, that helps penetration testers (or bad actors) to find and exploit SQL Injection vulnerabilities. Another is SQLMap, an open-source penetration tool used to automatically scan, detect, and perform SQL injections and take control of databases.\nE. Set Up a WAF\nUse a Web Application Firewall (WAF) to protect web applications with access to databases. A WAF can filter out malicious traffic data by using a set of security rules to monitor and filter suspicious traffic behavior.\nCloudBric’s Cloud-Based Smart Web Application Protection (SWAP) is one of the most comprehensive WAF solutions against SQL injections, and other common threats. SWAP uses a patented logic (including pattern matching, semantics, and heuristic analysis) and a ruleset to automate attack detection and mitigation.\nSQL injections pose a great threat to web apps and sites. Hackers can steal, modify, and even destroy entire databases.\nThe best way to prevent an SQL injection is to parameterize statements. This ensures that all parameters (like inputs) that are passed into SQL statements are treated safely. Another way to defend against these threats is to validate or filter all user inputs.\nTo stay ahead of the game, we also recommend enforcing your user privileges and access rights, using an advanced WAF, and always pen-testing the website’s defenses.']	['<urn:uuid:abaa912c-7fba-43f5-9f56-f4a1abede194>', '<urn:uuid:744f28a5-65f6-4866-99a0-409456138607>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T03:34:55.284799	9	98	3121
31	who funded export town beautification projects	Local businessman Joseph 'Junior' Hall, who died in 2006, bequeathed a $2.7 million trust fund to the borough for beautification projects.	"[""Old Westinghouse Christmas tree finds home in Export\nFor decades, it shined like a beacon of Christmas alongside the Parkway East. With thousands of rhythmically blinking colored lights, the Westinghouse Christmas tree was a staple of the holiday season in the eastern suburbs of Pittsburgh.\n“It's a historic tree,” said Export Councilman John Nagoda, who with his wife, Michelle, found the tree last year in a warehouse full of old Westinghouse equipment they purchased through an online auction.\n“Tens of thousands of people have seen that tree and now, it's come to Export.”\nBorough workers in the Westmoreland County community installed the 35-foot classic tree from the former Westinghouse Electric Corp.'s campus in Churchill last week at the entrance to the future Joseph “Junior” Hall Community Park.\nUntil the mid-1990s, the seven-building complex along the Parkway East known as the George Westinghouse Research & Technology Park was the hub of Westinghouse's research and technology activity.\n“This tree was amazing. Every Christmas since I was a little girl, I'd see it from the road,” said Michelle Nagoda of Export. “You just had to see it.”\nThe old Westinghouse company sold the Churchill research park buildings in 1985, but continued to own the land. Westinghouse acquired CBS in 1997, and later changed its name to CBS and moved to New York.\nThe company in 1999 spun off its nuclear business that became the current Westinghouse Electric Co., headquartered in Cranberry. Westinghouse officials couldn't be reached for comment.\nThe tree actually is a large pole fitted with streams of lights.\nThe fixture took a lot of work to reassemble after years in storage, John Nagoda said, and it was lit for the first time in Export during Friday's snowy Light Up Export Night event.\n“It's been a part of the eastern suburbs for a long time. It was a focal point at the holidays,” said Nagoda, who has been on Export Council since 1975 and buys the contents of storage areas.\nHe declined to say how much he and his wife paid for the Westinghouse equipment, some of which he keeps and some of which he sells.\nAs they were going through the storage space, John Nagoda said, he found the tree and an accompanying menorah. They hope to put up the menorah next year in Export.\nThey said they considered offers to buy the tree, but decided instead to honor local businessman Joseph “Junior” Hall, who died in 2006, by erecting it in Export. Hall was known for his love of Christmas, and bequeathed a $2.7 million trust fund to the borough for beautification projects.\n“The fact that (the tree) adorns the entrance to the park that bears his name is carrying on his tradition in a much larger way,” Nagoda said.\nThe Nagodas, who work with Export's historical committee, said it's fitting that a historic tree now has a permanent home in a historic town and will entertain visitors.\n“The tree never blinks the same way twice,” John Nagoda said.\nExport Mayor Michael Calder said borough workers have been fine-tuning the sequencing of the lights this week.\nCalder said bringing the tree to Export — a first for the tiny borough — would make it a staple of the community's holiday celebrations.\n“It's a very visual reminder the that holiday season is upon us,” he said.\nDaveen Rae Kurutz is a staff writer for Trib Total Media. She can be reached at 412-856-7400, ext. 8627, or firstname.lastname@example.org.\nShow commenting policy\nTribLive commenting policy\nYou are solely responsible for your comments and by using TribLive.com you agree to our Terms of Service.\nWe moderate comments. Our goal is to provide substantive commentary for a general readership. By screening submissions, we provide a space where readers can share intelligent and informed commentary that enhances the quality of our news and information.\nWhile most comments will be posted if they are on-topic and not abusive, moderating decisions are subjective. We will make them as carefully and consistently as we can. Because of the volume of reader comments, we cannot review individual moderation decisions with readers.\nWe value thoughtful comments representing a range of views that make their point quickly and politely. We make an effort to protect discussions from repeated comments either by the same reader or different readers.\nWe follow the same standards for taste as the daily newspaper. A few things we won't tolerate: personal attacks, obscenity, vulgarity, profanity (including expletives and letters followed by dashes), commercial promotion, impersonations, incoherence, proselytizing and SHOUTING. Don't include URLs to Web sites.\nWe do not edit comments. They are either approved or deleted. We reserve the right to edit a comment that is quoted or excerpted in an article. In this case, we may fix spelling and punctuation.\nWe welcome strong opinions and criticism of our work, but we don't want comments to become bogged down with discussions of our policies and we will moderate accordingly.\nWe appreciate it when readers and people quoted in articles or blog posts point out errors of fact or emphasis and will investigate all assertions. But these suggestions should be sent via e-mail. To avoid distracting other readers, we won't publish comments that suggest a correction. Instead, corrections will be made in a blog post or in an article.\n- FBI helping Delmont track down overseas scam artist who ‘catfished’ $220K\n- Planner advises Murrysville to be flexible on drilling\n- Murrysville council to discuss budget next week\n- 2 injured in Route 22 crash in Murrysville\n- Election prompts venue change for FR school board, Export Council meetings\n- Delmont official, husband go all out for Halloween\n- Newlonsburg bra donations to help African women start businesses\n- Chili champions have chances to put best bowls forward in Murrysville, Delmont""]"	['<urn:uuid:4adaa339-fa03-480a-986e-b10be4a3341d>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T03:34:55.284799	6	21	962
32	As someone studying literary award processes, I'm curious about how the National Book Award judges make their selection - what is the process for choosing finalists and winners?	The judges read all submitted books in their category over summer (150-500 titles). As of 2013, they first create a longlist of ten titles announced in mid-September, then narrow it to five Finalists announced in mid-October. The Winner in each category is decided collectively by the panel over lunch on the day of the Awards Ceremony in mid-November. The judges can use any criteria they deem appropriate, as long as they don't conflict with official guidelines.	['How the National Book Awards Work\nWhat Is the National Book Award?\nEstablished in 1950, the National Book Award is an American literary prize administered by the National Book Foundation, a nonprofit organization. A pantheon of such writers as William Faulkner, Marianne Moore, Ralph Ellison, John Cheever, Bernard Malamud, Philip Roth, Robert Lowell, Walker Percy, John Updike, Katherine Anne Porter, Norman Mailer, Lillian Hellman, Elizabeth Bishop, Saul Bellow, Donald Barthelme, Flannery O’Connor, Adrienne Rich, Thomas Pynchon, Isaac Bashevis Singer, Alice Walker, Charles Johnson, E. Annie Proulx, and Colum McCann have all won the Award.\nWho Are the Judges?\nEach year, the Foundation selects a total of twenty Judges, including five in each of the four Award categories: Fiction, Nonfiction, Poetry, and Young People’s Literature. Historically, Judges are published writers who are known to be doing great work in their genre or field, and in some cases, are past NBA Finalists or Winners. As of 2013, judging panels will no longer be limited to writers, but now may also include other experts in the field such as literary critics, librarians, and booksellers. One of the five Judges on each panel is selected as the panel chair. This person acts as the voice of the panel and the liaison to the Foundation. The Foundation staff takes no part in the Judges’ deliberations, except to verify a submission’s eligibility.\nWho Can Submit Books?\nEach April, the Foundation sends the official National Book Awards guidelines and entry forms to the publishers in its master database. Those publishers who do not receive the materials automatically can call or email the Foundation to request a copy. Authors cannot submit their books themselves; they must have their publishers contact us directly. However, the guidelines are always available for informational purposes here: www.nationalbook.org/nbaentry.html.\nIn order to be eligible for the Award, a book must be written by an American citizen and published by an American publisher between December 1 of the previous year and November 30 of the current year. Self-published books are only eligible if the author/publisher publishes the work of other authors in addition to his own. Books published through services such as iUniverse are not eligible for the Award.\nEach publisher must submit a completed entry form to the Foundation by May 15. They must then mail one copy of each entered book to the Foundation, as well as one copy to each of the five Judges in the appropriate category, by July 1. The entry fee is $135 per book.\nHow Are the Finalists Chosen?\nEach panel reads all of the books submitted in their category over the course of the summer. This number typically ranges from 150 titles (Poetry) to upwards of 500 titles (Nonfiction). As of 2013, each panel will now compile a “longlist” of ten titles, to be announced in mid-September. They will then narrow down that list to five Finalists, to be announced in mid-October. They may arrive at these choices using whatever criteria they deem appropriate, as long as they do not conflict with the official Award guidelines.\nThe Finalists Announcement has taken place at various literary sites around the country, from William Faulkner’s front yard in Oxford, Mississippi (2005) to the Flannery O’Connor Childhood Home in Savannah, Georgia (2010). In 2011, the Finalists Announcement was made on Oregon Public Broadcasting’s morning radio program “Think Out Loud,” and in 2012, the announcement was made on TV for the first time, on MSNBC’s “Morning Joe.”\nHow Are the Winners Chosen?\nNo one, not even the Foundation staff, learns who the Winners are until the day of the National Book Awards Ceremony and Benefit Dinner, which takes place in mid-November in New York City. That afternoon, over lunch, each panel collectively decides who the Winner in their category will be. Often, this decision has been made ahead of time, but occasionally the panel works to come to a consensus until the very last minute. The panel chair announces the Winner at the Ceremony that evening.\nWhat Does the Award Entail?\nThe night before the Awards, each Finalist receives a prize of $1,000, a medal, and a citation from the panel at a private Medal Ceremony. Immediately following the Medal Ceremony, all twenty Finalists read from their nominated books at the Finalists Reading. The four Winners in Fiction, Nonfiction, Poetry, and Young People’s Literature are announced the following evening at the National Book Awards Ceremony and Benefit Dinner, where each Winner receives $10,000 and a bronze sculpture.\nOnce an author has been a National Book Award Finalist or Winner, he or she becomes a permanent member of the National Book Foundation family. We do our best to keep in touch with both the authors and publishers, promote the authors’ new books and upcoming readings, and invite them to future National Book Award-related events. Check us out on Facebook, Twitter, and Tumblr to see how we continue to support past Winners and Finalists all year round.']	['<urn:uuid:a101fd47-c708-44c0-ae6d-fdc058da56ac>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T03:34:55.284799	28	76	823
33	Can DNA testing tell what breed a shelter dog is?	Yes, there are three DNA-based breed identification tests available. The Wisdom Panel tests can determine mixed breed ancestry, verify designer dogs with purebred parents, or evaluate genetic patterns against known purebred dogs. Parentage testing with DNA from two registered purebred parents can also prove breed in some cases.	"['Show All Answers Hide All Answers\n- Q. Are oral cheek swabs as good as a blood sample?\n- A. It is the same DNA collected by both means. We have found no difference in using either sample type.\n- Q. How soon after birth can I DNA test my animal?\n- A. Since DNA is the same from birth, you may take the sample as soon as you are comfortable putting the swab into the mouth. We recommend isolating the animal(s) to be sampled for one hour to prevent cross-contamination of DNA.\n- Q. Do I have to use all the swabs?\nA. Please use all the swabs in a kit for one animal. One kit is sufficient DNA for any number of tests. For example, use one kit for coat color as well as for disease testing for the same animal. We do, however, provide separate kits for DNA Storage, profile, parentage, breed identification, and DCM testing.\nDNA Collection Instructions\n- Q. How long will the testing take?\n- A. Most tests offered by VetGen take up to two weeks from receipt of DNA sample if we get a result on the first try. Exceptions are Profiling and Parentage which take a minimum of three to four weeks, DCM tests take approximately 4 weeks for turnaround & the Wisdom Panel Test for Mixed Breed, Purebred and Designer dogs which take a minimum of six to eight weeks.\n- Q. I paid for the DNA testing. Why can\'t I be told the results?\n- A. VetGen confidentiality is formed with those indicated under ""RESULTS GO TO"" on the submission form that comes into the lab with the DNA sample. If the kit was sent or given to someone else, they must include your name, phone, fax or email for our confidentiality to be extended. Even being listed as ""Owner"" of the animal does not extend confidentiality.\n- Q. I don\'t see my breed listed on the Price Sheet under disease tests. Do you do disease testing for my breed?\nA. All VetGen disease testing is breed specific. As soon as a new test becomes available it is included on the price sheet.\nFinding Tests for your Breed\n- Q.We have just adopted a dog from the shelter. Can VetGen do a DNA test to see what breed or breeds it is?\nA. VetGen offers three distinct DNA-based breed idenitification tests. Our new Wisdom Panel tests can determine the ancestry of a Mixed Breed dog, or it can give you the peace of mind that you own a true Designer Dog with purebred parents or even evaluate the genetic patterns of your dog in comparision to the genetic signatures observed in our database of known Purebred dogs.\nVetgen can also perform a parentage test involving DNA collected from two registered purebred parents and the offspring in question to prove breed in some situations.\nHere are the links to the more information about those tests talked about in the answer.\nWisdom Panel Mixed Breed Indentification Test Wisdom Panel Designer Dog Identification Test Wisdom Panel Purebred Identification Test Profiling/Parentage Service']"	['<urn:uuid:7d45b71f-090b-4719-b4c7-a2c3cc5bd261>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T03:34:55.284799	10	48	518
34	dna nanorobots key features precision medicine applications and delivery footprint compared gesicles	DNA nanorobots feature tissue penetration, site-targeting, stimuli responsiveness, and cargo-loading capabilities for precision medicine applications. However, they face challenges with stability and high production costs. In contrast, gesicles provide efficient delivery with no additional footprint, as they don't leave persistent Cas9 expression in target cells, reducing off-target effects while enabling precise control of dose and timing.	"['Perspective | Open Access\nYong Hu, ""Self-Assembly of DNA Molecules: Towards DNA Nanorobots for Biomedical Applications"", Cyborg and Bionic Systems, vol. 2021, Article ID 9807520, 3 pages, 2021. https://doi.org/10.34133/2021/9807520\nSelf-Assembly of DNA Molecules: Towards DNA Nanorobots for Biomedical Applications\nDNA nanotechnology takes DNA molecule out of its biological context to build nanostructures that have entered the realm of robots and thus added a dimension to cyborg and bionic systems. Spurred by spring-like properties of DNA molecule, the assembled nanorobots can be tuned to enable restricted, mechanical motion by deliberate design. DNA nanorobots can be programmed with a combination of several unique features, such as tissue penetration, site-targeting, stimuli responsiveness, and cargo-loading, which makes them ideal candidates as biomedical robots for precision medicine. Even though DNA nanorobots are capable of detecting target molecule and determining cell fate via a variety of DNA-based interactions both in vitro and in vivo, major obstacles remain on the path to real-world applications of DNA nanorobots. Control over nanorobot’s stability, cargo loading and release, analyte binding, and dynamic switching both independently and simultaneously represents the most eminent challenge that biomedical DNA nanorobots currently face. Meanwhile, scaling up DNA nanorobots with low-cost under CMC and GMP standards represents other pertinent challenges regarding the clinical translation. Nevertheless, DNA nanorobots will undoubtedly be a powerful toolbox to improve human health once those remained challenges are addressed by using a scalable and cost-efficient method.\n1. Main Text\nIn nature, DNA molecule is typically used by biological systems to store and transmit genetic information. Over the past decade, DNA nanotechnology takes DNA molecule out of its biological context and use it as fundamental building block to build nanostructures in well-defined yet almost arbitrary sizes and shapes via complementary base-pairing , thereby providing a means to tailor nanostructures’ biological availability and activity. Tremendous development in assembling functional DNA nanostructures  has integrated elementary functions and then elicited various stimuli-responsive mechanisms to resolve daunting tasks in a programmed manner with molecular accuracy. DNA nanostructures have evolved from the initially static state to the increasingly enabled, dynamic state. That is to say, DNA nanostructures have entered the realm of robots and thus added a dimension to cyborg and bionic systems.\nImportantly, as a pliable and robust molecule, DNA mechanically behaves like an entropic spring. Hence, it can be elastically stretched and bent by external forces and then reconstitute itself in proper conditions . DNA in the assembled nanostructure retains most of these spring-like properties . This affects the assembly’s bending and torsional rigidity along with its inherent architecture. Thus, the mechanical properties of the assemblies can be tuned from wire-like flexibility to beam-like stiffness by deliberate design . Furthermore, stiff beams can be connected by short oligonucleotides to constitute a hinge and its analogue like slider, crank-slider, or Bennet linkage to facilitate an angular motion, linear motion, reversible 3D motion cycle and/or other restricted, mechanical motion .\nWith delicate design, DNA nanostructures can outperform conventional nanomaterials made of synthetic polymers for diagnostic and sensing applications through their unique features of stimuli responsiveness. For instance, DNA nanorobots constructed with several fluorescent dye-modified oligonucleotides have been used for spatiotemporal mapping of a wide scope of ions (i.e., H+, Cl-, Na+, K+, and Ag+) based on the conformational transition of i-motif in response to protons or other ions in living cells using Förster resonance energy transfer (FRET) [6–8]. Other than aforementioned targets, ATP, DNA, RNA, and antibody have been detected as well because a variety of DNA-directed specific interactions (i.e., DNA-ATP, DNA-DNA, DNA-RNA, and DNA-protein)  can be employed to trigger the closing or opening of DNA nanorobots with sophisticated shapes like plier , capsule [9, 10] (Figures 1(a) and 1(b)) and others. The conformation transitions of those can be visualized by atomic force microscope (AFM) with the capabilities to determine the existence of targets at molecular resolution.\nFurthermore, DNA nanorobots can act as an ideal candidate for precision therapeutic applications through a combination of several unique features of tissue penetration, site-targeting, and cargo delivery. Meanwhile, DNA nanorobots have been equipped with various logic gates (AND, OR, XOR, NAND, NOT, CNOT, and a half adder, Figures 1(b) and 1(c)) [10, 11], which can be utilized to display different drug outputs and interface with living systems ranging from cultured cells to mammals. As a powerful toolbox for cancer treatment, DNA nanorobots have been simultaneously loaded with therapeutic cargo and fastened along the periphery by targeting aptamer, so as to explicitly accumulate on tumor sites where in response to molecular trigger the cargo was released to inhibit tumor growth . Because DNA nanorobots are conveniently regulated and biologically amenable, we believe that DNA nanorobots will even enable treating specific tumors via several potential ways like cascade drug delivery, spatiotemporally controlled drug release, and combination with immunotherapy. The last case could for example reprogram immune systems using CpG sequence-rich and/or antigen-modified DNA nanorobots, which will indeed have far-reaching outcomes.\nEven though DNA nanorobots have seen immense advancements, their limited stability and behavior in physiologically relevant conditions have raised concerns regarding insufficient circulation time and biodistribution. Consequently, an increasing amount of efforts, such as covalent cross-linking, crossover design, and mineralization, have been invested to increase resistance against disassembly, denaturation, and enzyme digestion . While many of those approaches have indeed resulted in significant improvements in nanorobot’s properties in physiological conditions, in most cases, they may be incompatible with a dynamic switching of the DNA nanorobots so as to eventually interfere with the loading and release of therapeutic cargo, binding of diagnostic biomarkers, and diagnostic and/or therapeutic performance. In this context, control over nanorobot’s stability, cargo loading and release, analyte binding, and dynamic switching both independently and simultaneously represents the most eminent challenge that biomedical DNA nanorobots currently face.\nTypical laboratory synthesis nowadays allows production of nanomole scale of DNA nanorobots for cost of >1000 dollars. Previous successful trail has reported that intravenous administration of DNA nanorobots into a mouse needs about 1 nanomole per dose , and the amount for an adult human would translate to be about as large as 300 nanomoles per dose that is prized at as high as >300,000 dollars per dose. Else, such biologics has to comply with CMC and GMP standards due to the issues of sterilization, purification, and batch-to-batch consistency, thus, further increasing the production cost. In this context, scaling up DNA nanorobots with low-cost under CMC and GMP standards represents other pertinent challenges regarding the clinical translation. Although DNA nanorobots still remain miles away from the real-world applications, we can foresee that they would improve human health once those remained challenges are addressed by combining possible scalable and cost-efficient methods of liquid-phase oligonucleotide synthesis , chip-based staple strand production [15, 16], DNazyme-catalyzed production [17, 18], and biotechnological “mass production” of DNA nanorobots [19, 20].\nThe data used to support the findings of this study are available from the corresponding author upon request.\nConflicts of Interest\nThe authors declare that there are no conflicts of interest regarding the publication of this article.\nThe project was supported by the Fundamental Research Funds for the Central Universities, China (22120210364).\n- N. C. Seeman and H. F. Sleiman, “DNA nanotechnology,” Nature Reviews Materials, vol. 3, article 17068, 2017.\n- M. R. Jones, N. C. Seeman, and C. A. Mirkin, “Programmable materials and the nature of the DNA bond,” Science, vol. 347, no. 6224, p. 1260901, 2015.\n- S. B. Smith, Y. J. Cui, and C. Bustamante, “Overstretching B-DNA: the elastic response of individual double-stranded and single-stranded DNA molecules,” Science, vol. 271, no. 5250, pp. 795–799, 1996.\n- D. J. Kauert, T. Kurth, T. Liedl, and R. Seidel, “Direct mechanical measurements reveal the material properties of three-dimensional DNA origami,” Nano Letters, vol. 11, no. 12, pp. 5558–5563, 2011.\n- A. E. Marras, L. Zhou, H.-J. Su, and C. E. Castro, “Programmable motion of DNA origami mechanisms,” Proceedings of the National Academy of Sciences of the United States of America, vol. 112, no. 3, pp. 713–718, 2015.\n- S. Modi, M. G. Swetha, D. Goswami, G. D. Gupta, S. Mayor, and Y. Krishnan, “A DNA nanomachine that maps spatial and temporal pH changes inside living cells,” Nature Nanotechnology, vol. 4, no. 5, pp. 325–330, 2009.\n- K. Leung, K. Chakraborty, A. Saminathan, and Y. Krishnan, “A DNA nanomachine chemically resolves lysosomes in live cells,” Nature Nanotechnology, vol. 14, no. 2, pp. 176–183, 2019.\n- A. Kuzuya, Y. Sakai, T. Yamazaki, Y. Xu, and M. Komiyama, “Nanomechanical DNA origami \'single-molecule beacons\' directly imaged by atomic force microscopy,” Nature Communications, vol. 2, no. 1, p. 449, 2011.\n- S. Li, Q. Jiang, S. Liu et al., “A DNA nanorobot functions as a cancer therapeutic in response to a molecular trigger _in vivo_,” Nature Biotechnology, vol. 36, no. 3, pp. 258–264, 2018.\n- S. M. Douglas, I. Bachelet, and G. M. Church, “A logic-gated nanorobot for targeted transport of molecular payloads,” Science, vol. 335, no. 6070, pp. 831–834, 2012.\n- Y. Amir, E. Ben-Ishay, D. Levner, S. Ittah, A. Abu-Horowitz, and I. Bachelet, “Universal computing by DNA origami robots in a living animal,” Nature Nanotechnology, vol. 9, no. 5, pp. 353–357, 2014.\n- S. Nummelin, B. Shen, P. Piskunen, Q. Liu, M. A. Kostiainen, and V. Linko, “Robotic DNA nanostructures,” ACS Synthetic Biology, vol. 9, no. 8, pp. 1923–1940, 2020.\n- Z. Wang, L. Song, Q. Liu et al., “A tubular DNA nanodevice as a siRNA/chemo-drug co-delivery vehicle for combined cancer therapy,” Angewandte Chemie International Edition, vol. 60, no. 5, pp. 2594–2598, 2021.\n- G. Creusen, C. O. Akintayo, K. Schumann, and A. Walther, “Scalable one-pot-liquid-phase oligonucleotide synthesis for model network hydrogels,” Journal of the American Chemical Society, vol. 142, no. 39, pp. 16610–16621, 2020.\n- A. N. Marchi, I. Saaem, J. Tian, and T. H. LaBean, “One-pot assembly of a hetero-dimeric DNA origami from chip-derived staples and double-stranded scaffold,” ACS Nano, vol. 7, no. 2, pp. 903–910, 2013.\n- Y. Hu and C. M. Niemeyer, “From DNA nanotechnology to material systems engineering,” Advanced Materials, vol. 31, no. 26, article 1806294, 2019.\n- Y. Jia, L. Chen, J. Liu, W. Li, and H. Gu, “DNA-catalyzed efficient production of single-stranded DNA nanostructures,” Chem, vol. 7, no. 4, pp. 959–981, 2021.\n- Q. Li, Z. Tong, Y. Cao, and H. Gu, “DNAs catalyzing DNA nanoconstruction,” Chem, 2021.\n- F. Praetorius, B. Kick, K. L. Behler, M. N. Honemann, D. Weuster-Botz, and H. Dietz, “Biotechnological mass production of DNA origami,” Nature, vol. 552, no. 7683, pp. 84–87, 2017.\n- J. Liu and H. Gu, “Biotechnological production of ssDNA with DNA-hydrolyzing deoxyribozymes,” STAR protocols, vol. 2, no. 2, article 100531, 2021.\nCopyright © 2021 Yong Hu. Exclusive Licensee Beijing Institute of Technology Press. Distributed under a Creative Commons Attribution License (CC BY 4.0).', 'Gesicles enable CRISPR/Cas9-mediated gene editing with high efficiency and no additional footprint\nThe Guide-it CRISPR/Cas9 Gesicle Production System contains everything you need to easily produce gesicles that will efficiently target your gene of interest.\nOverview of gesicle technology\nThis video discusses how gesicles are made and used and the problems they were designed to overcome.\nWatch Dr. Baz Smith give a webinar on gesicle technology.\nWhile CRISPR/Cas9 is a powerful technique for genome manipulation, two significant challenges remain: obtaining efficient delivery of the Cas9/sgRNA complex to all cell types, and leaving no additional footprint (i.e., persistent and elevated expression of Cas9 in target cells) that could lead to off-target effects. To address these challenges, we have developed a system of cell-derived nanovesicles called gesicles. Gesicles contain active Cas9 protein complexed with an sgRNA specific to a gene of interest. Thus, there is no persistent expression of Cas9, because no coding gene is present. Additionally, they are engineered with glycoproteins on their surface that mediate binding and fusion with the membrane of a wide range of target cells. These features enable gesicles to knock out genes with high efficiency and in a broader range of cell types than plasmid-based delivery methods. Finally, use of this method allows for control of the dose and duration of the Cas9-sgRNA complex in the cell, further reducing the chance of off-target effects.\nMechanism for producing gesicles to deliver a Cas9-sgRNA ribonucleoprotein complex\nGesicles are made by transfecting a 293T-based producer cell line with a vector containing an sgRNA for your target locus and our gesicle packaging mix, which contains everything you need in an optimized format. Loaded gesicles can then be collected from the supernatant and added to target cells for efficient editing with no footprint.\n|293T producer cells that have been transfected with our gesicle packaging mix and an sgRNA expression plasmid begin to form gesicles, stimulated by a nanovesicle-inducing glycoprotein.|\n|The producer cells express active Cas9 endonuclease which forms a ribonucleoprotein (RNP) complex with your expressed sgRNA. Gesicle loading utilizes the iDimerize system, which enables inducible protein-protein interactions: The membrane-bound CherryPicker red fluorescent protein on the surface of the gesicle is tagged with one dimerization domain, and the Cas9 endonuclease contains another dimerization domain. A small heterodimerizer ligand is added to link these two domains together, thereby enriching the Cas9-sgRNA RNP complex into the gesicle.|\n|The red fluorescent protein-labeled gesicles are now loaded with active Cas9 complexed with your target sgRNA. Loaded gesicles pinch off from the producer cells and saturate the surrounding medium. Gesicles can be collected from the supernatant, yielding a concentrated stock of your Cas9-sgRNA gesicles. The gesicles can be used immediately on your target cells, or stored for over one year at –70°C.|\n|Harvested gesicles can be applied to a broad range of target cell types. In the presence of protamine sulfate, gesicles fuse to the membrane of the target cell via cell-surface glycoproteins and release the Cas9-sgRNA RNP complex into the cell. The membranes of cells that have been successfully targeted with gesicles are transiently labeled with red fluorescent protein. Critically, the absence of dimerizer ligand in the target cell culture medium and the presence of a nuclear localization signal on the Cas9 endonuclease ensures that the complex dissociates from the red fluorescent protein and is transported to the nucleus. Once there, efficient targeted gene editing takes place. After editing occurs, the Cas9-sgRNA RNP complex is degraded over time by endogenous cell processes, leaving no remaining footprint that would be capable of producing off-target effects.|\nGesicle production system components\nThe Guide-it CRISPR/Cas9 Gesicle Production System contains everything you need to easily produce gesicles that efficiently target your gene of interest. The kit includes lyophilized Xfect Transfection Reagent and gesicle packaging mixes which contain coding sequences for the nanovesicle-inducing glycoprotein, Cas9 endonuclease, and CherryPicker red fluorescent protein. A pre-linearized vector for cloning your target sgRNA and A/C Heterodimerizer for loading gesicles are also provided. Gesicle production can be performed in any 293T-based cell line, but we recommend the Gesicle Producer 293T Cell Line for optimal production results. Older or non-validated cell lines may produce gesicles less efficiently, thereby decreasing the gesicle concentration and subsequent editing efficiency.\nGesicle production system workflow\nThe Guide-it CRISPR/Cas9 Gesicle Production System workflow is simple and straightforward. Your sgRNA for a gene of interest is cloned into the provided linearized expression plasmid, pGuide-it-sgRNA1. Next, this cloned plasmid is mixed with dH2O and the Guide-It CRISPR/Cas9 Gesicle Packaging Mixes 1 and 2. Following a 10-minute incubation, the complete nanoparticle mix is applied to 293T-based producer cells in the presence of the provided A/C Heterodimerizer ligand. After 48–72 hours, gesicles containing active Cas9 protein complexed with your sgRNA can be collected and concentrated via centrifugation. Gesicles can be used immediately, or stored at –70°C for more than one year.\nThe Guide-it CRISPR/Cas9 Gesicle Production System is a complete kit that enables highly efficient gene editing without any additional footprint. This kit contains all of the reagents necessary to prepare custom gesicles to deliver Cas9 and a user-defined, gene-specific guide sequence. Gesicle-based delivery of Cas9 protein and sgRNA using this system results in successful genome editing in a broader range of cell types compared to delivery by plasmid transfection. Critically, gesicles do not result in persistence of the Cas9 endonuclease, which in turn reduces the chances of off-target effects and provides precise control of the dose and timing of gene editing.']"	['<urn:uuid:609644cb-ef2c-455a-9646-ee87e37f3cc7>', '<urn:uuid:37528d1d-fce2-4cf6-8d80-4bdfcea0d323>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T03:34:55.284799	12	56	2688
35	refrigeration system types maintenance requirements	There are three main types of refrigeration systems: evaporative cooling, mechanical-compression, and absorption systems. For maintaining these systems, regular cleaning of coils and condensers is essential, along with proper defrosting when ice buildup exceeds 1/4 inch. The system's efficiency depends on proper temperature control, adequate ventilation, and routine inspection of components like the power cord and thermostat.	['The refrigeration cycle basically involves the movement of refrigerant from one place to the next and in different forms with the ultimate goal of pulling down temperatures whether in a cabinet, counter or even cold room format.\nWhat’s the basic refrigeration cycle?\nOne very important part of all HVAC systems is the basic refrigeration cycle. It contains four major components: the compressor, condenser, expansion device, and evaporator. The system pipes refrigerant through these four components in a loop, giving the cycle its name and keeping your home cool!Feb 14, 2020.\nWhat are the 4 cycles of refrigeration?\nThe 4 Main Refrigeration Cycle Components The compressor. The condenser. The expansion device. The evaporator.\nWhy is the refrigeration cycle called a cycle?\nIt has the maximum efficiency for a given temperature limit. Since it is a reversible cycle, all four processes can be reversed. This will reverse the direction of heat and work interactions, therefore producing a refrigeration cycle.\nWhat are the basic parts of a refrigerator?\nThe main working parts of a refrigerator include: a compressor, a condenser, an evaporator, an expansion valve, and refrigerant.\nWhat are the 4 basic component of refrigeration system?\nThe refrigeration cycle contains four major components: the compressor, condenser, expansion device, and evaporator.\nWhat is the basic unit of mechanical refrigeration?\nThe diagram below illustrates the four basic components of every refrigeration system. These include the Evaporator, the Compressor, the Condenser, and a Metering Device. A refrigerant is then circulated through each component for the sole purpose of removing heat.\nHow many types of refrigeration cycle are there?\nWhat Are the Different Types of Refrigeration Systems? While the four types of refrigeration systems have many similarities, they have just as many differences. To help you get ahead of the curve, we’ve broken down each one in some summaries below.\nHow does a refrigeration cycle work?\nThe refrigeration cycle starts and ends with the compressor. The refrigerant flows into the Compressor where it is compressed and pressurised. At this point, the refrigerant is a hot gas. The refrigerant is then pushed to the Condenser which turns the vapour into liquid and absorbs some of the heat.\nWhat is SI unit of refrigeration?\nExplanation: KJ/s or KW is the S.I. unit of refrigeration, which is converted to Tonnes as commercial unit.\nWhat is the principle of refrigerator?\nThe main principle behind the refrigerator’s working is that a gas or a liquid changes its temperature when forced through a capillary tube or an expansion valve, separately kept in an insulated system where no external heat transfer occurs.\nWhat are the main principles that cause the basic refrigeration cycle to work?\nThe absorption of the amount of heat necessary for the change of state from a liquid to a vapor by evaporation, and the release of that amount of heat necessary for the change of state from a vapor back to the liquid by condensation are the main principles of the refrigeration process, or cycle.\nWhat are the basic components of a refrigerator and their functions?\nInternal Parts of a Refrigerator and its Functions Compressor. A compressor is the main part of a refrigerator. Refrigerant. Refrigerant is a cooling liquid that spreads inside the refrigerator and carries out the cooling part. Condenser. Evaporator. Thermostat. Defrost System. Diffuser. Freezer.\nWhat are the types of refrigerator?\nWhat are the Different Types of Refrigerators? Top Freezer Refrigerator. Side-by-Side Refrigerator. Bottom Freezer Refrigerator. French Door Refrigerator. Counter-Depth Refrigerator. Mini Fridge.\nWhat is mechanical refrigeration cycle?\nMechanical refrigeration, often referred to simply as refrigeration, is a process by which heat is removed from a location using a man-made heat-exchange system. It typically involves cyclic heat absorption using a fluid refrigerant and a compressor system.\nWhat refrigerant is water?\nWater (R718) as a refrigerant is one of the oldest fluids being used for refrigeration applications down to about the freezing point.\nWhat is COP efficiency?\nThe Co-efficient of performance (COP) is an expression of the efficiency of a heat pump. A given heat pump used for air cooling has a COP = 2. This means that 2 kW of cooling power is achieved for each kW of power consumed by the pump’s compressor. COP is indicated without units.\nWhat are the 3 types of refrigeration?\nHere are the different types of refrigeration systems: Evaporative Cooling. Evaporative cooling units are also referred to as swamp coolers. Mechanical-Compression Refrigeration Systems. Mechanical compression is used in commercial and industrial refrigeration, as well as air conditioning. Absorption. Thermoelectric.\nWhat is classification of refrigeration?\nBased on the working principle, refrigeration systems can be classified as vapor compression systems, vapor absorption systems, gas cycle systems, etc.\nWhere is refrigeration cycle used?\nThe vapor-compression refrigeration is the most widely used cycle for refrigerators, air- conditioners, and heat pumps.\nWhy compressor is used in refrigeration cycle?\nThe compressor is used in commercial refrigeration by letting the refrigerant run the circuit. The compressor plays a role in the system like a heart in a living creature’s body. The power comes from the electric motor fixed in the system. This process eases the pressure on the refrigerant’s system.\nWhat is a kW of cooling?\n‘Cooling capacity’ is the measure of an air conditioning system’s ability to remove heat from a room, thus making the room ‘cooler’. Generally the unit used for this measurement is specified in Kilowatts (kW).\nWhat is refrigerant capacity?\nCooling capacity is the measure of a cooling system’s ability to remove heat. It is equivalent to the heat supplied to the evaporator/boiler part of the refrigeration cycle and may be called the “rate of refrigeration” or “refrigeration capacity”.\nWhat is kW in refrigeration?\nOne kilowatt is equal to 0.284345 refrigeration ton: 1 kW = 0.28434517 RT. So the power P in kilowatts (kW) is equal to the power P in refrigeration tons (RT) times 3.5168525: P(kW) = P(RT) × 3.5168525.\nWhich gas is used in fridge?\nModern refrigerators usually use a refrigerant called HFC-134a (1,1,1,2-Tetrafluoroethane), which does not deplete the ozone layer, unlike Freon. A R-134a is now becoming very uncommon in Europe. Newer refrigerants are being used instead.', 'Mini fridges have become an indispensable appliance in many homes, dorm rooms, offices, and even recreational vehicles. While these compact refrigerators provide convenience and versatility, they can also be energy hogs if not used and maintained wisely.\nIn this article, we will explore various strategies and tips to make your mini fridge more energy-efficient, helping you save money on electricity bills and reduce your environmental impact.\nOptimal Temperature Settings\nThe first step towards energy efficiency in your mini fridge is ensuring it operates at the right temperature. Set your mini fridge to the recommended temperature range, typically between 37°F (2.8°C) and 40°F (4.4°C) for the refrigerator compartment, and 0°F (-17.8°C) for the freezer. Adjusting the temperature to these optimal levels prevents the compressor from overworking, thereby reducing energy consumption.\nWhere you place your mini fridge can significantly impact its energy efficiency. Avoid situating it in direct sunlight or near heat-producing appliances like stoves or ovens. Ensure proper ventilation around the fridge, allowing heat to dissipate effectively. Additionally, maintain a reasonable gap between the fridge and the wall to promote airflow.\nRegular Cleaning and Defrosting\nAccumulated dust and frost can impede your mini fridge’s efficiency. Regularly clean the coils and condenser at the back of the fridge to ensure proper heat exchange. Moreover, defrost the freezer compartment when the ice buildup exceeds 1/4 inch (0.6 cm). A frost-free freezer consumes more energy, so manual defrosting can significantly contribute to energy savings.\nSmart Loading and Organization\nEfficiently organizing the contents of your mini fridge can impact its overall performance. Allow sufficient space between items for proper air circulation. Avoid overloading the fridge, as overcrowded shelves can obstruct airflow, causing the appliance to work harder. Group similar items together to minimize the time the door is open, reducing the strain on the compressor.\nInvest in Energy-Efficient Appliances\nIf you are in the market for a new mini fridge, consider purchasing one with an Energy Star label. Energy Star certified appliances meet strict energy efficiency guidelines set by the Environmental Protection Agency (EPA). These fridges typically use less electricity and can significantly reduce your energy bills over time.\nUse LED Lighting\nIf your mini fridge comes with internal lighting, consider replacing traditional bulbs with energy-efficient LED lights. LED lights consume less energy and generate less heat, contributing to the overall efficiency of your appliance.\nSeal Gaskets and Door Maintenance\nA proper seal is crucial for maintaining the efficiency of your mini fridge. Check the gaskets around the door regularly for any signs of wear or damage. If you notice any issues, replace the gaskets promptly to ensure a tight seal. A well-sealed door prevents cold air from escaping and warm air from entering, reducing the workload on the compressor.\nConsider External Temperature Control\nIn extreme temperatures, the external environment can affect your mini fridge’s performance. If possible, place the fridge in a temperature-controlled room. In extremely hot conditions, you can use a fan or provide additional ventilation to help dissipate heat more effectively.\nUtilize Power-Saving Features\nMany modern mini fridges come equipped with power-saving features. Check if your appliance has a power-saver switch or an eco-mode. As per BuyKitchenFinds, these features optimize the fridge’s performance by adjusting the cooling cycles based on the internal temperature, reducing energy consumption without compromising food safety.\nRegular Maintenance Checks\nPerform routine maintenance checks on your mini fridge to identify any potential issues before they escalate. Inspect the power cord for damage, ensure the thermostat is functioning correctly, and check for any unusual noises. Timely maintenance can prevent energy wastage and prolong the lifespan of your appliance.\nMaking your mini fridge more energy-efficient involves a combination of proper usage, regular maintenance, and strategic choices. By implementing the tips outlined in this article, you can not only save on energy bills but also contribute to a more sustainable and eco-friendly lifestyle.\nFrom temperature optimization to investing in energy-efficient appliances, these steps empower you to maximize the efficiency of your mini fridge while minimizing its environmental impact.']	['<urn:uuid:46dc6589-f8d5-4476-a4a9-d87734a4306d>', '<urn:uuid:a4d7894a-8f84-4102-acce-cd2eb6bba5bc>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T03:34:55.284799	5	57	1682
36	I'm learning about American history and wonder how many federal employees had to go through background checks during the Red Scare. Can anyone tell me the exact number?	More than 2,000,000 men and women were subject to loyalty investigations regardless of their status in Government offices. These investigations were mandatory for anyone applying for a federal job.	['McCarthyism and its Effects on America\nMcCarthyism not only destroyed the lives and careers of many Americans but also the innocent image of the country. Senator Joe McCarthy from Wisconsin was the same as any man. But when he cried Communism the world seemed to listen.\nFollowing the Cold War between Russia and the United States there came many hardships, such as unemployment and high inflation. These hardships produced a restless society. The society then looked for something or someone to blame (Fried, 39). They found someone to blame. Communists. Throughout the country there was a witch hunt known as the Red Scare. A basic idea was formed: Communism was evil. Anyone who participated in such evil was considered illegitimate and were to be excluded from such things as sharing ideas, and jobs (Reeves, 136). This fear of Communism or anti-Communism as it was called could be described as a type of “virus.” When all was calm in America the virus would fade, but the moment a crisis struck, the virus came back stronger than ever (Feuerlicht, 35). Communism was a threat not only for countries overseas but a threat for America and its people. It was a threat on the American way of life, a bruise on the phrase “the right to life, liberty, and the pursuit of happiness.” (Feuerlicht, 45) And McCarthy helped spread this fear.\nMcCarthy and his ways challenged the Bill of Rights. “When free speech or due process are denied to any individual everyone’s rights are jeopardized. Today’s oppressors may become tomorrow’s accursed group.” (Feuerlicht, 154) And nothing is guaranteed more than the destruction of America when the freedoms promised by the Bill of Rights are denied (Feuerlicht, 154). McCarthy installed a fear in the people. But people feared tremendously the loss of their jobs. They feared that their political afflictions would reflect on their job status (Reeves, 99). By trying to keep America from becoming a Communist nation, McCarthy and his followers turned the country into an anti – Communist nation. A country that thrived on freedom and was kept in tact by laws would become a country that was moved by fear alone. Laws would hold no meaning and innocent people would be accused (Feuerlicht, 154).\nCommunists were everywhere. But they were hard to identify because Communism was a philosophy. There was no way a person could provide supporting evidence that someone believed in something (Feuerlicht, 154). According to McCarthy there were over 200 Communists in the State Department (Feuerlicht, 55). And some 57 cases of people who fall into the category of card – carrying Communists, loyalist of the Communist Party, or risks to the nation (Divine, 265). Despite the issue of Communists in America there was a bigger issue, Communist spies. Though the truth of the matter is that all countries spy and are spied on, even America. The shock felt by the nation was not lessened (Feuerlicht, 54). Communist spies were the worst kind because they were not the type that stole plans for new weapons they worked from within. They were the people helping us build our policy (Divine, 265). These Communist spies were also hard to find because very few made it easy and confessed to being a spy. One such a person was a German named Klaus Fuchs. He confessed to spying for the Russians while he worked on the development of the atom bomb (Fried, 120).\nMany Americans lives and careers were lost due to McCarthy and his accusations. Hollywood’s leaders resisted allowing politicians to regulate their hiring practices, but following the HUAC hearings the “blacklists” began in Hollywood. No one who was known to be a communist would be employed (Fried, 77-78). Producers started questioning their employees on many topics including politics and affiliations. In the case that the employee’s answers did not fit with the Waldorf Statement which was that no known Communist would be hired, the employee was fired, or let go (Fried, 78). With one exception anyone who confessed, and gave the names of other “Communists” were spared (Garraty, 531). After the blacklists, and the scare of Communism films started containing only anti-Communist themes. Somewhere around forty of these anti-Communism films were made. None of them being very popular or profitable (Fried, 78).\nThe blacklists continued in Hollywood while in the Government offices more than 2,000,000 men and women were subject to loyalty investigations no matter what their status was. The investigations were also required to be done on any person who applied for a federal job (Feuerlicht, 47). Lists of subversive organizations and groups were compiled. Its members weren’t even given a chance to defend themselves. If they belonged to one of the groups listed, they were guilty of “Sympathetic association” and were fired (Feuerlicht, 47). People even began to be punished for things they hadn’t even done. They were punished for what they might do (Feuerlicht, 47-48). Soon almost everyone found their loyalty being questioned. Anyone who liked Russian music, had ever read or owned a book on Communism, especially those who stood up for equal rights of African Americans, and those who stood for civil liberties for Communists (Feuerlicht, 48). Even having a political opinion that was not popular gained you the treatment of a criminal (Garraty, 531).\nNo Part of Society was left untouched. Companies like General Electric, General Motors, CBS, the New York Times, New York City Board of Education, and the United Auto Workers followed Hollywood’s example and fired employees for being Communists. In fact very few companies didn’t fire people (Reeves, 99). Even the schools went along with the dismissal procedures outlined by McCarthy, HUAC, and the FBI. Anyone who did not fit their standards was fired (Reeves, 99). Quite a few people ended up falling victim of these firings. And an estimated 20 percent of these fired people came from the educational system (Reeves, 99).\nMcCarthy was a fraud from the beginning. He never had anything to go on. In fact “the shakier his evidence got the bolder he became.” (Feuerlicht, 57). Although McCarthy didn’t have a shred of evidence it didn’t stop him from making reckless accusations (Sawtelle,1). McCarthy didn’t even know what he said half the time in fact even he didn’t know exactly what he said in the speech at Wheeling but because accusations were beginning to take off, McCarthy could think of no reason not to follow and take off with them (Feuerlicht, 56). And as the public attention grew McCarthy made more speeches condemning Communists ( Garraty, 531). McCarthy never showed any evidence of his accusations. This was in part because he never had any. There was no list of names and the numbers he came up with were the result of good math skills (Feuerlicht, 55). And if anyone ever tried to catch him in a lie he would accuse them of being a “Communist sympathizer.” (Sawtelle, 1) Many things contributed to the fall of McCarthyism but probably the most significant was the condemnation of McCarthy by the U.S. senate in 1954. This condemnation ultimately led to the end of McCarthy’s power over the people and destroyed McCarthyism (Sawtelle, 2).\nEven after the fall of McCarthy people and countries still spy on each other. Only now the spying has passed the level it was at during the McCarthy years (Feuerlicht, 152). The spying reached its peak when in 1967 it was ordered by president Lyndon Johnson’s administration officials for the Army to spy on civilians who were involved in the antiwar, anti poverty, and civil rights movements (Feuerlicht, 152). The government continued its “snooping” by putting wire taps on telephones with no court approval. They strongly opposed anyone who tried to put a stop to their “snooping” (Feuerlicht, 152).\nEven after McCarthy was gone, the fear lingered. People feared a comeback. They were afraid, afraid that behind each corner could be McCarthy (Feuerlicht, 139). Although McCarthy’s attack on freedom was defeated the possibility of future attacks could not be ignored (Sawtelle, 2). One of McCarthy’s victims was quoted saying that “to write freedom into a law is not enough. Unless freedom is practiced, it withers. It must be affirmed and reaffirmed in every generation.” But in McCarthy’s case it was not (Sawtelle,2). Even the consequences of his hate teachings were felt long after McCarthy’s time. They were still fresh in the mind of Lieutenant William Calley when he was found guilty of Murder he was quoted saying that the Army taught him only that the enemy was Communism. “They didn’t give it a race, they didn’t give it a sex, they didn’t give it an age,” sobbing. “They never let me believe it was just a philosophy in a man’s head.” (Feuerlicht, 153). ” But Communism was a philosophy, and like all philosophies it needs to be examined and understood. Instead of declaring a holy war against the concept of Communism, America might have defended freedom more successfully and more honorably if she had declared war on the oppression and injustice that make men Communists.”\nCommunism was indeed a philosophy but it was a philosophy that controlled the American people for years. It left its menacing mark on a society that was otherwise good. America may never fully recover from Senator Joe McCarthy and his McCarthyism\nMcCarthyism and its Effects on America']	['<urn:uuid:cd63e612-32e4-46e9-9c57-9964afbb402c>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T03:34:55.284799	28	29	1554
37	antibacterial properties selenium nanoparticles vertical farming applications minimum inhibition concentration space saving methods	Selenium nanoparticles show antibacterial properties with a minimum inhibition concentration of 5%, above which the antibacterial activity increases. These nanoparticles are effective against both Gram-negative and Gram-positive bacteria, with concentrations under 25 ppm effectively inhibiting bacterial growth. When applied to vertical farming systems, which use vertically stacked layers to conserve space and minimize water usage, selenium nanoparticles could help create sterile growing environments. Vertical farming already reduces water consumption by 95% compared to traditional methods, and integrating selenium nanoparticle treatments could further enhance its efficiency by preventing bacterial contamination.	"['Evaluation of the Anti-Bacterial Effect of Selenium Nanoparticles in Peri-Implantitis Patients\nAim: To determine the concentration of selenium nanoparticles that inhibit the growth of anaerobic bacteria with signs and symptoms of implant failure increasing around implants.\nMaterial and methods: It included 10 partly edentulous subjects (five females, five males) aged 30-73 years with one or more periimplantitis implants. Peri-implantitis was defined as: I the existence of seeping and additional decay on testing and (i) Radiographic images showed negligible bone misfortune >1.8 mm after 1 year of ability."" The integration models were: incompletely edentulous patients with one implant determined to have peri-implantitis in any case; (ii) No anti-microbial therapy for half a year prior to clinical evaluation.\' A total of fifteen implants were diagnosed with peri-implantitis following this description. Subgingival bacterial specimens were collected from infected implants of each person using sterile paper points.\nResults: Revealed that selenium nanoparticles\' minimum inhibition concentration for Total anaerobic bacteria was (5 percent), this concentration showed growth on plain BHI-A media after re-culturing\nConclusion: The antibacterial activity increased from 5 percent and above with the rise in the concentration of selenium nanoparticles, and selenium nanoparticles had no impact at 0.5 percent, 1 percent, 2 percent concentration.\nMinimum inhibition concentration, Selenium, Nanoparticles, Anaerobic bacteria\nSelenium, a significant trace ingredient, is essential for good health and should be consumed on a daily basis. The ability of covalently bound selenium to catalyse the formation of superoxide radicals (O2-), which can prevent bacterial attachment to solid surfaces, has been demonstrated . In humans, selenium is needed for the synthesis of 25 selenoproteins (such as glutathione peroxidase). Cancers of the lungs, liver, colorectal, kidney, oesophageal, gastric cardia, thyroid, and bladder are also covered by it .\nNew methods to fight bacterial drug resistance and biofilm formation have been established in the last few decades.\nThe two main categories of these strategies are controlled antibiotic delivery and the development of non-drug antimicrobial materials. Techniques for improving drug penetration into bacterial biofilms are also being investigated.\nSelenium nanoparticles (Se NPs)–a trace, essential metalloid ingredient–have recently emerged as a promising antimicrobial material in both suspension and immobilized types. Importantly, these particles have been shown to be extremely low in mammalian cell toxicity, making them a potential antimicrobial agent .\nThe bacterial relation is crucial in determining the performance and outcome of a Ti-based implant system. Surface alteration of titanium appears to be an effective way of improving the advantage of clinical treatment by coating or adding antibacterial properties of metals or amalgams to reduce the amount of microscopic organisms and microbial grasp .\nIn vitro and in vivo experiments were used to investigate microbial colonization and antibacterial movement in metallic and ceramic embedded materials. Titanium does not have antibacterial properties, but titanium inserts can cause plaque to form. Since these are the sections that are brushed as a method of plaque control, the adapted surfaces can withstand wear. Dry cycle surface modifications have been used in the medical and dental fields to provide excellent wear resistance as well as to create dainty and glue fine pottery .\nTo determine the concentration of selenium nanoparticles that inhibit the growth of anaerobic bacteria with signs and symptoms of implant failure increasing around implants.\nPreparation of Culture Media\nBrain heart infusion agar (BHI.A)\nBy the guidance of Oxoid Laboratories Company, the readiness of this media was suspended by 47 gm of powder in 1000 ml of refined water at that point blended well using attractive stirrer to ensure the entire amount of the powder disintegrated. The medium was sanitized by the Headings of the Creator. Autoclave at 121°C for fifteen minutes after it has been poured into sanitized petri dishes can be used for sanitized media, and then set aside to cool until used .\nBrain heart infusion – blood agar (BHIB.A)\nThis media arrangement was guided by the Oxoid Company. This medium allowed cooling at 45 °C after cleansing and then added 5-7 percent ml of blood and the medium was poured into sterilized petri dishes to cool at room temperature, set and then placed in the refrigerator until used .\nPrepared of Brain Heart Infusion Broth BHI.B\nAccording to processing, 34.5 gm of it was used to prepare the media, dispersed in one thousand ml of distilled water and locked with caped bottles. Autoclave cleaning was done at a pressure of 15 Ibs, 121 °C for 15 minutes .\nPrepare of Muller Hinton agar\nBy following the manufacturer\'s instructions, 35 agar powder was used in 1 liter of water refining until the complete agar powder was disintegrated to sanitize it by autoclaving it .\nMethod of sterilization\nMedia, water, and phosphate buffer solution (PBS) were used in sterilization by autoclave at 121°C and pressing factor of 15 pounds/inch2 for 15 minutes, but mouth reflection, kidney dishes, and all spotless glasses were handled for 60 minutes by dry air broiler at 180 Co. Dettol\'s clean arrangement sanitized the chairs and floor of the laboratory .\nSample and culture\nIn 10 patients at the dental school, anaerobic microbes were extracted from the mouth with signs of embedded deceit and bone loss. A swab and sterile paper points were collected from the mouth by anaerobic microorganisms .\nThis method involves delicate scouring of pockets about 11 mm with a sterile q-tip, and then immunizing an essential separation medium along these lines, such as Sabouraud dextrose agar (SDA) . Swabs were distilled on Sabouraud dextrose agar at 37°C for 24-48 hours and forcibly brooded, then stored at 4°C for additional analysis .\nCulture and Analysis of Microbes\nA study of anaerobic bacteria mixed in a vortex for 2-3 minutes in glass universal tubes containing 5 ml of Phosphate buffer solution Bacteria were absorbed and traced by the sterile loop, then incubated in blood agar for 96 hours. At 37°C, plan tubes containing 10 millilitres of BHI broth were also inoculated and incubated at 37°C for 48 hours. All plates and tubes were used gas pack to incubate it in anaerobic condition.\nMaintenance of bacterial isolate\nStates were collected from the bacterial agar media from bacterial confines and transferred to 10 ml of sterile BHI stock and hatched at 37°C in anaerobic for 24 hours. These supplies were kept cooler once used, and twice a month to month this technique was rehashed .\nActivation of isolated anaerobic microorganisms\nBacterial inoculums were performed by extending unadulterated disconnects to ten millilitres of sterile BHI stock that were vigorously brooded for 24 hours at 37°C and anaerobically for anaerobic microscopic species .\nDetermination of viable count\nFair actual search using sequential weakening with PBS, CFU/ml was completed and 0.1 ml of 10-3-10-5 was vaccinated with BHI. A brooded anaerobically at 37°C for 24 hrs. .\nGram\'s stain was put on the distraction components of blood agar in the sterilization state. A little inoculum was taken from a disconnected area, emulsified on a glass slide in a drop of traditional saline to frame a suspension, spread, dried and heat-fixed. Gram\'s staining was done for 1 moment starting with gem violet and then washed with water, stained with iodine and, dried, extracted colors for half an hour with ethanol oil. The counter was stained for an extra moment with safranin, cleaned and dried. The slides were examined under a light magnifying lens with a 100X amplification for staining properties, cell morphology, and course of action.\nCalculate the activity of nanoparticles selenium against anaerobic bacteria\nIn this research, the antimicrobial effect of selenium NPs on total anaerobic bacteria using the agar diffusion method was described using ten isolated numbers.\nAround twenty-five millilitres of Muller Hinton agar media were placed in a separate partition of the sterile Petri dishes and waited until set. Brooded for 24hrs at 37°C. to ensure that the media are sterilized. Approximately 0.1 ml of all-out anaerobic (dilution 10-1) cell forming unit/ml confines were triggered and spread on MH agar plates and left at room temperature for twenty minutes, at that point equal size and depth wells made with clean hardened steel Cork drill were set up in the agar in the MH agar 6 mm in diameter. In each plate, three holes. In different fixations, 0.5,1, ,3,5,7 percent, each very much was loaded up with selenium nanoparticles Plates were left at room temperature f or 10 minutes and subsequently brooded at 37 °C in an anaerobic container with a 24-hour gas pack. There were restriction zones around the width of each hole. Under aseptic conditions, the breadth of restraint around the wells containing the test materials was measured and reported after hatching. The area of the inhibition zone can be determined by the Vernia calipers.\nDetermination of minimum inhibition concentration (MIC) for anaerobic bacteria\nAfter pouring it into sufficient Petri dishes, all selected concentrations were dispersed separately with BHI-A to obtain 25 millilitres of agar to become strong, then brooded with 0.1 millilitre of separately activated anaerobic bacteria. Incubated for one day at 37°C including control plates (negative control containing BHI-A with microbial inoculum without the addition of selenium nanoparticles and BHI-A containing plates and different concentrations of selenium nanoparticles without microbial inoculum) Examination of all prepared Petri dishes in order to notice bacterial growth. The minimum inhibition concentration should be considered to be the lowest concentration destroyed by microorganisms.\nResults and Discussions\nAntibacterial activity of selenium nanoparticles on microorganisms (agar well diffusion method). Results revealed that Minimum Inhibition Concentration of selenium nanoparticles for Total anaerobic bacteria was (5%), this concentration showed growth after re-culturing on plain BHI–A media (there was no effect of selenium nanoparticles at concentration 0.5%,1% ,2% and 3% ) number of isolates within the MIC and the concentrations of the selenium nanoparticles as shown in Table 1.\nMean of MIC for 10 samples of each 5, 7, and 10 % of selenium as shown in Table 2.\nBoth samples were diluted in standard saline at a 1:10 ratio (0.9 percent NaCl). In normal saline, serial dilutions (10-1 to 10-5) were made, and 100l were plated on blood agar. Using a gas bag, all plates were incubated at 37 C0 for 72 hours. The colonies were counted and registered as colony-forming units/ml (CFU/ml) at various dilutions. The results were calculated using 105 diluent, as shown in the table .\nThe antibacterial property of selenium was tested using the agar dilution process, which was obtained from the Biology Department\'s culture collection at Baghdad University. Dextrose Sabouraud was collected. Blood infusion broth (BHI) incorporation with agar that recommended the form of microorganisms to be examined, which was anaerobic bacteria, was done at concentrations of selenium ranging from 1% to 10%. To achieve homogeneity, the mixture was mixed in an ultrasonic bath. The investigated microbial strain was cultured overnight in the prepared assay media and then diluted before use. The number of cell forming units (CFU) was determined by dropping 20 lL drops of each diluted microbial culture on the surface of the respective assay medium (in triplicate), which was restricted to anaerobic bacteria and incubated at 37 C for one day.\nThe nanostructures were active against Gram-negative and Gram-positive bacteria at concentrations ranging from 0.5 to 25 ppm. As a result, SeNPs effectively inhibited bacteria at concentrations of less than 25 ppm. When SeNPs were used to inhibit the proliferation of both S. epidermidis and S. aureus when SeNPs were used as antimicrobial agents, MIC values of about 100 ppm were found when cultured with aerobic bacteria, while MIC values of 125 ppm were found when SeNPs were used to inhibit the proliferation of both S. epidermidis and S. aureus when SeNPs were used as antimicrobial agents. This may be due to the Se NPs\' naked surface coming into close contact with the bacteria\'s surface., The treatment with the nanostructures caused changes in both bacterial strains, including destruction of the outer cell membrane. Furthermore, after treatment with SeNPs, cell analysis is easily visible. As a result, visible cell damage was observed, like bacterial deformation and collapse, as well as many cracks in the cell membrane. However, other mechanisms, such as direct cell damage caused by nanostructure morphology, may also be hypothesized .\nCoating external surgical equipment or surfaces that need to be sterilized with medium and higher concentrations of SeNPs could be beneficial [17-19]; the more Al particles in the design, the higher the basic intensity of the oxygen atoms that cause bacterial activity damage. The MIC was found to be the lowest at 5 percent selenium.\nThe action of NP selenium is essential in the inhibition of anaerobic bacteria. Antibacterial activity increased as selenium concentration increased from 5% to 10%, and selenium nanoparticles had no impact at concentrations of 0.5 percent, 0.8%, 1%, 2% and 3%, indicating that the MIC of selenium nanoparticles that obtained antibacterial properties was 5% of selenium in concentration.\nConflict of Interest\nThe authors declare that there is no conflict of interest.\nAll authors listed have made a substantial, direct and intellectual contribution to the work, and approved it for publication.\nAll datasets generated or analyzed during this study are included in the manuscript and/or the Supplementary Files.\nThis study was carried out in accordance with the recommendations for human use. The protocol will be submitted to ethics committee and Ethical approval has been obtained and attached.\n- Wang Q, Webster TJ. Nanostructured selenium for preventing biofilm formation on polycarbonate medical devices. J Biomed Mater Res 2012; 100:3205-10.\n- Rayman MP. Selenium and human health. Lancet 2012; 379:1256-68.\n- Smith AW. Biofilms and antibiotic therapy: Is there a role for combating bacterial resistance by the use of novel drug delivery systems?. Adv Drug Deliv Rev 2005; 57:1539-50.\n- Orapiriyakul W, Young PS, Damiati L et al. Antibacterial surface modification of titanium implants in orthopaedics. J Tissue Eng 2018 ;9:2041731418789838.\n- Biswas DP, O\'Brien-Simpson NM, Reynolds EC, et al. Comparative study of novel in situ decorated porous chitosan-selenium scaffolds and porous chitosan-silver scaffolds towards antimicrobial wound dressing application. J Colloid Interface Sci 2018; 515:78-91\n- Ferraris S, Spriano SJ. Antibacterial titanium surfaces for medical implants. Materials Sci Eng 2016; 61:965-978.\n- Del Curto B, Brunella MF, Giordano C, et al. Decreased bacterial adhesion to surface-treated titanium. Int J Artif Organs 2005; 28:718–730.\n- Kumar HS, Parvathi A, Karunasagar I, et al. Prevalence and antibiotic resistance of Escherichia coli in tropical seafood. World J Microbiol Biotechnol 2005; 21:619–23.\n- Zhou X, Li Y. Atlas of oral microbiology: From healthy microflora to disease. Springer Nature 2021.\n- Ali AK, Basima GH, Abbas S. Potential effect of 2% chlorhexidine gel in the implant screw hole on bacterial count. Int J Med Res Health Sci 2017; 6:125-32.\n- Marsh PD, Devine DA. How is the development of dental biofilms influenced by the host?. J Clin Periodontol 2011; 38:28-35.\n- Reibel J. Prognosis of oral pre-malignant lesions: Significance of clinical, histopathological, and molecular biological characteristics. Crit Rev Oral Biol Med 2003; 14:47-62.\n- Byadarahally Raju S, Rajappa S. Isolation and identification of Candida from the oral cavity. Int Sch Res Notices 2011; 2011.\n- Mohammed AA, Hamad TI. Assessment of anti-bacterial effect of faujasite from patients with periimplantitis. J Res Med Dent Sci 2021; 9:166-70.\n- Belay N, Rasooly A. Staphylococcus aureus growth and enterotoxin A production in an anaerobic environment. J Food Prot 2002; 65:199-204.\n- Haenle M, Zietz C, Lindner T, et al . A model of implant-associated infection in the tibial metaphysis of rats. Sci World J 2013; 2013.\n- Abdal Dayem A, Hossain MK, Lee SB, et al . The role of reactive oxygen species (ROS) in the biological activities of metallic nanoparticles. Int J Mol Sci 2017; 18:120.\n- Geoffrion LD, Hesabizadeh T, Medina-Cruz D, et al. Naked selenium nanoparticles for antibacterial and anticancer treatments. ACS 2020; 5:2660-9.\nCitation: Osama AR Alheeti, Abdalbseet A Fatalla, Evaluation of the Anti-Bacterial Effect of Selenium Nanoparticles in Peri-Implantitis Patients, J Res Med Dent Sci, 2021, 9(12): 96-101\nReceived: 28-Sep-2021 Accepted: 23-Nov-2021', 'As urban populations continue to grow, entrepreneurs are going beyond traditional farming to find new ways to feed everyone while minimising the effect on our land and water resources. Vertical farming is one such method that has been used all around the world. Food crops may be conveniently farmed in urban settings using Vertical Farming by planting in vertically stacked layers to conserve space and require little energy and water for irrigation.\nVertical farming is the process of producing crops in layers that are vertically stacked. Controlled-environment agriculture, which tries to maximise plant development, and soil-less farming techniques such as hydroponics, aquaponics, and aeroponics, are frequently used.\nBuildings, shipping containers, tunnels, and abandoned mine shafts are among popular structures used to host vertical farming systems. There are approximately 30 hectares (74 acres) of functioning vertical farms around the globe as of 2020. Vertical farming, in conjunction with other cutting-edge technology such as customised LED lighting, has resulted in crop yields that are more than ten times greater than those obtained by standard agricultural methods.\nVertical farming is still in its early stages in India, but there are a few entrepreneurs and agri-tech enterprises aiming to revolutionise the area.\nVertical Farming Background and Concept\nGilbert Ellis Bailey originated the phrase “vertical farming” and published a book named “Vertical Farming” in 1915. William Frederick Gerick pioneered hydroponics at the University of California, Berkeley, in the early 1930s.\nke Olsson, a Swedish ecological farmer, devised a spiral-shaped rail system for growing plants in the 1980s and proposed vertical farming as a method of raising vegetables in cities.\nProfessor Dickson Despommier invented the concept of vertical farming in 1999. His idea was to grow food in urban areas, utilising less distance and saving time in transporting food produced in rural regions to cities.\nHe aimed to produce food in urban areas in order to have fresher goods available sooner and at a reduced cost. As a result, vertical farming is defined as the cultivation and production of crops/plants in vertically stacked layers and vertically inclined surfaces.\nThe plants are vertically piled in a tower-like form in the physical arrangement. This reduces the amount of space needed to cultivate plants. Following that, a combination of natural and artificial lighting is employed to ensure an ideal atmosphere for the plants’ effective growth. The third component is the plant’s growth medium. Aeroponic, hydroponic, or aquaponic growth media are employed instead of soil as the growing medium.\nAs the methodology gets more scientific, the process’s efficiency grows, and as a result, vertical farming becomes more sustainable, consuming 95 percent less water than previous agricultural methods.\nAlso Read, Oxagon: The World’s First Floating City in the World\nVertical Farming Techniques\nIt is a method of producing food in water without the use of soil by employing mineral fertiliser solutions.\nThe primary benefit of this strategy is that it lowers soil-related cultivation issues such as soil-borne insects, pests, and illnesses.\nAeroponics was inspired by NASA’s (National Aeronautical and Space Administration, USA) endeavour in the 1990s to develop an effective technique to grow plants in space. There is no growth medium in aeroponics, hence there are no containers for growing crops. Instead of water, mist or nutrient solutions are utilised in aeroponics. Because the plants are attached to a support and the roots are sprayed with nutritional solution, there is very little space, very little water, and no soil required.\nThe name aquaponics is derived from the combination of two words: aquaculture (fish farming) and hydroponics (the process of growing plants without soil in order to develop symbiotic interactions between the plants and the fish). The symbiosis is established by feeding nutrient-rich waste from fish tanks to hydroponic production beds called “fertigate.”\nIn turn, the hydroponic beds act as biofilters, removing gases, acids, and chemicals from the water, such as ammonia, nitrates, and phosphates. Furthermore, the gravel beds serve as a home for nitrifying bacteria, which aid in nutrient cycling and water filtering. As a result, the newly cleansed water may be recirculated back into the fish tanks.\nThe Benefits of Vertical Farming\nVertical farming offers various advantages, making it promising for agriculture’s future. The land need is fairly minimal, water usage is 80% less, water is recycled and stored, pesticides are not used, and in the case of high-tech farms, there is no true reliance on the weather.\nA vertical farm makes farming possible within the constraints of a metropolis. When the farms are close by, the food is delivered swiftly and is always fresh, as opposed to the chilled stuff commonly seen in stores. Transportation reduction minimises the cost of fossil fuels and the accompanying emissions, as well as transportation spoilage. Vertical farming, like anything else, has its limitations. The biggest issue is the initial capital expenses for building the vertical farming system.\nThere are further expenditures associated with building the structures as well as their automation, such as computerised and monitoring systems, remote control systems and software, automated racking and stacking systems, programmable LED lighting systems, temperature control systems, and so on.']"	['<urn:uuid:a088b9d5-feda-442b-91c3-5699115d46fa>', '<urn:uuid:38abc93f-c15d-442d-abe3-4bcfbd141419>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T03:34:55.284799	13	89	3496
38	What is the main difference between Tonino Picula's role as Minister of Foreign Affairs and the responsibilities of the Vatican's Secretary for Relations with States?	While both roles involve international relations, their responsibilities differ significantly. As Croatia's Minister of Foreign Affairs (2000-2003), Picula focused on national diplomatic achievements like Croatia's candidacy for NATO and EU, and signing the Stabilization and Association Agreement. In contrast, the Vatican's Secretary for Relations with States has broader ecclesiastical responsibilities, including managing the Holy See's diplomatic relations with all states, handling concordats, representing the Holy See in international organizations like the UN, and participating in the appointment of bishops in countries with treaties with the Holy See.	"[""Secretariat of State (Holy See)(Redirected from Chancery of Apostolic Briefs)\nThis article needs additional citations for verification. (September 2013) (Learn how and when to remove this template message)\nThe Secretariat of State is the oldest dicastery in the Roman Curia, the central papal governing bureaucracy of the Catholic Church. It is headed by the Cardinal Secretary of State and performs all the political and diplomatic functions of the Holy See. The Secretariat is divided into two sections, the Section for General Affairs and the Section for Relations with States, known as the First Section and Second Section, respectively.\nHistory of the Secretariat of StateEdit\nThe origins of the Secretariat of State go back to the fifteenth century. The apostolic constitution Non Debet Reprehensibile of 31 December 1487 established the Secretaria Apostolica comprising twenty-four Apostolic Secretaries, one of whom bore the title Secretarius Domesticus and held a position of pre-eminence. One can also trace to this Secretaria Apostolica the Chancery of Briefs, the Secretariat of Briefs to Princes and the Secretariat of Latin Letters.\nPope Leo X established another position, the Secretarius Intimus, to assist the Cardinal who had control of the affairs of State and to attend to correspondence in languages other than Latin, chiefly with the Apostolic Nuncios (who at that time were evolving into permanent diplomatic representatives). From these beginnings, the Secretariat of State developed, especially at the time of the Council of Trent.\nFor a long time, the Secretarius Intimus, also called Secretarius Papae or Secretarius Maior, was almost always a prelate, often endowed with episcopal rank. It was only at the beginning of the pontificate of Innocent X that someone already a Cardinal and not a member of the Pope's family was called to this high office. Pope Innocent XII definitively abolished the office of Cardinal Nephew, and the powers of that office were assigned to the Cardinal Secretary of State alone.\nOn 19 July 1814, Pope Pius VII established the Sacred Congregation for the Extraordinary Ecclesiastical Affairs, expanding the Congregatio super negotiis ecclesiasticis Regni Galliarum established by Pius VI in 1793. With the apostolic constitution Sapienti Consilio of 29 June 1908, Saint Pius X divided the Sacred Congregation for Extraordinary Ecclesiastical Affairs in the form fixed by the Codex Iuris Canonici of 1917 (Can. 263) and he specified the duties of each of the three sections: the first was concerned essentially with extraordinary affairs, while the second attended to the ordinary affairs, and the third, until then an independent body (the Chancery of Apostolic Briefs), had the duty of preparing and dispatching pontifical Briefs.\nWith the apostolic constitution Regimini Ecclesiae universae of 15 August 1967, Pope Paul VI reformed the Roman Curia, implementing the desire expressed by the bishops in the Second Vatican Council. This gave a new face to the Secretariat of State, suppressing the Chancery of Apostolic Briefs, formerly the third section, and transforming the former first section, the Sacred Congregation for the Extraordinary Ecclesiastical Affairs, into a body distinct from the Secretariat of State, though closely related to it, which was to be known as the Council for the Public Affairs of the Church.\nOn 28 June 1988, John Paul II promulgated the apostolic constitution Pastor bonus, which introduced a reform of the Roman Curia and divided the Secretariat of State into two sections: the Section for General Affairs and the Section for Relations with States, which incorporated the Council for the Public Affairs of the Church.\nThe head of the Secretariat of State is the Secretary of State, who is a cardinal. The Cardinal Secretary of State is primarily responsible for the diplomatic and political activity of the Holy See, in some circumstances representing the Pope himself.\nSection for General AffairsEdit\nThe Section for General Affairs handles the normal operations of the Church including organizing the activities of the Roman Curia, making appointments to curial offices, publishing official communications, papal documents, handling the concerns of embassies to the Holy See, and keeping the papal seal and Fisherman's Ring. Abroad, the Section for General Affairs is responsible for organizing the activities of nuncios around the world in their activities concerning the local church.\nSubstitute for General AffairsEdit\nThe Section for General Affairs is headed by an archbishop known as the Substitute for General Affairs, or more formally, Substitute for General Affairs to the Secretary of State. The current Substitute for General Affairs to the Secretary of State is Archbishop Giovanni Angelo Becciu. There have been 9 substitutes since 1953:\n- Nicola Canali (21 March 1908 – 24 September 1914)\n- Federico Tedeschini (24 September 1914 – 31 March 1921)\n- Giovanni Battista Montini (13 December 1937 – 17 February 1953) (Later Pope Paul VI)\n- Angelo Dell'Acqua (17 February 1953 – 29 June 1967)\n- Giovanni Benelli (29 June 1967 – 3 June 1977)\n- Giuseppe Caprio (14 June 1977 – 28 April 1979)\n- Eduardo Martínez Somalo (5 May 1979 – 23 March 1988)\n- Edward Idris Cassidy (23 March 1988 – 12 December 1989)\n- Giovanni Battista Re (12 December 1989 – 16 September 2000)\n- Leonardo Sandri (16 September 2000 – 1 July 2007)\n- Fernando Filoni (1 July 2007 – 10 May 2011)\n- Giovanni Angelo Becciu (10 May 2011 – )\nAssessor for General Affairs of the Secretariat of StateEdit\nThe deputy to the Substitute for General Affairs, effectively deputy chief of staff, is called the Assessor for General Affairs of the Secretariat of State. The current Assessor for General Affairs of the Secretariat of State is Monsignor Paolo Borgia.\n- Eduardo Martínez Somalo (1970 – 11 December 1975)\n- Giovanni Battista Re (12 January 1979 – 9 October 1987)\n- Crescenzio Sepe (10 October 1987 – 2 February 1992)\n- Leonardo Sandri (4 February 1992 – 22 July 1997)\n- James Michael Harvey (22 July 1997 – 7 February 1998)\n- Pedro Lopez Quintana (7 February 1998 – 12 December 2002)\n- Gabriele Giordano Caccia (17 December 2002 – 16 July 2009)\n- Peter Bryan Wells (16 July 2009 – 9 February 2016)\n- Paolo Borgia (4 March 2016 – present)\nSection for Relations with StatesEdit\nThe Congregation for the Ecclesiastical Affairs of the Kingdom of France was set up by Pope Pius VI with the Constitution Sollicitudo omnium ecclesiarum in 1793 to deal with the problems created for the Church by the French Revolution. In 1814, Pope Pius VII gave this office responsibility for negotiations with all governments, renaming it the Extraordinary Congregation for the Ecclesiastical Affairs of the Catholic World (Latin: Congregatio Extraordinaria Praeposita Negotiis Ecclesiasticis Orbis Catholici). Some years later, Pope Leo XII changed its name to the Sacred Congregation for Extraordinary Ecclesiastical Affairs (Latin: Sacra Congregatio pro Negotiis Ecclesiasticis Extraordinariis), which remained its title until 1967 when Pope Paul VI separated this body from the Secretariat of State, calling it the Council for the Public Affairs of the Church. This Council was later replaced by the present Section for Relations with States.\nThe Section is responsible for the Holy See's interactions with civil governments. According to the relevant articles of the apostolic constitution Pastor bonus, the responsibilities of the Secretary for Relations with States are:\n- for the Holy See's diplomatic relations with states, including the establishment of concordats or similar agreements;\n- for the Holy See's presence in international organizations and conferences such as the United Nations;\n- in special circumstances, by order of the Supreme Pontiff and in consultation with the competent dicasteries of the Curia, provides for appointments to particular Churches, and for their establishment or modification;\n- in close collaboration with the Congregation for Bishops, it attends to the appointment of bishops in countries which have entered into treaties or agreements with the Holy See in accordance with the norms of international law.\nThe Section is headed by an Archbishop, the Secretary for Relations with States, who reports to the Secretary of State. His staff includes a Prelate, the Under-Secretary for Relations with States, and is assisted by Cardinals and Bishops. The Secretary for Relations with States is often called the foreign minister of the Holy See, and the Under-Secretary is often called the deputy foreign minister.\nThe current Secretary for Relations with States is Archbishop Paul Gallagher. The current Undersecretary for Relations with States is Monsignor Antoine Camilleri. The current Delegate for Pontifical Representations is Archbishop Jan Pawłowski and the current Head of Protocol is Monsignor José Avelino Bettencourt.\n- Secretariat of State by GCatholic.org\n- Profile of the Secretariat from vatican.va\n- The Pope's Team: the Vatican's Secretariat of State from Catholic Culture"", 'This biography of a living person needs additional citations for verification . (September 2016) (Learn how and when to remove this template message)\n|Member of the European Parliament |\n1 July 2013\n|Observer of the European Parliament for Croatia|\n1 April 2012 –1 July 2013\n|Mayor of Velika Gorica|\n17 June 2005 –17 June 2009\n|Preceded by||Ivan Šuker|\n|Succeeded by||Dražen Barišić|\n|Minister of Foreign Affairs|\n27 January 2000 –22 December 2003\n|Prime Minister||Ivica Račan|\n|Preceded by||Mate Granić|\n|Succeeded by||Miomir Žužul|\n|Born||31 August 1961|\nMali Lošinj, PR Croatia, FPR Yugoslavia\n|Political party|| Social Democratic Party |\nProgressive Alliance of Socialists and Democrats\n|Alma mater|| University of Zagreb |\n(Faculty of Humanities and Social Sciences)\nTonino Picula (born 31 August 1961) is a Croatian politician currently serving his third term as a Member of the European Parliament for Croatia, having successfully run in 2013, 2014, and 2019 European elections.He got involved in politics in the early 1990s and had served four consecutive terms as a member of the Croatian Parliament, having been elected in 2000, 2003, 2007, and 2011 parliamentary elections as a member of the center-left Social Democratic Party (SDP). He served as Minister of Foreign Affairs from 2000 to 2003 under prime minister Ivica Račan, and as mayor of Velika Gorica from 2005 to 2009.\nPicula is a member of the center-left Social Democratic Party (SDP) and sits with the Progressive Alliance of Socialists & Democrats in the European Parliament.\nPicula was born in Mali Lošinj and completed both primary and secondary education in Šibenik, Dalmatia. He graduated sociology at Faculty of humanities and social sciences, University of Zagreb.\nAfter the 2000 parliamentary elections in which SDP, under Ivica Račan, won in a broad coalition, Picula was appointed Minister of Foreign Affairs and served a full term until 2003. During his term in office, Croatia had several important foreign-relation successes, including becoming a candidate for NATO and the European Union and joining World Trade Organization. He signed the Stabilization and Association Agreement on behalf of Croatian government, and submitted the country\'s application for membership in the EU.\nDuring local elections in 2005, he was elected mayor of Velika Gorica. He also led the SDP branch in Velika Gorica from 1997 to 2000.\nOn 1 April 2012, as part of preparations for Croatia\'s full EU membership, Sabor appointed Picula to the European Parliament as one of the 12 ""observer"" members from Croatia.At the first Croatian European Parliament elections in 2013, he was elected to the remainder of the Parliament\'s 2009–2014 term, and then again to two full terms in 2014 and 2019.\nPicula has been a member of the Committee on Foreign Affairs (AFET)since 2013 where he currently serves as a senior member responsible for coordinating the work of the Socialists and Democrats in the Committee. He is a substitute member of the Delegation for relations with the United States of America (D-US), and as such, in June 2020, had been appointed the European Parliament’s first standing rapporteur for relations with the USA.\nPicula has been a strong proponent of the EU enlargement, and has held it is EU’s most successful policy and most powerful tool for promoting democracy, prosperity and peace.In November 2019, AFET appointed him to the position of the Rapporteur for Recommendations on the Western Balkans ahead of the May 2020 Summit in Zagreb. The key recommendations of the Report were to ensure that the improved negotiation methodology has the full EU membership as its ultimate goal and that the EU provides clear and predictable rules and criteria and applies them consistently, thereby restoring its credibility. His dedication to EU – Western Balkans relations is also reflected in his memberships in the Delegation for relations with Bosnia and Herzegovina, and Kosovo (DSEE) as well as in the Delegation for relations with Serbia. In 2019 he was named Co-Rapporteur for IPA III Financial Instrument for Pre-Accession Assistance and Standing Rapporteur on Montenegro. As of January 2020 he is also the Chairman of the Working Group for the Western Balkans, one of the five AFET working groups.\nIn the current 9th parliamentary term, he is also a substitute member in the Subcommittee on Security and Defence (SEDE),and a full member of the newly founded Special Committee on Foreign Interference in all Democratic Processes in the European Union, including Disinformation, and the Delegation to the Euronest Parliamentary Assembly (PA).\nRegional Development and Islands Policy\nPicula has dedicated much of his parliamentary work to regional development policies with a particular emphasis on European islands. In 2014, he helped establish the European Parliament Intergroup on Seas, Rivers, Islands and Coastal Areas (SEARICA). After serving a full term as its Vice-Chair in charge of islands, he was elected its Chair in 2019.Picula gained prominence as island policy champion when in 2016 he secured 2 million EUR for establishing the Clean Energy for EU Islands (CE4EUI) Secretariat through an amendment to the EU budget. In 2019, Picula proposed two additional amendments worth a total of 4 million euros. After an evaluation by the Commission and lengthy negotiations within the European Parliament and with the Council, Picula secured additional 2 million EUR for the CE4EUI Secretariat, and new 2 million EUR for the same model to be applied to all rural areas. Success of these initiatives led to signing of The Memorandum of Split calling for a long-term framework for cooperation to advance the energy transition for European islands, and the inclusion of the Clean Energy for EU Islands Initiative in the European Green Deal.\nPicula has also been appointed EP Rapporteur for cohesion policy commitments on climate change, which will be one of the most important tools in the coming years when it comes to monitoring whether taxpayers\' money is really invested in energy and environmentally sustainable projects and businesses.He is a substitute member of the Committee on Regional Development (REGI) and Committee on Agriculture and Rural Development (AGRI), and a member of the European Parliament Intergroup on the Welfare and Conservation of Animals.\nEarlier Parliamentary Terms\nDuring the 7th and 8th Parliamentary terms he served as the Chair of the European Parliament delegation for relations with Bosnia and Herzegovina, and Kosovo (2014-2019), substitute member of the Delegation to the EU-Serbia Stabilisation and Association Parliamentary Committee (2014-2019), and was responsible on behalf of the S&D Group for a number of reports such as those on the Accession Agreements with Ukraine and with Northern Macedonia,and the European Defence Union. He was briefly a substitute member of Committee on Petitions (PETI), and Delegation to the Parliamentary Assembly of the Union for the Mediterranean in 2013 and 2014. From 2014 until 2017, he was also a member of the Subcommittee on Security and Defence (SEDE).\nIn 2017, The Water Saving Project, initiated and coordinated with ESIN and funded by MEP Picula, was awarded the “Greening the Islands Award”The project aimed at taking action at the other end of the water problem: instead of encouraging water production, it aimed to reduce the use of freshwater through clever communication, smart engineering and wise governance, keeping in mind possible disadvantages such measures might have on people and businesses.The project initially gathered 8 islands from 4 different EU member states: Lastovo and Vis from Croatia, Houat and Sein from France, Ithaka and Tilos from Greece, Cape Clear and Inis Oir from Ireland.\nIn 2018, Picula was voted “MEP of the year” by the European Small Island Federation (ESIN), an organization representing 359,000 islanders on 1,640 small European islands.\nIn 2020, Picula was nominated for the „The Energy Award“ by the Parliament Magazine\nUlrike Lunacek is an Austrian politician who served as State Secretary for Cultural Affairs in the government of Chancellor Sebastian Kurz in 2020. She is a member of the Austrian Green party The Greens – The Green Alternative, part of the European Green Party.\nIndrek Tarand is an Estonian politician and Member of the European Parliament (MEP) from Estonia. He is an Independent politician, but a member of the European Green Party.\nAxel Voss is a German lawyer and politician of the Christian Democratic Union of Germany. He has been a Member of the European Parliament since 2009 and became coordinator of the European People\'s Party group in the Committee on Legal Affairs in 2017. His parliamentary work focuses on digital and legal topics.\nPablo Zalba Bidegain is a Spanish economist and politician and has been a Member of the European Parliament (MEP), for the European People\'s Party, in office June 2009 to November 2016. From 2016 to 2018 he served as President of the Official Credit Institute (ICO).\nPetri Juha Sarvamaa is a Finnish politician who has been serving as Member of the European Parliament (MEP) since 2014. He is a member of the National Coalition Party, part of the European People\'s Party. Before becoming an MEP, he had a long career as a journalist at Finland\'s national broadcasting company Yle.\nBiljana Borzan is a Croatian physician and politician who has been member of the European Parliament for Croatia since 1 July 2013, having been elected to the position at the 2013, 2014 and 2019 elections. She is a member of the center-left Social Democratic Party of Croatia (SDP) and serving as the Vice President of the Socialists & Democrats in the European Parliament with the portfolio ""New Economy that works for all"" since 2019.\nFranck Proust is a French politician of the Union for a Popular Movement who served as a Member of the European Parliament from 2011 until 2019, first vice-chair of the French EPP Group delegation in the European Parliament. He is first deputy mayor of Nîmes. Being originally an entrepreneur, he is also an insurance agent in Nîmes.\nEliza Vozemberg-Vrionidi is a Greek lawyer and politician who has been serving as a Member of the European Parliament (MEP) for New Democracy since 2014.\nKaja Kallas is an Estonian politician and the prime minister of Estonia since 26 January 2021. She has been the leader of the Reform Party since 2018, and a member of Riigikogu since 2019, and previously from 2011 to 2014. Kallas served as a member of the European Parliament from 2014 to 2018, representing the Alliance of Liberals and Democrats for Europe. Before her election to parliament, she was an attorney specializing in European and Estonian competition law.\nBodil Valero is a Swedish politician who served as a Member of the European Parliament (MEP) from 2014 until 2019. She is a member of the Green Party, part of the European Green Party. In parliament, Valero served as the green coordinator in the Security and Defence Committee (SEDE).\nRamón Jáuregui Atondo is a Spanish politician. A member of the Spanish Socialist Workers\' Party (PSOE), he served as Minister of the Presidency (2010–2011) during the Second Zapatero Government. He also was member of the European Parliament in two separate spells.\nMiapetra Kumpula-Natri is a Finnish politician who currently serves as a Member of the European Parliament (MEP) from Finland. She is a member of the Social Democratic Party, part of the Progressive Alliance of Socialists and Democrats. She has been a Member of the European Parliament since 2014.\nBirgit Sippel is a German politician who has been serving as a Member of the European Parliament (MEP) since 2009. She is a member of the Social Democratic Party, part of the Party of European Socialists.\nHilde Vautmans is a Belgian politician of the Open Vlaamse Liberalen en Democraten who has been serving as a Member of the European Parliament since January 2015, representing the Dutch-speaking electoral college of Belgium.\nOlga Sehnalová is a Czech politician, who, since July 2009 has served as a Member of the European Parliament, representing Czech Republic for the Social Democratic Party\nJens Gieseke is a German politician who has been serving as a member of the European Parliament since 2014. He is a member of the Christian Democratic Union, part of the European People\'s Party.\nThe 9th European Parliament was elected in the 2019 elections and is to last until the 2024 elections.\nVladimír Bilčík is a Slovak university lecturer and politician of the TOGETHER - Civic Democracy political party who has been serving as a Member of the European Parliament since 2019. For the European Parliament elections in 2019, Vladimír Bilčík ran as a leader of TOGETHER - Civic Democracy political party in a back-then Coalition Progressive Slovakia & TOGETHER - Civic Democracy. He got 26 202 preferential votes in total.\nDaniel Freund is a German politician who has been serving as a Member of the European Parliament since July 2019. He is a member of Alliance 90/The Greens at national level, and sits with the Group of the Greens/European Free Alliance in the European Parliament.\n|Minister of Foreign Affairs |\n|Velika Gorica Mayor of |']"	['<urn:uuid:9650003f-0c89-4ba1-9792-c54ebe2aea8b>', '<urn:uuid:b04de860-84b0-4883-bbe7-3ec40ab5cef3>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-13T03:34:55.284799	25	87	3554
39	when did pompeii volcano actually erupt	While it was long believed that the eruption of Vesuvius occurred on August 24-25, 79 AD, recent evidence suggests the actual date was October 24-25, 79 AD. This later date is supported by archaeological findings including autumnal fruits in Pompeii and heavy clothing worn by victims, as well as a charcoal inscription discovered on a wall in Pompeii referring to events after October 17. The eruption began with earthquakes on August 20th, leading to a massive explosion four days later that turned the sky black with ash. The most deadly phase occurred the following day when part of the mountain imploded, causing earth tremors and tidal waves, followed by a pyroclastic flow that reached temperatures up to 800°C.	['Pompeii volcanic disaster\nBay of Naples, Southern Italy\nArticle and pictures, Tony Hart-Wilden\nThe city of Pompeii on the west coast of Italy was originally founded in the 6th century BC by the Greeks. In 80 BC, temples houses and an amphitheater were constructed there by the Romans. The town had been built in the shadow of Vesuvius an active volcano. On August 20th 79 AD a minor eruption caused a series of small earthquakes, and four days later on the afternoon of August the 24th there was a massive explosion that turned the sky black as tons of ash reigned down upon the fleeing population. A thermal energy blast a hundred thousand times greater then that of the nuclear bomb dropped on Hiroshima was unleashed. For almost 1700 years the city and its long dead occupants lay buried until Italian architect Domenico Fontana discovered the ruins during construction of a tunnel. Excavations began over two hundred years ago and still continue. But instead of destroying the city much of the ash had ultimately preserved the former occupants and the buildings they once lived in, and both are forever frozen in time.\nI was staying in Naples Italy, and to reach Pompeii itself it was about an hours drive south. I would advise people to avoid driving in Italy if at all possible, and take an organized coach trip as Italian drivers are some of the worst in the world. I got to Pompeii at about ten in the morning. It is well sign posted being one of the areas major tourist attractions, and it is located just a few minutes off of the motorway. You will see Mount Vesuvius in the distance on your left hand side, long before you reach Pompeii itself, and once you park the car it is only a few minutes walk to the entrance. There are several restaurants and stalls selling souvenirs along the outside road. But if you want anything to eat or drink, make sure you buy it before you go inside, as there is nothing available once you are in the city itself. Although it’s just gone 10 am there are already dozens of coach loads of tourists being offloaded, and it is close to 90 F.\nThere is a admission fee, and you have the option of getting an electronic audio guide. One thing to realize is, this is that the area that you will be walking around is several square miles, and there is little shade and no water or restrooms. But although this is inconvenient, it does help to preserve the city as it once was and there are few signs of the 21st century once you enter. First you walk up a slight hill and pass through what is known as Port Marina, this was one of the gates to the city and is the one closest to the sea. Although from where I’m standing I can’t actually see the ocean, but there is an explanation for this. Before the volcanic eruption the sea was only 500 meters away. But the explosion produced so much ash and debris, that it created an artificial shoreline of almost two kilometers. In later years this area was built upon and there is now a village where the sea once was. As you pass through Port Marina you can see that the layout of the streets ahead of you are much like those of a modern day city. There is a main street and many side streets in a grid pattern, that lead up and down the slight hill.\nIf you look over to your left there is a mountain range with one particular mountain much larger then any of the other ones. This is Mount Versuvous . What you will notice is that it does appear to be a considerable distance away from the city. Most people assume that Pompeii was built beneath it on its slopes. But it is actually several miles away. On your right hand side there are some ruins that were actually destroyed by the earthquake in 62 AD as opposed to the volcanic explosion in 79 AD. This was once the temple of Venus, but only a few stones remain. The streets are much narrower then those in a modern city, and either side there are blocks of flat stones that once acted as pavements in front of the houses and shops. Today they sky is completely blue, it’s hot and there’s very little breeze. But on the afternoon of 79 AD on August the 24th the sky turned black. The volcano was almost 9 km away and still managed to bury the entire city as the sun was blotted out by volcanic ash. The eruption continued to the next day, it was a gradual process that at its peak was estimated to have thrown out over a hundred thousand tons of deb town and its inhabitants to a depth of almost ten feet gives some idea of its power. After about ten minutes walk I came to a large square with once elaborate ruins running around all sides.\nThis area is where the temple of Apollo and the Forum once stood. Although the roofs no longer remain, many of the marble columns and statues still survive. You can peer across the square to see Mount Vesuvius framed in the background and this gives some of the most dramatic views in the entire city. The Forum was the center of political and religious life for the people of Pompeii ; along its sides are large stone pedestals upon which statues of the city’s most important people was stood. If you continue along the main street you will pass many roman villas. Several of them still have ornate pictures painted on the walls that were almost perfectly preserved under the ash. Many of the mosaics on the floors are also virtually untouched. Although many parts of the buildings still stand, it was the weakness of the roofs that caused the deaths of many of the inhabitants. Those that couldn’t escape tried to shelter in their homes, and gradually as the weight of the ash built up, the structure collapsed on top of them. Although a large part of Pompeii has been excavated there is still about a fifth of it that is still buried. You can walk around the city for hours and not see it all.\nEverywhere you go there are examples of architecture almost two thousand years old. This is what makes it so different from a museum, it’s not hidden behind a glass case and you can actually touch\nand walk among the exhibits. This was an entire community destroyed in a matter of hours by the same volcano that still looms on the horizon in front of you. Further on, the main street slopes downwards. You first pass a building that was once a barracks for the roman gladiators, then beyond that is a house known as that of the Chaste Lovers. Painted on the walls are several pictures showing very little signs of decay. Although thousands of years old the colors barely seem to have faded, and great care has been taken to preserve and restore them. The population of Pompeii at the time of the eruption was made up of about 40% slaves, and the rest were noblemen, soldiers and merchants. One of the results of the volcanic ash clouds was to bury people where they fell, and preserve them much like fossils.\nIn 1860 a man by the name of Guiseppe Fiorelli came up with the idea of filling the spaces that the body’s once made in the ashes, with plaster of Paris. This captured in minute detail their moment of death. There are many of these body casts throughout the city. Many of them at the exact locations they fell almost two thousand years ago. Although many had fled Pompeii after the start of the first eruption on August the 24th, it was on August the 25th that the volcano became its most deadly. Part of the mountain imploded in on itself that caused massive earth tremors and tidal waves to sweep inland across the bay of Naples. Then a pyroclastic flow composed of gas, ash and volcanic rock drifted across towards the town of Pompeii at great speed, only taking about five to six minutes to cover the 9 km to the town.\nIt was this lethal combination of super heated air reaching temperatures of up to 800 C and a black suffocating ash cloud that killed so many. As you walk along the street down towards the amphitheater, you pass ruins that were once people’s homes, shops where people once brought bread and temples where they once prayed. A few thousand years later you can still walk along the same streets that they did. I have been to other “ghost towns” in Arizona, and Nevada. But the fact that this was mostly built of stone rather then wood meant it is far better preserved despite its greater age. Just before the amphitheater is the Palaestra which is a huge grass walled courtyard. There was also once a large swimming pool which just likes a modern day pool, slopes from a shallow to a deep end. If you continue beyond this, there is the entrance to an amphitheater that was built in 80 BC. This is believed to be the oldest known roman amphitheater ever discovered and lies largely intact at the east end of the city.\nIf you walk out into the middle of the arena you notice that the ground level is actually below that of the surrounding area. This is because the earth to support the angled stone seats around the arena is supported by the earth dug from its center. Much of the structure was destroyed by the earthquake in AD 62 but was rebuilt, before being completely buried in AD 79. There are two entrances to the amphitheater one at the north end the other at the south. Next to both of these there are two small rooms known as “Sporgliarii” these are believed to be where the bodies of the dead gladiators that fought in the arena were brought after battle. The amphitheater also once had a canopy around its upper region that provided shade for the crowds during hot weather. If you compare this to today’s sports stadiums, you can see how similar in function and design they still are even after two thousand years. If you walk back out of the north entrance of the amphitheater you come to a grassy area, which leads back to the main street of the city. These streets would at one time have been clogged with dead bodies from people trying to escape.\nIn modern times excavations began in the 1700’s during which, aside from the ruined buildings, almost 1800 scrolls written in Greek were discovered in the rubble. In 1763 an inscription was found that confirmed that the city was indeed that of Pompeii. In 1798 when the French invaded Italy under Emperor Napoleon the extent of the city walls were uncovered. In 1835 excavations ceased. Then in 1838 queen Victoria visited the site. In 1865 the city was divided into grids and the houses numbered, and modern scientific excavation principles were applied. In 1943 the allies dropped 150 tons of bombs on Pompeii in the belief that the Germans were using it to store ammunition. In 1963 the city of Pompeii again became overgrown and fell into disrepair. Then excavations began once again, and in 1980 150 skeletons were found within the arches of the marina. Today work still continues, but there are almost forty of the 160 acres that the city covers that are still hidden. Vesuvius still towers in the distance and after AD 79 it is has erupted frequently.\nMajor eruptions occurred in 472 AD and 1036 AD. In 1631 between 4,000 to 18,000 were killed as the volcano again exploded causing its crater to expand from 1 to 3 miles. This also once again created earthquakes and a tidal wave that swept across the bay of Naples Versuvous erupted again in 1906, and also in 1944. On this occasion it destroyed allied bombers stationed at Poggiomarina, and the Germans used the illumination from the molten lava to guide their bombing raids on Naples . Another major eruption is long overdue, and currently almost a million people now live in its path.\nA guide to Pompeii – The Telegraph\nThe unexpected catastrophe – BBC History\nMount Vesuvius – Geology.com\nVisiting Pompeii – a practical guide – The Rome Toolkit\n“Tourists swarmed the ancient site” – The Guardian', 'VESUVIUS | Revealing ancient mysteries of the 79 d.C. eruption\nA new multidisciplinary study has allowed better dating of the event and following the eruption effects up to a thousand kilometers distance\nThe event would have happened between 24 and 25 August of 79 d.C. This is the date that, based on current knowledge, the eruption of Pompeii occurred, according to the famous letter from Pliny the Younger to Tacitus. But the true date is another!\nAfter centuries of volcanic quiescence, the 79 d.C. eruption of Vesuvius destroyed part of the territory and surrounding towns. Almost two millennia after the eruption, an international team of researchers has analyzed again that event to get to a comprehensive state-of-the-art on the knowledge of the most famous eruption ever, starting from the exact date on which it occurred.\nThe integration among field studies, laboratory analyses, and historical fonts has allowed following all phases of the eruption over time, from the magma chamber to the ash deposition in areas extremely far from Vesuvius, with traces up to Greece.\nThe study “The 79 CE eruption of Vesuvius: a lesson from the past and the need of a multidisciplinary approach for developments in volcanology”, recently published on the prestigious journal ‘Earth-Science Reviews’, has been coordinated by Istituto Nazionale di Geofisica e Vulcanologia (INGV), in collaboration with Istituto di Geologia Ambientale e Geoingegneria at Consiglio Nazionale delle Ricerche (IGAG-CNR), Centro Interdipartimentale per lo Studio degli Effetti del Cambiamento Climatico (CIRSEC) and Dipartimento di Scienze della Terra at Università di Pisa, Laboratoire Magmas et Volcans at Clermont-Ferrand (LMV) in France, and School of Engineering and Physical Sciences (EPS) at the Heriot-Watt University of Edinburgh in UK. The study has been conducted in the framework of the project ‘Pianeta Dinamico’ funded by INGV.\nThe team of multidisciplinary researchers has collected and then critically analyzed the huge scientific production available for this eruption, by integrating it with new discoveries.\n“Our work examines with a wide and multidisciplinary approach different aspects of the 79 d.C. eruption, by integration of historical, stratigraphic, sedimentological, petrological, geophysical, paleoclimatic and modeling data, referring to the magmatic and eruptive processes of one among the most famous and devastating events occurred in the Neapolitan volcanic area’’, explains Mauro A. Di Vito, a volcanologist at INGV and coordinator of the study. ‘‘The work starts by dating the eruption, which would have occurred in the autumn and not on 24 August of 79 d.C. as previously thought, and continues by analyzing the volcanological data available in the proximity of the volcano, getting to thousand kilometers distance where traces of the eruption have been found in the form of fine ash’’.\n“Since the 13th century, the date of 24th August has been the subject of debate among historians, archaeologists and geologists because it is inconsistent with numerous pieces of evidence”, says Biagio Giaccio, researcher of the Igag-Cnr and co-author of the article. “Such as, for example, the finds in Pompeii of typically autumnal fruits or the heavy tunics worn by the inhabitants that were badly reconciled with the date of 24-25 August”, adds Giaccio. However, the definitive proof of the inaccuracy of the date only emerged a few years ago. “An inscription in charcoal on the wall of a building in Pompeii which translated quotes ‘The sixteenth day before the calends of November, he indulged in food in an immoderate way’, indicating that the eruption certainly occurred after 17 October”, continues Giaccio.\nThe date more accredited is therefore now that of 24-25 October.\nThe study has been integrated by a quantitative assessment of the impact of single phases of the eruption on those areas, including archaeological sites, close to the volcano.\n‘‘The spirit of our work has been that of understanding how a past event can represent a window to the future, by opening to new perspectives for the study of similar events that might occur again’’, continues Domenico Doronzo, volcanologist of INGV and co-author of the research. ‘‘This study will allow improving the applicability of forecasting models, from precursory phenomena to impact in eruptive and depositional processes. It will also allow contributing to reducing the vulnerability of those areas and infrastructures exposed to volcanic risk, not only close to the volcano but also – as the 79 d.C. event teaches – at a hundred kilometers distance from it’’.\n“In recent years, it has become increasingly important to understand the impact of eruptions on climate also to be able to study the origin and impact of some short climatic variations. However, we still do not know a lot – and with the appropriate resolution – of the climatic conditions at the time of the eruption of 79 d.C.” comments Gianni Zanchetta of the University of Pisa and co-author of the research.\n“In this work, we combined knowledge on regional climatic conditions at the time of the eruption to attempt a first synthesis” comments Monica Bini of the University of Pisa, co-author of the research. “Also, to direct future research on this aspect which still has many dark sides”.\nThe results of this study have received the appreciation of a world icon of volcanology like Raymond Cas, Prof. emeritus at the School of Earth Atmosphere and Environment of the Monash University (Australia) ‘‘The 79 AD eruption of Vesuvius volcano is one of the most iconic in the field of physical volcanology’’, states the Emeritus Professor Ray Cas. ‘‘Observations of the eruption as well as endless studies of the deposits and interpretation of the eruption processes underpin many of the concepts and understanding of explosive eruption processes in modern volcanology. A review of what is known about the eruption and its deposits is therefore very important to research volcanologists, and justifies a thorough and long review paper, such as this one. The authors are to be congratulated on the extremely comprehensive details extracted from the huge historical record and contemporary scientific literature on this iconic eruption’’.']	['<urn:uuid:f6ee0c61-edfb-409f-8be7-bb171de61367>', '<urn:uuid:e2fe2541-b7f9-4640-927b-f7ad90b328c8>']	open-ended	direct	short-search-query	distant-from-document	three-doc	novice	2025-05-13T03:34:55.284799	6	118	3103
40	infrastructure projects impact on climate and city life eastern china	In Shanghai, large infrastructure projects must consider the city's subtropical climate with monsoon influences and coastal position, including risks of flooding and typhoons from May to November. Similarly in Beijing, major infrastructure developments like the Fengtai High Speed Railway project are being designed to improve urban life while considering environmental factors. The project includes features like mirror-paneled viaducts that reflect weather conditions and surrounding landscape, while also incorporating multifunctional walkways and green spaces to enhance city living.	"['Weather Overview for Shanghai\nShanghai sits on the vast\nYangtze River delta on China’s east\ncoast, around half way between Hong Kong and Beijing. With a\nmassive 20 million inhabitants, it is large city and one of the world’s largest\nhas a humid subtropical climate marked by four distinct seasons. Climate is strongly\ninfluenced by the Asiatic monsoon winds and the city’s coastal position on the East China Sea. Temperatures vary from the mid 30s in\nsummer to below 0°C in winter. The weather is pretty good year round, although\nsummer can get a little hot and muggy for some and winter is pretty harsh. On\nbalance, autumn enjoys the best weather of the year.\nThis is the hottest and wettest time of year in Shanghai. In June the\naverage temperature climbs to 24°C with highs averaging at 27°C. July and\nAugust are the hottest months of the year when the average temperature reaches\nhighs climb to 32°C and beyond - even up to 40°C - and it doesn’t drop much\nbelow 25°C at night. From September it starts to cool, but it is still a very hot\nThe summer heat is often punctuated by heavy rains, usually\nfalling in afternoon thunderstorms that are heavy but short-lived. The heat and\nprecipitation make summer very humid and the overall effect is oppressive. The\nwettest months are June and September with over 150mm average\nrainfall each. July and August average around 130mm. Summer thunderstorms\ncan sometimes bring very heavy rain to Shanghai\nand there is a risk of flooding in certain parts of the city.\ncan occasionally be hit by typhoons, although direct hits are rare. These\nintense storms form in the northwest Pacific Ocean\nand if they make land they can cause enormous damage. Typhoon season runs from\nMay to November.\nSeptember 2007 saw Shanghai\ncome in the path of a very strong typhoon that led to 1.6million people being\nevacuated from the city and its surrounds. The region is very well prepared,\nhowever, and often it is just a case of closing the doors and windows and\nsheltering indoors for 24 hours.\nThis is one of the best times to visit Shanghai. Temperatures are very comfortable\nand rainfall is low. Late September and early October are\nespecially good times; the summer rains have died down and temperatures range\nbetween the mid teens and mid 20s. October averages just 60mm rainfall, a sharp\ndecline from the 160mm that is the norm for September. November rainfall is\neven less around 50mm and it’s another great month, though it sees the first\ncool temperatures with night time lows dropping to single figures. Daily highs\nare still in the high teens, however.\nWinter can be cold in Shanghai,\nalthough its coastal position does mean temperatures don’t drop as low as in\ninland areas. January is the coldest month of the year, with an average of 4°C,\nranging from lows of 0°C to highs of 8°C. December and January are only a\nlittle warmer, averaging around 6°C or 7°C. While temperatures\nbelow zero are rare they can occur during particularly cold snaps, and snow can\neven fall. Winter is the driest time of the year in Shanghai, and the abundance of sunshine\nmakes it very pretty. However, strong winds can make it feel much cooler than\nthe recorded temperature.\nSpring is another good time to visit Shanghai. Temperatures are a little cooler\nthan autumn and rain a little more frequent but the weather is generally very\npleasant. March is still cool, with an average of 9°C, but temperatures quickly\nincrease throughout spring with May averaging\n20°C. Rainfall quickly increases too, however, with April and May seeing the\nfirst of the heavy rains and averaging over 100mm.', 'Over the past decade, as part of the economic stimulation program, China\'s high-speed railway system has been growing at rapid pace to the world longest, connecting conveniently major cities and catering at peak times such as Chinese New Year to millions of travelers. Massive, monumental overpasses, carrying railway lines and trains running at a speed of up to 350 km/h are familiar to everyone having lived in or travelled to and from Beijing or another Chinese megacity. While these viaducts have been constructed primarily according to safety than to urban landscaping considerations, the areas below often appear abandoned, are at best filled with fenced parking spaces, or are just waste land and idle.\nThe Transport Oriented Development (TOD) of the Beijing Fengtai High Speed Railway project includes the construction of the New Fengtai Railway station as well as the New Fengtai Railway station access bridge with a total length of almost 10 km - the core element being a large bridge viaduct entering the Chinese capital from the south western outskirts, cutting into the city\'s Fengtai district whilst rising above rivers, other bridges as well as several highways and intersections.\nI. Point of Departure: Separation and Interference.\nSimilar to other railway viaducts, as they enter and pass through urban areas, the Fengtai High Speed Railway Bridge appears more like a disruptive element than an integrated part in its surroundings. This interference and disharmony with the overall Beijing urban landscape raised the attention of the local city government especially as this particular infrastructure project is developed adjacent historical sites.\nCrossboundaries was asked to look into the landscape and urban design of the designated area and to come up with a more integrated and inclusive solution. The area to be redesigned was defined as starting from the Lugou Bridge and scenic area outside the Fifth Ring Road in the West, continuing towards the East to the New Fengtai High Speed Railway station zone and continuing further across the Western Forth Ring Road, stretching in total over about 10km.\n""The relationship between a city\'s infrastructure, its urban landscape and its inhabitants shouldn\'t be a mutually restricting one, but rather interactive, lively and infused with vitality"", says Dong Hao, co-founder of Crossboundaries, ""Infrastructure is resulting in an improvement of the lifestyle and even an increase of income for many urban residents, at the same time the arrival of residents can stimulate consumption power and gives the opportunity to optimize the use of the available land for different kinds of activities.""\nWhat initially, according to the brief, was a project to create a more aesthetic, harmonious relationship between Fengtai Bridge and its surroundings, evolved soon into a comprehensive TOD design concept. Just ""beautifying"" a bridge was not an option for Crossboundaries.\nII. Transforming the Landscape: Integration and Optimization\nThe main conceptual idea of Crossboundaries\' proposed land- and cityscape design was to ""weave"" and to ""stitch"" together the different sites and zones that are currently separated by the new Fengtai bridge viaduct, in order to tie them together in a more organic, natural way and to create high quality spaces. The planners further approached the design in a way that allows for a step-by step-implementation starting from the Lugou Bridge over the total stretch of 10 km from east to west.\nReflections. Lugou Scenic Area.\nThis old stone bridge, crossing over Yongding River (in English also called ""Marco Polo Bridge""), is located about 15 km from today\'s city center. A battle that started here in July 1937 marked the beginning of the Second Sino-Japanese War, hence Lugou bridge, together with the historic 17th-century Wanping Fortress (today a museum) and its surroundings remain until today a point of historic significance for the Chinese.\nPassing by about 1 km further to the north and clearly visible from Lugou Bridge is the colossal viaduct of the New Fengtai Bridge. Its design follows mainly safety regulations, it appears as an ""interrupter"", cutting through and set rather unharmonious in the urban landscape.\nAs an initial step, to ""blend"" the colossus into its surroundings, Crossboundaries proposed to line the sides and the pillars of the Fengtai Bridge with mirror panels, absorbing and mirroring on their shiny outside the ever-changing landscape around them, an attempt to integrate the massive new bridge architecture into the urban environment. With the high-speed trains, masterpieces of technical progress, seemingly floating above, the mirrored surfaces reflect the sky, different weather situations, distant mountains, the water in the rivers and the green scenery.\nNoise reduction. Practical Considerations.\nThe continuous rhythmic surface of the mirrored paneling can potentially also function as an acoustic shield. Stainless-steel mirror finish coated sheets are applied in different reflection grades (6K, 8K, or 10K) to the bridge, creating the reflective surface visible towards the viewer. On designated areas, wherever noise reduction is vital, the panel is reinforced with a sandwich board, containing a mineral wool insulation layer that offers a much higher level of noise absorption.\nStainless steel has good self-cleaning properties, but still needs to be maintained and cleaned to keep its self-repair mechanisms and a good appearance active. Basic dirt is sufficiently washed off by rain, but since Beijing usually doesn\'t get too much rainfall, some additional cleaning within the general maintenance works is necessary.\nJoining Together: Trails as Threads.\nFor Crossboundaries, it became quickly evident, that just ""covering up"" the bridge would only provide an aesthetical touch up and be mere decoration. Urban planning considerations obliged the architects to look further into how to create more effective and functional interactions of a city\'s residents & visitors with its infrastructure, providing sustainable solutions for optimized use of space and for connections as well as allowing for all kinds of activities and interplay between citizens and infrastructure.\nThe initial approach adopted by the architects was to break through the protected safety zone along the rail tracks of the elevated infrastructure, to stimulate integration instead of separation. After consulting the bridge construction authorities, the planners had found out that the area generated by this 16m wide, heavily fenced corridor on both sides of the tracks is basically idle and unused, apart from evacuation zones every 3km. Multifunctional walkways, combining strolling, running, cycling, leisure and other activities are proposed to be inserted along both sides of the bridge.\nThe height of this trail changes continuously. Following the surrounding terrain, it can remain on the ground, combined with green space, such as a leisure park becoming the entrance of the trail. Here and there it can be lifted from the ground to above the road levels and form naturally interior spaces underneath that can be used for shops, F&B, recreational activities or other leisure functions.\nCrossboundaries divided the Fengtai Bridge area development into three planning sections, according to the terrain conditions and different elevations.\nStarting from about 5km in the West of Lugou Bridge, Crossboundaries\' plan includes turning an area that consist mainly of factories and idle land into sub-urban, multifunctional sports and leisure park zones, where visitors potentially stay from half to up to a full day. In this section, many trails are lifted above ground and hence allow for different sized commercial outlets underneath. In this way these spaces can be filled with supporting functions for urban residents (retail, F&B, etc.) visiting the nearby sport and leisure park.\nIn the very middle of the viaduct is the 3km long zone of the Fengtai Station, where the rail tracks are again all running on ground level. It is surrounded by a large number of residential areas and newly planned subway line connections. As a consequence, the areas above represent large available surfaces that can potentially be further developed into a larger urban park, connecting both sides, the North and South of the railway line and becoming a gathering point for citizens living nearby but also visitors arriving by subway.\nThe most East 2km section of the railway viaduct is passing over the West Fourth Ring Road, with the railway again lifted above, even the public highway. Crossboundaries developed a “Linear Park” for the areas running parallel to the existing tracks on the ground level, offering plenty of usable area for markets, sport & leisure and other activities for the residents of the surrounding districts.\nGradual Change. Steps to Implementation.\nDespite the usual speed in which large construction projects emerge in China, a project of the scope and complexity of the urban planning proposed by Crossboundaries for the Fengtai High Speed train viaduct area is also a collective process. It requires extensive dialogue, exchange and also acceptance among all the stakeholders - such as local and central government authorities, residents, businesses and other involved parties. The architects therefore divided the overall landscape implementation into four steps, gradually carried out over a period of 10 years to allow for enough time for alignment and a thorough approach in realizing this project.\n1. Improve the landscape within the security fences along rail tracks.\n2. Extend the green areas beyond the fence to the areas outside of it, begin assimilation with city landscape, partial opening.\n3. Remove fence and connect with surrounding landscape through different level trails, that are open to the public. ¬\n4. Expand landscape into 3D - elevate landscape, plan supporting businesses and functions, sustainable development, finalize implementation of TOD principles.\nThe mirror paneling could be applied in Step 1 already, as the application would take place within the existing fence, or be carried out at a later stage.\nWrap up. Sustainable Urban Design.\n""Our concept is based on a very conscious interaction with the existing spaces, the resources and the infrastructure of a city, and we adopt a special TOD approach in order to conceive social and multifunctional transportation areas"", co-founder Binke Lenhardt says,\n""In our view, infrastructure provides to people not only more convenience, but potentially also more diversity. Looking into solutions for harmonious co-existence, stimulations of interactions and integration of infrastructure, urban landscape and people are for us at the intersection of successful urban planning considerations. As architects we strongly believe that we have a responsibility to realize concepts for a livable city in the future, in order to achieve a sustainable urban living environment.""']"	['<urn:uuid:cd44dc91-35c5-4ca3-b826-2fd06c47bfcf>', '<urn:uuid:86e48bd9-9835-4b0f-b980-a714e8d77689>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T03:34:55.284799	10	77	2312
41	How is collaboration defined in leadership contexts?	Collaboration is a strategy for building relationships and getting things done, involving people with diverse interests working together to achieve mutually satisfying outcomes. Decisions are made by consensus through ongoing negotiations.	"['Send the link below via email or IMCopy\nPresent to your audienceStart remote presentation\n- Invited audience members will follow you as you navigate and present\n- People invited to a presentation do not need a Prezi account\n- This link expires 10 minutes after you close the presentation\n- A maximum of 30 users can follow your presentation\n- Learn more about this feature in our knowledge base article\nSocial Change Model of Leadership Development\nTranscript of Social Change Model of Leadership Development\nControversy with Civility\nConsciousness of self\nSocial Change Model of Leadership Development\nLet\'s explore leadership...\nIs he a leader?\nWhat about him?\nWhat about you?\nCan you be a leader?\nLeadership is a relational and ethical process of people together attempting to accomplish positive change.\n""Never doubt that a small group of thoughtful committed citizens can change the world. Indeed, it is the only thing that ever has."" Margaret Mead\nIntroductions and Discussions\nWhy are you here?\nWhat interests you about your issue area?\nSet ground rules\nExpectations for workshop and the year\nWhat is leadership to you? Activity: Leadership Barometer\nDebunking some myths\nLeaders are made, not born.\nIn today\'s fluid organizations, leadership occurs at all levels.\nHaving a charismatic personality is not a prerequisite for leadership.\nThere is not one right way to lead an organization or group. Effective leadership will be determined by personal experience and the context of the setting.\nBeing a manager and being a leader are not the same things.\nLeadership is a teachable discipline\nLet\'s move to a working definition (one I personally happen to really like)...\nBased on what we\'ve learned...\nAnd this guy?\nAssumptions of the Model\n""Leadership"" is concerned with effecting change on behalf of others and society.\nLeadership is collaborative.\nLeadership is a process rather than a position.\nLeadership should be value-based.\nAll students (not just those who hold formal leadership positions) are potential leaders.\nService is a powerful vehicle for developing students\' leadership skills.\nLeaders must be tough enough to fight,\ntender enough to cry,\nhuman enough to make mistakes,\nhumble enough to admit them,\nstrong enough to absorb the pain,\nand resilient enough to bounce back\nand keep on moving.\nSeven C\'s Represent Critical Values\nthe value ""hub"" which gives meaning and purpose to the 7 C\'s. Change is the ultimate goal of the creative process of leadership--to make a better world and a better society for self and others\nmeans being aware of the beliefs, values, attitudes, and emotions that motivate one to take action.\nrefers to thinking, feeling, and behaving with consistency, genuineness, authenticity, and honesty toward others. Congruent persons are those whose actions are consistent with their most deeply-held beliefs and convictions. Clearly, personal congruence and consciousness of self are interdependent.\nWE LEAD Workshop 2010\nis the psychic energy that motivates the individual to serve and that drives the collective effort. Commitment implies passion, intensity, and duration. It is directed toward both the group activity as well as its intended outcomes. Without commitment, knowledge of self is of little value. And without knowledge of self, commitment is easily misdirector. Congruence, in turn, is most readily achieved when the person acts with commitment and knowledge of self.\nis to work with others in a common effort. It constitutes the cornerstone value of the group leadership effort because it empowers self and others through trust. Collaboration multiplies group effectiveness by capitalizing on the multiple talents and perspectives of each group member and on the power of that diversity to generate creative solutions and actions. Collaboration empowers each individual best when there is a clear-cut ""division of labor.""\nmeans to work with shared aims and values. It facilitates the group\'s ability to engage in collective analysis of the issues at hand and the task to be undertaken. Common purpose is best achieved when all members of the group share in the vision and participate actively in articulating the purpose and goals of the leadership development activity. Recognizing the common purpose and mission of the group helps to generate the high level of trust that any successful collaboration requires.\nrecognizes two fundamental realities of any creative group effort: that differences in viewpoint are inevitable, and that such differences must be aired openly but with civility. Civility implies respect for others, a willingness to hear each other\'s views, and the exercise of restraint in criticizing the views and actions of others. This is best achieved in a collaborative framework and when a common purpose has been identified. Controversy can often lead to new, creative solutions to problems, especially when it occurs in an atmosphere of civility, collaboration, and common purpose.\nis the process whereby the individual and the collaborative group become responsibly connected to the community and the society through the leadership development activity. To be a good citizen is to work for positive change on behalf of others and the community. Citizenship thus acknowledges the interdependence of all who are involved in or affected by these efforts. It recognizes that the common purpose of the group must incorporate a sense of concern for the rights and welfare of all those who might be affected by the group\'s efforts. Good citizenship thus recognizes that effective democracy involves individual responsibility as well as individual rights.\nWho are you?\nActivity: Write your eulogy\nBridging the Individual and the Group\nCrossing the Line\nCommunity- Moving to Action\nWhat is collaboration?\nCollaboration is a strategy for building relationships and getting things done. It involves people with diverse interests working together to achieve mutually satisfying outcomes.\nDecisions are made by consensus...not to be confused with consent or everyone\'s preffered option. It means a decision everyone can support--> one that will give everyone a common purpose. It takes time, but is required for true collaboration.\nCollaboration is an on-going series of negotiations. Negotiation doesn\'t bring about change in beliefs. It leads to agreed-upon changes in behaviour through commitments. Problem solving negotiation can be an effective collaboration tool as you consider others\' interests as well as your own.\nReview ""Guidelines for Effective Collaboration"" on p. 27\nDon\'t forget about culture and diversity. How might the WE LEAD group culture affect collaborations with community partners and student groups? Before collaboration can occur, it is imperative that you pro-actively name norms and standards with your collaborators so that the group chooses them, thus identifying the thinking of the group as a whole (p. 28). Does this remind you of any activity from yesterday?\nCollaborative Process Stages and Steps\nIdentify Assets and Capabilities your organization might provide in collaborations (see Worksheet 1)\nDetermine Benefits your organization might seek in Collaborations (see Worksheet 2)\nList your organization\'s current relationships (worksheet 3)\nDelegate responsibilities for guiding collaboration development\nAt this state you are ready to research potential organizations to assess the strategic fit and opportunities (see worksheet 4)\nIn Stage III, the collaborating organizations are ready to develop shared expectations on projects and to determine how to contribute strategic benefits to each organization. See the seven steps to project collaboration.\nAfter your organization has engaged in strategic collaborations, it is important to appraise and renew them. This allows your organization to maximize the benefits, update your planning, and continue to incorporate collaborations into long-range planning.\nCollaboration Reflection Questions\nThink iof ineffective collaborations you may have experienced. What went wrong?\nIf you had to do it over the next time, what steps would you take?\nReflect on your own cultural norms and standards. Can you think of times when they were not helpful? What would you do differently to balance differing norms and standards?\nThe Stages of Group Development\n1. Orientation (Forming)\nFeeling moderately eager with high expectations\nTesting the situation and central figures\nDepending on authority and hierarchy\nNeeding to find a place and establish oneself\nExcited about new possibilities\n2. Dissatisfaction (Storming)\nExperiencing a discrepancy between hopes and reality\nFeeling dissatisfied with dependence on authority\nFeeling frustrated around goals, tasks and action plans\nFeeling incompetent and confused\nCompeting for power and/or attention\n3. Resolution (Norming)\nResolving discrepancies between expectations and reality\nResolving polarities and animosities\nDeveloping harmony, trust, support, and respect\nDeveloping self-worth and confidence\nGiving more openly and providing more feedback\nSharing responsibility and control\n4. Production (Performing)\nFeeling excited about participating in team activities\nWorking cooperatively and interdependently with peers and subgroups\nFeeling team strength\nFeeling positive about task successes\nPerforming at high levels\n5. Termination (Adjourning or Mourning)\nFeeling concern about impending dissolution\nFeeling sadness or gratification\nDecreasing or increasing task activity\nExperiencing a decrease or increase in morale\nFeeling exhilaration about accomplishments\nFeeling close to other members of the group\nMovement through the group development stages is NOT ALWAYS LINEAR.\nGroup Development Reflection Questions\nWhat are positive techniques I can use when a group is Storming--and we need to be Norming or Performing?\nWhat personal responsibility do I have as a group member to help other group members learn and grow?\nHow might I handle conflict or tension within a group?']"	['<urn:uuid:82a72d7a-85cf-4a9b-86cb-8a989480040d>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T03:34:55.284799	7	31	1499
42	comparison mummy sleeping bag vs quilt weight insulation temperature	Quilts are more compressible and lighter than mummy sleeping bags since they don't have zippers, hoods, and bottom insulation. Both types use either down or synthetic insulation. Down provides better warmth-to-weight ratio and compresses better, but loses insulation when wet unless treated. Synthetic insulation is heavier and less compressible but maintains warmth when wet. Both use temperature rating systems with comfort, transition/lower limit, and risk/extreme categories to indicate their warmth capabilities.	"['Learn Which Bag Rating You Need for Any Season\nAlan is our resident expert on all things wild camping. He loves nothing better than to get out of the house with just his ex-Army sleeping gear and a stove, and head off to the remotest spot he can find near his home on the south coast of the UK.\nA key factor to consider when choosing the best sleeping bag for backpacking or camping is its temperature rating.\nHowever, understanding sleeping bag temperature ratings can sometimes be a little tricky, and hard to know which temperature rating to choose, especially if you\'ll be wearing clothes inside the bag.\nIn this guide, we\'re going to explain all the details about sleeping bag temperature ratings and everything you need to know to help buy sleeping bags that have the right warmth level for any season.\nSleeping Bag Season Ratings\nSleeping bags are rated by season. Season rating starts from summer months (season one) to extremely cold seasons (season five).\nSeason one: ideal for summer camping, the temperature ranges from 5°C and above.\nSeason two: for late spring and early autumn with temperatures around 0-5°C.\nSeason three: autumn, early winter with no-frosts, and mid-cold nights. Temperatures between 0 to -5°C.\nSeason four: best suitable for cold winter nights with frost and high altitude, temperature as low as -10°C.\nSeason five: suitable for extremely cold, high altitude, and polar conditions. Temperatures as low as -40°C.\nTo understand sleeping bag ratings better, it will help to know how these bags are tested in the laboratory, the limitations in the process, and the terms used in rating the sleeping bags.\nLab-tested temperature ratings (EN and ISO) and non-EN/ISO are the most important things to consider when shopping for a sleeping bag.\nWhat Are Sleeping Bag Temperature Ratings?\nA sleeping bag temperature rating is a number by which sleeping bag manufacturers put on their sleeping bags to give people an idea about the best conditions for using a particular sleeping bag.\nIt basically gives you an idea of the temperature limit and comfort level a sleeping bag can be used in which makes understanding the EN/ISO ratings for sleeping bags important.\nThe reason for this rating was due to the lack of standard temperature ratings for sleeping bags because the fact that a manufacturer rated a bag for 15F doesn\'t mean that the bag could safely be used under such conditions.\nThis has also made it confusing for customers to buy a warmer bag since they have to compare other features like the sleeping bag materials, weight, and price.\nSo in 2005, the EN 13537 Standard rating for sleeping bags was introduced to help customers compare different sleeping bag brands. A new ISO standard was later introduced in 2017 to serve as an updated version of the EN standards, and as the new protocol for all sleeping bag ratings onward.\nAlthough there\'s a lot of science, technology, and testing involved in this rating, understanding how it works can help you choose the right sleeping bag for your adventures.\nThe Sleeping Bag Rating Test\nThe temperature rating system is quite complex, but you can download the standard (you have to pay) for more guidance regarding the testing procedure. However, here\'s is a quick version of the guide:\nFor testing purposes, the testing companies use a heated manikin that\'s covered in high-tech sensors instead of human beings.\nThe manikin is placed inside the sleeping bag on a foam sleeping pad and then placed inside a purpose-built cold chamber.\nEvaluators will then pay close attention to the data they get from the manikin\'s sensors. These data include:\n- The time it takes for the manikin\'s heat to accumulate in the sleeping bag\n- The temperature tog at which heat starts escaping from the sleeping bag\n- Whether the sleeping bag can maintain a relatively steady temperature level\n- The temperature at which the sleeping bag becomes ineffective\nThis information is then put into a specialized computer which spews out a series of numbers. The numbers are then interpreted by the testing professionals and also used to determine the sleeping bag rating.\nSleeping Bag Temperature Rating Systems\nIt is important to keep in mind that there\'s more than one number associated with the sleeping bag\'s temperature ratings.\nAlthough most sleeping bags are marketed to specific temperatures such as 20°F, these ratings are simplified for marketing purposes.\nThe temperature rating system gives a range for sleeping bags instead of a specific number and not all manufacturers use this rating. However, a sleeping bag that uses this rating will have EN13537 written on it next to the rating.\nThe ISO 23537 standard temperature ratings for sleeping bags is broken into three categories which are:\n- Comfort: this is the temperature at which a person can comfortably sleep inside the sleeping bag\n- Transition/lower limit: this is the highest temperature at which someone can sleep inside the bag with the zip open and their head and arms out. A temperature hotter than this will make a person start sweating and be unable to sleep\n- Risk/extreme: the minimum temperature at which a person would survive at 6 hours without abnormally low body temperature (hypothermia). It is not advisable to use sleeping bags under this condition unless in an emergency.\nOut of these ratings, the ""comfort"" level is probably the most useful sleeping bag ratings. You can read the level at which the sleeping bag is rated and make a decision based on how warm you want it to be.\nTypes Of Sleeping Bag Insulation\nSleeping bag insulation is the final thing to consider when buying a versatile sleeping bag. There are two types of sleeping insulation; down and synthetic sleeping bag insulation.\nDown Sleeping Bag Insulation\nThe down sleeping bag insulation has an excellent warmth to weight ratio and is a go-to choice for lightweight mountaineers and backpackers.\nThe insulation property of this type of sleeping bag insulation is severely limited once it is wet and can take a long time to dry out.\nIn recent years, the down sleeping bag insulation has seen a major advancement in water repellent capabilities by companies such as Down Tek.\nSo if you frequent wet or snowy environments and are longing for the lightweight and warmness offered by down sleeping bags, the Down Tek can be the best solution for you.\nSynthetic Sleeping Bag Insulation\nSynthetic sleeping bags are usually made from polyester, or a variant and work similar to down sleeping bags but are heavier and less compressible.\nThe heaviness and less compressible features of these bags make them less appealing to people who are trying to keep their gear small and lightweight.\nHowever, this bag can manage to maintain its insulation when wet, which is an advantage for some mountaineers, campers, and backpackers.\nHow Do Sleeping Bag Ratings Affect Your Comfort?\nNow that you have a solid understanding of sleeping bag ratings, you\'re probably wondering how it affects your camping experience.\nIt is important to note that these temperature ratings are not set in stone, but just some guidelines. They are designed to help you make informed decisions when buying sleeping bags, and it is not guaranteed that they will keep you perfectly warm at night.\nSo you should keep the following in mind when choosing a sleeping bag based on its temperature rating:\n1. What You Wear While Sleeping Matters\nThe temperature ratings don\'t take every possible situation into account, so they didn\'t consider what you wear while sleeping.\nFor example, someone wearing a thick jacket that has more fill power while they sleep will feel much warmer in cold weather conditions than someone who\'s wearing shorts.\nThis means that you can have a lot of control over how warm you are at night regardless of your sleeping bag temperature ratings.\n2. Your Sleeping Pad\nOne of the primary purposes of sleeping bags is to provide insulation from the ground, and your sleeping pad can be a factor in your sleeping comfort.\nYour sleeping pad can make a lot of difference in your sleeping comfort, and it doesn\'t matter how warm your sleeping bag is if your sleeping pad can\'t provide you with enough insulation from cold ground temperatures.\nHow Does Sleeping Bag Design Affect Warmth?\nAs I mentioned above, many factors contribute to the warmness of a sleeping bag apart from the temperature ratings. These factors can range from the sleeping bag shape, features, fill power, sleeping pad, and age of the bag.\nTo better understand this, let\'s first take a look at how a sleeping bag functions. It works just like a thermos; when a warm object (human body) is put inside the bag, it helps that object stay warm.\nIt is important to note that any extra space inside the bag will take our body more energy to heat up. This means that it is better to find a sleeping bag that fits you perfectly without leaving a ton of extra space. While the extra space can be great for comfort, it can make the bag feel drafty and colder than its actual temperature rating.\nA slim mummy-shaped sleeping bag that has a little room will weigh less and take less energy from our body to keep it warm. But it can be a little bit uncomfortable, so it is left for you to decide on the balance.\nAnother factor that contributes to warmth is the style of the bag. For example, sleeping quilts precede the traditional mummy bag for a minimalist blanket-style design.\nOne of the biggest drawbacks of sleeping quilts is; a hood instead of wrapping warmly around your neck, and also cuts off more in the neck/shoulder regions. Also, sleeping quilts don\'t usually make a complete circle around your body but are instead secured around your sleeping pad. Although these design features save weight, the drawback is that they sacrifice warmth.\nLastly, the age of your sleeping bags can also affect its warmth. An older sleeping bag will lose a certain amount of its warmth and down, which is unavoidable to some extent. However, you can minimize the effects by taking proper care of the bag and storing it in an open place rather than stuffing it in the stuff sack when not in use. Keeping it outside its stuff sack rather than having it compressed for a long period will let it breathe, which will help it to stay firm and healthier.\nHow Sleeping Pads Affect Sleeping Bag Warmth\nThe sleeping pad matters a lot more than you might think and can have a great impact on warmth. It doesn\'t only provide a comfortable sleeping surface, but also provides insulation with the freezing or cold ground.\nSleeping pads are measured in terms of warmth and Insulation and have a key metric (R-value) which ranges from less than 1 for s summer pad, and even up to 8 or higher for a winter pad.\nAll-season sleeping pads have R-values between 2 to 4 which is enough to provide solid insulation from most non-freezing grounds. And cold seasons require higher R-value sleeping pads.\nThe most important thing to remember is that you\'re putting together a sleeping system, this means that focusing only on temperature ratings alone and ignoring other key factors like R-value, fill power, and sleeping bag shape will not guarantee warmth.\nFinal Thoughts - Sleeping Bag Temperature Ratings\nBy now, you have a solid understanding of sleeping bag temperature ratings and the other factors that make a sleeping bag warmth.\nThe standard sleeping bag temperature rating is broken into 3 categories, comfort, transition/lower limit, and risk/extreme.\nIt is important to note that temperature rating alone doesn\'t guarantee warmth, but other factors such as the sleeping bag shape and your sleeping pad also come to play.\nFrequently Asked Questions about sleeping bags temperature ratings\nHow Warm Should My Sleeping Bag Be?\nHow warm your sleeping bag should be depends on the type of weather conditions you are camping in.\nIf you\'re camping in the middle of winter, you\'ll want a sleeping bag with a temperature rating of 10°, and more likely 0° or lower. However, a sleeping bag with a temperature rating of 20 degrees can get you through most of the year without discomfort.\nIdeally, you should think about how cold or hot it will be while you\'re on the trip and buy your sleeping bags accordingly.\nWhat Do The Degrees On Sleeping Bags Mean?\nThe degrees on sleeping bags indicate the lowest temperature at which a sleeping bag can be able to keep you warm (i.e., 20 degrees celsius should be able to keep you comfortable down to 20°C). However, there are no standard measurement ratings in the U.S., and that\'s why measurement ratings and their real-world meanings vary from one manufacturer to another.', 'When looking to strike the perfect balance between warmth and weight with your backpacking sleep system, quilts have some serious advantages over sleeping bags. More compressible than sleeping bags and boasting an excellent weight to warmth ratio, the best backpacking quilts will keep your pack light and still keep you warm enough to sleep comfortably in a hammock or tent.\nThe beauty of quilts is that they are incredibly versatile. You can open them up when too warm, and close them into mummy sleeping bags when temperatures plummet. With all these benefits, it’s no surprise that backpacking down quilts have become increasingly popular among backpackers, thru-hikers, and mountaineers.\nFor hot-sleepers, active sleepers, summer camping, warmer climates, and hammock camping, it doesn’t get better than a quilt. To help you find the best backpacking quilt, our reviews will present the best backpacking quilts on the market while the buying guide further down will explain the key features to pay attention to in order to pick the right quilt.\nHow To Choose A Backpacking Quilt – Buying Guide\nWeight and Packability\nWhen considering ultralight sleeping quilts for backpacking, weight plays a very important role in your decision. Since they don’t have zippers, hoods, and bottom insulation, camp quilts are lighter than mummy sleeping bags. Backpacking quilt weights range from slightly over 1 pound to 2 pounds. While the lightest option will be the most attractive option when you’re counting ounces, making a wise investment requires finding the right balance between quality, comfort, and weight.\nPackability is also an important consideration. Good backpacking quilts fold down and compress into a very compact package you can easily stow away in your backpack. To ensure the quilt you get won’t take too much space in your backpack, check the packed size or stuff sack dimensions. For ease of compressing, most quilts for camping come with a compression stuff sack.\nInsulation and Temperature Ratings\nIn terms of insulation, it’s a choice between a down backpacking quilt or synthetic insulation. Down is a superior option as it provides more insulation for its weight. Due to this, a down quilt is lighter than a synthetic quilt with the same temperature rating. Down also compresses better than synthetic insulation.\nWet weather is where synthetic insulation shines. Unlike down, it doesn’t lose its insulating properties when wet. However, the best down quilts feature water-resistant treatments that prevent down insulation from clumping. As for the fill power, the higher the number, the higher the warmth.\nAnother way to gauge whether a backpacking quilt will be able to keep you warm is to check its temperature ratings. The comfort temperature rating tells you the kinds of temperature conditions in which a sleeping quilt will keep you toasty and comfortable, while the lower limit is the survival rating.\nSince you will be using your quilt in the great outdoors, you need a quilt that won’t get wet when it rains or due to exposure to dew. The shell and liner fabric should be made of water-resistant nylon or polyester fabric.\nAs for the insulation, synthetic insulation is water-resistant and able to keep you warm even in wet conditions. If you opt for a down quilt, ensure the down insulation has a water resistance treatment so it will be able to keep you warm no matter the weather.\nMaterial and Durability\nCamping and backpacking are hard on gear. Your quilt should be tough enough to handle the rigors of outdoor adventures. Ripstop nylon and polyester with a high denier rating are the best materials for the shell and liner of a quilt. These materials exhibit impressive resistance to abrasion and don’t tear or puncture easily. The higher the denier rating, the thicker and more durable the material. A 20D rating strikes a good balance between weight and durability\nSize & Shape\nTo make sure the quilt you choose will be long and wide enough to accommodate you and still offer room for movement, check the length and width dimensions of the sleeping quilt when open. Most camping quilts offer regular, long, and short versions. As for shape, some quilts have a rectangular shape while others have a mummy shape. The ideal shape to go for comes down to your sleeping position and whether you sleep in a hammock or on the ground.\nDesign and Features\nFoot Box: The foot box is designed to keep your feet toasty warm. Most backpacking quilts have a drawstring at the bottom allowing you to open the foot box if your feet get too warm. Some quilt sleeping bags feature a closed foot box while others allow you to spread the quilt flat and use it as a blanket.\nPad Attachment: If you are going to be sleeping on a camping pad, make sure the sleeping quilt you choose has pad straps. These straps allow you to connect the camping pad and the quilt to ensure it stays in place throughout the night.\nDraft Collar: A draft collar helps stop drafts from getting in from where you have points of weakness, such as a zipper. This seals out the cold and traps the warmth in keeping you toasty warm.\nQ: What Is A Quilt?\nA quilt is a warm bed covering composed of a down or synthetic filling sandwiched between a top and bottom material. When it comes to camping, a quilt is a versatile sleeping system that can function as a mummy bag, an underquilt, or a top quilt blanket. Ultralight backpacking quilts have an excellent warmth to weight ratio that make them attractive for backpacking and hiking.\nQ: Why Choose A Camping Or Backpacking Quilt?\nWhen you you want to keep your pack light and compact and still have a warm and comfortable sleeping setup, a camping quilt is the ideal sleeping solution. By dispensing with zippers, hoods, and bottom insulation, backpacking quilts manage to be extremely lightweight while still providing the warm and comfort of a mummy sleeper bag. Another advantage of choosing a camping quilt is the versatility. You can cinch it up when temperatures dip, open it up if you get too warm, or spread it as a quilt blanket.\nQ: How To Clean And Store A Backpacking Quilt?\nThe best way to clean your backpacking quilt without damaging it is to follow the cleaning instructions provided by the manufacturer. We recommend handwashing or machine washing your quilt in a front-loading machine on delicate cycle. The proper way to store your quilt is in a breathable mesh sack. This will ensure it doesn’t lose its loft or get damp.\nGlobo Surf Overview\nWhen backpacking, hiking, or climbing, you want the best bang for every ounce. That is where the best quilts for backpacking shine. Synthetic or down quilts for backpacking provide the same warmth ratings as the best sleeping bags but at a fraction of the weight. Hopefully, this guide has been able to answer all of your questions so you are able to pick the best backpacking quilt for your needs.']"	['<urn:uuid:e002491e-ae52-4cfd-b819-d7c48838be52>', '<urn:uuid:944c3ad5-e63b-47ad-bccc-60f55c6ba6e6>']	factoid	direct	long-search-query	distant-from-document	three-doc	expert	2025-05-13T03:34:55.284799	9	71	3306
43	interested researcher curious earliest evidence when did continental plates begin moving on earth	According to research findings, plate tectonics may have started more than 4 billion years ago, during the first 500 million years of Earth's history. This is much earlier than previously believed. The evidence comes from analysis of ancient zircons found in Western Australia, which indicated heat flow patterns characteristic of subduction zones where tectonic plates converge.	"['A new picture of the early Earth is emerging, including the surprising finding that plate tectonics may have started more than 4 billion years ago — much earlier than scientists had believed, according to new research by UCLA geochemists reported Nov. 27 in the journal Nature.\n""We are proposing that there was plate-tectonic activity in the first 500 million years of Earth’s history,"" said geochemistry professor Mark Harrison, director of UCLA’s Institute of Geophysics and Planetary Physics and co-author of the Nature paper. ""We are reporting the first evidence of this phenomenon.""\n""Unlike the longstanding myth of a hellish, dry, desolate early Earth with no continents, it looks like as soon as the Earth formed, it fell into the same dynamic regime that continues today,"" Harrison said. ""Plate tectonics was inevitable, life was inevitable. In the early Earth, there appear to have been oceans; there could have been life — completely contradictory to the cartoonish story we had been telling ourselves.""\n""We’re revealing a new picture of what the early Earth might have looked like,"" said lead author Michelle Hopkins, a UCLA graduate student in Earth and space sciences. ""In high school, we are taught to see the Earth as a red, hellish, molten-lava Earth. Now we’re seeing a new picture, more like today, with continents, water, blue sky, blue ocean, much earlier than we thought.""\nThe Earth is 4.5 billion years old. Some scientists think plate tectonics – the geological phenomenon involving the movement of huge crustal plates that make up the Earth’s surface over the planet’s molten interior – started 3.5 billion years ago, others that it began even more recently than that.\nThe research by Harrison, Hopkins and Craig Manning, a UCLA professor of geology and geochemistry, is based on their analysis of ancient mineral grains known as zircons found inside molten rocks, or magmas, from Western Australia that are about 3 billion years old. Zircons are heavy, durable minerals related to the synthetic cubic zirconium used for imitation diamonds and costume jewelry. The zircons studied in the Australian rocks are about twice the thickness of a human hair.\nHopkins analyzed the zircons with UCLA’s high-resolution ion microprobe, an instrument that enables scientists to date and learn the exact composition of samples with enormous precision. The microprobe shoots a beam of ions, or charged atoms, at a sample, releasing from the sample its own ions, which are then analyzed in a mass spectrometer. Scientists can aim the beam of ions at specific microscopic areas of a sample and conduct a high-resolution isotope analysis of them without destroying the object.\n""The microprobe is the perfect tool for determining the age of the zircons,"" Harrison said.\nThe analysis determined that some of the zircons found in the magmas were more than 4 billion years old. They were also found to have been formed in a region with heat flow far lower than the global average at that time.\n""The global average heat flow in the Earth‘s first 500 million years was thought to be about 200 to 300 milliwatts per meter squared,"" Hopkins said. ""Our zircons are indicating a heat flow of just 75 milliwatts per meter squared – the figure one would expect to find in subduction zones, where two plates converge, with one moving underneath the other.""\n""The data we are reporting are from zircons from between 4 billion and 4.2 billion years ago,"" Harrison said. ""The evidence is indirect, but strong. We have assessed dozens of scenarios trying to imagine how to create magmas in a heat flow as low as we have found without plate tectonics, and nothing works; none of them explain the chemistry of the inclusions or the low melting temperature of the granites.""\nEvidence for water on Earth during the planet’s first 500 million years is now overwhelming, according to Harrison.\n""You don’t have plate tectonics on a dry planet,"" he said.\nStrong evidence for liquid water at or near the Earth’s surface 4.3 billion years ago was presented by Harrison and colleagues in a Jan. 11, 2001, cover story in Nature.\n""Five different lines of evidence now support that once radical hypothesis,"" Harrison said. ""The inclusions we found tell us the zircons grew in water-saturated magmas. We now observe a surprisingly low geothermal gradient, a low rate at which temperature increases in the Earth. The only mechanism that we recognize that is consistent with everything we see is that the formation of these zircons was at a plate-tectonic boundary. In addition, the chemistry of the inclusions in the zircons is characteristic of the two kinds of magmas today that we see at place-tectonic boundaries.""\n""We developed the view that plate tectonics was impossible in the early Earth,"" Harrison added. ""We have now made observations from the Hadean (the Earth’s earliest geological eon) — these little grains contain a record about the conditions under which they formed — and the zircons are telling us that they formed in a region with anomalously low heat flow. Where in the modern Earth do you have heat flow that is one-third of the global average, which is what we found in the zircons? There is only one place where you have heat flow that low in which magmas are forming: convergent plate-tectonic boundaries.""\nThree years ago, Harrison and his colleagues applied a technique to determine the temperature of ancient zircons.\n""We discovered the temperature at which these zircons formed was constant and very low,"" Harrison said. ""You can’t make a magma at any lower temperature than what we’re seeing in these zircons. You look at artists’ conceptions of the early Earth, with flying objects from outer space making large craters; that should make zircons hundreds of degrees centigrade hotter than the ones we see. The only way you can make zircons at the low temperature we see is if the melt is water-saturated. There had to be abundant water. That’s a big surprise because our longstanding conception of the early Earth is that it was dry.""']"	['<urn:uuid:2df9f643-e55c-40c9-bab2-501bf436b1c6>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-13T03:34:55.284799	13	56	1001
44	What are the benefits and risks of operating tractors on farms?	Tractors provide significant benefits to farming operations - they can do the work of 17 men and 50 horses, allowing farmers to plow, plant and harvest more acres with fewer workers and without needing animal rest breaks. However, tractors also pose serious risks - the noise makes communication difficult during operation, inexperienced operators can cause accidents, and they contribute to environmental issues like soil erosion and CO2 emissions that affect climate change. Additionally, many farmers still use old equipment which increases safety hazards.	['Unfortunately, a large number of dangerous incidents occur in the countryside each year. When doing farm work, every person should take extra care.\nIn theory, the larger the farm, the greater the risk of an accident. This is due to, among other things, a large number of agricultural machines, equipment, pens, farm rooms and potentially dangerous animals. Every farm owner or manager should continually perform or contract for preventive work to reduce the risk of dangerous situations.\nTransporting and operating farm machinery\nOne of the most dangerous activities performed in agriculture is the operation of machinery. This is often due to vehicles that are well past their prime. On many farms, farmers work with modern tractors and harvesters, but a large number of farmers still use old equipment. Why is it dangerous to work with machines and vehicles?\n- The first reason is that communication is much more difficult. Often when machinery is operating, the noise is so great that it is impossible to warn the operator of potential danger.\n- Driving farm vehicles and operating machinery is often done by inexperienced people. Before anyone begins this type of work, they should be trained and initially perform their duties under the supervision of someone more experienced.\n- It is unacceptable for minors to drive vehicles. Although educational campaigns have significantly reduced this phenomenon, such situations still occur. Children are often unaware of the dangers of operating heavy equipment and disregard health and safety rules.\nEvery owner or manager of a farm should fill in the survey available at this link http://www.bhpwrolnictwie.pl/pdf/lkdzn.pdf. You will find many interesting questions, especially about children.\nPreventive fire measures\nOne of the key issues on any farm, even a small farm, is fire prevention. Farm rooms are often constructed of varnished wood. If you add to this a large amount of dry hay, you get an extremely flammable combination.\n- An absolute staple is fire extinguishers located in an easily accessible place. It is good practice to choose large fire extinguishers, which are more effective than their smaller counterparts in the event of a fire. Fire extinguishers must also be located in rooms where animals are present.\n- An often overlooked issue is electrical wiring. It is imperative that all circuits be protected with so-called circuit breakers. In addition, every electrical box should be locked.\nStorage of dangerous substances and tools\nMany farms use crop protection products to increase crop yields, among other things. It is necessary to store them in a closed room where children and animals do not have access. It is not uncommon for these substances to be dangerous in high concentrations.\n- Another issue is the storage of various types of animal medicines, cleaning products and even grease and oil. All of these substances pose a real danger, so they cannot be kept in public areas.\n- Children should also not have access to dangerous tools that are relatively easy to hurt themselves with.\nChemicals are dangerous to both skin and eyes. They also often give off choking fumes. If swallowed, they can cause far-reaching health consequences.\nFirst aid on a farm\nAnyone who is permanently or periodically on a farm should be able to administer first aid. This also applies to children. First of all, everyone must know the emergency number 112 and learn how to briefly and honestly present a dangerous event. Such conversations can be carried out several times on a trial basis, so that everyone will know what to ask the employee of the dispatch center.\nMain photo of the article: source: Designed by FreepikLeave a comment', 'By Kristin Wypiszynski\nOne element of human life that has had an effect on our environment is agriculture. We depend on agriculture for our survival and are constantly trying to improve yields, fertilizer, pesticides, etc. Specifically, our advancement in agricultural machinery has allowed our species to increase our farming efficiency. One object that helps represent how humans impacted the environment is the tractor. Through the implementation of the tractor as common day farming machinery, the agricultural industry has forever changed and continues to expand our touch on the environment.\nFarming in Europe and America in the late nineteenth century was ruled by the technology of a horse drawn plow until the invention of the tractor. Horses were the power that pulled plows through the ground. However, new technology was expanding our means of attaining power. In 1892, the first gasoline-powered tractor was introduced to the state of Iowa (Trautmann). The original tractors were too expensive for the common day farmer to purchase. Not until the 1920s did tractors become immensely popular among the common day farmer (Crotty). The high cost in the original tractors was due to their immense size and use unneeded use of metals. One maker of a line of tractors was the company Ford and Son. Pictured above is one of the earliest Fordson tractors, Model F, which was largely popular in America in the 1920s (Crotty). The Fordson tractors were revolutionary because they conserved space and did not use as many parts as the initial larger tractors (Crotty). In the Fordson tractor, the engine, transmission, and axle housings were all fastened together to form the rudimentary structure of the tractor (Crotty). The less parts put onto the tractor, the less expensive it was, and therefore more affordable for the average farmer (Crotty). These tractors could do the work of 17 men and 50 horses resulting in a vastly changed industry.\nNow, farmers could do more work with less human labor, no animals, and not have to take breaks. In an interview with Herbert Heine, a farmer born in 1921 in Nebraska, he explains that tractors could hook up many different types of machineries so that they could “plow, plant, and harvest more acres with fewer workers” (Reinhardt). In addition, he discusses how with horses, they would tire out and would have to take a noon rest. With the addition of tractors, Herbert and his family had no need for a break and “[they] farmed more on account of it” (Reinhardt). Even though most farmers probably took breaks, the breaks lasted just as long as they needed to recover, not the animals. Another farmer born in 1912, Kenneth Jackson, explains how animals replaced by the machine were a large improvement. Instead of harvesting hay to keep the horses fed, a farmer could produce another crop in that field that he could sell, adding to his profit (Jackson).\nAn improvement on farming technology meant an increased profit for farmers, higher yields, and more cultivated land. These positive benefits were short lived in the 1920s because as the 30s rolled around, the Dust Bowl hit the Great Plains of American soil (Thompson). Tractors were not the sole source of the dust bowl. In reality, they only represent part of the responsibility (Thompson). First, there were a few years of draught in the early 1930s that started the cycle (Thompson). The Great Plains are an area with little rain fall, light top soil, and high winds (Thompson). The grass on the land was used a sort of anchor when the strong winds would blow (Thompson). The grass roots were strong enough to keep all dirt and plants on the ground. After cultivation of these areas, winds easily picked up lose topsoil and would result in “Black Blizzards,” large dust clouds (Thompson). The dust blew across the country and covered farms everywhere, destroying many of the crops (Thompson). A major contributor to the Dust Bowl was the farming techniques used in the 19th and early 20th centuries (Thompson). During WWI, farmers were enticed by the raising prices of grain and sought to plow new lands that happened to be covered with natural grass (Thompson). The advanced technology of the tractor allowed them to increase this process at a much higher rate, contributing to the amount of devastation that the Dust Bowl had brought (Thompson).\nIn response to the devastation of the Dust Bowl, in 1935 Franklin D. Roosevelt’s New Deal organized the Soil Conservation Service (Trautmann). This program was responsible for teacher farmer new agricultural techniques that would help reclaim the damaged land (Trautmann). Some of these methods included contour plowing, terracing, and strip-cropping in an attempt to reduce runoff and erosion (Trautmann). Also, Wind Breaks were planted along fields in order to stop the strong winds from picking up the topsoil (Trautmann). Lastly, this program taught farmers different tillage methods that helped reduce exposed soils (Trautmann). This effort did help and fields were used again, but they were used for the human benefit of agricultural gains, not restored to prairieland.\nThe alteration of soils by the use of tractors had, has, and will continue to effect the environment (Trautmann). One major consequence from tractor plowing is soil erosion. The process of erosion removes topsoils that are ample in organic matter and plant nutrients (Trautmann). Tractors contribute to the process by plowing up land and removing topsoil. Erosion causes depletion of the organic matter helps retain water and nutrients in the root zone (Trautmann). In addition, eroded soil negatively affects water reservoirs when it becomes runoff (Trautmann). Streams, rivers and lakes are clogged by eroded soil and this then increases flooding and destruction of habitation for fish and other water life (Trautmann). It would be very helpful if topsoils were not hard to reproduce. However, it takes up to 300 years for natural creation of topsoil (Trautmann). Therefore, when the topsoil is removed, it is basically irreplaceable. Erosion does vary from field to field (Trautmann). Depending on the soil type, slope of the field, the fields’ drainage patterns and crop management practices, erosion may be very common or very rare in a field (Trautmann). Globally, America has ten times the amount of soil erosion as is naturally replenished (Trautmann).\nTractors also contribute to climate change (Stein). Tractor engines emit CO2 that contributes to the rising temperatures of our world that then has its own impact on the environment (Stein). Not all of climate change is in result to the use of tractors, but they are a contributor to our world’s rising temperatures. Some climate change can work out positively by bringing milder winters to areas with harsh ones and longer growing seasons in areas in need of such (Stein). However, mainly climate change results in negative effects (Stein). In these cases, the poor are typically the victims of this harm because they are those who lack the resources to properly respond to climate change (Stein). For example, one of the most poor nations, Bangladesh, is projected to lose 17.5% of its land is the seal level rises to about 1 meter higher (Stein). If this happens, millions of people will be displaced in Bangladesh (Stein). In addition, many islands in the Indian Ocean and South Pacific may get swallowed up completely if the sea level is to rise (Stein).\nAgriculture is a necessity of life for humans. There is no way that it could be cut out of our lives and result in no negative repercussions. Farming technology, such as the tractor, has enabled our species to exploit the land at a faster rate than ever before. If not regulated, disasters such as the occurrence of the dust bowl rake our community. Using fewer tractors is not the only way to cut back on CO2 emissions, but it would help the process. Proper farming techniques help a great deal in the cut back of erosion but the only way that CO2 emissions can cut back are if we use tractors less. This object is a key resource in common day agriculture that we cannot afford to get rid of and yet our environment then has to take the toll. The Anthropocene is demonstrated through this man-made object, its exploitation of the land, and the way it affects our environment.\nCrotty, D. (2011) The Fordson Tractor in Museum Victoria Collections\nAccessed 19 April 2016\n2 Primary Sources:\nKenneth Jackson – Starting a 1920s Tractor. By Bill Ganzel and Claudia\nReinhardt. Perf. Kenneth Jackson. Farming in the 1920s. Transcript.\nTrading Horses for Tractors. By Claudia Reinhardt. Perf. Herbert Heine.\nWessels, Living History Farm – York, Nebraska. Web. 19 Apr. 2016. <http://www.livinghistoryfarm.org/farminginthe20s/machines_01.htm>.\nCrotty, D. (2011) The Fordson Tractor in Museum Victoria Collections\nAccessed 19 April 2016\n“Ford Tractors.” Ford Tractors and the Ford Farm Tractors History. Web. 19 Apr.\nParton, William J., Stephen J. Del Grosso, Ernie Marx, and Amy L. Swan.\n“Agriculture’s Role in Cutting Greenhouse Gas Emissions.” Issues in\nScience and Technology 27, no. 4 (Summer 2011).\nReinhardt, Claudia, and Bill Ganzel. “1920s Machines – Tractors.” 1920s\nMachines – Tractors. Web. 19 Apr. 2016.\nStein, Mary L., “The National Academies.” The Cost of Energy, Environmental Impact. The\nNational Academy of Sciences, 2016. Web. 19 Apr. 2016. <http://needtoknow.nas.edu/energy/energy-costs/environmental/>.\nThompson, Henry J., History.com Staff. “Dust Bowl.” History.com. A&E Television\nNetworks, 2009.Web.19 Apr. 2016. <http://www.history.com/topics/dust-bowl>.\nTrautmann, Nancy M., Keith S. Porter, and Robert J. Wagenet. “Modern\nAgriculture:Its Effects on the Environment.” NATURAL RESOURCES\nCORNELL COOPERATIVE EXTENSION (2012). Pesticide Safety Education Program. Cornell Cooperative Extension. Web. 04 Apr. 2016.']	['<urn:uuid:c392f53f-339f-47f5-b88b-d5667e19c4b0>', '<urn:uuid:b0220a78-a088-41dc-a873-c239c8ea057f>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T03:34:55.284799	11	83	2182
45	celiac disease symptoms gluten free cost	Celiac disease can manifest through 200 known symptoms, including abdominal pain, bone or joint pain, canker sores, numbness in hands and feet, and dermatitis herpetiformis (an itchy skin rash). While following a strict gluten-free diet is necessary for those with celiac disease, it proves to be expensive. Not only are gluten-free alternatives costlier, but the diet is also challenging to maintain due to the widespread presence of gluten in common foods and the risk of cross-contamination. Even naturally gluten-free items like nuts or lentils can be cross-contaminated during processing, requiring careful attention to food selection and preparation.	"['Celiac Disease Diet: What You Can and Can\'t Eat With Celiac Disease\nIn case you haven\'t noticed, supermarket shelves and restaurant menus are filling up with more and more gluten-free items, from chickpea pasta to cauliflower pizza crust. And while a gluten-free diet has become undeniably trendy, for those with celiac disease, it\'s not a lifestyle choice—it\'s a necessity.\nThe cause of celiac disease is unknown, and it can develop at any age. It\'s estimated to affect 1 in 10 people across the world today, according to The Celiac Disease Foundation (CDF). The effects can range vastly. In fact, there are 200 known symptoms of celiac, which can occur in the digestive system or other parts of the body.\nCoping with celiac disease requires a lifelong commitment to avoiding gluten in all forms. Unfortunately, this group of proteins is hiding in many popular foods, making it difficult to discern what\'s safe to eat. Wondering how to recognize gluten on a nutrition label? Not sure which grains you\'re supposed to stay away from? Looking for some celiac-friendly alternatives to some of your favorite foods? Whether you or a loved one in your household is adjusting to this diet following a celiac disease diagnosis, or you\'re simply curious about the difference between celiac and gluten intolerance, read on to find out the answers to all your burning questions about this common disorder.\nWhat is celiac disease?\nAccording to the CDF, celiac disease is an autoimmune disease that is detrimental to the lining of the small intestine when gluten—a protein found in wheat, barley, and rye—is ingested.\nWhen someone with celiac disease consumes gluten, their body instantly triggers an immune response that attacks the small intestine, thus hindering nutrient absorption.\nAmanda A. Kostro Miller, an RD and LDN who serves on the advisory board for Smart Healthy Living, notes that common symptoms of celiac disease include:\n- abdominal pain\nHowever, the CDF reports that adults with celiac disease may or may not have digestive symptoms, and can experience other symptoms as well. These can include:\n- bone or joint pain\n- canker sores in the mouth\n- numbness or pain in the hands and feet\n- dermatitis herpetiformis (an itchy skin rash)\nAccording to Miller, your doctor can test for celiac disease via an analysis of antibodies and/or a biopsy of the small intestine.\nWhat\'s the difference between celiac disease and gluten intolerance?\nBoth celiac disease and gluten intolerance involve an adverse reaction to gluten. However, it\'s important to distinguish between the two. The former entails an autoimmune response to gluten, the latter is a condition that results from an inability to properly metabolize and absorb the protein.\nIt has long been assumed that people with gluten sensitivity do not experience the same intestinal damage as those with celiac disease. However, a 2016 study conducted by Columbia University Medical Center demonstrated that when individuals with gluten sensitivity (who never tested positive for celiac disease) were put on a diet that included wheat, some did experience a certain degree of gut cell damage.\n""People\'s reactions to gluten can range from minor to severe,"" says Miller. ""If you have a gluten intolerance, you may find that you can tolerate a certain amount of gluten and/or you can tolerate wheat/barley/rye as ingredients in certain foods. However, those with celiac should avoid all foods that contain gluten and/or ingredients that contain gluten.""\nFor this reason, board-certified endocrinologist Dr. Anis Rehman recommends that anyone who has been newly diagnosed with either celiac disease or non-celiac gluten sensitivity seek out detailed medical counseling with a dietitian.\nUnlike celiac disease, Miller says there is no test for gluten intolerance. That\'s why she advises seeing a registered dietitian if you suspect you have this condition—as they may recommend an elimination diet to confirm that gluten is the culprit of your symptoms.\nRELATED: Your guide to the anti-inflammatory diet that heals your gut, slows the signs of aging, and helps you lose weight.\nFoods you can eat on a gluten-free diet\nDespite needing to avoid gluten, there are many foods you can still enjoy if you\'ve been diagnosed with celiac disease. In fact, some foods, like fresh and frozen produce, legumes, and dairy products are naturally gluten-free.\nAccording to Andres Ayesta, RD, LD, and founder of Vive Nutrition, as well as Monica Auslander Moreno, RD, LD/N, and nutrition consultant for RSP Nutrition, here are some of the core foods that are safe to eat on a gluten-free diet:\n- Fresh, frozen, canned or dried fruit (with no gluten-containing additives)\n- Fresh, canned, or frozen vegetables (with no gluten-containing additives)\n- Meat, poultry, and seafood (as long as they\'re not breaded or dipped in flour)\n- Dairy products (except some flavored milks and yogurts)\n- Beans and legumes\n- Nuts and seeds\n- Certain grains (quinoa, millet, rice, amaranth, and teff)\n- Oils and vinegars\nYou\'ll likely have no issue finding a plethora of gluten-free crackers, pasta, bread, cereal, and much more on the supermarket shelves. However, when it comes to any processed foods, you need to be extra diligent about checking labels to ensure the products are, in fact, free of gluten and have not been cross-contaminated.\nFoods you can\'t eat on a gluten-free diet\nIf you have celiac disease, it\'s imperative to stay away from gluten-containing foods—not only to avoid potential discomfort, but also to prevent damage to your GI tract.\nSo, it probably goes without saying that a slice from a traditional pizza joint or an Italian sub is off the table. But you\'ll also need to skip any bread, pasta, baked goods, or other products made with wheat, barley, rye, or triticale. That means croutons, breadcrumbs, and even seitan are a no-go. While fresh fruits and vegetables are fine, Miller warns that frozen and canned versions sometimes contain sauces or flavorings that feature gluten.\nHere are some other foods to avoid while on the celiac disease diet, according to Dr. Sashini Seeni, a general practitioner of medicine at DoctorOnCall:\n- Products containing wheat, barley, rye, or triticale\n- Processed meats (salami, sausage, hot dogs, etc.)\n- Processed cheeses\n- Flavored yogurts with gluten-containing preservatives or other additives\n- Semolina flour/couscous\n- Spelt flour\n- Graham flour\nThere are many other sneaky foods with gluten to be aware of, like soy sauce (which contains fermented wheat), pickles (which contain malt vinegar), and pudding (which contains wheat-based thickeners). According to Ayesta, condiments and seasonings often contain gluten as well. Even some canned soups and bottled salad dressings have wheat thickeners to give them a creamy texture.\nIt\'s critical to keep in mind that some foods that don\'t contain gluten are still at risk for cross-contamination. Oats, for example, are naturally gluten-free—but some are processed in facilities that also process wheat, barley, and rye. That\'s why it\'s a good idea to check the label to ensure the product has been processed in a gluten-free facility.\n""The list of potential ingredients with gluten is exhaustive,"" says Moreno. ""Celiac sufferers need to speak with their doctors about their particular sensitivity and needs.""\nWhen you\'re eating out, it\'s important to clearly communicate with your server about your need to avoid gluten. Be sure to specify that this need is due to celiac disease, not gluten sensitivity or a dietary choice. While certain dishes may appear safe, there could be gluten-containing additives in the sauces or seasonings that you don\'t know about. For instance, some restaurants add pancake batter (which is chock-full of gluten) to omelets in order to make them more fluffy.\n""In a restaurant, you may need to worry about gluten cross-contamination on cooking surfaces,"" adds Miller. ""Most foodservice operations cannot guarantee that they are gluten-free. Some people with severe gluten sensitivities may develop symptoms if food touches a surface that had flour or other gluten products on it. While I do not recommend being fearful of our food system if you have celiac disease, it is important to remain vigilant and voice your nutrition needs to those who cook for you so that they are aware.""\nFortunately, Ayesta notes that many kitchens now have gluten-free certifications, which indicate that they use practices to avoid gluten cross-contamination.\nHow to read food labels\nDetermining if a product contains gluten requires some practice. Although some products are specifically labeled ""gluten-free,"" the U.S. Food and Drug Administration doesn\'t require manufacturers to disclose gluten on food labels (only gluten-containing ingredients, like wheat).\nIf the packaging includes a ""gluten-free"" label, then you know that as per FDA guidelines, it includes less than 20 ppm (parts per million) of gluten. Still, not every gluten-free product has this label, so you may need to do some further investigating to assess whether it\'s safe to eat or not. Miller recommends checking the allergen warning section, which is usually located near the ingredients list, for wheat. Keep in mind, though, that a ""wheat-free"" label does not necessarily mean that a food is gluten-free, because it still may still include rye and barley. This is why it\'s advisable to scan the ingredients list for all forms of wheat, rye, barley/malt, and their derivatives. Corn flakes and rice puffs are considered gluten-free cereals, but they often contain malt extract/flavoring.\nKeep a lookout for hydrolyzed wheat protein, hydrolyzed wheat starch, wheat flour, bleached flour, bulgur, wheat germ oil or extract, and wheat or barley grass, all of which either contain gluten or may be cross-contaminated. Many of the thickeners added to soups, salad dressings, and sauces, have wheat in them, but guar gum, xanthan gum, and carob bean gum are all celiac-friendly alternatives.\n""Understand which products are at the highest risk for cross-contact with gluten,"" says Miller. ""Flours and grains, for example, have high levels of contact with gluten, so consumers should purchase flours and grains specifically labeled gluten-free.""\nOther ingredients that may potentially contain gluten include modified (food) starch, (hydrolyzed) vegetable protein, (hydrolyzed) plant protein, vegetable starch, dextrin, and maltodextrin. You should be cautious when you spot terms like ""natural flavoring"" or ""artificial flavoring"" in the ingredients as well, as these can sometimes be made from barley.\nWhen in doubt, always check with the manufacturer to clarify whether or not a product that includes any of these ingredients contains gluten, or could be cross-contaminated.\nGluten-free substitutions of popular foods\nJust because you have celiac disease doesn\'t mean you have to forfeit your beloved pastries or pizza. Fortunately for those with celiac disease, it\'s easier than ever to find safe substitutions for popular gluten-containing foods, including baking mixes, bread, crackers, cookies, and more.\nCraving a hearty plate of pasta? Seek out gluten-free products that are made from quinoa, rice, chickpea, or corn flour rather than wheat flour. Rice noodles and mung bean noodles are naturally gluten-free as well.\nCakes, cookies, and cereal that utilize almond meal or coconut flour, in place of wheat, rye, or barley, are also acceptable. Other gluten-free flour options include potato flour, pea flour, soy flour, arrowroot flour, tapioca flour, hemp flour, rice flour, sorghum flour, and buckwheat flour.\nWhen it\'s time for taco night, use corn tortillas or brown rice tortillas instead of flour ones.\nWhile there are many gluten-free crackers on the market, rice cakes are also a phenomenal option when you\'re seeking a crunchy accompaniment to cheese, gluten-free hummus, or salsa.\nMany pizzerias make gluten-free versions of their pies, but if they also use wheat flour in the same kitchen, it\'s a good idea to ask whether there\'s a possibility for cross-contamination. You can also make your own pizza at home with a cauliflower or spaghetti squash crust.\nAnd when a lazy weekend calls for a comforting batch of pancakes, all you have to do is swap out the wheat flour or all-purpose flour with cornmeal. As an added bonus, cornmeal contains more satiating protein than traditional flour.\nClearly, the celiac disease diet requires some strict restrictions and special considerations, but luckily, an increasing number of restaurants and food manufacturers are making it a point to ensure that there are tasty alternatives for those who need to avoid gluten. The more you can educate yourself on gluten-containing ingredients, the more easily and effectively you\'ll be able to read nutrition labels to make smarter, safer shopping choices.', 'For people with conditions such as celiac disease, avoiding gluten is crucial to health. However, sticking to a gluten-free diet is expensive, socially challenging and linked to nutritional inadequacies.\n- theconversation.com 1\nThere are many reasons why someone would follow a gluten-free diet. People who have a gluten-related disorder such as the autoimmune condition celiac disease, non-celiac gluten sensitivity or wheat allergy have no option but to completely exclude gluten in their diet.\nAvoiding gluten is vital to manage symptoms, which can include gastrointestinal distress, and to prevent adverse health outcomes. In the case of celiac disease these outcomes can include serious health issues such as nutritional deficiencies, weak bones and infertility.\nGluten-free diets have also been widely popularized as a fad or weight-loss diet. Claims that gluten-free diets promote weight loss are not supported by evidence.\n- Unfortunately, given the presence of gluten in common foods, adhering to a gluten-free diet is expensive, challenging and linked to nutritional inadequacies. Furthermore, obtaining a diagnosis for celiac disease or non-celiac gluten sensitivity in the first place is fraught with difficulty.\nWhat is gluten?\nGluten is a protein found in some grains, most commonly wheat but also barley, rye and triticale, (a hybrid crop of wheat and rye). Gluten is the component that gives your sourdough structure and stretch.\nGluten can also be an unexpected ingredient, such as in soy sauce, or even cosmetics and lotions. Cross contamination occurs when gluten-containing items come in contact with foods intended to be gluten-free, and can happen to nuts or lentils.\nWho follows a gluten-free diet in Canada?\nAs public health nutrition researchers, we were interested in exploring gluten avoidance in Canada, including determining who avoids gluten and their eating patterns. Using the 2015 Canadian Community Health Survey (CCHS), we found that 1.9 per cent of Canadians follow a gluten-free diet.\nThe 2015 Canadian Community Health Survey is the best and most current data we have about nutrition in Canada. However, the northern territories were excluded, as were Indigenous people living on reserves. Very little is known about gluten-related disorders among Indigenous Peoples, and it’s likely many remain undiagnosed.\nWe did not find a significant difference between the body mass index (BMI), a measure of weight for height, of people who avoid gluten and people who do not. This serves as further evidence against following a gluten-free diet for weight loss.\nComparing dietary gluten avoidance across the provinces, we found that Ontario and Québec had the lowest prevalence at 1.5 per cent. In comparison, the Atlantic provinces had the highest at 2.9 per cent. Even when taking into account other factors, such as ethnicity, income and education, living in Ontario or Québec was associated with lower likelihood of avoiding gluten.\nIt’s not clear what is driving these provincial variations. However, Ontario’s health plan does not cover the serological testing required for a celiac disease diagnosis outside of hospital. This is likely a major barrier for Ontarians in getting a diagnosis for celiac disease, which may be influencing the number of people following a gluten-free diet.\nChallenges of following a gluten-free diet\nWe found that Canadians who were following a gluten-free diet had a significantly lower intake of folate. Folate is an essential B vitamin that is particularly important for people who are of childbearing age and may become pregnant because it helps prevent neural tube defects.\nGluten-free diets were also found to be low in calcium and vitamin D, which are critical for maintaining bone health. This is particularly important for individuals with celiac disease and non-celiac gluten sensitivity who are at increased risk for bone mass loss.\nCanadians who avoid gluten were also more likely to avoid dairy, meat, eggs and fish/shellfish, which may further contribute to nutritional inadequacies. However, we did find that Canadians who avoid dietary gluten were more likely to take nutritional supplements than other Canadians.\nIn addition to the nutritional impact of a gluten-free diet, we reported associations with location of eating and food preparation. Gluten avoiders were significantly more likely to eat at home, compared to both people with no avoidances, and other dietary avoidances, such as being vegan or vegetarian.\nSpecifically, gluten avoiders consumed about one-third of the calories from restaurant-prepared foods as other Canadians. This is likely due to fear of cross contamination. Eating outside the home, for any reason, has been previously identified as a major concern for Canadians with celiac disease that can affect social activities and relationships, which contributes to a reduced quality of life.\nFollowing a gluten-free diet can have a variety of adverse effects on the individual, such as nutritional inadequacies, challenges eating outside the home and adverse impact on quality of life. However, our findings show that gluten-avoiders are incredibly adaptable. They are cooking at home, taking nutritional supplements, and coping with the social challenges. But there are some larger factors that would help to minimize these challenges.\n- First, improving access to gluten-free foods at restaurants, as well as improving the nutritional quality of gluten-free foods, will address some aspects of quality of life and nutritional inadequacies we have identified in Canada.\n- Second, improving access to serological testing for celiac disease in primary care in Ontario is essential.\n- Finally, we would like to put to rest the myth that eating gluten-free will aid in weight-loss. There is no evidence to support this claim.\nThese measures will help ensure Canadians are not avoiding gluten unnecessarily, and Canadians who do avoid gluten have access to nutritionally adequate food and the social benefits of eating at restaurants.']"	['<urn:uuid:b3eeb758-f930-4044-8936-c4e0dc8a73d4>', '<urn:uuid:bf03a99b-c026-473e-a703-8a8d12f6723d>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T03:34:55.284799	6	97	2963
46	educational benefits hands on learning school meals compare outdoor activities teaching methods student development ecosystem awareness	Both outdoor activities and school meal programs provide valuable hands-on learning experiences for students. In the Everglades trip example, students learned about ecosystems through direct experiences like kayaking, biking, and meeting with marine biologists to understand environmental impacts. Similarly, the Farm to School program provides hands-on learning opportunities through farm visits, cooking classes, and school gardens, teaching students about agriculture and food science. Both approaches help students develop environmental consciousness - whether through witnessing wildlife in their natural habitat or understanding how local food sourcing reduces environmental footprints. These experiential learning methods also build practical life skills, from safety awareness around wildlife to making informed choices about nutrition and healthy eating habits.	['The Phoenix School’s 6th-8th graders recently got back from the Florida Everglades and have spent the last two weeks going over their research and discussing what they had learned on the journey. Since the Kindergarten through fifth graders had also completed a similar imaginary journey, the two groups were able to share notes and discuss their findings.\nThere was much debate about who really had the better time.\nThere is no debate however that the actual trip to the Everglades itself was an exciting adventure for all involved, especially for two students James (7th grade) and Emma (6th grade). Throughout the week the students rode bikes through Shark Valley, canoed through the mangroves, explored wild Anhinga trails, navigated into Cypress domes on a slough slog, and kicked back at the Hoosville Hostel in Homestead.\n“The first night we were there, even before we went to the hostel, we stopped at Anhinga Trail,” James explained. “It’s this really beautiful space with a boardwalk where you can see Anhingas. Each student had a chance to walk to the ends of the boardwalk by themselves, which was really creepy because you could hear the little slaps of the tails of fishes on the water as you moved.\n“But every so often you would hear a much larger slap on the water, and those were the alligators,” he added.\nThe next day, the students met up with Christopher Kavanaugh, a marine biologist for the Everglades ecosystem who talked to the kids about the salinity of Florida Bay and the impact of boat passage through the delicate environment.\nAnother exciting part of that week was a 2.5-mile kayak trip in Flamingo right next to Florida Bay.\n“We didn’t see much on the way up there,” James remembered, “we were just kayaking. Then on the way back, the incredible things started to happen.\n“A manatee came up to our kayak and bumped it before swimming past underneath us. If it had been something else it would have been frightening, but manatees are so friendly and curious.”\nThe last adventure of the trip was a 15-mile bike loop through Shark Valley where students could see more flora and fauna both up close and from above at the observation tower halfway through the ride,\nEmma (6th grade) found the bike trip to be especially interesting because this was her first time doing something like this.\n“On the bike trip, I couldn’t believe how easy it was. I didn’t fall one time. Now I know what people mean when they say ‘It’s as easy as riding a bike,’” she said.\nDuring the trip, the students also had to be wary about accidentally running over the local wildlife.\n“At one point there was a large alligator just lying in the trail,” Emma recalled. “We just went around it. Fortunately, the alligator didn’t mind us at all, because if it did I don’t think I could’ve gotten my bike up to more 15 miles per hour which is how fast an alligator can run.”\n“Here pedestrians get the right of way, there, the alligators get the right of way,” James explained.\nAt Phoenix we always start Thanksgiving week by cooking, in multi age teams of students ranging from K-8th grade. These connections allow students to learn real life and valuable lessons from the variety of teachable moments. The Thanksgiving holiday reminds us of the importance of connection – not just with friends and family, but within our school. Phoenix is a family of students, alums, teachers, and friends. Our annual Thanksgiving Feast at school allows us to be able to celebrate together as one large family, connecting current students, with past generations of students.\nOur community works together, where teams have to learn collaboration, and older team leaders learn to guide their younger partners through different aspects of cooking, from cutting and peeling apples and potatoes to different measuring terms, like rounded teaspoons versus flat teaspoons. Did you know cooking involves a variety of sciences, math, discovery, taste, textures, team building, real life learning and so much more? Students learn fractions in action, and how they apply to real life situations. During cranberry sauce making, students actively observe the physical changes of cranberries while they are being heated and cooked, and from sour berries to sweet sauce! Even the simple act of cooking can bring feelings from joy to frustration, to exploding taste buds and smells!\nOlder students learn to lead, guide and share their knowledge, while engaging the younger grades, encouraging them to participate and teaching them how to do certain jobs safely. Younger students are able to fully participate from measuring, to cutting to cooking the food. Students have to learn how to read recipes accurately and follow directions, otherwise their creations might just not succeed! These are real life skills, at all grade levels, and they teach students patience, team building, collaboration, guiding, teaching, and knowing when to lead and when to allow others the spotlight and so much more. All of these skills will transfer into situations students will find themselves in throughout their whole life, from highschool, to college and to their careers.\nThe students spend two days preparing and cooking a Thanksgiving Feast for our school community. From cooking and cleaning to decorations and dessert all grades are actively involved with creating their feast. On our final day we redesign the school to create one long table for the students, a smaller table for our Alumni visitors and a teachers table. Students are able to learn the dining etiquette of a formal meal, but in a very kid friendly way! By gathering around the table, students are able to share stories of their family traditions and enjoy the feast that they were a part of creating.\nThis also opens the opportunity for our community to reflect on the things we are grateful for, to find gratitude and begin to think about setting new goals for following year. And while it is important to recognize the gift of giving to the community, it is also the time to show respect and indifference to each other. Students are given the opportunity to reflect, in a group sharing moment students comment on what they are thankful for. Kindergarteners are often thankful for something in their family, while the older students often reflect on life events or global events that are making them think on a deeper level. Being thankful for parents is a common theme, from being given the opportunity to go to a school they value to pushing them to grow outside their box, to providing them with a safe home.\nStudents keep our local shelter, Lifebridge, in mind and make extra food to donate for their Thanksgiving Feast the following day. Students here are used to giving, helping, volunteering and being thoughtful. From a young age students learn to be mindful and considerate of their peers, their community and to students around the globe. As active members of the EarlyAct Club of the Salem Rotary Club our K-8 students discuss, vote and implement community actions that involve volunteering or donations. The learn early that giving comes in many forms—time, energy, money, goods and services, and more. But all have something in common at their core: they are gifts offered without expectation or implication of repayment, only the desire to create a better future.\nHave a safe and grateful long weekend!\nThe Week of Thanksgiving', 'Farm to School: Sourcing School Meals Locally\nEducation is more than what happens in the classroom. Students are learning about the world they live in, how to interact with each other, and how to stay healthy. People are becoming more environmentally aware and conscious of what children put in their bodies, particularly as awareness about obesity has spread.\nSchool districts are starting to focus on something that impacts both student health and the environment they live in:\n- The nutritional content of their school food\n- The local content of their school food\nIn some cases, the availability of junk food at schools has made them co-conspirators in the development of unhealthy eating habits. Ignoring local food sources in favor of imported food only increases the environmental footprint of schools. Locally sourcing healthy lunch options for students is an excellent opportunity to develop nutritionally- and environmentally-conscious young citizens.\nWhat Is the Farm to School Movement?\nThe USDA’s Farm to School Program is an effort to bring local food into schools to help students educationally and nutritionally, as well as to support the local economy. The USDA distributes about $5 million in grants each year to help schools connect to local producers and to bring students hands-on learning opportunities, such as farm visits, cooking classes and school gardens. The food isn’t limited to fruits and vegetables — local producers can be farmers, ranchers or manufacturers.\nWhat is it?\n“Farm to School” is both a federal term for a program and one applied to a movement that includes all efforts to introduce locally or even regionally produced food into the schools. This includes agricultural produce and even meat and poultry that goes into the meals students eat in the cafeteria. It also encompasses various hands-on learning opportunities. It is all part of an integrated approach to learning that extends beyond the classroom.\nBenefits of the Farm to School Programs\nWhen schools join the Farm to School movement, they are making connections that will have long lasting effects. This applies to both the children at the school as well as the farmers who produce the food. When schools source local food they find that children benefit in a number of ways. Farm to School programs provide:\n- Fresher food that does not have to travel lengthy distances\n- An educational connection between food and nutrition\n- Healthy eating habits\n- Educational opportunity for students and their families to make informed choices about their food\n- A chance to learn more about the agricultural resources in their region\n- Teachers a chance to teach students about agriculture, food science and other related topics\n- A lesson in protecting the environment by decreasing a school’s environmental footprint\n- A chance to learn about small-scale farming if a school garden is incorporated\nMoreover, it is not only children and the education system that benefit. The local producers do as well. Research indicates that sourcing local or regional food helps growers, producers and others involved in the local food economy.\nSolving a Problem without Creating Any New Ones\nThe Farm to School movement is one that works toward solving several problems: decreasing childhood obesity by instilling healthy eating habits, helping the local economy and encouraging environmental consciousness in students. With the potential to support local food producers and inspire the minds of children, this program does more than fill bellies with healthy food!\nPhoto: Locally-sourced school lunch served at a Washington, D.C. middle school. Credit: DC Central Kitchen / Flickr']	['<urn:uuid:981acaa2-2aad-4e0a-af22-6efa6c57b52e>', '<urn:uuid:109b29de-71f0-49d2-b388-aeb158d75335>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T03:34:55.284799	16	112	1821
47	I'm dealing with some difficult situations and wondering - how do Reality Therapy and CBT compare in their approach to helping people make better choices?	Reality Therapy and CBT have different approaches to helping people make better choices. Reality Therapy focuses on evaluating current choices and taking responsibility for decisions in the present moment, even when the options aren't ideal, helping people recognize they have choices even in difficult situations. CBT, on the other hand, works by identifying and changing unhelpful ways of thinking and behaving, teaching concrete coping skills and tools that need to be practiced as homework. While both focus on the present, CBT specifically targets changing maladaptive thought patterns and behaviors through structured learning and practice, while Reality Therapy emphasizes evaluating and taking responsibility for the choices available to us.	"['Overview of Cognitive Behavioural Therapy (CBT)\nCognitive-behavioral therapy (CBT) is a form of psychotherapy that is often used to help first responders cope with stress and trauma. CBT is based on the idea that our thoughts, feelings, and behaviors are interconnected, and that changing our thoughts and behaviors can help us to better manage our emotions and stress.\nIn CBT, a therapist works with the first responder to identify negative or inaccurate thought patterns and behaviors that may be contributing to stress or anxiety. The therapist then helps the first responder to develop more positive and effective ways of thinking and coping.\nSome common techniques used in CBT include:\n- Cognitive restructuring: This involves identifying and challenging negative thought patterns and replacing them with more positive and realistic ones.\nExposure therapy: This involves gradually exposing the first responder to stressful or anxiety-provoking situations in a safe and controlled environment to help them overcome their fears.\nBehavioral activation: This involves helping the first responder to identify and engage in activities that bring them joy and a sense of accomplishment.\nRelaxation training: This involves teaching the first responder relaxation techniques such as deep breathing and progressive muscle relaxation to help them manage physical symptoms of stress and anxiety.\nCBT is often conducted over a series of sessions, and the first responder may be given ""homework"" assignments to practice the techniques they learn in therapy. CBT has been shown to be an effective treatment for a range of mental health conditions, including anxiety and depression, and can be a valuable tool for helping first responders to manage the stress and trauma they experience in their work.\nHow CBT can help manage stress\nCBT can be an effective tool for helping first responders to manage stress by targeting the underlying thoughts and behaviors that contribute to their stress and anxiety. Here are some ways that CBT can help:\n- Identifying and challenging negative thought patterns: First responders are often exposed to traumatic and stressful situations that can lead to negative thoughts and beliefs, such as ""I\'m not good enough"" or ""I\'m in danger."" In CBT, the therapist works with the first responder to identify these negative thought patterns and challenge them with more realistic and positive ones.\nDeveloping coping strategies: CBT can help first responders to develop effective coping strategies for managing stress and anxiety. This might involve learning relaxation techniques, developing problem-solving skills, or engaging in activities that bring a sense of joy and accomplishment.\nBuilding resilience: CBT can help first responders to build resilience by teaching them skills for managing difficult emotions and situations. This can include developing a sense of purpose and meaning in their work, building supportive relationships with colleagues and loved ones, and practicing self-care.\nImproving communication skills: First responders often work in high-pressure and rapidly changing situations that require effective communication skills. CBT can help first responders to develop better communication skills, such as active listening and assertiveness, which can reduce stress and improve their ability to manage challenging situations.\nOverall, CBT can help first responders to develop the skills and strategies they need to better manage the stress and trauma they experience in their work. By targeting the underlying thoughts and behaviours that contribute to stress, CBT can help first responders to build resilience, develop effective coping strategies, and improve their overall well-being.\nTechniques for challenging negative thought patterns\nChallenging negative thought patterns is an important component of cognitive-behavioural therapy (CBT) and can be a useful tool for managing stress for first responders. Here are some techniques for challenging negative thought patterns:\n- Identify the negative thought: The first step in challenging negative thought patterns is to become aware of them. For first responders, negative thoughts may include beliefs such as ""I can\'t handle this"" or ""I\'m not good enough.""\nChallenge the evidence: Once the negative thought has been identified, the first responder can examine the evidence for and against the thought. They can ask themselves questions such as ""Is this thought based on fact or assumption?"" and ""What evidence do I have to support or refute this thought?""\nConsider alternative explanations: The first responder can then try to come up with alternative explanations for the situation that are more balanced and realistic. For example, instead of thinking ""I\'m not good enough,"" they could reframe the thought as ""I\'m doing the best I can in a difficult situation.""\nReframe the thought: Once the first responder has identified alternative explanations, they can reframe the negative thought in a more positive and balanced way. For example, instead of thinking ""This situation is too stressful for me to handle,"" they could reframe the thought as ""This situation is challenging, but I have the skills and resources to manage it.""\nPractice, practice, practice: Challenging negative thought patterns takes practice, and it can be helpful to work with a therapist or counsellor to develop these skills. The more the first responder practices challenging negative thoughts, the easier it will become over time.\nBy using these techniques to challenge negative thought patterns, first responders can develop more realistic and positive ways of thinking about themselves and their work, which can help to reduce stress and improve overall well-being.\nStrategies for changing behaviour and improving mood\nThere are several strategies that first responders can use to change their behaviour and improve their mood. Here are some examples:\n- Set achievable goals: Setting achievable goals can help first responders to feel more in control and improve their mood. For example, setting a goal to complete a specific task within a certain timeframe can provide a sense of accomplishment and boost mood.\nPractice self-care: First responders often put others\' needs before their own, but taking care of oneself is essential for managing stress and improving mood. Engaging in activities such as exercise, mindfulness, and hobbies can help to reduce stress and improve mood.\nDevelop a support network: Building a support network of colleagues, family, and friends can provide emotional support and reduce feelings of isolation. Regularly connecting with others can help to improve mood and reduce stress.\nUse positive affirmations: Repeating positive affirmations can help first responders to challenge negative thoughts and beliefs. Examples of positive affirmations might include ""I am capable and competent"" or ""I am making a difference in people\'s lives.""\nEngage in problem-solving: Engaging in problem-solving can help first responders to feel more in control and improve their mood. For example, if a specific situation is causing stress, they can brainstorm solutions and create an action plan to address the problem.\nPractice relaxation techniques: Relaxation techniques such as deep breathing, progressive muscle relaxation, and visualization can help to reduce stress and improve mood. These techniques can be practiced regularly, even during brief breaks throughout the day.\nBy using these strategies to change behaviour and improve mood, first responders can develop a greater sense of control over their work and improve their overall well-being. It\'s important to note that seeking support from a therapist or counsellor can also be helpful in developing these strategies and addressing underlying mental health concerns.', 'What is Solution Focused Therapy?\nWhen we work from solution-focused therapy, we focus on the here and now and what we need to do to reach our goals. Moreover, we are working together on finding your solutions instead of exploring and reinforcing your problems. This approach is very practical and can help clients find their solutions quickly. Suppose you have been in therapy before and are used to telling your story repeatedly and relieving these problematic experiences. In that case, this will feel very different and uncomfortable. This intervention aims not to focus on the problem and to focus on your solutions to these problems.\nWhat is Reality Therapy?\nWhen we work from reality therapy, we focus on the here and now and the choices we have in front of us. Often, we are in difficult situations with and we feel as if we have “no choice” therefore, we feel trapped, depressed, anxious, angry, powerless, and helpless. In reality, we have choices, the choices before us may not be great choices or the best choices we want to make, yet we have found ourselves having to choose from the lesser of two options. Therefore, we will evaluate how you see the goals you have achieved and those you have not, deciding if your goals are realistic and ready and willing to make a change to accomplish this goal? This theory’s foundation is taking responsibility for the choices in front of us and choosing more practical actions to help us. Again, this is not an approach to explore the past in detail. Instead, we want to understand where we are right now and what we need to do to reach your goals.\nWhat is Cognitive Behavioral Therapy (CBT)?\nWhen we work from cognitive-behavioral therapy (CBT), we are exploring what is going on right now, sometimes requiring us to understand the past better, so we can be healthier for the future. Often, we struggle with depression, anxiety, substance abuse, eating disorders, trauma, relationship issues, and familial problems. We experience many problems because we have unhelpful ways of thinking or seeing a situation or learned maladaptive (bad) coping skills, behaviors, and patterns to these problems.\nOur goal in using CBT is to change your unhelpful ways of thinking, feeling, and behaving to help you learn useful coping skills for these complex problems. This theory allows you to take concrete steps to change what you are currently feeling, thinking, and doing that is not working and replace it with more valuable skills to reduce or solve the problematic behaviors.\nThis intervention requires learning and reviewing the coping skills, tools, and steps for you to take then to leave our session and practice these tools outside of the counseling session as “homework.” If you do not practice using these new skills outside the session, you will likely not see any meaningful positive results.\nIn order to proceed with booking an appointment, you will need to fill out and sign the online fillable PDF forms listed below. Once you have completed and submitted the forms to our HIPAA approved and encrypted email address, we will process your documents and get in touch with you within 24 business hours.\nProceed with Booking\nIf You are In A Crisis, Get Help Immediately!\nIf you’re in pain, struggling, and needing immediate help then call 911, go to the nearest emergency department, call lifeline at 1-800-273-8255, or chat through their Lifeline Chat services. Additional information can be found on their website www.suicidepreventionlifeline.org\nIf, for unseen reasons, you do not hear from me or I am unable to reach you, and you feel you cannot wait for a return call or feel unable to keep yourself safe, please go to your local hospital Emergency Room or call 911. You can also call the National Suicide Prevention Lifeline: 1-800-273-8255 and ask to speak to the mental health worker on call. I will make every attempt to inform you in advance of planned absences and provide you with the name and phone number of the mental health professional covering my practice.\nThe National Suicide Prevention Lifeline is a national network of local crisis centers that provides free and confidential emotional support to people in suicidal or emotional distress 24 hours a day, seven days a week. They are committed to improving crisis services and advancing suicide prevention by empowering individuals, promoting professional best practices, and building awareness https://suicidepreventionlifeline.org.\nPresident Signs National Suicide Prevention Designation Act Into Law.\nThe President recently signed the National Suicide Hotline Designation Act into law. 988, the new three-digit number for the National Suicide Prevention Lifeline, is to be completed by July 2022. In the meantime, please continue to share 1-800-273-TALK (8255) with anyone wishing to connect to the Lifeline. 988 is NOT CURRENTLY ACTIVE nationally and may not connect callers to the Lifeline. https://www.vibrant.org/president-signs-988-into-law']"	['<urn:uuid:dca1a553-8309-4f52-82dd-932e3095846b>', '<urn:uuid:759c7019-9f8b-4e23-b4d0-2834181d1fc0>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	novice	2025-05-13T03:34:55.284799	25	108	1982
48	how can conservation tourism and electric vehicles both help reduce carbon emissions and protect environment	Both conservation tourism and electric vehicles contribute significantly to environmental protection. Tourism, when done through partnerships like those at Chilo Gorge Safari Lodge, helps protect ecosystems and supports local communities in conservation efforts. Electric vehicles help reduce carbon emissions and air pollution since they don't produce tailpipe emissions, making them significantly more environment-friendly than gasoline-powered vehicles. Additionally, EVs have lower operating costs due to reduced reliance on fossil fuels, while tourism revenue creates incentives for communities to conserve natural resources through the principles of Conservation, Communities, and Commerce.	"['Interview with the Award-Winning Conservationist Pioneer, Clive Stockil\nWe speak to this legendary conservationist about winning the Tusk Lifetime Achievement Award for Conservation, what the key is to successful conservation in Zimbabwe and in Africa, his greatest successes and setbacks, and how he was influenced as a young boy by his Changana peers.\nGrowing up in Zimbabwe, who and what would you say had a hand in you becoming a guide and conservationist?\nI consider it a privilege to have been born and raised in this truly wild environment.\nMy Changana peers were my constant companions, whose understanding of the diverse fauna and flora created an insatiable interest in my young mind.\nTheir ability to read paths and landscapes like most of us read the daily newspaper, to interpret who had passed the night before, who was the hunter and who became the meal, was phenomenal.\nMy mentor, barefoot professor, was a Changana friend called Musisinyani Thomas Chauke, a true naturalist whose knowledge and passion for the wild natural world was contagious. I consider it an honor and privilege to have spent many days and nights in the bush with him, learning and understanding basic bush law. This in turn led me to pursue an interest in conservation through local cultures, recognizing that in the final analysis, success will come down to a balance between humans and wildlife. The greatest challenge will be for space.\nHow did it feel collecting the Tusk Lifetime Achievement award?\nIt was truly a great honor and privilege to receive the Tusk Conservation Award at this prestigious event held in London on the 13th September 2013.\nIt was at this historical event that I receive the inaugural “Prince William Award for Conservation in Africa” In recognition of a life time commitment, kindly supported by Investec Asset Management.\nIt was humbling, challenging, encouraging. It was inspirational; it gave hope not only to me but to all who I have had the privilege of working with over many years.\nWhat have been your greatest successes and setbacks in the fight for conservation?\nSome of my greatest successes I count are turning conflict in to cooperation between the Mahenye Community and the Gonarezhou National Park, participating in the process of pioneering a concept of co-existence between people and wildlife. Also the restoration of 350,000 hectares of degraded land, changing land use, restoring natural eco-systems and the restocking of the Save Valley Conservancy with all indigenous large mammal species including 600 elephants 60 Rhino (both black and white) and many other plains game species.\nThe greatest challenges I’ve faced have been the integration of the Save Valley Conservancy as an acceptable form of land use in to the Land reform program implemented by the Government of Zimbabwe in 2000. This once again underscores the competition for space. To achieve sustainable conservation communities need to be included. Wildlife tourism needs to contribute to sustainable development for neighbouring communities.\nDo you still enjoy guiding guests and tracking wildlife with them in the Save Valley Conservancy and Gonarezhou?\nIt is always a pleasure meeting and sharing some of these unique wilderness areas with those that enjoy and appreciate the true values of our natural heritage and biodiversity.\nWatch the Chilo Gorge Safari Lodge and Clive’s inspiring video on how conservation starts with the community:\nYou say people come first in conservation, can you give us some examples of where this has worked?\nA good example is the Mahenye Community Project. This evolved out of conflict between a local Changana community and the Gonarezhou National Park over natural resource use and competition for space. The concepts that emerged out of this conflict later became the core principals incorporated in the CAMPFIRE programme, which now has seen over forty other districts in Zimbabwe implement this programme. Indeed many of the concepts have been copied or modified by other southern African countries. Whilst there will always be new challenges we need to appreciate that this is evolution in progress in the quest of achieving sustainable development. What has been recognised is that this experiment turned conflict into cooperation for the benefit of both man and beast through coexistence in a fragile environment.\nAnother is Chilo Gorge Safari Lodge, which is an example of a partnership between the Mahenye Community and the private sector, this partnership has seen the development of a school, assisting in the maintenance of the local clinic, and employs 34 members of the community in the lodge, this has a direct benefit of contributing to the local cash flow through the monthly wages paid to staff. This partnership has recently undertaken a ground breaking project with the support of the European Union to establish the first community conservancy, where the community has decided to create the Jamanda Wilderness, an area of approximately 7000 hectares which shares a 10 kilometer boundary with the Gonarezhou National Park. This will see the community benefiting directly from wise land use and conservation through tourism.\nCan you tell us about the CAMPFIRE program?\nCAMPFIRE – stands for “Communal Areas Management Program For Indigenous Resources”. This created an opportunity to empower communities and to involve them in taking responsibility in managing their natural resources through sustainable use for the benefit of the community at large whilst ensuring the protection of the environment.\nThe principals are sound, there have been many successes, the danger is, where there is money and power higher political structures could possibly hi-jack this program by imposing levies and taxes which would reduce the incentive to the producer communities.\nWhat can Jacada guests do to contribute to conservation whilst staying at Chilo Gorge Safari Lodge? And in a wider sense, how do you think tourism to Africa can do more to help conserve its wildlife and support local communities?\nFirst and foremost guest contribution is through choosing to stay at Chilo Gorge Lodge ; a higher occupancy means more benefits to the community.\nWhere aesthetic values are not understood by many cultures, where the daily struggle is providing for the family needs, and improving livelihoods is the focus of the community leadership.\nAesthetic values are considered a luxury, if we want success in protecting some of these unique ecosystems and landscapes then we all need to consider the basic 3C principals:\nConservation – Communities – Commerce\nCommerce through tourism will contribute to creating the necessary incentives to communities to conserve global assets, this is achieved through partnerships which includes the tourist. The progressive tourist should research and chose lodges where these smart partnerships continue to contribute to the maintenance of our delicate ecosystems whilst understanding the fine line between human needs and the conservation of the environment for the benefit of future generations.\nAnd finally, do you have a favourite animal or bird of the African bush?\nThis regrettably is a question I can’t answer, with over 50 mammal and 450 bird species, added to an unknown number of insects, reptiles and aquatic life, all are special in their own way, the more one tries to understands each species the more you realize how unique and different they are, and how little we know about them. The role each species plays in maintaining ecological balances is what is special.\nWe can arrange for Jacada guests, who request it in advance, to be guided personally by Clive during their stay at Chilo Gorge Safari Lodge. Speak to our Africa team today for more details.', ""The Europe electric vehicles market is projected to exhibit a growth rate (CAGR) of 39.30% during 2023-2028. The increasing government support and incentives, stringent emission regulations, advancements in battery technology, rising awareness about environmental concerns and changing consumer attitudes, rapid urbanization and air quality concerns, and significant technological innovation and investment represent some of the key factors driving the market.\nAn electric vehicle (EV) is an automobile powered by electricity, utilizing one or more electric motors for propulsion instead of traditional internal combustion engines that rely on fossil fuels. These cutting-edge vehicles have gained substantial popularity in recent years due to their environmental benefits, improved technology, and increased accessibility. Electric vehicles operate on the principle of storing electrical energy in rechargeable batteries, which power the electric motor to generate the necessary torque and rotation for movement. The elimination of tailpipe emissions makes EVs significantly more environment-friendly than their gasoline-powered counterparts, contributing to reduced air pollution and a decreased carbon footprint. The range of electric vehicles varies depending on battery capacity and advancements in battery technology, however it has been steadily improving, making them more practical for everyday use. Charging infrastructure is also expanding rapidly, with numerous charging stations becoming available in urban areas, making it easier for EV owners to recharge their vehicles. Besides the environmental advantages, electric vehicles also offer lower operating costs due to the reduced reliance on expensive fossil fuels and simplified maintenance, as they have fewer moving parts than traditional vehicles. As governments and societies worldwide prioritize sustainability and seek ways to combat climate change, electric vehicles play a crucial role in shaping a cleaner and more sustainable transportation landscape for the future.\nEurope Electric Vehicles Market Trends:\nThe growth of EVs heavily relies on a robust charging infrastructure. European countries have been investing in the development of public charging networks, making it more convenient for EV owners to recharge their vehicles, especially in urban areas and along major highways. Additionally, the increasing awareness regarding environmental issues and the need to reduce carbon emissions has led to changing consumer attitudes toward EVs. Consumers are now opting for electric vehicles as a conscious choice to lower their carbon footprint and contribute more to a sustainable future. Other than this, Europe's growing urbanization has increased concerns about air quality and pollution in cities. Due to this, electric vehicles are becoming a popular choice among the masses as they offer a cleaner and quieter alternative to traditional internal combustion engine vehicles, addressing these environmental concerns. Besides this, the integration of electric vehicles into ride-hailing and car-sharing services has played a role in promoting their adoption. Many ride-hailing companies are increasingly incorporating electric vehicles into their fleets, exposing more people to the benefits of EVs. In line with this, the rise of electric vehicles has attracted significant investment from both established automakers and new players in the automotive industry. This surge in investment has led to the development of innovative EV models with improved features and functionalities, making them more appealing to consumers. In line with this, European governments have been proactive in supporting the transition to electric mobility. They offer various incentives such as financial subsidies, tax benefits, reduced registration fees, and access to bus lanes and toll-free roads for EV owners. These incentives aim to make electric vehicles more affordable and attractive to consumers, encouraging higher adoption rates. Furthermore, Europe has some of the most stringent emission regulations in the world. The European Union's CO2 emission targets for car manufacturers have put pressure on automakers to produce low or zero-emission vehicles. To avoid heavy fines, manufacturers are increasingly investing in electric vehicle production to meet these strict regulatory standards. Moreover, technological advancements in battery technology have improved the range and performance of electric vehicles. Lithium-ion batteries have become more efficient, offering longer driving ranges and faster charging times, addressing one of the major concerns of potential EV buyers.\nEurope Electric Vehicles Market Segmentation:\nIMARC Group provides an analysis of the key trends in each segment of the Europe electric vehicles market report, along with forecasts at the regional and country levels for 2023-2028. Our report has categorized the market based on component, charging type, propulsion type, and vehicle type.\n- Battery cells and packs\n- On-board charger\n- Fuel stack\nThe report has provided a detailed breakup and analysis of the market based on the component This includes battery cell and packs, on-board charger, and fuel-stack.\nCharging Type Insights:\n- Slow charging\n- Fast charging\nA detailed breakup and analysis of the market based on the charging type has also been provided in the report. This includes slow charging and fast charging.\nPropulsion Type Insights:\n- Battery electric vehicle (BEV)\n- Fuel cell electric vehicle (FCEV)\n- Plug-in hybrid vehicle (PHEV)\n- Hybrid electric vehicle (HEV)\nThe report has provided a detailed breakup and analysis of the market based on the propulsion type This includes battery electric vehicle (BEV), fuel cell electric vehicle (FCEV), plug-in hybrid vehicle (PHEV), and hybrid electric vehicle (HEV).\nVehicle Type Insights:\n- Passenger vehicles\n- Commercial vehicles\nA detailed breakup and analysis of the market based on the vehicle type has also been provided in the report. This includes passenger vehicles, commercial vehicles, and others.\n- United Kingdom\nThe report has also provided a comprehensive analysis of all the major regional markets, which include Germany, France, United Kingdom, Italy, Spain, and others.\nThe report has also provided a comprehensive analysis of the competitive landscape in the Europe electric vehicles market. Competitive analysis such as market structure, key player positioning, top winning strategies, competitive dashboard, and company evaluation quadrant has been covered in the report. Also, detailed profiles of all major companies have been provided.\nEurope Electric Vehicles Market Report Coverage:\n|Base Year of the Analysis\n| Historical Period\n|Scope of the Report\n||Exploration of Historical and Forecast Trends, Industry Catalysts and Challenges, Segment-Wise Historical and Predictive Market Assessment:\n- Charging Type\n- Propulsion Type\n- Vehicle Type\n||Battery Cell and Packs, On-Board Charger, Fuel-Stack\n|Charging Types Covered\n||Slow Charging, Fast Charging\n|Propulsion Types Covered\n||Battery Electric Vehicle (BEV), Fuel Cell Electric Vehicle (FCEV), Plug-In Hybrid Vehicle (PHEV), Hybrid Electric Vehicle (HEV)\n|Vehicle Types Covered\n||Passenger Vehicles, Commercial Vehicles, Others\n||Germany, France, United Kingdom, Italy, Spain, Others\n||10% Free Customization\n|Report Price and Purchase Option\n||Single User License: US$ 2699\nFive User License: US$ 3699\nCorporate License: US$ 4699\n|Post-Sale Analyst Support\n||PDF and Excel through Email (We can also provide the editable version of the report in PPT/Word format on special request)\nKey Questions Answered in This Report:\n- How has the Europe electric vehicle market performed so far and how will it perform in the coming years?\n- What has been the impact of COVID-19 on the Europe electric vehicle market?\n- What is the breakup of the Europe electric market on the basis of component?\n- What is the breakup of the Europe electric vehicle market on the basis of charging type?\n- What is the breakup of the Europe electric vehicle market on the basis of propulsion type?\n- What is the breakup of the Europe electric vehicle market on the basis of vehicle type?\n- What are the various stages in the value chain of the Europe electric vehicles market?\n- What are the key driving factors and challenges in the Europe electric vehicles market?\n- What is the structure of the Europe electric vehicles market and who are the key players?\n- What is the degree of competition in the Europe electric vehicles market?\nKey Benefits for Stakeholders:\n- IMARC’s report offers a comprehensive quantitative analysis of various market segments, historical and current market trends, market forecasts, and dynamics of the Europe electric vehicles market from 2017-2028.\n- The research study provides the latest information on the market drivers, challenges, and opportunities in the Europe electric vehicles market.\n- Porter's five forces analysis assist stakeholders in assessing the impact of new entrants, competitive rivalry, supplier power, buyer power, and the threat of substitution. It helps stakeholders to analyze the level of competition within the Europe electric vehicles industry and its attractiveness.\n- Competitive landscape allows stakeholders to understand their competitive environment and provides an insight into the current positions of key players in the market.""]"	['<urn:uuid:6ee5bb6b-9adf-421f-91a4-f3e674c9c6b4>', '<urn:uuid:b7467957-f944-4d46-8681-dc3eebb65bfb>']	open-ended	with-premise	long-search-query	similar-to-document	three-doc	novice	2025-05-13T03:34:55.284799	15	88	2615
49	moicano and judo thunder who fought most recently after entering ufc promotion	Abdul Razak Alhassan (Judo Thunder) fought more recently in the UFC, competing at UFC 228 in September 2018 where he defeated Niko Price. Moicano's mentioned UFC fight was against Rafael dos Anjos at UFC 272, but this was after his earlier UFC 271 fight against Alexander Hernandez.	"[""Renato “Moicano” Carneiro is one tough son of a gun. The UFC lightweight took a short-notice opportunity, and although he tasted defeat, many new fans were gained.\nUFC 272’s co-main event this past weekend took a hit when the original opponent for Rafael dos Anjos, Rafael Fiziev, tested positive for COVID-19 at the start of fight week. Many names stepped up, but in the end, it was Moicano who got the opportunity.\nHeading into the fifth and final round, some serious swelling began developing around Moicano’s left eye which led to the doctors entering the octagon to assess the damage. Referee Marc Goddard gave Moicano 30 seconds into the round to prove he was fine otherwise the fight would be stopped. That warning served its purpose as Moicano rallied to win the round on two of the three judges' scorecards.\nDays after the relatively one-sided unanimous decision defeat, Moicano is feeling good and dealing with no pain despite his battered appearance.\n“I’m really thankful the [ref] didn’t stop the fight because otherwise, I would not know what I’m made of,” Moicano said to TMZ Sports. “I always try to go forward, I always try to fight back, and for me, the referee did his job, it was a great job.”\nFor the 32-year-old Moicano, the loss acted as the fifth of his 22-fight career (16-5-1) and snapped a two-fight winning streak. In typical short-notice fill-ins, we’ll see fighters get right up off the couch to make the best of the situation. In Moicano’s case, he had fought just a month prior at UFC 271 when earning a second-round submission against Alexander Hernandez.\nWith all the circumstances and outcome in mind, Moicano would still do it all over again if he had to.\n“Yes, of course, I don’t regret it,” he said. “I knew I was not 100 percent for the fight because I was coming from a good training camp against Hernandez but it was for three rounds and I finished the guy. I flew straight to Brazil, and I was enjoying my vacation,” Moicano said. “You know, drink some beers, eat some steaks. But, the opportunity shows up, and when an opportunity like that shows up you have to give it a try, and I was ready to beat him. His game plan was better than mine.\n“He took me down, what I got mad about everything was that he dominated me on the ground, I’m mad about that. I’m really mad about that, I’m going to have to go to the gym and work on my jiu-jitsu, work on my wrestling. I was not expecting that but I knew I didn’t have the time for training but I still would take the fight.”\nDeal. Kayla Harrison says PFL promised they’d bring in big-name fighters after signing new contract.\n$$$. Joanna Jedrzejczyk still passionate about fighting but admits she makes more money outside the octagon.\nScolding. Stephen A. Smith wonders how Jorge Masvidal was “not ready for a wrestling match.”\nScared. Alex Pereira foresees Israel Adesanya running from potential rematch.\nBreak. Rafael dos Anjos explains reason for letting up against Renato Moicano at UFC 272.\nMMA Fighting’s Damon Martin speaks with Bellator featherweight champion A.J. McKee\nReaction to Kayla Harrison re-signing with PFL.\nThe Fight with Teddy Atlas: Chael Sonnen and Anthony Smith.\nKamaru Usman with Shannon Sharpe.\nFighter vs. Writer. A.J. McKee and Kevin Lee chat with MMA Fighting’s Damon Martin.\nSOCIAL MEDIA BOUILLABAISSE\nKhabib gazes on.\nNo. I’d be in Bellator fighting the taller chick in that pic. https://t.co/mpZsctR8s5— Kayla Harrison (@KaylaH) March 8, 2022\nA Funky novel.\nPumped to announce my book is coming out this year. When Corona hit I needed projects to keep myself busy so I started on this! Took longer than I anticipated, but it turned out great. Hope you enjoy reading about my journey as much as I enjoyed living it https://t.co/XbxyQDnvcF pic.twitter.com/7Ju7TDyE7P— Funky (@Benaskren) March 8, 2022\nhttps://t.co/hZZBYr8fnK pic.twitter.com/NxVHzDq5UW— Ariel Helwani (@arielhelwani) March 9, 2022\nFancy fighter Sanko.\nIt’s been a rejuvenating few weeks time to get back to work. Onwards. pic.twitter.com/McM2rZoCoX— Robert Whittaker (@robwhittakermma) March 8, 2022\nSupport for Kayla.\nYou can hate on Kayla Harrison all you'd like. But we're prizefighters. We fight for MONEY. There's no health plan or retirement fund for us.— Jay Perrin (@Joker_Gang35) March 8, 2022\nIf you could make a million a year you would in a heart beat. ♂️\nDiana Avsaragova (4-0) vs. Kyra Batara (8-4); Bellator 276, Mar. 12\nDamir Hadzovic (14-6) vs. Steve Garcia Jr. (12-4); UFC Vegas 52, Apr. 23\nIlima-Lei Macfarlane (11-1) vs. Justine Kish (7-5); Bellator 279, Apr. 23\nKyoji Horiguchi (29-4) vs. Patchy Mix (15-1); Bellator 279, Apr. 23\nSergio Pettis (22-5) vs. Raufeon Stotts (17-1); Bellator 279, Apr. 23\nCris Cyborg (25-2) vs. Arlene Blencowe (15-8); Bellator 279, Apr. 23\nLiang Na (15-5) vs. Silvana Gomez Juarez (10-4); UFC 275, June 11\nThanks for reading!\nIf you find something you’d like to see in the Morning Report, hit up @DrakeRiggs_ on Twitter and let him know about it. Also follow MMAFighting on Instagram and like us on Facebook."", 'Abdul Razak Alhassan\nAbdul Razak Alhassan (born August 11, 1985) is a Ghanaian mixed martial artist who competes in the Welterweight division in the UFC. A professional competitor since 2013, Alhassan formerly competed for Bellator and Legacy Fighting Championship.\n|Abdul Razak Alhassan|\n|Born||August 11, 1985|\n|Other names||Judo Thunder|\n|Height||5 ft 10 in (1.78 m)|\n|Weight||170 lb (77 kg; 12 st)|\n|Reach||73 in (185 cm)|\n|Style||Judo, Muay Thai|\n|Fighting out of||Fort Worth, Texas, United States|\n|Rank||Black belt in Judo|\n|Mixed martial arts record|\n|Mixed martial arts record from Sherdog|\nMixed martial arts careerEdit\nAfter training and competing in judo for 22 years and earning his black belt, Alhassan transitioned to mixed martial arts. He made his professional debut in 2013, winning via TKO in just 25 seconds. Alhassan then compiled an overall record of 6-0 before being signed by the UFC.\nUltimate Fighting ChampionshipEdit\nAlhassan made his promotional debut at UFC Fight Night: Mousasi vs. Hall 2 against Charlie Ward on November 19, 2016. He won via knockout just 53 seconds into the first round and was awarded a Performance of the Night bonus.\nAlhassan faced Sabah Homasi on December 2, 2017 at UFC 218. He won the fight via TKO in the first round. The win was fraught with controversy as referee Herb Dean had stopped the fight prematurely.\nDue to the controversial stoppage in the first fight, a rematch with Homasi was held on January 20, 2018 at UFC 220. He won the fight via knockout in the first round. The win earned him the Performance of the Night bonus and made the Sportscenter top 10.\nChampionships and awardsEdit\nMixed martial arts recordEdit\n|Professional record breakdown|\n|11 matches||10 wins||1 loss|\n|Win||10–1||Niko Price||KO (punch)||UFC 228||September 8, 2018||1||0:43||Dallas, Texas, United States|\n|Win||9–1||Sabah Homasi||KO (punch)||UFC 220||January 20, 2018||1||3:47||Boston, Massachusetts, United States||Performance of the Night.|\n|Win||8–1||Sabah Homasi||TKO (punches)||UFC 218||December 2, 2017||1||4:21||Detroit, Michigan, United States|\n|Loss||7–1||Omari Akhmedov||Decision (split)||UFC Fight Night: Gustafsson vs. Teixeira||May 28, 2017||3||5:00||Stockholm, Sweden|\n|Win||7–0||Charlie Ward||KO (punch)||UFC Fight Night: Mousasi vs. Hall 2||November 19, 2016||1||0:53||Belfast, Northern Ireland||Performance of the Night.|\n|Win||6–0||Jos Eichelberger||TKO (punches)||Legacy FC 61||October 14, 2016||1||0:57||Dallas, Texas, United States||Catchweight (175 lbs) bout.|\n|Win||5–0||Ken Jackson||TKO (punches)||Rage in the Cage 47: Night of Champions||August 13, 2016||1||0:40||Shawnee, Oklahoma, United States||Middleweight bout.|\n|Win||4–0||Bryce Shepard-Mejia||KO (punch)||Bellator 143||September 25, 2015||1||1:26||Hidalgo, Texas, United States||Welterweight debut.|\n|Win||3–0||Matt McKeon||TKO (punches)||Rocks Xtreme MMA 12||February 28, 2015||1||0:47||Harker Heights, Texas, United States||Middleweight debut.|\n|Win||2–0||Matt Jones||TKO (punches)||Bellator 111||March 7, 2014||1||1:23||Thackerville, Oklahoma, United States||Catchweight (190 lbs) bout.|\n|Win||1–0||Kolby Adams||TKO (punches)||Xtreme Knockout 20||November 23, 2013||1||0:25||Arlington, Texas, United States||Catchweight (180 lbs) bout.|\n- Simon, Zane (16 November 2016). ""UFC Belfast gets last minute WW fight"". Bloody Elbow. Retrieved 7 February 2017.\n- ""Abdul Razak Alhassan - Official UFC Profile"". UFC.com. Retrieved September 18, 2018.\n- ""The accidental assassin: Abdul Razak Alhassan on his reluctant path to UFC Fight Night 109"". 25 May 2017. Retrieved 31 May 2017.\n- Al-Shatti, Shaun (7 March 2014). ""Bellator 111 results: Dantas ve. Leone"". Retrieved 7 February 2017.\n- Alexander, Mookie (25 September 2015). ""Bellator 143: Warren wins snoozer, Grove gets TKO"". Retrieved 7 February 2017.\n- ""UFC Fight Night 99 results: Abdul Razak Alhassan drops Charlie Ward in 53-second slugfest"". 19 November 2016. Retrieved 7 February 2017.\n- Dann Stupp (2016-11-19). ""UFC Fight Night 99 results: Abdul Razak Alhassan drops Charlie Ward in 53-second slugfest"". mmajunkie.com. Retrieved 2016-11-19.\n- Tristen Critchfield (2016-11-19). ""UFC Fight Night 99 bonuses: Marshman, Lee, Ledet, Alhassan collect $50K"". sherdog.com. Retrieved 2016-11-19.\n- Marcel Dorff (2017-04-05). ""Omari Akhmedov will fight KO-Specialist Abdul Razak Alhassan in Stockholm"" (in Dutch). mmadna.nl. Retrieved 2017-04-05.\n- Ben Fowlkes (2017-05-28). ""UFC Fight Night 109 results: Omari Akhmedov hands Abdul Razak Alhassan first loss"". mmajunkie.com. Retrieved 2017-05-28.\n- Alexander K. Lee (2017-09-28). ""Sabah Homasi to fight Abdul Razak Alhassan at UFC 218"". mmafighting.com. Retrieved 2017-09-28.\n- ""UFC 218 results: Abdul Razak Alhassan TKOs Sabah Homasi in first, despite wide protests of stoppage"". MMAjunkie. 2017-12-03. Retrieved 2017-12-03.\n- ""Abdul Razak Alhassan vs. Sabah Homasi shifted to UFC 220"". MMAjunkie. 2017-12-14. Retrieved 2017-12-15.\n- ""UFC 220 results: Abdul Razak Alhassan leaves no doubt in rematch with Sabah Homasi"". MMAjunkie. 2018-01-21. Retrieved 2018-01-21.\n- ""UFC 220 bonuses: Daniel Cormier, Abdul Razak Alhassan among $50,000 winners"". MMAjunkie. 2018-01-21. Retrieved 2018-01-21.\n- Greg Moore (2018-02-15). ""Matt Lopez vs. Alejandro Perez added to UFC on Fox 29 at Gila River Arena"". azcentral.com. Retrieved 2018-02-15.\n- Sherdog.com. ""Ricky Rainey Replaces Abdul Razak Alhassan, Meets Muslim Salikhov at UFC on Fox 29"". Sherdog. Retrieved 2018-03-31.\n- Staff (2018-08-09). ""Abdul Razak Alhassan vs. Niko Price added to UFC 228 in Dallas"". mmajunkie.com. Retrieved 2018-08-09.\n- ""UFC 228 results: Abdul Razak Alhassan blasts Niko Price in just 43 seconds"". MMAjunkie. 2018-09-09. Retrieved 2018-09-09.\n- ""Report: UFC fighter Abdul Razak Alhassan indicted on March sexual assault charge"". Bloody Elbow. Retrieved 2018-09-26.']"	['<urn:uuid:93c4e727-d802-4626-a605-2cd62758a242>', '<urn:uuid:098133b2-78b6-429f-b253-2c6276da2d3a>']	factoid	with-premise	long-search-query	distant-from-document	comparison	expert	2025-05-13T03:34:55.284799	12	47	1640
50	connection between ground level ozone formation health impacts chemical reaction origin	Ground-level ozone forms when nitrogen oxides mix with volatile organic compounds (VOCs) and react with sunlight and heat. This occurs near sources of gasoline vapors, car exhaust, chemical storage, and factory emissions. The health impacts include reduced lung function, shortness of breath, chest pain when inhaling, wheezing, coughing, and can trigger or worsen respiratory conditions like asthma, bronchitis, and emphysema.	"['7 ways to reduce air pollution - Oxygen Essay Example\n7 Ways to Reduce Air Pollution\nStep 1: Understand Where Air Pollution Comes From\nAccording to the federal Environmental Protection Agency (EPA), there are six major causes of air pollution in the United States. These are ground-level ozone, particulate matter, lead, sulfur dioxide, nitrogen oxides and carbon monoxide.\nessay sample on ""7 ways to reduce air pollution""? We will write a cheap essay sample on ""7 ways to reduce air pollution"" specifically for you for only $12.90/page\nWhile most people have heard of carbon monoxide, lead and particulate matter, they might be surprised to learn that the primary source of air pollution today is ground-level ozone. Unlike the natural ozone layer that surrounds the earth and helps regulate temperature by shielding it from the sun’s harmful rays, ground-level ozone occurs when nitrogen oxides mix with volatile organic compounds (VOCs). The chemical reaction that follows emits ground-level ozone that can lead to numerous health problems. Upper respiratory ailments such as asthma, bronchitis and emphysema are all related to ground-level ozone. So where does this chemical reaction occur? Anywhere you have gasoline vapors, car exhaust fumes, a large storage of chemical agents, and factory or utility plant emissions. Reduce these key elements and you can reduce the amount of ground-level ozone you are exposed to.\nStep 2: Reduce Your Use of Automobiles\nAutomobiles do more than just contribute to ground-level ozone. The making of gasoline requires the burning of coal and oil which causes an increase in sulfur dioxides, another of the six leading causes of air pollution. The EPA says petroleum refineries are key producers of sulfur dioxides and the more time spent behind the wheel of a car means more air pollution for everyone to breathe.\nWhile it may not be possible to completely eliminate your use of automobiles, try consolidating errands and shopping to keep from making multiple trips to the same location. If you live near a commuter railway, make a commitment to take the train at least one day a week to work. Looking for a new set of wheels? Why not buy a hybrid. These great vehicles combine electric and fuel\nenergy to get better mileage and many produce nearly zero emissions.\nStep 3: Plant More Plants\nNASA recently discovered that many household plants, like the Gerbera Daisy, Peace Lily and English Ivy are instrumental in removing carbon monoxide from the air. Operating much like the human liver, these common indoor plants actually filter harmful chemicals and dangerous compounds from the air, absorbing the toxins through tiny pores in their leaves and “digesting” the pollution through their stems, roots and out through the soil.\nUsing these natural air filters in your home or office can greatly reduce the amount of indoor air pollution and help eliminate recurring colds and respiratory problems. According to the NASA study, other helpful varieties for clean air are the bamboo palm, Chinese evergreen and any of the Dracaena trees.\nStep 4: Go Solar\nElectricity might seem a green way to heat your home, but the VOCs generated by electrical utility plants are among the highest in all forms of manufacturing. Nitrogen oxides are also a byproduct of electrical utilities and as we learned in Step 1, combining the two can lead to deadly increases in ground-level ozone. Utility companies produce more sulfur dioxides than petroleum plants and the amount of other resources necessary to operate the plants make electric utilities a less-than optimum choice when looking to “power” your home.\nToday’s solar panels are unique in both design and installation. Whereas previous generation panels were large and unsightly perched above your roof, modern versions are colorful, install directly into the roof tiles and can usually generate enough electricity to power your home, heat your hot water and have enough left over to sell back to the utility company.\nStep 5: Get the Lead Out\nThe dangers of lead-based paint have been known since roughly the 1970s, however, recent environmental issues surrounding imported toys have caused\neveryone to rethink the use of lead in common household products. Leaded fuels were phased out after the 1990 Amendment to the federal Clean Air Act, making trash-burning, battery storage, and utility-leaching the major sources of household lead pollution.\nHave your gas and electric appliances checked to make sure there are no leaks in the lines or shorts in the wiring. Never burn trash or use your fireplace to get rid of excess garbage. If you have old batteries lying around, call your County Department of Environmental Health and ask where you can legally dispose of them. Most importantly, check the label on painted items such as furniture, decorator items and children’s toys. If you are unsure if the item contains lead-based paint, contact the manufacturer and ask. If they are unable to tell you, return the item for a refund or get rid of it altogether.\nStep 6: Never Dust Again\nWell, not really. But be careful about the kind of dust you stir up. Much of the thick brown haze you see over large urban areas is a combination of dust from construction sites, smoke from factories and the emissions from cars mixed together. While you might not be able to control the number of cars on the road or the types of factories that operate, you can watch your yard for dry patches and do your part to eliminate dust. This is especially important if you use a lot of chemical fertilizers or other treatments on your patch of ground because those chemicals will mix with others once airborne and could cause even greater health problems.\nIf you have a large non-landscaped area of your yard, make sure it stays damp and is not allowed to completely dry out, creating dust. If you are not ready to plant, sprinkle the area with water every few days to keep the dust in check. Better yet, cover with plastic sheeting to keep the ground from drying out while generating your own solar watering system at the same time.\nStep 7: Get Cozy\nSince electrical plants contribute to both carbon monoxide and nitrogen oxides, the burning of coal and petroleum generates ground-level ozone and sulfur dioxides, and the use of gas-powered heating systems can raise interior carbon monoxide levels to dangerous levels. Instead of turning on the heat, why not put on a sweater? Put another blanket on your bed during colder winter months. Keep cozy sweatshirts and plush chenille throws near the sofa for those evenings in front of the television. Snuggle next to someone to keep warm.\nSimple changes can make a huge difference in the amount of air pollution you are exposed to. Start small and build on your successes. Pretty soon, we will all be breathing a sigh of relief.', 'Air pollution causes: What’s making us sick?\nHere’s a breakdown of air pollution particles that cause health and environmental problems.\nFri, Aug 05, 2011 at 11:57 AM\nIt’s summertime and the living is wheezy.\nThe time of year when most people want to be outside is also a time when air pollution poses the greatest risk to your health. What are air pollution causes? Well, the heat and sunlight of a summer day cook up ozone, an invisible gas that at high enough levels can reduce lung function, aggravate asthma, cause coughing, throat irritation and even tightness in your chest when breathing.\nBut, ozone isn’t the only type of air pollution with adverse effects on your health. Other types of air pollution have been linked to stroke and heart attack, development of asthma and other lung disorders in children, the development of atherosclerosis (hardening of the arteries) in adults and even cognitive decline in the elderly.\nParticulate matter, the main ingredient in hazy smog, is comprised of very tiny liquid and solid particles floating in the air. The smaller the particle, the bigger the risk to your health. Particles 10 micrometers, or microns, in diameter or smaller because pass through the throat and nose and enter the lungs. Ten microns is teeny tiny – a human hair is 40 to 120 microns in diameter.\nParticulate air pollution is a mix of smoke, soot, dust, salt, acids, dust and metals kicked into the air from cars, diesel-burning trucks, wood burning stoves, fireplaces, coal power plants, wildfires and windblown dust.\nThere is strong scientific evidence linking short-term expose to the finest particulate air pollution (2.5 microns or less) to heart attacks, strokes and cardiovascular death among those with heart disease, according to a 2010 statement by the American Heart Association.\nParticulate air pollution has also been linked to decreased lung function, aggravated asthma and development of chronic bronchitis.\nGround level ozone forms when emissions from cars and power plants react chemically with sunlight and heat. Even in healthy adults, high levels of ozone can cause shortness of breath, chest pain when inhaling, wheezing and coughing.\nOzone can also trigger asthma attacks among those with the condition. Higher ozone levels can also aggravate other chronic lung diseases such as emphysema and bronchitis.\nNitrogen dioxide (NO2) forms from emissions from cars, trucks and buses, power plants, and off-road equipment. Nitrogen dioxide may also be a major component of indoor air pollution, the byproduct of kerosene heaters or improperly vented gas stoves and heaters\nEven short-term exposure to nitrogen dioxide – just 30 minutes to 24 hours – can cause inflammation of the throat and lungs in healthy people and aggravate symptoms in those with asthma. Continued exposure to high nitrogen dioxide levels may lead to the development of acute or chronic bronchitis\nThe largest sources of sulfur dioxide emissions are coal and oil burning power plants, according to the U.S. Environmental Protection Agency. Power plants generate nearly three-quarters of sulfur dioxide emissions.\nSulfur dioxide reacts with other compounds in the atmosphere to form particulate pollution – considered the most harmful type of air pollution. Exposure to sulfur dioxide also aggravates asthma symptoms. The EPA also says there is a connection between short-term exposure to sulfur dioxide and increased visits to emergency departments and hospital admissions for respiratory illnesses, particularly among children, the elderly, and asthmatics.\nKnow more about air pollution causes? Leave us a note in the comments below.']"	['<urn:uuid:b449f458-99ba-47d8-a47a-b4e800d6c5bc>', '<urn:uuid:9ac925f0-acc4-479a-b5f0-4c190888db4e>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T03:34:55.284799	11	60	1713
51	How do NARM and Polyvagal Theory differ in their approach to safety?	NARM focuses on addressing adaptive survival styles and complex trauma through a 'bottom-up' and 'top-down' approach integrating somatic and relational skills, while Polyvagal Theory emphasizes the social engagement system and neurophysiological safety through specific techniques like slow speaking and positioning against walls.	['Transforming Trauma: The NeuroAffective Relational Model (NARM) for Addressing Adverse Childhood Experiences and Resolving Complex Trauma\nBrad Kammer, LMFT, LPCC\nFriday, Mar. 3, 2023\n9:00 am - 1:15 pm\nQualifies for 4 CEs (4 hours of instruction with one 15-minute break)\nThis webinar will introduce you to the NeuroAffective Relational Model (NARM) for addressing adverse childhood experiences and resolving complex trauma. NARM is a promising therapeutic modality for addressing life-long psychobiological symptoms and interpersonal difficulties that arise from unresolved attachment, relational, developmental, cultural and intergenerational trauma.\nNARM weaves together a “bottom-up” and “top-down” approach, integrating somatic, psychodynamic and relational clinical skills, that helps clients address the adaptive survival styles they’ve used to manage the impact of ACEs and C-PTSD. Offering a new perspective on post-traumatic growth, focusing on relational health and heartfulness, NARM provides a powerful model for supporting personal and collective transformation.\nAt the conclusion of this presentation, participants will be able to:\nIdentify how the NeuroAffective Relational Model (NARM) fits into the current field of Trauma-Informed therapeutic approaches.\n- Differentiate NARM from other Trauma-Informed therapeutic approaches.\n- Differentiate between shock trauma (PTSD) and developmental/complex trauma (C-PTSD).\n- Understand the life-long impacts of unresolved adverse childhood experiences and complex trauma from a NARM perspective.\n- Identify the 5 NARM Adaptive Survival Styles.\n- Identify the NARM Relational Model.\n- Identify the NARM 4 Pillars model.\n- Identify the NARM focus on Heartfulness and Post-Traumatic Growth.\nRSVP here. Please note that once you have RSVPd for this event you will receive a link to register for the Zoom Meeting. You must complete this second step to gain access to the event link.\nAbout Brad Kammer, LMFT, LPCC\nBrad Kammer, LMFT, LPCC, is a California Licensed Marriage & Family Therapist and Professional Clinical Counselor. Brad trained as a Somatic Psychotherapist and has worked in the field of trauma for over 2 decades, specializing in working with Adverse Childhood Experiences (ACEs) and Complex Post-Traumatic Stress Disorder (C-PTSD).\nBrad began his career as a Humanitarian Aid Worker in Asia which introduced him to personal and collective trauma. He became passionate about supporting individuals and communities in the transformation of trauma. Brad has since focused his work on the integration of Somatic Psychology, Interpersonal Neurobiology, Relational Therapies and wisdom from Spiritual Traditions and Traditional Cultures. He has a special interest in the fields of developmental, cultural and intergenerational trauma.\nBrad is passionate about bringing trauma-informed work to individuals and communities outside the clinical setting. For nearly 20 years, Brad has taught as a college professor at both undergraduate and graduate levels. He has been involved in community education, outreach and consulting to numerous communities and organizations. Brad has also been a trainer for Somatic Experiencing® (SE®). And more recently, he began producing a podcast entitled: Transforming Trauma.\nBrad is a Senior Trainer and Training Director of the NARM Training Institute. He teaches the NeuroAffective Relational Model® (NARM®) internationally and has provided consultation to thousands of therapists and other helping professionals around the world.\nBrad lives in a small town in Northern California with his family where he enjoys anything involving nature, travel, music, food, and learning about and connecting with new people.\nZoom Meeting. Requires pre-registration with Zoom; please make sure you register with Zoom with the link provided in your confirmation email.\nRedwood Empire Chapter CAMFT (Provider #57173) is approved by the California Association of Marriage and Family Therapists to sponsor continuing education for LMFTs, LCSWs, LPCCs, and/or LEPs. Redwood Empire Chapter CAMFT maintains responsibility for this program/course and its content.\nDisability Accommodation: To request an accommodation for a disability, please email email@example.com.\nRefunds: Refunds may be requested up to 7 days before the event by contacting firstname.lastname@example.org. No refunds are given beginning 7 days prior to the event. If a member cannot attend the event due to a mandatory evacuation or Public Safety Power Shut-off the day of the event, they can request and receive a refund with verification of address.\nCE Certificate: You must attend the entire presentation and complete a post test and an evaluation to receive your CE credit certificate. At the conclusion of this educational event, after confirming attendance, an email with a link to the test and evaluation form will be sent to all attendees who attended the online event. Once you complete and submit your evaluation, you will have immediate access and be able to print or save your CE Certificate. This course meets the qualifications for 4.0 hrs of continuing education (CE) credit for LMFTs, LCSWs, LPCCs, and/or LEPs as required by the California Board of Behavioral Sciences\nNotice of recording: All of our CE events are recorded or filmed and may appear on our website. By participating in this event you may appear in that recording. By participating in this event you agree to be recorded/filmed and give permission for RECAMFT to use your likeness where it may appear on our website or in promotion/marketing materials.\nGrievances: direct grievances to email@example.com, and/or the chapter president at firstname.lastname@example.org.', 'At the recent Trauma Summit at the ICC in Belfast, many of the presentations mentioned the importance of psychological safety when recovering from traumatic experiences. Researchers of the advanced trauma recovery models have been saying for years that to find regulation in the nervous system after an overwhelming event has occurred, one of the first steps is to find safety in the body. When a tiger is chasing you, it seems obvious that you need to find a safe place so as not to be eaten. When a bomb goes off, your survival depends on finding somewhere that is safe and out of the danger zone. As humans, our need to seek safety continues psychologically long after the danger has gone. It is of course a little more complex than this, but essentially psychological safety is the key to recovery and to thriving.\nThis may seem irrelevant to the business environment, however the same theory applies when creating the best learning and work environment for a workforce. Stephen Porges, a well-known psychiatrist who developed the leading theory on psychological safety, the Polyvagal Theory, sums up his life’s work in a recent paper saying, “humans, as social mammals, are on an enduring lifelong quest to feel safe” (Porges, 2022). Many leading companies have identified psychological safety as one of the most important factors in unlocking team potential.\nThe Polyvagal Theory is a useful framework to understand our stress response\nThe Polyvagal Theory is a useful framework to understand our stress response and how to mitigate unnecessary anxiety states. It is in the safe neurophysiological state when we can be present, be creative, bond and cooperate with others, and be able to voice our thoughts and needs more easily. Our autonomic nervous system is regulated when in the safe state. In other words, our automatic response in any given situation is more regulated and appropriate. We don’t get irrationally anxious or irritated.\nPorges (2022) emphasises that our need to feel safe influences, ”our mental and physical health, social relationships, cognitive processes, behavioural repertoire, and serving as a neurophysiological substrate upon which societal institutions dependent on cooperation and trust function are based.” Understanding our emotional responses to different situations can give us more choice over how to behave, which helps to promote overall mental health and wellbeing.\n“The danger signalling is dialled down”\nIn the therapeutic trauma recovery field, trauma is commonly defined as any experience or series of experiences that overwhelm our nervous system (Levine, 2010), hindering an appropriate response. By this definition we call carry some degree of trauma history—most of us have been overwhelmed at some point in our lives. Understanding our own responses to stressful situations can help us to gain control of our emotions in difficult circumstances. Learning how to develop psychological safety for ourselves can be enormously empowering.\nUnderpinning the Polyvagal Theory is our social engagement system, which is the interaction of a group of cranial nerves in the brain that affect how we breathe, vocalise, listen, and make eye contact—how we socially interact with others. If we can employ techniques to engage these nerves, we can change the inner state of our social engagement system and therefore change our level of psychological safety.\nSimple tips to create psychological safety\nWhat does this look like in practice? A simple tip I often offer clients anxious when public speaking, is to speak slowly in long phrases. This engages not only the nerves involved in the vocal cords, but also those innervating our breathing mechanism by elongating the exhalation. Long exhalations combined with voice activation serves to increase vagal tone and thus an inner sense of safety, reducing feelings of anxiety and stress. Singing can have the same effect.\nAn interesting side benefit is that speaking slowly in long phrases, with expression in your voice, affects how others listen to you. They tend to be more engaged too! It is a win-win situation for everyone. I employ this technique time after time, always with a positive outcome. Next time you are anxious at having to give a presentation, start speaking slowly with expression in your voice and notice how quickly your nerves subside.\nAnother useful tip to enhance psychological safety given to me by Stephen Porges, is to stand or sit with my back against the wall when feeling anxious. With one half of my body safely against a wall, my nervous system only needs to scan 180 degrees for danger. Therefore, the danger signalling is dialled down considerably allowing me to focus only on those in front of me, instead of what might happen behind me.\nThere are many techniques to support psychological safety, but importantly, self-awareness of our physical and emotional state allows us to have more choice over what we say and how we behave. Nervous systemregulation, and therefore improved inner safety, will help both ourselves and our work community.\nThis article was first published in Northern Ireland Chamber Ambition magazine\nYou can learn more about implementing the polyvagal theory in practice at this workshop in February 2023\nLevine P. (2010) In an unspoken voice: How the body releases trauma and restores goodness. Berkeley, CA: North Atlantic Books\nPorges S. W. (2022). Polyvagal Theory: A Science of Safety. Frontiers in integrative neuroscience, 16, 871227.']	['<urn:uuid:21e59b85-93f6-4699-8f7f-dc04f065d1fe>', '<urn:uuid:1f38e1a6-a49f-46c1-8c63-9808fba34a98>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	expert	2025-05-13T03:34:55.284799	12	42	1711
52	history teetotal movement original location preston england yoga origins india compare dates	The teetotalism movement began in Preston, England in the early 19th century, specifically with the founding of the Preston Temperance Society in 1833. In contrast, yoga's origins date much further back, being developed in ancient India between 3200 and 1500 BCE, as evidenced by its mention in the ancient Vedic texts.	"['|This article needs additional citations for verification. (October 2012) (Learn how and when to remove this template message)|\n|Look up teetotal in Wiktionary, the free dictionary.|\nTeetotalism is the practice or promotion of complete personal abstinence from alcoholic beverages. A person who practices (and possibly advocates) teetotalism is called a teetotaler (also spelled teetotaller; plural teetotalers or teetotallers) or is simply said to be teetotal. The teetotalism movement was first started in Preston, England, in the early 19th century. The Preston Temperance Society was founded in 1833 by Joseph Livesey, who was to become a leader of the temperance movement and the author of The Pledge: ""We agree to abstain from all liquors of an intoxicating quality whether ale, porter, wine or ardent spirits, except as medicine.""\nThere is some dispute over the origin of the word teetotaler. One anecdote attributes the origin of the word to a meeting of the Preston Temperance Society in 1833. The story attributes the word to Richard Turner, a member of the society, who in a speech said ""I\'ll be reet down out-and-out t-t-total for ever and ever"".\nA variation on the above account is found on the pages of The Charleston Observer:\nTeetotalers.--The origin of this convenient word, (as convenient almost, although not so general in its application as loafer,) is, we imagine, known but to few who use it. It originated, as we learn from the Landmark, with a man named Turner, a member of the Preston Temperance Society, who, having an impediment of speech, in addressing a meeting remarked, that partial abstinence from intoxicating liquors would not do; they must insist upon tee-tee-(stammering) tee total abstinence. Hence total abstainers have been called teetotalers.— The Charleston Observer vol. 10, no. 44 (29 October 1836)\nAn alternative explanation is that teetotal is simply a reduplication of the first \'T\' in total (T-total). It is said that as early as 1827 in some Temperance Societies signing a \'T\' after one\'s name signified one\'s pledge for total abstinence. In England in the 1830s, when the word first entered the lexicon, it was also used in other contexts as an emphasized form of total; a comparable American English locution would be ""total with a capital T"" (an instance of the ""[word] with a capital [word-initial letter]"" snowclone).\nAccording to historian Daniel Walker Howe (What Hath God Wrought: The Transformation of America, 1815-1848, 2007) the term was derived from the practice of American preacher and temperance advocate Lyman Beecher. He would take names at his meetings of people who pledged alcoholic temperance and noted those who pledged total abstinence with a T. Such persons became known as Teetotallers.\nReasons and justifications\nSome common reasons for choosing teetotalism are psychological, religious, health, medical, familial, philosophical and social, or sometimes it is simply a matter of taste or preference. When at drinking establishments, teetotalers (or teetotallers) either abstain from drinking completely, or consume non-alcoholic beverages such as water, juice, tea, coffee and low-alcohol/alcohol-free beer.\nMost teetotaler organizations also demand from their members that they do not promote or produce alcoholic intoxicants.\nList of beliefs that make members teetotalers\nIslam, Jainism, Sikhism, Buddhism, Brahma Kumaris, The Church of Jesus Christ of Latter-day Saints (LDS Church or Mormons), Seventh-day Adventist Church, Amish, Old Order Mennonites, Conservative Mennonites, International Society for Krishna Consciousness (Hare Krishna movement), Quakers, the Baha\'i Faith, the Assemblies of God, and the Salvation Army are notable religious groups that require adherents to abstain from alcohol. Methodists also historically abstained from alcohol, though many Methodist denominations no longer teach this. Members are also required to refrain from selling such products. A translation of the New Testament, the Purified Translation of the Bible, translates in a way that promotes teetotalism.\nTeetotalism and organized religion\nAbstention of alcohol is a tenet of a number of religious faiths, including those of the Sikhs, Bahá\'ís, Hare Krishnas, Hindus (particularly Brahmins), Jains, Meivazhi-ites, Muslims, Pillays, and Scientologists.\nSimilarly, one of the five precepts of Buddhism is abstaining from intoxicating substances that disturb the peace and self-control of the mind, but it is formulated as a training rule to be assumed voluntarily rather than as a commandment.\nA number of Christian denominations also forbid the consumption of alcohol, including the Amish, Seventh-day Adventists, Mennonites (both Old Order and Conservative), Church of the Brethren members, and Christian Scientists. Many Christian groups, such as Methodists, Mormons, and Quakers, are often associated with teetotalism due to their traditionally strong support for temperance movements and prohibition. However, tenets forbidding the consumption of alcohol are variably practiced. In many Christian denominations, abstinence is not a religious requirement, but the tradition is strong enough to make ritual and recreational alcohol consumption a controversial issue among members. Members of the Salvation Army make a promise on joining the movement to observe lifelong abstinence from alcohol. Catholicism, the Orthodox Churches, and Anglicanism all require wine in their central religious rite of the eucharist, and while many Protestant churches often allow grape juice or alcohol-free wine in their communion services, only a few Protestants require a non-alcoholic beverage as official policy. (See Christianity and alcohol.)\nResearch on non-drinkers\nSeveral authors have conducted research looking at the social and subjective experiences of young people who do not drink alcohol in varied social settings. For example, Dominic Conroy and Richard de Visser published research in Psychology and Health which hinted at the types of strategies involved in refusing alcoholic drinks or explaining reasons for non-drinking.\nCaroline H. McClave published a comparison of 3 studies entitled Asexuality as a Spectrum: A National Probability Sample Comparison to the Sexual Community in the UK which studied asexual people and gray asexual people in comparison to those who are not on the spectrum of that sexual orientation. Interestingly, all three of the studies that were compared found that asexuals and gray-asexuals drank significantly less than the people not in those categories, and also more people with those sexual orientations abstaining from drinking altogether.\n- Road to Zion - British Isles, BYU-TV; http://byutv.org/watch/801-207\n- Gately, Iain (May 2009). Drink: A Cultural History of Alcohol. New York: Gotham Books. p. 248. ISBN 978-1-592-40464-3.\n- Quinion, Michael. ""Teetotal"". worldwidewords.org. Retrieved 22 April 2012.\n- The Charleston Observer vol. 10, no. 44 (29 October 1836): 174, columns 4-5.\n- ""Online Etymology Dictionary - T, page 5"". Retrieved 2007-04-30.\n- ""Alcohol, Tobacco & Drugs"". ag.org. General Council of the Assemblies of God. Retrieved 24 February 2015.\n- ""Alcohol"". Methodist Church in Britain. Retrieved 24 February 2015.\n- Conroy, Dominic; de Visser, Richard. ""Being a non-drinking student: An interpretative phenomenological analysis"". Psychology and Health. 29 (5): 536–551. doi:10.1080/08870446.2013.866673. ISSN 0887-0446.\n- Asexuality as a Spectrum: A National Probability Sample Comparison to the Sexual Community in the UK', 'Yoga, stretching, and meditation is often used in treatment centers for its many benefits of body, mind, and spirit. But what exactly is yoga and where did it originate from? Why does yoga help people in recovery from addiction?\nOrigins of Yoga\nYoga has been practiced for thousands of years, though a specific date has been debated by anthropologists. Somewhere between 3200 and 1500 BCE is the agreed upon years that the practice of yoga was developed. The term “yoga” was found in ancient India’s written texts called the Veda’s. These volumes are the oldest found literature scribed in Sanskrit and which give a detailed insight into early Hinduism. The Vedas are a collection of philosophies and ideas written into hymns, mantras, and ritual ceremonies that were meant to be practiced and passed down verbally for future generations. As the Vedas were meant to draw down spiritual powers which were to be used for the health and welfare of individuals and community, there was a physical practice created to support the four sacred Vedic texts. “Yoga is believed to be derived from the joining or “yoking” of the breath, sound, and movement of the Vedas.\nThe Rig Veda, Sama Veda, Yajur Veda, and Atharva Veda scripts were divided into separate yoga rituals and yoga practices.\nThe Rig Veda Yoga includes a collection of prayers and mantras meant to be vocalized to draw overall health and happiness. In this text, the word and explanation of “yoga” were found. The other texts focus on musical sounds and chanting spiritual worship and ritual design, and tools to remove or protect from negative forces.\nDuring the 3rd Century BCE, yoga stretched to include Buddhist practices which were integrated into daily focus and was thought to be the foundation to ground an individual into a state of calmness, insight, and balance.\nOut of the teachings, the six main schools of the foundational philosophy were created, one primary focus being Union with the Divine or Yoga. Some of the approaches fell away over the last several hundred years, while others have survived and become internationally popular, yoga heading the way as the most recognized and practiced Vedic influence.\nFor the next few thousand years, the practice of yoga became more specified in its development and included a set of poses which were thought to uphold the foundation of the belief, and which would help the participator transcend the body and strive for the highest level of consciousness. This focus was meant to access high spiritual realms, mysticism, and rid physical pain while obtaining enlightenment, reducing the worldly suffering.\nYoga in Treatment\nYoga will most likely be on the list of weekly programming, if not daily. Some treatment centers may even base their entire programming on yoga sessions. It has been found to be useful in several areas of reducing stress. Other benefits that yoga has in order to obtain and maintain sobriety include:\n- Personal awareness and insight\n- Increased physical strength\n- Feeling better about self\n- Positive habit building\n- Tool for triggers\n- Decrease in pain\n- Increased flexibility\n- Increased endorphins\n- Stabilization of moods/emotional health\n- Decrease in depression\n- Sleep patterns normalize\nStress reduction: as yoga is a physical activity, the gentle stretching and holding of poses will increase stamina and strength, during which endorphins, will be released and cortisol ( stress hormone) will diminish. There have been studies which have measured the effects of pre and post yoga practice in relation to stress reduction. Yoga will include the conscious practice of mindful breathing, a technique often used in substance abuse treatment to be a quick and powerful response to stressful situations.\nPain relief and physical health: Yoga can be the one physical activity which a person who has chronic and/or severe pain can participate in. Yoga has been proven to open up areas that are “stuck” or locked, due to a past injury or illness. Yoga is self-paced, and instructors will be able to guide you into moderated poses when someone has physical limitations. Yoga gives the body increased flexibility and a higher degree of motion with the stretching and lengthening of muscles and tendons. Circulation will improve, blood sugar levels balanced, and pain decreases. A person’s strength and stamina will increase, making it an overall benefit for personal health. Yoga can reduce weight and support a healthy BMI. Yoga does not have to be rigorous for the benefits of weight loss, gentle stretches and poses have also been evidence of obtaining a healthy weight. Often in recovery, people who stop using drugs may gain weight, because of their caloric intake increases. Yoga is a great adjunct to their program to maintain the desired size.\nYoga and the brain\nNeuroplasticity is defined by how the brain can change through a person’s lifetime. Synaptic thought and brain activity can be increased or diminished. Due to the highly destructive nature of drugs and alcohol on the brain, it is imperative to help rebuild and restructure the brain. Yoga can help with neuroplasticity by the integration of the health triad: mental, emotional, spiritual as certain therapies only focus on one at a time, By doing so, the brain develops new synaptic thoughts and neuro causeways which would circumvent relapsing behaviors.\nAwareness: ridding the body of addictions and focusing on self through treatment can often be increased by yoga. Breathwork which reduces stress will also increase relaxation and help with repatterning negative self-thoughts and fears. We hold our breaths when we are afraid, and being mindful and aware of our breath can often subside fears and phobias. The practice of yoga in itself is a focus on connecting us to living in the now and staying present, often used for people in addiction recovery. In addition, memory can improve, focus and concentration increase, and coordination of thoughts to actions become normalized. The practice of yoga will ultimately create serenity and inner peace, which will allow for better awareness of self and the world around you.\nLearning yoga in treatment is a gentle tool that can be utilized when transitioning out of treatment. Yoga classes are literally international. However, if you live rurally and return to an area without a yoga center, there are dozens of online instructions that you could follow, or you could take what you had learned from the recovery center and create a self-paced, at home yoga and meditation practice.\nTypes of Yoga\nHatha yoga is the broad term that is defined as yoga that uses physical poses. This is the majority of what the western world would define yoga to be. If there is a practitioner advertising or teaching at the facility as a hatha yoga teacher, they most likely will be able to teach the beginner a series of poses and it is a good introduction to the practice of yoga in general. Hatha yoga can be understood as introductory level, and move into Ashtanga and Vinyasa. Hatha yoga is taught at treatment centers so that clients can work at their pace while developing an understanding on the benefits. Ashtanga\nAshtanga yoga is based on the Vedic principals and ancient texts that were turned from philosophy into practice. In America, it became popular in the 1970’s and is continuously utilized and popular today. There are physical poses used in a sequence which combines breathing technique to each movement. Ashtanga yoga and Vinayasa style are similar, however, Ashtanga has the same sequence and poses in the same order. It is rigorous, and more demanding, which will be used to increase the heart rate due to the physical focus and strength building. This practice is good for people who like routine and want to increase their proficiency before moving on to something new.\nVinyasa yoga is similar to Ashtanga, with the main difference being that Vinyasas can change the sequence of poses in each class. Vinyasa is a Sanskrit word that translates roughly to “placing special” or referring to the sequence of poses. In Vinyasa, the poses are transitioned in a conscientious and smooth approach, which the teacher will instruct, keeping the challenges of each individual in the group. Vinyasa will also concentrate on breath control in regards to the sequence of poses and transitions of them. Vinyasa is also movement focused and will utilize music in the classes to keep it lively. The classes will develop poses with the individuals who attend, and several of the poses will be repeated in different order, with one or two new introduced each class. This is good for people who like to change up their routine but don’t want to learn everything new.\nVinyasa classes are known for their fluid, movement-intensive practices. Vinyasa teachers sequence their classes to smoothly transition from pose to pose, with the intention of linking breath to movement, and often play music to keep things lively. The intensity of the practice is similar to Ashtanga, but no two vinyasa classes are the same. If you like to change up routine while developing and challenging yourself, Vinyasa may be the practice for you.\nBikram and Hot Yoga\nBikram yoga was developed by Bikram Choudhury from India who created a Hatha practice of 26 poses for 90 minutes and breathing exercises in a hot room. It was brought to the United States in the 1970’s and although gained a following, has been controversial towards its founder. Allegations of misconduct, approach, cult-like atmosphere, and sexual assault became public, damning the practice. However, practitioners of this style swear by the rapid results of the style and approach. Bikram trademarked his style, and subsequent teachers may find themselves in legal trouble if they vary from the sequence. Bikram yoga teachers become certified and who have completed several weeks course training. This seems to be the antithesis of yoga. The heated classrooms that are between 95 and 105 degrees, will add to the stretching and flexibility, and will also challenge people physically, making it more of a “work out” than a gentle approach. Bikram is physically demanding and causes sweating, which students will need enough water to stay hydrated, and appropriate mats and towels.\nHot yoga is basically the same as Bikram but doesn’t have it in the name because of the above- mentioned copyright reasons. The sequence of postures can differ from his 26 poses, but it will be set up to run about the same length and heat the room. Sweating and physical challenges are the center of hot yoga.\nIyengar is another style of yoga developed by an Indian man and popularized in the West in the 1970’s. It is a mindful and specific style of yoga, that pays close attention to each pose. A teacher will instruct and assists in how to properly hold the pose in correct alignment, in order to obtain the most benefit within each posture.\nIyengar practice will use yoga “props” to help as well. Rolled up blankets, blocks, cloth straps, mats, that are given will help the student increase and maintain the pose. It is helpful in developing concentration, letting go, obtaining clarity, and personal insight. It is not meant to go fast or be aerobic, though it will increase strength and stamina. Iyengar offers challenges in the mental and emotional realms whilst developing an increase of physical wellness. This style is especially beneficial for chronic pain, recovery from injury, and strength building without damaging existing conditions. Balance and stability will increase, often beneficial for people in recovery.\nIyengar has integrated several hundred poses from classical Hatha and can be used by the beginning to advanced student. Often, there can be the range of students performing the same pose, making allowances to challenge each level and modify with or without the use of props. Over time, the student can see their advancement of certain poses as they work on perfecting the alignment of breath and body.\nThis style of yoga concentrates the classes by developing technique in sequence and attention to timing and that incorporates breath control amount spent in each posture. It is good for those who need to pay attention to detail, who have a tendency to rush and get ahead of themselves, who need to learn the art of “letting go” and who have physical challenges such as age-related issues or recovery from disease and injury.\nThe instructor is often verbally helpful and will visit each student to assist them in finding the correct alignment. They are initially certified after at least two years of training and obtaining mentorship and will receive certifications for subsequent levels.\nRestorative yoga is an excellent style, to begin with if a client has anxiety, is feeling overwhelmed, is suffering from fatigue, or has multiple health issues. It is sometimes called “yin yoga” and also uses props like Iyengar in order to help the student with any challenges. In many restorative classes, there will be a part of the class that designates the pose to complete relaxation, like the “child’s pose” or an easy, non-taxing physical posture where the body can relax in a mindful state. Restorative is very beneficial for clients in recovery who have not slept well and who need time to rest and recover. A class in restorative yoga, followed by a Hatha practice, is commonly offered in treatment.\nIf you are offered yoga classes in treatment, take advantage of them: keep an open mind and willingness to learn something new, or expand on something you already know. The key to success with yoga is to participate, being at ease with yourself and your limitations, and not what anyone else can or cannot do.\nTo learn more about our world-class program and luxury approach, call 1-855-700-8648 today.']"	['<urn:uuid:4a4c7d1b-ca5b-431a-bcfb-fab46a2be36a>', '<urn:uuid:0f54d460-cf60-4825-bcec-25587eb7a1ad>']	factoid	with-premise	long-search-query	distant-from-document	comparison	expert	2025-05-13T03:34:55.284799	12	51	3404
53	How do modern nutrition resources address weight management challenges, and what are the key safety considerations in supplement-based approaches?	Modern nutrition resources address weight management through tools like the Mosby's NUTRITRAC program, which allows creation of customized profiles and analysis of food intake and energy output using a database of over 3,000 foods and 150 activities. The Weight Management chapter specifically covers food misinformation and fads, addressing dangers and vulnerable groups. Regarding supplement safety, it's crucial to understand that supplements should be add-ons, not replacements for a healthy diet. Before using supplements, one must consider if the scientific community understands how the supplement works, if there's proof of claimed effects, potential food or medication interactions, whether it's necessary for health, affordability, and freedom from contaminants. The FDA only removes supplements from the market after proving they are hazardous, and supplement labels must carry disclaimers stating they're not intended to diagnose, treat, cure, or prevent disease.	"[""Williams' Basic Nutrition & Diet Therapy, 13th Edition\nEnter promotional code 05984 at checkout.\nPart of the popular LPN Threads Series, Williams’ Basic Nutrition & Diet Therapy is the market leader for a reason: you get coverage of hot topics, emerging trends, and cutting edge research, plus all the essentials for providing the best nutrition care. Written in a clear, conversational style, the book begins with the fundamental concepts of nutrition and then applies those concepts to diverse demographic groups in different stages of life. You also learn how selected disease processes work, and how to help communities and individuals achieve health and healthy living. A free CD contains Nutritrac, a computer program that helps you analyze case studies and create customized client profiles.Table of Contents\nNew to This Edition\n- More than 50 new illustrations include more age and culturally diverse images as well as more illustrations of disease states.\n- New assessment tools in the text include the Mini Mental State Examination, PAR-Q (Physical Activity Readiness Questionnaire), body composition measurement tools, and tools for energy requirement calculations.\n- Drug-Nutrient Interaction boxes highlight potential adverse effects of specific medications.\n- Updated statistics on diseases and conditions illustrate emerging trends and hot topics such as obesity and supplement use.\n- Updated Choose Your Foods: Exchange Lists for Diabetes in the appendix includes new content for culturally diverse populations.\n- A new figure illustrates the complex processes of digestion and metabolism.\n- Water Balance chapter includes the DRIs for fluids and provides the water content of selected foods.\n- Nutrition in Infancy, Childhood, and Adolescence chapter adds information on the growing problem of overweight and obese children.\n- Weight Management chapter covers food misinformation and fads, addressing the dangers and the groups vulnerable to such misinformation.\n- Gastrointestinal and Accessory Organ Problems chapter includes recent research on the pathogenesis of celiac disease along with the principles and selected foods of the gluten-free diet for treatment.\n- Coronary Heart Disease and Hypertension chapter is updated to follow the now-standard Therapeutic Lifestyle Change (TLC) diet to treat hypertension.\n- Surgery and Nutritional Support chapter includes considerations and diets used in treatment for the post-bariatric surgery patient.\n- An engaging design includes colorful openers, illustrations, boxes, tables, and text layout.\n- Clinical Applications and For Further Focus boxes highlight hot topics and analyze concepts and trends in depth.\n- Case studies in clinical care chapters focus attention on related patient care problems.\n- Key Concepts and Key Terms condense critical information into easy-to-find boxes.\n- Diet therapy guidelines include recommendations, restrictions, and sample diets for a number of major clinical conditions.\n- Cultural Considerations boxes discuss how a patient’s culture can affect nutritional concepts in practice.\n- Challenge questions use true/false, multiple-choice, and matching formats to test your understanding of chapter content.\n- Critical thinking questions challenge you to analyze, apply, and combine concepts.\n- Chapter summaries put content into perspective in terms of the “big picture” in nutrition.\n- Internet-based research and learning is emphasized and expanded throughout the text, citing key websites.\n- Useful appendixes include information on cholesterol content, fiber content, cultural and religious dietary patterns, and more.\n- A companion website contains case studies applying chapter content to real-life examples, 350 study questions for instant self-assessment, the most recent growth charts from the CDC, the ADA’s Nutrition Care Process, and links to online information sources.\n- Mosby’s NUTRITRAC Nutrition Analysis and Weight Management CD offers the perfect clinical practice tool, letting you create customized personal profiles and analyze food intake and energy output — by using a database of more than 3,000 foods and more than 150 sporting, recreational, and occupational activities.\n- Unique! Content threads share features with other LPN/LVN titles from Elsevier for a consistent learning experience.\nBy Staci Nix, MS, RD, CD, Professor, Division of Nutrition, College of Health, University of Utah, Salt Lake City, UT"", 'Chapter 16. Performance Nutrition\nCurrent trends also include the use of supplementation to promote health and wellness. Vitamins, minerals, herbal remedies, and supplements of all kinds constitute big business and many of their advertising claims suggest that optimal health and eternal youth are just a pill away. Dietary supplements can be macronutrient (amino acids, proteins, essential fatty acids), micronutrient (vitamins and minerals that promote healthy body functions), probiotic (beneficial bacteria such as the kind found in the intestines), and herbally ( often target a specific body part, such as bones) based.\nSome public health officials recommend a daily multivitamin due to the poor diet of most North Americans. The US Preventive Task Force also recommends a level of folate intake which can be easier to achieve with a supplement. In addition, the following people may benefit from taking daily vitamin and mineral supplements:\n- Women who are pregnant or breastfeeding\n- Premenopausal women who may need extra calcium and iron\n- Older adults\n- People with health issues that affect their ability to eat\n- Vegetarians, vegans, and others avoiding certain food groups\nHowever, before you begin using dietary supplementation, consider that the word supplement denotes something being added. Vitamins, minerals, and other assorted remedies should be considered as extras. They are add-ons—not replacements—for a healthy diet. As food naturally contains nutrients in its proper package, remember that food should always be your primary source of nutrients. When considering taking supplements, it is important to recognize possible drawbacks that are specific to each kind:\n- Micronutrient Supplements. Some vitamins and minerals are toxic at high doses. Therefore, it is vital to adhere to the Tolerable Upper Intake Levels (UL) so as not to consume too much of any vitamin. For example, too much vitamin A is toxic to the liver. Symptoms of vitamin A toxicity can include tinnitus (ringing in the ears), blurred vision, hair loss, and skin rash. Too much niacin can cause a peptic ulcer, hyperglycemia, dizziness, and gout.\n- Herbal Supplements. Some herbs cause side effects, such as heart palpitations and high blood pressure, and must be taken very carefully. Also, some herbs have contraindications with certain medicines. For example, Valerian and St. John’s Wort negatively interact with certain prescription medications, most notably antidepressants. Additionally, there is a real risk of overdosing on herbs because they do not come with warning labels or package inserts.\n- Amino Acid Supplements. Certain amino acid supplements, which are often taken by bodybuilders among others, can increase the risk of consuming too much protein. An occasional amino acid drink in the place of a meal is not a problem. However, problems may arise if you add the supplement to your existing diet. Most Americans receive two to three times the amount of protein required on a daily basis from their existing diets—taking amino acid supplements just adds to the excess. Also, certain amino acids share the same transport systems in the absorption process; therefore, a concentrated excess of one amino acid obtained from a supplement may increase the probability of decreased absorption of another amino acid that uses the same transport system. This could lead to deficiency in the competing amino acid.\nSupplement Claims and Restrictions\nThe Food and Drug Administration (FDA) regulates supplements, but it treats them like food rather than pharmaceuticals. Dietary supplements must meet the FDA’s Good Manufacturing Standards, but are not required to meet the standards for drugs, although some companies do so voluntarily. Also, although supplement manufacturers are allowed to say a particular ingredient may reduce the risk of a disease or disorder, or that it might specifically target certain body systems, these claims are not approved by the FDA. This is why labels that make structural and functional claims are required to carry a disclaimer saying the product is not intended “to diagnose, treat, cure, or prevent any disease.” In addition, in the United States, supplements are taken off the market only after the FDA has proven that they are hazardous. To revisit the topic of structural and functional claims refer back to Chapter 12 “Nutrition Applications”.\nBefore Taking Supplements\nThe phrase caveat emptor means “buyer beware,” and it is important to keep the term in mind when considering supplementation. Just because a product is “natural” does not mean it can’t be harmful or dangerous, particularly if used inappropriately. The following are helpful questions to explore before deciding to take a supplement:\n- Does the scientific community understand how this supplement works and are all its effects well known?\n- Is there proof that the supplement actually performs in the manner that it claims?\n- Does this supplement interact with food or medication?\n- Is taking this supplement necessary for my health?\n- Is the supplement affordable?\n- Is the supplement safe and free from contaminants?\nLastly, please remember that a supplement is only as good as the diet that accompanies it. We cannot overstate the importance of eating a healthy, well-balanced diet designed to provide all of the necessary nutrients. Food contains many more beneficial substances, such as phytochemicals and fiber, that promote good health and cannot be duplicated with a pill or a regimen of supplements. Therefore, vitamins and other dietary supplements should never be a substitute for food. Nutrients should always be derived from food first.\nFood: The Best Medicine\nPoor dietary choices and a sedentary lifestyle account for about 300–600 thousand deaths every year according to the US Department of Health and Human Services. That number is thirteen times higher than the deaths due to gun violence. The typical North American diet is too high in saturated fat, sodium, and sugar, and too low in fiber in the form of whole fruits, vegetables, and whole grains to keep people healthy. With so many threats to optimal health it is vital to address those factors that are under your control, namely dietary and lifestyle choices. A diet that supplies your body with the needed energy and nutrients daily will result in efficient body functioning and in protection from disease. Making sound nutritional choices can also provide support for individuals undergoing treatment for short-term or chronic conditions. Finding a balance between nutritional needs with concerns about drug interactions can hasten recovery, improve quality of life, and minimize the side effects from treatment protocols.\n- Nutrition and Athletic Performance. American College of Sports Medicine.Medicine & Science in Sports & Exercise. 2016; 48(3), 543- 568. https://journals.lww.com/acsm-msse/Fulltext/2016/03000/Nutrition_and_Athletic_Performance.25.aspx. Accessed March 17, 2018. ↵\n- Choosing a Vitamin and Mineral Supplement—Topic Overview. WebMD.com. http://www.webmd.com/food-recipes/tc/choosing-a-vitamin -and-mineral-supplement-topic-overview. Last revised March 11, 2018. ↵\n- Watson S. How to Evaluate Vitamins and Supplements. WebMD.com. http://www.webmd.com/vitamins-and-supplements/lifestyle-guide -11/how-to-evaluate-vitamins-supplements. Accessed March 11, 2018. ↵\n- Why Good Nutrition Is Important. CSPINET.org. http://www.cspinet.org/nutritionpolicy/nutrition_policy.html. Accessed March 9, 2018. ↵']"	['<urn:uuid:cb4bedb3-e351-4e63-9a0e-62a10a7e8d78>', '<urn:uuid:464d33f3-57c8-448b-b25d-495e55823efc>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T03:34:55.284799	19	136	1772
54	Do animal medicines go through the same approval process?	No, medications intended for use in animals are submitted to a different center within FDA, the Center for Veterinary Medicine (CVM) in a New Animal Drug Application (NADA). These are specifically evaluated for their use in food animals and their possible effect on the food from animals treated with the drug.	"['New Drug Application\nThe Food and Drug Administration\'s New Drug Application (NDA) is the vehicle in the United States through which drug sponsors formally propose that the FDA approve a new pharmaceutical for sale and marketing. Some 30% or less of initial drug candidates proceed through the entire multi-year process of drug development, concluding with an approved NDA, if successful.\nThe goals of the NDA are to provide enough information to permit FDA reviewers to establish the complete history of the candidate drug. Among facts needed for the application are:\n- Patent and manufacturing information\n- Drug safety and specific effectiveness for its proposed use(s) when used as directed\n- Reports on the design, compliance, and conclusions of completed clinical trials by the Institutional Review Board\n- Drug susceptibility to abuse\n- Proposed labeling (package insert) and directions for use\nTo legally test the drug on human subjects in the U.S., the maker must first obtain an Investigational New Drug (IND) designation from FDA. This application is based on nonclinical data, typically from a combination of in vivo and in vitro laboratory safety studies, that shows the drug is safe enough to test in humans. Often the ""new"" drugs that are submitted for approval include new molecular entities or old medications that have been chemically modified to elicit differential pharmacological effects or reduced side effects.\nThe legal requirement for approval is ""substantial"" evidence of effectiveness demonstrated through controlled clinical trials. This standard lies at the heart of the regulatory program for drugs. Data for the submission must come from rigorous clinical trials.\nThe trials are typically conducted in three phases:\n- Phase 1: The drug is tested in 20 to 100 healthy volunteers to determine its safety at low doses. About 70% of candidate drugs advance to Phase 2.\n- Phase 2: The drug is tested for both efficacy and safety in up to several hundred people with the targeted disease. Some two-thirds of candidate drugs fail in Phase 2 clinical trials due to the drug not being as effective as anticipated.\n- Phase 3: The drug is typically tested in several hundred to several thousand people with the targeted disease in double-blind, placebo controlled trials to demonstrate its specific efficacy. Under 30% of drug candidates succeed through Phase 3.\n- Phase 4: These are postmarketing surveillance trials in several thousand people taking the drug for its intended purpose to monitor efficacy and safety of the approved marketed drug.\nThe legal requirements for safety and effectiveness have been interpreted as requiring scientific evidence that the benefits of a drug outweigh the risks and that adequate instructions exist for use, since many drugs have adverse side effects.\nMany approved medications for serious illnesses (e.g., cancer) have severe and even life-threatening side effects. Even relatively safe and well understood OTC drugs such as aspirin can be dangerous if used incorrectly.\nThe actual applicationEdit\nThe results of the testing program are codified in an FDA-approved public document that is called the product label, package insert or Full Prescribing Information. The prescribing information is widely available on the web, from the FDA, drug manufacturers, and frequently inserted into drug packages. The main purpose of a drug label is to provide healthcare providers with adequate information and directions for the safe use of the drug.\nThe documentation required in an NDA is supposed to tell the drug’s whole story, including what happened during clinical tests, what the ingredients of the drug formulation are, results of animal studies, how the drug behaves in the body, and how the company manufactures, processes and packages it. Currently, the FDA decision process lacks transparency, however, efforts are underway to standardize the benefit-risk assessment of new medicines. Once approval of an NDA is obtained, the new drug can be legally marketed starting that day in the U.S.\nOnce the application is submitted, the FDA has 60 days to conduct a preliminary review, which assesses whether the NDA is ""sufficiently complete to permit a substantive review."" If the FDA finds the NDA insufficiently complete (reasons can vary from a simple administrative mistake in the application to a requirement to re-conduct testing), then the FDA rejects the application by sending the applicant a Refuse to File letter, which explains where the application failed to meet requirements.\nAssuming the FDA finds everything acceptable, they decide if the NDA needs a standard or accelerated review, and communicates acceptance of the application and their review choice in another communication, known as the 74-day letter. A Standard Review implies an FDA decision within about 10 months while a Priority Review should complete within 6 months. The decision comes in a Complete Response Letter.\nRequirements for similar productsEdit\nBiologics, such as vaccines and many recombinant proteins used in medical treatments are generally approved by FDA via a Biologic License Application (BLA), rather than an NDA. The manufacture of biologics is considered to differ fundamentally from that of less complex chemicals, requiring a somewhat different approval process.\nGeneric drugs that have already been approved via an NDA submitted by another maker are approved via an Abbreviated New Drug Application (ANDA), which does not require all of the clinical trials normally required for a new drug in an NDA. Most biological drugs, including a majority of recombinant proteins are considered ineligible for an ANDA under current US law. However, a handful of biologic medicines, including biosynthetic insulin, growth hormone, glucagon, calcitonin, and hyaluronidase are grandfathered under governance of the Federal Food Drug and Cosmetics Act, because these products were already approved when legislation to regulate biotechnology medicines later passed as part of the Public Health Services Act.\nMedications intended for use in animals are submitted to a different center within FDA, the Center for Veterinary Medicine (CVM) in a New Animal Drug Application (NADA). These are also specifically evaluated for their use in food animals and their possible effect on the food from animals treated with the drug.\n- ""The Drug Development Process"". U.S. Food and Drug Administration. January 4, 2018. Retrieved May 1, 2018.\n- ""The Drug Development Process. Step 4: FDA Drug Review"". U.S. Food and Drug Administration. January 4, 2018. Retrieved May 1, 2018.\n- Commissioner, Office of the. ""Public Health Focus - FDA and Marijuana"". www.fda.gov. Archived from the original on April 28, 2018. Retrieved April 30, 2018.\n- ""The Drug Development Process. Step 3: Clinical Research"". U.S. Food and Drug Administration. January 4, 2018. Retrieved May 1, 2018.\n- ""The Drug Development Process. Step 1: Discovery and Development"". U.S. Food and Drug Administration. January 4, 2018. Retrieved May 1, 2018.\n- Food, Drug, and Cosmetic Act, Section 505; 21 USC 355]\n- 21 CFR 201.5: Labeling Requirements for Prescription Drugs and/or Insulin\n- ""Daily Med:Current Medication Information"". Archived from the original on November 12, 2008. Retrieved October 10, 2007.\n- Liberti L, McAuslane JN, Walker S (2011). ""Standardizing the Benefit-Risk Assessment of New Medicines: Practical Applications of Frameworks for the Pharmaceutical Healthcare Professional"". Pharm Med. 25 (3): 139–46. doi:10.1007/BF03256855. Archived from the original on February 6, 2012.\n- Kathie Clark (December 15, 2009). ""Updates from the Regulators:FDA"". The eCTD summit. Archived from the original on July 16, 2011.\n- ""Merck KGaA Receives Refuse To File Letter From FDA On Cladribine Tablets New Drug Application"". medicalnewstoday.com. Archived from the original on March 5, 2010. Retrieved April 30, 2018.\n- ""Archived copy"". Archived from the original on March 8, 2010. Retrieved February 23, 2010.CS1 maint: Archived copy as title (link)\n- ""Cadence Pharmaceuticals Announces Priority Review and Acceptance of NDA Submission for Acetavance for Treatment of Acute Pain and Fever"". drugs.com. Archived from the original on July 11, 2017. Retrieved April 30, 2018.\n- ""FDA, CDER Office of Generic Drugs"". fda.gov. Archived from the original on May 28, 2009. Retrieved April 30, 2018.\n- ""C&EN: COVER STORY - BEYOND HATCH-WAXMAN"". pubs.acs.org. Retrieved April 30, 2018.\n- Henninger, Daniel (2002). ""Drug Lag"". In David R. Henderson (ed.) (ed.). Concise Encyclopedia of Economics (1st ed.). Library of Economics and Liberty.CS1 maint: Extra text: editors list (link) OCLC 317650570, 50016270, 163149563\n- Chapter 11: Prescription Drug Product Submissions in: Fundamentals of US Regulatory Affairs, Eighth Edition 2013']"	['<urn:uuid:533abac9-692c-499a-87d7-9a087ebf7ea3>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T03:34:55.284799	9	51	1358
55	composition proportion percentage minerals water air organic matter typical soil	Soil is composed of approximately 45% minerals (sand, silt and clay), 20-30% air, 20-30% water and 5-10% organic matter.	['Book Review: Grow Your Soil! by Diane Miessler\nHarness the Power of Microbes to Create Your Best Garden Ever\nStorey Publishing, January 2020\n- Price: $16.95\n- Size: 6 x 9\n- Pages: 176\n- Format: Paperback ISBN: 9781635862072\n- Other formats: Ebook\nGrow Your Soil! is an introduction to soil biology and gardening in eight chapters. It is written as if describing how to build a house (but starting with the roof!). Diane Miessler writes in plain English, with a light style, and her book has the endorsement of Elaine Ingham, who writes the foreword, saying that Diane’s humor and tongue-in-cheek joy make this book a joy to read. People were once told that using inorganic fertilizers and pesticides was the only way to grow enough food for a starving world. Elaine simply states “That was a flat-out lie.”\nDiane’s encouragement to garden in partnership with the soil food web lists the many benefits of a healthy environment, healthy flavorful food, and the satisfaction of doing what you believe is right. She has a ten-point list of suggestions for creating healthy living soil using no-till systems, lots of mulches, home-grown fertilizers, and by encouraging biodiversity. The fundamentals of soil science are explained – soil is about 45% minerals (sand, silt and clay), 20-30% air, 20-30% water and 5-10% organic matter. A teaspoon of good soil contains more microbes than there are people in the US, more species than all the vertebrates on Earth, several yards of fungal hyphae, a few thousand protozoa and several dozen nematodes (mostly good ones). Soil is our planet’s third largest carbon sink (after the oceans and fossil fuels). Healthy soil is continually pulling carbon dioxide from the air and sequestering it in the organic matter and humus. We want to have as much sequestered carbon as possible, both to reduce the amount in the atmosphere and so that we can use it to grow food.\nDiane’s mulch recommendations are to generally aim for a mix of one-third green matter (which feeds bacteria) and two-thirds brown (which feeds fungi), but steering towards more green matter for annual vegetables, more brown for woody perennials, in line with the predominant life-form each type of crop does best with.\nThe cover crops section first describes the plants, then how and when to use them. I had a brief worry that people would go out and plant buckwheat or sweet potatoes in winter, until I read on! In fact, Diane does suggest you can sow buckwheat whenever you like, and it will be dormant until the right spring weather occurs. In our central Virginia climate this does not work. Buckwheat seed rots in cold wet soil. Buckwheat can germinate in a warm early spring spell and be struck down by a following frost before it has made much growth at all. As always, it pays to discuss ideas you haven’t tried before with nearby gardeners.\nThis book has a good basic description of the Soil Food Web, for new gardeners or anyone who is a bit mystified about what’s happening in the soil. And for those over 50 whose biology classes only included the two plant and animal “kingdoms”, here are explanations of the classes of bacteria, fungi and archaea, the main types of soil microbes. Archaea are neither bacteria nor eukaryotes (tiny organisms that have their DNA in a nucleus). Archaea are similar to eukaryotes in some ways, but have more resistance to extreme conditions. In the soil they work as decomposers.\nNext up are the algae, protozoa and nematodes. The algae spectrum goes from one-celled photosynthesizing life-forms to giant kelp. In the soil they provide nutrients and increase plant resistance to diseases. Protozoa are one-celled animals, which release excess nutrients from their meals of bacteria and fungi, in a plant-available form. They help balance the numbers of bacteria in the soil. Nematodes are (mostly) microscopic roundworms that are mostly benign, from our perspective, and healthy populations keep the destructive nematodes in check. Arthropods (including insects, spiders, mites, ticks and scorpions) are shredders of organic matter in the soil (while eating smaller life-forms).\nBigger soil-dwellers include worms, slugs, snails, and small mammals. By the way, Diane explodes the myth that coffee grounds can control slugs, and claims to have videos to prove it untrue. And she tells us that fence lizards eat harlequin bugs. (I think she lives in California). Western fence lizards are centered in California, and according to the National Wildlife Federation, Eastern fence lizards are found between New York and northern Florida and as far west as Ohio and Arkansas. I want some!\nThe next section of the book explains Cation Exchange Capacity (CEC), a measure of how many positively charged ions (cations, nutrients like Mg, K, Ca, ammonium) can be held by the negatively charge soil particles. Diane likens this to the pantry. Soils with a low CEC can’t hold many cations, and the key to increasing the CEC is to increase the soil organic matter content. Clay soils may have a high CEC, but the nutrients may be held too tightly to be useful to plants. The solution to this problem is also to increase the soil organic matter content.\nDiane offers several ways to increase the organic matter, and one of her favorites is biochar. Biochar in its original form is more or less sterile, not nutritious at all, but in the soil it can act like humus on steroids – it is very good at absorbing water, hosting microbes, reducing plant diseases and lasting a long time in the soil. I have been skeptical about some of the claims for biochar, and of the net gains in reducing global heating. Diane does not make any wild claims (she’s not selling the stuff). She is open about the fact that the mechanism for suppressing disease is not yet understood.\nAs I said, Diane is not selling biochar. In fact she describes how to make your own on a small scale with an “upside-down” outdoor fire (with all due safety precautions). Big pieces of wood are arranged on the ground in an open airy stack, and a small fire is lit on top with tinder and kindling. This means the fire produces little smoke (all smoke is air pollution). The fire is thoroughly doused with water once everything is glowing but not flaming. Those wanting to make biochar on a bigger scale are referred to a double-barrel biochar burner on YouTube.\nThe next section is on photosynthesis, minerals and soil testing. Diane describes the effects of too much, too little and just right amounts of the main soil nutrients first. A deficiency of phosphorus shows up as blue-purple colors on the older leaves. She doesn’t mention phosphorus surplus, although she does confirm that excess phosphorus added to the soil will usually be locked up and become inaccessible to plants. Potassium deficiency can cause yellow leaf edges. Next up are other macro-nutrients, such as Calcium, magnesium and sulfur. Calcium deficiency leads to stunted new growth, brown around the edges, perhaps with yellowing between the veins. Bulb and fruit formation can be damaged, as with blossom end rot of tomatoes, caused by insufficient calcium reaching the fruits. By contrast, a magnesium deficiency leads to older leaves becoming yellow between the veins and around the edges, perhaps with purple, reddish or brownish discoloration. Sulfur shortage can lead to “unthrifty” plants. Shortages of any of these can be remedied by the addition of more organic matter.\nMicronutrient shortages can also be helped by organic matter, although in Virginia I have noticed that we do sometimes need to add boron on its own (in tiny amounts).\nDiane describes how to test soil, understand the results, and remedy the situation. Try adding organic matter first, and only tinker with the specifics if the general remedy is not enough. For instance, if your soil biological activity is low, you may find that piling on organic matter doesn’t help. Use compost to add some more life to the soil and get a better balance of diners to dinners. There is a helpful one-page “Order of Operations for Fixing Soil”: Correct the pH; correct the calcium level; correct any excesses (usually by adding gypsum); correct the macronutrient deficiencies and lastly correct the micronutrient and trace element deficiencies. Clear instructions like this are so valuable to newer gardeners!\nThere is a chapter on making compost and compost tea. She suggests thinking of compost as a sourdough starter, and mulch as the flour. Both are valuable, and they work well together. Making good compost is a valuable skill to learn. Try for the a good balance of high nitrogen materials and high carbon materials, with enough water. Turn the pile, assess its progress, add what it seems to need. Rinse and repeat. Diane recommends against spending money on fancy compost bins. “Compost needs love, not a container.” There is value in turning the pile and seeing how it’s doing. If it’s fully enclosed in a tumbler, you might miss the signs that it needs a specific kind of care. Here is encouragement to learn the art and science of compost making.\nWorm bins are a great way to use kitchen scraps to produce worms and compost, especially in winter, as worm bins need to be in a non-freezing place to stay alive. I disagree with Diane about using the liquid leaching from the bottom of the bin as a “compost tea” See my review of The Worm Farmer’s Handbook by Rhonda Sherman. This liquid might not be good for your plants. To make compost tea, put some of the wormcastings in water and bubble air though it. Instructions are in Diane’s book a few pages later.\nAnother small industrious worker is the black soldier fly. The (harmless) maggots of these (harmless) flies will out-compete other (disease-carrying and/or biting) flies in eating up kitchen scraps in an odorless way. They are also a favorite food of poultry, and there are clever ways of setting up a bsf bin so that the pupal stage will “self-harvest” by walking up a ramp and dropping into a collecting box. See YouTube for all the details.\nAfter explaining these various aspects of growing good soil, Diane pulls everything together into a chapter on Building a Garden That Feeds Itself. Here you can learn about sprinkler irrigation, mulching, planting, and selecting good tools. The next chapter covers being a good neighbor, by having a good-looking, good-smelling, productive garden that gets frequent attention. Diane advocates for pulling weeds and dropping them on the bed, without worrying about weed seeds or plant diseases. I can see this would work best in a smaller garden where things don’t get out of control, and in drier climates with fewer diseases and less chance for weeds to re-root. There’s a panel about roses that I didn’t read. (Roses are a great trap crop for Japanese beetles; I’m not a flower grower!) A big help to beginners is the glossary at the end, and the bibliography of books on soil life.\nIf you are a beginner organic gardener, or you’re looking for a book for someone in that category, this book has a clear user-friendly approach. It won’t scare off newbies with too much detail.']	['<urn:uuid:24db52df-5066-4fa4-b79c-4fea0e649ed2>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-13T03:34:55.284799	10	19	1879
56	How do coffee farming practices impact migratory bird conservation?	Coffee farming practices significantly impact migratory bird conservation. Traditional shade-grown coffee farms provide crucial habitat for one-third of U.S. breeding migratory birds during winter months, supporting species like ruby-throated hummingbirds, gray catbirds, and Baltimore orioles. However, the conversion to sun-coffee farming has devastated bird populations, with studies showing a dramatic decrease from 10 to 4 common migratory species in sun-grown farms. The solution involves supporting shade-grown, organic, and fair trade coffee, which maintains natural habitats and helps ensure the long-term survival of migratory songbirds.	"['When the migrants return home to the tropics, the challenges are even more daunting, for in many places their home habitat has been devastated. Can we do anything to help? Scott Weidensaul in his comprehensive summary of bird migration, Living on the Wind (North Point Press, 1999), summarizes in this way: “So many ecosystems are under assault in the tropics, so many seemingly inexorable pressures are working against conservation, that it is easy to despair. But one of the more intriguing ways to save migratory songbirds, and many tropical plants and animals with which they coexist, may also be the simplest: Have a cup of coffee. Strangely enough, this global addiction is both responsible for considerable environmental destruction and capable of reversing some of the damage.”\nNow the bad news. In 1970, a fungal blight appeared in Brazil. Panicked farmers, with government encouragement, switched from shade-tolerant varieties to a dwarf coffee shrub which grows well in full sun. However, these sun-coffee farms are biological deserts; researchers found 90 percent fewer bird species on such farms. Weidensaul summarizes: “The coffee bushes grow in neat, orderly rows, packed close together and devoid of tree cover. Deprived of companion plants and organic mulch that foster soil fertility and prevent erosion, the farms must be augmented with synthetic fertilizers, and the coffee shrubs ... must be soaked with liberal applications of insecticides, herbicides, and fungicides, usually applied by workers with little training or protective clothing.”\nThe United States is the bastion of free market economy. You can be a good capitalist, an environmental activist, an advocate of social justice for farm laborers, and a birder committed to species protection by insisting that the coffee you buy is shade-grown coffee. Some of those terms may seem contradictory or incompatible. If you are offended by being a capitalist, focus on the activist and advocacy - or vice versa, if you prefer.\nThe fact is that how the beans in our morning cup of coffee were grown is more important to the long term diversity of our local bird life than the many dollars we spend on squirrel-proof bird feeders (there is no such thing) and those bags of bird seed. And don’t tell me you love watching the birds at your feeder if you are drinking cheap supermarket coffee out of a can. You are indulging yourself without being responsible.\n2) “certified organic” - grown without the use of agrochemicals. Yields are lower than for standardly grown coffee, but not substantially so. It is also much safer for the poor laborers working the coffee plantations.\n3) “certified fair trade” - With fair trade certification, coffee farmers band together into cooperatives and receive a set price for their coffee. The cooperatives must be democratically run and not practice discrimination.\nLast week, I was in Philadelphia. I went to a large chain supermarket in an old working-class neighborhood to buy coffee. I was surprised to find a large section of the coffee aisle devoted to organic, shade-grown, fair trade coffee. Our local supermarkets don’t begin to compare to the offering I found in Philadelphia. Here in southern Vermont, we pride ourselves in being “green,” eco-conscious, and socially responsible, but we have been lazy toward our local coffee retailers.\nI harbor hopes that in a few years I will be able to take grandchildren into the woods for lessons in the language and life of birds. I hope they will be confused, initially, by the abundance of the songs and sounds they hear - rather than easily learning a few scattered songs. That initial confusion will mean that the neotropical migrants are surviving their long journeys and finding good habitat along the way.\nI hope I can give grandchildren the pleasure of good birding. In a small way, I going to help by having a cup of coffee - shade-grown coffee, organic, and fair trade. Please join me.', 'Migratory Birds Favor Shade-Grown Coffee\nSince its (apocryphal) discovery in East Africa by a shepherd who watched his goats joyfully cavort across the pasture shortly after eating the red berries from an unassuming shrub, coffee traditionally has been grown in the shade, under a canopy of trees offering habitat to a variety of avian species. However, in the early 1970s, ""shade-grown"" coffee began to give way to coffee grown under full sun, a transformation that would negatively impact numerous migratory and resident birds.\nAn estimated one-third of the migratory birds that have breeding grounds in the U.S. are believed to seek solace in the warmer climes of Mexico, Latin America and the Caribbean during the unforgiving winter months of the north. Many will make their homes in shade-grown coffee farms. These farms provide an attractive environment for scores of bird species, be they year-round avian residents such as toucans or parrots, or migratory birds such as ruby-throated hummingbirds, gray catbirds, Baltimore orioles or the cerulean warbler, a species of bird listed as vulnerable by the International Union for Conservation of Nature (IUCN).\nHybrid, sun-tolerant dwarf coffee plants were introduced to produce bigger yields, and immediately became popular among many coffee growers. However, clear cutting to make room for the dwarf varieties has taken its toll on the forest environment and its inhabitants. Though research is sparse on the full impacts of sun-grown coffee plantations on migratory birds, the Smithsonian Migratory Bird Center (SMBC) indicates that ""[T]he few studies that have been conducted have found that the diversity of migratory birds plummets when coffee is converted from shade to sun. One study found a decrease from 10 to 4 common species of migratory birds."" The SMBC adds: ""As for the overall avifauna, studies in Colombia and Mexico found 94-97% fewer bird species in sun-grown coffee than in shade-grown coffee.""\nThe shaded farms not only protect birds from harsh weather, they also harbor orchids, ferns, and lichens upon which some birds nest, and provide food in the form of fruit and insects. In return, birds aid the coffee farmers by consuming pests harmful to coffee plants and their berries, such as the coffee berry borer, considered the most destructive insect to sun-grown coffee farms. The sun-filtering canopy of a shade-coffee plantation also helps keep soil moist and therefore less susceptible to erosion. It combats global warming by filtering carbon dioxide. Unlike sun-grown coffee, shade-grown coffee does not depend on heavy inputs of environmentally unfriendly fertilizers, herbicides, or pesticides - relying instead on leaf litter to provide nutrients and inhibit weed growth, and providing habitat for other pest-removing animals such as frogs, spiders and ants.\nA number of coffee growers now participate in third party coffee certification processes which verify that the coffee has been produced in accordance with established socially and environmentally responsible standards, such as ""organic,"" fair trade,"" and ""shade-grown."" Conscientious consumers seeking to preserve habitat for resident and migratory birds would do well to seek out coffee bearing SMBC’s own organic, shade-grown ""Bird Friendly"" label.']"	['<urn:uuid:3bf09751-30cf-460f-adba-930520d1d44b>', '<urn:uuid:1a43d597-c1df-40a0-9cee-6f6ced96e576>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T03:34:55.284799	9	84	1154
57	I've read conflicting accounts about the Shahnama's themes - some say it's purely historical while others claim it's mostly mythology. As a literature student, I'd like to understand what kinds of narratives it actually contains.	The Shahnama contains both mythical and historical elements. It begins with mythical history, starting from the creation of the universe and the first king. However, it also incorporates real historical events, though these are treated in a non-historical manner. For example, it includes sections about Alexander the Great's arrival in Iran and about the last independent Persian dynasty before Islam. In the case of Alexander the Great, he is portrayed as a great king who becomes Persian and embarks on a quest to discover the truth about eternal life, transforming him into both a historical figure and a vehicle for exploring moral questions about death and immortality.	['|Artist / Origin||\nUnknown artist, Tabriz, Iran\nRegion: West Asia\nPeriod: 1000 CE - 1400 CE\n|Material||Opaque watercolor, gold, and ink on paper|\n|Dimensions||H: 23 ¼ in. (59.05 cm.), W: 15 5/8 in. (39.69 cm.)|\n|Location||Museum of Fine Arts, Boston, MA|\n|Credit||Courtesy of Bridgeman Art Library|\n|Oleg GrabarProfessor Emeritus, Institute for Advanced Study|\n“Alexander Fights the Monster of Habash” from the Shahnama\nThe Shahnama was written around the year 1000 by a poet named Ferdowsi, who was born and lived most of his life in what is now Northeastern Iran. It is the most celebrated epic poem in Iran. It deals with the mythical history of a country and with the real history a country. For instance, it has several sections on Alexander the Great who came to Iran, or in the history of the last independent Persian dynasty before the appearance of Islam. Those are historical events. They are treated in a non-historical manner, but they are historical. But most of the poem deals with the mythical history that begins with the creation of the universe and the first king appears. Alexander the Great is a great king who becomes a Persian, and he goes to the end of the earth in the Shahnama to find out the truth about eternal living. And at the end of the earth, there is a tree, and on that tree there were figures that spoke to Alexander the Great. And he asked them, ‘Am I going to live forever?’ They said, ‘No, you are going to die like everybody else.’ And that saddens him and he goes away and dies. But the point is that he was transformed into a figure both of history and of moral issue. How do you deal with death? How do you deal with the ambition of eternal life, versus the reality of everybody’s death? Ferdowsi wrote in extremely accessible Persian. People knew it by heart. It is the memory of a very broad ethnic group and depending on the provinces from which you came, there were variances to the story.\nIllustrations of the Shahnama begin around 1300 when Iran was ruled by Mongols. From that moment on, they were constantly illustrated. There are many great manuscripts of Shahnama. One is a manuscript done around 1335, of which about sixty illustrations have remained. The painters wanted you to feel that the past was beautiful, that it was great, that the world was terrific. When you look at these paintings you immediately recognize the subject matter—a love scene, a fight, a battle, whatever it is. And you recognize all the people around, almost all of them. But then you realize that they never could sit or be placed the way they are. The setting makes you accept this as being real, and there is something very operatic about Persian painting. It’s all fake. The same people reappear, the same choruses reappear, from one scene to another scene, one story, another story, and are constantly there to do whatever those subject demands. And that’s a very original feature of that art.\nAfter Khomeini took power in ’79, the Shahnama was banned in Iran, because it was a poem to the glory of kings, and not of the faith. Now it is back in some popularity.”']	['<urn:uuid:db7c7f80-55f3-4fc2-8968-0cec1d3cb045>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T03:34:55.284799	35	107	546
58	pain relief time pelvic fracture kyphoplasty	For pelvic fractures, healing time varies by age and health and may take several months, while with kyphoplasty, most patients report significant pain reduction within 48 hours.	"[""A pelvic fracture is defined as one or more breaks, also known as fractures, of the bones that make up the pelvis. Several organs, blood vessels, and nerves are located in this area. Because of this, a pelvic fracture is a serious injury that needs immediate care to prevent current and future complications.\nCopyright © Nucleus Medical Media, Inc.\nPelvic fractures are caused by:\nFactors that may increase your chance of a pelvic fracture include:\nA pelvic fracture may cause:\nYour doctor will ask about your symptoms and medical history. A physical exam will be done to assess the extent of your injury. You may be referred to a doctor who is a trauma specialist and/or a doctor who is a bone specialist.\nTests may include:\nImaging tests can evaluate the pelvic region and surrounding structures. These may include:\nA pelvic fracture is a serious injury that may be complicated by injuries to other parts of the body. Proper treatment can prevent long-term complications. Treatment will depend on how serious the fracture is, but may include:\nInitial treatment focuses on managing life-threatening problems, such as bleeding or shock. The fracture may be held in place with a sheet wrap or an external fixation device. With an external fixation device, screws are inserted through the bones and connected to a frame on the outside of the body.\nTraction may be used realign and stabilize the fracture if surgery can't be done right away.\nStable fractures will heal without surgery. Unstable fractures are treated with surgery. Some fractures can be set with an external fixation device. Others may require repair with internal pins, screws, or plates.\nExtra support may be needed to protect, support, and keep the pelvic bone in line while it heals. A walker or crutches may be needed to keep weight off the pelvis.\nPrescription or over-the-counter medications may be given to help reduce inflammation and pain. Blood thinners reduce the risk of blood clots.\nHealing time varies by age and overall health. Young people and those in better overall health heal faster. It may take several months for an unstable fracture to heal.\nComplications of a pelvic fracture can be temporary or permanent. These include:\nActivities will need to be adjusted while your pelvic bone heals, but complete rest is rarely required.\nPhysical therapy or rehabilitation will be used to improve range of motion.\nTo help reduce your chance of a pelvic fracture:\nThe American Pediatric Surgical Association\nOrtho Info—American Academy of Orthopedic Surgeons\nCanadian Orthopaedic Association\nWomen's Health Matters\nCollinge C, Tornetta P III. Soft tissue injuries associated with pelvic fractures. Orthop Clin North Am. 2004;35(4):451-456.\nFemoral shaft fracture—emergency management. EBSCO DynaMed Plus website. Available at:http://www.dynamed.com/topics/dmp~AN~T910561/Femoral-shaft-fracture-emergency-management. Accessed August 30, 2017.\nFracture management of unstable pelvic fractures. American Association of Orthopaedic Surgeons website. Available at: http://www.aaos.org/news/aaosnow/jul09/clinical8.asp. Accessed August 30, 2017.\nFrakes MA, Evans T. Major pelvic fractures. Crit Care Nurse. 2004 Apr;24(2):18-30.\nGrotz MR, Allami MK, Harwood P, et al. Open pelvic fractures: epidemiology, current concepts of management and outcome. Injury. 2005;36(1):1-13.\nMcCormack R, Strauss EJ, et al. Diagnosis and management of pelvic fractures. Bull NYU Hosp Jt Dis. 2010;68(4):281-291.\nMohanty K, Musso D, Powell JN, Kortbeek JB, Kirkpatrick AW. Emergent management of pelvic ring injuries: an update. Can J Surg. 2005;48(1):49-56.\nPelvis fractures. American Academy of Othopaedic Surgeons Ortho Info website. Available at: http://orthoinfo.aaos.org/topic.cfm?topic=a00223. Updated February 2016. Accessed August 30, 2017.\nQuick TJ, Eastwood DM. Pediatric fractures and dislocations of the hip and pelvis. Clin Orthop Relat Res. 2005;(432):87-96.\nTornetta P III, Templeman DC. Expected outcomes after pelvic ring injury. Instr Course Lect. 2005;54:401-407.\nLast reviewed August 2017 by EBSCO Medical Review Board Warren A. Bodine, DO, CAQSM Last Updated: 8/29/2014"", 'Nonsurgical Treatments for the Pain of Spine Fractures Caused by\nFor people who have already experienced spinal fractures caused by\nosteoporosis, there have been few effective treatments available until\nrecently. Now, two safe, nonsurgical, interventional radiology\ntreatments offer hope for those suffering from the pain of\nvertebral fractures—vertebroplasty and kyphoplasty.\nVertebroplasty is an outpatient procedure using X-ray\nimaging and conscious sedation. The interventional radiologist inserts a\nneedle about the width of a small straw through a nick in the skin of\nthe back, directing it under fluoroscopy (continuous moving X-ray\nimaging) into the fractured vertebra. The physician then injects the\nmedical-grade bone cement into the vertebra. The cement hardens,\nstabilizes the bone and prevents further collapse. This stops the pain\ncaused by bone rubbing against bone.\nKyphoplasty is a slightly more involved technique for\ntreating painful compression fractures of the spine. In this procedure,\na balloon is inserted into the collapsed vertebral body and inflated,\npushing the bones back to your natural height. Once the balloon is\ndeflated and removed, bone cement is then injected into the cavity\ncreated by the balloon to stabilize the bone.\nKyphoplasty is usually performed under sedation or general anesthesia,\noften with an overnight hospital stay. Your interventional radiologist\nmay recommend kyphoplasty rather than vertebroplasty for treatment of\nyour recent compression fracture, as kyphoplasty has been shown to\nrestore some height to the vertebra involved and may allow the bone\ncement to be delivered with greater precision.\nBenefits of the Procedures\nBecause vertebroplasty and kyphoplasty are a minimally invasive\nNo surgical incision is needed, just a small nick in the skin\nNo stitches are required\nIn the case of vertebroplasty, no hospitalization or general\nanesthesia is required\nMost patients report that their pain is gone or significantly better\nwithin 48 hours and that they return to normal activity shortly after\nthe procedure. Studies have shown that from 75 to 90 percent of people\ntreated with vertebroplasty will have complete or significant reduction\nof their pain.\nCauses of Spine Fractures\nSpinal fractures, also called compression fractures, are caused by a\nweakening of the bone in the spine. A fracture occurs when the weakened\nvertebrae of the spine collapse, usually in the middle (thoracic) or\nlower (lumbar) spine.\nIn older adults, osteoporosis—a progressive weakening of the bone over\ntime—is the primary cause of spinal fractures. Fractures typically occur\nin women over the age of 60 who are affected by osteoporosis. Spinal\nfractures can also occur among younger people whose bones have become\nfragile due to the long-term use of steroids or other drugs to treat\ndiseases such as lupus, asthma and rheumatoid arthritis.\nSymptoms and Diagnosis\nOsteoporosis is called a ""silent disease,"" because bone loss occurs\nwithout symptoms. People may not know they have osteoporosis until their\nbones become so weak that a simple strain, twist of the body, bump or\nfall causes a bone fracture. The most common site of fracture is in the\nvertebrae, the bones that make up the spinal column.\nInitially, the fracture may feel like severe back pain. However, when\nmore than one vertebra collapses, a loss of height or spinal\ndeformities, such as kyphosis or ""widow\'s hump,"" may result. For some\nindividuals, the fracture stabilizes itself and the pain goes away. But\nfor many, the pain persists because the crushed bone continues to move\nand break. For many people with osteoporosis, a spinal fracture means\nseverely limited activity, constant pain and a serious reduction in the\nquality of their lives.\nThe symptoms of osteoporosis may resemble other bone disorders or\nmedical problems, so always consult your physician for a diagnosis. In\naddition to a complete medical history and physical examination,\ndiagnostic procedures for osteoporosis may include a family medical\nhistory, X-rays, a bone density test and blood tests.\nFactors that increase the likelihood of developing osteoporosis include:\nA family history of osteoporosis\nPostmenopausal or an abnormal absence of menstrual periods\nAnorexia or bulimia\nA diet low in calcium\nLong-term use of corticosteroids or anticonvulsants\nFor more information, call 1.866.CALL.MLH.']"	['<urn:uuid:32503a0b-5c5f-40c2-a434-c4d59f91fe8b>', '<urn:uuid:41bd226a-ab94-479e-a47c-2c311e6bdbbd>']	factoid	direct	short-search-query	similar-to-document	comparison	novice	2025-05-13T03:34:55.284799	6	27	1282
59	Which requires more frequent professional monitoring: dental or general health?	Dental health requires more frequent professional monitoring than general health screenings. The ADA recommends dental examinations and cleanings every six months for all adults and children, as even regular brushing cannot remove all plaque buildup. In contrast, general health screenings like eye examinations are typically recommended less frequently - once every two years for most people, or annually for those at high risk. Both types of monitoring are preventive measures, but dental care requires more regular professional intervention to maintain optimal health.	['What are ways to stay pain and disease free in life?\nA healthy person is a productive, happier, and less-stressed person. However, poor lifestyle choices can often rob someone of their health. Read on to identify five ways to maintain good health and live pain- and disease-free life.\nHealthy Diet Plan\nPoor nutrition is directly linked to poor health and a less lifespan. A healthy diet plan involves a well-balanced diet that prevents nutritional diseases like iron deficiency. A healthy meal plan lowers the risk of lifestyle diseases such as diabetes and sleep apnea associated with obesity. It is mainly caused by consuming too much sugar-and-fat-filled food that is also low on fiber. Alternatively, a poor diet is responsible for eating disorders such as bulimia and anorexia. To develop a healthy relationship with food, create a meal plan consisting of enough water, fruits, and vegetables. If alcohol is part of your diet, partake responsibly.\nModerate Alcohol Consumption\nExcessive alcohol consumption can be harmful to your health. Drunk driving impairs drivers’ motor and cognitive skills, thus endangering the driver’s life, their passengers’, and that of other road users. In 2018, driving under the influence (DUI) contributed to nearly 36% of all traffic-related deaths in the state of Alaska. Irresponsible drinking can also lead to addiction. However, when taken in moderation, alcohol improves digestion and raises high-density lipoproteins (HDL) levels, which prevent heart disease.\nAdequate physical activity is not only good for mental well-being, but also for physical health. Working out improves cognitive functions and promotes the release of endorphins and feel-good hormones that create a euphoria that relieves anxiety. Working out improves cardiovascular and respiratory movement. Intensive exercises elevate the heart rate to pump blood faster to keep up with work output. The lungs also work better at supplying oxygen, preventing cardiovascular conditions such as coronary heart disease and arrhythmia.\nDuring exercise, your body burns more calories than at rest. Yet, the metabolic rate at rest is still enhanced after a workout as the body uses energy to repair muscles and replenish glycogen levels used during exercise. Incorporating balance and muscle strengthening activities in your training can improve your motor skills and reduce the risk of falling.\nMaintain Proper Posture\nPoor posture has adverse health effects such as scoliosis, back pain, and tension headaches. On average, 20% of people suffering from acute low back pains advance into persistent chronic low back pains at 12 months. Stresses on posture include carrying weight, sitting or standing in an awkward position, and gravity. Poor posture causes wear and tear in your joints and ligaments, increases the likelihood of injuries, and makes some organs like the lungs function less efficiently. Ways to improve posture include:\n- Sit upright and avoid slouching\n- Use ergonomic aids such as laptop stands\n- Proper gait while walking\n- Sleeping on your back or side with neck support\nRegular Health Check Up\nDoctors advise preventive health checkups to understand the risk factors and treat diseases that do not show symptoms. In ophthalmology, checkups are crucial to identify early-stage eye problems and treat them accordingly. In the United States, 93 million grown-ups are susceptible to blindness. Yet, only 50% of these have been to an optician within the last year. A routine eye examination should be scheduled once every two years and annually for people who are at high risk. Predisposing factors of eye problems include diabetes, old age, and history of hereditary eye sickness.\nBy catching a disease early, health screening saves invasive treatment one would have undergone had the disease progressed. One also maintains a good relationship with their physician, with whom you can discuss health matters that you may be uncomfortable discussing with anyone else.\nThere are health benefits to drinking alcohol, like improved heart health, but only if taken in moderation. Exercise, too, improves heart health. Routine health screening is vital to catch diseases before they get out of hand. Creating a meal plan allows you to stay on top of your weight, thus preventing obesity.\nDiscussion about this post', 'Prevention is Key – Routine dental exams & Hygiene visits twice per year\nAn ounce of prevention…The best way to maintain a healthy mouth and to save money on dental care is to have routine comprehensive dental exams. The American Dental Association (ADA) recommends that adults and children have a dental exam approximately every six months.\nWhat is done in a routine dental exam?\nAt a routine exam (dental checkup) we examine the outside of your mouth (including the TMJ or jaw joint), and the inside of your mouth. We check to make sure that the tissues in your mouth are healthy, and we examine your gums to check for gum disease and/or periodontal disease. And of course, we check your teeth to make sure that all of your existing crowns and fillings are intact, and to make sure that you don’t have any new cavities present. And as part of our intraoral exam, we perform an oral cancer screening.\nA Cleaning Every Six Months is Critical\nThe ADA recommends that you have a dental cleaning and exam every six months.\nBrushing alone isn’t enough\nMany people believe that they don’t need to have dental cleanings because they brush and floss their teeth regularly. And others think that they can skip brushing and flossing and that having their dental cleaning will “fix everything.” In fact, both assumptions are wrong.\nAs much as you brush and floss at home, you will get a buildup of plaque and hardened plaque (calculus) that you simply cannot remove with a toothbrush. Some people have light amounts of this type of buildup, and other people have extensive buildup. Regardless of the cause, only a professional cleaning can remove this type of debris on your teeth.\nAnd if you don’t brush and wait until your cleaning to care for your teeth, the chronic inflammation you will likely have can lead to serious problems\nWhat if I don’t have regular dental cleanings?\nIf you don’t have regular dental cleanings or if you don’t take care of your teeth in between cleanings, you may wind up losing teeth. You won’t lose teeth right away, but not caring for your teeth over a long period of time can lead to chronic inflammation and weakening of the gum tissues that hold the teeth in place. This condition is called periodontal disease (gum disease), and in many cases, it’s preventable.\nDo I need x-rays at every visit?\nGenerally speaking, no. We do take a full set of x-rays the first time you see us as a patient, and after that we recommend a full set every 3-5 years. And although the levels of radiation used in dental x-rays are incredibly low (we use digital x-rays for the lowest exposure possible), we do need to take routine check-up x-rays when indicated.\nPatients who have a history of extensive decay, or with conditions that can lead to decay such as dry mouth (xerostomia), often need x-rays more frequently than patients who have never had a cavity, and who have normal flow of saliva.\nContact us today to schedule your exam & cleaning – it’s never too late to get a fresh start with your oral home care.']	['<urn:uuid:199d4840-41f7-406c-a406-af59855430fc>', '<urn:uuid:460e6445-c217-4f9c-8a3e-2237b5c2fb02>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-13T03:34:55.284799	10	82	1208
60	bottom up top down proteomic approaches comparison protein fragments quantification methods differences	The fundamental division in proteomics experiments is between top-down and bottom-up approaches. Top-down approaches, like 2D gel electrophoresis, attempt to quantify entire intact proteins. In contrast, bottom-up approaches break proteins into smaller fragments, typically proteolytic peptides, and then use algorithmic methods to reconstruct protein identities. For quantification, methods like TMT isobaric mass tagging can be used in bottom-up approaches to enable concurrent identification and quantitation of proteins from multiple samples through tandem mass spectrometry.	['Proteomics refers to a collection of methods used to discover and quantify the abundance and state of proteins in cells and tissue. Given that proteins are the primary drivers of cellular function, understanding which proteins are present and how they relate to one another can provide powerful insight into cell and systems biology. Likewise, analyzing proteins as biomarkers has proven useful for diagnosing disease and stratifying patients for personalized medicine.\nThe most basic division among proteomics experiments is the use of top-down versus bottom-up approaches. Top-down approaches, such as 2D gel electrophoresis, attempt to quantify entire proteins. In contrast, bottom-up approaches break proteins into small fragments, often proteolytic peptides, and algorithmically reconstruct protein identities.\nA second important distinction is between “discovery” and “targeted” proteomic methods. Discovery proteomics addresses global changes in the relative abundance of multiple proteins; whereas targeted proteomics precisely quantifies a priori selected proteins or protein isoforms, e.g. phosphorylated proteins.\nThis article primarily focuses on data acquisition using mass spectrometry (MS). MS-based approaches are the most widely used in the research community and the technology is rapidly evolving.\nLikely, the most commonly used discovery proteomics method is so called “shotgun” proteomics which is used to identify multiple proteins in a mixture of unknown composition, e.g. cell lysate. The protein mixture is digested with proteolytic enzymes, typically trypsin. The resulting mixture of peptides is separated according to their relative hydrophobicity using reverse phase HPLC and sequentially eluted directly into a mass spectrometer. In the mass spectrometer, the vaporized peptides are ionized and focused through a series of electromagnetic chambers (traps). In a typical MS-MS-type experiment, peptides are electromagnetically separated according to their mass charge ratio (m/z) and ion intensity is quantified, e.g. by a PMT. Thus, the signal is recorded as a three dimensional chromatogram of ion intensity, peptide m/z, and elution time (MS1 scan). The acquisition software selects the most abundant ions and then refills the trap with peptides in a selected window of m/z. The peptides are bombarded with energized gas to randomly fragment the peptide. Since proteins bonds are weakest between amino acids, this creates a series of fragment peptides of varying length. The intensity of the fragment ions is then quantified (MS2). The data from the MS2 scans are used to determine the identity of the peptide, typically by matching the m/z profile of the various fragment ions to the expected fragmentation patterns from an in silico digested library of proteins (see data analysis below). Peptide abundance is generally calculated using the area under the curve for the elution profile of the precursor peptide in the MS1 scan. This acquisition scheme is called “data-dependent” because the data from the MS1 scan is used to determine which peptides are analyzed in the MS2 scan.\nTargeted proteomics or data-independent acquisition takes a somewhat different approach where individual peptides are selected ahead of time (often from separate preliminary discovery proteomics experiments). While the starting material and separation methods are similar, the instrument is programmed to look for peptides with a particular m/z. Synthetic peptides matching the peptide of interest are spiked into the sample for use as an internal reference standard. Peptide abundance is calculated from MS1 data. More about targeted approaches to come.\nNon-Mass Spectrometry Based Methods\nIn addition to mass spectrometry, other methods such as antibody microarrays and multidimensional polyacrylamide gel electrophoresis can be used to describe and quantify complex protein mixtures. These techniques are typically lower cost and may be better (particularly relative to shotgun proteomics) at quantifying proteins with relatively low abundance. However, targeted MS methods are largely supplanting these older methods except in specialized cases.\nMore content to come!\n- Whole cell proteomics\n- Analysis of post-translational modifications\n- Kinase profiling\n- Identification of protein-protein interactions\n- Protein identification\nMore content to come!\n- Ion trap\n- Triple quad\nMore content to come!\nA wide variety of commercial and open source software are available. Some of the more commonly used packages are list below:\n- Proteome Discover\n- Protein Pilot\n- SEQUEST Sorcerer\n- MaxQuant (See also the documentation for running MaxQuant on the Gizmo cluster)\nUpdated: June 11, 2020Edit this Page via GitHub Comment by Filing an Issue Have Questions? Ask them here.', 'Amine-reactive Thermo Scientific TMT* Isobaric Mass\nTagging Kits and Reagents enable quantitative tandem\nlabeling of proteins extracted from cells and\ntissues for identification and analysis by mass\nEach isobaric Tandem Mass Tag* Reagent is composed of an amine-reactive NHS-ester group, a spacer arm and an MS/MS reporter. The reagents label peptides prepared from cell-based or tissue samples, and make a structure M-F-N-(R)-peptide, either two samples for the duplex kit or six samples for the sixplex kit. For each sample, a unique reporter mass results in the MS/MS spectrum, providing a simple means of identification.\nChanges in protein expression and post-translational modifications are essential mechanisms of biological regulation and disease. Advancements in mass spectrometry (MS) instrumentation, bioinformatics and quantification methods, such as label-free quantification, metabolic labeling and chemical tagging, now enable researchers to identify and quantitatively analyze thousands of proteins in a given sample with a high degree of accuracy.\nIsobaric chemical tags are tools that enable concurrent identification and quantitation of proteins in different samples using tandem mass spectrometry. This procedure is somewhat like the iTRAQ quantitative proteomics analysis. They are small chemical molecules with identical structure that covalently attach to the free amino termini of lysine residues of peptides and proteins, thereby labeling various peptides in a given sample\nFigure1. Structure of a Tandem Mass Tag\nEach member has a unique mass and reports\nsample-specific abundance of a labeled peptide\nduring MS/MS analysis.\nCleavable linker: Preferentially fragments under\ntypical MS/MS conditions to release the mass\nMass normalizer: Each member has a unique mass that\nbalances the mass reporter, ensuring the same\noverall mass for all tags in a set.\nReactive group: Reactive NHS ester provides\nhigh-efficiency aminespecific labeling of\nTandem Mass Tags\nconsist of TMT0 (zero), the TMT2 two-plex set and\nthe TMT6 six-plex set\nThe TMT0 tag allows testing and optimization of\nsample preparation, labeling, fractionation and MS\nfragmentation for peptide identification and\nreporter detection without using the more costly\nisotope-labeled compounds. The TMT2 reagent set\nallows two-plex protein profiling for small studies.\nThe TMT6 reagent set allows six-plex protein\nprofiling for multiple conditions, including time\ncourses, dose responses, replicates or multiple\nsample comparisons. Each TMT tag is based on the\nsame chemical structure, eliminating the need to\nmodify labeling conditions or HPLC separation\nconditions between experiments.\nFigure2. The TMT family of isobaric tag reagents\nTMT0 has no isotopic substitutions and is\nused for method development.\nA pair of isobaric mass labels with a single\nisotopic substitution per tag is used for simple\npairwise comparisons of relative protein expression.\nA six-plex of isobaric mass labels each with five\nisotopic substitutions per tag is used. Used for\ncomplex analyses including multi-plex patient\nscreening, time-course analysis or dose escalation\nDuring the MS/MS\nanalysis, each isobaric tag produces a unique\nreporter ion signature that makes quantitation\npossible. In the first MS analysis, the labeled\npeptides are indistinguishable from each other;\nhowever, in the tandem MS mode during which peptides\nare isolated and fragmented, each tag generates a\nunique reporter ion. Protein quantitation is then\naccomplished by comparing the intensities of the six\nreporter ions in the MS/MS spectra.\nFigure3. Procedure summary for MS experiments with TMT Isobaric Mass Tagging Reagents\nAs many as six\ndifferent samples can be mass-labeled and analyzed\nin a single mass spectrometry experiment.\nsummary for TMT Reagent experiments for mass\nabundance of the target protein or peptide fragment\nin six different samples is easily measured by\ncomparing the signature mass peaks generated by the\ndifferent mass tags.\nFigure5. Protein Profiling with TMT Tags\ntandem mass tagging enables protein\nidentification and quantitation from multiple\nsamples of cells, tissues or biological fluids\nconsistent chemistry allows efficient\ntransition from method development to multiplex\nquantitation, enabling biomarker discovery\namine-reactive NHS-ester activated reagents ensure efficient labeling of membrane and post-translationally modified proteins\nexpandable system allows concurrent\nmultiplexing of up to six different samples in a\noptimized fragmentation and fully supported\nquantitation with Proteome Discoverer* 1.0 for all\nThermo Scientific LC MS/MS platforms, such as LTQ\nXL* and LTQ Orbitrap* XL Systems\nProtein identification and quantitation from\nmultiple samples of cells, tissue or biological\nProtein expression profiling of normal vs.\ndisease states or control vs. treated\nMultiplex up to six different samples\nconcurrently in a single experiment\nQuantitative analysis of proteins for which no\nantibodies are available\nIdentification and quantitation of membrane and\npost-translationally modified proteins\nIdentification and quantification of hundreds to\nthousands of proteins in a single experiment\net al. (2003).\nTandem mass tags: a novel quantification strategy\nfor comparative analysis of complex protein mixtures\nby MS/MS. Anal. Chem.75 (8), 1895-1904.\net al. (2008). Relative\nquantification of proteins in human\ncerebrospinal fluids by MS/MS using 6-plex\nisobaric tags. Anal. Chem. 80(8), 2921\nNilsson, C.L. et al. (2010). Quantitative\nphosphoproteomic analysis of the\nSTAT3/IL-6/HIF1alpha signaling network: an\ninitial study in GSC11 glioblastoma stem cells.\nJ. Proteome Res. 9:430-43.']	['<urn:uuid:987af130-b07a-4e61-abb5-7ed905094d2a>', '<urn:uuid:04ae6799-6a15-4a9a-bfdc-e0441caaa80d>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T03:34:55.284799	12	74	1506
61	similarities differences between counseling session types psychotherapy occupational therapy	Psychotherapy and occupational therapy sessions have different approaches and focuses. Psychotherapy sessions involve talking therapy in a supportive environment where clients discuss problems openly with the therapist, who helps identify and change thought patterns. Occupational therapy sessions are more focused on practical activities, with initial assessments to identify strengths and difficulties, followed by hands-on intervention sessions. Both therapies provide comprehensive assessment and tailored treatment approaches based on individual needs.	['WHAT IS PSYCHOTHERAPY\nUsing psychotherapy, our therapists help people of all ages live happier, healthier and more productive lives.\nPsychotherapists apply scientifically validated procedures to help people develop healthier, more effective habits. There are several approaches to psychotherapy including cognitive-behavioural, interpersonal and other kinds of talking therapy that help individuals work through their problems.\nPsychotherapy is a collaborative treatment based on the relationship between an individual and the therapist. Grounded in dialogue, it provides a supportive environment that allows you to talk openly with someone who’s objective, neutral and non-judgemental. You and your therapist will work together to identify and change the thought and behaviour patterns that are keeping you from feeling your best.\nBy the time you’re done, you will not only have solved the problem that brought you in, but you will have learned new skills which will enable you to cope better with whatever challenges may arise in the future.\nWHEN SHOULD YOU CONSIDER PSYCHOTHERAPY\nBecause of the many misconceptions about psychotherapy, you may be reluctant to give it a try. Even if you know the realities instead of the myths, you may feel nervous about trying it yourself.\nOvercoming that nervousness is worth it because any time your quality of life isn’t what you want it to be, psychotherapy can help.\nSome people seek psychotherapy because they have felt depressed, anxious or angry for a long time. Others may want help for a chronic illness that is interfering with their emotional or physical well-being. Some individuals may have short-term problems they need help navigating. They may be going through a divorce, facing an empty nest, feeling overwhelmed by a new job or grieving a family member’s death.\nSigns that you could benefit from therapy include:\n- You feel an overwhelming, prolonged sense of helplessness and sadness.\n- Your problems don’t seem to get better despite your efforts and help from family and friends.\n- You find it difficult to concentrate on work assignments or to carry out other everyday activities.\n- You worry excessively, expect the worst or are constantly on edge.\n- Your actions, such as drinking too much alcohol, using drugs or being aggressive, are harming you or others.\nWHAT ARE THE DIFFERENT TYPES OF PSYCHOTHERAPY?\nThere are many different approaches to psychotherapy and therapists generally draw on one or more of these. Each theoretical perspective acts as a roadmap to help the therapist understand their clients and their problems and develop solutions.\nThe kind of treatment you receive will depend on a variety of factors: current psychological research, your therapist’s theoretical orientation and what works best for your situation.\nFor example, therapists who use cognitive-behavioural therapy have a practical approach to treatment. You may be asked to carry out certain tasks designed to help you develop more effective coping skills. This approach often involves homework assignments. Your therapist might ask you to gather more information, such as logging your reactions to a particular situation as they occur. They may want you to practice new skills between sessions, such as practising pushing elevator buttons if you have an elevator phobia. You might also have reading assignments so you can learn more about a particular topic.\nIn contrast, psychoanalytic and humanistic approaches typically focus more on talking than doing. You might spend your sessions discussing your early experiences to help you and your psychologist better understand the root causes of your current problems.\nYour therapist may combine elements from several styles of psychotherapy. In fact, most therapists don’t tie themselves to any one approach. Instead, they blend elements from different approaches and tailor their treatment according to each client’s needs.\nThe main thing to know is whether your therapist has expertise in the area you need help with and whether they feel that he or she can help you.', 'What is Occupational Therapy?\nOccupational Therapy is a family-centred health profession concerned with promoting health and well-being through participation in everyday activities. Our occupational therapy service offers support and advice for children and families to enable them to participate in everyday activities, and to help children further develop their skills. Occupational Therapists (OT’s) work with children with any condition or impairment that affects their ability to perform everyday activities, including tasks related to self-care (such as brushing their teeth or dressing), play and school (such as handwriting).Back to sub-menu\nThe JCU Health Occupational Therapy Clinic is a student-led service, providing support to children, adolescents and their families. The service supports the further education of Occupational Therapy students and their professional skills under the supervision of a qualified Occupational Therapist. We strive to provide a high quality occupational therapy service based upon the needs of the child and family. As we are part of James Cook University, we actively support research related to the development of evidence‐based Occupational Therapy practice and you may have the opportunity to participate in various research projects.Back to sub-menu\nWho do we see?\nChildren and adolescents who present with difficulties in the following areas:\n- Coordination and motor skills\n- Handwriting, copying, drawing, forming letters or shapes\n- Concentrating or paying attention\n- Constant repetitive movements\n- Organising themselves and their own space\n- Everyday activities (e.g., toileting, feeding, dressing)\n- Making friends and socialising with peers\n- Becoming upset over loud noises, bright lights, strong smells, feeling different textures, and swinging movements\nServices may include:\n- Comprehensive assessment & report\n- One on one intervention sessions\n- Home and/or school programs\n- Group sessions\n- Consultation with schools and other service providers\n- Combined Speech Pathology and Occupational Therapy Clinic\nHow to refer\nWe accept referrals from parents/guardians and health and educational professionals. The cost of our service is $50 for the initial assessment (including a feedback session & report) and $30 for subsequent appointments (fees subject to change). This is in order to cover some administration costs. If families have any concerns regarding their ability to pay this fee please contact the service to discuss further.\nReferral forms are available by contacting JCUHealth on 4781 4495, or by downloading the JCU Occupational Therapy Referral . Email or fax the completed form to JCUHealth.\nBack to sub-menu\nWhat can I expect from my child’s first appointment?\n- Upon initially meeting each child we complete an assessment to identify their strengths and difficulties. Every assessment is different and tailored around the age and ability of the child/ young person, their participation on the day, and the needs and priorities that have been identified in the referral and initial discussions.\n- Following the initial assessment (first appointment) we offer an additional appointment to discuss the contents of the report and answer any questions regarding this. This second session is complimentary.\n- The initial assessment will focus on the child/ young person, developing rapport with them and completing assessment tasks with them. This is to obtain as much information as possible during this initial visit. There will also be time at the end of the appointment to discuss additional priorities.\n- We appreciate that at times there may be information that you may not want to discuss in front of your child. If there is the case please advise us as discussions can also occur over the phone, in alternate areas or in follow-up sessions.\n- When the OT is completing activities with your child/ young person, it is best if you can avoid giving them prompts, helpful hints or pointing out mistakes. Our goal is to observe what the child/ young person can do without any support. There may also be times when we seek your encouragement and participation.\nWhat to bring to my first appointment?\n- Please bring any additional reports you have from other health professionals, and/or information from your child’s school which you feel may be beneficial for us to see. This may include samples of written work completed at school, or for homework.\n- Please bring your child’s glasses, or other items that they use regularly to assist them complete tasks at home or in the classroom.\n- Please ensure your child has running shoes/ sneakers with them if possible.\nOccupational Therapy Clinic Staff\n- Elizabeth Browne – Occupational Therapist, Practice Educator\n- Leanne Anderson – Occupational Therapist']	['<urn:uuid:c360f931-895b-4540-b264-c76adb1c0a8f>', '<urn:uuid:d0948dc2-b104-4390-a2a9-0f0dda573015>']	factoid	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-13T03:34:55.284799	9	69	1369
62	kitchen paint types durability storage needs	For kitchen cabinets, semigloss or satin paint are best choices, with semigloss being the most durable and stain-resistant. High-quality paint in the $25-per-gallon range is recommended over cheaper $10 options. When planning storage, it's crucial to carefully consider cabinet space needs to avoid either overcrowding the kitchen or having insufficient storage for appliances and supplies. Poor storage planning is a common renovation mistake that affects kitchen functionality.	['Want to give your kitchen a facelift without draining your wallet? Consider painting the cabinets.\nWood cabinet refinishing entails removing the existing paint, sanding the surface (depending on the type of paint you are using) and applying new paint to the bare wood. This cosmetic project revives cabinets that have become scratched, faded or discolored, and breathes new life into an old kitchen.\n“Refinishing cabinets is a fantastic, cost-effective way to transform the look and feel of a kitchen,” says Dan DiClerico, home expert at HomeAdvisor, a remodeling project resource. It’s also a fraction of the cost of doing a full kitchen remodel, which runs, on average, $22,632, according to HomeAdvisor. Even replacing old kitchen cabinets with new cabinets starts at about $4,000 for a typical 10-by-12-foot kitchen, according to HouseLogic, an online resource for homeowners.\nHomeAdvisor says the average cost to repaint kitchen cabinets is about $1,000 – if you hire a professional. If you do it yourself, your base cost would be $200 to $600, depending on the brand of paint and the supplies you have to buy, according to the Kitchn, a kitchen design and renovation website. You might already own some of the supplies you’ll need.\nThough cabinet refinishing isn’t as large of an undertaking as a full kitchen remodel, it’s still a big project that takes time, patience and elbow grease. Before diving into step-by-step instructions, let’s look at how to choose the right paint.\nA quick primer on paint\nThere are six main types of interior paint: matte, matte enamel, eggshell, satin, gloss and semigloss. DiClerico finds that either a semigloss or satin paint is best for cabinet refinishing. He says semigloss is the most durable and stain-resistant, and it won’t lose its luster if you scrub it with a sponge.\nWant a more modern look? “The trend in kitchens these days is for a softer matte look, including on countertops and floors, as opposed to the high polish that was popular a decade ago,” DiClerico says.\nThere are also a number of specialty paints that can be used on cabinets. Chalkboard paint, for example, “could be a fun option, converting a cabinet or pantry door into a message center,” DiClerico says. Chalk paint (not the same as chalkboard paint) is a good option if you want to avoid sanding first – but it can require multiple coats and still appear uneven in places, according to designer and artist Jeanne Oliver.\nIf you’re going for an antique look, consider another specialty option: milk paint. Its organic formula, which includes milk and color pigments, is a nontoxic option that delivers an old-fashioned, translucent finish, DiClerico says, but “specialty paints won’t give you anywhere near the protection of a traditional high-performing latex paint.”\nWhichever paint finish you choose, choose a high-quality product. “There are plenty of fine options in the $25-per-gallon range,” he says. “Stay away from the $10-per-gallon economy stuff. You’ll end up needing multiple coats so it might not save you much in the long run, plus it won’t stand up to the demands of a busy kitchen.”\nIf you have doubts about which paint to select, ask for help at your local paint store.\nA step-by-step guide\nFirst, remove all food and kitchen items from your cabinets and drawers. Then label all doors and drawers with their respective location so you can reinstall them correctly. Put labels on the inside of doors and drawers so they don’t interfere with the refinishing process, DiClerico says.\nRemove the drawers, doors, knobs and other hardware with a screwdriver. (A little WD-40 can help you remove rusted hinges.) Store the items in a plastic bag if you’re going to reuse them. If you plan on installing new hardware in a different style, fill the existing hardware holes with wood putty. This gives you a clean surface to drill in the new hardware later.\nNext, clean layers of grease and dirt off the cabinets using a wood cleaner, trisodium phosphate or mineral spirits. “If you don’t clean them, the new paint finish isn’t going to go on properly,” DiClerico says.\nAfterward, sand the wood using a medium-grit (60 to 100 grit) sandpaper either manually or using an electric sander, concentrating on rough areas where the existing paint or varnish has bubbled or peeled. Then lightly sand the entire cabinet surface in the direction of the wood grain. “You don’t want to be gouging the wood with a heavy hand,” DiClerico says. “You’re doing a light sanding to knock the shine off the existing finish so that your new paint can be absorbed into the wood.”\nAfter sanding, clear the cabinet surfaces of dust with a damp rag and let them dry. When dry, apply a coat of primer. A paint roller will make quick work. “Primer not only helps bond the paint to the cabinets but also improves their durability and decreases the chances of bubbling and peeling later on,” DiClerico says. “If you can do this work outside, things will dry more quickly, and you’ll have better ventilation.”\nAfter the primer is applied, it’s time to paint the cabinets. DiClerico recommends applying two coats of paint using a high-quality paint brush, which will be able to reach any crevices in the door detail. (If you’re working with flat-fronted cabinets, a roller will do the trick.) For an even finish, “it’s a good idea to do a light sanding between coats with a fine-grit sandpaper, say 220-grit,” he says.\nAfter painting, let the cabinets dry. “It might take a full day to let them dry depending on the humidity,” DiClerico says, and it will take the paint at least a week – and up to a month in some cases – to fully cure. Most people don’t want to live that long with an unfinished kitchen, but if you can wait a couple days before reattaching the doors and hardware, that will help protect the finish. If not, treat them gently for the first week or so, opening and closing softly, and avoid cleaning the painted surfaces with a sponge or cloth.\nThen step back and bask in the glory of your newly refinished kitchen.', 'Table of Contents\nCommon kitchen renovation mistakes to avoid\nRenovating the kitchen is one of the most exciting projects for homeowners. A well-designed kitchen can be the centerpiece of your home, a space for family gatherings, and a place to explore your culinary passions. However, DIY kitchen renovation can quickly become a nightmare if you don’t plan properly, budget wisely, or avoid common mistakes that many people make. In this article, we will discuss the top ten kitchen renovation mistakes that you should steer clear of.\nMistake #1: Not Setting a Budget\nThe first and most crucial mistake that many people make is not setting a budget for the renovation project. Without a budget, you could quickly overspend on items that you don’t need, or cut corners on critical features. To avoid this, you should spend some time researching the costs of various kitchen renovation items such as cabinets, flooring, appliances, and countertops to create a realistic budget.\nMistake #2: Ignoring Your Kitchen’s Layout\nA common mistake when renovating a kitchen is ignoring the existing layout. While you may want to change the flow of your kitchen, some changes may not be possible due to the location of pipes, electrical outlets, or load-bearing walls. So, before you get started, take the time to study your kitchen’s layout and plan accordingly. Think about the functionality of the space and how you can improve it.\nMistake #3: Choosing Inappropriate Materials\nChoosing inappropriate materials can lead to a costly mistake. For example, choosing a material that is not durable enough can result in frequent repairs, while choosing materials that are too high-end can be a wasteful expense. Additionally, choosing the wrong design style could limit the value that your remodel could return in the future. Before you buy, research the materials, durability, and style you are interested in.\nMistake #4: Inadequate Lighting\nLighting is a key element in any kitchen renovation project. Adequate lighting makes it easier to cook, clean, and adds to the overall ambiance of your space. However, inadequate lighting can lead to a poorly lit kitchen that is neither functional nor safe at night. To avoid this mistake, consider installing a combination of natural, ambient, and task lighting in your kitchen.\nMistake #5: Overlooking Storage Space\nWhen renovating the kitchen, people often overlook the importance of storage. Cabinets and shelves are essential in the kitchen as they help keep the space organized and clutter-free. However, it is easy to overdo it with too many cabinets that make the kitchen feel cramped or too few that don’t meet the storage needs of your family. Therefore, you should carefully plan your storage needs, considering the size of your appliances and how to make the best use of every inch of your kitchen space.\nMistake #6: Not Hiring a Professional\nSometimes, the urge to DIY your kitchen renovation project can get overwhelming. While DIY can be a budget-friendly option, it can also lead to costly mistakes that could have been avoided by seeking professional help. A licensed builder or designer could help you avoid the mistakes mentioned above and provide valuable insights into the renovation process.\nMistake #7: Ignoring the Importance of Appliances\nAppliances are an essential part of the kitchen and can make or break your renovation project. To avoid this mistake, research the appliances that will meet your needs, both functionally and aesthetically.\nMistake #8: Choosing the Wrong Contractor\nHiring the wrong contractor ( Ultimate Guide to Finding Trustworthy Kitchen Contractors ) can be a costly mistake. Low-cost bids can be tempting, but they could mean a less experienced contractor that cuts corners or uses lower quality materials. Therefore, it is essential to do your research and choose a reputable contractor with experience in kitchen renovation.\nMistake #9: Skimping on the Finishing Details\nThe finishing details in your kitchen renovation project are often the details that make the biggest impression. Skimping on the finishing details such as cabinet ( Revolutionize Your Kitchen with Genius Cabinet and Drawer Space Maximization Tips ) knobs and handles, backsplash, and paint or wallpaper could have a detrimental effect on the overall aesthetic.\nMistake #10: Not Considering Functionality\nWhile the kitchen’s aesthetics are important, functionality should be a significant consideration. After all, the kitchen is where you prepare and cook your meals. Consider the positioning of the stove, sink, and refrigerator to ensure convenience while cooking and workflow.\nBy avoiding the ten kitchen renovation mistakes listed above, you can save yourself time, money, and headaches during the renovation process. Take your time, plan carefully, and choose quality materials and professional contractors to ensure that your kitchen renovation project is a success. The result will be a beautiful, functional kitchen that you can enjoy for many years to come.\nFAQ: Common DIY Kitchen Renovation Mistakes\nWhat are some common DIY kitchen renovation mistakes?\nSome common DIY kitchen renovation mistakes include underestimating the budget, not planning the layout properly, skipping important steps like measuring and leveling, not hiring professionals when necessary, and choosing cheaper materials over quality ones.\nWhy is underestimating the budget a mistake?\nUnderestimating the budget can lead to running out of funds before the renovation is complete, compromising on important features, or using lower quality materials. Itâs important to have a realistic budget that takes into account unexpected expenses.\nHow important is planning the layout of the kitchen?\nPlanning the layout of the kitchen is crucial to ensure that the space is functional and efficient. Skipping this step can leave you with a kitchen thatâs difficult to navigate, lacks storage space, or is poorly designed.\nWhat are some common layout mistakes to avoid?\nCommon layout mistakes include not leaving enough space for traffic flow, placing appliances too far apart, not having enough counter space, and not considering the work triangle. Itâs important to plan the layout carefully to avoid these mistakes.\nWhy is measuring and leveling important?\nMeasuring and leveling are important steps to ensure that everything fits together properly and is level. Skipping these steps can lead to uneven countertops, crooked cabinets, and other issues that can affect the overall appearance and function of the kitchen.\nWhen should you hire professionals?\nItâs important to know your limitations and hire professionals when necessary. This could include hiring an electrician to install new wiring or a plumber to move pipes. Attempting to do these tasks yourself could be dangerous and cause damage to your home.\nWhat are the consequences of choosing cheaper materials?\nChoosing cheaper materials may seem like a way to save money, but it can actually end up costing more in the long run. Cheaper materials may not last as long, require more maintenance, and not be as aesthetically pleasing as higher quality materials. Itâs important to choose quality materials that will stand the test of time.\nWhat are some tips for avoiding these mistakes?\nSome tips for avoiding these mistakes include creating a realistic budget, properly planning the layout, measuring and leveling carefully, knowing when to hire professionals, and choosing quality materials over cheaper ones. Itâs also a good idea to do your research and seek advice from experts.\nWhat should you do if you make a mistake during the renovation?\nIf you make a mistake during the renovation, itâs important to address it as soon as possible to minimize the damage. This could involve calling in a professional to fix the issue or finding a creative solution on your own. Itâs important to learn from your mistakes and use them as a learning opportunity for future projects.\nCommon DIY Kitchen Renovation Mistakes\n1. Underestimating the budget:One of the most common mistakes that homeowners make when renovating their kitchens is underestimating the budget. It’s essential to have a realistic idea of the costs involved before you start. The true cost of a kitchen renovation project includes the materials, appliances, labor, and any other related expenses.\nThis workbook helps you to determine your budget, estimate the costs, track expenses, and save money. It covers the essential aspects of kitchen renovation, including appliances, cabinets, countertops, sinks, lighting, and flooring. It also includes helpful tips on selecting the right products to achieve your desired look and functionality.\nIf you’re hiring a contractor for your kitchen renovation project, it’s important to find a reliable and trustworthy professional. This book provides practical advice on how to choose a contractor, negotiate contracts, and manage the project to ensure that you get the best results.\n2. Not considering the kitchen layout:Another significant mistake that homeowners make is not considering the kitchen layout. A well-designed kitchen layout can improve functionality, efficiency, and aesthetics.\nThis book is an excellent resource for kitchen design ( Uncover the Secrets to a Dream Kitchen Design Youâve Always Wanted ) and layout ideas. It covers everything from planning to installation, including appliances, cabinets, countertops, and lighting. The book shows how to create a functional and beautiful kitchen that meets your needs and preferences.\nThis e-book provides practical tips and advice on kitchen layout and design. It covers various aspects of kitchen design, including space planning, traffic flow, storage, and organization. The book provides design ideas for different kitchen sizes and shapes, so you can choose the best layout for your space and needs.\n3. Overestimating DIY skills:DIY kitchen renovation projects can be rewarding, but it’s essential to know your limitations. Overestimating your DIY skills can lead to costly mistakes and even safety hazards.\nThis e-book is an excellent resource for those who are new to DIY kitchen renovation. It provides step-by-step guidance on how to plan, design, and execute a kitchen renovation project. The book covers the essential aspects of kitchen renovation, including plumbing, electrical, and structural work.\nHandyman Magazine is a great resource for DIY enthusiasts. It covers various topics, including home renovation, repairs, and maintenance. The magazine provides practical advice and tips on DIY projects, tools, and materials. It also features inspiring stories of successful DIY projects.\n4. Improper installation of cabinets:Installing cabinets is a crucial part of a kitchen renovation project. Improper installation can affect the functionality and aesthetics of the kitchen.\nThis guide provides step-by-step instructions on how to install cabinets correctly. It covers various aspects of cabinet installation, including measuring, leveling, and securing the cabinets. The guide also provides tips for ensuring that your cabinets are properly aligned and sturdy.\nA power drill is an essential tool for cabinet installation. It helps to make precise holes for screws and fasteners. There are various types of power drills available, including corded and cordless models. Choose a drill that is powerful, easy to use, and reliable.\n5. Forgetting about lighting:Lighting is an essential aspect of a beautiful and functional kitchen. Neglecting to plan for proper lighting can lead to poor visibility and an unpleasant ambiance.\nThis lighting kit provides bright and energy-efficient ( Tips: sustainable kitchen appliance ) lighting for your kitchen cabinets. It’s easy to install and includes all the necessary components, including LED strips, connectors, and a power supply. The kit is available in various sizes and colors to suit your needs.\nPendant lights are a popular choice for kitchen lighting. They are stylish and functional, providing a pleasant ambiance and ample illumination. There are various designs and colors available, from modern to traditional. Choose a pendant light that complements your kitchen’s style and color palette.\n6. Using the wrong materials:Choosing the wrong materials can affect the durability, functionality, and aesthetics of your kitchen. It’s important to select high-quality, durable materials that can withstand daily wear and tear.\nGranite is a popular choice for kitchen countertops. It’s durable, heat-resistant, and easy to maintain. Granite countertops come in various colors and patterns, so you can choose the one that best suits your kitchen’s style.\nStainless steel appliances are durable, easy to clean, and stylish. They complement any kitchen style and color palette. Choose a stainless steel appliance that matches your kitchen’s design and meets your needs.\nConclusion:A kitchen renovation project can add value and functionality to your home. Avoiding common mistakes and using the right products can help you achieve the desired results. Consider your budget, kitchen layout, DIY skills, cabinet installation, lighting, and materials when planning your kitchen renovation. Select high-quality products that meet your needs and preferences. With proper planning and execution, you can transform your kitchen into a beautiful and functional space.']	['<urn:uuid:a0d3bc93-a054-4d0e-a5a6-49609ef2e71f>', '<urn:uuid:6cf8c9ed-3d0a-43ca-8f62-04e833225c40>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T03:34:55.284799	6	67	3080
63	What happens to make beer taste like buttered popcorn, and what are the temperature ranges brewers use to fix this issue in different types of beer?	The buttered popcorn flavor (diacetyl) is a compound formed by yeast during fermentation. For ales, this issue naturally resolves at 62°F-75°F if given enough time (at least 10 days). For lagers, brewers use a 'diacetyl rest' by raising the temperature to 65-72°F near the end of fermentation. For wheat and Belgian styles, temperatures between 62°F-85°F are used.	['By Chris Jennings\nFor the Oregon Beer Growler\nThe tasty brews we enjoy are, for the most part, created using the same four ingredients: hops, malt, water and yeast. These ingredients all have their own unique flavors and aromas that are most of the time simple to identify. They do, however, lend off flavors and aromas if the beer or wort are not handled properly. The most common undesirable traits are diacetyl, dimethyl sulfide (DMS), oxidation, sourness and skunkiness. These flavors and aromas can be prevented by knowing where in the process they occur and how to avoid allowing them to form. All of the off flavors listed, with the exception of DMS, occur in the final stage of our beer making process.\nIdentifying Off Flavors and Fixes\nDiacetyl is one of the compounds formed by yeast during fermentation. It has a buttered popcorn or butterscotch flavor and aroma. Most often it’s found in beers that have been rushed or lagers that have not been given a chance to rest. If the yeast is given enough time at the right temperature the diacetyl will be reabsorbed. With lagers, a diacetyl rest is used to clean up the flavor of the beer. Once fermentation is close to complete, you want to bring a lager up to ale temperature (65-72 F) to allow the yeast to finish working. This rest occurs naturally with ales as long as they are allowed to finish fermentation without being rushed. A good rule for most ales is to allow them to ferment at least 10 days.\nDimethyl sulfide is a compound that is naturally occurring in the malt that we use to make our homebrews. It can be characterized by the aroma or flavor of creamed corn or cooked vegetables. One of the reasons we boil is to evaporate this compound. When boiling you want the wort to be completely uncovered so that the compounds don’t condensate on a lid and end up right back in the wort. Chilling the wort as fast as possible is also a very important step because the compound is formed between 120-200 F.\nOxidation occurs when the finished beer is exposed to a large amount of air. It will lend to a cardboard or flat flavor and can smell almost like sherry. In older beers, this is sometimes a desirable flavor. In younger brews, however, it means that either the beer was transferred with too much exposure to the air or that it sat too long in the fermenter. Once fermentation is complete, the beer needs to be transferred to an air-tight, sealable container, like a bottle or keg, that carbon dioxide can be added to in order to purge the air. During the transfer, you want to fill the container from the bottom up by using a siphon.\nSometimes a sour beer can be a desirable thing, but when it’s unintended it can be a terrible experience. Sour flavors and aromas can mean that during the brewing process something that touched the wort or beer was not properly sanitized. After the boil is complete, everything that touches our brew needs to be thoroughly sanitized. This will ensure that no wild yeast or bacteria get into our brews, potentially ruining them.\nWe all have had a skunky beer. Though it has become a common flavor for some brands, it is never a good thing. The skunky flavor is created when ultraviolet light hits the beer or wort once the boil is over. The light breaks down compounds in the hops that then become a skunky flavor rather than the wonderful piney-citrus notes we all love in a good IPA. Make sure that any clear fermentors are covered and hidden from sunlight and use bottles that are not green or clear. These measures will help prevent your homebrews from becoming skunky.\nAll off flavors are avoidable. As long as we pay attention to the causes, we can ensure that none of our brews end up being dumped down the drain.\nHarrumph Session Ale [AG]\nHarrumph Session Ale [Extract]\nOBG Blog Archives\nWelcome to our archive pages! Read stories from the print edition of the Oregon Beer Growler from June 2012 to January 2018. For newer stories, please visit our new website at:', 'Fermentation is the heart of the brewing process. During fermentation, wort created from raw materials is converted to beer by yeast. Fermentation is usually divided into three stages: primary, secondary, and conditioning (or lagering). Fermentation is when yeast produce all of the alcohol and aroma and flavor compounds found in beer. Manipulation of temperature, oxygen levels, and pitch rate as well as yeast strain selection will all dramatically affect the production of aroma and flavor compounds produced during fermentation.\nThe primary stage of fermentation begins when the yeast is introduced into cooled, aerated wort. The yeast quickly utilize the available oxygen to produce sterols, a vital compound for culture expansion. When the oxygen is gone, the yeast switch to the anaerobic phase where the majority of wort sugars are reduced to ethanol and CO2. Yeast growth occurs during primary fermentation. The extent and rate of yeast growth is directly related to the production of aroma and flavor compounds.\nPrimary Fermentation Summary:\n• Depletion of dissolved oxygen\n• Acidification / reduction in pH\n• Yeast growth or culture expansion\n• Ethanol and CO2 production\n• Production of flavor compounds such as esters, diacetyl, sulfur containing compounds, etc.\n• Consumption of most wort sugars\nThe temperature of the primary fermentation should be regulated according to the desired flavor and aroma profile. The following is a guideline:\nPrimary Fermentation Temperatures:\n• Ales: 62°F – 75°F (17°C – 24°C)\n• Lagers: 46°F – 58°F (8°C – 14°C) *Note: Lager fermentations can be started warmer (~60°F, 15.5°C) until signs of fermentation (gravity drop, CO2 production, head formation) are evident. Cool to desired fermentation temperature once signs of fermentation are observed.\n• Wheat and Belgian styles: 62°F – 85°F (17°C – 29°C)\nThe secondary stage of fermentation refers to the stage of fermentation after the majority of the wort sugars have been consumed and there is a sharp decrease in the rate of fermentation. During this period, most of the final sugars are depleted and some secondary metabolites are converted by the yeast. Yeast flocculation and settling begins to occur due to the increase in alcohol content and the depletion of sugar and nutrients. Diacetyl reduction takes place during secondary fermentation and during the diacetyl rest that some brewers incorporate into the secondary stage of fermentation.\nSecondary Fermentation Summary:\n• Decreased rate of ethanol and CO2 production\n• Diacetyl Conversion\n• Reduction of some flavor compounds by yeast metabolism or CO2 scrubbing\n• Terminal gravity is reached\n• Yeast flocculation and settling begins\nSecondary Fermentation Temperatures:\n• Ales: Same as primary fermentation (higher temperatures will increase diacetyl reduction rates)\n• Lagers: 40°F - 60°F (4°C - 15°C). Some brewers allow the beer to increase in temperature to speed the diacetyl reduction. This increased temperature is usually only sustained for 24 to 48 hours.\n• Wheat and Belgian Beers: Same as primary fermentation (higher temperatures will increase diacetyl reduction rates).\nThe conditioning stage takes place when the terminal gravity has been reached and the tank is cooled to refrigeration temperatures (31°F - 38°F, 0°C - 3°C). During this time the yeast continues to flocculate and settle. The yeast also conditions the beer by reducing various undesirable flavor compounds. Ales do not benefit from long conditioning times like lagers do. The desirable flavors in ales will decrease with age and therefore it is recommended that conditioning be as short as possible before packaging. Exposure to oxygen at this stage is extremely detrimental to beer quality.\n• Most of the yeast is removed from beer\n• Formation and precipitation of haze forming proteins\n• Reduction and mellowing of harsh flavors\n• Reduction of sulfur compounds, diacetyl, and acetaldehyde\n• Flavor stabilization']	['<urn:uuid:07318343-d21d-4d4e-adb6-6237d4db0d87>', '<urn:uuid:1d44c5cc-d4b6-4c24-bf9b-cfbabb30f707>']	factoid	direct	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-13T03:34:55.284799	26	57	1323
64	I'm developing support programs for Alzheimer's caregivers and would like to know more about the Cleveland Clinic Lou Ruvo Center's approach. What specific strategies does their caregiver support program use to help spouses cope with the challenges of caring for Alzheimer's patients?	The Ruvo Center's program consists of two initial sessions with just the patient's spouse, followed by four sessions that include friends and family members. The program focuses on managing daily living skills and provides practical advice. For example, they teach caregivers to adapt tasks for patients, such as giving them a small basket with few items to sort when they want to help with laundry but can no longer properly fold clothes. They emphasize the principle 'We distract, never argue.' Additionally, caregivers have constant access to phone counselors for support during crises. This program, which was created by New York University, has proven effective, successfully delaying nursing home placement by an average of 18 months.	"[""Alzheimer's Reading Room\nToday’s Las Vegas Review-Journal tells about a caregiver support program at the Cleveland Clinic Lou Ruvo Center for Brain Health.\nThe story of Leonard and Jean Georges is told by columnist Craig L. Moran. Leonard has Alzheimer’s disease.\nLeonard’s life work was helping turn around businesses that were in financial trouble. About six or seven years ago, Jean noticed that Leonard seemed to be taking on some near impossible business ventures, yet, she just assumed that Leonard was feeling secure in his retirement and willing to expose himself to more financial risk. Jean never even considered the idea that Leonard’s increasingly poor judgments had a medical cause.\n“You’re always making excuses for strange behavior when someone you love starts to act strangely.”After a 60 year harmonious marriage, Jean found her life filled with conflict and Leonard filled with agitation. Doctors told Jean that Leonard’s behavior was “a normal part of aging”. Even when Jean suggested that Leonard might have Alzheimer’s, doctors told her that Alzheimer’s could not be confirmed until after death.\nAbout two years ago, Jean sought medical attention for herself at the Mayo Clinic in Phoenix. She was feeling very tired, and, soon after, had a heart attack. According to Craig Moran,\n“The doctors there told her that if she wanted to live, she could no longer care for her husband. The stress, they said, would kill her.”\nJean says, “The most difficult thing I have ever had to do in my life is put my husband in a home.”\nThe new program at the Ruvo Center’s goal is to “help spouses adapt better to the challenges of the disease in a loved one, so they, too, don’t become ill.” Susan Hirsch, director of social services at the Ruvo Center said, “Too often we see where caregivers become ill and depressed, and we think this program can be a real help.”\nThe program includes two sessions with just the patient’s spouse, followed by four sessions that include friends and family members.\nAccording to Hirsch, advice is given on managing the skills of daily living.\n“If, for example, the person with Alzheimer’s wanted to help with the laundry but no longer has the ability to fold and sort clothes, the caregiver might want to give him or her a basket with a few items to work with. . . We distract, never argue.”\nCaregivers have constant access to phone counselors for support. From my own experience with my great grandmother, I can see how important it is to have someone to call in times of crisis.\nThis program, created by New York University , has been able to delay the placement of a spouse with Alzheimer’s in a nursing home by an average of 18 months!\nThe full article about this story can be found here.\nMax Wallack is a student at Boston University Academy. His great grandmother, Gertrude, suffered from Alzheimer's disease. Max is the founder of PUZZLES TO REMEMBER. PTR is a project that provides puzzles to nursing homes and veterans institutions that care for Alzheimer's and dementia patients.\nInsight and Advice\n- 60 Good Reasons to Subscribe to the Alzheimer's Reading Room\n- About the Alzheimer's Reading Room\n- Alzheimer's CareGiving -- Insight and Advice\n- Test Your Memory for Alzheimer's (5 Best Self Assessment Tests)\n- Communicating in Alzheimer's World\n- Worried About Alzheimer's Disease -- You Should Be\n- What is Alzheimer's? What are the Eight Types of Dementia?\n- Does the Combination of Aricept and Namenda Help Slow the Rate of Decline in Alzheimer's Patients\n- Alzheimer's Disease Statistics\n- Is it Really Alzheimer's or Something Else?\n- Ten Symptoms of Early Stage Alzheimer's\n- Ten Tips for Communicating with an Alzheimer’s Patient\n|The Alzheimer's Action Plan||300 Tips for Making Life Easier|\nOriginal content Max Wallack, the Alzheimer's Reading Room""]"	['<urn:uuid:bf9f1caa-0b4f-4f9b-bbaf-0903b62d3eef>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T03:34:55.284799	42	115	640
65	What are funding models and quality concerns in open access?	Open access publishing employs different funding models, including article processing fees paid by authors or institutional bodies covering publication costs. While article processing fees can restrict authors with poor institutional support, institutional funding models ensure free access for both authors and readers. Regarding quality, open access journals can maintain rigorous standards through peer review processes - for example, 61% of linguistics open access journals use double-blind peer review. However, quality concerns have led to the emergence of predatory publishers who exploit the author-pays model without providing proper peer review services.	"['“Maximizing the access to existing knowledge is a means for supporting scientific progress..”\nThe ultimate aim of open access is to remove barriers in the accessibility of scientific knowledge. Offering free access to scientific publications is crucial for large groups of users with poor or no institutional support, e.g., researchers in developing countries or users without institutional coverage. Beyond increasing the population of users (which is valuable enough), open access follows from the core mission of science: scientific knowledge is incrementally built upon existing knowledge, which means that maximizing the access to existing knowledge is a means for supporting scientific progress.\nGOLD OPEN ACCESS\nA frequent source of skepticism to open access practices relates to the maintenance of quality standards. The developments in the publication media resulted in a wealth of publications, journals and publishers with the usual inflation effects on the quality of the outcome.\nIt is an interesting question how open access practices may have contributed to such developments. Are scientific products with varying quality standards caused by open access practices or just made better accessible?\nObviously enough, quality assurance procedures can be applied to scientific publishing independently of the accessibility measures. Gold open access refers to publications in open access media (OA journals or OA publishers) using the usual procedures for quality assurance. The Directory of Open Access Journals lists 556 journals with “Linguistics” as one of their subjects. The targeted audience and the publication processing procedures certainly vary very much in these entries, but 344 out of 556 journals (i.e., 61%) follow the current quality control standards (double blind peer review).\nOPEN ACCESS TO PUBLISHING\nOpen access means no cost for the end user, but that’s just one side of the publication process. The flip side is the author. However, open access to publications does not automatically mean open access to publishing. This depends on the budget model for covering publication costs.\nArticle processing fees are a typical OA funding model. This model restricts the users with poor or no institutional support to the role of end user. Furthermore, the publisher depends on the number of publications, i.e., the interest of the publisher potentially interferes with the interest of maintaining rigorous quality control.\nThe alternative model involves an institutional body or association that covers the publication costs and guarantees free access for both sides, authors and readers.\nOur commitment to open access needs this extension: open access to publications and open access to publishing. This is not only about increasing the number of authors and readers that have access to the scientific process (though this is valuable enough). It is an inherent part of our commitment to scientific progress to remove any barriers in the dissemination of scientific findings.\n“It is an inherent part of our commitment to scientific progress to remove any barriers in the dissemination of scientific findings.”\nTHE JOURNAL OF THE GERMAN LINGUISTICS SOCIETY\nOpen Access does not only apply to the articles that are published since 2017 and are freely accessible from the publication date on but also to the complete backlist of contents since volume 1, issue 1 (1982). Publication costs are covered by the association (i.e., by the subscribers), following the practice of the print journal.\nThere has hardly ever been a decision in the journal management that has caused less debate than this one. Open access radically changes the worldwide visibility of the journal’s contents. 13 years after the Berlin Declaration on Open Access to Knowledge in the Sciences and Humanities, the commitment of individual researchers and scientific societies to open access is evident.\nRead the other articles\n[Title image by littlehenrabi via gettyimages]', 'Helpful Links for those Writing & Publishing Scholarly Information\nGuide to Ethical Writing\nThe Office of Research Integrity (part of the U.S. Department of Health and Human Services) maintains a learning module and publication by Miguel Roig, Avoiding Plagiarism, Self-Plagiarism, and Other Questionable Writing Practices: A Guide to Ethical Writing. Roig presents 28 guidelines for ethical writing, including:\n- Guideline 10: Authors who submit a manuscript for publication containing previously disseminated data, reviews, conclusions, etc., must clearly indicate to the editors and readers the nature of the previous dissemination. The provenance of data must never be in doubt.\n- Guideline 12: In the domain of conferences and similar audio-visual presentations of their work, authors should practice the same principles of transparency with their audiences.\nPlease review Roig\'s guide and remind others to do the same.\n- COPE: The Committee on Publication Ethics provides guidelines on responsible authorship and case studies of research publishing questions.\n- ICMJE: The International Committee of Medical Journal Editors provides resources and recommendations for research conduct, reporting, and editing.\nSecuring Copyright Permissions When Writing\nSecuring permission to use copyrighted materials in teaching can be quite different from seeking permission for use of others\' copyrighted materials while writing and publishing. This section focuses on permissions from a researcher\'s or author\'s perspective.\nBasic information about copyright is available at\n- Review Dr. Kenneth D. Crews\'s Procedures for Securing Permission, a guide to asking permissions to use copyrighted works.\n- Use the Columbia Copyright Advisory Office Permissions guide to find models of letters to write when contacting a copyright owner.\nAll UB students and faculty should be familiar with responsible and ethical publishing practices. This includes knowing answers to:\n- Who should be listed as an author? Who should not be listed?\n- What constitutes self-plagiarism?\n- What is salami-slicing data, and what is the harm?\n- What is redundant publication, and what is the harm?\n- Can you submit an article to more than one journal?\n- What is a journal\'s right of first publication?\n- Is plagiarism the same as copyright infringement?\nMany federal agencies, and an increasing number of private funders, require researchers to do things such as:\n- ...provide a Data Management Plan for their research\n- ... make their findings publicly accessible. This may involve...\nIf this is the case, below you will find helpful links to relevant information.\nData Management Plans\nA Data Management Plan (DMP)...\n- ...describes the data you expect to acquire or generate during the course of a research project\n- ...tells how you will manage, describe, analyze, and store those data\n- ...documents what mechanisms you will use at the end of your project to share and preserve your data\n- ...is an integral part of the research process\nOpen Access Repositories\nUB ScholarWorks is UB\'s digital repository, in which digital scholarship materials produced by the University of Bridgeport community are collected, preserved, and distributed. Read about how to add materials to UB ScholarWorks here.\nYou may want to deposit a copy of your work in a disciplinary repository. These can be found via sites such as:\nOpen Access Publishing\nOpen access means: ""digital, online, free of charge, and free of most copyright and licensing restrictions"" (Suber, 2012).\nSee the Open Access Publishing tab in this toolkit for more information.\nNew scholars today who are about to publish have more options than ever before:\n- Traditional subscription: Individuals and institutions \'subscribe\' to the journal issues, paying to access.\n- Open Access: Follows a different funding model that allows individuals and institutions to access without paying. May (legitimately) involve the \'author-pays\' model.\n- Hybrid: The journal includes a mixture of both subscription and open-access articles\n- Monograph: A number of articles on a related topic are gathered together but published in a one-off \'book\' volume.\nIt\'s important to remember that not all publishers are the same. For example, while open access publishing provides exciting new options for getting published, some scholars have fallen victim to predatory publishers. Predatory publishing occurs when a publisher of a journal follows the author-pays model of open access publishing, but exploits the authors by failing to actually provide peer review/editing services. Such journals tend to be mostly unknown and tend to end up with a bad reputation, and so are best avoided. For this reason, it\'s important to evaluate a publisher before agreeing to publish anything with them.\nHow to Evaluate a Publisher\nUse the Think-Check-Submit checklist. Review it before submitting work or agreeing to serve as a reviewer.\nQuestions to be asked about journal publishers:\n- Who is on the editorial board? Are the editors clearly identified? Are their email addresses and/or phone numbers provided directly on the publisher\'s site?\n- Can you confirm that the editors really are serving in that role? Do the editors have this position listed on their online CV? Can you confirm with the editor-in-chief via email?\n- What is the acceptance rate of the journal? (Note: Some legitimate megajournals, such as PLOS ONE, accept any methodologically sound study that passes the scrutiny of peer-reviewers. This is a new model that attempts to share good research regardless of trends in popularity or research interests, but megajournals should be closely evaluated for quality leadership, editors, reviewers, etc. Not all megajournals are of the same quality.)\n- What is the impact factor of the journal? (Keep in mind that impact factors can be manipulated and are increasingly seen as an inaccurate measure of quality that help large publishers and hurt small, but legitimate, operations. Furthermore, new journals are likely to have a lower impact factor than more established titles.) Some scholars are now making use of altmetrics, considered by some to offer a more accurate, wholistic view of the influence of an author\'s work.\n- Who founded the journal? Who owns it/runs it now? Do they have an academic background?\n- Can you retain your copyright, or any subset of copyrights you want to keep, to your work? For example, can you use the publication in presentations, in course readers, in future publications if revised and expanded? Can you add it to your own site, or archive a copy in an online archive/repository?\n- Are there clear guidelines for authors, including when and if fees to authors may be assessed? Legitimate publishers will be upfront with their publication practices. For authors, there should be no surprises or uncertainty about a publisher\'s procedures.\nQuestions to be asked about monograph publishers:\n- Who is on the editorial board? Are the editors clearly identified? Are their email addresses and/or phone numbers provided directly on the publisher\'s site?\n- Is there an editor for your specific subject area? A good example for comparative purposes is the University of Chicago Press and their list of editorial staff.\n- Can you confirm that the editor really is serving in that role? Does the editor have this position listed on their online CV? Can you confirm with the editor via email?\n- Who founded the publishing house? Who owns it/runs it now? Do they have an academic background?\n- Are there clear guidelines for authors? Legitimate publishers will be upfront with their publication practices. For authors, there should be no surprises or uncertainty about a publisher\'s procedures.\nThe questions above are all important to consider during your evaluation of a publisher/publication. No single consideration is more important than the others. Publishing is a complex business, and these questions are designed to work in conjunction with each other to give you an overall picture of a publishing organization.\nAll authors, editors, and reviewers should be familiar with guidelines from the Committee on Publication Ethics: Principles of Transparency and Best Practice in Scholarly Publishing. Being an informed author is important for your own publication record, and for the records of those you mentor or influence.\nPlaces to Publish\n- DOAJ (Directory of Open Access Journals): ""The Directory of Open Access Journals is a service that indexes high quality, peer reviewed Open Access research journals.... The Directory aims to be comprehensive and cover all open access academic journals that use an appropriate quality control system ... and is not limited to particular languages or subject areas.""\n- International Association of Scientific, Technical & Medical Publishers: ""STM is the leading global trade association for academic and professional publishers. It has over 120 members in 21 countries who each year collectively publish nearly 66% of all journal articles and tens of thousands of monographs and reference works. STM members include learned societies, university presses, private companies, new starts and established players.""\n- Society for Scholarly Publishing: ""The Society for Scholarly Publishing ... is a nonprofit organization formed to promote and advance communication among all sectors of the scholarly publication community through networking, information dissemination, and facilitation of new developments in the field.""\n- OASPA (Open Access Scholarly Publishers Association): OASPA\'s mission ""is to represent the interests of Open Access (OA) journal and book publishers globally in all scientific, technical and scholarly disciplines. This mission will be carried out through exchanging information, setting standards, advancing models, advocacy, education, and the promotion of innovation.""\n- WAME (World Association of Medical Editors): WAME is a 501(c)(3) nonprofit voluntary association of editors of peer-reviewed medical journals from countries throughout the world who seek to foster international cooperation among and education of medical journal editors. Membership in WAME is free and all decision-making editors of peer-reviewed medical journals are eligible to join. Membership is also available to selected scholars in journal editorial policy and peer review.""\nAbout Publishing Agreements*\n*NOTE: The information offered here is not intended to serve as legal advice, but merely as a general introduction to the topic.\nAny work in a fixed, tangible form of expression belongs to the creator (except for a few limited cases, such as works made for hire) and is automatically protected by copyright. This means that, early on, as you prepare to publish your work, you own the copyright to your articles, chapters, etc.\nHowever, when you sign a publishing agreement, you may be transferring your copyright to another person or entity. It\'s important to know that:\n- Transferring copyright means that you lose some, or all, of your rights because you give them to someone else. This can adversely impact your ability to use your own work as you wish in the future, such as teaching with it or sharing it.\n- If you sign away all of your copyright, that may affect your ability to:\n...make the work accessible in a digital repository\n...use part of the work as a basis for a future publication\n...send copies of the work to colleagues\n...share copies of the work with students\n...comply with the NIH Public Access Policy or other funding agency policies\n...present the work at conference or meeting and give copies of the work to attendees\n...use a different or extended version of the work for a future publication\n...make copies of the work for personal use and educational use\n...use graphs, charts, and statistical data for a future publication\n...use the work for educational use such as lecture notes or study guides\n...comply with public access mandates\n...deposit supplemental data from the work in an institutional or subject repository\n...place a copy of the work on electronic reserves or use for student course-packs\n...include the work in future derivative works\n...make an oral presentation of the work\n...include the work in a dissertation or thesis\n...use the work in a compilation of works or collected works\n...expand the work into a book form or book chapter\n...retain patent and trademark rights of processes or procedures contained in the work\n- -Adapted from this list.\n- Since publishing agreements are legal contracts, don\'t forget that you can negotiate with the publisher before signing.\n- Publishers do not automatically have to become the holder of the copyright in order for your work to be published by them. In other words, it is entirely possible for you to retain some or all of the rights to your own work.\nHow do I negotiate with a publisher?\nAfter approaching a publisher, authors are presented with a publishing agreement. If they wish to negotiate the terms of such an agreement, authors will attach an addendum (their desired changes) to the contract. The publisher will then respond to that, with any negotiations proceeding from there.\nThe following site offers useful links to ways in which authors typically seek to amend publishing agreements, including suggested author addendums. Or you might find the Scholars Copyright Addendum Engine (which will generate an addendum for you which preserves the rights considered most important to authors) useful.\nWhat is Open-Access Publishing?\nOpen access means: “digital, online, free of charge, and free of most copyright and licensing restrictions” (Suber, 2012).\nOpen access is a model of publishing that has a number of potential benefits to authors, to researchers, and to the general public. Open access can mean increased citation counts, improved retention of author rights so you can use your publications as you wish in the future, and better access to information for researchers at other institutions and in other parts of the world.\nTypes of Open Access\nSharing your research openly can be achieved in a number of ways. Any of these options support open access.\nGreen OA - Self-Archiving in Open Access Repositories\n- Review your publication agreement before signing it.\n- Check the terms related to archiving on an institutional repository, and tell your publisher you want to retain the right to deposit a copy of your work in your institution’s online archive/repository.\n- At UB, this is UB ScholarWorks. (Some journals will allow you to archive a copy of your article in UB ScholarWorks after an embargo period.) You can submit your article now and set up an embargo period in UB ScholarWorks, which will release a copy of your work after the embargo period is up.*\n- *Make sure you upload the version you are allowed to add to UB ScholarWorks (a post-print or final manuscript is different from the publisher’s version).\n- Alternatively, you may want to deposit a copy of your work in a disciplinary repository. Search for open access repositories using The Directory of Open Access Repositories, OpenDOAR.\nGold OA - An Article Processing Charge has been paid for the article to remain open-access\n- This fee can be paid for by an individual, a department, an institution or even by grant monies\n- Publish in an Open Access journal that is recognized by and listed in both DOAJ and OASPA.\n- Find out how to work with publishers to retain your rights by using the Publishing Agreements area of this toolkit.\nPay To Publish?\nSome publishers ask for fees, or article processing charges (APCs), to make your publication openly accessible. Is this okay?\nIf the journal makes all of their publications available open access immediately, then there are no subscription fees. In this case, APCs are the publisher’s only revenue stream for the journal title. In this case, charging APCs is reasonable.\nIf the journal makes only some of their publications OA, then the publisher receives revenue from both subscriptions to the journal and APCs. In general, this is not considered a good practice.\nOther considerations: How committed is the publisher to Open Access? Is OA their main publishing model, or one of several? And, what do they do with their revenue?']"	['<urn:uuid:4efa91b2-1028-4bec-9e02-70fdda8c9992>', '<urn:uuid:87fa3773-1a37-40a6-b077-9ebbcdb9608f>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T03:34:55.284799	10	90	3178
66	How do they check for celiac disease before and after going gluten-free?	Before going gluten-free, doctors use serologic blood tests to check for specific antibodies (tTG and EMA tests), followed by an endoscopy with biopsy of the small intestine for confirmation. However, if someone has already started a gluten-free diet, these tests may give false-negative results. In such cases, genetic testing becomes the primary diagnostic tool, as it's the only test that isn't affected by diet changes. The genetic test can rule out celiac disease but cannot definitively prove its presence - it only indicates the potential for developing the condition.	"['What is Celiac Disease?\nCeliac disease is an autoimmune reaction from eating gluten. Your body responds by attacking the small intestine. Overtime this condition has a severe impact upon absorption of nutrients.\nExperts are yet to pinpoint a definitive cause. However, they have established a number of underlying factors: other autoimmune diseases, viruses, infant nursing methods and even traumatic periods in life can trigger it.\nCeliac disease is also a hereditary condition. This means it runs in families. If you have a parent or sibling with the disease there is a 1 in 10 chance of developing it.(1)\nThis condition is also known as sprue or coeliac.\nLining the small intestine you have small finger like projections. These are called villi and they help the body absorb all the essential goodness from your food. When they are damaged, nutrients cannot be absorbed properly into the body.\nGluten is the name for the protein found in grains. People with celiac disease only react to the form of gluten found in wheat, barley, and rye.\nWhen someone with celiac disease eats anything containing this protein,it damages the villi. This causes a drastic chain of events within your digestive system and your body.\nCeliac disease can be suffered both in childhood or as an adult, yet once you have it, it stays with you for life.\nThis condition is divided into three types: classical, non- classical and silent.(2)\nPeople can have signs and symptoms of malabsorption. This means the small intestine is not absorbing enough nutrients and fluids.\nThey may also have diarrhea or pale, foul-smelling, fatty stools. They experience weight loss.\nEdema is another indication associated with classic celiac disease. This means that excess fluid build up in body tissues. It causes swelling usually in the feet, legs or hands.(3)\nChildren have growth failure and can be irritable or unhappy.\nThis is associated with milder signs and symptoms of stomach and intestinal issues.\nThere can be unexplained anemia due to iron deficiency.\nThey may have just one of the other symptoms of the disease, or many. The symptoms will be classified as mild.\nSometimes people have the disease but display none of the symptoms. The disease usually shows up if they are screened for damage to the intestines.\nIt is thought that between 50% and 90% of people that have celiac disease are not diagnosed.(4)\nThe symptoms for celiac disease can vary from one person to another. Adults and children also display contrasting signs. Even infants under 2 can experience different symptoms to that of older children.(5)\nCommon signs of celiac for infants include: vomiting, chronic diarrhea and swollen bellies. They can also have a poor appetite.\nTheir stools will be pale and smell foul and they may fail to thrive and have muscle wastage.\nOlder Children Symptoms\nOlder children will also experience diarrhea or constipation. Pale, nasty smelling stools and weight loss are also indications of this condition.\nAs they develop they could be shorter than other children of the same age and puberty might also be delayed.\nBehavioral issues and irritability are not uncommon. There are also neurological symptoms.These include attention deficit hyperactivity disorder (ADHD) and learning disabilities.\nChildren also experience headaches, lack of muscle coordination and seizures.\nThe most common symptoms for adults are diarrhea, fatigue and weight loss.\nThey will experience bloating and gas and abdominal pain as well as nausea, vomiting or constipation.\nCeliac disease can prevent the body from absorbing enough iron, this means anemia can occur.\nMouth ulcers or damage to dental enamel on the teeth is another sign to look out for.\nIt can cause headaches and also affect the nervous system. The result is numbness and tingling in the feet and hands. Impaired balance is another sign.\nProblems with memory, thinking, language and judgement are also possible.\nYour spleen can be damaged. The spleen helps fight infection and keep your blood cells healthy.\nUnexplained infertility or recurrent miscarriages are also be indications of this disease.\nThe rash normally appears on the elbows, knees, torso, scalp and buttocks. It is an indication of gluten intolerance.\nIf you are tested for celiac disease via a blood test or genetic screening your doctor may ask you to have an endoscopy. This is the gold standard test for this condition.\nThis is an outpatient procedure where a scope is placed through your mouth to allow a view of the small intestine.\nA biopsy or sample of the small intestine can be taken at the same time. The results of the biopsy will show how much damage there is to the villi.\nThis damage falls into four types.(10)\nThis shows no damage to the villi. The intestinal lining is normal and it is unlikely that you have celiac disease.\nThis shows no damage to the villi. It is a result that is expected if you have been following a gluten free diet. It may also be the result expected from family members of someone with the disease.\nThis type is rare. The villi are normal and the only symptom you have is the skin rash dermatitis herpetiformis\nThis shows the villi are shorter than they should be or not present at all. This is the result when you are experiencing symptoms of the disease.\nThe only successful treatment for Celiac disease is following a gluten free diet. This is a lifelong commitment.\nA referral to a dietitian can help you plan your diet.\nFollowing a gluten free diet will help the small intestine to heal.\nInformation surrounding gluten-free foods are now widely available. The internet provides a wealth of information explaining the grains you can eat.\nFresh foods like fruits, vegetables, meat, poultry, fish, dairy, beans, legumes, and nuts are gluten-free options.(11)\nFoods Containing Gluten\nWheat is the main source of gluten.\nIt can also be found in many other grains and foods. These include: barley, bulgur, durum, farina, graham flour, malt, rye, semolina, spelt and triticale.\nMany foods are now clearly labelled if they are gluten free. Be aware there may be hidden gluten in some products. These include modified food starch, preservatives and food stabilizers.\nThere are other items that contain gluten which you will need to consider. These include prescription and over-the-counter medications.\nSome vitamin and mineral supplements or herbal and nutritional supplements can contain gluten. Other items include: lipstick, toothpaste, mouthwash, envelope and stamp glue.\nVitamins and Dietary Supplements\nMany people with celiac disease are deficient in some types of vitamins and minerals. If this is the case taking a gluten free multivitamin pill will help.(12)\nWhen you are diagnosed you may be screened to check your bone health. The disease can make your bones thinner. You will possibly be prescribed medication or advised to take a dietary supplement.(13)\nYour doctor will perform tests to check the damage to your small intestine. If it is severely inflamed or damaged they can prescribe steroids to help it heal.\nCorticosteroids or autoimmune medications are considered if you have refractory Celiac disease.(14)\nThis means a gluten free diet alone does not relieve the symptoms. The damage to the villi in the small intestine does not heal with a gluten free diet alone.\nEndoscopy, Biopsy and Blood Tests\nYour physician can refer you for blood tests. This will check for levels of antibodies that attack an enzyme called tissue transglutaminase. The antibodies attack this enzyme mistaking it for gluten.(15)\nYou can also be referred for an endoscopy. This allows a doctor to look at the small intestine for signs of damage. They are also able to take tissue samples or a biopsy during this medical process.(16)\nWhen performing these tests you need to continue eating gluten or the tests will not give a true result.\nWhat is celiac disease? Celiac disease is an autoimmune reaction from eating gluten, a protein found in barley, wheat and rye.\nWhat are the signs of celiac disease? Typical signs of this disease are digestive issues. These include diarrhea, vomiting, abdominal pain and constipation. An itchy skin rash is common in people of all ages. You may feel very tired or irritable. Sometimes there are no signs at all.\nHow do you develop celiac disease? Celiac disease is hereditary. This means it could be in your genes. People who suffer with other autoimmune diseases, such as type 1 diabetes or rheumatoid arthritis will often have celiac disease. However, the precise cause isn’t known.\nHow are you diagnosed for celiac disease? Your doctor may take blood tests. The tests will look for raised levels of certain antibodies in your blood. They can also refer you for an endoscopy or biopsy. This will help the doctor identify whether there is damage to the small intestine. Some people don’t have symptoms of the disease and only these tests will reveal if they have it.\nWhat is the best treatment for celiac disease? The only current treatment for this disease is to follow a strict gluten free diet. The diet must be followed for life. Accidentally eating gluten could trigger the symptoms again.\nWhat are the long term complications of celiac disease? This disease can lead to additional serious health problems if not treated. These include the development of other autoimmune disorders like type I diabetes and multiple sclerosis. Other complications are osteoporosis, infertility and miscarriage. It can cause neurological conditions like epilepsy and migraines. It can stunt growth. It can also contribute to intestinal cancers.\nIs celiac disease considered a disability? Celiac disease does not appear to be considered a disability. You may get certain social security benefits to help you if the condition is severe.(17)\nIs there a cure for celiac disease? There is no cure for this condition. It can only be managed by following a gluten free diet. Many stores and restaurants now have foods suitable for people with celiac disease.\nIs celiac disease life threatening? Celiac disease is not life threatening. However, there are some serious conditions associated with the disease. These include diabetes, cancer and liver problems.(18)\nCeliac disease affects the autoimmune system. Eating gluten causes it to attack and damage the small intestine. This leaves the body lacking in nutrients.\nIt’s a lifelong disease which has to be carefully managed otherwise it can seriously impact upon quality of life.\nThe disease is caused by an intolerance to gluten which is a protein found in grains.\nThere is no known cure at the present time however, it can be controlled by following a gluten free diet. Getting used to this type of diet may be difficult and you may need the help of a dietitian.\nThankfully this condition has been acknowledged as a real problem which affects so many lives. This is why you can now find many food products available in stores geared for a gluten-free diet.', 'Genetic and Blood Tests for Celiac DiseaseGenetic testing tells you whether celiac disease is a possibility, and blood tests look for certain antibodies that are signs of active disease.\nGenetic testing (a blood test or cheek swab to check for the presence of certain genes) and serologic testing (a blood test to look for certain antibodies) are both used to help diagnose celiac disease.\nSerologic Testing for Celiac Disease: How Does It Work?\nA serologic test is performed on someone to look for the presence of active celiac disease. For a serologic test, your doctor or clinician will take a blood sample which a lab analyzes for the presence of certain antibodies. Normally, your immune system produces antibodies that protect you from disease. But in celiac disease, the immune system responds not to dangerous invaders like bacteria and viruses, but instead to the gluten (certain proteins found in wheat, barley, and other grains) in the food you eat. Several types of serologic tests may be used to diagnose your celiac disease.\n- Anti-tissue transglutaminase: This test, also known as the tTG, or the tTG-IgA, is the most sensitive test available. One drawback is that the presence of some other conditions can cause a false positive on this type of test.\n- Anti-endomysial antibody (EMA) test: While less sensitive than the tTG test, the EMA test is more specific to celiac disease. The EMA can be used as a follow-up test for people who have the conditions known to yield a false positive on the tTG.\n- Total serum IgA: Your doctor may order this blood test, but it does not directly diagnose celiac disease; instead, this test reveals whether you are deficient in an antibody called immunoglobulin A. If you are low on this antibody, neither the tTG nor the EMA will give an accurate test result. This test can also provide valuable information about how your immune system, in general, is working.\nLimitations of Serologic Testing for Celiac Disease\nSerologic testing methods are only effective in people who have been regularly eating gluten-containing foods. So if a person has stopped eating gluten-containing foods, such as breads, cereals, and pasta, because they suspect they may have celiac disease, they may end up with a false-negative serologic test. “If blood tests come back negative, but you’re still experiencing symptoms, see a GI (gastroenterologist); a biopsy [intestinal] may be in order, despite the negative blood work,” says Carol Shilson, executive director of the University of Chicago Celiac Disease Center. On the other hand, a positive tTg or EMA test does not mean that a person definitely has celiac disease — only an intestinal biopsy can prove that — but it is certainly suggestive of the diagnosis in a symptomatic person.\nGenetic Testing for Celiac Disease: Who Should be Tested?\n“Serologic testing is the screening test. Genetic testing is only done in unusual and complicated circumstances, where there are diagnostic uncertainties for one reason or another,” explains Arthur DeCross, MD, an assistant professor of medicine and the director of the gastroenterology fellowship program at the University of Rochester Medical Center in New York.\n“Usually the biggest reason [for the genetic test] is that the patient put themselves on a gluten-free diet before getting properly tested. This is a big mistake,” says Dr. DeCross, noting that a gluten-free diet can compromise the results of other screening methods like serologic testing and endoscopic biopsy.\n“This (genetic) test is good for ruling out the disease, especially in first-degree relatives of someone who is biopsy-diagnosed,"" explains Shilson. ""For first-degree relatives (parent, child, or sibling), 1 in 22 have celiac disease, often with no symptoms.”\nFor this reason, the genetic test can be performed on close relatives of someone with celiac disease. If the genetic test is negative, then the parents, children, or siblings of a celiac disease patient will not need to be followed for the rest of their lives to look for development of the disease. If, however, a close relative has a positive genetic test, he probably will continue to be screened for celiac disease with serologic testing at regular intervals so that it can be caught early, if it does develop.\nGenetic Testing for Celiac Disease: Pros and Cons\nThe genetic test is accurate, but the information it can provide is limited. “The gene test, performed at a reliable lab, is very accurate,” Shilson says. And the gene test cannot be compromised by environmental factors like diet, which can affect the outcome of your blood work and biopsy tests.\nPositive results are not a definitive diagnosis; they only signal that the potential for celiac disease exists. “Genetic testing cannot prove that you have celiac disease, but in many circumstances it can essentially prove that you are not at risk to get it,” says DeCross.\nGenetic tests and blood serologic tests can give doctors information about a patient’s risk for celiac disease, and can help them determine whether further testing, such as a biopsy, is necessary. With the right diagnosis, someone who has symptoms that indicate celiac disease can get the answers to help manage his condition.']"	['<urn:uuid:9d734d4f-f9fb-49c0-952d-46f52e27f1bd>', '<urn:uuid:bdee4c5e-b378-488c-b9f9-f714299f16cc>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T03:34:55.284799	12	89	2649
67	I'm interested in how different cultures use technology in art. What are the main differences between how experimental films like Free Radicals approached technology, and how modern Latin American artists handle technology in their work?	Free Radicals represented a pure experimental approach to film technology through its innovative scratch animation and rhythmic synchronization with music. In contrast, contemporary Latin American artistic approaches to technology, particularly in Argentina, have been shaped by complex cultural and institutional factors. While some institutions tend to favor spectacular technological displays with emphasis on lighting effects and automated movements, a new generation of artists is emerging who use technology more thoughtfully. These artists, like Claudia Valente and Sebastián Pasquel, create works that don't simply showcase technology but integrate it meaningfully into their artistic concepts, using relatively simple and affordable components to express deeper ideas rather than just technological prowess.	"['A real-time comparison, for scholarly purposes, of Len Lye\'s 1958 experimental animation FREE RADICALS and the opening minutes of Fernando Solanas and Octavio Getino\'s 1968 Grupo Cine Liberación activist film LA HORA DE LOS HORNOS/HOUR OF THE FURNACES\n[Len] Lye\'s Free Radicals (1958) […] is a black and white scratch animation short, cut to the insistent rhythmic accompaniment of an African drum solo.* It immediately calls to mind the unforgettable opening scenes of Octavio Getino and Fernando Solanas\' Third Cinema classic La hora de los hornos/The Hour of the Furnaces (1968).** While equally exciting and radical, these strikingly similar films hint at the extent to which the world and the political landscape had changed in the decade between their respective release dates, and between then and now. We have no way of knowing if Getino and Solanas knew of Len Lye\'s film. We know, however, that films exchange ideas, talking to one another across time, and that the conversation between radical aesthetics and radical politics is ongoing, with both daring to \'make it new\' and set the world on its feet, by turning it upside down. [Jerry Whyte, \'Free Radicals\', CineOutsider.com, December 11, 2011. Online at: http://www.cineoutsider.com/articles/stories/f/free_radicals_1.html]\nA demonstration and a lesson, The Hour of the Furnaces imports into cinema the affirmative aesthetics of the written political treatise. A collective ideal informs the whole film. It anticipates a liberated time. It’s not the product of a single voice but of a chorus of poems (Marti, Césaire), manifestos (Fanon, Guevara, Castro, Juan José Hernández Arregui) and films (by Fernando Birri, Joris Ivens, Nemesio Juárez). It conjoins the powers of didacticism, poetry and agogy (the agogic qualities of a work concern its rhythmic, sensible, physical properties – a notion suggested by the French aesthetician Etienne Souriau). Stylistically, the film uses all possible audiovisual techniques, from flicker to contemplative sequence shots (for instance, the final three-minute shot that reproduces a picture of the dead Che Guevara’s face with his eyes wide open), from collage to direct cinema, from blank screen to animated effects, from the rigours of the blackboard to the hallucinogenic properties of the fish-eye, from classical music to anglophone pop hits. Cinema is an arsenal and here all its weapons are unsheathed. [Nicole Brenez, \'Light my fire: The Hour of the Furnaces\', Sight and Sound Magazine, 8 March 2012. Online at: http://www.bfi.org.uk/news/light-my-fire-hour-furnaceshttp://www.bfi.org.uk/news/light-my-fire-hour-furnaces]\n\'Agogics\' is a musical term that designates the use of agogic accents, that is accents consisting in a lengthening of the time-value of the note. The philosopher Étienne Souriau extended the use of the term to include all the arts existing in time. He defined \'agogics\' as \\what characterizes an artwork that takes place in time, through movement, and specifically through the creation of a fast or slow pace, or the use of different rhythms.\' For musicians, the notion is related to gesture, to physical movements, to a bodily interaction with their instrument, to a sense of speed, an energy, a precise handling of a piece. [Christian Jacquemin et al, \'Emergence of New Institutions for Art-Science Collaboration...\' [date unknown], Online at: seadnetwork.files.wordpress.com/2012/11/jacquemin_final1.pdf. Hyperlinks added by FSFF]\nSouriau developed his idea of the agogic as an explicit reaction to the ‘rather banal description [of] arts of space in contrast to the phonetic and cinematic arts’. [Of] interest is Popper’s use of the term to describe the quality of temporal pattern that he identifies in a range of works. At one extreme [..].] is the velocity and dramatic choreography of a Len Lye installation. The term agogic conflates speed, acceleration and duration and would appear to be a significant aspect of kinetic form. [Jules Moloney, Designing Kinetics for Architectural Facades: State Change (New York: Taylor and Francis, 2011), p. 64.]Film Studies For Free\'s author has been a tad busy elsewhere lately - and there will be lots to catch up with at this blog in due course.\nBut today FSFF is thrilled to present its (by distance) contribution to a teach-in which took place earlier today. This video was produced in a few hours this morning, using readily available materials for quotation (see above and below), in solidarity with a campus occupation that you can choose to read more about here.\nStaff and students of media and cultural studies, working in a deeply personal, activist capacity, gave short presentations on their research and thinking about ideas of resistance, occupation and neoliberalism in the context of the university and beyond, in order to consider how research in their fields might offer new and diverse perspectives on activism and resistance.\nFSFF has always liked its politics, like its film studies, to have rhythm and timing, so its research project today foregrounds those elements. You can find its author\'s scholarly discussion of this kind of ""real-time"" videographic comparison here: \'Déjà-Viewing? Videographic Experiments in Intertextual Film Studies\', MEDIASCAPE: Journal of Cinema and Media, Winter 2013.\nFSFF has devoted a number of previous entries to online and openly accessible resources on Third Cinema and revolutionary aesthetics in the past. See especially this bumper post and this more recent one.\nAnd you can find the two films quoted from above, in low-res online video versions, as per the details and links below.\n- Free Radicals by Len Lye (1958). Online at: http://video.frieze.com/film/len-lye-free-radicals/\n- La hora de los hornos/Hour of the Furnaces by Fernando Solanas and Octavio Getino (1968). Online at: youtu.be/jQOXKoMHOE0', 'PhD Candidate at The National Scientific and Technical Research Council (CONICET)\nUniversidad de Buenos Aires/Universidad Nacional de Tres de Febrero\nKeywords: Latin America, Argentina, new media art field, imaginaries of modernization, identity, institutional strategies, art, science, technology, politics\nOn Shaky Ground\nThe intersection between art, science, and technology around the globe has become an extensive, complex, and relatively undefined territory, shaped by certain characteristics in constant struggle.  Its practices embody the convergence of multiple disciplines, media, and methodologies, which not only elude fixed categorizations of technological artistic practices, but also challenge the traditional conception of material and object-oriented artwork, individually or collectively produced. As a matter of fact, these conventional notions still largely dominate the history, theory, and aesthetics of contemporary art. In this regard, Domenico Quaranta has defined the new media art field as an independent art world, since its development has taken place quite separately from the growth of the broader world of non-technology-based contemporary art.  In the same vein, Geert Lovink asserted that new media artworks are forms in search of a form. He claims: “New media, to its credit, has been one of the very few art forms that has taken the programmatic wish to blow up the walls of the white cube seriously. This was done in such a systematic manner that it moved itself outside of the art system altogether.” \nAt the crossroads of artistic, scientific, and technological spheres, technology-based projects are generally conceived as artworks, although they may also be described as inventive devices, machines, apparatuses, artifacts, or experiments. These classifications derive from their focus on the research phases, the primacy of the creative process over the finished work, and the successive instances of trial and error that guide their execution. Due to their inner condition of new media artworks, they often function under the logic of a machine. Their operations don’t admit mistakes: if the piece doesn’t work, the aesthetic experience cannot be fulfilled. This is one of the main reasons why artists involved with technology-based projects need to acquire knowledge from other fields, such as robotics, informatics, physics, and neuroscience. Furthermore, on several occasions artists team up with engineers, biologists, and/or mathematicians, which enables them to go through the multifaceted investigation and production process.\nDespite the fact that these aspects have determined new media art produced throughout the world, I argue that in the Latin American context, and especially in Argentina, this situation is even more complex given two principles that I recognize as identitarian and institutional. Whereas the first aspect is related to the imaginaries of modernization that have weaved through the Argentinian new media artistic framework, the second refers to the arduous development of the institutions that aim to promote the exchange between art, science, and technology in the country.\nImaginaries of Modernization\nAccording to Beatriz Sarlo, Argentinian imaginaries of modernization have been shaped by a dichotomy between European modernity and local traditions.  The country has been run through poetic and ambivalent conceptions. Throughout its history, it has faced several attempts to build a local industry by appropriating technologies and by adopting European models, and later on, North American ones. The utopia of Argentina as an icon of a modern nation that emerged by the end of the 19th century and persisted into the following one, gave birth to local, cultural, and technological spheres taken as universal domains, executed by universal men. Based on ideas of progress and growth, both deeply rooted in positivist philosophy, the so-called “Generation of ‘80” stood for the concept of “civilization” against “barbarians” (the indigenous population that inhabited the country back then).  Thus, Argentina is commonly known as the “Paris of Latin America”: instead of recognizing itself as a mestizo country, it has erased its indigenous history, chasing the illusion of the “white nation.” Imported models have been considered exemplary and neutral paradigms that were uncritically introduced and pursued. In this sense, the mimetic repetition of exogenous codes turned into a common strategy to legitimize Argentinian practice.\nNowadays, the outcome of this process has given way to the creation of a local new media art field that repeatedly highlights technology—for instance, by putting a strong emphasis on lighting effects or automatizing rapid movements of the devices that compose the work. An example is the recent call for submissions from Centro Cultural Recoleta, a well-known cultural center in Buenos Aires, for events focused on innovation to be included in its annual program. The initiative is mainly interested in all kinds of artistic projects that deal with innovation in terms of astonishing technological developments: attractive laser installations, impactful interactive screenings, frenzied DJ and VJ mixes, etc. In consequence, the utopian imaginaries of modernization that have built a partnership between technological novelty and cultural progress are still active. As a result, the characteristics of our local context are usually ignored or viewed from the perspectives of other realities, thus avoiding the foundation of a personal and independent discourse.\nNeither attitude considers the inherent reciprocal tensions between art, science, technology, and politics. As Claudia Kozak puts it: “Since the technical/technological phenomenon of each era is bounded by a certain society, this implies certain historical and social hegemonic construction of the technological meaning.… As long as the technological poetics take over their contemporary technical phenomenon, they are also political.”  When the author points out that new media art overlaps the artistic and technical phenomena in a particular social fabric that intertwines individuals, tools, institutions, and systems of thought, she is suggesting that the encounter of art and technology is not a neutral junction. Therefore, it is possible to overcome the common position that tends to accept, passively and uncritically, the modern technological project as an undeniable mandate, understanding instead that it has evident political nuances. \nThe imaginaries of modernization enter institutions, and institutions shape specific “ways of being-with technology.”  Consequently, institutions embody a particular technological rationality and sensitivity, which are projected onto their programs. This shows that the meaning of technology goes beyond its instrumental definition, understood as the practical application of knowledge. It follows that technology implies a complex map that not only interconnects material objects, rules, and operations, but also the use that particular individuals give to them, as long as they decide institutional action lines. In other words, the imaginaries of modernization are expressed in the institutional programs, while all their decisions and activities—the institutional management as a whole—contribute to devise the aforesaid imaginaries. The study of both the imaginaries of modernization and the institutional practices aims to subvert certain paradigms of thinking and action that have marked the Argentinian new media art field.\nIf we go over some of the initiatives implemented by Argentinian institutions during the last years, we notice that many of them haven’t undertaken criteria to promote an analytical perspective regarding the local implications of the relationship between art and technology. For example, this is the case of Noviembre Electrónico, an event held in the Centro Cultural San Martín, which depends on the Ministry of Culture of Buenos Aires. On the government’s website, it is described as a space of convergence for artists, developers, designers, and thinkers of digital culture and electronic arts. This definition highlights the encounter of the main actors of the field, but doesn’t focus on the conceptual lines that justify this confluence.\nTracing the history of the Argentinian venues committed to fostering the exchange, production, exhibition, and study of new media art, we see they’ve faced several difficulties that have affected the sustainability of their projects. The growth and expansion of institutions have not been continuous or linear. Due to political, economic, and sociocultural circumstances that have taken place in the past decades, important endeavors came to an end. As an example, this was foreseen by the closure of the Instituto Di Tella during Juan Carlos Onganía’s dictatorship in 1970: and later on, by the end of the ‘70s, the cessation of the interdisciplinary activities developed at the Centro de Arte y Comunicación, two pioneering places for this research. The aforementioned episodes announced a pattern that seemed to be repeated in other cases caused by appalling political circumstances, which cannot be compared to later democratic regimes. This pattern was the result of institutional strategies that, once boosted and disseminated, impacted the cultural scene and eventually dissolved. This dynamic could be called “outbreak growth” and remains valid today. A recent case study is Espacio Fundación Telefónica in Buenos Aires, whose changed management in 2013 shifted its interest from new media art towards other topics related to innovation and the role of technology in contemporaneity. Nowadays, Argentina still lacks museums, cultural centers, or foundations entirely dedicated to new media art.\nAs a result, the identitarian and institutional factors hampered the investigation and encouragement of an Argentinian new media art language. Despite the fact that the country has become the scene for a great number of artists who have been experimenting with the many possibilities offered by technologies, there is still not enough critical reflection about the course that the local production is taking.\nTo reverse this trend, the institutional platforms must face a different direction through the implementation of programs that could deconstruct the established imaginaries, and hence explore local mindsets connected to Argentinian research and the creative context. They should bring transformations by stimulating a local new media art that doesn’t necessarily stick to the exposition of regional, historical, and political events, nor to low technologies (e.g., dismantling and recycling all kinds of devices). It is required to outline a discourse that may assume the local condition without placing technologies at the epicenter of the artworks. Instead, technology should be used in a relevant way, in accordance with the concepts that the artworks develop throughout the exploration of possible relations among materials, devices, and poetics.\nIn this line of work, some interesting projects have been carried out. Espejos de cobre (2015) is an installation by Claudia Valente conceived as an iridescent acrylic mirror. Its folds arise from the detailed investigation of the growth pattern of copper sulfate crystals, which were grown by the artist. Her observations have shown that the atoms of the crystals present an ordered distribution and that their angles remain fixed during their reproduction. The encoded mathematics in nature has been transferred to a mechatronic object that replicates the crystal’s morphogenesis while reflecting the visitor on its faceted structure.\nOn the other hand, Guadalupe Chávez and Gabriela Munguía have been investigating the relationship between art, science, and robotics through the development of Eisenia (2014), a robotic sculpture that considers contemporary technologies and models of production. The engine is made from mechanic and electronic devices based on the functioning of 3D printers. The sculpture has motors that regulate the dripping of hydro nutrients produced by numerous Californian worms that live inside the machine. Accordingly, the printing mechanism causes the seeding of wheatgrass, which grows on a semi-hydroponic substrate.\nAnother interesting case is the work of Sebastián Pasquel, who has been creating useless machines integrated by automaton mechanisms that perform simple actions and collect portraits of the artist’s family found in photo albums. f-242 (2015) consists of different pictures engraved on gypsum boards, which are gradually grinded by rotary polishing machines that turn gypsum into powder. This constant action gives the installation an ephemeral nature that contrasts with the remaining small mounds of plaster displayed next to the boards. The slow and incessant motion reappears in 120 80 mm/Hg (2013), where a fine red thread moves from the spool to a light box with the portrait of the artist’s mother, crossing a cartography of newspaper obituaries. As the thread goes from one point to the other, it seems to sketch the graphic recorded by an electrocardiogram. In fact, 120 80 mm/Hg makes reference to normal adult blood pressure.\nGermán Sar’s reactive installation entitled Mecánica de la dialéctica (2014) presents two opposite typewriters disposed on independent bases, three meters apart. Their keys are locked and don’t present any characters on them. A long reel of white paper, with no inscriptions on it, joins both machines. When a motion sensor detects the proximity of the visitor, the typewriters automatically begin to type spaces, generating repetitive mechanical sounds. This action propels the continuous movement of the paper, which creates a sort of analogic loop. By means of the inquiry of opposed ideological positions, Sar creates a metaphor for the dialectical game that comes with all kinds of power clashes. Additionally, the title alludes to the dialectical-logical process that defines the intellectual work.\nAn analogous metaphor of machine and human behavior characterizes Juan Rey’s works. Printed Circuit Boards (PCB) show different images (such as the artist’s brain and paper balls) traced by thin copper lines, connected to photovoltaic panels and LED lights. Once the device is illuminated, the current pulse goes through the copper lines, the circuit is completed, and the light turns on. Besides referring to the features of electronic circuits, these pieces suggest metaphors of the circulation of ideas and the possibility of “shedding light” on new ones.\nDue to their autonomous functioning, these are prime examples of works that undermine the traditional notion of new media artwork. What is more, they were created with fairly simple and affordable parts, even though they neither pursue a low-tech aesthetic, nor try to address “Latin American-ness.” Like Valente, Pasquel, Sar, Chávez, and Munguía’s installations, the technologies involved are always choreographed through the formal, aesthetic, and conceptual axes of each artistic proposal, depending on personal investigations that concurrently introduce singular poetics to the Argentinian scene.\nIn short, interesting, local, and technology-based art may often arise when artists do not attempt to speak about Latin America as the region is seen from abroad, for this ultimately tends to reinforce the confrontation between peripheral and core countries. Rather, they forge their own discourse from Latin America that might end up outlining a new way of thinking and perceiving the art of the region and the region itself.\n- I refer to all kinds of artistic practices that make material, aesthetic, and conceptual use of electronic and/or digital technologies in different phases of the creative process. It is a broad scene, constituted by a wide variety of manifestations, such as interactive installations, immersive environments, virtual and augmented reality, artificial life, data visualization, locative media, parametric design, bio art, etc. Although certainly video in all its forms—video installation, video performance, video sculpture, etc.—is part of the new media art field, it isn’t included in this study because its development has had a different logic than the one examined in this essay.\n- Domenico Quaranta, Beyond New Media Art (Brescia: Link Editions, 2013).\n- Geert Lovink, “New Media: In Search of The Cool Obscure,” 2007, accessed February 25, 2016, http://bampfa.berkeley.edu/media/lovink.mp4.\n- Beatriz Sarlo, Una modernidad periférica: Buenos Aires 1920 y 1930 (Buenos Aires: Nueva Visión, 2007), 15.\n- Between 1880 and 1910 Argentina was governed by the Generación del 80 (Generation of ‘80), whose members held the highest political and economic positions. Among other policies, they intended to turn Argentina into a modern nation by establishing a European-style liberal political regime.\n- Claudia Kozak, ed., Tecnopoéticas argentinas: archivo blando de arte y tecnología (Buenos Aires: Caja Negra, 2012), 182–183.\n- Héctor Schmucler, “Apuntes sobre el tecnologismo o la voluntad de no querer,” Artefacto (December 1996), accessed February 26, 2016, http://www.revista-artefacto.com.ar\n- Carl Mitcham, “Tres modos de ser con la tecnología,” Anthropos, no. 94/95 (1989): 14.\nJazmín Adler is an Argentine art historian, researcher, and curator who lives and works in Buenos Aires. She is pursuing her doctoral studies with a scholarship from the National Scientific and Technical Research Council (CONICET). Her research interests cover the relationship between art, science, technology, and politics in Latin America.\nShe is a member of Ludión: Latin American Exploratory of Poetic/Political Technologies (Instituto Germani, Universidad de Buenos Aires) and teaches “Theory and Aesthetics of Electronic Arts,” “Theory and Aesthetics of Interactive Art,” and “Development and Production of Electronic Arts Projects” in the Masters in Technology and Aesthetics of Electronic Arts Program (Universidad Nacional de Tres de Febrero), as well as “Digital Art II” at Universidad Maimónides. Over the past years, she has curated new media art exhibitions such as Sincronías en abismo (Buenos Aires Photo), VISCERAL (Panal 361) and Mecánicas de pensamiento: obras en proceso (Museo Castagnino).']"	['<urn:uuid:71464135-296d-4fa2-ab29-db90bfffd649>', '<urn:uuid:ae333040-5066-4ba9-b0dc-6d36be470994>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T03:34:55.284799	35	108	3608
68	How has the business of space exploration changed since the first human spaceflight?	Space exploration has transformed dramatically since the first human spaceflight in 1961 by Yuri Gagarin. Initially dominated by government agencies, space activities have become increasingly commercialized. Private companies have attracted over $18.4 billion in investment from 2000 to 2017, with ventures ranging from space tourism to asteroid mining. NASA is now actively supporting private sector involvement, planning to end direct funding for the International Space Station by 2025 and allocating $2.6 billion for private lunar missions. The focus has also shifted from direct space colonization research to more practical 'routes' to settlement, including space tourism, asteroid mining, and planetary defense, though securing consistent profits remains a challenge for private companies.	['One would be forgiven for thinking that space colonization was born and died with Gerard O’Neill. The shadow he cast over the field is a long one, and the work he and his fellows did on the subject permeates every aspect of space colonization. Although not as celebrated or publicized, work on space colonization has continued to this day – new papers, theories, concepts, and designs for habitats are released every year, and with the sudden interest in asteroid mining, the world is primed for a new look at space colonization. In this conclusion to Living in Space, we will take a look at where the field of space colonization is now, and where it might be heading.\nO’Neill’s work on orbital colonies was first publicized in the mid-1970s and was covered in detail in the previous Living in Space segments. In short, O’Neill, with the help of his students and other scientists, put forth the idea that space colonization should be the next goal for Space Agencies of the world following the Apollo landings. His team designed several space habitats and eventually released the idea to the public in the form of a book –” The High Frontier.”\nIn the years following O’Neill’s efforts, several authors and scientists joined him and released their own texts on the subject – the vast majority of which agreed with many of O’Neill’s ideas, making small tweaks to the designs or the details of living in space. In 1977 alone, a half a dozen books on the subject were released, each author adding a little to the concept. This was the peak of interest in Space Colonization. Disaster struck later that year however when Senator William Proxmire cut funding for colonization research from NASA’s budget. This move killed off almost all public and scientific interest in the concept overnight. Similarly, while many believed that the idea of building solar power stations was an attractive idea during the late 70s when oil prices skyrocketed, when prices dropped in 1980, funding for space solar power research dried up quickly. These two events were a one-two punch to the early Space Colonization movement, and saw the field shrink to a few hardcore proponents.\nNevertheless, some research continued. The 90s saw a surge in interest in the field of asteroid mining following the release of “Mining The Skies” by John S. Lewis. This led to what some call the second space colonization movement. Some books were published applying Lewis’ ideas to colonization, and the concept of asteroid mining to the space industry as a whole. It was during this time that NASA was building the International Space Station (ISS) and many saw this as our first step to colonizing the stars.\nThis second push would also eventually peter out, but did provide us with the first wholly new space habitat design in decades, the Lewis One.\nLewis One was designed in 1991 by Al Globus and published at Princeton. Lewis One is a settlement on the same scale as the Stanford Torus but with a cylinder-like design, consisting of a long tube sandwiched between a massive solar panel and a large T shaped heat sink. The Lewis One makes several improvements over the Torus; we asked Globus what made his designs better than O’Neill’s:\n“It has thermal rejection. It uses interior cylinders to increase working/storage areas. It is a simpler geometry as it uses only artificial light. It needs less shielding per unit [of] living area than the Stanford Torus. It rotates the shielding which eliminates a horrendous failure mode in the Stanford Torus. It has 0g areas for tourists. It has a geometry suitable for building more settlements in a shirt sleeve environment.”\nWhile Lewis One marks an important step in the current space settlement movement, it still has several issues. The lack of real sunlight and greater mass for the same projected population as a Torus particularly stand out.\nThese issues and others led Globus to design the Kalpana One, the latest in space settlement design.\nKalpana One started life when two Indian students spent several months working with Globus on space settlements, their work culminating in the design for Kalpana One. Named after Kalpana Chawla, one of the victims of the Columbia disaster, Kalpana is a smaller design than the Lewis One, housing around 3000 people. The shape is relatively unique, consisting of a short cylinder, with internal gravity once again produced by rotation.\nKalpana One, more so even than the Lewis One, was designed to fix the issues inherent in O’Neill’s designs, providing a solution to the excessive shielding mass, requirement for large mirrors to bring in sunlight, and rotational instability. Additionally, Kalpana One was designed to be a low Earth orbit (LEO) habitat that could also be built in various other positions around the solar system, unlike the O’Neill designs which were designed primarily as Lagrange point habitats.\nThe full research papers of Kalpana One and Lewis One are viewable on NASA’s ARC Settlement page, one of the few NASA funded space settlement initiatives remaining. The papers are very accessible and discuss construction, specifics of every aspect of the designs, and potential drawbacks and downsides for their respective designs. The Settlement page also includes a variety of CGI renders and virtual tours of the habitats.\nGoing by the above, it may seem like the topic of Space Settlement is effectively dead. A handful of papers and two designs in over 30 years may seem dire, but in fact the conversation has merely shifted. While research and debate continues, the field has moved from direct discussion of habitats to discussing and researching ‘routes’ to settlement. No longer do we see books and papers on colony designs and their implications, instead we see discussion on asteroid mining, space tourism, and planetary defense. “There’s not a whole lot going on for the simple reason that there is no institutional money available,“ Globus told us. “There is money available for the paths to space settlement (tourism, space solar power, and planetary defense), or at least the first and third. There is a lot of progress in those areas.”\nThis can be seen as a very positive shift – one of the main mistakes made by O’Neill and his contemporaries was putting the horse before the cart. Discussing the resources we could exploit in asteroids and the endless energy potential of space solar power makes it clear why space is a valuable place to live, and why its colonization could benefit humanity as a whole. O’Neill’s work was revolutionary, and its form inspiring, but by focusing primarily on the end goal (“we should live in space”) he alienated many; this was one of the causes of the downfall of the Space Settlement movement.\nWith Planetary Resources planning to mine the skies, SpaceX eyeing Mars as a colonization goal, and NASA publicly stating their ”end goal” is to one day colonize space, the space settlement movement is primed to rise once again, in a different form perhaps, but with the same fire that drove advocates in the 70s. The wealth of space is still there, waiting for us to reach out and grasp it – perhaps this time we will have the will and the means to reach them\n Proxmire would later earn the ire of space researchers everywhere by cutting funding to scores of NASA projects and space research, including SETI. He would later apologize for slashing SETI funding but never to O’Neill.', 'On 12th April 1961, cosmonaut Yuri Alekseyevich Gagarin became the first man in space when he completed one orbit of the earth in the Russian Vostok spacecraft. A similar, yet equally extraordinary event occurred 43 years later when, on 21st June 2004, Mike Melvill crossed the 100-km boundary line that marks the edge of space in the rocket-powered aircraft SpaceShipOne. With this achievement, he became the world’s first commercial astronaut, securing the US$10 million Ansari X Prize for the SpaceShipOne team.\nMelvill’s flight harnessed all the latest aeronautical technology – the culmination of more than 45 years of human space exploration. However, SpaceShipOne’s mission is memorable for what it did not use: a single government dollar.\n“Making the cosmos profitable may prove harder than putting a man on the moon.”\nSince that pivotal moment, space has become increasingly commercialised. According to analytics and research firm Bryce Space and Technology, space-related start-ups attracted over US$18.4 billion of investment from 2000 to 2017.\nHundreds of new companies have formed, countless entrepreneurs have mobilised, and along with a few charismatic billionaires, they have all entered a different kind of space race. The idealistic milestones of the Cold War space race are no longer relevant – this contest is far more pragmatic. Making the cosmos profitable, however, may prove harder than putting a man on the moon.\nThe new space companies looking to build a free market beyond the Kármán line – the boundary where space begins: 100 km above sea level – all face a nagging incongruity. Unlike public agencies such as NASA and the European Space Agency, privately funded companies must eventually turn a profit. Currently, however, the only reliable way of generating profit has been through contract work for publicly funded space agencies.\nSatellite, equipment and crew launches, along with research opportunities paid for by public coffers, have enabled the burgeoning private space industry to grow. Moreover, government support for private companies in space is only set to increase. The most recent budget presented to the US Congress calls for NASA to end direct funding for the International Space Station by 2025, effectively handing off earth’s lower orbit and the maintenance of the ISS to outside agencies. Another more lucrative opportunity for new space companies came in late 2018 when NASA announced the allocation of US$2.6 billion over the next decade for private companies to fly small payloads to the moon.\nNASA is candid about their rationale to contract private companies for lunar exploration and development. The move may have a basis in cost-cutting, but NASA‘s stated intention is to increase competitiveness thereby encouraging the complete commercialisation of space. The realisation of this goal, however, is not unlike our knowledge of deep space: nobody knows what it will look like or how to get there.\nAn Economy in a Vacuum\nIn terms of funding, current activities in space are entirely public or a mixture of both public and private. For true space commerce to exist, private sector companies must offer products or services for private sector customers using solely private sector capital. Presently, satellite television is one of the only viable industries which meets these stipulations, but other commercial endeavours are starting to take off.\nPrivate space travel or space tourism is perhaps the most visible expression of the commercialisation of space. Regardless of its viability, it’s certainly the most publicised among the services offered by the private sector through companies like Richard Branson’s Virgin Galactic. The company plans to launch its first paying passenger in 2019, with more lining up ready to pay US$250,000 for an extraterrestrial ride. The potential scope of space tourism was further elucidated in September 2018 when Elon Musk’s SpaceX announced it would ferry Japanese billionaire Yusaku Maezawa around the moon in 2023.\nCommodities are another consideration: the precipitous demand for metals resulting from the exponential growth of populations reliant on digital technologies has private companies looking elsewhere for resources. Not only has this recent push generated extensive interest in excavating the ocean’s floor, but it has also seen the inception of several new space mining companies. In the near future, the private industry hopes to make significant profits by securing valuable minerals from asteroids and other celestial bodies.\nA 2017 report by investment firm Goldman Sachs declared that the financial and technological barriers presented by asteroid mining were low – even comparable to terrestrial mining operations. The Government of Luxembourg is invested: their Space Resources initiative, worth US$227 million, seeks to establish Luxembourg as the European hub for space resources.\nStill, there are some who believe the best opportunities for the commercialisation of space lie in the immediately viable GPS and imaging sectors. Orbiting satellites that can provide everything from communication to spectrum optics services for private industry can turn real profit today rather than sometime in the next 30 years.\nThen there’s logistics – closer to the ground and less glamorous but arguably as important are the start-ups developing practical solutions for space infrastructure. Life support systems, crew habitats, traffic management and food products are the framework that other, more grandiose ventures will depend on over the next several decades.']	['<urn:uuid:e9eacd4a-138f-4132-8739-b13a198e3846>', '<urn:uuid:fce92499-5b39-4dba-aa72-a1ccea93ab90>']	open-ended	with-premise	concise-and-natural	similar-to-document	three-doc	novice	2025-05-13T03:34:55.284799	13	110	2102
69	What is the minimum airspeed typically required for an aircraft to perform an aileron roll maneuver while maintaining proper control?	The minimum airspeed needed depends on the aircraft's design, but is generally about 120 to 200 knots.	"['The aileron roll is an aerobatic maneuver in which an aircraft does a full 360° revolution about its longitudinal axis. When executed properly, there is no appreciable change in altitude and the aircraft exits the maneuver on the same heading as it entered. This is commonly one of the first maneuvers taught in basic aerobatics courses. The aileron roll is commonly confused with a barrel roll.\nThe aileron roll is commonly executed through the application of full aileron in one direction. In some lower powered general aviation and aerobatic training aircraft, prior to applying aileron input, the pilot must begin the maneuver by trading altitude for airspeed (i.e. diving). This helps achieve enough airspeed to complete the roll without losing rudder and aileron control. The minimum airspeed needed depends on the aircraft\'s design, but is generally about 120 to 200 knots. Because full aileron is applied, structural limitations prevent many aircraft from performing the maneuver at very high speeds.\nStarting from level flight, the pilot pitches the aircraft up about 10 to 30 degrees above the horizon, into a brief climb. The purpose of pitch-up is twofold. This causes an increase in altitude which minimizes altitude loss and airspeed gain. As the aircraft begins to roll, it starts to lose lift. When the wings are vertical, the only lift generated is a small amount from the fuselage, and the aircraft will begin to lose altitude. The brief climb compensates for the loss, allowing the aircraft to complete the roll at the same altitude the maneuver began. When the aircraft is completely inverted, the increased pitch results in greater angle of attack, enabling the inverted wing to generate lift.\nAfter the initial pitch-up, the pilot places the elevators in the neutral position. Failure to do this will cause the aircraft to continue pitching up during the upright part of the maneuver, and downward in the inverted part, resulting in something resembling a barrel roll. The pilot then applies full aileron, accomplished by moving the stick to either the right or left. As the aircraft rolls about its longitudinal axis, the nose will begin to drop. Upon completing the roll, the nose will usually be 10 to 30 degrees below the horizon, so the pilot will need to pitch-up to return to level flight.\nAn aileron roll is an unbalanced maneuver. As the roll begins, the aircraft will have a tendency to yaw away from the angle of bank, referred to as ""adverse yaw."" The pilot will usually need to apply the rudder in the direction of the bank to keep the aircraft balanced. An aircraft performing an aileron roll will actually fly along a slightly helical path, and a very light, positive g force will be maintained.\nAn aileron roll is similar to the slow roll, and the two maneuvers are often confused with each other. However, unlike a slow roll, an aileron roll is performed at the maximum roll rate, and is uncontrolled in the pitch axis. It consists of a constant attitude change during the maneuver; from the initial pitch-up to the plane following a slight corkscrew path as the nose drops, followed by the final pitch-up. If the pilot picks a reference point on the horizon, directly ahead of the plane, the nose will actually appear to trace a shape similar to the letter ""D"" above this reference point.\nThe aileron roll is commonly used in air shows and aerial combat training. The use of the pure aileron roll in air combat is contentious, but many common maneuvers bear heavy dependence on the aileron roll. Examples of this are the Immelmann turn, barrel roll, and Split S.\nAn aileron roll carried out by a pilot as a sign of victory or celebration is known as a victory roll.\nTest pilots commonly employ the aileron roll to evaluate an aircraft\'s turning characteristics (e.g. time to turn).\n- Aerobatic Figures\n- Geza Szurovy, Mike Goulian (1994). Basic Aerobatics. Tab Books. pp. 49–51.\n- Ward; et al. (2006). Introduction to Flight Test Engineering (3rd ed.). New York: Kendall Hunt.']"	['<urn:uuid:84a96067-3d4e-4504-9794-7adaf3d8c259>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T03:34:55.284799	20	17	678
70	I'm curious about Virgin Galactic's space flights - what's the actual flying experience like, and how much do I need to pay for a ticket?	The space flight experience involves a 90-minute journey where a mothership called WhiteKnightTwo carries the space plane to 50,000 feet. The space plane then detaches and fires its rocket engine, reaching speeds of 2,300 miles per hour and climbing to more than 50 miles above ground. Passengers experience weightlessness for about a minute and get views of Earth and space. The plane then glides back to Earth using a feathering system. As for tickets, they are currently priced at $450,000, requiring an initial deposit of $150,000.	['Branson’s brief joy ride is more than two decades in the making. He founded Virgin Galactic in 2004 with the goal of creating a winged spacecraft capable of taking up to eight people, including two pilots and six passengers, on rocket-powered flights that reach more than 50 miles above Earth, which the US government considers the boundary marking outer space.\nCNN Business will also be sharing the livestream and running a live blog with updates.\nHere’s everything you need to know before the big event.\n- Beth Moses, who holds the title of Chief Astronaut Instructor at Virgin Galactic and will handle the training for all of the company’s future customers. She’s flown to space on VSS Unity once before, during a 2019 test flight. Moses, an aerospace engineer, won’t just be along for the ride. She’ll be ensuring her fellow passengers stay safe and ensure that Virgin Galactic collects all the data it needs because this flight will be, at the end of the day, still a test flight.\n- Colin Bennett, who is the company’s lead operations engineer. Bennett will help evaluate the overall experience and ensure the cabin equipment is in good shape.\n- Sirisha Bandla, Virgin Galactic’s vice president of government affairs and research. Bandla will be on board for the science. Virgin Galactic frequently flies experiments to makes use of the microgravity environment, and on this flight Bandla will be handling a University of Florida research project that involves handling “handheld fixation tubes,” according to the company.\nVirgin Galactic says that Branson’s job will be to use his “observations from his flight training and spaceflight experience to enhance the journey for all future astronaut customers,” according to the company.\nWhat will happen?\nWhen most people think about spaceflight, they think about an astronaut circling the Earth, floating in space, for at least a few days.\nThat is not what Branson will be doing on VSS Unity, which is the only operational SpaceShipTwo spaceplane that Virgin Galactic has in its arsenal, though the company is building others.\nVSS Unity will be affixed to a massive mothership, called WhiteKnightTwo, that looks like two sleek jets attached at the tip of their wings. The mothership takes about 45 minutes to cruise along and slowly climb with VSS Unity to about 50,000 feet. Then, when the pilots give the go-ahead, SpaceShipTwo drops from between WhiteKnightTwo’s two fuselages and fires up its rocket engine, swooping directly upward and roaring past the speed of sound.\nVSS Unity is a suborbital space plane, meaning it won’t drum up enough speed to escape the pull of Earth’s gravity. Instead, it’ll rocket at more than three times the speed of sound — about 2,300 miles per hour — to more than 50 miles above ground. At the top of the flight path, Branson and his fellow passengers will briefly experience weightlessness. It’s like an extended version of the weightlessness you experience when you reach the peak of a roller coaster hill, just before gravity brings your cart — or, in Branson’s case, your space plane -— gliding back down toward the ground.\nAfter about a minute the engine shuts off, leaving the spacecraft and the passengers suspended in microgravity as SpaceShipTwo rolls onto its belly and offers the passengers sweeping views of the Earth below and the inky black void above.\nTo conclude the trip, SpaceShipTwo uses what’s called a feathering system to raise its wings, mimicking the shape of a badminton shuttlecock to reorient the vehicle as it begins to fall back to Earth. It then lowers its wings as it glides back down to a runway landing.\nA group of reporters will be allowed in to watch the launch. CNN Business will post live updates here as well as carrying live TV coverage.\nHow is this different from what SpaceX and Blue Origin do?\nBezos’ Blue Origin took a far different approach for its suborbital space tourism rocket. The company’s New Shepard vehicle is a capsule and rocket system that fires off vertically from a launch pad, sending passengers on a screaming 11-minute flight to more than 60 miles high before the capsule deploys parachutes to bring them gently back down.\nBut when the companies begin commercial operations, Blue Origin and Virgin Galactic will be direct competitors. They’re both after the demographic of ultra-wealthy thrill seekers willing to fork over hundreds of thousands of dollars to experience a supersonic gut punch and a few minutes of weightlessness.\nElon Musk — the other, other space billionaire — is running a far different operation than what Blue Origin and Virgin Galactic will put on display this month.\nThough Branson’s other company — Virgin Orbit — has put a rocket in orbit, and Bezos’ Blue Origin plans to get there eventually with a rocket called New Glenn, neither company has made quite the headlines or the waves in the space sector as SpaceX has.\nHow risky is this?\nSpace travel is, historically, fraught with danger. Though the risks are not necessarily astronomical for Branson’s jaunt to suborbital space, as Virgin Galactic has spent the better part of the last two decade running its space planes through test flights.\nStill, any time a human straps themselves onto a rocket, there are risks involved — and Branson has apparently decided that, for him, it’s worth it.', 'Reaching the stars may not just be a metaphor anymore as companies race against each other to make this fantasy of many, a reality. The space tourism industry is at a very nascent stage, at the moment, with huge prospects to expand. According to Northern Star Resources estimates, revenues from commercial orbital and suborbital tourism is likely to grow to over $3.4 billion by 2028.\nExhibit 1: Space tourism industry by revenues\nRichard Branson’s Virgin Galactic (SPCE) has many firsts to its name- it was the first space tourism company to go public and the first one to take a billionaire out into space in their own spacecraft. However, its journey to space has not been smooth, marred by repeated flight delays due to safety concerns. The company showed signs of recovery as it reopened ticket sales last week, for its much-anticipated commercial space travel, scheduled for later this year.\nTicket to space is Virgin Galactic’s ticket to income\nWhile Virgin Galactic’s primary business is taking people on a space ride, it has to wait until late 2022 to start commercial space trips. Currently, the company is at a pre-revenue stage. From the second quarter of 2020 to the first quarter of 2021, the company did not post any revenues. However, carrying payload cargo and offering engineering services to the U.S. government generates side income for the spacecraft manufacturer.\nIn last year’s September quarter, the company earned bumper revenues from ad sponsorships and the ticket sales that followed the successful flight of founder Branson into space. It had sold about 700 out of the planned 1,000 tickets as of November, indicating a growing interest in space travel. For a 90-minute journey, the tickets are now priced at $450,000, including an initial deposit of $150,000. The company said it had received a good response for the hiked ticket price from $250,000 earlier, reaffirming people’s willingness to shell out more money for this luxurious excursion. The passengers will be able to view the earth from space and experience out-of-seat weightlessness for a few minutes.\nExhibit 2: Virgin Galactic’s quarterly revenues (2019-2021)\nSource: Company Financials, Data compiled by Stockal\nImproved cash position and significant reduction in losses in 2021\nOverall, in the year 2021, Virgin Galactic nearly halved its losses to $352.90 million from the previous year, helped by labour productivity and postponement of some non-critical work. It also reported an improved cash position with about $931 million held in cash, as of December 31, compared to $679 million a year ago. In addition to this, the company had raised $425 million last month through a debt offering to fund its commercial spaceflight operations.\nThe billionaires’ space race\nWhat differentiates Virgin Galactic from Elon Musk’s SpaceX and Jeff Bezos’ Blue Origin is that Richard Branson’s space tourism company has been publicly traded since its October 2019 IPO while the other two continue to be private entities. This means that Virgin Galactic has to conduct its market-disrupting business under the hawk eyes of Wall Street regulations by opening its books to the public.\nThe company’s primary focus is on giving non-astronauts private individuals a taste of space through its reusable spaceships. Besides, its way of launching rockets makes it very different from its peers. Virgin Galactic does not fire its rocket vertically from the ground. The craft is flown by a jet called WhiteKnightTwo to 50,000 feet, from where the spaceship detaches to move in a near-vertical ascent to 300,000 feet.\nSpaceX on the other hand was founded with the aim of making reusable rockets to lower the cost of flying into space. Since then, it has blasted payloads into space for the government and NASA and routinely sends astronauts to the International Space Station (ISS). But Musk’s ultimate aim is to build a city of 1 million people on Mars by 2050.\nBlue Origin also focuses on carrying payloads and people into space, besides building its own set of satellites for internet services. Bezos’ space flight shortly followed rival Branson’s maiden trip in July last year. The company sent Star Trek actor William Shatner in October, making him the oldest human in space at the age of 90.\nThe “SPACE” ahead for Virgin Galactic\nThe company’s mission to put safety first before anything else led to delays in the launch of commercial services which meant losses in the short term for the company. However, this strategy is right from a long term perspective, saving Virgin Galactic from a total crash in case of any mishappenings.\nBy mid-2023, Virgin Galactic plans to provide three flights a month with its VSS Unity and VSS Imagine spacecraft. It is also emphasising on rapid production of its Delta class of spaceships which will fly once a week with six passengers– a greater number of times than its previous two designs. In order to reduce costs and focus on design and final assembly, Virgin Galactic will outsource the production of the majority of Delta’s parts. It aims to produce six Delta crafts each year, with the first one expected in late 2025 and paying passengers in the next year.\nAccording to Chief Financial Officer Doug Ahrens, the company expects adjusted EBITDA margins of over 30% once it turns into a profitable business. It also expects to become cash flow positive by 2026.\nAre you ready to take off with Virgin Galactic?\nAs the management reiterated that it is on track for commencing private space flights later this year, the idea of space tourism has started to gain a real shape. Moreover, the reopening of ticket sales promises a steady source of revenue for a company with virtually zero to negligible sales currently.\nOne major challenge to sales is the expensive pricing of the tickets which will take years to come down for more people to be able to afford the services. Besides, how many of the travellers will be willing to repeat this few minutes of space thrill and buy a ticket again? For now, Virgin Galactic is branding its service as a niche experience that has attracted a lot of attention from millionaires.\nThe company is also looking at providing transcontinental flights from one place on earth to another, fulfilling the ever-present need for a faster mode of transportation. Just like how the aviation industry has boomed over the decades, superfast point-to-point travel also has a huge potential to generate big revenues for the company.\nHence, looking from a long-term investment perspective, Virgin Galactic is that growth stock which should definitely take off after a period of laggard growth. So, are you ready to grab a seat on Virgin Galactic’s spacecraft, and its growth journey by investing in its stock?']	['<urn:uuid:9ac985d9-6337-49a9-8913-6a788d0283c9>', '<urn:uuid:99018818-2b53-4bf0-bc17-8fe2deb1db29>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T03:34:55.284799	25	86	2002
71	I'm researching famous Mexican-American civil rights cases from the 60s. What was significant about the Edcouch Elsa School student protest case?	The Edcouch Elsa School walkouts became the first major legal victory for the Mexican American Legal Defense Fund. The students were initially expelled for participating in the boycotts, which were part of the Chicano student protests happening across the southwest. They were demanding ethnic studies, better educational opportunities, and the hiring of Mexican American teachers in the Rio Grande Valley, an area that was predominantly Mexican American.	"['Q&A - 8 Questions with Renee Tajima-Peña\nThe Academy Award nominee talks about family ghosts and her latest, \'Calavera Highway.\'\nFor years, Renee Tajima-Peña and her husband Armando Peña talked about searching for his father Pedro who vanished as if in a dream during childhood.\n""But life happened,"" said Renee. More specifically, marriage, a baby and an untimely death happened. The marriage was to Armando and later came baby Gabriel.\nThe death of Rosa, Armando\'s mother, was devastating to her sons — all seven of them, said the Sansei filmmaker perhaps best known for her Academy Award-nominated documentary, ""Who Killed Vincent Chin?""\nBut it also raised some questions about the Peña\'s family past that needed answers, so Renee, Armando, his brother Carlos and Gabriel piled into a car for a trip across America to bring Rosa\'s ashes back to her native Texas.\nThey captured all of the raw emotions with a camera.\n""Calavera Highway,"" is Renee\'s latest documentary about her own family\'s haunted past. While screening the film in Dublin, Ireland, she caught up with the Pacific Citizen through e-mail. — Lynda Lin\nPacific Citizen: How difficult was it to make this film when the subjects are your family members?\nRenee Tajima-Peña: It was difficult for all of us because Armando and Carlos were dredging up a painful history that they had never quite reconciled. They were talking a lot between themselves, unbeknownst to me, about what they were finding out, or afraid they would find out. On the other hand, these guys grew up without a father, and [are] very, very independent. I\'m a part of the family, so they certainly weren\'t impressed that I was a film director, and by nature aren\'t given to taking direction. Plus my kid is a pretty hardheaded little guy. The three of them were a handful to say the least.\nPC: You include footage of giving birth to Gabe. Talk about the importance of this scene.\nRTP: The film explores how these seven brothers learned how to be men and fathers without the benefit of a father of their own. That scene was central to establishing that theme with the narrator and central character, my husband Armando, and his own emotional journey. What is interesting to me about Armando and his brothers is they\'ve become very involved dads, despite growing up with absent fathers.\nPC: What was the biggest challenge of traveling by car halfway across the country with a child?\nRTP: Gabriel was about four when we shot the film. We actually took two vans — one for the characters and essential filming crew so that we could shoot while driving. The other one was for luggage, equipment, running errands, etc. To tell you the truth, traveling on a documentary is generally a lot of fun. We hire people we enjoy spending time with, and it\'s basically, \'road trip!\' We talk, joke around, hang out together.\nPC: Did you see any similarities in Armando\'s family history and your own?\nRTP: My family started immigrating from Japan at the same time Armando\'s family started migrating to Texas, during the early 1900s. His grandfather landed in the Rio Grande Valley to harvest oranges and pick cotton; my grandfather went to Hawaii to cut cane on the sugar plantations.\nWe also had the same political coming of age in the student movement. He participated in the historic 1968 Edcouch Elsa School walkouts. It was part of the Chicano student blowouts going on all over the southwest at the time. They were demanding ethnic studies, better educational opportunities, hiring Mexican American teachers (the Rio Grande Valley is and was overwhelmingly Mexican American).\nThe boycotters at Edcouch Elsa were expelled and their case became the first major legal victory for the Mexican American Legal Defense Fund. A few years later I was involved in various strikes and walkouts demanding Asian American studies at my school in California. Even though we came from different cultures and economic backgrounds, this politicization and immigration heritage means we both \'get it\' about the other\'s background.\nMy family was interned at Heart Mountain, Wyoming. Last year my husband, Gabriel, and another Mexican American friend and his son went on a fishing trip to June Lake and they stopped to show the kids Manzanar. I thought that was cool.\nPC: In the backdrop of this family profile is a dark spot in American history — the Bracero Project and subsequent deportations tore families apart. What parallels do you see between the past and present day immigration policy?\nRTP: Huge parallels. When I read about ICE raids and little kids watching their parents being rounded up and detained, I can\'t help but think those children are like Armando\'s brothers watching their father being hauled away by immigration authorities when they were little boys. I grew up with my grandparents, who could never go home for over 50 years because the Issei couldn\'t become naturalized citizens until what, 1959? So the idea of how families are torn apart is very real to me.\nPC: I laughed out loud at the scene where Armando pushed miso soup as a possible answer to cancer. Was this your influence?\nRTP: No, that was all Armando. He lived in the Sawtelle neighborhood of Los Angeles for years, ever since he was in graduate school at UCLA. That\'s an old Japanese American community, so he has always been familiar with Japanese food. And he\'s a bookworm, which we make a lot of fun of in the film. So when his mother was diagnosed with cancer, he researched alternative medicines and macrobiotic diets. That\'s why he introduced her to miso soup.\nTheir mother, Rosa, complelely got into miso, shitake and different teas. It was very familiar to her because she had grown up in rural Mexican American communities with medicinal herbs and folk medicines. I remember I was once walking with her in Central Park in New York and she noticed some kind of herb she recognized from Texas. I had to stop her from picking it up because of all the rat poisons and stuff they use in the city.\nPC: Armando made a life-changing revelation about the identity of his father. Has he come to terms with it in real life?\nRTP: But the film was documenting real life! His conclusion in the film basically describes what he believes about the situation. In the end, his mother was the central force in her sons\' lives, and the notion of a father was not important in the end. He\'s told me it\'s something you could never really reconcile. But he has his own son now, and he\'s got all his brothers.\nPC: What are you doing in Dublin? And what is your next project?\nRTP: I\'m showing two films, \'Calavera Highway\' and \'The New Americans\' in a Master\'s class I\'m giving here. I am executive producing \'Whatever It Takes.\' The director is a very talented, first-time director, Christopher Wong. You can see a trailer of the film on the Web site: www.whateverittakesdoc.com.\nTo buy \'Calavera Highway\' on DVD, go to: www.pbs.org/pov/calaverahighway.Printer-friendly version']"	['<urn:uuid:4f10f1a0-92c9-47a1-9114-5ba0e46088e2>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T03:34:55.284799	21	67	1183
72	What tourist attractions can someone visit nowadays in the City of San Jose del Monte to enjoy nature and outdoor activities?	The City of San Jose del Monte offers several natural and outdoor attractions including: VS Orchidarium, a 5-hectare garden with hundreds of orchid species in Barangay Sto. Cristo; a Pineapple Farm in Barangay Tungkong Mangga; Kaytitinga Falls in Barangay San Isidro; Marina Fishing Resort, a 5-hectare man-made lake in Barangay Sapang Palay Proper; and Mt. Balagbag which offers trekking/camping opportunities and views of the Manila skyline.	['“The Evergreen City”\nThe idea of reduccion was widely spread during the Spanish Colonial Period. This was one of the systems used to spread Catholicism to the remote areas of Bulacan. Reduccion or reduction of population was practiced in the largely populated areas particularly in Meycauayan, Bulacan and San Jose, Centro Da Baloges.\nIn March 1750, a decree from the Archbishop of Manila on the creation of new municipalities was announced in Lagulo Church in Meycauayan. The decree included the list of families who volunteered to be relocated. Where previously the only occupants were Itas and Dumagats, San Jose del Monte (SJDM) became a municipality as a result of reduccion from Meycauayan. These families brought with them rice, wine, nganga and salt from Lagulo (now Malhacan) in exchange for the wild pigs, deer, yantok and almasigan of the Itas and Dumagats. Solares, including intended lots for main roads, were peacefully distributed to the new occupants after being measured and surveyed.\nThe municipality of SJDM was founded on March 2, 1752. The population, not exceeding 200 people, belonged to the family of farmers and stonecutters of Libtong and Meycauayan. They lived a simple lifestyle and raised fish, root crops, vegetables, fruits and other natural products.\nUnder the American regime in 1901, SJDM, being less progressive, weak and lacking in peace and order was placed under the political supervision of Sta. Maria, Bulacan. Under American dispensation in 1918, the town became an independent Municipality with Honorable Ciriaco Gallardo as the first Municipal Mayor.\nThe Japanese Imperial Army took over the local government of San Jose del Monte from 1942-1943. In resistance, the municipality formed its own guerrilla unit. SJDM experienced many casualties when the Americans bombed the Poblacion on January 11, 1945 and again on January 14, 1945. The Municipal Building was burned by dissidents on October 10, 1950.\nThe year 1961 marked the opening of the first Government Resettlement Project, the Sapang Palay Resettlement Area, covering 752 hectares.\nIn January 25, 1978, nine barangays were created under P.D. 1921. With the passage of the new local government code in 1991 came the reformulation of equal wealth sharing between the national and local units and the realization of the residents of having their own barangays. This move led to the creation of an additional 41 barangays under Provincial Ordinance promulgated by RA 337 in December 1991.\nWith the advent of the new millennium, SJDM’s population registered a staggering 315,807. It continues to grow as private subdivisions mushroom in strategic areas of the Municipality.\nOn September 10, 2000, SJDM was proclaimed as a Component City under Republic Act No. 8797. It is said to be the largest town in the whole province of Bulacan in terms of land area and population. Also known as the “Balcony of the Metropolis”, it is the first City in the province of Bulacan and recorded as the 86th City of the Philippines.\nOn December 18, 2003, the City of San Jose became the 1st Lone Congressional District in Bulacan.\nIn August 2007, the city’s population numbered to 439,090 based on the National Census conducted by the NSO.\nPopulation/ Language/ Area\nThe City of San Jose del Monte (SJDM) comprises 59 barangays and encompasses a total land area of 10,553 hectares (based on data from the Land Management Bureau). The Local Government Unit (LGU), however, claims an actual territorial area of 31,294 hectares which include disputed land areas with adjacent municipalities. Based on the National Statistics Office Survey, SJDM registered a population of 315,807 in 2000.\nLocated at the northeast periphery of Metro Manila, the City is bounded by the Bulacan municipalities of Marilao and Santa Maria on the west, and Norzagaray on the north. Quezon province lies to its east, Rizal Province to its southeast and Kalookan City to its south. Quirino Highway serves as the main road which allows SJDM access to Norzagaray on the north and Kalookan City on the south. The City is likewise accessible from Santa Maria on its west via the Santa Maria-Tungkong Mangga Provincial Road.\nSJDM was proclaimed the first City of Bulacan on 10 September 2000. In the Provincial Physical Framework Plan for 1998 to 2007, about one third of the City is proposed as an Urban Expansion Area. This same area lies along the Norzagaray-SJDM Growth Corridor, one of the three growth corridors proposed for the province of Bulacan.\nProducts and Services\nMajor agricultural crops are leafy vegetables, root crops, cassava, pineapple, mango and coffee beans.\nThe major income earner of the city is large- and small-scale swine production. There are 60 commercial livestock and poultry farms in the city.\nThe city has 3 major business located in Tunkong Mangga, Muzon and Sampol Market. They are into wholesale and retail trade.\nMajor industries in the city include iron and marble works, furniture, handicrafts, food processing, housing, etc.\n- VS Orchidarium – a 5 hectares garden where hundreds of orchid species can be found, located at Barangay Sto. Cristo.\n- Pineapple Farm – the sweet pineapples are planted here, located at Barangay Tungkong Mangga.\n- Cattle Creek Golf and Country Club – Favorite recreational area of some executives and businessmen, located in Barangay Sapang Palay Proper.\n- Kaytitinga Falls – an esteemed attraction of the city which can be found in the elevated Barangay of San Isidro.\n- Marina Fishing Resort – a 5 hectares man-made lake located at Barangay sapang Palay Proper.\n- Mt. Balagbag Trekking/Camping Site – an added attraction also comes in view in the beautiful Mt. Balagbag where you can see a breathtaking Manila skyline.']	['<urn:uuid:bceaaa08-2a59-43e7-a010-aca129acfe97>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T03:34:55.284799	21	66	929
73	theater programs autism college benefits	Theatre Arts programs serve dual purposes - at institutions like Le Moyne College, they train students to use theatre to entertain, inspire, challenge and build community through faculty-directed shows and student projects. For individuals with autism spectrum disorder (ASD), theater participation has been shown through research to improve social skills, reduce anxiety, and enhance social communication through peer interaction and role-playing activities.	"[""The Visual and Performing Arts Department at Le Moyne College in Syracuse, New York takes learning far beyond textbooks and lectures. Educating the Whole Artist is the department’s driving force, and students are encouraged to take the reigns on many academic and artistic projects during the year.\nLe Moyne College offers students an exceptional educational experience characterized by its foundation in the liberal arts, its commitment to excellence, and its faculty, who are wholly invested in their roles as teachers, mentors, and advisors. Academic offerings include more than 30 undergraduate programs in the humanities and natural sciences, as well as graduate programs. Of special note is the Visual and Performing Arts Department at Le Moyne College. Five programs are offered in the Visual and Performing Arts: Theatre Arts, Music, Dance, Arts Administration, and Visual Art.\nThe Le Moyne College Visual and Performing Arts Department boasts two performance spaces, a dance studio, a scene shop, classrooms, a scenic & graphic design studio, music practice rooms, and other spaces that support the department. The main theater features moveable seating and state-of-the-art lighting and sound equipment allowing for quality theater productions and featured artists. Our secondary performance space is a small black box theater, which is used for rehearsals, auditions, workshops, and performances.\nStudents in the Theatre Arts program are trained to use theatre to entertain, to inspire, to challenge, and to build community. The Theatre Arts program produces three faculty directed shows a year in addition to two (or more) student directed, written, or devised projects. Any student involved in this award-winning program experiences the rich, collaborative, and joyous nature of theatre art while seeking ways to express themselves creatively and to serve their community. Theatre majors also have the opportunity to study abroad or to double major in other disciplines, such as Communications, Psychology, Peace and Global Studies, Creative Writing, Political Science, and many more.\nLe Moyne’s vibrant and growing Music program provides students with the opportunity to continue their musical studies and participate in a wide range of performing ensembles. Students of all majors can deepen their understanding of musical language and performance, and students may choose to pursue a music minor. Faculty-directed ensembles include a full orchestra, jazz ensemble and multiple choral groups, including an award-winning vocal jazz ensemble. The Music and Theatre Arts programs enjoy close collaboration every other year through a co-produced fully-staged musical.\nThe Dance Minor is open to students from all majors and levels of experience. The Dance Minor program is designed to give students dance technique, choreographic and performance training, and an understanding of the artistry and the context of dance in society. The Dance Minor offers students an opportunity to explore multiple approaches to physical expression through exciting courses, including Musical Theatre Dance, Modern Dance Technique, and many more!\nThe interdisciplinary Arts Administration program offers students the opportunity to apply management concepts in an arts environment. The program is a partnership between the College of Arts & Sciences and the Madden School of Business and is the fastest-growing minor on campus. Students in the program study arts marketing, fundraising, financial planning, and many other topics, which are then applied through internships with Arts and Cultural organizations. The program also regularly hosts visits by national arts leaders – recent guests include executives from the Cincinnati Symphony Orchestra, Disney Theatricals and the Dallas Black Dance Theatre.\nThe Le Moyne College Visual Art program offers a variety of courses in diverse disciplines, including sculpture, ceramic, drawing, illustration, and art history, as well as black-and-white, color, and studio photography. These courses encourage students to explore and develop their personal creativity, and take pride and pleasure in their artistic achievements. Students may exhibit their work in many venues, including the annual student art show at the Wilson Art Gallery, and are also encouraged to join the student-led Art Club.\nLe Moyne College, situated on a picturesque 160-acre campus, is located in Syracuse, New York. It has an enrollment of approximately 2,800 undergraduate students and 600 graduate students. Le Moyne brings in students from 33 states across the United States and 35 foreign countries. For the past 21 years, U.S. News & World Report has recognized Le Moyne’s academic excellence and value by naming it one of “America’s Best Colleges” among regional universities in the North. The College has also, for the past four years, been named by The Princeton Review as one of the best colleges in the Northeast.\nParticipant ReviewsWrite a Review\nThis program hasn't been reviewed yet. Write the first review!\n“I never would’ve imagined I’d be leading various groups, directing and so much more. The support the faculty and students give one another truly allows everyone to meet his or her true potential in everything we produce.”\n“While working events or partaking in meetings we were treated as professionals, not as students. We were expected to live up to that standard, and that really made an impression on me.”\n“Having been in multiple vocal groups I learned a great deal about how to collaborate with others to achieve a mutual goal… a skill that my colleagues are now having to learn.”\n“From my participation in all three musical groups I learned time management and how to handle many different tasks at the same time. These groups taught me versatility and that in life you need to be open to different things if you want to succeed.”\n“I have always enjoyed music, but my participation in the College’s programs has drastically impacted my outlook on the importance of music.”"", ""Treating ASD With Theater: Interview with Expert Dr Blythe Corbett\nTheater provides opportunities for youth with ASD to improve communication and social skills.\nAutistic spectrum disorder (ASD) is “characterized by impairment in social competence as well as restricted interests and behaviors.”1 These deficits, which many consider to be lifelong and resistant to treatment, manifest across multiple domains.2 Traditional treatments involve psychotropic drugs and specialized education.\nHowever, other less traditional interventions may be helpful. One such intervention is the involvement of autistic youth in theater, both as performers and as audience. An article3 that appeared in the December 2016 issue of the AMA Journal of Ethics explored the role that theater might play in the lives of children with ASD.\nTo gain further insight into this novel intervention, Psychiatry Advisor interviewed Blythe Corbett, PhD, author of the article. Dr Blythe is an Associate Professor in the Department of Psychiatry and Behavioral Sciences and in the Department of Psychology, Director of the SENSE Lab, and an investigator in the Vanderbilt Kennedy Center in Nashville, Tennessee.\nPsychiatry Advisor: What is the role of theater in the life of a child with ASD?\nDr Corbett: The role of theater is two-fold. One aspect is expressive. All forms of art have the potential to nurture talent, reduce stigma, and enable others to witness the abilities of a person with a disability. Art enables those with mental illness to build community and form a new identity. In the case of theater, performance can provide both entertainment and education about many vital aspects of human experience, stimulate the imagination, and provide opportunities to observe social communication in a live setting. Whether the person with ASD is on the stage or in the audience, theater can provide a unique setting in which to reduce rigidity of thinking and learn communication and other social skills.\nPA: What is your approach to using theater as an intervention for children with ASD?\nDr Corbett: Our group utilizes an empirically validated approach called Social Emotional NeuroScience Endocrinology (SENSE) theater. We train non-autistic children and adolescents to actually be the interventionists. Some of them are young actors, who become role models for the children with ASD, showing them how to interact socially with others, how to initiate conversations, or how to understand humor. We use a variety of techniques, including role-playing, improvisation, theater games, video modeling, and character development.\nPA: Has your group conducted any studies of SENSE theater in children with ASD?\nDr Corbett: Our group has performed studies that found SENSE to be effective in improving social skills and reducing anxiety in youth with ASD. For example, we studied 30 youth with ASD (aged 8 to 14 years) and found that SENSE improved social competence and reduced trait anxiety typically associated with social interaction with peers.2\nPA: What interested you in this type of research?\nDr Corbett: My background is as a professional writer and actress, so I have an appreciation of the impact of theater in how we perceive and respond to the world. Since I also work with autistic children who are challenged in social communication, I thought this might be an ideal population in which to see whether theater can have an impact on enhancing these skills. I also have been involved with research in social play and stress. From this I realized that the social communication of children with ASD can be enhanced by engaging with a peer. This led to the idea of a peer-mediated intervention.\nPA: What obstacles do youth with ASD have that might prevent them from enjoying the benefits of participating as an audience member at the theater?\nDr Corbett: There are actually several obstacles that autistic individuals face in the theater and in other cultural and recreational activities. Hyper- and hyposensitivity to an array of visual auditory and tactical stimuli can lead to difficulties with crowds, flickering lights, darkness, and loud noise. An even more significant obstacle is stigma. Although autism has become more familiar to the public through books, the media, and news, and there are fewer negative stereotypes than there used to be, stigma remains a major problem. Most parents of children with ASD report that individuals with autism are stigmatized. In autism, as in other mental illnesses, ignorance, prejudice, and discrimination continue to be apparent.\nPA: What role might psychiatrists play in addressing the needs of youth with ASD?\nDr Corbett: In treating children with ASD, it is important for psychiatrists to think beyond the clinical setting and the more classical models of pharmacologic interventions and traditional therapies. A treatment plan should encompass other players within the community, including family, school, and community at large, and should retain a broad focus on the full functioning of the individual.\nFor example, psychiatrists can communicate with the school, perhaps facilitated by a social worker, and identify whether there is a “buddy program” in which the child with autism can be paired with a “helper” in the school. Classes in school can teach musical expression, public speaking or theater that can augment the more typical “social skills interventions” already in place, allowing the child with autism to engage with the community in an intentional way. This is part of the child's treatment plan, but it utilizes naturally occurring resources within the community.\nPsychiatrists can also play an important role in education and advocacy. I emphasize the importance of access to events such as theater performances. Participating in these events, feeling part of a community and society, is an important part of what enriches our lives as human beings and what makes them satisfying. I feel it is the responsibility of those of us in the mental health profession to help individuals with mental illness to gain greater access. This involves advocacy as well as education of the public to dispel myths and stereotypes.\nPA: Are there other theater-based programs that use theatrical techniques as a form of treatment for children with ASD?\nDr Corbett: The Autism Theater Initiative and the Theater Development Fund have created “autism-friendly” performances with accommodations such as brighter lighting, reduced sound, and preparatory story guides. These make the experience less intense and stressful, and allow children with autism unprecedented access to theater.\n1. American Psychiatric Association (APA). Diagnostic and Statistical Manual of Mental Disorders (DSM-5). Washington, DC: APA, 2013.\n2. Corbett BA, Blain SD, Ioannou S, Balser M. Changes in anxiety following randomized control trial of a theatre-based intervention for youth with autism spectrum disorder. Autism. 2016 May 5; doi:10.1177/1362361316643623. [Epub ahead of print]\n3. Corbett BA. Autism, art, and accessibility to theater. AMA J Ethics. 2016;18(12):1232-1240.\n4. Corbett BA, Key AP, Qualls L, Fecteau S, et al. Improvement in social competence using a randomized trial of a theatre intervention for children with autism spectrum disorder. J Autism Dev Disord. 2016;46(2):658-672.""]"	['<urn:uuid:021e1cb9-2bcd-4517-82ff-e540aaa9eed9>', '<urn:uuid:f8503d25-0cee-4d31-9ee5-d0b8c6290f73>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T03:34:55.284799	5	62	2056
74	temple of sun beijing location history construction original purpose	The Temple of the Sun is located in the Jianguomenwai diplomatic district of Beijing. It was built in 1530 and was one of four shrines where the emperor worshiped key heavenly bodies. The 50-acre park houses a main stone altar, which is a flat disk about 20 feet across and raised about two feet off the ground. During the Cultural Revolution (1966-1976), the altar was badly damaged by Mao's zealots, and the park became a dumping ground for rubble when the city's walls were torn down. The altar has since been reconstructed.	['A view of Ritan Park in Beijing, which houses the Temple of the Sun. It was built in 1530, one of four shrines where the emperor worshiped key heavenly bodies.CreditAdam Dean for The New York Times\nWhen I first came to Beijing in 1984, the city felt dusty and forgotten, a onetime capital of temples and palaces that Mao had vowed — successfully, it seemed — to transform into a landscape of factories and chimneys. Soot penetrated every windowsill and every layer of clothing, while people rode simple steel bicycles or diesel-belching buses through the windy old streets.\nThen, as now, it was hard to imagine this sprawling city as the sacred center of China’s spiritual universe. But for most of its history, it was exactly that.\nIt wasn’t a holy city like Jerusalem, Mecca or Banaras, locations whose very soil was hallowed, making them destinations for pilgrims. Yet Beijing’s streets, walls, temples, gardens and alleys were part of a carefully woven tapestry that reflected the constellations above, geomantic forces below and an invisible overlay of holy mountains and gods. It was a total work of art, epitomizing the political-religious system that ran traditional China for millenniums. It was Chinese belief incarnate.\nBeijing’s cosmology changed in the 20th century, especially after the Communist takeover in 1949. Its great city walls and many of its temples and distinctive alleys, or hutong, were destroyed to make way for the new ideals of an atheistic, industrial society. The 1980s brought economic reforms and uncontrolled real estate development, which wiped out almost all of the rest of the old town. Lost was a vast medieval city of 25 square miles and also a way of life, just as the local cultures of the world’s other great cities have been swamped by our restless times.\nOver the years, I have watched some of this transformation, first as a student, then a journalist and now a writer and teacher. Like many people who have fallen in love with this city, I was disheartened and felt Beijing’s culture was lost.\nBut in recent years, I have begun to think I was wrong. Beijing’s culture is not dead; it is being reborn in odd corners of the city and in unexpected ways. It is not the same as the past, but still vibrant and real — ways of life and belief that echo bygone days.\nI see this in two places in this city where I now live. One is the Temple of the Sun neighborhood in the eastern part of the city, and the other a Taoist temple in the western part. These are places that seemed forgotten and irrelevant, but they have slowly taken on a new importance in recent years as Chinese search for new values and beliefs to underpin their post-Communist society.\nFor most of my time in Beijing, I have always lived within walking distance of the Temple of the Sun. A 50-acre park in the Jianguomenwai diplomatic district, the temple was built in 1530, one of four shrines where the emperor worshiped key heavenly bodies. The others are dedicated to the moon, the earth and heaven. The Temple of Heaven is easily the most famous, but the Temple of the Sun reveals more because it is less of a showpiece.\nLike virtually every landmark in Beijing, the temple was badly damaged during the Cultural Revolution. This was a period of radical Communist violence from 1966 to 1976, when every place of worship and many symbols of the past were attacked. The main stone altar, a flat disk about 20 feet across and raised about two feet off the ground, was smashed by Mao’s zealots. Later, the park became a dumping ground for rubble when the city’s walls were torn down.\nI came to know the park eight years after that traumatic period ended. I studied Chinese language and literature at Peking University from 1984 to 1985 and biked over to this area because it had become the country’s chief diplomatic district and one of the few places where homesick Westerners could buy chocolate and postcards. In the post-Mao era, China was opening up, and so it built embassies and modern apartment blocks to house foreign diplomats and journalists. This area became an international hub, with a “Friendship Store,” an International Club and a western-style hotel that had one of the city’s few bakeries. I came for the croissants, but stayed for the tree-lined streets and the Temple of the Sun.\nI remember walking through the park, its altar recently reconstructed, but many of the buildings so dilapidated that the grounds seemed abandoned. A few Beijing residents ventured in occasionally to fly kites from the original stone balustrades, which were cracked and discolored like old bones. Diplomats’ children would run around the altar’s low peripheral wall testing its acoustics; if you whispered into it, a friend could hear your voice a dozen feet away.\nIn 1994, I returned to China to work for seven years as a journalist, first for The Baltimore Sun and later for The Wall Street Journal. I ended up moving into one of the diplomatic compounds and the neighborhood became my home. Again, I was drawn to the Temple of the Sun.\nBack then the park had an entrance fee that kept it relatively empty, especially in what was now a crowded, bustling city. It cost only 50 cents to enter, but China was still relatively poor and people weren’t inclined to spend their time on exercise. You worked, you went home, you rested. Parks were for special occasions. It was possible to walk the roughly one-mile perimeter path and encounter only a few people, often diplomats from the nearby embassies or spooks keeping an eye on things.\nThe Temple of the Sun wasn’t just empty of people but looked barren. This was a time when Chinese parks rarely had grass. Instead, the hard-packed dry earth of arid Beijing was raked by crews every few days. It was odd but had an austere beauty that set off the ginkgo and persimmon trees that lined the paths.\nBy the time I returned to China in 2009 to work as a writer and teacher, all of this had changed. China had enjoyed three decades of fast economic growth and government coffers were overflowing. Besides aircraft carriers, the Olympics and high-speed rail, it spent its money on parks and greenery.\nThe Temple of the Sun gained grass, new trees, flower beds of tulips in the spring and geraniums in the summer and stands of bamboo that are so foreign to this colder part of China that they have to be laboriously bundled up against the cold each autumn.\nBest of all — or worst, depending on your selfishness — the authorities also got rid of the entrance fees. Suddenly the park was part of the city, embraced by residents eager for activity. Unlike years ago, many Chinese want to exercise, and so the park is now filled with joggers in black spandex sprinting past restaurant workers in greasy smocks.\nBut this need for green space clashes with another trend in China: the surrender of public areas to the rich. Just as Beijing’s bike lanes have become turning lanes for cars and its sidewalks overrun with motorbikes delivering hot meals to the upper-middle class, huge swaths of the Temple of the Sun have been sacrificed to benefit a wealthy minority.\nSince about 2000, I estimate that about 15 percent to 20 percent of the park’s area has been rented out to relatively high-end restaurants, an exclusive social club, a German beer garden, a yoga yard, a strange antique furniture store that is always empty (and smells of some sort of dodgy corruption scheme), a Russian restaurant and stores exhibiting wholesale wares for Russian traders — all commercial activities that don’t belong in this great old park.\nWith so much of the park’s area lost to these money-spinning activities, the Temple of the Sun has been reduced to the rebuilt altar in the center, a small hill, a tiny lake and one main path. With no entrance fees and no space, the path is so crowded that it sometimes feels like a fast-spinning hamster wheel that one enters and exits at one’s peril.\nAnd yet I still love the park. When I follow the counterclockwise flow, I keep an eye out for the skyscrapers peeking through the weeping willows, the Tai Chi master by the lake and the old pines that somehow have survived the tumult. I even listen for the screech of the tacky children’s amusement park with its half-broken choo-choo trains.\nBut the park is more than a window into people’s daily lives; for the government, it is once again a way to increase its legitimacy. The authorities run a tiny museum that exhibits, as if real, recreations of the smashed altar pieces. It has also put a big steel fence around the altar to show its earnestness in protecting cultural heritage. And it has erected an information board explaining the temple’s history while excising all mention of the Mao-era losses. The goal: assuring Chinese that the Communist Party, which once attacked tradition, is now its guardian.\nOver the past few months, this message has been reinforced by colorful propaganda posters lauding traditional ways to run a family. Famous theorists from past millenniums are introduced and their works given a quick explanation. We learn the virtues of obedience and of listening to one’s parents, and of course taking care of them, all preoccupations of a government whose decades of draconian family planning policies have left it with a rapidly aging population and a rebellious youth that ignores its parents.\nOnce in a while, somewhat awkwardly, the Communist state even recreates the old rituals. In March, some friends of mine, retirees who are amateur singers and musicians, were hired as extras for a ceremony on the spring equinox. About 30 of them dressed up in gowns and Qing dynasty-era hats and marched solemnly to the altar. Accompanied by a small orchestra of musicians playing gongs, cymbals and kettle drums, they strode up to a table filled with imitation dead animals laid out for sacrifice. A young man dressed as the emperor then kowtowed and made the ritual offerings, all under the strict guidance of experts from the local cultural affairs bureau who had read accounts of the ancient practices. Later, videos streamed around social media platforms like WeChat, reinforcing the popular idea that the past is returning.\nRecreating traditional values is one of the Chinese leader Xi Jinping’s key domestic policies, but anything like a return to the past seemed impossible in the 1980s. Being raised in a fairly religious household, I had been curious what Chinese believed. I didn’t expect or want Chinese people to share my beliefs, but I figured they must believe in something.\nThat had seemed like a mistaken assumption. Looking for Chinese religion one autumn afternoon, I rode my bike for an hour down to the White Cloud Temple, the national center of China’s indigenous religion, Taoism. This religion coalesced in the second century out of folk religious beliefs and the teachings of philosophers like Lao-tzu and Chuang-tzu. The White Cloud Temple dates from the 13th century and is the headquarters of the national Taoist association.\nThe temple was beautiful but seemed inconsequential. Its main axis of five halls to various deities had been mostly untouched by the Cultural Revolution, and the incense and the old trees gave it a timeless feel. But it was empty of worshipers. The halls and courtyards felt like those token places of worship in Communist countries that were more like museums than functioning centers of a living religion. Surrounded by Communist-era housing and a belching power plant, the temple was much like the Temple of the Sun, a relic of a bygone era.\nBut over the past decade or so, Chinese have been searching for meaning in their lives. After decades of adopting foreign ideologies like fascism, communism and neo-liberalism, they wonder what remains of their culture. Temples like White Cloud and belief systems like Taoism are part of this search for answers.\nAnd so, cleverly, the government has invested heavily in religions like Taoism (as well as Buddhism and folk religion, but less so in Christianity or Islam). The White Cloud Temple is trying to reclaim some of China’s traditional medical heritage by opening a clinic in a newly refurbished wing of the temple. The state also built a new Taoist academy to train priests. Slowly, a Taoist revival has spread across China.\nYou can sense this by walking through the temple. The admission fee of $6.50 does keep out many people, but the temple is still filled with priests heading off to classes or preparing for ceremonies. And on either side of the main axis are two new strings of courtyards with temples to various gods.\nFor fun, but also to see the sorts of Taoist-related products that people buy for their homes nowadays, it’s well worth visiting the temple’s main gift shop. Inside the main gate is one filled with unusual products like wall clocks decorated with the eight trigrams and the swirling Tai-chi symbol, as well as scepters, swords and even Taoist robes if you want to go back home dressed like an immortal. It also sells stone rubbings of some of the temple’s steles, including strange representations of the human bodies showing the energy channels, or meridians, of Chinese medicine.\nCompared with the sacred city of the past, today’s Beijing is a slightly out-of-control urban area of highways and high-rises, subway and suburbs. The old cosmological tapestry is in shreds.\nBut it is a place where places have meaning. The urban historian Jeffrey F. Meyer, who wrote “The Dragons of Tiananmen: Beijing as a Sacred City,” points out that Chinese capitals always reflect the governing ideology. This is true of all capitals, of course, and Mr. Meyer also wrote a book on Washington about the ideas behind its monuments.\nBut unlike open societies, which are messier and where the official message is often lost or at least softened by competing voices, Beijing is still the capital of an authoritarian state. Beijing’s message is still the state’s message, perhaps not perfectly but still audibly. This state once despised tradition but now supports it. And so the city changes — not back to the past but into something made up of ideas from the past — of filial piety, respect for authority, traditional religions, but also privilege for the rich. As Mr. Meyer put it, then as now, “Beijing was an idea before it was a city.”']	['<urn:uuid:71363373-6aed-4cdc-a0b7-b8aaf14b75e0>']	open-ended	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-13T03:34:55.284799	9	92	2448
75	I'm worried about global warming and heard trees can help - how much of the world's forests have we actually lost since humans started cutting them down?	According to a 2015 study in the journal Nature, 46 percent of trees have been felled since humans started cutting down forests.	"['What Happens If All Of A City\'s Trees Are Cut Down? 🎥\nTrees are of invaluable importance to our environment and to human well being; they contribute to the environment by providing oxygen, better air quality, improving our climate, conserving and storing water and preserving soil. During the process of photosynthesis, trees take in carbon dioxide and produce the oxygen we breathe. Not only are trees essential for life, but as long living species on earth, they give us a link between the past, present and future. It is critical that woodlands, rainforests and trees in urban settings, such as parks, are preserved and sustainably managed across the world.\nFor us humans, every aspect of our life is reliant on the natural environment including the food we eat, the air we breathe, the water we drink, the clothes we wear and the products that are made and sold to create jobs and drive the economy. Wood has been used for thousands of years for fuel, as a construction material, for making tools and weapons, furniture and paper. More recently it emerged as a feedstock for the production of purified cellulose and its derivatives, such as cellophane and cellulose acetate.\nBut the worldwide occuring, ongoing deforestation, has an almost irreversible impact on our environment. Deforestation is the conversion of forested areas to non-forest land for use such as arable land, pasture, urban use, logged area, or wasteland. Deforestation can also be seen as removal of forests leading to several imbalances ecologically and environmentally, resulting in declines in habitat and biodiversity. Urbanisation, mining, fires, logging and agricultural activities are few of the causes of deforestation.\nThe destruction of trees therefore encourages global warming. Changing temperatures will alter which organisms can survive in an ecosystem.\nA National Geographic article from February 2019 states:\n""As the world seeks to slow the pace of climate change, preserve wildlife, and support billions of people, trees inevitably hold a major part of the answer. Yet the mass destruction of trees—deforestation—continues, sacrificing the long-term benefits of standing trees for short-term gain.\nForests still cover about 30 percent of the world’s land area, but they are disappearing at an alarming rate. Between 1990 and 2016, the world lost 502,000 square miles (1.3 million square kilometers) of forest, according to the World Bank—an area larger than South Africa. Since humans started cutting down forests, 46 percent of trees have been felled, according to a 2015 study in the journal Nature. About 17 percent of the Amazonian rainforest has been destroyed over the past 50 years, and losses recently have been on the rise.""\nThese are disturbing facts about the continuing deforestation worldwide. 68 percent of the world population is projected to live in urban areas by 2050, says UN (United Nations). Today, 55 percent of the world\'s population lives in urban areas, a proportion that is expected to increase to 68 percent by 2050.\nSo when it comes to “cities”, what word comes to mind? For me it is busy, loud and polluted. And when you think of “forests”? Most likely it will be peaceful.\nWhat if cities would be something different to how we know them?\nTrees improve the livability of our cities for countless reasons. However, for many years tree coverage in our urban areas has been decreasing. Large mature trees which reach the end of their lives are often replaced with smaller species - if at all. These replanted trees then struggle to establish and reach maturity due to the demands of paved surrounds around them. The value trees offer our cities are not just relevant to landscape architects, urban planners, developers and local authorities, they are of critical importance to urban populations as a whole.\nPlease watch the following video. Explore what makes trees a vital part of cities, and how urban spaces throughout history have embraced the importance of trees.\nBy 2050, it’s estimated that over 65% of the world will be living in cities. We may think of nature as being unconnected to our urban spaces, but trees have always been an essential part of successful cities. Humanity has been uncovering these arboreal benefits since the creation of our first cities thousands of years ago. So what makes trees so important to a city’s survival? Stefan Al explains.\n🎥 (5:25) What happens if you cut down all of a city’s trees? - Stefan Al (TED-Ed)\nOne of the simple solutions that everyone can participate in is planting trees. Each part of the tree contributes to climate control, from leaves to roots, in three primary ways: they lower temperatures, reduce energy usage and reduce or remove air pollutants.\nWhether you plant trees around your home and property, in your community, nature reserves or in our national forests, they do help fight climate change.\nAs you can see, trees are beneficial to tackle climate change and an important part of keeping our environment healthy in many ways. Their contribution doesn’t stop here.\nTrees truly ""green"" our planet in a countless number of ways. Why not plant one today?\nNational Geographic, Feb 2019 (viewed 28.04.2020)']"	['<urn:uuid:b44d80b6-c9dc-413f-8e04-6ba49ea13f27>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T03:34:55.284799	27	22	848
76	glass blowing history phoenicia rome murano development	Glass blowing originated in Phoenicia in the 1st century BC, then spread through the Romans across Europe. Later, in 1291, Murano island became the epicenter of glass production, with master glassmakers like Angelo Barovier creating masterpieces such as the Barovier Cup in 1460, establishing Murano as Europe's leading glass-making center until the 18th century.	"[""The Art Of Glass Making\nWhen we think about art with a capital A, we think of ancient Greek sculptures or the world’s most famous baroque paintings, but works of glass don’t immediately come to mind. However, glass art has been around for much longer than oil painting has, with the added bonus that it’s not just decorative but it can also be a functional medium!\nDifferent Types of Glass Art\nGenerally speaking, glass art is the umbrella term used for all individual works of art that are substantially or completely made of glass. There are many types of glass art and the differences between them mainly relate to the techniques that are used in the manufacturing process. These different techniques can be divided into three categories: hot glass, warm glass and cold glass. “Hot” glass works are born through a process of melting, which requires a temperature of around 2000 degrees. On the other hand, “warm” glass works can be produced by simply heating glass in an oven or kiln to manipulate its form. Glass may also be “cold worked”, changing the surface of the glass by using techniques that involve tools or chemicals.\nRoman Gladiator Cup blown in a two-part mold, ca. A.D. 50-80 (The Met)\nOne of the oldest known and most traditional techniques is glass blowing, an example of a “hot” method. Using a blow tube (or pipe), the glassblower blows air into the liquid glass, inflating it to produce a specific form and then allowing it to gradually cool down and harden. The earliest evidence of glass blowing was found in the Eastern Mediterranean and on the edges of Western Asia, formerly known as Phoenicia, stemming from the 1st century BC. It was quickly adopted by the Romans and then spread across Europe and Northern Africa. The most common form of hot working is still glass casting, which is done by pouring molten glass into a mold. Glass artists that use this technique often also work with pâte de verre, a paste that is made up of grit granules and held together by a gum. It can be applied to the inner surface of a mold to create a rich effect on the skin of the glass.\nVase Pâte de Verre\nSlumping can be seen as a typical example of the “warm” glass technique and occurs at a maximum of 1400 degrees. At this temperature, flat plates or sheets of glass will start to bend. They may be placed over a mold or left to flow freely, allowing gravity to do its work. Separate pieces of glass can also be fused together at a slightly higher temperature of 1600 degrees. And if you have ever heard of frosted glass then it wasn’t intended as a pun, it is actually a very common “cold” technique. It refers to the surface of the glass being etched away (hence why it is also referred to as etched glass), which can be done using either acid or by sandblasting the glass. Cold techniques are often combined with hot and warm techniques to add texture to the skin of casted or molded pieces. Similarly, hot and warm techniques are sometimes combined by first blowing the glass and then placing it in the oven to shape it into the desired form.\nHistorical Examples of Glass Art\nThroughout history, there have been many fine examples of glass art that were both decorative and functional. First and foremost, the Roman mastering and perfecting of the glass blowing technique coincided with the rise of the Roman empire, resulting in an enormous production of glassware. Most of the elaborately shaped jugs and jars that remain today were actually used by merchants to transport and market their goods, whereas drinking vessels such as gladiator cups were made to be sold as collectibles or souvenirs.\nThe world famous Gothic stained glass windows at Sainte-Chapelle in Paris\nStained glass windows are by far the most emblematic example of glass art in the middle ages and still remain largely intact, another testimony to great craftsmanship. Less obvious is the fact that medieval church glass was almost always hand-blown, resulting in either cylinder glass or crown glass. In both cases, a bubble of air had to be blown into a gather of molten glass, which was then either manipulated into a cylinder to be cut in half and flattened, or spun around to allow the centrifugal force to cause the bubble to open up and flatten. The varying colours were created through adding different metallic salts during the manufacturing process.\nVenetian Murano Glass Chandelier\nIn 1291 the Venetian island of Murano became the epicenter of Venetian glass production and would continue to hold a monopoly over high-quality glass making for centuries to come, something that was largely due to its innovativeness. The skillful Murano glass makers developed and refined several different techniques and crafted anything from luxury tableware to jewellery. Around 1700 Murano introduced the first glass chandeliers, heavily decorated with garlands, flowers and leaves, calling them “bouquets”. These characteristic chandeliers instantly became a huge success and are still being produced today.\nArt Nouveau and Art Deco Glass Art\nDuring the Art Nouveau and Art Deco era’s (what's the difference?), glass making once more reached new heights thanks to manufacturers such as Daum and Lalique. The Daum crystal studio in Nancy was founded in 1878 by Jean Daum and became the market leader in Art Nouveau decorative glass around the turn of the century, under the leadership of his sons Auguste and Antonin. They were known for combining several techniques to create elaborate vases and for reviving the pâte de verre technique. In 1905, the Art Nouveau jeweller René Lalique opened a shop in Paris where he also started selling his handmade perfume bottles. The craft clearly suited him, as Lalique decided to devote himself entirely to glass making by establishing a factory in Alsace in 1921. There, he developed a technique of contrast between clear and frosted glass, whilst also applying more geometric designs, eventually leading to Lalique being crowned as the master of Art Deco glassworks at the 1925 International Exposition of Modern Industrial and Decorative Arts in Paris.\nDaum Frères Art Nouveau vase and an Art Deco vase.\nContemporary Glass Art\nNowadays, the rules of glass art have become much less strict and there are several artists that are reinventing the craft in their own, authentic ways. For example, the Yugoslavian artist Jelena Popadic (1964) creates glass objects that are decorated with symmetrical patterns which refer to cell structures, using different techniques. This glass-blown Amber Granate was further shaped through applying a deep sandblasting technique to create an amazing contrast between the more matted deep reliefs and the polished and transparent protruding elements.\nJelena Popadic, Amber Granate, 2011\nOn the contrary, Susan Hammond’s (UK, 1960) glass objects are more tranquil and free flowing, yet there is something monumental about them. Inspired by nature, the emptiness in and around these glass-blown structures also interacts with the piece, leaving its muted colour tones to serve only as an accent. Lastly, Bernard Heesen’s (NL, 1958) innovational aspirations fit well into the Dutch Leerdam glassmaking tradition. Fortuity plays a big part in the expression and the form of his glass-blown products, vases and arabesques that can often be found at the crossroads of craftsmanship and industry and art and kitsch, elements of irony that Heesen likes to play with in his work.\n(Left) Susan Hammond, NT, 2017 (Right) Bernard Heesen, Aiguière, 2017\nAt Gallerease you can find an extensive online collection of antique, modern and contemporary glass art. Visit the pages of Peter Korf de Gidts, Kunstzalen A. Vecht, Lennart Booij Fine Art and Rare Items, Galerie Frans Leidelmeijer, Antes Art 1900, Passage Arts, Het Ware Huis, Galerie NUMMER40, Galerie Carla Koch and Galerie van Strien to discover more of these gems!"", 'The glass making has long-standing roots which can be traced back over three thousand years ago. It’s quite difficult to determine with certainty which people can boast the discovery of glass making, which probably was the consequence of an accidental invention.\nAccording to an ancient Phoenician legend, handed down by Pliny the Elder (AD 23 –AD 79), some merchants, returning from Egypt with a large shipment of sodium carbonate, stopped one evening along the banks of a river to rest. They used few blocks of saltpeter to light a fire and cook some food. The fire kept on burning all night long, and in the morning they found with amazement a new bright and transparent material: glass!\nThe oldest archaeological finds of glass pastes date back to around the fourth millennium BC and were found in the area stretching from the Mesopotamian basin to Egypt: mostly rings, beads, seals, inlays and plates, and this goes to show that the most ancient techniques only allow the production of objects of small dimensions, intended to ritual uses or created for ornamental purposes.\nApparently, in the early days of the process, the rarity due to the difficulty of production, was to consider the glass as a precious material like semiprecious gems and stones, with which were realized the jewelry.\nFrom the Hellenistic period there was an increase in long-haul trades, and industries began to produce large-scale, and commonly used luxury items. This is when the glass manufacturing experienced a period of prosperity, spreading throughout the Mediterranean.\nAt the time of emperor Augustus the manufacture of glass, along with the other arts, experienced profound changes. Glass artifacts of that period have been found throughout Europe (France, Switzerland, Austria, Germany and Spain), and in those days the glass industry experienced a productivity growth.\nImportant production centers were created everywhere, thanks to itinerant craftsmen ready to bring their skills wherever there was demand for glass.\nThe art of glass spread throughout Europe especially during the Roman Empire, and certainly, the phenomenon that favored this expansion is to be found in the orientation to daily use of glass production.\nThe center of glassmaking in Italy was the island of Murano. Murano’s reputation as a center for glassmaking began when the Venetian government, in order to prevent fires in the city centre, ordered to move all glass furnaces to the island of Murano in 1291. From then on all the necessary measures to prevent possible industrial espionage were taken.\nMurano glass production became such a thriving industry that, until the end of the 18th C, the island was Europe’s leading centre for glass making.\nThe art of glass blowing in Murano achieved soon great results, due to the technological innovations introduced by Angelo Barovier, the greatest glassmaker of the 15th C, who in 1460 created the most precious masterpiece of glass art: the famous “Barovier Cup”, made of enameled blown glass, now preserved in the Glass Museum in Murano.\nA visit to Murano glass making factories today offers the opportunity to see how glass is still made, with the same techniques used in the past. You can also enrich the experience with the visit of the Glass Museum to admire the precious old glass works of art which are still today inspiring the glass masters of Murano.']"	['<urn:uuid:64c01b2b-03b6-437a-860d-c04333da3fc0>', '<urn:uuid:e294203e-c509-457a-a9a0-c93750adacb6>']	factoid	with-premise	short-search-query	similar-to-document	three-doc	expert	2025-05-13T03:34:55.284799	7	54	1860
77	What's the difference between caring for an indoor avocado tree and propagating outdoor tropical plants in terms of water requirements?	Indoor avocado trees need light but frequent watering to keep the soil moist but not soaking wet, with yellow leaves indicating overwatering. For propagating outdoor tropical plants, the watering requirements vary by method - a simple propagating unit using a 1-gallon can needs watering only every two weeks due to its water reservoir system. For larger outdoor propagation setups, a mist head attached to a water hose is used, with cooler areas like mauka Kona and Hilo requiring only intermittent spraying rather than constant misting.	['Some folks new to Hawaii are overwhelmed by the great variety of tropical plants available here. Fortunately, in Hawaii, everyone can have a green thumb if they have the desire.\nPlanning a beautiful garden to surround your home is loads of fun. Tropical gardening books and magazines help give you ideas. The Kona Outdoor Circle has a great horticultural library and, of course, you can get ideas from neighbors with attractive gardens. Once you have decided on the perfect plants to fit your needs, check out our local nurseries and garden centers. If the plants you like are not readily available, you might consider propagating your own. Some plants are not available at local nurseries, but you may see a fantastic specimen of what you want in your neighborhood. Your neighbor will likely be willing to give you some cuttings. Many plants are easily grown in this manner.\nAllowing nature to grow roots on tips of plants requires the simplest of facilities. This ranges from a plastic bag containing a handful of damp sphagnum moss to a cast-off wood box containing a 2-inch layer of gravel topped off with a 4-inch layer of perlite or a mixture of peat and perlite.\nTo gain confidence in the rooting technique, try using a 1-gallon can. It will make a dependable propagating unit. Remove the top and wash out any residue. Punch a ring of six or eight holes in the side halfway between the top and bottom. Fill the can with gravel or coarse cinder to within 1 inch of the holes. Cover the gravel with a 1-inch layer of sphagnum moss and then fill the remainder of the can with coarse sand.\nYou now have a propagating unit with its own water reservoir. Water below the side holes in the can moves up from the bottom of the can by capillary action, keeping the sand and the rooting cuttings moist. Under normal conditions, this simple unit will need watering about every two weeks.\nA 1-gallon can will hold a score of cuttings. To ensure success with the first batch of cuttings, try your luck with coleus or crotons. These plants are a neophyte’s friend. They will root even with over-coddling. Edible hibiscus and the tapioca plant, or cassava, are also easily started in this way. You might try some of the more difficult plants like camellias or vireya rhododendrons.\nIf you want to get into the mass production of cuttings, then build a wooden box about 36-by-36-by-12 inches deep. Next, nail four legs to the box so that it will become a waist-high, no-stoop unit 3 feet square and 1 foot deep. Place the unit in semi-shade and fill with a few inches of cinder topped with about 4 inches of damp perlite or peat. Next, buy a mist head that can be attached to a water hose. Set the constant day misting device in the center of the propagating box and you are ready for rooting cuttings. In cooler parts of the island like mauka Kona and Hilo, an intermittent spray will suffice.\nThe best time to make cuttings is in early morning. This is the time when the largest amount of water is in the plant. Keep the moisture in the cuttings by placing them on a moist paper or cloth when removed from the plants. Do not stick the ends of the cuttings directly into water when removed from the plant. Also don’t place cuttings in any propagating medium until 15 minutes after they have been made. This will tend to discourage disease problems.\nMake cuttings from 4 to 6 inches long, preferably with the cut made just below the node, and leave as many leaves as possible on the cuttings. Stick them in the propagating medium 1.5 to 2 inches deep and space so the leaves overlap about one third. Firm the medium and water.\nTransplant into pots after 1-inch-long roots have developed. Be sure to get them out of the propagation bed before they are hard-looking and yellow. Keep them in pots or cans until the roots are well-developed. This may be just a few months for crotons or more than a year for some of the slower plants.\nWhen it comes to propagating tropical fruit trees, many are propagated by seed, but the quality may not be equal to the parent plant. Mango, avocado, citrus and macadamia nut trees require grafting for best results. Another propagation method used is air layering. It is most often used in the propagation of rainbow shower trees. These are in full bloom much of the year except where they are improperly pruned. Hat racking describes this type of butchering — the tree looks more like a hat rack than a tree.\nTrees that require grafting or air layering are best supplied by a local nursery or garden center. If you want to try your hand at grafting or air layering, check out “The New Sunset Western Garden Book.” There are also gardening classes frequently taught through the University of Hawaii College of Tropical Agriculture and Human Resources. Contact the nearest UH Extension office. To become a gardening expert, ask about the UH master gardening classes.\nOnce you get your green thumb groove on, you are considered a real kamaaina.', 'Helen CamacaroGetty Images\nJust about any dish can profit from avocados. Regardless of whether you would like to take omelets, salads, sandwiches or smoothie recipes to the upcoming amount, nutrient-prosperous avocados are a excellent selection. This coronary heart-healthful fruit (yep, believe it or not, it really is a fruit) is outside of versatile, creating it the great food stuff staple.\nAnd the ideal component is that it would not call for a ton of hard work to increase an avocado tree indoors. All you have to do is hold on to a leftover pit and seize a few essential provides to enjoy the gains of this delicious fruit. As well as, this very simple (and reasonably priced) gardening project is enjoyable for youngsters to attempt.\nFull disclosure: There is a caveat. Count on avocado trees to get in between 5 to 13 years to make fruit. It is also tough for this to occur indoors, which points out why the fruit can be so dear at grocery retailers occasionally. If you are prepared to give it a check out, adhere to these 5 fundamental ways to increase your personal avocado tree from a pit:\nMaterials You’ll Need to have\nRising an Avocado Tree\n1. Conserve an avocado pit (with no slicing or breaking it) and wash off any residue. Let dry, then insert 3-4 toothpicks about halfway up the facet of the pit.\n2. Suspend the pit wide conclude down in a drinking glass or jar. Fill the container with more than enough h2o to submerge the base third of the seed, the Missouri Botanical Back garden advises.\n3. Spot the glass in a heat location out of direct daylight and alter the h2o regularly. Roots and a sprout need to look in about 2-6 weeks. If not, begin with one more seed.\n4. When the sprout gets about 6 inches tall, lower it again to about 3 inches to really encourage much more root progress.\n5. As soon as the stem grows out all over again, plant the pit in an 10-inch pot crammed with loaded potting soil. Now it truly is time to enable your avocado tree increase, mature, improve!\nObserve: You can acquire older trees instead of commencing from scratch. Amazon sells grafted, 4-ft tall avocado trees that may possibly generate fruit in 3-4 yrs alternatively of 10.\nCaring for an Avocado Tree\nSpot the pot in a sunny spot and drinking water lightly but usually. The goal is to continue to keep the soil moist but not sopping damp, California Avocados recommends. You can place the tree outdoor in the summer as lengthy as temps stay above 45°F. Sometimes prune your plant (each and every 6 inches or so) to stimulate fullness.\nTip: Yellow leaves signal you happen to be overwatering. Dial back again to avoid root rot.\nYou can also plant avocado trees outdoors in USDA Zones 10-12, a.k.a., areas with no frost. They do greatest in abundant, nicely-drained soil with full sun. H2o 2-3 periods per 7 days by soaking the soil completely and then allowing it dry out ahead of watering once more.\nAvocado Recipe Concepts\nAlthough your tree won’t generate fruit anytime before long, round out your environmentally friendly-thumb project with some avo-themed dishes from the Superior Housekeeping Examination Kitchen. Initial up: Our go-to common guacamole recipe, with just the ideal amount of money of lime and jalapeño. You won’t be unhappy!\nLook at out these other favorites that make avocado the star of the display:\nThis written content is designed and maintained by a 3rd bash, and imported on to this page to assist buyers offer their e mail addresses. You may well be capable to uncover additional data about this and very similar articles at piano.io']	['<urn:uuid:3fa2932b-dc8c-4635-8919-faff04ea50ee>', '<urn:uuid:e4c756a0-f89b-4178-8699-ac8e61573694>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-13T03:34:55.284799	20	85	1506
78	What's the highest temperature at which Nickel 201 can safely be used when working with dry chlorine and hydrogen chloride gases?	Nickel 201 alloy may be used in temperatures up to 550C in dry chlorine and hydrogen chloride.	"[""Nickel 201 - UNS N02201\nAlloy 201, UNS N02201, is commonly associated with ASTM B160 - ASTM B161 - ASTM B162 - ASTM B366 - ASTM B564 - ASTM B622.\nAlloy 201 is commonly known as Nickel 201 or Ni201. Alloy 201 is a commercially pure Nickel alloy with lower Carbon content than alloy 200.\nMechanical properties here.\nRound Bar, Plate, Tubing, Pipe, Fastener\nThe Nickel 201 alloys is an alloy that contains commercially pure wrought Nickel and has good mechanical properties that extend over a wide range of temperatures. The nickel alloy 201, also known as ni201, has excellent resistance to many corrosives, hydroxides in particular.\nNickel 201 has native properties to provide strong resistance to corrosion in acids and alkalis. It is especially useful under reducing conditions. The alloy has outstanding resistance to caustic alkalis, which includes its molten state. The alloy shows good resistance in acid, alkaline, and neutral salt solutions. However, when Nickel 201 is in oxidizing salt solutions severe corrosion will occur. The Nickel 201 alloy is resistant to all dry gases at room temperature. In dry chlorine and hydrogen chloride, Nickel 201 alloy may be used in temperatures up to 550C. Its resistance to mineral acids varies according to temperature, concentration, and solution aeration. Its corrosion resistance displays better in de-aerated acid.\n- Nickel 201 displays good corrosion resistance in acids and alkalis and is most useful under reducing conditions.\n- Outstanding resistance to caustic alkalis, which includes its molten state. The alloy shows good resistance in acid, alkaline, and neutral salt solutions.\n- Caution: in oxidizing salt solutions, severe corrosion will occur with this alloys\n- This alloy is virtually immune to inter granular attack above 315C. However, chlorates must be kept to a minimum.\nWe can cold work high strength into your material to meet your high-performing requirements. We also facilitate size conversions, hot and cold rolling, and heat treating materials, as well as our machining capabilities.\nFor more information, contact us (or call 1-800-945-8230) and request our GFM Bulletin; you can view our brochure online! There's also more information about our offered services on our production capabilities page.\nWe have expanded our abilities to work smaller diameter bar down to nominal wire. Also, check out our weld wire to finish the job right!\n- Manufacture and handling of sodium hydroxide, particularly at temperature above 300C.\n- Production of viscose rayon. Manufacturing of soap.\n- Analine hydrochloride production and in the chlorination of aliphatic hydrocarbons such as benzene, methane and ethane.\n- Manufacturing of vinyl chloride monomer.\n- Reactors and vessels in which fluorine is generated and reacted with hydrocarbons\n|Density||8.9 g/cm cube|\n|Specific Heat||440 J/kg K|\n|Electrical Resistivity||8.5 micro ohms cm|\n|Curie Temperature||358 C|\n|Melting Range||1435-1445 C|\n|Thermal Expansion||(106K) 14.3 (20-300 C)|\nThe typical properties listed can usually be provided in rounds, sheet, strip, plate, & custom forgings. We have the equipment to produce small quantities in special sizes to meet our customers’ specific needs.\n|MP Table 1|\n|Type||Ultimate Tensile (ksi)||Yield Strength (ksi)||Elong. % in 2 in.|\n|MP Table 2|\n|Yield Strength||Rp 0.2%|\n|20 C Temp||80 N/mm sq|\n|100 C||70 N/mm sq|\n|200 C||65 N/mm sq|\n|300 C||60 N/mm sq|\n|400 C||55 N/mm sq|\nPlease, note that the specs listed are for reference and are not comprehensive nor indicative of the actual specifications listed on the Material Test Report (MTR). If you have a special spec requirement, then please reach out to our sales department at 1-800-472-5569.\n|Metal Type||UNS N02201|\n|Pipe||ASTM B622, ASTM B161|\n|Tube||ASTM B622, ASTM B161, ASTM B163|\nNickel and cobalt based alloys can be difficult to machinine. However, it should be emphasized that these alloys can be machined using conventional production methods at satisfactory rates. These alloys harden rapidly, generate high heat during cutting, weld to the cutting tool surface and offer high resistance to metal removal because of their high shear strengths. The following are key points which should be considered during machining operations:\n- CAPACITY - Machine should be rigid and overpowered as much as possible.\n- RIGIDITY - Work piece and tool should be held rigid. Minimize tool overhang.\n- TOOL SHARPNESS - Make sure tools are sharp at all times. Change to sharpened tools at regular intervals rather than out of necessity. A 0.015 inch wear land is considered a dull tool.\n- TOOLS - Use positive rake angle tools for most machining operations. Negative rake angle tools can be considered for intermittent cuts and heavy stock removal. Carbide-tipped tools are suggested for most applications. High speed tools can be used, with lower production rates, and are often recommended for intermittent cuts.\n- POSITIVE CUTS - Use heavy, constant, feeds to maintain positive cutting action. If feed slows and the tool dwells in the cut, work hardening occurs, tool life deteriorates and close tolerances are impossible.\n- LUBRICATION - lubricants are desirable. Soluble oils are recommended especially when using carbide tooling.\nData referring to mechanical properties and chemical analyses are the result of tests performed on specimens obtained from specific locations of the products in accordance with prescribed sampling procedures; any warranty thereof is limited to the values obtained at such locations and by such procedures. There is no warranty with respect to values of the materials at other locations.\nReferencesUlbrich's information on alloy 201""]"	['<urn:uuid:936cc09d-bc0f-4fde-af13-e29810f4787d>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T03:34:55.284799	21	17	881
79	How do air systems protect workers and save money?	Engineering controls like proper ventilation systems protect all workers by reducing or eliminating hazards (rather than just protecting individuals wearing PPE), while natural ventilation strategies can reduce energy use and lower bills through smaller mechanical systems. According to Carnegie Mellon research, natural and mixed-mode ventilation systems can achieve 47-79% HVAC energy savings and provide 0.8-1.3% savings on health costs.	"['|Understand your MSDS with the MS-Demystifier||Search ALL our MSDS info|\nWhen substitution of hazardous chemicals or processes is simply not possible, additional measures to control employee exposure need to be taken.\nWhile personal protective equipment (PPE) such as respirators can help protect an individual from a hazardous material, engineering controls protect all workers by reducing or eliminating the hazard. For example, someone who is spray-painting can wear a respirator to avoid inhaling toxic fumes, but nearby workers without any respiratory protection will be exposed. Through the use of proper local exhaust ventilation everyone can be protected.\nGet your corrosion-resistant polyethylene acid storage cabinets from Safety Emporium.\nNote: In certain circumstances, administrative controls can be successful in controlling employee exposure to contaminants; e.g., maintenance operations involving toxic substances can sometimes be performed at night in the absence of the usual production staff (but not to comply with the PEL of OSHA-regulated carcinogens).\nThis does not mean that engineering and other types of controls (including PPE) are mutually exclusive. Employers may need to use multiple types of controls to prevent employee overexposure. Also note that the degree to which engineering controls are required depends on the particular standard. For example, 29 CFR 1910.95(b)(1) Occupational Noise Exposure, allows employers to rely on personal protective equipment (PPE) and a hearing conservation program rather than engineering and/or administrative controls if hearing protectors will effectively reduce the noise hazard to acceptable levels.\nGet your PPE training handbooks, posters and videos at Safety Emporium.\nOSHA considers economic feasibility to be a major issue with engineering controls. Requirements that would threaten the economic viability of an entire industry cannot be considered economically feasible under the OSH Act. OSHA may permit PPE in lieu of engineering controls (at least until engineering controls become a less significant burden for the company) when all of the following apply:|\nIn those limited situations where there are no feasible engineering or administrative controls, full abatement can be allowed by PPE.\nThe preferred methods for reducing chemical exposure, in order of general effectiveness, are illustrated in this diagram:\nSpecifically, these steps are:\nEngineering controls may receive specific mention on SDS\'s. For example, when a material has a low PEL, there may be a section explicitly labeled ""Ventilation requirements"". Specific machinery may be recommended - or perhaps specifically discouraged. For example, ductless recirculating laboratory hoods might not provide sufficient protection from certain highly toxic materials; in these cases a properly vented fume hood (perhaps outfitted with a scrubber) would be required.\nTwo of the most common engineering controls found on an MSDS are local exhaust and general ventilation. There are dozens of types of engineering controls.\nSee also: administrative controls, fume hoods, respirators, personal protective equipment (PPE), ventilation.\nAdditional definitions from Google and OneLook.\nEntry last updated: Wednesday, June 13, 2018. This page is copyright 2000-2019 by ILPI. Unauthorized duplication or posting on other web sites is expressly prohibited. Send suggestions, comments, and new entry desires (include the URL if applicable) to us by email.\nDisclaimer: The information contained herein is believed to be true and accurate, however ILPI makes no guarantees concerning the veracity of any statement. Use of any information on this page is at the reader\'s own risk. ILPI strongly encourages the reader to consult the appropriate local, state and federal agencies concerning the matters discussed herein.', 'Natural VentilationExisting Commercial\nWhat is Natural Ventilation?\nNatural ventilation uses passive strategies to supply outdoor air to a building’s interior for ventilation and cooling. Natural ventilation systems rely on natural forces, such as wind and temperature differences between a building and its environment, to drive the flow of fresh air through a building. Cross ventilation uses air-pressure differentials caused by wind, and stack ventilation uses the increased buoyancy of air as it warms up. Both work on the principle of air moving from a high-pressure to a low-pressure zone.\nMany modern commercial buildings utilize tightly sealed building envelopes and mechanical HVAC systems to achieve energy efficiency (see Air Infiltration). Natural ventilation strategies can introduce fresh air to airtight buildings, replacing all or part of a mechanical system, and reduce energy and operating costs and improve indoor air quality. Mixed-mode systems use a combination of natural ventilation from operable windows (either manually or sensor controlled) and mechanical systems that include air distribution equipment and refrigeration equipment for cooling (see Demand Control Ventilation).\nHow to Implement Natural Ventilation\nImplementing a successful natural ventilation design or mixed-mode system requires an understanding of the local climate, intended building use, building geometry, and occupant behavior (see Building Orientation, Tree Protection and Placement, and Building Evaluation). For example, in coastal locations, desirable cool sea breezes can reduce cooling loads. It is important to be aware that outdoor air in these areas may also have high moisture content that needs to be managed (see Moisture Control). Other locations may not be ideal for natural ventilation due to local sources of air pollution, allergens, or outdoor noises. The design of internal spaces and the size and placement of openings in the building impacts ventilation as well. Operable windows, fans, and corridors help direct airflow using natural ventilation principles (see Individual Comfort Controls and Views and Operable Windows).\nFor naturally ventilated spaces, the Minimum Indoor Air Quality Performance requirement for LEEDv4 Operations + Maintenance Existing Buildings refers to the natural ventilation procedure from ASHRAE Standard 62.1–2010 (or a local equivalent, whichever is more stringent).\nCoolVent is a natural ventilation simulation tool developed by the Massachusetts Institute of Technology (MIT) to evaluate the effects of different natural ventilation strategies on a building’s energy use and occupant comfort. The CoolVent website also provides resources and case studies about natural ventilation (see Energy Modeling).\nThe Passive House Institute US (PHIUS+ 2018) building certification sets standards for airtight construction, and natural and controlled mechanical ventilation for all types of high-performance, energy-efficient buildings. The International Passive House Association (iPHA) provides a program and resources that address refurbishment with passive house components. The Passive House Institute developed the iPHA to promote the Passive House Standard wordwide.\nHigh, operable, clerestory windows, controlled by the building’s energy management system, cool the Zion Canyon Visitor Center by allowing hot air to escape while low windows introduce cool air at ground level (see Figure 1).\nNatural ventilation strategies can reduce energy use, and lower energy bills and installation and maintenance costs by allowing for smaller mechanical systems and extending the life of HVAC equipment. When part of a mixed-mode system, natural ventilation can increase occupant satisfaction and indoor air quality by offering occupants greater personal control over thermal comfort, ventilation levels, and connection to the outdoors (see Biophilia).\nA study by Carnegie Mellon found that natural ventilation and mixed-mode conditioning can achieve 0.8-1.3% savings on health costs, 3-18% productivity gains and 47-79% in HVAC energy savings, for an average ROI of at least 120%.\nIn the event of power outages and the loss of air conditioning, natural ventilation can maintain thermal comfort for building occupants, particularly at night, when colder outside air can mix with warmer air inside the building (see Building Flush). Mixed-mode systems add redundancy and flexibility to the building’s ventilation and HVAC systems, providing back-up during power-outages, mechanical failures, security threats, or the release of air-borne contaminants.\n Whole Building Design Guide (WBDG). Natural Ventilation. https://www.wbdg.org/resources/natural-ventilation (accessed March 14, 2018).\n CoolVent. 2018. Basics of Natural Ventilation. Massachusetts Institute of Technology. http://coolvent.mit.edu/intro-to-natural-ventilation/basics-of-natural-ventilation/ (accessed February 16,2019).\n Center for the Built Environment (CBE) Berkeley University. What is Mixed Mode? https://www.cbe.berkeley.edu/mixedmode/aboutmm.html (accessed March 14, 2018).\n WBDG. Natural Ventilation. http://www.wbdg.org/resources/naturalventilation.php (accessed March 12, 2018).\n USGBC. Minimum Indoor Air Quality Performance. https://www.usgbc.org/credits/existing-buildings/v4/eq103 (accessed February 16, 2019).\n Passipedia. Refurbishment with Passive House Components. https://passipedia.org/planning/refurbishment_with_passive_house_components (accessed February 16, 2019).\n Carnegie Mellon (2004), Guidelines for High-Performance Buildings. Ventilation and Productivity.\n- American Society of Heating, Refrigerating and Air-Conditioning Engineers\n- CBE Berkeley Mixed-Mode Ventilation\n- CoolVent – The Natural Ventilation Simulation Tool by MIT\n- Passive House Institute US (PHIUS +2018)\n- Passipedia – Refurbishment with Passive House Components\n- Torcellini et al. S. Lessons Learned from Case Studies of Six High-Performance Buildings. National Renewable Energy Laboratory. Natural Ventilation P 52-63.\n- Whole Building Design Guide – Natural Ventilation']"	['<urn:uuid:c984fdb0-3bc2-44a2-a3e4-ac4672cb8203>', '<urn:uuid:c1434576-8416-4c8b-9091-19912ec64623>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T03:34:55.284799	9	59	1358
80	As someone interested in both business and environmental issues, I'd like to know how companies measure their market power and how this connects to environmental benefits in urban areas?	Companies measure their market power using the Lerner index, which compares a firm's price to its marginal cost of production. The formula is L = (P-MC)/P, where P is price and MC is marginal cost. A higher Lerner index indicates greater market power. When it comes to environmental benefits in urban areas, the implementation of green infrastructure provides multiple measurable advantages. Research has shown evidence of positive impacts on indoor and outdoor microclimate control, energy consumption reduction, noise reduction, and air quality improvement through fine-dust filtration. These environmental benefits are particularly important for neighborhood improvement and contribute to both physical and psychological well-being in urban areas.	"[""1. Suppose the demand for output is given by Q = 120 – P and the marginal (and average) cost of production is constant, at 30. Assuming two firms and Cournot competition, compute the Nash equilibrium outputs, prices, and profits. Graph the reaction curves. Calculate the Lerner index for the typical firm in this market.\n2. Can you provide definitions of the following concepts in economics for me?\n- Values Q and P\n- Cost of production, especially average cost of production and marginal cost of production\n- Cournot competition model\n- Nash equilibrium\n- Reaction functions curves\n- Lerner index\nIn the Cournot competition model developed in France in 1838 by mathematician Augustin Cournot to reflect a duopoly, Q and P represent quantity of output (Q) and product price (P). On a graph, P is the vertical (up-and-down) axis (line) while Q is the horizontal (side-to-side) axis.\nCosts of production are classed as total cost, average cost and marginal cost. Total cost is the sum of all variable costs, such as the variable cost of raw materials, and all fixed costs, such as the fixed cost of operating costs.\nAverage cost is the average cost per each unit produced: Average cost is total cost divided by the total number of units produced during a fiscal period. Consequently, average cost can be stated as the cost to produce one unit of product.\nMarginal cost builds on the average cost (cost of one unit) to calculate the added cost of one unit more of the product. In other words: If production is X and average cost Y, then X+1 will equal Y+?. The formula for calculating the cost that results from a one unit change in production output is this:\nMarginal Cost (MC) = Change in Total Cost = ΔTC\nChange in Output Δq\nCournot competition assume a duopoly restrict, per definition, to two firms. [Research has shown the model can be expanded to multiple firms.] Cournot assumes that quantity is the variable and price is the constant.\nIn other words, companies fix their production on a predetermined price range so that price is stable and so that quantity of output is variable. [Research has shown that, in fact, it is the other way round: quantity is stable while price that is the variable (changeable) factor.]\nThe Nash equilibrium reflects conditions when prices are stable, as in the Cournot model, and is thus used instead of the price taking competitive equilibrium model that reflects variable prices. Nash equilibrium is appropriate to the Cournot duopoly model because it assumes stable prices with variable quantity output.\nWith stable prices for two companies--both of which operate under the same assumptions of stable price and homogenous (identical) products--the Nash equilibrium reflects the industry price set by the demand curve, thus reflects the revenues of each company in the duopoly. The Nash equilibrium show the intersection where both firms are choosing optimal production output in consideration of the output of the other firm.\nIn other words, with prices stable, the only variable is production output. The Nash equilibrium shows the optimal output for each company base upon, or given, the output of the single competitor in the duopoly.\nReaction function curves: With R being the reactions of firms 1 and 2 (R1 and R2), this is shown in graphed reaction functions curves where q1=R1(q2) gives one firm's optimal output in reaction to a given output (known output) for the competitor q2 firm. The vertical axis is R2(q1) while the horizontal axis is R1(q2). Equilibrium is where q1 and q2 intersect on the R2-R1 graph (see image).\nThe Lerner index uses exact price information, cost structure of a firm, or price elasticity of demand to measure market power. The level of market power for each firm is measured on the Lerner index by measuring price (P) to marginal cost (MC). When using price elasticity of demand, the Lerner index is the inverse of elasticity to maximized price. The formula is:\nL = P-MC = 1\n[Image from Wikipedia Commons and provided by Twisp, Bluemoose (Own work) (Public domain).]\na) The demand equation is:\nAverage cost=Marginal cost=30\nTo determine the Cournot-Nash equilibrium, the reaction function for each firm has to be calculated first.\nProfit for Firm 1=total revenue-total cost\nDifferentiating this profit function with respect to Q1, and setting the derivative equal to zero gives the reaction function for the first firm,\nBy symmetry, reaction function for the second firm,\nSubstituting the value of Q2,\nAnd also, `Q_2=$30`\nTo determine the price at profit maximization, Q1 and Q2 has to be substituted into the demand equation:\nPutting the values for price and quantity into the profit function,\n`P_2 ` is also equal to $900.\nb) The price is $60, marginal cost is $30\nSo, Lerner index `=(P-MC)/P=(60-30)/60=0.5`"", 'Topical Collection ""Urban Green Infrastructure for Climate-Proof and Healthy Cities""\nInterests: urban green infrastructure; nature-based solutions; integral rainwater management; soil and water bioengineering; vegetation technologies; landscape construction and design; natural hazard and risk mitigation; resilience research\nInterests: urban green infrastructure; integral rainwater management; vegetation technologies; technical substrates; plant use\nInterests: telehealth; environmental health; open innovation in science; health communication; preventive medicine; public health\nSpecial Issues, Collections and Topics in MDPI journals\nSpecial Issue in International Journal of Environmental Research and Public Health: The Future of Healthcare: Telemedicine, Public eHealth, and Big Data\nSpecial Issue in Hygiene: COVID-19: Health and Hygiene\nIn the light of climate change adaptation, urban green (and blue) infrastructures are becoming increasingly recognized, referring to urban and settlement planning and the design of climate-adapted housings and buildings. Green and plant-based solutions (such as green roofing, green façading, street and open space greenery etc.) have become an issue in combatting urban heat and associated health stresses.\nIn 2013, the European Commission launched the Green Infrastructure Strategy, defining Green Infrastructure (GI) as a policy objective in order to preserve natural capital. To create and improve the knowledge base was one of the top concerns to enhance strategic developments for implementation. By using nature-based solutions, urban green infrastructure is intended to preserve and advance biodiversity and resilience in cities and peri-urban areas.\nDuring the past seven years, activities, policies and research have multiplied, and an increasing number of studies have proven evidence on the positive impacts of urban green referring to indoor and outdoor microclimate and temperature control, energy consumption, noise reduction, fine-dust filtration and air quality improvement. Moreover, benefits from urban green on social, physical and psychological health and well-being are apparently associated and increasingly important for neighborhood improvement and district upgrades.\nHowever, a deeper understanding of the complex interactions between urban green and human health and wellbeing is needed to develop objectives and interventions for green implementation and retrofit. The engagement of different scientific disciplines is key for improving current approaches and creating small-scale and larger-scale sustainable GI solutions.\nThis Special Issue focuses on urban green infrastructure for sustainable climate change adaptation and health and sociocultural improvement. It aims at advancing and sharing the current insights in the impact of urban green infrastructure on microclimate, energy demand, health and sociocultural structures and activities.\nThe purpose of this issue is to provide up-to-date knowledge in technologies, planning and implementation of urban green infrastructure in order to advance urban sustainability and secure healthy and climate-proof urban environments. It further addresses novel insights in related public health issues. There are still many challenges regarding practical, ethical, and legal concerns, and evidence-based approaches covering technical, natural-sciences, planning, governance and health aspects are scarce.\nWe invite authors to submit articles to this Special Issue on urban green infrastructure related to the broader spectrum of technical and natural, social and medical science and the evaluation of these aspects in urban and peri-urban settings. We welcome theoretical and empirical contributions as well as review articles, and the submission of research work by interdisciplinary teams and international groups is of significant interest.\nThe issue will therefore refer to and supplement the Special Issue Green Infrastructures and Climate Change and other previously published findings on the microclimatic and energy-related benefits that urban green infrastructure provides. Additionally, it will provide most current considerations on technology advancement and health and socio-cultural benefits for advancing planning and implementation strategies.\nProf. Dr. Rosemarie Stangl\nDr. Ulrike Pitha\nAss. Prof. Dr. Daniela Haluza\nDr. Ingrid Kaltenegger\nManuscript Submission Information\nManuscripts should be submitted online at www.mdpi.com by registering and logging in to this website. Once you are registered, click here to go to the submission form. Manuscripts can be submitted until the deadline. All papers will be peer-reviewed. Accepted papers will be published continuously in the journal (as soon as accepted) and will be listed together on the collection website. Research articles, review articles as well as short communications are invited. For planned papers, a title and short abstract (about 100 words) can be sent to the Editorial Office for announcement on this website.\nSubmitted manuscripts should not have been published previously, nor be under consideration for publication elsewhere (except conference proceedings papers). All manuscripts are thoroughly refereed through a single-blind peer-review process. A guide for authors and other relevant information for submission of manuscripts is available on the Instructions for Authors page. Sustainability is an international peer-reviewed open access semimonthly journal published by MDPI.\nPlease visit the Instructions for Authors page before submitting a manuscript. The Article Processing Charge (APC) for publication in this open access journal is 1900 CHF (Swiss Francs). Submitted papers should be well formatted and use good English. Authors may use MDPI\'s English editing service prior to publication or during author revisions.\n- green infrastructure\n- green-blue infrastructure\n- green retrofit\n- urban planning\n- climate change adaption\n- microclimatic improvement\n- health impact\n- public health']"	['<urn:uuid:bdf68058-4d72-49a7-a14a-335b6cf24bb4>', '<urn:uuid:080598fc-c286-4277-9ba0-6d93da8307f6>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T03:34:55.284799	29	106	1619
81	techniques evaluate software speed stability automatic manual compare benefits	There are two key approaches to evaluating software speed and stability. The manual approach involves human testers performing operations and visually looking for performance issues, but this is time-consuming and unreliable since human perception of frame rates varies significantly. The automated approach uses tools like dumpsys for UI testing to measure precise frame timing data and detect issues like jank, while load testing tools automatically simulate thousands of virtual users to measure system performance under load. The automated approach is more efficient and reliable, providing concrete metrics like response times, error rates, and frame timing data that can be analyzed systematically. Additionally, automated tests can be integrated into development pipelines for continuous performance monitoring, unlike manual testing which only provides periodic feedback.	"[""User interface (UI) performance testing ensures that your app not only meets its functional requirements, but that user interactions with your app are buttery smooth, running at a consistent 60 frames per second (why 60fps?), without any dropped or delayed frames, or as we like to call it, jank. This document explains tools available to measure UI performance, and lays out an approach to integrate UI performance measurements into your testing practices.\nMeasure UI performance\nIn order to improve performance you first need the ability to measure the performance of your system, and then diagnose and identify problems that may arrive from various parts of your pipeline.\ndumpsys is an Android tool that runs on the device and dumps interesting information about the status of system services. Passing the gfxinfo command to dumpsys provides an output in logcat with performance information relating to frames of animation that are occurring during the recording phase.\n> adb shell dumpsys gfxinfo <PACKAGE_NAME>\nThis command can produce multiple different variants of frame timing data.\nAggregate frame stats\nWith Android 6.0 (API level 23) the command prints out aggregated analysis of frame data to logcat, collected across the entire lifetime of the process. For example:\nStats since: 752958278148ns Total frames rendered: 82189 Janky frames: 35335 (42.99%) 90th percentile: 34ms 95th percentile: 42ms 99th percentile: 69ms Number Missed Vsync: 4706 Number High input latency: 142 Number Slow UI thread: 17270 Number Slow bitmap uploads: 1542 Number Slow draw: 23342\nThese high level statistics convey at a high level the rendering performance of the app, as well as its stability across many frames.\nPrecise frame timing info\nWith Android 6.0 comes a new command for gfxinfo, and that’s framestats which provides extremely detailed frame timing information from recent frames, so that you can track down and debug problems more accurately.\n>adb shell dumpsys gfxinfo <PACKAGE_NAME> framestats\nThis command prints out frame timing information, with nanosecond timestamps, from the last 120 frames produced by the app. Below is example raw output from adb dumpsys gfxinfo <PACKAGE_NAME> framestats:\n0,27965466202353,27965466202353,27965449758000,27965461202353,27965467153286,27965471442505,27965471925682,27965474025318,27965474588547,27965474860786,27965475078599,27965479796151,27965480589068, 0,27965482993342,27965482993342,27965465835000,27965477993342,27965483807401,27965486875630,27965487288443,27965489520682,27965490184380,27965490568703,27965491408078,27965496119641,27965496619641, 0,27965499784331,27965499784331,27965481404000,27965494784331,27965500785318,27965503736099,27965504201151,27965506776568,27965507298443,27965507515005,27965508405474,27965513495318,27965514061984, 0,27965516575320,27965516575320,27965497155000,27965511575320,27965517697349,27965521276151,27965521734797,27965524350474,27965524884536,27965525160578,27965526020891,27965531371203,27965532114484,\nEach line of this output represents a frame produced by the app. Each line has a fixed number of columns describing time spent in each stage of the frame-producing pipeline. The next section describes this format in detail, including what each column represents.\nFramestats data format\nSince the block of data is output in CSV format, it's very straightforward to paste it to your spreadsheet tool of choice, or collect and parse with a script. The following table explains the format of the output data columns. All timestamps are in nanoseconds.\n- Rows with a ‘0’ for the FLAGS column can have their total frame time computed by subtracting the INTENDED_VSYNC column from the FRAME_COMPLETED column.\n- If this is non-zero the row should be ignored, as the frame has been determined as being\nan outlier from normal performance, where it is expected that layout & draw take longer\nthan 16ms. Here are a few reasons this could occur:\n- The window layout changed (such as the first frame of the application or after a rotation)\n- It is also possible the frame was skipped in which case some of the values will have garbage timestamps. A frame can be skipped if for example it is out-running 60fps or if nothing on-screen ended up being dirty, this is not necessarily a sign of a problem in the app.\n- The intended start point for the frame. If this value is different from VSYNC, there was work occurring on the UI thread that prevented it from responding to the vsync signal in a timely fashion.\n- The time value that was used in all the vsync listeners and drawing for the frame (Choreographer frame callbacks, animations, View.getDrawingTime(), etc…)\n- To understand more about VSYNC and how it influences your application, check out the Understanding VSYNC video.\n- The timestamp of the oldest input event in the input queue, or Long.MAX_VALUE if there were no input events for the frame.\n- This value is primarily intended for platform work and has limited usefulness to app developers.\n- The timestamp of the newest input event in the input queue, or 0 if there were no input events for the frame.\n- This value is primarily intended for platform work and has limited usefulness to app developers.\n- However it’s possible to get a rough idea of how much latency the app is adding by looking at (FRAME_COMPLETED - NEWEST_INPUT_EVENT).\n- The timestamp at which input events were dispatched to the application.\n- By looking at the time between this and ANIMATION_START it is possible to measure how long the application spent handling input events.\n- If this number is high (>2ms), this indicates the app is spending an unusually long time processing input events, such as View.onTouchEvent(), which may indicate this work needs to be optimized, or offloaded to a different thread. Note that there are some scenarios, such as click events that launch new activities or similar, where it is expected and acceptable that this number is large.\n- The timestamp at which animations registered with Choreographer were run.\n- By looking at the time between this and PERFORM_TRANVERSALS_START it is possible to determine how long it took to evaluate all the animators (ObjectAnimator, ViewPropertyAnimator, and Transitions being the common ones) that are running.\n- If this number is high (>2ms), check to see if your app has written any custom animators or what fields ObjectAnimators are animating and ensure they are appropriate for an animation.\n- To learn more about Choreographer, check out the For Butter or Worse video.\n- If you subtract out DRAW_START from this value, you can extract how long the layout & measure phases took to complete. (note, during a scroll, or animation, you would hope this should be close to zero..)\n- To learn more about the measure & layout phases of the rendering pipeline, check out the Invalidations, Layouts and Performance video\n- The time at which the draw phase of performTraversals started. This is the start point of recording the display lists of any views that were invalidated.\n- The time between this and SYNC_START is how long it took to call View.draw() on all the invalidated views in the tree.\n- For more information on the drawing model, see Hardware Acceleration or the Invalidations, Layouts and Performance video\n- The time at which a sync request was sent to the RenderThread.\n- This marks the point at which a message to start the sync phase was sent to the RenderThread. If the time between this and SYNC_START is substantial (>0.1ms or so), it means that the RenderThread was busy working on a different frame. Internally this is used to differentiate between the frame doing too much work and exceeding the 16ms budget and the frame being stalled due to the previous frame exceeding the 16ms budget.\n- The time at which the sync phase of the drawing started.\n- If the time between this and ISSUE_DRAW_COMMANDS_START is substantial (>0.4ms or so), it typically indicates a lot of new Bitmaps were drawn which must be uploaded to the GPU.\n- To understand more about the sync phase, check out the Profile GPU Rendering video\n- The time at which the hardware renderer started issuing drawing commands to the GPU.\n- The time between this and FRAME_COMPLETED gives a rough idea of how much GPU work the app is producing. Problems like too much overdraw or inefficient rendering effects show up here.\n- The time at which eglSwapBuffers was called, relatively uninteresting outside of platform work.\n- All done! The total time spent working on this frame can be computed by doing FRAME_COMPLETED - INTENDED_VSYNC.\nYou can use this data in different ways. One simple but useful visualization is a histogram showing the distribution of frames times (FRAME_COMPLETED - INTENDED_VSYNC) in different latency buckets, see figure below. This graph tells us at a glance that most frames were very good - well below the 16ms deadline (depicted in red), but a few frames were significantly over the deadline. We can look at changes in this histogram over time to see wholesale shifts or new outliers being created. You can also graph input latency, time spent in layout, or other similar interesting metrics based on the many timestamps in the data.\nSimple frame timing dump\nIf Profile GPU rendering is set to In adb shell dumpsys gfxinfo\nin Developer Options, the\nadb shell dumpsys gfxinfo command prints out timing\ninformation for the most recent 120 frames, broken into a few different categories with\ntab-separated-values. This data can be useful for indicating which parts of the drawing pipeline\nmay be slow at a high level.\nSimilar to framestats above, it's very straightforward to paste it to your spreadsheet tool of choice, or collect and parse with a script. The following graph shows a breakdown of where many frames produced by the app were spending their time.\nThe result of running gfxinfo, copying the output, pasting it into a spreadsheet application, and graphing the data as stacked bars.\nEach vertical bar represents one frame of animation; its height represents the number of milliseconds it took to compute that frame of animation. Each colored segment of the bar represents a different stage of the rendering pipeline, so that you can see what parts of your application may be creating a bottleneck. For more information on understanding the rendering pipeline, and how to optimize for it, see the Invalidations Layouts and Performance video.\nControlling the window of stat collection\nBoth the framestats and simple frame timings gather data over a very short window - about two seconds worth of rendering. In order to precisely control this window of time - for example, to constrain the data to a particular animation - you can reset all counters, and aggregate statistics gathered.\n>adb shell dumpsys gfxinfo <PACKAGE_NAME> reset\nThis can also be used in conjunction with the dumping commands themselves to collect and reset at a regular cadence, capturing less-than-two-second windows of frames continuously.\nDiagnosing performance regressions\nIdentification of regressions is a good first step to tracking down problems, and maintaining high application health. However, dumpsys just identifies the existence and relative severity of problems. You still need to diagnose the particular cause of the performance problems, and find appropriate ways to fix them. For that, it’s highly recommended to use the systrace tool.\nFor more information on how Android’s rendering pipeline works, common problems that you can find there, and how to fix them, some of the following resources may be useful to you:\n- Rendering Performance 101\n- Why 60fps?\n- Android, UI, and the GPU\n- Invalidations, Layouts, and Performance\n- Analyzing UI Performance with Systrace\nAutomate UI performance tests\nOne approach to UI Performance testing is to simply have a human tester perform a set of user operations on the target app, and either visually look for jank, or spend an very large amount of time using a tool-driven approach to find it. But this manual approach is fraught with peril - human ability to perceive frame rate changes varies tremendously, and this is also time consuming, tedious, and error prone.\nA more efficient approach is to log and analyze key performance metrics from automated UI tests. Android 6.0 includes new logging capabilities which make it easy to determine the amount and severity of jank in your application’s animations, and that can be used to build a rigorous process to determine your current performance and track future performance objectives.\nTo learn more about performance tests on Android, see Automated Performance Testing Codelab. In this codelab, you’ll learn how to write and execute automated tests and review the results to understand how to improve your app performance."", 'Load testing is a vital technique for evaluating an application‘s performance under real-world user traffic. This comprehensive guide will explain what load testing is, why it‘s important, and how to effectively execute load tests during the software development lifecycle.\nWhat Exactly is Load Testing?\nLoad testing simulates multiple users accessing an application simultaneously to determine how the system holds up under different traffic volumes. It helps identify performance bottlenecks before launch so developers can optimize the application.\nThe goal of load testing is to understand application behavior under expected real-world loads. Load testing tools generate scripted user interactions that mimic actual usage patterns. For example:\n- Users browsing product pages\n- Adding items to the shopping cart\n- Checking out and making purchases\nBy scripting common user workflows, thousands of virtual users can be simulated to replicate anticipated traffic volumes and usage spikes.\nAs these virtual users interact with the application, response times, error rates, throughput, and other key metrics are measured to spot performance issues.\nLoad tests simulate multiple concurrent users to model real-world traffic.\nWell-executed load tests follow a systematic methodology to uncover 90% of potential performance problems before launch. Fixing these early in the development lifecycle is much cheaper and faster.\nWhy is Load Testing Absolutely Critical?\nFor modern web and mobile applications, fast performance and stability are essential to user satisfaction. Even minor hiccups frustrate users and damage brand reputation.\n- 49% of users expect a web page to load in 2 seconds or less. (Akamai research)\n- A 1 second delay in page load time can reduce conversions by 7%. (Google research)\n- 80% of users will abandon a slow mobile app after only 2 tries. ([Dimensional Research](https://www. dimensionalresearch.com/resources/solutions-in-action/improve-your-mobile-app-experience.white-paper.pdf))\nWithout load testing, companies risk expensive production outages and frustrated users once the system goes live. Load testing provides confidence the application can withstand peak traffic volumes.\nCost of Performance Problems\nPoor performance under load results in lost revenue, lower productivity, and increased infrastructure costs.\n- Amazon estimated that a 1 second delay cost them $1.6 billion in sales each year. (Akamai)\n- 80% of IT professionals reported performance issues reducing workforce productivity. (AppDynamics Survey)\n- On average, performance problems lead to 100 additional servers being provisioned to compensate. (Netflix)\nThis data shows how critical performance is, and why load testing forms a key line of defense.\nHow Does Load Testing Actually Work?\nLoad tests work by simulating concurrent users carrying out typical activities on the application while measuring performance. Let‘s break down the steps:\n1. Define Test Scenarios\nRealistic user workflows are scripted, for instance:\n- Browsing product category pages\n- Searching for items\n- Adding products to cart\n- Checking out\n- Logging in\n- Updating account\nDifferent scenarios can be combined to model complex user interactions.\n2. Configure Load Model\nThe load model defines the number of concurrent virtual users in each scenario, gradually increasing to peak levels:\n|Step||Virtual Users||Load Level|\nSample load model with an increasing number of virtual users.\n3. Execute Test & Monitor\nThe load test runs each scenario under the target user load while performance is measured from the client perspective. Backend infrastructure is monitored to detect bottlenecks.\n4. Analyze Results\nResponse times, error rates, throughput and other metrics are analyzed to uncover optimization opportunities. Issues are ranked by severity.\n5. Tune & Retest\nDevelopers optimize bottlenecks between test runs. Tests are executed iteratively until performance goals are met at peak load.\nExecuted methodically, load tests surface issues for resolution before launch. Retesting ensures enhancements actually improve performance.\nSummary of the core load testing methodology.\nCritical Load Testing Metrics\nVital load test metrics include:\nResponse time – The time for application pages and API calls to fully load. High response times indicate bottlenecks. The 95th percentile response time filters outliers for a stable view.\nThroughput – Requests per second successfully handled by the application. Low throughput signals potential capacity issues.\nError rate – Percentage of failed requests under load reveals stability problems.\nResource utilization – Memory, CPU, and network consumption highlights overloaded system components.\nAnalyzing these metrics identifies optimization opportunities to resolve before launch. The metrics are measured from actual user locations for accuracy.\nLoad Testing Best Practices\nFollow these expert best practices for successful load testing:\n- Define objective success criteria – Quantifiable goals for response times, throughput, and errors that indicate acceptable performance. Know precisely when a test passes or fails.\n- Model realistic usage – Base test scenarios and data on real user workflows captured in production monitoring and analytics. Achieve authenticity.\n- Leverage real user data – Seed test data from production logs and A/B testing variants to achieve real-world conditions.\n- Ramp load gradually – Steadily increase users over multiple steps to uncover issues as load builds. Avoid unrealistic spike testing.\n- Run long duration tests – Execute tests over hours or days to uncover memory leaks and performance degradation over time.\n- Monitor infrastructure – Watch key application and infrastructure metrics to isolate the true bottlenecks.\n- Iterate and retest – Optimize priority issues between runs for continuous improvement until success criteria are met.\nFollowing these best practices ensures your load tests accurately model real-world conditions and provide actionable results.\nSummary of key load testing best practices.\nCloud Load Testing Services\nCloud load testing services like BlazeMeter, LoadNinja and Neustar provide on-demand load generation without needing to build complex internal test environments.\nThese services offer key capabilities:\n- Instant provisioning of thousands of load test agents globally\n- Script recording and test configuration through intuitive web UIs\n- Detailed analysis of end-user response times, errors, and infrastructure metrics\n- APIs and CI/CD integration to enable automated testing\n- Pay-per-use pricing instead of large fixed costs\nCloud services enable running full scale tests on-demand which minimizes infrastructure costs. Teams can focus on generating insights rather than maintaining test environments.\nOpen Source & Commercial Tools\nFeature-rich open source and commercial load testing tools are available for on-premises usage, including:\nJMeter – Leading open-source tool featuring easy script recording, thread groups to model user loads, and plugins to extend functionality.\nGatling – High performance open-source tool with a domain-specific scripting language optimized for load testing.\nLoadRunner – Mature commercial tool from Micro Focus with comprehensive scripting, correlation, load generation, and analysis.\nNeoLoad – Commercial tool with scriptless recording, CI/CD integration, advanced analytics, and geographic load balancing.\nSelf-managed tools provide complete control over test environments and advanced scripting capabilities for complex scenarios.\nIntegrating Load Testing into CI/CD Pipelines\nTo prevent performance regressions, load tests should be integrated into CI/CD pipelines. Lightweight tests can execute on every build, while full end-to-end tests run nightly or weekly.\nKey practices include:\n- Run API-level load tests per build to validate backend. Mock dependencies.\n- Execute automated UI tests against preview environments on every merge.\n- Schedule full scale load tests weekly to validate complex flows.\n- Set load test success criteria per environment, increasing until production.\n- Leverage canary analysis to validate changes in production with a percentage of users.\nThis testing shift left approach catches regressions early and provides safety nets at each stage. Engineers get rapid feedback to address issues.\nGeographically Distributed Load Generation\nApplications with worldwide users must be tested from different geographic regions to accurately simulate real usage. However, building global test environments is extremely complex and expensive.\nCloud load testing services help solve this by dynamically provisioning load generators near target users. This provides accurate network conditions and latencies for each region.\nResidential proxies also assist with geo-distributed load testing. By routing traffic through diverse residential IPs, tests can simulate users in different countries from a central location. Leading tools integrate proxies to easily model regional user populations.\nRealistic Load Testing with Residential Proxies\nTo generate realistic load, the IP addresses used during testing must mimic actual user traffic:\n- Residential IP addresses – Addresses assigned to home ISPs rather than data centers\n- Geographic diversity – Mix of IP addresses from different cities, states and countries\n- IP rotation – Frequent rotation of source IP addresses per thread\n- Unlimited IPs – Large, constantly changing pool of IP addresses\nResidential proxies satisfy these requirements by routing traffic from real user devices through to the application. This contrasts with data center proxies with easily identified IP ranges:\n|Residential Proxies||Data Center Proxies|\n|Broad geographic distribution||Limited regions|\n|Random IP rotation||Static allocations|\n|Unlimited IP pool||Constrained, fixed IPs|\n|Residential ISP IPs||Data center IP ranges|\nIntegrating residential proxies into load tests is straightforward. Leading tools like BlazeMeter, Loader.io, and JMeter have built-in integrations for proxy rotation.\nThis enables easily simulating geo-distributed users during local load testing. More realistic test traffic improves the validity of test results.\nResidential proxies enable realistic load testing from a central location.\nLoad Testing for Mobile Apps\nThere are several techniques available to load test native and mobile web applications:\n- Real devices – Install and test apps directly on racks of real smartphones and tablets. Provides high fidelity results but requires significant infrastructure.\n- Emulators – Load generators leverage integrated Android and iOS emulators to simulate mobile user traffic. Limited fidelity but low overhead.\n- API testing – Test backend APIs under load to validate server-side performance and infrastructure scaling. Lightweight compared to full UI testing.\nA combined strategy is optimal:\n- Test APIs every build to validate services and integration points\n- Run emulators daily to verify app UI and user flows\n- Schedule quarterly real device tests to benchmark performance\nThis provides comprehensive confidence across backend services, app UI, and end-user experiences.\nCommon Load Testing Mistakes to Avoid\nWhile load testing is valuable, common mistakes diminish its effectiveness:\n- No clear success criteria – Key metrics and thresholds for minimum acceptable performance left undefined\n- Unrealistic usage modeling – Not matching actual workflows and access patterns skews results\n- Ramping up load too quickly – Causes artificial spikes unlike gradual production usage ramps\n- Not monitoring infrastructure – Missing server health metrics prevents isolating true bottlenecks\n- Short test durations – Fails to uncover long-term resource leaks and performance degradation\n- Minimal result analysis – Lack of diagnosis into root causes of issues\n- Late cycle testing – Outages occur because insufficient time remains to fix issues\nAvoiding these pitfalls ensures your load testing practice provides maximum value and actionable insights.\nConclusion and Key Recommendations\nLoad testing confirms your application performs well under real-world user volumes before launch. Follow these recommendations for effective load testing:\n- Integrate into CI/CD pipelines to catch issues early and prevent regressions\n- Model authentic user behaviors based on production analytics for accurate results\n- Validate both front-end and back-end using real browsers, emulators, APIs, and services\n- Utilize load testing best practices like gradual load ramps, long runs, and iterative optimization\n- Leverage cloud services and tools for flexibility and rapid provisioning of large test environments\n- Employ residential proxies to simulate geo-distributed users from a central location\n- Analyze and retest frequently to continuously improve performance at each load level\nAdopting load testing best practices prevents performance problems and ensures your software can deliver great user experiences at any scale. The downstream savings from avoiding launch issues are enormous.\nHopefully this guide provides a comprehensive overview of the importance of load testing, how to execute it effectively, and key tools and techniques for success. Please let me know if you have any other questions!']"	['<urn:uuid:a6b42f34-289e-4ae0-b7d4-f68fdcf0caf0>', '<urn:uuid:7e066c80-2454-4b34-b781-8afdb882e0ee>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T03:34:55.284799	9	122	3869
82	As an entomologist researching insect symbolism, how do the raven and honey bees communicate in literature?	The raven communicates through a single repeated word 'Nevermore' in response to questions, while honey bees communicate through an elaborate 'waggle dance' to inform other bees about food sources. Both forms of communication serve crucial purposes - the raven's response drives the narrative of loss and despair, while the bees' dance ensures colony survival by sharing information about nectar and pollen locations.	['“The Raven” is a famous narrative poem by American writer Edgar Allan Poe, first published in 1845. It tells the story of a man who is visited by a mysterious raven one night, and how the bird’s repeated refrain of “Nevermore” drives the man to madness. This article will cover the poem The Raven summary in a detailed manner for you to easily understand as a literature student.\nThe Raven Summary By Edgar Allan Poe\nThe poem begins with the man, who is unnamed, reading a book in his chamber on a bleak December night. He is mourning the loss of his beloved Lenore, who has died. Suddenly, he hears a tapping at his chamber door, and he goes to investigate. When he opens the door, he finds no one there, but he hears a whisper, “Lenore.” He opens the door again, but only darkness remains.\nAs he returns to his room, he hears the tapping again, this time at his window. He opens it to find a raven perched on the sill. The man is surprised by the bird’s presence but welcomes it inside. The raven sits on a bust of Pallas, the ancient Greek goddess of wisdom, and the man begins to speak to it, asking for its name. The raven responds, “Nevermore.”\nThe man is fascinated by the bird’s ability to speak and begins to ask it questions. He asks if he will ever see Lenore again, but the raven only responds with its refrain, “Nevermore.” The man becomes increasingly agitated and demands that the bird leave him alone. However, the raven remains perched on the bust, its red eyes glaring at the man.\nThe man begins to sink into despair, and the raven’s repeated refrain of “Nevermore” becomes a haunting reminder of his loss. He becomes convinced that the bird is a symbol of his own doom, and that it has been sent by the devil. He begs the bird to leave him, but it only responds with “Nevermore.”\nAs the night wears on, the man becomes more and more frenzied, and the raven’s presence becomes unbearable. Finally, he accepts that he will never be rid of the bird and that it will continue to haunt him for the rest of his days. He sinks to the floor in despair, as the raven continues to croak its mournful refrain of “Nevermore.”\n“The Raven” is a powerful and haunting poem that has captivated readers for generations. Its themes of grief, loss, and the inevitability of death have resonated with people around the world, making it one of Poe’s most enduring works. Its eerie, gothic imagery and powerful use of language have cemented its place in literary history as a masterpiece of American literature.\nThe Raven Analysis Edgar Allan Poe\n“The Raven” by Edgar Allan Poe is a haunting narrative poem that explores themes of grief, loss, and the overwhelming power of the human psyche. Poe uses vivid and often disturbing imagery to create a dark and foreboding atmosphere, which is heightened by the use of repetition and symbolism.\nOne of the most striking aspects of the poem is the repeated use of the word “Nevermore,” which is spoken by the raven in response to the narrator’s questions. This word becomes a kind of refrain, echoing through the poem and adding to the sense of despair and hopelessness that the narrator experiences. The raven itself becomes a powerful symbol of death, doom, and the inevitability of loss. Its black feathers, red eyes, and ominous presence all contribute to its sinister and otherworldly character.\nThe poem’s central theme is the idea of loss and the profound impact that it can have on the human psyche. The narrator is consumed by grief over the loss of his beloved Lenore, and his encounter with the raven only serves to intensify his feelings of despair and hopelessness. The poem explores the idea that grief can become all-consuming, leading to a kind of madness or obsession that can be difficult to shake.\nAnother important aspect of the poem is the use of symbolism. The raven is not just a symbol of death and doom, but also of wisdom and knowledge. Its perch on the bust of Pallas, the ancient Greek goddess of wisdom, adds another layer of symbolism to the poem, suggesting that even in the midst of despair and loss, there is still the possibility of gaining knowledge and understanding.\nOverall, “The Raven” is a complex and deeply layered poem that continues to captivate readers to this day. Its exploration of the human psyche, its powerful use of language and imagery, and its haunting and melancholy tone all contribute to its enduring appeal.\nThe Raven Rhetorical Analysis Edgar Allan Poe\n“The Raven” by Edgar Allan Poe is a masterful example of rhetorical devices used in literature. Poe uses a variety of techniques to create a haunting and powerful atmosphere in the poem, which contributes to its enduring popularity and appeal.\nOne of the most notable rhetorical devices used in the poem is repetition. The word “nevermore” is repeated throughout the poem, creating a sense of foreboding and despair. The repetition of this word serves to emphasize the finality and inevitability of loss and reinforces the central theme of the poem. Additionally, the repetition of certain phrases, such as “doubting, dreaming dreams no mortal ever dared to dream before,” adds to the hypnotic quality of the poem, drawing the reader in and keeping them engaged.\nPoe also makes use of alliteration and internal rhyme to create a rhythmic and musical quality to the poem. For example, in the lines “Deep into that darkness peering, long I stood there wondering, fearing,” the repetition of the “d” sound and the internal rhyme of “peering” and “fearing” contribute to the poem’s sense of unease and tension.\nAnother important rhetorical device used in the poem is symbolism. The raven itself is a powerful symbol of death and despair, and its presence serves to reinforce the central theme of the poem. The use of the bust of Pallas as the raven’s perch is also symbolic, representing the tension between knowledge and darkness, and the idea that even in the midst of despair and loss, there is still the possibility of gaining wisdom and understanding.\nFinally, Poe’s use of imagery is also noteworthy. The poem is full of vivid and often disturbing images, such as the narrator’s “sorrow for the lost Lenore,” the “silken sad uncertain rustling of each purple curtain,” and the raven’s “fiery eyes” and “beak that shone like ebony.” These images serve to create a sense of unease and tension, drawing the reader into the narrator’s dark and troubled world.\nOverall, “The Raven” is a powerful example of the use of rhetorical devices in literature. Poe’s use of repetition, alliteration, symbolism, and imagery all contribute to the poem’s haunting and unforgettable atmosphere, making it one of the most enduring works of American literature.\nHow Do You Explain The Raven And Its Visit In The Poem?\nIn Edgar Allan Poe’s “The Raven,” the raven is a symbolic representation of the narrator’s grief and despair. The bird’s visit to the narrator’s chamber is a supernatural occurrence that contributes to the eerie and foreboding atmosphere of the poem.\nThe raven’s visit is prompted by the narrator’s repeated questioning about his lost love, Lenore. The bird’s response of “nevermore” to each question is a symbolic representation of the narrator’s inability to find solace or closure in his grief. The repetition of this word adds to the sense of despair and hopelessness that the narrator experiences.\nThe raven’s black feathers, red eyes, and ominous presence all contribute to its symbolic meaning. The color black is often associated with death and mourning, while the red eyes suggest a demonic presence. The bird’s unchanging expression and relentless presence add to the sense of unease and tension in the poem.\nThe raven’s perch on the bust of Pallas, the ancient Greek goddess of wisdom, adds another layer of symbolism to the poem. This placement suggests that even in the midst of despair and loss, there is still the possibility of gaining knowledge and understanding. However, the raven’s repeated response of “nevermore” suggests that the narrator will never find the answers or closure that he seeks.\nOverall, the raven’s visit is a symbolic representation of the narrator’s grief and despair. Its presence contributes to the dark and haunting atmosphere of the poem, and its response of “nevermore” serves to reinforce the central theme of loss and the inability to find solace or closure.\nIs The Raven In The Poem The Raven Real?\nIn the context of the poem, it is unclear whether the raven is a real bird or a supernatural entity. Poe leaves it up to the reader’s interpretation to decide whether the bird is an actual raven or a figment of the narrator’s imagination.\nHowever, it is important to note that “The Raven” is a work of Gothic fiction, a genre known for its use of supernatural and mystical elements. In this genre, it is not uncommon for authors to blur the lines between reality and imagination, and to use symbolism and metaphor to create a sense of unease and mystery.\nGiven the supernatural elements present in the poem, it is possible to interpret the raven as a symbol of the narrator’s grief and despair, rather than a literal bird. The raven’s ominous presence, black feathers, and red eyes all contribute to its symbolic meaning as a representation of death and loss. Additionally, the bird’s unchanging expression and relentless presence suggest a supernatural quality, adding to the eerie and unsettling atmosphere of the poem.\nOverall, while it is not explicitly stated whether the raven in the poem is real or imaginary, its symbolic significance and the Gothic genre in which it is situated suggest that it may be more than just a literal bird.\nThe Raven Meaning Edgar Allan Poe\n“The Raven” by Edgar Allan Poe is a poem about the narrator’s grief and despair over the loss of his love, Lenore. The raven that visits the narrator’s chamber serves as a symbolic representation of his unending sorrow and inability to find closure. The bird’s response of “nevermore” to each of the narrator’s questions reinforces the central theme of loss and the finality of death. Through the use of symbolism, repetition, and imagery, Poe creates a haunting and unforgettable atmosphere that captures the universal human experience of grief and loss.\nI hope that you liked the summary of The Raven poem, written by Edgar Allan Poe, provided by me in this very article. You can also check out my other articles on The Cuckoo Poem Summary and Nineteen Hundred and Nine Poem Summary.', 'Pollinators play an important part of human existence. In recent years some pollinators have been struggling to survive. The preservation of our ecosystem including the food we consume daily relies on the health and abundance of our pollinators.\nLet’s start off by familiarizing what the role of a pollinator actually is. A pollinator is an agent that transmits or removes the pollen from flowers, grasses, trees and weeds. Pollination is the transfer of pollen from the anthers (male part) to the stigma (female) part of flowers. The most popular pollinators are bees and wasps, flies, butterflies and birds. Surprisingly to many, bats and rodents are pollinators too. However, bees are responsible for the majority of pollination.\nHOW POLLINATION HAPPENS. Plants contain reproductive cells, which are known as gametes. There are both male and female gametes. The male gametes are found inside minute pollen grains on the anthers (oval shaped and pollen producing) of the flower. Female gametes are found in the ovules of a flower. When the male and female gametes are joined, this process is known as pollination.\nPollination plays a critical role in the life cycle of flowering plants. It’s the sexual reproduction process of flowering plants, which results in seeds germinating into new plants. Flowering plants contain the necessary elements that are needed for sexual reproduction.\nPollen can’t move on its own. So, pollination relies heavily on other sources to move or transport it. Insects, mammals and birds gather the pollen from the male anthers and carry it to the female stigma. Wind also can be a conductor to transport pollen. This process is called anemophily. Both plant crops and trees are pollinated by the wind. Some examples are: barley, corn, rice, rye, oats and wheat, firs, pines and spruce. Some species of hardwood trees are utilized for the production of nuts. There are many different species of flowers. Each one has its own color, shape and odor that it emits. What attracts pollinators to them are the sugary nectar and pollen that each one contains.\nMOST PROLIFIC POLLINATORS. Bees, which are social insects, are the most responsible for the pollination of our food supply. Arguably, the one that is referenced the most is the western honey bee (Apis mellifera). The western honey bee is responsible for pollinating about 80 percent of our food supply. Some of the crops they are responsible for pollinating are apples, alfalfa, blueberries, strawberries, raspberries and almonds. The pollination of almonds relies solely on the honey bee. Pollination performed by the honey bee and other insects is known as entomophily.\nThe honey bee belongs to the insect order Hymenoptera, which is the third largest order of insects. This insect order is composed of ants, bees, sawflies and wasps. As our pollinators, they are the most beneficial insect order to mankind. Honey bees are eusocial as they belong to a complex society. Their colonies consist of one breeding female queen, a few thousand males (drones) and a large population of sterile female bees known as workers. These insects go through a complete metamorphosis, which consists of egg, larva, pupa and adult. The gestation time varies for each cast from 16 days for the queen to 21 days for the worker and 24 days for the drone. The life expectancy of the queen is three to four years. The drone dies after mating or is banished from the hive prior to winter. The worker bee can live up to six weeks during summer months and even longer in the winter contingent upon the geographic location of the hive. Honey bees in general are very docile.\nThe female does possess a stinger and she does not want to sting, unless provoked. In the event she would sting a human or an animal she would die. This is due to the fact that the stinger, which is located at the end of the abdomen, becomes removed. All of the types of bees co-exist with one another in a habitat known as a hive. Depending on the locale of the hive, there could be approximately 40,000 to 80,000 bees during peak summer months. This is the time of year the hive would be the strongest. The worker bees are the ones who forage for a food source. Once the food source is located, they communicate with one another in a well-choreographed dance know as the “waggle dance.” This informs the others of where the specific food source is located.\nDECLINING POPULATION. There has been an alarming decline of these insects over the years. One of the most well- known phenomenon is colony collapse disorder (CCD). CCD was first reported in 2006 by beekeepers as they experienced significant losses to their hives during the spring. These losses are attributed to the worker bees vacating the colony, leaving behind a queen, honey and a few nurse bees to care for the immature bees and the queen. CCD was once thought to be a major factor of bee decline. However, the number of reported cases has declined over the last five years.\nBecause there has been a decline in both managed colonies and bees in the wild, there are additional elements contributing to this. Based on research that is ongoing and conducted by multiple governing/extension agencies, universities and research labs, one factor is climate change. Different parts of the United States that experience unseasonably mild winters have altered the schedule of blooming flowers. When the bees first emerge for the new season, the flowers, which are their food source, have already bloomed and died. A second factor would be the elimination of habitat. Plush, rural areas that are normally full of flowers, weeds and grasses have been changed into suburban or urban settings.\nAnother reason that’s widely discussed in the consumer media is pesticide use. The neonicotinoid class of insecticides in particular has been singled out. Neonicotinoids can be applied to the soil, used on crops as well as used as a seed treatment. Eventually, it reaches the nectar and the pollen, which then in turn can be ingested by the insect. There is ongoing research that is being conducted by many governing/extension agencies, universities and other organizations regarding neonics and honey bee health. Just this year, the state of Maryland became the first state to ban the use of neonicotinoids to consumers. However, individuals who are properly trained and licensed can still utilize this material when following specific guidelines.\nLastly, another factor in honey bee health is the Varroa mite (Varroa destructor). This mite was first reported in the U.S. in 1987; it originally emanated from Asia. This parasite attaches itself to the honey bee and sucks hemolymph fluid (blood) from the bee. They are also transmitters of pathogens with one of them being the deformed wing virus. Presently, beekeepers say the Varroa mite is the main culprit of colony loss today. Honey bees and their hives are also susceptible to the tracheal mite, hive beetle and wax moths. Education, proper nutrition, routine inspection, record keeping and the use of Integrated Pest Management (IPM) practices will assist in keeping one’s colony as healthy as possible.\nWHAT CAN WE DO? As we struggle to help save the bee and bring more of them back into our world, there are many things that society — and pest management firms — can do.\nOne is education. While honey bees can swarm and can be found in the voids of homes and commercial structures, it’s important that the bees are not treated with pesticides. They should be removed by a local beekeeper or a designated pest management firm.\nProfessionals and customers alike can create an environment conducive to a pollinator habitat. For some, this includes eliminating the use of all pesticides and switching to an all-organic program. It may include an organic lawn and garden program that utilizes IPM techniques. Natural pest management may be incorporated such as the use of lady bugs and/or lacewings to control aphids and spider mites, nematodes to control grubs, etc.\nHomeowners can install plants and flowers that are conducive to pollinator attraction. Different species of flora attract different pollinators. There is an abundance of resources available via the web (check out the U.S. Forest Service publication titled “Attracting Pollinators to Your Garden Using Native Plants”. Once a specific species of flora is found, it is recommended to check the USDA Plant Hardiness Zone map to determine if the selected flora will thrive in your geographic location.\nIf pest management firms or customers want to take this initiative one step further, they can become a beekeeper hobbyist. This is when an individual manages a beehive or multiple beehives on his or her property. The first step would be to obtain literature and other resources to educate oneself. Two superb magazine publications are Bee Culture & The American Bee Journal. This is, in part, to determine if this is right for you and/or your customer’s family.\nDue to the vast interest in keeping bees, the term “backyard beekeeper” has been coined. For those who don’t necessarily have the land to keep bees there is also urban beekeeping that takes place in a city setting. Urban beekeeping has exploded during the last several years. It utilizes rooftops, balconies and other spaces for people to manage their own bee hives. To illustrate just how popular urban beekeeping has become, there are almost 300 registered hives in the New York City metropolitan area. London is also another major city where urban beekeeping is prevalent.\nIt’s good to check with your municipality or city ordinances to determine if the keeping of bees is legal. Join your local beekeepers club. It’s a great way to network, meet other keepers and obtain knowledge.Purchase your equipment and bees through local beekeepers or various online stores. If keeping bees is not for you but you would like to help… some beekeepers will place their hives on your property, they will care for them and in exchange you will be rewarded with honey. This is another way of making an impact.\nYou are now on your way to becoming a beekeeper and contributing to preserving our eco-system.\nThe author is a service specialist and master technician with RK Environmental Services, Westwood, N.Y.']	['<urn:uuid:8fa833d1-b8a4-4238-b464-5b46475facd8>', '<urn:uuid:e48e99b7-843c-47c7-abab-4b0e15d81d6a>']	factoid	with-premise	concise-and-natural	similar-to-document	three-doc	expert	2025-05-13T03:34:55.284799	16	62	3494
83	What misconception about learning math does brain science reject?	Brain science shows there is no such thing as a math person – anyone can learn mathematics to high levels.	['Youcubed.org is a free K-12 math resource from Stanford University’s Graduate School of Education. Many of you are familiar with the Week of Inspirational Math, but here are a few of the other resources you’ll find on the site.\nMathematical Mindset Algebra is a 4-week curriculum unit developed by Youcubed to introduce algebraic concepts at any grade level. In the Week 3 materials, students continue exploring linear functions, this time comparing them to quadratic functions while creating and analyzing multiple representations of both functions. Youcubed also has a discussion group on Facebook for those who would like to share ideas about the algebra curriculum with others, and Week 4 will be available on September 25!\nMindset Mathematics for Grades 3-6\nCreated by Jo Boaler, Jen Munson and Cathy Williams, Mindset Mathematics is designed around the principle of active student engagement, with tasks that reflect the latest brain science on learning. Open, creative, and visual math tasks have been shown to improve student test scores, and more importantly change their relationship with mathematics and start believing in their own potential. The tasks in Mindset Mathematics reflect the lessons from brain science that:\n- There is no such thing as a math person – anyone can learn mathematics to high levels.\n- Mistakes, struggle and challenge are the most important times for brain growth.\n- Speed is unimportant in mathematics.\n- Mathematics is a visual and beautiful subject, and our brains want to think visually about mathematics.\nWith engaging questions, open-ended tasks, and four-color visuals that will help kids get excited about mathematics, Mindset Mathematics is organized around nine big ideas which emphasize the connections within the Common Core State Standards (CCSS) and can be used with any current curriculum.\nOnline Courses for Teachers\nStanford offers two online courses for teachers: Mathematical Mindsets and How To Learn Math For Teachers.\nThe Mathematical Mindsets course helps educators inspire and boost math achievement. You’ll learn the latest neuroscientific research on the best methods by which students learn math, as well as the specific methods and approaches you can use to successfully help your students develop a growth mindset.\nHow To Learn Math For Teachers explores the new research ideas on mathematics learning and student mindsets that can transform students’ experiences with math. Whether you are a teacher preparing to implement the new State Standards, a parent wanting to give your children the best math start in life, an administrator wanting to know ways to encourage math teachers or another helper of math learners, this course will help you. Watch this video to view an overview of the online course.\nBoth of these courses are self-paced, and participants will receive a Record of Completion upon finishing the course. Tuition for each course is $99 payable to Stanford. I am currently setting up inservices in PD Place so that teachers who complete a course can be awarded clock hours through NSD (more details to follow).\nFree Online Course for Students (or anybody)\nHow to Learn Math is a free class for learners of all levels of mathematics. It combines really important information on the brain and learning with new evidence on the best ways to approach and learn math effectively. Many people have had negative experiences with math, and end up disliking math or failing. This class will give learners of math the information they need to become powerful math learners, it will correct any misconceptions they have about what math is, and it will teach them about their own potential to succeed and the strategies needed to approach math effectively. If you have had past negative experiences with math this will help change your relationship to one that is positive and powerful.\nNOTE: Teachers are welcome to take this free course, but no clock hours can be awarded for this online course since the course doesn’t supply a Certificate of Completion. Teachers who wish to earn clock hours should consider taking the course, How To Learn Math For Teachers (see above).\nYoucubed has a variety of resources and articles that may help parents (and teachers!) support their child’s mathematical learning. ~']	['<urn:uuid:ca1bb8c1-bdee-4d81-ba9a-b2da535a5880>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T03:34:55.284799	9	20	686
84	best backyard lighting avoid neighbor complaints	To avoid disturbing neighbors with garden lighting, it is better to use several medium-voltage small lights rather than a single floodlight. Bulbs should not be brighter than 100 watts, as this will create a 'black hole' of unlit space behind and may annoy neighbors. Multiple smaller lights create a lively and warm feeling without dazzling.	['Redesigning Your Garden\nMaking a Plan\nBefore you begin, check with your local planning office to see whether your garden is covered by any planning regulations. This may be the case if, for instance, you live in a conservation area. Mature trees may be protected by a Tree Preservation Order. If your house is a listed building or there is one nearby, you may also encounter restrictions on layout and height in the garden. If you need to apply for permission for any changes, apply as soon as possible, as sometimes the process can be slow.\nWhat is on your garden wish list?\nThink about what you want your garden to do. It should reflect your lifestyle, the people who will be enjoying it and the time you plan to spend in it, whether it is relaxing, playing or tending the plants.\nIf you have small children and they enjoying playing ball games, a large lawn is a good choice, space allowing. Fill borders with robust evergreen shrubs and grasses rather than delicate flowers.\nIf you enjoy entertaining in your garden, lay a patio surrounded by fragrant, low-maintenance borders.\n- It’s best to plan your project on paper first. You can always refine your ideas by developing several versions, but date them so you know which one is most recent.\n- Map out the existing garden. Make sure you include everything in it. A plan made to a scale of 1cm:1m (1:100) is a workable scale for most gardens. If you have a small garden, you may be able to use a scale of 1cm:0.5m (1:50).\n- Identify your boundaries. Mark whether they are fences, walls or hedging. Make a note of hedging, fence or wall style too and for hedging show the spread of foliage. Boundaries can look almost insignificant on a plan, but they can be a strong visual presence within the garden, completely changing the intended design.\n- Mark out the house. Include all major structures and any areas of hard landscaping. Then draw in details like paths, sheds, compost bins, pergolas, and water features.\n- Note the existing borders. Draw in flower beds, herb or vegetable gardens, and other large areas of planting.\n- Mark the orientation of the space. Note down North and mark areas where shade is cast at different times of day.\n- Remember utilities. Mark the position of any utilities, including water, sewage and electricity. It may be useful to show underground services and overhead services using different colours. The location from underground services can often be identified using the position of inspection hatches and above ground piping.\nUsing a Professional\nYou might decide to employ a professional designer. They can bring a more polished finish to the final design and may have creative, practical ideas on how to make the most of your space. If you do hire a professional, be very clear what you are looking for in the brief and ask for a written contract if one is not provided. Most problems with professional design work arise from the designer misunderstanding what the client wants, or the client changing their mind about what they want half way through the process. A written agreement can help to avoid this problem.\nChoose a qualified professional who is willing to spend time in discussion with you at the planning stage and who will take into account your wishes and your needs. But remember that they are the expert and it makes sense to listen to their advice.\nA Full-Size Plan\nOnce you have created your plan on paper, the best way to test how well it will work in practice is to mark it out on the ground and live with it for a while.\nThe simplest method is to use a hosepipe, length of rope or brightly-coloured spray paint to mark out the shape of the borders, lawns, and other large areas. Use stakes and twine to represent the exact shape and height of the hedges.\nOutline the paths with stakes and heavy string that is clearly visible – brightly-coloured string or twine is idea.\nMake a Note of Which Direction Your Garden Faces\nIf you understand the orientation of your garden, you will be much better able to make the most of your space. You will also be able to identify the best position for a patio, garden pond and other important features. Selecting plants to suit your space is also important – some plants love full sunshine, others prefer semi-shade and still others thrive in cooler, evening sun. Take into account shadows thrown by hedges and large trees when planting, as they can be just as solid as the shadows created by buildings.\nAdd Lighting to Your Plan\nStart thinking about where to position garden lighting in the early stages of planning. Too often, lighting is little more than after-thought, added after everything else has been positioned. By including lighting in your plan from the beginning, you willbe able to optimise its use within the garden.\nA light at each entrance, and several positioned along pathways and on the patio, will aid visibility – and add extra life and interest – late into summer evenings and on dull winter afternoons. It’s a good idea to illuminate any steps, raised areas and doors leading into the house.\nPlan to have a number of lighted areas, not just one. It is much more effective to have several medium-voltage small lights rather than a single floodlight: the lights will not dazzle and, at night, your garden will feel lively and warm. A bulb brighter than 100 watts will accentuate the ‘black hole’ of unlit space behind and may also annoy your neighbours!\nConsider the View\nDon’t forget to look beyond your garden to its immediate surroundings. There may be something you would like to hide from view – electricity pylons, telegraph poles, unattractive buildings – while optimising the nicest views.\nWork Out the Size of a Terrace or Patio\nCalculate the size you think you need – and then add another 1 metre to each side. Size is difficult to judge exactly, so be as generous as you can when planning.\nPlanning for a Water Feature\nThink carefully about the kind of water feature that is fitting for your style of garden.\n- You may consider a raised or sunken pond, a waterfall, a natural swimming pool, or a small bubble feature, depending on the space you have available and your priorities for the garden.\n- The site itself will have an impact on your choice: a running stream or waterfall is more natural on a sloping site, whereas a pond needs a level area where the ground is easy to excavate. If you have rock close to the surface or a high water table, a sunken pond will be impractical.\n- Remember to include electrical provision for a pump or lighting, if needed.\nNext: see Making the Most of Your Space']	['<urn:uuid:fbaf5dad-6917-4528-88e4-36e1eea4bcdb>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T03:34:55.284799	6	55	1160
85	As a teacher exploring new methods, how do effective language learning beliefs help, and what tech training issues limit success?	Effective language learning beliefs help by increasing learner motivation and persistence through challenges. When learners have realistic beliefs about what they can achieve, they have better learning outcomes and are more likely to persevere. However, technology training programs often focus only on basic computer literacy skills rather than comprehensive integration. Research shows that teachers struggle to effectively use technology in classrooms due to insufficient training that fails to combine technological knowledge with pedagogical and content expertise. To succeed, training must go beyond just teaching technical skills to help teachers understand how technology can support actual teaching and learning.	"['What second language acquisition can teach us about learning to code: Part 1\nBy Kristen Foster-Marks\nPart 1: Learner beliefs about language learning\nKnowing how to program computers is a lucrative skill—and arguably more necessary than ever in a world where computer technologies have become ubiquitous across industries and domains. These days, we often hear, “All companies are software companies”, and a quick glance at any job board confirms that indeed, most companies are actively hiring software developers and engineers.\nUnfortunately, the demand for software engineers has outpaced supply for some time now. Despite record numbers of graduates from traditional computer science programs and non-traditional programs like coding boot camps, there are still substantially more programming positions to be filled than qualified applicants to fill them.\nNot only that, but it is increasingly recognized that technology skills degrade rapidly if software developers and engineers do not make a substantial effort to engage in continuous learning and development. In fact, it is estimated that the skills and knowledge an engineer acquires today will be stale a mere two years from now.\nThese factors have forced both individuals and companies to heavily invest in upskilling, and for companies, this has included both their technical and non-technical workforce. This is easier said than done, however. Upskilling an existing cadre of programmers—and training groups of potential programmers—efficiently and successfully depends on institutional support and intention, as well as individual action and persistence.\nThe stakes are enormous: If this upskilling is not done well—if it is not done intentionally and with an awareness of pedagogical principles—the investment is unlikely to pay off at scale.\nThis situation begs the question:\nWhat are the factors that contribute to the rapid and successful acquisition of programming languages and technologies?\nAny tech leader and tech practitioner—whether current or aspiring—should be interested in the answers to this question, and we may be able to provide some of those answers by turning to the scholarly field of second language acquisition.\nWhat is second language acquisition, and what does it have to do with learning to code?\nSecond language acquisition (or SLA) is a sub-discipline of applied linguistics. Its researchers investigate the factors that influence—both positively and negatively—the human ability to learn languages in addition to one’s first/native language. The field burgeoned in the late 1960s following large financial investments by national governments; the military branches of these governments had a huge stake in quickly and effectively teaching their intelligence forces to gain fluency in key target languages in order to achieve goals in espionage and foreign diplomacy.\nSLA’s foundational theories were supplied by the long-established fields of linguistics, psychology, child language acquisition and language education. Today, it is a vast discipline with multiple dedicated academic journals, international conferences and symposiums, and long-running debates about the factors that help and hinder successful language learning.\nMany of the questions and problems that SLA has strived to answer in the domain of human language learning are analogous to those facing learners of computer languages and technologies. Some of the major lines of inquiry in SLA have included:\n• How does knowledge of one’s first/native language affect acquisition of a foreign language?\n• What effect does age have on the learning process and ultimate attainment?\n• Is implicit, incidental learning likely to lead to acquisition—or is focused, formal learning necessary to gain fluency?\n• What learner beliefs help or hinder the language learning process?\nThe stakes that catalyzed SLA’s inception feel as high as the stakes facing companies today as they struggle to keep their technical workforce filled and skilled; the stakes are certainly as high for individual engineers fighting to keep their skills relevant and themselves employable.\nMy personal connection to SLA and coding\nI spent most of my 20s teaching English to English Language Learners (ELLs) first in South Korea, and then at Colorado State University. During this time, I completed a Master’s degree in English with an emphasis in Teaching English as a Second or Foreign Language; SLA was a central topic of study in this program. After all, one of the field’s central objectives is to determine what constitutes effective instruction, and thus language teachers are expected to be well-versed in historical and current thinking about how foreign languages are acquired.\nAs it turns out, few are willing to pay teachers a liveable wage, and so in 2016, I made my own high-stakes decision to learn web development. I believe very strongly that my understanding of SLA has contributed to my success in learning programming languages as an adult—and it has been a fascinating experience to observe and theorize about the applications of SLA research to my own learning experiences.\nThus, the purpose of this article is to share these connections and applications with both individuals hoping to gain or increase their proficiency in programming languages, as well as the leaders and executives who hope to facilitate this technical upskilling.\nThe nine sub-domains of SLA\nThere are many, many aspects of SLA that can provide insight into the acquisition of programming languages—enough to warrant a book-length treatment, in fact. Those sub-domains include:\n• The linguistic environment\n• Learner cognition\n• Learner motivation and affect\n• Developmental sequences\n• Crosslinguistic influence\n• Social dimensions\n• Learner beliefs about language learning\nIn each article of this series, we’ll explore different sub-domains, examining their key questions and findings, as well as how we can apply these findings to the learning and acquisition of programming languages and technologies.\nLet’s begin with examining learner beliefs about language learning and how it connects with learning to code.\nWhat SLA teaches us about language learner beliefs\nSince Elaine Horwitz’s seminal study of language learner beliefs in 1987, thousands have followed suit in investigating language learners’ beliefs and mythologies. Early research focused on cataloging those beliefs, but more recently, researchers have endeavored to understand how learner beliefs help or hinder the language learning process and ultimate acquisition of a language. And while SLA researchers have uncovered a wide array of language learning beliefs and their ramifications, perhaps more important has been the discovery that some learner beliefs are simply not true, in that they have been discredited by scholarly research and the documented testimony of teaching practitioners.\nJust a few of these debunked (yet persistent) myths include:\n• “I can become fluent in Arabic solely by studying vocabulary and grammar out of a textbook.”\n• “I am not technically fluent in Mandarin unless I have flawless, native-like pronunciation.”\n• “I should be able to master German after three to four years of high school study.”\n• “If I speak in class before my pronunciation is perfect, my pronunciation will be incorrect forever.”\n• “I’m too old to learn Spanish. I should have done it back in high school when I had the chance.”\n• “The best way to learn Portuguese is through translating great Portuguese literature.”\nYou’ll be hard pressed to find an SLA researcher who entertains any of the above beliefs. Take the last one, for example. Countless studies have been conducted over the past several decades that show that language teaching methodologies which provide ample opportunities for receiving authentic input (listening) and producing authentic output (speaking) lead to better ultimate acquisition than traditional literature translation exercises. While there is no doubt something to be gained linguistically in translating literary samples of a language, if one’s ultimate goal is to use that language in authentic contexts, then learning Spanish through watching contemporary telenovellas and then discussing them with friends in Spanish is much more effective than translating ""Don Quixote"" in solitude.\nDespite having been discredited through academic investigation, erroneous language learning beliefs remain prevalent in language learning communities across the world (even amongst language instructors), and unfortunately, they have the potential to increase learner anxiety and deplete learner motivation to persist through challenges. It is widely documented in SLA research, for example, that achieving native-speaker-like pronunciation—colloquially referred to as “speaking without an accent”—is nearly impossible for adult learners.\nIn part, this is because accent-less pronunciation is conceptually impossible to define. Just consider the myriad dialects of the English language across cultures and geographies—within the United States, alone, you’ll find multiple ways to pronounce even the seemingly straightforward word “butter.” Further, the critical period hypothesis proposes that after a certain age (likely around two or three years old) it is extremely difficult for our articulatory apparatus to be trained to make sounds not already in its functional repertoire.\nThis is where unrealistic beliefs can prove a hindrance to the learning process: For a language learner who expects to speak a foreign language without an accent, struggling to achieve what is perhaps not even physiologically possible can be extremely demoralizing and lead to eventual abandonment of the learning goal. As a world traveler, former language teacher and tech worker who often works alongside non-native English speakers, I’ve heard countless apologies over “imperfect” English accents. I have emphasized to these folks: No good language teacher would hold a student to the standard of “perfect” pronunciation, and we should regard speaking with an accent as something to be celebrated.\nHow language learner beliefs and myths impact those learning a new programming language\nSo, how is the inquiry into language learner beliefs relevant to learning programming languages and technologies? I strongly believe that findings borne out of the aforementioned research provide insight into the experiences of those who are:\n• Learning to code for the first time\n• Learning a second, third or nth programming language\n• Learning a new technology stack\nHere’s where the analogies are strongest: SLA studies have demonstrated that the more realistic the beliefs language learners hold on the possibility of things like perfect pronunciation and grammatical accuracy, the better their learning outcomes. For this reason, recognizing those beliefs and then analyzing their truth is necessary for both teachers and learners.\nWhen I was a language teacher, I devoted the first class session of each semester to surveying and then deconstructing my students’ beliefs—especially those with no empirical support or outright debunked by the research—so that they could avoid the damaging effects of those beliefs on their motivation and ultimate skill attainment.\nComputer language learners should go through the same exercise. There are innumerable myths (enabling and hampering) that we new and experienced programmers hold about ourselves and the best ways to develop our skills, including:\n• “I need to learn language x really well before I can be on a team that uses language x.”\n• “Asynchronous code reviews are the most effective way to pass knowledge from more experienced developers to newer developers.”\n• “If I’m ever going to be a senior developer, I must be an expert in all of the technologies my team uses.”\n• “I’ll never be a great programmer if I don’t build side projects during my free time.”\n• “As a coding bootcamp graduate, I’ll never be as good of a programmer as my degree-holding colleagues.”\nDo any of these ring true for you or your team? Take a moment to reflect.\nDo you think that outside of your personal experience and mindset, there is strong evidence to support any of these beliefs? Could any of them be baseless and even destructive to your learning process? What other beliefs do you hold that may be helping or hindering you in your acquisition of computer languages and technologies?\nThe steps of identification and reflection have been crucial for me in overcoming those beliefs that have proven impediments to my tech skill development, as well as in harnessing those that have kept me motivated.\nTakeaways for programming learners and their leaders\nDuring my journey in learning how to code, I have discovered that I am in constant possession of beliefs about my individual potential. Sometimes my self-beliefs are positive, and sometimes they are negative. Sometimes, they are informed by external information about the world, and sometimes by my own inner thoughts. They always impact my motivation and persistence.\nFor example, back in 2016 when I began my coding boot camp, I truly believed that through the immersive, hands-on experience that was being replicated by boot camps across the country, I could reach a level of proficiency in six months that would make me employable as a junior developer. The evidence was all around me; the strategy had been proven out time and time again.\nYet, there were people in my social circle who were suspicious, and who even verged on discouraging me through questioning whether this path was realistic, thus suggesting that I might be embarking on an (expensive) mistake.\nThey were wrong, of course. If I had not seen the irrefutable evidence around me, and if I had thus not strongly held the belief that my goal was realistic, it’s easy to see how I might have failed to persevere during those months of intense, twelve-hour days typing away at the keyboard, ignoring the outside world, suffering daily failures in the process of acquiring a really hard skill.\nI have also held the belief, however, that my ultimate potential as a computer programmer is limited by both my nontraditional path into programming, as well as my lack of desire to spend my personal time outside of work tinkering with side projects. And to be honest, I’m still wrestling with these beliefs. I don’t want to hold these beliefs. I understand that at times, they are demoralizing and inhibiting. But as I have continued in my career, I have encountered more and more outstanding engineers who have come from non-traditional backgrounds, and as this evidence accumulates, I am increasingly chipping away at these beliefs.\nIf you’re like me, and want to start drawing inspiration from SLA in order to learn a new language or help your team upskill, I think these takeaways are good starting points:\n- Reflect on your implicit beliefs about the ideal process and conditions for learning computer languages and technologies, as well as yourself as a learner. How are those beliefs helping or hindering you? For those that hinder you, is there any actual evidence that these beliefs are true?\n- As a manager or team lead, begin a conversation with your developers one-on-one to start exploring their beliefs and mythologies around themselves as computer programmers and the process of technology upskilling. You might find that you can unlock your developers’ latent potential by identifying and working through any inhibiting or detrimental beliefs that they hold.\n- When designing an upskilling program for yourself, your team or your company, take the time to understand what current research reveals about the best strategies and processes for doing so. Harness these to accelerate and deepen skill acquisition.\n- When introducing an upskilling program or initiative to your team, take the time to understand and deconstruct their beliefs that have the potential to decrease the effectiveness of the initiative.\nContinuing to draw insights from SLA\nThere are certainly general learning theories that apply to the learning of any skill, domain or process, but I believe the parallels between the acquisition of human languages and programming languages are particularly strong. Over the last several years, I’ve perused academic literature to see if any research agendas or studies have popped up in order to empirically explore the applications of SLA research to the acquisition of computer languages and technologies. Thus far, there’s not much.\nGiven this lack of scholarly treatment, I invite you to join me as I continue to explore these connections between SLA and tech skill development by diving into SLA’s other subdomains in this multi-part blog series.\nIn the meantime, If you’re interested in diving into some SLA research, I recommend reading Patsy Lightbown and Nina Spada’s introductory text ""How Languages are Learned"" for a concise, accessible introduction to and exploration of SLA. If you’re feeling intrepid and want to get lost in the academic literature, check out Lourdes Ortega’s graduate-level textbook ""Understanding Second Language Acquisition,"" and find a way to gain access to a database of scholarly articles so that you can follow the citations!\nBibliography and suggested reading\nBaldwin, L.P., Macredie, R.D. Beginners and programming: insights from second language learning and teaching. Education and Information Technologies 4, 167–179 (1999).\nBluestone, K. (2015). Acculturation, Interpersonal Networks, and the Learner’s Sense of Self: The Effects of Social Relationships on Second Language Learning. Working Papers in Applied Linguistics and TESOL, 9(2), 1–30.\nDubravac, V., & Latić, E. (2019). The Plasticity of Students’ Language Learning Beliefs: The Interplay of Gender, Grade and Educational Level. Journal of Language and Education, 5(4), 36-53.\nHorwitz, E. K. (1987). Surveying student beliefs about language learning. In A. Wenden & J. Rubin (Eds.), Learner strategies in language learning (pp. 110-129). London, UK: Prentice Hall.\nLightbown, Patsy.Spada, Nina Margaret. (2013) How languages are learned. Oxford : Oxford University Press,\nOrtega, L. (2009). Understanding second language acquisition. Hodder Education.\nRobertson, S.A., & Lee, M.P. (1995). The application of second natural language acquisition pedagogy to the teaching of programming languages—a research agenda. SIGCSE Bull. 27, 4 (Dec. 1995), 9–12.\n5 keys to successful organizational design\nHow do you create an organization that is nimble, flexible and takes a fresh view of team structure? These are the keys to creating and maintaining a successful business that will last the test of time.Read more\nWhy your best tech talent quits\nYour best developers and IT pros receive recruiting offers in their InMail and inboxes daily. Because the competition for the top tech talent is so fierce, how do you keep your best employees in house?Read more\nTechnology in 2025: Prepare your workforce\nThe key to surviving this new industrial revolution is leading it. That requires two key elements of agile businesses: awareness of disruptive technology and a plan to develop talent that can make the most of it.Read more', ""The Many Faces of TPACK/Investigating Turkey's Technology Initiatives with TPACK\nInvestigating Turkey’s Technology Initiatives with TPACK\nby Halil Kayaduman\nRapid advancements in technology have affected almost all areas people involve in. By means of technological advancements, the processes of living and working have changed to provide better environments in which people can maintain their lives. As well as the other areas like banking, transportation, or public affairs; educational areas have also positively affected by technological advancements. It is stated in the literature that Information and Communication Technologies (ICT) has several positive contributions to educational settings. For example; Godfrey (2001) stated that ICT can provide rich learning environments, let learners adapt content relevant to their needs, foster flexible knowledge construction and allow learners to progress at their own pace and so forth. Therefore, we can say that technology integration is inevitable to meet the needs of educational settings. Moreover, it can provide more effective educational environments. Knowing that, many countries around the world have invested so much money to integrate technology into education.\n2. Why TPACK in the Process of ICT Integration?\nTPACK is the combination of technology, pedagogy and content. It stands for Technological Pedagogical Content Knowledge (TPACK). Mishra and Koehler (2006) stated that ”TPACK is the basis of good teaching with technology and requires an understanding of the representation of concepts using technologies; pedagogical techniques that use technologies in constructive ways to teach content; knowledge of what makes concepts difficult or easy to learn and how technology can help redress some of the problems that students face; knowledge of students’ prior knowledge and theories of epistemology; and knowledge of how technologies can be used to build on existing knowledge and to develop new epistemologies or strengthen old ones” (p. 1029). This model is really important for the process of technology integration in teaching and learning. It implies that there should be a combination of three key points for successful technology integration which are: technology, pedagogy, and content. Considering these three key elements, it could be easier to integrate technology into educational settings. Shin and his colleagues (2009) claimed that teacher’s comprehension toward TPACK is very important for successful technology integration. Perhaps, the reason why previous ICT integration projects did not attain their goals is due to considering the technology part only, and ignoring the other two key elements which are; pedagogy and content. Or, some earlier ICT projects have just focused on Technological Content Knowledge (TCK) while trying to integrate technology, rather than focusing on TPACK.\nWhile searching the literature, we can see that the studies consider the importance of TPACK for the process of technology integration. For instance; Abbitt (2011) investigated the relationship between TPACK and self-efficacy of pre-service teachers about technology integration. He found that there is significant and positive correlation between TPACK model and self-efficacy beliefs about technology integration. In another study, Jimoyiannis (2010) used the TPACK model for designing and implementing Technological Pedagogical Science Knowledge (TPASK) for science teachers. At the end of the study, teachers stated that their willingness and confidence in the ability of applying ICT in their own classrooms was increased. Moreover, Sahin, Aktürk, and Schmidt (2009) analyzed the vocational self-efficacy of pre-service teachers through the lens of TPACK. The results showed that participants who got high scores in TPACK in its all three dimensions, also got higher scores in vocational self-efficacy beliefs. This shows that TPACK is correlated to self-efficacy beliefs of teachers and subsequently they can be confident to use technology in their teaching activities. It can be understood from these studies that TPACK model may leverage the process of ICT integration. Thereby, we can safely say that, teachers can become more confident about technology integration in their classes, by using TPACK in high levels.\nTeachers are the key practitioners of ICT projects. Therefore, attaining the goals of projects substantially depends on teachers. This is why; comprehension of TPACK is a necessity to teachers for successful implementation of ICT integration. At this point, Dilworth et al. (2012) stated that the most effective uses of technology necessitate an understanding of content and pedagogical strategies at the same time. Thus; it is really important to consider TPACK, by that teachers can become more confident and willing to integrate technology into educational activities effectively.\n3. Turkey's Technology Initiatives\nIn Turkey, a couple of nationwide projects have been conducted to benefit from the potential of technologies in educational settings. National Basic Education Program (NBEP) and Movement of Enhancing Opportunities and Improving Technology (FATIH) can be given as examples of big technology integration projects in Turkey. To conduct the first Project, NBEP; Ministry of National Education (MEB) has taken loan from the World Bank about six hundred million dollars. The second project, FATIH, is being supported by the Ministry of National Education and the Ministry of Transportation, Maritime Affairs and Communications. The project is still continuing and it is stated that by the end, almost twenty billion Turkish Liras is going to be invested.\n3.1. National Basic Education Program\nThe purpose behind the NBEP project was to improve the quality of education and make schools the learning centers of community. The project consisted of two phases. The main purpose of the first phase was to provide technological devices to schools across Turkey. In the scope of this phase; computers, printers, scanners, TVs, educational software have been distributed to almost every school in the country. In other words, the MNE has set up information technology classrooms to all schools around Turkey. In addition to these technological devices, in-service training programs have been prepared for teachers and administrators to improve their computer literacy level. Following to the first one, the second phase has been prepared. The reason behind conducting this phase was to develop the first phase. In the scope of the second phase; an educational web portal was developed, educational software materials were distributed to the schools, more in-service training programs were provided to teachers and administrators. By means of this phase, it has been tried to overcome the difficulties of integration in educational settings and educate the pre-service teachers on usage of technology (MEB, 2007).\nAfter the NBEP project started, several research have been conducted about the integration process of ICT. Generally research about the process show that; equipment, software materials, and web sites have not been used appropriately to support educational activities. For example; Cüre and Özdener (2008) conducted a study to determine the level of teachers’ success in using ICT. The results of the study showed that teachers have lack of knowledge in using ICT in their classrooms. In another study, Yildirim (2007) has conducted a study to examine teachers’ current use of ICT in basic education schools in Turkey, and investigated the barriers of technology integration. The results showed that most of the teachers were not able to use ICT to help students’ learning across curriculum. They mostly used ICT tools for preparing handouts and tests. As the barriers of technology integration, teachers also stated the ineffective in-service training programs, inappropriateness of curriculum, lack of incentives, and lack of pedagogical support. In the study of Akbaba Altun (2006), she aimed at identifying the issues related to integrating computer technologies into education. It was found that teachers lacked of computer literacy skills, and in-service training programs for teachers were not sufficient, particularly in content areas. Askar and Usluel (2003) examined three primary schools and found out that teachers find using computers beneficial for the purpose of administrational and personal issues. However, they find using computers questionable for educational purposes.\nAll in all, NBEP was a big budget ICT integration project conducted in Turkey. It can be concluded from the literature that it has been a problematic process. Teachers generally are not able to integrate technology into their educational. As Askar and Usluel (2003) argued that even though teachers used computers for personal issues, they did not prefer to use it for educational activities. For this reason, the in-service teacher training programs were insufficient to develop Technological Knowledge (TK) and more importantly TPACK level of teachers for the successful implementation.\n3.2. Movement of Enhancing Opportunities and Improving Technology (FATIH)\nAfter the NBEP, there is now another huge project which is still being conducted in Turkey named “Movement of Enhancing Opportunities and Improving Technology”, shortly FATIH. It was released to the public in November 2010 with the intention of adapting new technology into education. The aim of this project is to provide ICT equipment to classrooms and attain the goals of education by supporting teaching processes with the help of ICT. Within the scope of FATIH project, 42.000 schools and 570.000 classrooms are going to be equipped with the latest technology and turned into computerized classrooms. Other aims of the project are, also, to provide educational e-content, increase effective usage of ICT in teaching programs, and arrange in-service teacher training programs to help the process of ICT integration (MEB, 2013).\nBeing invested almost twenty billion Turkish Liras (more than 11 billion US dollars), the FATIH project is claimed to be the biggest ICT integration project all over the world. Considering this large investment, we should pay more attention for the components of this project. Otherwise, it is going to be inevitable to see similar research consequences like other unavailing projects. It can be understood from the aim and scope of the FATIH project that teaching and learning environments are going to be reshaped by integrating technology. Toci and Peck (1998) claimed that providing technologies to educational settings only does not mean that they are going to be properly used as a part of learning and teaching process. So, it is not enough to provide just the equipment to schools. That’s why; each step of FATIH project should be carefully planned to attain the goals. Otherwise, this enormous investment, which has never been done so far, may be wasted.\nAn evaluation report of FATIH project was done by MEB (2012). In this context, the FATIH project which is being conducted in pilot schools was evaluated by four universities. There are several findings in this report. For instance, in terms of teachers; it was observed that teachers were still not able to benefit from technologies even though they had these skills. Teachers also stated that they did not make use of in-service teacher training programs efficiently and subsequently their workloads increased due to not having enough computer literacy skills. Students also stated that the workload of teachers have increased and teachers have lack of computer literacy skills. In terms of using technology in educational settings, teachers stated that intense interaction with tablet PCs caused to lose interest toward lessons for students and classroom management process has been difficult because of the tablet PCs. Similarly, students stated that they were in difficulty while doing their homework and taking notes during the classes because of tablet PCs. On the other hand, it could be seen on the report that smart boards were not being used in an appropriate way to support learning and teaching activities.\nAll in all; it can be understood from the report that even though technological devices have been provided to the schools; teachers, students and stakeholders had in difficulty to use these technologies for educational activities. This confirms what Toci and Peck (1998) said in their study. They had stated that providing technologies to educational settings did not mean that they were going to be properly used as a part of learning and teaching process. That is; this project is also moving on problematic and not able to support educational activities. The reason behind that may be because of training programs. Due to inefficient training programs, teachers could not benefit from their opportunities to support educational activities. By the help of TPACK in training programs, teachers may benefit from their opportunities in a better way. At this point, it could be said that teachers have Pedagogical and Content Knowledge (PK and CK) considering their teaching experiences. However, TK could be somewhat missing. This is because; while TK level of teachers are improved, TPACK can foster those skills and also help teachers integrate technology for their educational activities.\nIt could be said that ICT integration process in Turkey is questionable. At the first project, NBEP, the problem may stem from the ineffective in-service training programs. The first reason behind the failure is the training programs being not able to improve the teacher’s TK level even though the main goal of training programs was to improve that skill. In a study, Kayaduman, Sirakaya and Seferoglu (2011) stated that teachers take training programs to improve their computer literacy. Within the scope of these training programs; basic operations about computer literacy are just taught to in-service teachers. It could be said that the training programs in scope of ICT project, generally focus on Technological Knowledge (TK) rather than TPACK. Considering that teachers already have PK and CK, TPACK level should have been focused for the success. At the second project, FATIH, it is also going problematic as it can be understood from the first report prepared by MEB (2012). Similar reasons can be considered as stated for NBEP why it was not successful. At this point, Uluyol (2013) asked very important questions to be answered about in-service teacher training programs for FATIH project. The question is how teachers are going to be trained from the theoretical and pedagogical aspects as well as improving computer literacy skills. Kayaduman and his colleagues (2011) put emphasize on in-service training programs for FATIH project, as well. They claimed that teachers should be trained via face to face and distance education courses. However, these training programs should be in a continuous way not only once. That is; teachers are the hearts of education and ICT integration. It could be understood from teachers’ perspectives and attitudes, whether an ICT project fulfills its aim or not. Therefore, training programs should be carefully designed in order to foster the integration process. Referring to the in-service teacher training programs, TPACK model can positively contribute to integrate the technology into educational settings. While teachers are in the training programs, they may learn not only about using technology but also how technology can be used as a tool for learning and teaching. In this way, teachers may see the benefit of technology and how they can support their educational activities by the help of TPACK rather than only learning how to use them. Hence; it could be said that TPACK model can be a good framework to design in-service teacher training programs. Last but not least, by designing in-service teacher training programs based on TPACK, teachers may realize that technology is an integral part of learning and teaching process. Accordingly, integrating technology into education can get easier.\nAbbitt, J. T. (2011). An Investigation of the Relationship between Self-Efficacy Beliefs about Technology Integration and Technological Pedagogical Content Knowledge (TPACK) among Preservice Teachers. Journal of Digital Learning in Teacher Education, 27(4), 134–143.\nAkbaba Altun, S. (2006). Complexity of Integrating Computer Technologies into Education in Turkey. Educational Technology & Society, 9(1), 176–187.\nAskar, P., & Usluel, Y. K. (2003). Bilgisayarlarin benimsenme hizina iliskin boylamsal bir çalisma: Üç okulun karsilastirilmasi. Hacettepe Üniversitesi Egitim Fakültesi Dergisi, 24, 15–25.\nCüre, F., & Özdener, N. (2008). Ögretmenlerin Bilgi ve Iletisim Teknolojileri (BIT) Uygulama Basarilari BIT’ e Yönelik Tutumlari. Hacettepe Üniversitesi Egitim Fakültesi Dergisi, 34, 41–51.\nDilworth, P., Donaldson, A., George, M., Knezek, D., Searson, M., Starkweather, K., Strutchens, M., et al. (2012). Editorial: Preparing Teachers for Tomorrow’s Technologies. Contemporary Issues in Technology and Teacher Education, 12(1), 10–13.\nGodfrey, C. (2001). Computers in school: Changing technologies. Australian Educational Computing, 16(2), 14–17.\nJimoyiannis, A. (2010). Designing and implementing an integrated technological pedagogical science knowledge framework for science teachers’ professional development. Computers & Education, 55(3), 1259–1269. doi:10.1016/j.compedu.2010.05.022\nKayaduman, H., Sirakaya, M. & Seferoglu, S. S. (2011). Egitimde FATIH projesinin ögretmenlerin yeterlik durumlari açisindan incelenmesi. XIII. Akademik Bilisim Konferansi (AB11), 2-4 Subat 2011, Inönü Üniversitesi, Malatya.\nMEB. (2007). BT Entegrasyonu Temel Arastirmasi. Retrieved May 28, 2013 from http://ocw.metu.edu.tr/pluginfile.php/3298/course/section/1180/BT%20Entegrasyonu.pdf\nMEB. (2012). Fatih Projesi Pilot Uygulama Degerlendirmesi.\nMEB. (2013). Fatih Project. Retrieved May 28, 2013 from http://fatihprojesi.meb.gov.tr/tr/english.php\nMishra, P., & Koehler, M. J. (2006). Technological Pedagogical Content Knowledge: A Framework Teacher Knowledge. Teachers College Record, 108(6), 1017–1054.\nSahin, I., Akturk, A. O., & Schmidt, D. (2009). Relationship of Preservice Teachers’ Technological Pedagogical Content Knowledge with their Vocational Self-Efficacy Beliefs. Proceedings of Society for Information Technology & Teacher Education International Conference (Vol. 27, pp. 4137–4144).\nShin, T. S., Koehler, M. J., Mishra, P., Schmidt, D. A., Baran, E., & Thompson, A. D. (2009). Changing Technological Pedagogical Content Knowledge through Course Experiences. Retrieved May 28, 2013 from http://punya.educ.msu.edu/publications/Shin_et_al_SITE2009.pdf\nToci, M., & Peck, K. L. (1998). A systems approach to improving technology use in education. Canadian Journal of Educational Communication, 27(1), 19–30.\nUluyol, Ç. (2013). ICT integration in Turkish schools: Recall where you are coming from to recognize where you are going to. British Journal of Educational Technology, 44(1), 10–13.\nYildirim, S. (2007). Current utilization of ICT in Turkish basic education Schools: A review of Teacher’s ICT use and barriers to integration. International Journal of Instructional Media, 34(2), 171–186.""]"	['<urn:uuid:86dcc2dc-07a5-4a17-9571-eaf4867a4a36>', '<urn:uuid:1b28756d-6718-41a5-a8e6-73bbbb8081f4>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T03:34:55.284799	20	98	5818
86	help with fish tank heater size versus watts needed thinking of getting into marine fish tank setup	For a marine aquarium, you'll need about 3 watts per gallon for adequate heating to maintain a constant 75 degrees Fahrenheit (24 degrees Celsius). However, for more precise sizing, you should consider getting approximately 5 watts per gallon as a general rule of thumb. For example, a 5-10 gallon tank would need a 50-watt heater, while a 15-20 gallon tank requires a 100-watt heater. The exact wattage needed also depends on factors like room temperature and the tank's heat retention capabilities. For accurate temperature maintenance, you should also invest in an aquarium thermometer.	"['Basic Saltwater Aquarium Equipment\nChoosing the right filter and lighting depends on what you will keep in your saltwater aquarium.\nThis is not going to be one of those ""Choose a sturdy support for the tank, water is heavy..."" basic equipment articles. I shall take the liberty of assuming that you have some knowledge of aquariums, although you may never have kept one before.\nTo successfully maintain marine organisms in the home for any reasonable length of time, you will need an aquarium of some kind. This should be a rectangular tank of any size larger than, say, 10 gallons (38 liters), although the bigger the better. It will need a sturdy support near electricity, and a source of water should be convenient.\nThe tank should be made entirely of glass or acrylic. You will need a cover for the tank, and a heater of sufficient wattage to keep the aquarium at a constant 75 degrees Fahrenheit (24 degrees Celsius). About 3 watts per gallon is usually satisfactory.\nYou will need an assortment of accessories for maintenance chores: a couple of 5-gallon (20-liter) buckets, a larger plastic container of about 30 gallons (115 liters), some flexible hose, nets...in short, the same sort of stuff you need for a freshwater tank. You will also need both a filtration system and a lighting system, the selection of which you should give considerable thought to.\nWhen you think about equipment for a new marine aquarium, think about nutrients. That\'s right, nutrients — of which there are not very many in the waters around coral reefs. The novice saltwater enthusiast should always remember that the life forms destined to inhabit his or her marine aquarium come from the waters around coral reefs. For reasons explained below, most of what you will be doing when caring for your marine aquarium will be involved with removing nutrients from the water in the tank.\nSo let\'s think about nutrients for a moment. Nutrients are, of course, essential for the survival of most living organisms. However, one of the special features of the coral reef environment is the paucity of nutrients in the water. Reef organisms have developed marvelously efficient ways of capturing and, in many cases, recycling most of the available nutrients. The result is that the bulk of the nitrogen, phosphorus and organic carbon present in the reef habitat is found in the biomass, not dissolved in the water.\nIn the aquarium, which is a closed system and an ecologically incomplete environment even under the best of circumstances, excess nutrients begin to accumulate from the moment living organisms are added. This accumulation results in a decline in the water quality of the system. If nothing is done to reverse this process, water conditions will soon deteriorate to a point outside the range of tolerance of the life forms. They, in turn, will fare poorly.\nFiltration is therefore necessary to prevent, or at least retard, this gradual worsening of water conditions. Filtration, properly chosen and combined with water changes and the judicious application of tank additives, enables the aquarist to maintain aquarium water in good condition almost indefinitely.\nSunlight plays an important role in the nutrient recycling process that occurs naturally on coral reefs. This fact has important implications in the selection of aquarium lighting.\nChoosing suitable filtration and lighting is only one aspect of setting up a successful saltwater aquarium, but it is a very important aspect, for two reasons. First, a significant portion of the total cost of the aquarium will be spent for the filtration and lighting systems. Second, there is a trade-off between the initial investment in equipment and the amount of effort you will expend later in keeping the tank in the appropriate condition. Such effort may not only involve maintenance chores but, alas, may also be associated with medicating sick fish, replacing filter media and light bulbs, coping with prolific algae growth and repairing worn or faulty equipment. The time to be choosy and critical is now, when you are first setting up the tank, not six months from now when that bargain-priced pump fails while you are away for the weekend.\nThere is such a bewildering array of options available for filtering a marine tank. Which one is best? The correct answer, of course, is that no one filtration system is suitable for all applications, and that the choice for your particular aquarium should be determined by what you intend to keep in the tank. There are probably as many filtration theories as there are aquarists, but for simplicity\'s sake, I will discuss only three approaches to marine tank filtration. I shall refer to these as ""high-tech,"" ""natural"" and ""traditional"" setups.\nThoroughly dedicated marine hobbyists, by the way, can actually forgo filtration altogether, provided they are willing to carry out frequent, large water changes. The ultimate purpose of any filtration system is to extend the useful life of the water in the tank. And no filtration system, no matter how sophisticated, can eliminate the need for partial water changes. All filtration methods are applied with the intent of preventing changes in the chemistry of the aquarium water that would render it unsuitable (i.e., stress-producing) for the inhabitants of the tank.\nHigh-tech filtration systems seek to automate, insofar as possible, maintenance of the appropriate water quality. The goal is stability of the water chemistry with minimal labor for the aquarist. The redox controller (to be discussed later in this series) receives my vote for the centerpiece of such a system, although only a couple of years ago any tank fitted with a wet-dry filter was high-tech. If you are comfortable with or like to use computers, fax machines and so on, you may be a candidate for a high-tech aquarium system. To do it right, you will need a roomy budget.\nNatural filtration systems rely primarily on the good judgement of the aquarist, a protein skimmer and an ample quantity of live rock. This is my personal choice, but not one I can, in good conscience, recommend to the novice. Developing good judgement about marine aquarium husbandry requires some hands-on experience. For the beginner, it is wiser to employ some equipment that will provide a margin for errors in judgement.\nTraditional systems are, in my view, defined by the use of the undergravel filter. This somewhat less-than-satisfactory device has the decided advantage of being both cheap and almost foolproof. Its primary disadvantage is that proper cleaning is difficult to accomplish.\nIn the last few years, wet/dry filtration systems have received much attention, largely with regard to their application in maintaining so-called reef tanks. Any marine aquarium, however, whether reef or fish-only, will fare better and be easier to maintain if a wet-dry filter is installed instead of using an undergravel filter. Because traditionalists will be horrified at this allegation, and the novice aquarist may be horrified at the cost of a wet/dry filter, let me explain why I believe there is a decided advantage to this method of filtration over the more commonplace, and certainly cheaper, undergravel filter.\nAt minimum, we require a filter to process toxic ammonia into less toxic nitrate via the process known as biological filtration. If you do not clearly understand what is meant by biological filtration as it applies to aquarium maintenance, stop right here and do some reading on the subject. Complete descriptions of this process, which is achieved by beneficial bacteria, are found in virtually any decent book on aquariums, whether freshwater or saltwater. Excellent articles that deal with biological filtration in detail regularly appear in this and other magazines.\nIn a nutshell, your saltwater aquarium filtration system must be able to detoxify the ammonia (a form of nitrogen — a nutrient) that will be produced by the inhabitants of the tank, or they will die rather promptly. Both undergravel filters and wet/dry filters easily accomplish biological filtration.\nWet/dry filters, however, excel in two ways. They trap detritus in such a manner that it can be removed with much less effort than is required for an undergravel filter, and they are far easier to maintain, with little physical disruption of the tank.\nA wet/dry filter is essentially a watertight box (sump) with a smaller box (biological chamber) sitting on top. Water flows from the aquarium by gravity, trickles through the biological chamber, collects in the sump and is pumped back into the aquarium. The biological chamber is filled with pieces of plastic that become colonized by the beneficial bacteria that detoxify ammonia. The sump often houses additional filter media, or other equipment, and thus serves as a convenient place to hide this extra hardware underneath the tank, out of sight.\nThe trickling of water through the biological chamber permits maximum contact with the air, allowing essential oxygen to be dissolved in the water and harmful carbon dioxide to escape. Detritus, which consists mostly of dead bacteria, is trapped in a simple sheet of polyester fiber pad placed at the top of the biological chamber through which water from the tank must flow. Good filters have a nifty drawer in this position in order to facilitate periodic cleaning of this pad. Additional detritus accumulates in the sump, beneath the biological chamber, from which it may be siphoned with minimal disturbance to the tank itself. Maintenance chores are therefore more easily accomplished and, consequently, more regularly performed.\nWith an undergravel filter, in contrast, all of this stuff — filter media (substrate), powerheads, airstones, wires, hoses and so on — is in the tank. This not only spoils the appearance of the tank, but makes proper cleaning almost impossible without severe imposition upon the peace of mind of the tank\'s inhabitants.\nI have mentioned detritus several times. Why is detritus such a problem? It acts as a storage depot for nutrients, and, as I mentioned above, most of what you want a filter to do is eliminate excess nutrients from the aquarium. Phosphorus compounds, of which detritus is a rich storehouse, will promote thick, obscuring growths of algae if allowed to accumulate. Organic carbon compounds, dissolved in the water but also found abundantly in detritus, will provide food for undesirable and possibly disease-producing bacteria. The water in the tank will be analogous in purity to the air over the Los Angeles freeways and your fish will not thrive.\nIf you cannot justify the purchase of a wet/dry filter and must settle for an undergravel system, bear in mind the limitations of the latter. Resolve to spend more time maintaining the tank and resist the temptation to stock a large number of fish. I will present some important facts about choosing a wet/dry filter system in the next installment of this article.\nLighting plays a key role in the marine aquarium, but is a critical consideration only if you plan to keep invertebrates, such as anemones or corals. An aquarium that will not be home to such organisms should nevertheless be brightly illuminated. Reef fish are accustomed to high light levels in their natural habitat, and bright light will promote the growth of beneficial green algae. As you will come to understand, algae can be both a bane and a blessing in a marine aquarium.\nIf you plan on keeping anemones, which require bright light, you should consider fixtures that permit you to place several fluorescent lamps over the tank, or perhaps a metal halide system. I will discuss both types of lighting in a subsequent installment of this series.\nMost beginners opt for a tank containing fish and perhaps a few invertebrates, such as starfish or shrimps, that have no special light requirements. For these organisms, the standard fluorescent strip light that comes with most aquarium tanks is satisfactory. Such fixtures will accommodate a single fluorescent tube. Select a hood for the tank that will accommodate a tube of the maximum length that will fit across the top of the tank. The longer the tube, the brighter it will be.\nThe particular type of fluorescent tube is important, since all tubes are not created equal in terms of their light output or the aesthetic appeal of the color of the illumination. Some of the best are the Ultralume 5000K and Advantage X lamps manufactured by the Philips corporation. Some of the worst are cool white (the lamp usually supplied in hardware store fluorescent fixtures) and the various brands of lamps typified by the Sylvania Gro-Lux, which are designed for growing and displaying terrestrial plants. Gro-Lux type lamps are marketed widely under a variety of names for aquarium lighting. In my view, they are less than satisfactory.', 'Heaters in a nano aquarium is very important, more so than your average sized fish tank. Since nano aquariums hold a smaller volume of water compared to standard sized fish tanks, they do not retain heat very well. Even if the room temperature fall, larger aquariums will retain the heat for an extended period of time. It may take hours for the water temperature to start dropping. However, in a small nano tank, the rise and fall in room temperature will have a more immediate effect on the water temperature.\nAs ectothermic animals, fish rely on the environment to maintain their body temperature. If the temperature of the water falls out of their desirable range, they could die. Even if the water temperature is within a desirable range, if there is a rapid fluctuation in the temperature, it could stress the fish.\nSince nano aquariums are prone to temperature fluctuations, it is up to the fishkeeper to monitor and maintain the water temperature. Therefore, invest in a reliable aquarium heater for your nano aquarium. Don’t forget to get an aquarium thermometer as well.\nFor heater for larger aquariums refer to the Aquarium Heater Guide.\nWhat is Different about a Nano Tank Heater?\nA heater for a nano aquarium needs to be the right size.\nFirst, it must be the right size in terms of heating capacity, or wattage. The general rule of thumb is to get a heater with a capacity of 5 watts per gallon of water. If you have a small nano tank of 5-10 gallons, get a 50 watt heater. If you have a 15-20 gallon tank, get a 100 watt heater.\nSecond, consider the physical size and shape of the heater. If you have a nano tank, a standard heater may not fit. Find an aquarium heater that will fit. If possible, find a heater that is low profile. This will help maintain the aesthetics of your aquarium tank.\nDo I need a Heater for my Nano tank?\nSurprising as it may sound, not all nano tanks require a heater. Depending on where you live and what type of fish you wish to keep, you may or may not need an aquarium heater for your nano tank.\nFirst, if you live in a temperate climate, you may not need a heater. Also, if you intend to keep fish that prefer colder water temperatures such as goldfish and white cloud mountain minnows, you may not need a heater.\nHowever, if you intend to keep tropical fish, you will most likely need to invest in an aquarium heater. Many tropical fish prefer a water temperature of around 78 degrees Fahrenheit. Climates in most regions do not hold this temperature year round.\nMain types of Aquarium Heaters\nThere are three main types of nano aquarium heaters, which are submersible heaters, immersible heaters, and pad heaters.\nSubmersible heaters are the most popular option for nano aquarium tanks. They are often placed on the aquarium wall with suction cups. Since it is submersible, it can be installed vertically or horizontally.\nImmersible heaters are not fully submersible. The heating mechanism will be placed under the water level, but a portion of the heater must stay above the water. This means that you do not have to get your hands wet when you adjust the temperature level. However, you would have to make sure to turn the heater off during water changes because it will be exposed to dry air. If the heater is exposed to dry air, and it is left on, the heater may overheat and start to malfunction.\nPad heaters, or undergravel heaters, are common for small nano tanks under 5 gallons. They are designed to be installed on the very bottom of the aquarium. The temperature setting on these small pad heaters are often preset. This makes it very easy to install. Simply place it underneath the aquarium gravel, and plug it in. There is no need for calibration.\nWhat size of Aquarium Heater do I need?\nWhen choosing the right size for your aquarium heater, the general rule of thumb is to get approximately 5 watts per gallon of water. While this is only a general guideline, it can be useful when trying to estimate the wattage requirements.\nWhen sizing the heater size for your tank, various factors are taken into consideration.\n- Volume of water (size of tank)\n- Temperature of room\n- Amount of heat retained in the aquarium\n- Heat output variance depending on different heater manufacturer\nBelow is a chart which provides a starting point for choosing the correct heater size.\n|Tank Size||To heat by 5° C||To heat by 10° C||To heat by 15 °C|\n|5 Gallon||25 watts||50 watts||75 watts|\n|10 Gallon||50 watts||75 watts||75 watts|\n|20 Gallon||50 watts||75 watts||150 watts|\nBest Nano Aquarium Heaters\nThere’s a lot of different nano aquarium heaters on the market. Having many options is good, but it can make it difficult to choose the right heater. Here’s a few suggestions that we have for your nano aquarium, based on the size.\nBest Nano Aquarium heater for tanks of sizes 1 to 2.5 gallons\nIf you have a very small fish tank, then this may well be the heater for you. This is one of the smallest heater on the market with a temperature control.\nOut the box it is set for 78° F which is the ideal temperature for most tropical fish. Once the temperature in the tank exceeds this the heater automatically switches off and switches back on once the temperature has dropped below this mark.\nIt is a fully submersible design and has discreet lines, making unobtrusive in even the smallest of tanks.\nBest Nano Aquarium heater for tanks of sizes 2 to 5 gallons\nThe smallest adjustable heater on the market is the Hydor Theo. It is adjustable from 19 – 33° C with a simple twist of a dial. It is a fully submersible heater and is available in 25W and 50W, both of which are only 7 inches in length.\nIn recent tests conducted by a fish lab, it was found to maintain temperatures to within 1° F of its selected setting. A crucial benchmark for any nano tank heater.\nDue to its small size and the fact that it is fully submersible, this little heater can be placed almost anywhere in your tank.\nBest Nano Aquarium heater for tanks of size 6 to 10 gallons\nThe Cobal Neotherm is an awesome aquarium heater for nano tanks. However, this it comes in various sizes, and it is one of the best aquarium heaters for fish tanks of all sizes.\nMeasured accuracy has shown that it stays within 0.5° F of its set temperature, which is remarkably accurate.\nThis is another fully submersible aquarium heater, so the installation options can be very versatile.\nTips for Using Aquarium Heaters\nUnderstanding the heating requirements for your fish tank and choosing the right aquarium heater is important. However, it is also important to understand how to use it. Here’s a few tips on how to use your aquarium heater:\n- Always unplug your heater when changing the water. If the heater is left on while exposed to air, it can overheat and break.\n- Install the heater where there is adequate water flow. For example, placing it near the outflow of your aquarium filter. This will allow the heat to properly disperse throughout the aquarium. If the heater is place in a stagnant area, the water temperature may fluctuate throughout the aquarium.\n- Use a thermometer to keep an eye on the water temperature. Many heaters do have a temperature reading on the unit, but do not rely on this alone.']"	['<urn:uuid:a38ecf75-da2f-46c9-a07b-638cfea20658>', '<urn:uuid:4282ca4f-3665-4f3f-9a5e-40440bb7b0f9>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T03:34:55.284799	17	93	3382
87	urban stormwater systems computer technology applications	Urban stormwater systems increasingly rely on computer technology for effective management. Modern stormwater management employs computer modeling techniques and analytical models for urban drainage design. These technological applications help in designing collection systems, managing water flow, and implementing best management practices. This integration of computer techniques has become essential in both stormwater and flood management, allowing for more precise control and monitoring of systems, as evidenced by projects like Spirit of Brandtjen Farm, which uses central remote control and weather data scheduling for its irrigation system.	"['spirit of brandtjen farm\nLocation: Lakeville, Minnesota | Client: Tradition Development | Design team: Water in Motion, Inc. | Attributes: Stormwater Harvest & Reuse, Irrigation Design, Best Management Practices\nBack in 1932, Henry Brandtjen bought a plot of land located in Lakeville, Minnesota. An open wilderness for his family to roam and call home, the Brandtjen Farm soon became the largest in Minnesota. Even after all these years, this cozy front-porch lifestyle continues to this day. Now known as Spirit of Brandtjen Farm, this historic landscape is a master planned community.\nEmbodied by strong historic character and a rich community culture, Spirit of Brandtjen Farm has become one of the most popular master planned neighborhoods in the Twin Cities. Residents enjoy spacious green areas, numerous walking trails, and the original Brandtjen Family’s barn which is used as the community center. Surrounded by an outdoor pool, basketball and volleyball courts, and countless other recreational amenities, the spirit of the Brandtjen Family lives on within these community neighborhoods.\nTradition Development partnered with Water in Motion to design what is perhaps the largest landscape irrigation system in Minnesota, and more importantly, one of the most successful stormwater harvest & reuse projects in the Upper Midwest to date. With a total of 2,500 acres of development, the site is a work in progress - more specifically, this project is currently on year 10 (2017) out of a predicted 15.\nIrrigation as a Stormwater Management Tool (BMP)\nWater in Motion is passionate and experienced when it comes to conserving water. Becoming more and more popular, stormwater harvest and reuse systems are a very water efficient way for horticultural landscapes to shrink their eco-foot print and save money. Spirit of Brandtjen Farm is a prime example of just that. Take a look for yourself!\nAt the heart of this design is a large trunk pipe running throughout the entire landscape. Stormwater is drawn from multiple interconnected basins, or ponds, that flow through the trunk. Along the trunk are many points of connections where the stormwater continues its journey, quickly reaching the sprinklers.\nStormwater used for irrigation purposes flow through three main trunk pipelines known as South Trunk, Northeast Trunk, and Northwest Trunk. These three pipes are held in a centralized pump station where 2,100 gallons of water are delivered each minute. Additionally, the entire irrigation system is managed via central remote control and scheduled with both weather and soil-moisture data.\nTo ensure this development is as water efficient as possible, best water management practices are used within the horticultural irrigation.\nAt the current state, stormwater is responsible for irrigating 81 acres out of the total 2,500. This covers the development’s common areas, a few local retail landscapes, and even Parkview Elementary School. However, when fully developed, Spirit of Brandtjen Farm will receive stormwater from up to 194.5 acres-dramatically increasing stormwater irrigation coverage for this community.\nFrom a horticultural perspective, this sustainable system has the potential to harvest and distribute approximately 25,000,000 gallons of stormwater upon the site during the irrigation season.', '1 edition of EMERGING computer techniques in stormwater and flood management found in the catalog.\nEMERGING computer techniques in stormwater and flood management\n|Statement||edited by William James.|\n|The Physical Object|\n|Number of Pages||381|\nUrban Stormwater Management Stormwater management as an area of engineering practice is readily understood and defined. However, the term must be approached with caution since the practice of stormwater management has evolved rapidly over the las t ten years, and will no doubt conti nue to do so for the foreseeable future. As recently as. Designed for both students and practicing professionals, it addresses critical issues of water quality, focusing on the illustration and application of both hydrologic and economic water management techniques. Stresses applications using worked examples, case studies and problems. Software is to assist in solving more complex problems and to apply demonstrated techniques.5/5(2).\nThe new stormwater management techniques integrate stormwater collection, treatment, storage and reuse. Integrated management could be implemented on the roads, parking lots, depressed grassland. Concerns with Conventional Stormwater Management Conventional stormwater management has focused on removing stormwater from a site as quickly as possible to reduce on-site flooding. This has meant implementing management techniques, such as curb and gutter and piping systems, that discharge runoff to the nearest receiving water, or.\nMore recently, stormwater management guidelines emphasize the need to adopt a more holistic approach to stormwater management, which includes management responses such as stormwater quality improvement, stormwater reuse, and integration with landscape (ARMCANZ and ANZECC, ).Flooding, however, still remains the most important consideration in the management of urban Cited by: 2. 2 REQUIREMENTS AND GUIDELINES FOR FLOOD MANAGEMENT INTRODUCTION There are numerous documents related to the effective management of stormwater runoff and associated flood risk and environmental effects. The key driver in relation to the impact of urban developments on the wider environment is driven by the RMA.\nState of Rhode-Island and Providence Plantations. In General Assembly, August session, 1781.\nThe state of United States military forces: Hearing before the Committee on Armed Services, House of Representatives, One Hundred Sixth Congress, first session\nSystematized nomenclature of medicine\nThe spirit of the place\nBureau of Mines Research 1972\nMain economic indicators, 1955-1971\nLove and be silent.\nBeyond Silent Spring\nStudies in musicology\nGeorgia genealogical bibliography, 1968.\nBedtime for Frances [by] Russell Hoban : teacher guide\nTrends in capacity and utilization, Dec., 1972.\nCOMPENDIUM OF 2000 APPROPRIATIONS RELATING TO THE DEPARTMENT OF JUSTICE... U.S. DEPARTMENT OF JUSTICE... APRIL 2000\nGet this from a library. Emerging computer techniques in stormwater and flood management: proceedings of the conference. [William James; Engineering Foundation (U.S.); American Society of Civil Engineers. Urban Water Resources Research Council.; Canadian Society for Civil Engineering.;].\nurban flood mitigation and stormwater management Download urban flood mitigation and stormwater management or read online books in PDF, EPUB, Tuebl, and Mobi Format. Click Download or Read Online button to get urban flood mitigation and stormwater management book now.\nThis site is like a library, Use search box in the widget to get ebook that. Download Stormwater Management eBook in PDF, EPUB, Mobi. It includes technical details of the modern treatment of stormwater, the emerging issues of atmospheric deposition, run-on, and snow melt, the Epidemiologic Model, and field data on discharge concentrations of a variety of contaminants.\n(LID) designs, computer modelling techniques. Compliance with federal, state, and local stormwater programs revolves around the use of “Best Management Practices,” or BMPs, to manage stormwater. Given the ""built in"" water benefits of smart growth at the site, neighborhood and watershed levels, smart growth techniques and policies are emerging as BMPs to manage stormwater runoff over.\nI was a student of Dr. Wanielista, and went on to become a Civil Engineer specializing in water resources - specifically stormwater management. This book was invaluable to me when designing stormwater systems. I have never seen another book that even gets close. I highly recommend it. - 5/5(1). Answer: (Chapter 1) Stormwater management involves the control of “run off” from precipitation Why is stormwater management important.\nAnswer: (Chapter 1) Stormwater management is important to prevent physical damage to persons and property from flooding and to maintain the ecological integrity, quality and quantity of our water resources.\n10 Low-Impact Development Techniques To Improve Stormwater Management Posted by Gib Durden on PM Stormwater runoff can be a problem, especially in commercial areas or downtown areas where the removal of tree and ornamental plants, the addition of impervious surfaces and soil compaction impact the movement of water in an.\nAdams, B.J. and Bontje, J.B. () Microcomputer applications of analytical models for urban drainage design, in American Society of Civil Engineering (ed.), Emerging, Computer Techniques for Stormwater and Flood Management, American Society of Civil Engineering, New York, pp.\n– Google ScholarAuthor: J. Li, B. Adams. RE: Stormwater Management Book Suggestions beej67 (Civil/Environmental) 15 Dec 14 Stormwater pond design is a topic you will never find a good treatment of in any literature, because the requisite processes, procedures, and calculations vary dramatically by region, and are always in flux.\nThis comprehensive and topical book: Addresses the broad range of issues related to urban stormwater management with a specific focus on developing countries. Covers the main aspects of planning, design, operation and maintenance of urban drainage systems as well as socio-economic and institutional issues related to urban stormwater : J.\nParkinson. The comprehensive guide to understanding and implementing sustainable stormwater solutions. A practical introductory look at the methods and materials used in twenty-first century site design, Low Impact Development and Sustainable Stormwater Management familiarizes readers with new and emerging ideas and techniques for using rainfall runoff in a more ecologically friendly by: Using Smart Growth Techniques as Stormwater Best Management Practices 9 The elements related to stormwater ordi nances are likely to address the same aspects of project design as zoning codes, for exam ple, setbacks, street widths, landscaping and parking requirements.\nZoning administrators should be involved in the development ofFile Size: 1MB. stormwater management techniques that mimic natural hydrology into all aspects of development and community design, including streets and parking, homes and buildings, and parks, public spaces and landscaped areas.\nThe landscape areas used in low impact development create flood management and public health benefits, as well as “quality. Sign in to like videos, comment, and subscribe. Sign in. Watch later. World Water: Stormwater Management, focuses on current solutions that will help manage runoff and stormwater flows on municipal, industrial, and commercial lands.\nTopics will include low-impact development, green infrastructure, collection and conveyance, drainage systems, erosion and sediment control, water quality monitoring, treatment.\nWhile the City’s efforts to date have gone a long way to improve flood mitigation / stormwater management and to minimize the impacts of flooding, there is still room for improvement. This Flood Mitigation / Stormwater Management Plan is intended to be an umbrella type study, providing direction and identifying the actions.\nUnfortunately, this book can\'t be printed from the OpenBook. If you need to print pages from this book, we recommend downloading it as a PDF.\nVisit to get more information about this book, to buy it in print, or to download it as a free PDF. Stormwater Management 4 - 1 CHAPTER 4 INTRODUCTION Training Objectives The goal of this Chapter is to develop an understanding of: 1.\nthe gl obal objectives of watershed management from a historical and municipal perspective, 2. key environmental issues of importance in watershed management such as flood hazard, water quality impairment, erosion and baseflow depletion.\nThis publication aims to make information on innovative stormwater treatment technologies more available to New Hampshire’s urban planners, developers, and communities. Traditional runoff management techniques such as detention basins and infiltration swales may be preferable, but are not always practical for treating urban by: 1.\nFLOOD & STORMWATER MANAGEMENT UPDATE March 6, nafsma UPDATE 1 National Association of Flood & Stormwater Management Agencies P.O. Box Washington, D.C. T New EPA Administrator Scott Pruitt Sworn In – Quick Action To Review WOTUS Follows. RE: Stormwater Drainage System Design - Book Recommendation cvg (Civil/Environmental) 6 Aug 03 remembering that most of your designs will be submitted to and must be approved by public agencies, obtain the policies and standards from the .Adams, B.J.\nand J.B. Bontje. Microcomputer Applications of Analytical Models for Urban Stormwater Management. Proceedings of the Conference on Emerging Computer Techniques in Stormwater and Flood Management, Niagra-On-The-Lake, Ontario.\nAmerican Society of Civil Engineers, New York. –; November. Google ScholarCited by: SWM Planning and Design Manual - - References REFERENCES Adams, B. J., and Bontje, J. B., Microcomputer Applications of Analytical Models for Urban Drainage Design. Proceedings of the Conference on Emerging Computer Techniques for Stormwater File Size: 33KB.']"	['<urn:uuid:acde5de0-5790-4211-acf4-39e800156411>', '<urn:uuid:6577f7bf-cc79-4c2f-82b1-e16657c0c82a>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T03:34:55.284799	6	86	1944
88	minimum documents required for it consultant hipaa	IT consultants need a small binder containing documentation of HIPAA training, a statement describing when and where they might access patient health information and how they handle it, and copies of Business Associate Agreements signed with clients. Everything must be dated.	"['Documentation is the most important piece of HIPAA compliance for you and for your clients. The rules are very clear: If you do everything right and don\'t document it, then you are out of compliance. Luckily, the requirements for small covered entities (doctors, etc.) and business associates (you) are not overly burdensome. See the documents at the Health and Human Services web site, especially the PDF ""Security Standards: Implementation for the Small Provider.""\nStart With Yourself\nBefore you go sign Business Associate Agreements (see the first two posts on HIPAA), you need to make sure you are compliant. Assuming you are a pretty normal I.T. consulting company, you don\'t handle individually identifiable patient health information (patient records) in your office or one your computers, tablets, USB keys, or phones. In other words, there\'s nothing in YOUR office or possession that is protected information. So documenting your compliance is pretty straight forward.\nYou need to document how you handle protected information (in paper or electronic format) when you are providing service to the covered entity. Again, in most cases, you ""handle"" this information only when you are working on client computers or moving data. But you almost never see any actual patient information (hence the term ""individually identifiable"" patient health information).\nSo you need a tiny little binder that you can keep in your office. In that binder you need some kind of documentation that shows you have been trained on HIPAA. This could even be self-training by reading official government web sites. But you need to document it.\nNext you need a statement about when and where you might have access to individually identifiable patient health information and how you handle such information. Again, this can be a paragraph or two typed up, because you just don\'t have much exposure.\nFinally, you need copies of your Business Associate Agreements that you\'ve signed with all clients.\nEverything needs to be dated.\nThat\'s it for you. Basically, you need to be able to hand that binder to someone who wants proof of your HIPAA compliance.\nCovered Entities are a little more complicated because they obviously do have individually identifiable patient health information and access it every day. Remember the three components: training, compliance, and documentation. You\'ll need a slightly larger binder for your clients.\nFirst, you should have a section where you document each employee\'s training. Whether this was provided by you or someone else. If you hold a company-wide training, you simply need to describe that in a paragraph and list the employees who attended. For on-going training after that, the client will need to make sure this section is kept up to date.\nSecond, you need two sections on compliance. The first is a section on technical compliance (related to the HITECH Act - Health Information Technology for Economic and Clinical Health Act). The second is for client procedures about how their office operates. Luckily, you (probably) only have to be involved in the first.\nHITECH compliance consists of describing how data are handled, encrypted, etc. and how breaches are handled. For small offices this is not complicated. Electronic medical records (EMR) are going to be inside whatever software the client is using to manage their office. You need to describe how this information is stored, managed, and moved. By ""describe"" I also mean describing what you\'ve put in place to make sure the client is complying with the HITECH Act.\nThird, the binder needs a section on documentation. The binder itself is documentation, of course. But you need more. You need to put all HIPAA related policies, procedures, and documents here. This includes physical descriptions of how data are handled. It also includes copies of all signed Business Associate Agreements.\nFinally, you should have procedures in place to make sure that employee training is maintained, data handling procedures are followed, and documentation stays up to date. Personnel changes are a key piece of this. When someone new is hired, they need to be trained in company procedures and HIPAA generally. And that needs to be documented in the binder.\nIt\'s Just Another Day . . .\nMany techs I\'ve talked to are worried about HIPAA and concerned that they won\'t be able to take on the challenge of helping clients with HIPAA compliance. But don\'t panic. This is just another in the woods.\nThe I.T. business is always changing. And it changes fast. If you\'ve been in business five years, you\'ve seen major changes already. And if you\'ve been in business ten or fifteen or more years, the changes huge. We all learned the skills that got us where we are. So now we need to learn some new skills.\nThe biggest challenge seems to be getting doctors and other covered entities to follow the law. I\'d love to hear any strategies you have for that!\nEverything else is just an opportunity to expand your services and make more money.\n- - - - -\nPinterest Resource for You\nI\'ve created a Pinterest board where I\'m posting information on HIPAA compliance and fines. It\'s a place you can point your medical clients to if they tell you that there\'s no enforcement and they have nothing to worry about.\nIt\'s at: http://pinterest.com/karlpalachuk/hipaa-news/\n- - - - -\nAbout this Series\nSOP Friday - or Standard Operating System Friday - is a series dedicated to helping small computer consulting firms develop the right processes and procedures to create a successful and profitable consulting business.\nFind out more about the series, and view the complete ""table of contents"" for SOP Friday at SmallBizThoughts.com.\n- - - - -\nNext week\'s topic: Trip Charges\n|Check Out the #1 ranked Managed Services book at Amazon:|\nManaged Services in A Month\nby Karl W. Palachuk\n2nd Edition - Newly Revised and Updated with NINE new chapters\nOnly $24.95. The best deal in managed services today!']"	['<urn:uuid:e1b1361b-c00f-441a-9f2b-8c0c42059c70>']	factoid	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-13T03:34:55.284799	7	41	980
89	When should I use Illustrator instead of Photoshop?	Use Illustrator when creating logos, single-page graphic designs, and artwork that needs to be scaled to different sizes without losing quality. Illustrator is vector-based, making it perfect for creating clean-cut shapes and designs that might need to be resized from business card size to building size while maintaining clarity.	['This is a transcript from the video mentioned above: (please support the original creator by liking and subscribing to their channel!)\nToday. I’ll show you when to use Adobe Photoshop, Illustrator or Indesign [Music] you [Music] now most people are pretty familiar with Photoshop and what it can do people that use illustrator, however, a far rarer even rarer. So are those that use. Indesign in this video. I’m going to go through an overview of each one, so you know which one to use for which application let’s begin. Alrighty, let’s get started and in the most common program out of the three and that is. Photoshop. I’ve pasted it in a standard Chuck Norris, picture off the Internet. I’m gonna show you how the image is made up. Photoshop is what we call raster based, which is a fancy word for saying that it’s based on pixels. If you weren’t aware zooming in will show you exactly what this means. Pixel images are made up of a grid of different squares in each one of those has a color as you to mount this up to blend together and that’s how the image is created. Photoshop is number one for image editing Photoshop. It is perfect for taking the image off your phone, your camera or even the internet and touching it up in this example here. I’m going to play with the black and white balance By changing the curve. One of the real strengths of Photoshop is using the layer’s. Cassini arrow have David Hasselhoff and Chuck Norris on two separate layers So I can move them around independently of each other By using the transform tool, it’s very easy to resize, move, skew and add perspective, different objects. This is what makes. Photoshop so popular for those funny image edits where people swap the head from one thing on to another before we had Instagram filters. We have Photoshop filters, and there are heaps of them here. There’s a huge range of effects that you can achieve with these filters. Here’s an example of one this is liquefying liquify enables you to more pan image around using very little skill. The presets are very powerful and not much time having never done it before you can get something. That looks pretty crazy. [MUSIC] Another really prominent use for Photoshop is retouching fashion images whether need it or not, so in this example. I’m going to use the clone tool. The basically I can hold down the Alt key and set my target, and then when I come back and paint, I will transfer that bit of texture into its place, so this is commonly used to get rid of imperfections and little marks and things like that. It adds to the effect that people described as being airbrushed. I’ve established that Photoshop is the number one program for editing images, especially when it comes to print and Web. So why don’t we need illustrator? Well, let’s revisit that concept of Photoshop being pixel based here. I’ve got a simple pattern where I’ve drawn a rectangle and a circle Photoshop stores this information by recording the color of every single pixel on the grid. When we zoom in, we’ll see the problem with this, we can see you hear that Our circle is beginning to become pixelated the way to get around. This is to draw all of our shapes in an enormous resolution, so that way we have to zoom in for some time before it starts to pixelate. This, however, makes really big file sizes, which isn’t quite ideal if we switch to illustrator and have the same type of pattern and we can see the difference between raster and vector graphics. Every time I zoom in here, It simply redraws it and you can see that we never lose quality. If I hover the mouse over, I can see that the vector is highlighted, whereas Photoshop stores, the final image in a series of pixels on a grid illustrator stores, the instructions to create the image. In this example, it will keep instructions to know that there’s a rectangle here and a circle here, as well as the proportion and relative positioning of each every time we zoom in or out, it simply follows those instructions to redraw it on the screen and create it perfectly for this reason. Illustrator is perfect for doing things like logos. If we visit our teaching tackle logo here and zoom in, we can see that all of the little shapes that make it up are made out of vectors. The advantage here is that I can blow this up to the size of the building and it will never lose quality. I can also shrink it down really small for something like a business garden, and it’ll look great there too. I’ve pasted it in a picture of the Hopf to show that illustrator, in fact, can handle Raster graphics. It doesn’t automatically convert them to vectors, however, so if you zoom in that still will become pixelated, the illustrator is particularly good for quickly moving and placing different elements to create things like posters. It’s important to note that illustrator does have a range of grass to editing capabilities, but it’s not really anything compared to Photoshop, In fact, it calls it the Photoshop effects, but it’s still vastly cut down from what you’ll find in Photoshop, despite being vectorbase that fills an illustrator are quite powerful. Here we have two examples of different fills. The one on the left is a straight gradient. You can bring up your gradient panel. Select your object and then change your range of parameters to do with the gradient. You can also change from linear to radial and include extra colors. If you wish this example on the right isn’t as well-known. This one uses the mesh tool. You can see when I hover over. It’s divided the shape up into a bunch of segments. I come up with the white cursor tool and click on one of the segments. I can set the color for that segment and it will try and blend it into the other ones around it, here’s. An example of a poster made as an instruction sheet using illustrator, all of the individual shapes are vectors and gradients of it applied to make different parts Stand out in this program. It’s extremely easy to move things around and to place them. Here’s another example of some more advanced editing in Illustrator zooming in might make you think this is something that’s been done in Photoshop, but hovering the mouse over reveals that everything is in fact vectors. This section on the front. Here we can see is a gradient mesh, and we’ve also used some of the filters from the drop-down menus to get the drop shadow and the texture because this is an illustrator. Every time we zoom in or out its redrawn and it should never pixelate. Of course, illustrator is vector based. It’s also good for making files for laser cutting. You can see here the pieces for my heart puzzle from my Valentine’s Day 3d printing special. Each one of them is made up of vectors, and this is a type of path that the laser cutter needs to follow to be able to cut something out. We’ve established that Photoshop is great for single images. Illustrator is good for logos and single page layouts and that brings us to Adobe Indesign. Indesign is what you switch to when you’ve got to create a lot of something here. I’ve set up an example page to show you some of the ways that InDesign can work here. Some features that might be of interest. We come to either of the rulers and click and drag across. When you let go. It’ll leave a guideline now. When you’re moving things around, you can easily snap to that guideline. The text boxes are a little bit different to other programs. You can see that we can resize them and there were some text hidden underneath you’ll notice. There’s a red plus here, which is telling us there’s text. That’s not fitting well. We hit the plus it’ll. Prompt us to drag another text and then the text will continue in that. If I resize the original one, The text will flow between them as it needs to now. Images and InDesign are a little bit different as well. Well, we click on them. We have two functions. General resizing will resize the box and effectively crop the image. If you double click now, you can resize the actual image, but that doesn’t resize the box if you click on an image and hold down. Shift + Ctrl. Now you’re resizing the bounding box as well as the image in the same go. If we examine the components of our design, we can see the Indesign supports both raster and vector graphic zooming in on my Chuck Norris image shows me that it is, in fact still pixels. If I come over to my logo and zoom right in, you can see that the vectors that make it up are still traced around the outside. Despite this press being a vector from illustrator, you notice that the quality doesn’t look that great? That’s because InDesign where it can keeps a reference to the original file and then displays a preview quality version. Only that means if I update this design and illustrator, it will instantly update in InDesign saving me a lot of time. If I come up to view over print preview, it’ll transform to the highest quality possible– to show you how it’s actually going to print like. We touched on earlier. The real beauty of Indesign is when you need to do a lot of something because of its templating system, you’ll notice under my page’s panel that I have my master up the top, and then my actual page is down below. Let’s make a copy of my logo and set it up as a watermark, Then a double click on a master, and it turns black up here and the pages of blank then. I’m gonna paste in my logo. I’m gonna resize it and move it up to the top left hand corner. I’m gonna make a copy and put it in the top right hand corner. Now you will never actually place this without white space here, but as an example for now, it’ll do the job, but double click to come back to our page. You’ll notice that it’s been applied because if this page is labeled with a which corresponds to the a master up the top as they’re adding new pages, you’ll see that each one automatically bears the image that we set up in our master. If I want to come back to my master at anytime and edit the way this is positioned when I come back to the page, it’ll instantly be updated there as well. We also have some really good templating for our paragraph text. I come to this text box here and then make sure I have paragraph. Styles open. I can add a new one from the button down below If I double, click on it. The control box will come up. What I’m going to do is rename this one with whatever I feel in this case. I’m going to call it heading one. Now when I’m editing other text all I need to do to apply this. Formatting here is to click on heading 1 in a complicated document. You’re going to end up with probably five or six different paragraph styles. This means you can write and edit your text in a program with spellcheck and then when you paste it in, it’s very quick to come back and click single time to set the type of style that you want just like with the master layout paragraph. Styles can be edited very quickly by double-clicking them and then changing whatever we want soon as they hit, okay. Everything in the document will come through an update. You can see this. One here has turned accidentally to the heading one so all. I need to do is put my cursor somewhere in the paragraph and then do a single click to revert it. Let’s do a little recap. Photoshop excellent for image editing that includes the images off your camera and as well as graphic designs and single pictures that you’re making for a variety of applications illustrator, excellent for logos and in some cases, single page graphic design layouts and finally InDesign, which is set for making long documents where you can use the templating to make sure everything stays consistent. Well, that wraps it up. I hope you found this overview on comparison. Very informative. Thanks for watching and Ill. See you next time, Gday. It’s Michael again. If you liked the video, then please click like if you want to see more content like this in future click. Subscribe and make sure you click on the bell to receive every notification. If you really want to support the channel and see exclusive content, become a patron, visit my patreon page. See you next time.', 'Vectors and Raster Images is what every designer or graphic artist regularly uses. This leaves us with a question, which among these techniques is better to use? and when do you use vector or raster in your designs? This is a tough question for a designer or a graphic artist. There were even a confusion of what is vector and raster. In this post I would explain each technique? with their examples and the advantages of using one from the other.So let the battle begin and decide for yourself!!\nWhat is Vector?\nThe word “vector” is a synonym for line. They are composed of mathematically-defined geometric shapes—lines, objects and fills. When creating a vector image in a vector illustration program, node or drawing points are inserted and lines and curves connect notes together. This is the same principle as “connect the dots”. Each node, line and curve is defined in the drawing by the graphics software by a mathematical description. They usually are easily modified within the creating application and generally are not affected detrimentally by scaling (enlarging or reducing their size). If the image is increased in size, the equation is recalculated accordingly resulting in the image increasing in size with no loss of data or detail. A vector object will have a “wireframe” underneath the colors in the object. In a vector object, colors are like clothes over the top of a skeleton. They’re defined as solid objects, and can be moved around in full, or grouped together with other objects.\nIt can be as simple like this..\na little complicated like this..\na photo-realistic and much more complicated..\nCheck the details on this. You can see the outlines and how complicated it was done..\nPrograms or Software\nVector Graphics is most commonly done in programs such as Illustrator, Freehand, Corel Draw, Flash, Inkscape, Fireworks, or other “vector” illustration programs. Vectors in these programs (at the hands of skilled artists and draftsmen) can achieve a nearly photo-like quality or be beautifully abstract.\nVector images are defined by math, not pixels. They can be scaled up or down without any loss of quality. When an illustration (drawing) program sizes a vector image up or down, it simply multiplies the mathematical description of the object by a scaling factor. For example a 1″ square object would need to be multiplied by a factor of 2 in order to double in size. The math is simply recalculated to produce an object twice the size of the original. Because vector images scale up or down without the loss of image quality, they can be output at any resolution that a printer is capable of producing.\nDWH Logo when zoomed in a Vector Program. The image is still whole and without the loss of quality.\nSince vector images are composed of objects not pixels, you can change the color of individual objects without worrying about individual pixels. Coloring vector objects is similar to coloring with crayons in a coloring book. A drawing program will enable a user to click inside an object and define its color. A drawing program will also enable a user to define the color and width of lines. Coloring vector images is much easier than coloring bitmaps.\nVector images do not need to keep track of each individual pixel in an image, only mathematical descriptions. For this reason vector files are very small in file size. Vector files are composed of long mathematical descriptions dictating every aspect of the graphic. A 2-inch by 4-inch vector based logo will be the same files size as a 2-foot by 4-foot logo. The file size is the same because the only difference in file is one number defining the size of the file. A raster image file would need to keep track of a whole bunch of additional pixels as the file increases in size. Most vector-based logos are going to be under 100k (.10 megabytes). For this reason, vector files are ideally suited for transfer over the Internet.\nCommon vector formats include EPS (Encapsulated PostScript), WMF (Windows Metafile), AI (Adobe Illustrator), CDR (CorelDraw), DXF (AutoCAD), SVG (Scalable Vector Graphics) and PLT (Hewlett Packard Graphics Language Plot File)\nWhat is Raster?\nA raster image is a collection of dots called pixels. Each pixel is a tiny square with assigned color value. They are created using a grid of pixels to define the image. When you attempt to increase the size of an image created in a raster based program, the pixels defining the image can be increased in either number or size. Increasing the number of pixels or making the pixels bigger in an image results in the original data being spread over a larger area. Spreading the pixels over a larger area causes the image to begin to lose detail and clarity. When an image is scanned, the image is converted to a collection of pixels called a raster image. Scanned graphics and web graphics are the most common forms of raster images.\nIt can be a beautiful photograph..\na digital painting..\nor a web design..\nand many more..\nPrograms or Software\nRaster-based image editors, such as Photoshop, GIMP and other raster programs, revolve around editing pixels. When an image is rendered in a raster-based image editor, the image is composed of millions of pixels. At its core, a raster image editor works by manipulating each individual pixel. Most pixel-based image editors work using RGB color model, but some also allow the use of other color models such as CMYK color model.\nRaster graphics are resolution dependent. They cannot scale to an arbitrary resolution without loss of apparent quality. The resolution of a raster image or scanned image is expressed in terms of the dots per inch or dpi. A printer or scanner’s resolution is also measured in dots per inch. Typical desktop laser printers print at 300 - 600 dpi. Image setters are capable of printing over 2,500 dpi. Printers with higher dpi ratings are capable of producing smoother and cleaner output. The output quality of a printing device is dependent upon the resolution (dpi) of a bitmap or scan. A 300 dpi raster image will output at the same quality on a 300 dpi laser printer as on a 2,500 dpi image setter.\nTake a 300 dpi bitmap and increase the size in a graphics program, and presto - you have created a bad case of the “jaggies”. The only thing that happened is that the tiny pixel squares got bigger and created jaggy edges on your image. Decrease the size of your image and the squares get smaller. The image retains its original edge definition without producing “jaggies”. In other words, raster images do not scale up very well. The quality of an imprint produced from a raster image is dependant upon the resolution (dpi) of the raster image, the capabilities of the printing technology and whether or not the image has been scaled up.\nDWH Logo when zoomed in a Raster Program. The image is getting pixelated and jaggy edges are showing.\nWith any scanned color image, a large number of colors will be required to render a raster image reproduction of the original source artwork accurately. If scanned at 24-bin color depth (16 million colors), most human eyes could not tell the difference between the original image and the scanned raster image. Now if you scan the same image using the palette of 256 colors, it would be impossible to accurately reproduce the original colors because you have a smaller color palette to choose from. To get around this, scanners use a process called dithering to approximate colors that don’t occur in the current color palette.\nDithering produces a distinct dotted pattern that approximates the original color in the image. Dithering will deteriorate the quality of the scanned raster image. Now take all this complexity of colors and try to change colors, and you can see the biggest disadvantage of editing and manipulating raster images. In order to change colors in a raster image you must be able to isolate a specific color or range of colors and tell your software to change the color. This can be quite a challenge for even experienced Corel PhotoPAINT or Adobe PhotoShop users.\nIn order to accurately reproduce a raster image file, your graphics software must keep track of a large amount of information, including the exact location and color of each pixel in the collection of pixels. This results in huge file sizes for raster graphics. Higher resolutions (dpi) and greater color depths, produce bigger file sizes. A typical 2″ by 3″ 150 dpi black and white raster image logo will be less than 70k (.07 megabytes) in file size. The same file saved as a 300 dpi 24-bit (millions of colors) raster image logo might be 100 times larger (over 7 megabytes). When creating and scanning raster images, file size becomes a real issue, as big files tend to make your computer processor and hard drive work overtime. Transferring big files (over 1 megabyte) over the Internet requires a high speed Internet connection on both ends for timely uploads and downloads.\nCommon raster image formats include BMP (Windows Bitmap), PCX (Paintbrush), TIFF (Tag Interleave Format), JPEG (Joint Photographics Expert Group), GIF (Graphics Interchange Format) , PNG (Portable Network Graphic), PSD (Adobe PhotoShop) and CPT (Corel PhotoPAINT).\n- Saved file sizes are smaller.\n- Conversion from vector to raster is easy.\n- Resolution independent.\n- Vector files do not support photographic imagery well and often can be problematic for cross-platform exchange.\n- Raster files handle the subtleties of photographs very well as a general rule.\n- Raster can handle other effects very well and much easier.\n- Resolution Dependent.\n- Raster files can be very large if there is a large amount of detail and pixels in an image.\n- Raster conversion to vector is much more difficult.\nWhen to use Vector?\nWhen you are developing something for scale or sending to a printing company that demands it.\nUsually this boils down to business identity print work, logos, promotional posters, and major illustration artwork.\nWhen you want clean-cut and clearly defined shapes - vector is the only way to go.\nWhen to use Raster?\nWe use raster in photographs and images done in a raster program.\nWe use it when we put an effect on image, or editing and manipulating them.\nIt’s when we’re adding a defined texture and accent on our created designs.\nIf you want textures, fills and other effects, you use raster.\nYou decide. Which one’s better?\nWhich of the 2 do you prefer using? Do you have anymore pros and cons of both?\nLet me know of what you think. Vote on the poll and participate in the discussion by giving your comment.\n- - Printing Needs? We know great guys on Business Printing Services.']	['<urn:uuid:59500d12-b7d2-4748-8b39-246e6ce6cbfe>', '<urn:uuid:abe217ad-d477-45a2-be1c-96e658fcb5fb>']	factoid	direct	concise-and-natural	similar-to-document	three-doc	novice	2025-05-13T03:34:55.284799	8	49	4045
90	I'm analyzing power quality issues at a school and seeing some concerning harmonic levels - what typically causes third-order harmonics in educational settings?	In schools and other non-industrial installations, third-order harmonics are typically caused by personal computers, office equipment and electronic lighting.	['Taking the first steps towards green buildings\nIn his summer statement, Chancellor Rishi Sunak announced a billion-pound programme to help improve energy efficiency in schools, hospitals and other public buildings. Called ‘The Public Sector Decarbonisation Scheme’, the programme, which is part of UK’s efforts to reduce greenhouse gas emissions from the public sector by 50%, is expected to see substantial investments in energy efficiency and heating upgrades over the next year.\n‘The Public Sector Decarbonisation Scheme’ “will offer grants to public sector bodies, including schools and hospitals, to fund both energy efficiency and low carbon heat upgrades”. Schools and other educational institutions will be able to apply for grants from the government to make their buildings more energy-efficient under a new public sector decarbonisation scheme. These grants will include the £560 million in additional condition improvement funding for 2020-21 and £1 billion for 50 rebuilding schemes starting from September 2021.\nIt goes without saying that ‘The Public Sector Decarbonisation Scheme’ is laudable in its objectives, as the world as a whole is dealing with an unprecedented ecological crisis. However, perhaps the first thing facilities managers should invest in when looking to improve energy efficiency is an instrument that allows them to accurately measure and record energy usage. Having an initial benchmark – before embarking on any changes and upgrades – is essential for measuring improvement and progress through time.\nA portable energy logger (PEL) like the Chauvin Arnoux PEL103 is the ideal tool for accurately monitoring power consumption and much more, including harmonic levels, voltage imbalance and power factor. In addition, by using a PEL103 across entire buildings and recording the results over time, facilities managers will be able to gain detailed and revealing insights into what’s actually contributing to their energy bills.\nA PEL is an all-in-one instrument that measures a whole range of electrical parameters, such as voltage, frequency, current, real power, reactive power, harmonic levels and more. Crucially, the PEL doesn’t only measure these parameters, it also stores the results over a period of time that can range from a few minutes to months. This is essential, as some key issues, like equipment that is not needed during the shutdown but is still switched on and off automatically by a timer, can only be identified by looking at time-stamped energy usage records.\nA PEL103 can be installed quickly and easily in a distribution cabinet where it can monitor circuits for lighting, HVAC, display screens, computer systems and more. If it is to deliver its full range of benefits, the PEL should be set up to make recordings over time. A day is good, but a full week or even longer is likely to be even better, as interesting things often happen at weekends! For instance, according to a survey carried out by British Gas, up to 46% of the electrical energy used by SMEs was consumed outside normal business hours, so paying attention to those out-of-hours costs can yield big dividends.\nBuying a PEL can be a very profitable long-term investment and connecting to it remotely can give you valuable insights even if you can’t regularly visit the site. This is particularly useful during school holidays or during lockdown, when everyone is advised to work from home.\nThe PEL will give premises managers information about the power factor of the loads. Most electrical loads consume two “types” of power – active power and reactive power. The active power does useful things – light the lights, turn the motor and so on – while reactive power does nothing useful. But the catch is that you pay the same for active and reactive power! But where does power factor come in? It simply tells you how much reactive power your loads are consuming. If the power factor is 1.0, they consume no reactive power, but if it’s any lower – 0.9 or 0.8, say – then you’re paying for useless reactive power.\nThe good news is that it’s possible to ‘correct’ poor power factor, bringing it nearer to 1.0 and reducing the amount of reactive power you pay for. This correction usually takes the form of capacitors fitted near the main distribution board and you may already have them. But, over time, capacitors can lose their capacitance and, of course, the loads on your system may change. As a result, your power factor may be much worse than you think, and reactive power may be costing you a lot of money.\nIn case you’re wondering how monitoring energy efficiency actually works out in practice, Peter Halloway, Regional Sales Manager at Chauvin Arnoux describes how a recent project at a secondary school in Kent provided some eye opening findings. “We were working on an energy efficiency project at a typical secondary school and we logged the measurements over an eleven-day period,” remembers Peter. “The period included the half term holidays, a week of term time and a weekend. The logged results revealed some very interesting statistics.”\n“The total energy consumed in the period came to just over £2,000, which correlated well with the school’s annual electricity bill of around seventy thousand pounds. But even at the weekend, when there was no activity on the premises, there was still a load of around 30 A per phase. Also, there was a phase imbalance that was producing an excessive current flow in the neutral.”\nAnother important finding from the investigation was that harmonics were unexpectedly high, which is actually a common problem given the proliferation of non-linear loads in our fast-moving technological world. In this case, the data showed that the third- and fifth-order harmonics were dominant. Third-order harmonics in schools and other non-industrial installations are typically caused by personal computers, office equipment and electronic lighting, and in this installation the fifth-order harmonics were ultimately traced to the server UPS.\n“Having analysed the data we had recorded over the eleven-day period, we were able to recommend a solution that would balance the loads and explain how to reduce the harmonics by fitting filters,” Peter added. “An even simpler solution however would have been to educate staff to turn off lighting and equipment at the end of the day or even install systems to turn it off automatically.']	['<urn:uuid:56fe3180-c3b5-4e57-be96-2f91ba95006d>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T03:34:55.284799	23	19	1035
91	algorithms methods tools columbia university developed analyze regulatory networks cells	Columbia University has developed several experimentally validated algorithms: ARACNe for analyzing transcriptional interactions, MINDy for post-translational interactions in mammalian cells, MARINa for identifying master regulator genes that cause specific phenotypes, and IDEA for elucidating compound mechanism of action.	['Library of Integrated Network-Based Cellular Signatures (LINCS)\nData now available : To access computational and experimental results from Columbia University’s LINCS U01 centers, go to geWorkbench , the Department of Systems Biology’s desktop application for integrated genomics. Types of data that are now available include:\n- drug-drug synergy results from titration experiments\n- computed drug mechanism of action results\n- drug-drug similarity results obtained by calculating the distance between the mechanisms of action of two drugs\ngeWorkbench offers a number of visualizations, including titration curves, heat maps, and network displays of drug mechanisms of action.\nPlease note that the collection of data that is now available is preliminary and ongoing. The final collection of data is expected soon.\nThe Library of Integrated Network-based Cellular Signatures (LINCS) is a multi-institutional, collaborative project initiated by the National Institutes of Health whose goal is to identify and categorize molecular signatures that occur when cells are exposed to agents that perturb their normal function. Understanding how perturbations affect the behavior of cells is important because it can provide insights into the causes of disease and help researchers develop new therapeutic strategies.\nLINCS approaches this problem using tools from systems biology, chemical biology, computational biology, and other disciplines, including both high-throughput experimentation and sophisticated mathematical analysis. LINCS centers are using high-throughput screening to observe how various cell lines respond to molecules such as siRNAs and bioactive compounds, and are developing computational methods for analyzing and categorizing the signatures that result. The project is also developing an open access database and web-based tools that will foster a wide range of biological and biomedical research, enabling the broader scientific community to identify molecules within biological networks that hold promise as targets for new disease therapies.\nSystematically perturbing cells with pairs of agents will enable us to better understand the mechanisms of action of many drugs.\nThe Columbia University Department of Systems Biology was selected as a member of the LINCS consortium in 2011. We currently have two grants, the first focusing on technology development, and a second on the design of new computational tools. By using high-throughput experimentation to perturb cells with pairs of agents in a very systematic fashion, our goal is to generate and categorize the molecular signatures that result in a way that will enable us to better understand the mechanisms of action (MoA) of many drugs. Using a variety of computational algorithms we also aim to predict — without the time and expense of actually testing every possible combination in the laboratory — which combinations of small molecules are most likely to have synergistic effects in live cells.\nThe principal investigator for Columbia’s LINCS projects is Andrea Califano.\nIn addition to generating and analyzing molecular perturbation data, the Department of Systems Biology has also developed software that offers the scientific community access to the large repository of experimentally derived and computationally inferred drug synergy and mechanism of action data we are producing. Users can query the results of computations and experiments and probe for predicted and validated synergistic effects of drug pairs on a number of different cancer cell lines. The software also provides a graphical user interface to enable advanced visualization and interaction with the data. Additionally, we are developing an application programming interface (API) that will make LINCS data usable in third-party applications. The new software is included in geWorkbench , the bioinformatics platform of the National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet), also based at Columbia University.\nLINCS Project 1:\nProfiling Signatures of Synergistic Chemical Perturbations in Diverse Cellular Contexts\nIn the past, understanding complex drug interactions was largely a trial and error exercise. Researchers in the Department of Systems Biology are working to make combination therapy a rational discipline. By analyzing LINCS data and other publicly available datasets, we are generating molecular profile information that will improve scientists’ abilities to predict how drug combinations will interact and to identify combination therapies that may improve treatment for diseases such as cancer. This pilot study will:\n- generate a set of experimental assays — primarily using high-throughput, high-content methods — that can be used to characterize phenotypic changes in specific cell types\n- use a systems biology approach to generate a prioritizied list of likely synergistic drug combinations, based on existing expression profiles and IC50 profiles from LINCS and Connectivity Map (CMap) data\n- test drug combinations that our methods predict to be synergistic and compare experimental results to those of random drug combinations. This effort will establish objective benchmarks for estimating the performance of predictive algorithms.\n- characterize the synergistic behavior of drugs with orthogonal mechanism of action vs. drugs sharing a similar MoA. (Orthogonal mechanisms of action are MoAs that share little or no overlap.)\nWhile our main goal is to identify drug synergies that will improve the treatment of disease, this project will also enable us to generate several new functional interaction maps of cellular phenotypes (interactomes), including disease phenotypes such as breast carcinoma.\nAccessing drug synergy results\ngeWorkbench offers a simple interface for building custom queries based on the results of synergy experiments conducted at Columbia University. The annotations available for narrowing a query include the tissue type, cell line, drug names, assay type, and synergy measurement type. The results are presented in a tabular view, with visualization options including:\n- heat map display of drug vs. drug synergy score results\n- network view using Cytoscape to depict synergy strength between each pair of drugs\n- titration curves depicting the underlying data used to calculate synergy for individual drug pairs\nLINCS Project 2:\nA Systems Approach to Elucidate Mechanisms of Drug Activity and Sensitivity Using LINCS Data\nResearchers in the Department of Systems Biology have developed a repertoire of experimentally validated algorithms such as ARACNe (for the dissection of transcriptional interactions) and MINDy (for post-translational interactions in mammalian cells). We have also pioneered the development of methods for integrating information from various layers within complex regulatory networks. These include the algorithms MARINa (used to interrogate interactomes and to identify master regulator and master integrator genes that cause specific phenotypes), and IDEA (used to elucidate compound mechanism of action).\nBuilding on this methodological base, Columbia has begun to introduce novel approaches and develop new algorithms that are specifically designed for the analysis of LINCS data. As part of this project, we are currently working to:\n- elucidate the MoA and activity of small molecule and RNAi/cDNA mediated perturbations\n- identify genes that mediate a cell’s sensitivity or resistance to the activity of specific compounds\n- identify gene-gene, gene-compound, and compound-compound interactions whose modulation can synergistically induce a desired phenotype (e.g., apoptosis, cell cycle arrest, or loss of pluripotency)\n- use LINCS in vitro signatures to predict compound-related properties such as MoA, activity, sensitivity, synergy, and other phenomena in vivo\nAccessing drug activity and sensitivity data\ngeWorkbench offers a simple interface for building custom queries based on the results of drug similarity experiments conducted at Columbia University. The annotations available for narrowing a query include the tissue type, cell line, drug names, and the similarity algorithm used. As with the synergy query, the results are presented in tabular form, with additional visualizations including:\n- heat map display of drug-drug similarity scores\n- functional mode of action (fMoA) of a drug as represented by regulatory gene differential activity in response to the drug as determined using VIPER\n- network view of the differential expression of targets of selected regulatory genes using Cytoscape. This is the data underlying the VIPER calculation.']	['<urn:uuid:89540d7f-547b-4a68-aa86-9e5ef274574f>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-13T03:34:55.284799	10	38	1255
92	need to know how to recognize signs of heat illness in hot weather what symptoms to watch for	The key signs and symptoms of heat illness include muscle cramps, nausea, vomiting, headaches, dizziness, dry mouth, lightheadedness, skin that's not as elastic, and dark urine color. These symptoms can indicate the onset of heat illness. Heat illnesses are caused by a combination of dehydration and prolonged exposure to extreme temperatures, and can range from heat rash and heat cramps to more severe conditions like heat exhaustion and potentially deadly heat stroke.	"[""6 Super Tips to Beat the Heat and Stay Cool\nThe spring and summer months are full of fun and sun, but with the sun comes the heat. The summer heat can be dangerous to us all, especially our pets, the young, poor, and elderly. Heat illnesses are caused by a combination of Dehydration and prolonged exposure to extreme temperatures. Heat illnesses can be divided into many different conditions like Heat rash, Heat cramps, Heat exhaustion, and Heat Stroke. Heat Stroke can be deadly, however it can be easily prevented. We have assemble 6 Super Cool Tips to help you or your loved ones beat the heat.\n1. Drink plenty of fluids. Your body’s mechanism of cooling itself off is by sweating. The body is unable to cool itself off as efficiently in hot and humid temperatures. When sweating, you are losing mainly sodium and water. It is important to replace your electrolytes and fluids to keep the body balanced and to avoid dehydration or a heat related illness. Staying hydrated will help your body sweat and maintain a normal body temperature. Drink a quart of fluids an hour or drink enough water to prevent thirst. Avoid alcoholic beverages the day before or the day of. If your doctor has told you to limit fluids because of a health condition, be sure to check with them about how much extra you need to drink when the temperature rises.\n2. Use cooling aids and wear loose fitting, lightweight, light-colored clothing. Excess, dark or tight clothing holds in heat and doesn't let your body cool properly because it inhibits sweat evaporation. The use of cooling aids such as personal misters, cooling towels, cooling hats, and cooling bandannas will help maintain your comfort and a healthy body temperature while exposed to high temperatures.\n3. Seek Shade or a cooler place. Being in an air-conditioned building, even for just a few hours, is one of the best ways to prevent heat exhaustion. If your home doesn't have an air conditioner, consider spending time at a library or shopping mall. At the very least, find a well-shaded spot. Fans alone aren't adequate to counter high heat and humidity.\n4. Maintain a well balanced diet. Eat at least five cups of fruits and vegetables per day for optimum health, as they all contain various levels of water and the all-important nutrient potassium.\n5. Avoid hot spots. On a hot day, the temperature in your parked car can rise 20 F (11 C) in just 10 minutes. Let your car cool off before you drive it. Never leave children or anyone else in a parked car in hot weather for any period of time.\n6. Avoid sunburn. If you're going to be outdoors, wear a lightweight, wide-brimmed hat or use an umbrella to protect yourself from the sun, and apply sunscreen to any exposed skin. Having a sunburn reduces your body's ability to rid itself of heat.\nWith temperatures rising, so does the risk of heat related illnesses. Know the signs and symptoms of heat illness like Muscle Cramps,Nausea, Vomiting, Headaches, Dizziness, Dry Mouth, Lightheadedness, Skin not as elastic, and Dark urine color. These signs can indicate the onset of heat illness. Be cautious and remember these six tips and your summer days will be safe and comfortable.""]"	['<urn:uuid:0515cbe0-ddd5-4013-bf93-a67b254df710>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T03:34:55.284799	18	72	551
93	employment programs disabled workers museums legal requirements compare	ESMoA museum created employment pathways through paid internships for people with disabilities, addressing the low 17.5% employment rate among disabled persons. Legally, employers must provide reasonable accommodations for disabled workers, such as wheelchair ramps, unless financially impossible, and cannot make job-related decisions based on current, past, or perceived disabilities.	['In the US in 2015, 17.5 percent of persons with a disability were employed–compared to a 65 percent employment rate for people without a disability: a troubling statistic that should influence our discussions of museum labor and museum internships. This week on the Blog, Holly M. Crawford, education specialist at the El Segundo Museum of Art (ESMoA) shares how her museum, and others, are helping create employment pathways for people with disabilities. I hope you have other such stories to share–please chime in using the comments section, below.\nESMoA had been open less than a year in September 2013 when Chelsea Hogan and I met with Ed Lynch, founder and Executive Director of Mychal’s Learning Place, to brainstorm ways our two organizations could work together. Established in 2002, Mychal’s provides opportunities for children and adults with developmental disabilities to build self-esteem and independence. Ed was describing Mychal’s Path to Independence Program (P2I), a new program designed to equip Mychal’s adult students with the tools and support to live independently when Chelsea suggested we could have an internship for Mychal’s students at ESMoA. By October we had started the pilot internship with two students from Mychal’s P2I.\nLeading Students in the Gallery, Photo Credit: Adam Kissick\nFor four amazing years ESMoA and Mychal’s Learning Place in Hawthorne, CA have collaborated on the Mychal’s/ESMoA Internship Program. Each year Mychal’s and ESMoA nominate two interns from P2I to participate in a seven-month-long internship that runs from October – May overlapping with ESMoA’s School Visits program. This internship gives participants exposure to the inner-workings of our art laboratory, as interns work closely with the ESMoA team. Interns receive a weekly agenda outlining their roles and responsibilities at ESMoA which include but are not limited to greeting students, organizing art materials, and conducting research about artwork currently on view for ESMoA Experiences. Additionally, interns tour arts and cultural institutions in and around Los Angeles and learn about Visual Thinking Strategies to facilitate gallery discussions with school groups about an artwork.\nThe internship program complements Mychal’s P2I goals: Mychal’s students are challenged and empowered at ESMoA. They are part of a supportive team and learn skills to help them find gainful employment. As a program mentor, my appreciation for different learning styles has broadened as a result of working with our interns. One memorable intern with a diagnosis of Down syndrome would sometimes arrive for work feeling lethargic. As we worked with him to develop an energy-building routine, I noticed that our classes also tended to get tired or antsy during studio-to-gallery transitions during their visit. Our intern thought stretching would be a great way to help students transition between these two activities and help ground them for focused discussion. Because of his insight, I now build time into my lesson for a stretching exercise.\nInterns Visiting Watts Towers\nI have also observed that student visitors tend to be more patient and attentive during gallery discussions led by Mychal’s interns. As educators, we often say that our programs build empathy, and watching students engage with Mychal’s interns is empathy in action.\nThis summer ESMoA was awarded a SPARKS! Ignition Grant from the Institute of Museum and Library Services to fund the creation of ESMoA Academy, a free e-learning platform where we have developed our featured course: Spreading the SPARK of Creativity, a flexible blueprint intended for museum educators who are interested in developing an internship program that serves special needs populations.Delivered in concise video tutorials and supplemental materials, after completing this online course, participants will be better able to:\nImplement successful strategies for creating internship opportunities for adults with developmental delays.\nCreate a sustainable partnership model with a non-museum organization.\nAdvocate for access and resources for adults with disabilities seeking employment in cultural institutions.\nWe are in the final editing stages with the website and anticipate a release date in early December 2016. Users will be able to follow the program in real time as video lessons become available each month through May 2017.\nThis year we are also celebrating the fact that we are finally able to pay our Mychal’s/ESMoA interns. Previous iterations of the internship have been ‘experience only.’ Working with ESMoA’s Executive Board, we were able to secure a generous donation from the SKECHERS Foundation for stipends for our 2016-2017 interns. Since researching funding streams, I have learned about the Ticket to Work program, a federal program that helps individuals with disabilities achieve their employment goals and found opportunities through the California Arts Council and University of California system.\nWhen we first started the internship program four years ago, it seemed like it was the first of its kind. This past summer, however, I was excited to learn (via the NY-based Museum Access Consortium) about Lincoln Center’s inclusive hiring initiative, Access Ambassador Initiative. And my colleague Cecile Puretz (Contemporary Jewish Museum, San Francisco) told me about an exciting project between AccessSFUSDand the California Academy of Sciences. As these learning opportunities grow in numbers and recognition, museums are on the forefront of a learning revolution.\nHolly M. Crawford is an artist, educator and arts advocate based in Los Angeles. She is also a 2016 – 2017 ACTIVATE Cultural Policy Fellow. Holly has been an Education Specialist at the El Segundo Museum of Art (ESMoA) since 2013 and designs and facilitates programming for school, family, and adult audiences. As coordinator of the Mychal’s/ESMoA Internship Program, Holly provides museum internship opportunities to adults with developmental disabilities. Follow her on twitter at @artlab21 and @CallMeHawford.', '6 examples of disability discrimination in the workplace\nThe Americans with Disabilities Act protects you from being discriminated against due to a past, current, or perceived disability. Disability discrimination in the workplace is against the law but unfortunately, that doesn’t deter all violations. Here are 6 examples of disability discrimination and its presence in a work environment.\nFiring a candidate because of a disability\nEmployers are not permitted to make any job-related decision based on a candidate’s or employee’s disability. This includes hiring decisions, promotions, terminations, and other related aspects of employment. If an employer doesn’t hire a qualified applicant due to a disability, decides to reassign the employee to a lesser role, or fire him or her because of a disability, it would be in violation of the law and is considered disability discrimination.\nFor example, in Griffiths vs. Secretary of State for Work and Pensions, the court held that the dismissal of a worker due to disability-related absences, which violated the attendance policy, was unlawful and constituted discrimination.\nDiscrimination due to past disability\nEven if the disability is in an employee’s past, it is still unlawful to discriminate against him or her. If a worker has overcome cancer but is not given a promotion or a job offer due to the risk of a relapse, that is discrimination and it is against the law.\nFailing to accommodate an employee’s disability\nInstalling a wheelchair ramp at the office is an example of a reasonable accommodation for a worker who has a disability. Employers are required by law to make reasonable accommodations for disabled employees as long as it isn’t financially impossible or extremely difficult. A small business in its early days might not be able to afford the installation of a wheelchair ramp and therefore the employer would not have to complete this project by law.\nReasonable accommodation can be as simple as making alterations to the layout of the furniture in the lounge or providing additional equipment to assist an employee with their work.\nDiscrimination by perception\nEven if a worker is not disabled but is discriminated against because he or she is believed to have a disability, it’s still against the law. The assumption of a mental disability, disease or progressive sickness is not an excuse for any type of discrimination in the workplace.\nHarassing someone with a disability\nIt is against the law to harass an individual if he or she is disabled, had a disability, or is believed to have a disability. Harassment includes anything from an offensive remark to a verbal or physical threat. In most cases, the harassment must happen more than once and create a hostile work environment. The law protects disabled individuals from being harassed by anyone in the workplace, including supervisors and customers. Bullying, nicknames, inappropriate questions and unwanted jokes are all forms of disability harassment.\nDeclining a job offer to someone due to a family member’s disability\nIf an employee is married to a disabled spouse and is passed over for promotion because the decision maker believes that the employee cannot devote the adequate time to the job due to the disabled family member, that is considered disability discrimination and it is against the law.\nIf you believe that you have a disability discrimination case, contact an attorney with experience in disability law to help you.']	['<urn:uuid:61a3f3a6-54b3-4416-9521-ce9bf8b3b546>', '<urn:uuid:0d6c9d11-1d70-4b5d-bd06-4ddf3a29a113>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T03:34:55.284799	8	49	1475
94	What role can alternative energy sources play in addressing environmental concerns, considering both agricultural biofuels like sugarbeet-based ethanol and other renewable technologies?	Multiple renewable energy sources can address environmental concerns. Sugarbeet-derived ethanol serves as a biofuel that can be blended with petrol or diesel up to 10%, while also providing additional environmental benefits through its waste products used as green fodder, cattle feed, and organic manure. The crop improves soil conditions and can be cultivated in otherwise challenging environments like saline and alkali soils. Beyond biofuels, other renewable technologies include solar power, which harnesses the sun's energy through photovoltaic cells to generate electricity; wind power, which uses turbines to convert wind into mechanical energy and then electricity; and hydroelectric power, which generates electricity from water movement. These renewable resources are increasingly replacing traditional electricity generation methods that relied on nonrenewable resources like coal and oil, offering more environmentally sustainable solutions for power generation.	['AGR 301 :: Lecture 16 :: TROPICAL SUGARBEET Beta vulgaris spp. Vulgaris\nImportance of sugarbeet\n- Tropical sugarbeet is a biennial sugar producing tuber crop, grown in temperate countries\n- This crop constitutes 30% of total world production and distributed in 45 countries.\n- Now tropical sugarbeet hybrids are gaining momentum in tropical and sub tropical countries including Tamil Nadu as a promising energy crop and alternative raw materials for the production of ethanol.\n- Apart from sugar production, the value added products like ethanol can also be extracted from sugarbeet.\n- The ethanol can be blended with petrol or diesel to the extent of 10% and used as bio-fuel.\n- The sugarbeet waste material viz., beet top used as green fodder, beet pulp used as cattle feed and filter cake from industry used as organic manure.\nTropical sugarbeet now emerged as commercial field crop because of the favourable characters like\n- tropical sugarbeet hybrids suitable for Tamil Nadu\n- Shorter duration of 5 to 6 months\n- needs moderate water requirement of 60-80 cm.(iv) higher sugar content of 12 – 15% (v) improve soil conditions because of tuber crop and\n- grow well in saline and alkali soil.\n- The harvesting period of sugarbeet coincides with March – June, the human resource of sugar factory in the off season may efficiently utilized for processing of sugarbeet in the sugar mills, which helps in continuous functioning of sugar mills.\nHybrids and duration\nThe tropical sugarbeet hybrids suitable for cultivation in Tamil Nadu are\n- Indus and\n- The duration of these tropical hybrids will be 5 to 6 months depending on climatic conditions prevailing during crop growth period.\nClimate and season\n- Tropical sugarbeet require good sunshine during its growth period.\n- The crop does not prefer high rainfall as high soil moisture or continuous heavy rain may affect development of tuber and sugar synthesis.\n- Tropical sugarbeet can be sown in September– November coincide with North East monsoon with a rainfall of 300 – 350 mm well distributed across the growing period which favours vegetative growth and base for root enlargement.\n- The optimum temperature for germination is 20 – 250C,\n- for growth and development 30 - 350C and\n- For sugar accumulation in 25– 350C.\n- September to November and harvested during March and May.\n- Well drained sandy loam and clayey loam soils having medium depth (45” cm) with fairly good organic status are suitable.\n- Tropical sugarbeet require deep ploughing (45 cm) and followed by 2 – 3 ploughing to obtain a good soil tilth condition for favorable seed germination.\n- Ridges and furrows are formed at 50 cm apart.\nManures and Fertilizers\nManures and Fertilizers\n12.5 tonnes /ha\n2 kg /acre (10 pockets)\n37.5 kg / ha each at 25 & 50 DAS\nSeeds and sowing\n- Optimum population is 1,00,000-1,20,000 /ha.\n- Use only pellated seeds 1,20,000 Nos /ha which require 6 pockets (3.6kg / ha.-One pocket contains 20000 seeds (600 g)]\n- The recommended spacing is 50 x 20 cm.\n- The pellated seed is dibbled at 2 cm depth in the sides of ridges at 20 cm apart\nWeeding and Earthing up\n- The crops should be maintained weed free situation up to 75 days.\n- Pretilachlor 50 EC @ 0.5 kg /ha or Pendimethalin @3.75lit /ha can be dissolved in 300 litres of water and sprayed with hand operated sprayer on 0-2 day after sowing,\n- Followed by hand weeding on 25th day and 50th day after sowing.\n- The earthing up operations coincides with top dressing of N fertilizer.\n- Tropical sugarbeet is very sensitive to water stagnation in soil at all stages of crop growth\n- Irrigation should be based on soil type and climatic condition.\n- Pre-sowing irrigation is essential since at the time of sowing, sufficient soil moisture is must for proper irrigation.\n- First irrigation is crucial for the early establishment of the crop.\n- For loose textured sandy loam soil irrigation once in 5 to 7 days and for heavy textured clay loam soil once in 8 – 10 days is recommended.\n- The irrigation has to be stopped atleast 2 to 3 weeks before harvest.\n- At the time of harvest if the soil is too dry and hard it is necessary to give pre harvest irrigation for easy harvest. Light and frequent irrigation is recommended for maintaining optimum soil moisture\nPest and diseases\n- Pests - Aphids, Tobacco caterpillar and Flea beetles\n- Root and crown rot, Cercospora leaf spot and Root knot nematode\nIntegrated pest and disease management\n- Seed treatment with Pseudomonas fluorescens @ 10 g/kg of seed\n- Summer ploughing and exposing the field to sunlight\n- Crop rotation for 3 years with Marigold or gingelly or sunnhemp for root rot and nematode\n- Soil application of Trichoderma viride or Pseudomonas fluorescens @ 2.5 kg/ha mixed with 50 kg of FYM before planting\n- Sow castor as trap crop around and within fields to attract adult Spodoptera moth for egg laying\n- Set up light traps (1 mercury / 5 ha) for monitoring Spodoptera litura\n- Setting up pheromone -Pherodin SL @ 12/ha for Spodoptera litura\n- Removal and destruction of Spodoptera egg masses, early stage larvae formed in clusters\n- Hand picking and destruction of grown up Spodoptera caterpillar\n- Spraying Spodoptera nuclear polyhedrosis virus at 1.5 x 1012 POB/ha\n- Spray NSKE 5% for aphids flea beetles and for early instar caterpillars\n- Use of poison bait pellets prepared with rice bran 12.5 kg, jaggery 1.25 kg, carbaryl 50% WP - 1.25 kg in 7.5 lit water for Spodoptera litura\n- Spray any one of the following insecticides using a high volume sprayer covering the foliage and soil surface\n- Chlorpyriphos 20 EC - 2 ml / lit,Dichlorvos 76 WSC - 1 ml/lit, Fenitrothion 50 EC - 1 ml/lit, Spray malathion 50 EC (2 ml/lit) for flea beetle and leaf webber, Spray Imidacloprid 200 SL (0.2 ml/lit) or methyl demeton 25 EC (2 ml/lit) or dimethoate 30 EC (2 ml/lit) for aphids\n- Applying neem cake @ 150 kg/ha for root rot\n- Foliar spray of Mancozeb 2.5 g / lit or Chlorothalonil 2 g / litre of water for Cercospora leaf spot\n- Neem cake @ 1 t/ha or carbofuran @ 33 kg/ha as spot application on 30 days after sowing for nematode management\nHarvest and yield\n- The Tropical sugarbeet crop matured in about 5 to 6 months. The yellowing of lower leaf whirls of matured plant, Nitrogen deficiency and root brix reading of 15 to 18% indicate the maturity of beet root for harvest.\n- The average root yield of tropical sugarbeet is 80 – 100 tonnes / ha.\n- Harvesting should be timed so as the roots reach the factory within 48 hours for processing.\n- Till such time the roots should not be harvested.\nMultiple choice questions\n- Apart from sugar, _______ can be extracted from sugarbeet\n- Methane b. Ethanol c. Dimethyl ether\n- Ethanol produced from sugarbeet can be blended with petrol or diesel to the extent of ______ as biofuel\n- 12 % b. 10 % c. 15 %\n- Optimum seed rate for tropical sugarbeet is ______ kg/ha\n- 5.6 b. 3.0 c. 3.6\n- Spacing recommended for sugarbeet is ______\n- 50 x 20 cm b. 40 x 20 cm c. 50 x 10 cm\n- Fertilizer dose recommended for sugarbeet is ________ kg NPK /ha\n- 100 : 75 : 75 b. 100 : 100 : 100 c. 75 : 75 : 75\n- ________ crop is used as trap crop to attract Spodoptera in sugarbeet\n- Gingelly b. Castor c. Marigold\n- _________ % of root brix reading indicates the maturity of sugarbeet\n- 15 – 18 b. 18 – 25 c. 25 – 27\n- Yield potential of sugarbeet is ________ tonnes/ha\n- 100 -120 b. 80 – 100 c. 120 – 150\n|Download this lecture as PDF here|', 'More and more frequently, people are looking for a way to power their homes and businesses without having such a strong impact on the environment. For years, services have used and perhaps even abused the earth in an effort to provide electricity for their customers. That electricity was generated with coal, oil and other types of nonrenewable resources but today, people are looking for renewable forms of energy. There are a number of options available and we will consider a few.\nBefore we talk about the types of renewable energy, it’s important to consider the benefits of using them on a regular basis. In most cases, these types of services can be set up rather conveniently and the return on investment from the installation is going to be relatively quick. One of the benefits of using renewable energy is the fact that it continues to work on an ongoing basis and once it is put in motion, it requires very little maintenance. Regardless of the type of renewable energy service that is being considered, this will be true.\nThe following 3 types of renewable energy are being used today. In fact, the technology that is associated with these types of energy is growing and as it does so, the use is growing as well. They provide the ability to power our homes and businesses without having a severe impact on the earth.\nSolar Power – Although there may be many different forms of renewable energy, one of the most convenient and most powerful is the energy that comes from the sun. When it is able to be harnessed properly, it could easily be converted into usable energy and the possibilities are truly endless. Here is a basic understanding of how it works.\nSolar power gathers energy from the sun. The sun is putting out energy on a constant basis and even though only a very small part of that energy reaches the earth, it is thought to be enough to provide for our needs many times over. The power from the sun is collected by something that is known as a photovoltaic cell. That cell has the ability to take the energy from sunlight and convert it into usable electricity.\nA single photovoltaic cell is able to produce a very small amount of energy but those cells are often put together in panels and together, they can gather a lot of energy for our use. In fact, there are times when these panels are even put out in farms that may span many acres. The energy that they produce can then be channeled into the grid and ultimately, into our homes and businesses.\nWind Power – The wind is another form of usable energy and in essence, it is also a form of solar power. After all, the sun heats the earth and as the earth is spinning, the uneven heating is what causes the wind and weather patterns.\nA wind turbine will typically either have 2 or 3 blades and as the wind blows, the blades will turn. The blades turn a shaft that ultimately, goes into a generator and the generator produces usable electricity. In its most basic sense, the wind turbine uses the wind to create mechanical energy and then the mechanical energy is converted into electricity.\nHydroelectric Power – This type of power uses the movement of water, regardless of whether it is a river or the tides in the ocean. It does so in a similar way to wind power, in that the water movement turns a shaft that causes a generator to create electricity. This type of renewable energy is already in use and it can be seen in dams around the world. Additional resources are being considered for using the tidal forces, and it may just produce enough electricity to power our needs for many years to come.\nThese 3 types of renewable resources are not only a possibility, they are already a reality as they are already being used. Around the world, renewable electricity is being produced more and more frequently and as it is produced, it is replacing the type of electricity that abused the earth through nonrenewable resources.']	['<urn:uuid:3d299a9e-6cd2-40c4-9c70-299b2f4df23e>', '<urn:uuid:7c1434d5-c897-4126-ba7f-6d33d3f4245f>']	open-ended	direct	verbose-and-natural	distant-from-document	three-doc	expert	2025-05-13T03:34:55.284799	22	131	2033
95	differences nonprofit profit fundraising organizations	The key distinction between nonprofit and for-profit fundraising organizations lies in how they operate and handle money. Nonprofit organizations use funds raised for charitable causes and rely heavily on volunteers. They must register with state authorities, file annual reports, and maintain compliance with IRS regulations by filing Form 990. In contrast, for-profit companies that offer fundraising opportunities direct money toward company profits rather than charitable causes. These companies typically pay their fundraisers instead of using volunteers. Some unethical for-profit companies may attempt to mislead by appearing as charities to get people to work for little or no pay. It's important to verify an organization's nonprofit registration status before getting involved in fundraising work.	"['Fundraising opportunities come in many different shapes and sizes. You may be looking for a suitable occasion for a sponsored run, or wondering how you can fit fundraising into your daily life. This page should give you some ideas.\nCommunity/ national events\nMany charities provide regular fundraising opportunities in the form of national fundraising events. Different states usually hold their own version of the national event, although occasionally a big national fundraising event takes place in a central location. One example of an event which takes place in different locations all over the country is the Komen Race for the Cure, a 5km run/walk in aid of breast cancer. Komen races happen in many different states at various times of the year. Click here to go to the Komen site and find a race near you.\nThe New York Marathon is one of the most famous races in the country. It is usually held on the first Sunday in November. Entrants come from overseas as well as many US states, and run the marathon for many different reasons, but it is increasingly being seen as a fundraising opportunity, and many charities are represented among the runners.\nRegular fundraising opportunities\nFundraising doesn\'t have to mean selling cookies, holding a collecting tin or doing a sponsored event. If you\'re prepared to make a small but regular time commitment (for example, a few hours a week) you will be able to raise hundreds, if not thousands of dollars for your favorite charity by being a volunteer. Do one shift a week at a charity shop, or ask an organization if they need someone to stuff envelopes for appeal mailings. Be proactive when looking for this kind of opportunity; think about what you\'d ideally like to be doing, and then call the organizations most likely to let you do this. Most nonprofits are very grateful for volunteers. Another advantage of this kind of fundraising is that it lets you raise funds without a lot of face-to-face work, which can be off-putting for many. Shy people should see our page of fundraising ideas to find out how it\'s possible to fundraise without being pushy.\nFundraising for businesses\nMany for-profit companies offer ""fundraising"" opportunities, but you should be aware that the money you raise will not be going to a good cause but towards the company\'s profits. Because of this, for-profit companies usually pay fundraisers rather than expecting them to work on a voluntary basis.\nHowever, some unscrupulous for-profit companies advertise sales jobs as ""fundraising opportunities"" and try to give the impression of being charities so that fundraisers will work for little or no money. The only way to avoid being tricked like this is to check whether or not the company you are thinking of working for is registered as a nonprofit organization. See our page on the legal issues of nonprofit fund raising for more information. Always be suspicious if an organization asks you to hand over money before being allowed to begin work. For more information about working in the fundraising sector, see our fundraising jobs page.', 'By James Gilmer\nAs the end of the year approaches, it’s time to take stock of your nonprofit’s compliance requirements of the past year and also to begin planning for the year ahead. Nonprofits are held to high standards of government regulation and public oversight, so staying compliant is critical to keeping your nonprofit running smoothly.\nNonprofits are commonly required to manage multiple requirements across federal and state government agencies, and also to maintain documentation in the event of an inquiry or audit. Compliance can be generally grouped into two categories: filings that are submitted to government agencies and records that should be kept internally.\nThe following is a brief overview of the tasks that most nonprofits must complete at the end of the year. Keep in mind that not all of these items will apply to all organizations and that this list is not exhaustive.\nYour nonprofit is required to file reports periodically with the Secretary of State in which it is incorporated and any states it is qualified as a foreign corporation. The annual report updates the public records with changes to your nonprofit’s address, registered agent, and leadership each year. Report due dates vary by state, so pay attention to your state’s deadline and check if you have filed your most recent report. If your nonprofit is registered in multiple states, make sure you have adequate systems in place to track the various due dates. Failure to file periodic reports can incur steep penalties and result in your nonprofit’s registration being revoked.\nYour nonprofit is also required to maintain a registered agent in each state where it has registered to do business. If you have appointed individuals, make sure they are aware of their appointment and can reliably serve as your nonprofit’s agent. If you have appointed a registered agent service company, make sure you renew your service.\nLastly, conduct a review to see if qualification (Secretary of State registration) is necessary in any other states. Common reasons include hiring a new employee, purchasing property, opening a physical location, and as required to meet state charitable solicitation requirements.\nState Fundraising Compliance\nDepending on your state and fundraising activities, your nonprofit may be required to register with the state’s Attorney General for charitable solicitation (a.k.a. fundraising). You generally have to renew this registration annually, so make sure you have filed this year in each state. Failure to register or renew can again lead to strict penalties. Most importantly, your donors can and do search state databases to see whether your charity is legitimate.\nFundraising registration and renewal is a complex topic, and your individual requirements will vary depending on your finances and activities. For more state-specific information, you can review this Fundraising Compliance Guide. At the end of the year, review your fundraising activities, including specific initiatives and all states in which you ask for contributions. Having done so, it is possible you need to register in one or more states, or to file a renewal shortly after your fiscal year ends.\nIRS Compliance and Form 990\nEvery year, your nonprofit must file a return with the IRS, known as the “990.” There are several versions of the 990 return, and the form you file depends on your organization’s revenue. It updates the IRS with changes to your leadership, contact info, mission, activities, and finances.\nIRS Form 990 is due each year exactly four months and fifteen days after the end of your fiscal year. For nonprofits on a calendar year, this is May 15th. Failure to disclose your organization’s information in full, or failure to file at all can lead to a loss of your hard-earned federal tax exemption and/or penalties that accumulate daily. By the way, if you have incorporated in this fiscal year, and you have yet to receive your 501(c)(3) determination, you still must file IRS Form 990, or you can jeopardize becoming tax exempt from the outset.\nInternal Compliance and Recordkeeping\nIt’s important to keep thorough records of meetings, and changes to your organization, leadership, and activities. Not only will good recordkeeping help you stay organized, but it may come to your defense in the event of an audit. Your individual list will vary, but here is a brief list of items to review annually:\n- Hold annual board meeting, and keep minutes\n- Ensure bylaws are in place and up to date\n- Review all policies, including conflict of interest and executive compensation policies\n- Conduct financial performance review and reporting\n- Create and approve next year’s budget\n- Hold elections for officers and directors\nNot only does the IRS want you to keep good records, but your donors may as well. For instance, if you apply for a grant, or are trying to gain the support of a prominent donor, they may request records, including financial statements. Being able to readily produce such records help your donors to feel secure in their gift to a responsible, compliant organization.\nThe Value of Compliance\nWe’ll admit, “end of year compliance” is a bit of a misnomer. Compliance responsibilities are ongoing, from constant recordkeeping to staggered due dates of government applications and renewals, so make sure you have the right people and systems in place to manage it all. However, many of these activities coincide with preparing the 990, so the end of the year is a logical time to conduct a full review of compliance and plan for the year ahead. With a proactive approach to meeting the requirements briefly outlined in this article, your nonprofit will set itself up for success in the year ahead.\nStaying compliant adds tremendous value to your organization. Staying in good standing with the state and the IRS will avoid loss of tax exemption, and loss of limited liability protection for the officers and directors. Perhaps most importantly, donors, foundations, and other contributors want to give to responsible, credible, and fully registered organizations. By choosing to stay compliant, you make choosing your organization much easier for them!\nJames Gilmer is a compliance specialist for Harbor Compliance, which establishes 501(c) nonprofits and helps them stay compliant. Harbor Compliance assists charities in every state and several countries abroad. James serves on the Board for two nonprofits in Lancaster, Pennsylvania.']"	['<urn:uuid:5a926c5b-16d4-414a-a632-ab434b5b4879>', '<urn:uuid:b3eb6d28-e478-47a1-958f-80ca8179e254>']	open-ended	direct	short-search-query	distant-from-document	three-doc	novice	2025-05-13T03:34:55.284799	5	113	1554
96	what goals and environmental challenges nordefco military operations face nordic defense and environment impact	NORDEFCO has established several goals for 2025, including improving military capability, enhancing coordination in training and exercises, and strengthening Nordic-Transatlantic relations. However, these military operations face significant environmental challenges. Military activities can harm the environment through emissions from vehicles and aircraft not designed for fuel efficiency, landscape damage and contamination from training areas, and high resource consumption in areas with minimal waste management infrastructure. While Western militaries are increasingly subject to environmental regulations and are working to reduce emissions and improve chemical management, institutional resistance to environmental standards remains a barrier, particularly when operational success is prioritized over environmental considerations.	"['NORDEFCO Annual Report 2020\nThe Nordic Defence Cooperation, or NORDEFCO, is the intergovernmental collaboration of the Nordic countries on matters of military and defense. Its five member states are Denmark, Finland, Iceland, Norway, and Sweden. This report details the accomplishments of the recent Danish chairmanship, the organization\'s structure, and the goals for the cooperative in the coming years.\nFind the full version here: https://www.nordefco.org/files/NORDEFCO-annual-report-2020.pdf\n""Foreword: Danish Chairmanship 2020\nThe Nordic Defence Cooperation is important to Denmark. We need a strong Nordic defence cooperation to contribute to stability and security in our region. Therefore, the Danish NORDEFCO chairmanship in 2020 has been a key priority for Denmark - and for me personally.\nThis annual report is a testament to the important progress made in 2020 to develop and strengthen NORDEFCO. It accounts for the priorities and focus areas that have guided the Danish chairmanship and achievements obtained. We were only at the beginning of the Danish chairmanship, when our new NORDEFCO crisis consultation mechanism proved its relevance in very real terms; A missile attack in Iraq affecting Nordic troops on training mission followed by the outbreak of the COVID-19, underlined the importance of being able to rapidly convene and consult each other during crises.\nThese examples clearly illustrate how far we have come in deepening our Nordic defence cooperation. How close together we have moved. They underline the importance of continuing to improve the ability to act together in crisis and conflict - a core ambition in NORDEFCO Vision 2025 and a main goal for the Danish chairmanship. Together, we have taken significant steps this year to help us to achieve this goal. Furthermore, the COVID-19 pandemic, as the defining global challenge in 2020, has shown the robustness and agility of NORDEFCO. Our work has continued in a largely undisrupted manner – yet virtually.\nOne important focus area this year has been to strengthen Nordic-Transatlantic relations and move forward with the aim to ensure joint participation in important exercise activities. Moreover, we have shared perceptions of the Arctic to enhance situational awareness and be better equipped to address security challenges in the region. Within the area of cyber security we have improved our understanding of challenges in regard to recruitment, education and retainment of competences and mapped areas where increased cooperation could be of particular benefit. We have broadened the discussion on total defence with perspectives from the Haga-cooperation on emergency management, and pushed the important agenda of green defence forward to contribute to overall CO2 reductions.\nFurthermore, at the virtual fall ministerial meeting on 5th November, Denmark joined Finland, Norway and Sweden in the commitment to improve and sustain security of supply between Nordic countries. As COVID-19 has shown us, security of supply is key in a crisis situation, and the agreement allows for better cooperation both in peacetime and beyond. In addition, all four countries signed an agreement on Nordic export control in order to simplify procedures and facilitate Nordic cooperation in the defence material area.\nIn closing, I would like to thank my Nordic colleagues and everyone who has contributed to promoting and developing the Nordic defence cooperation this year. The Nordic countries have different security affiliations, but we all share the commitment and responsibility to keeping our region safe and secure. Nordic defence cooperation is key to this endeavour.\nI look forward to the coming year under the Finnish chairmanship of NORDEFCO.\nTrine Bramsen Danish Minister of Defence\nNORDEFCO Vision 2025\nOn the 13th of November 2018, the Nordic Ministers of Defence adopted the following vision for enhance Nordic defence cooperation towards 2025: We will improve our defence capability and cooperation in peace, crisis and conflict. We ensure a Close Nordic political and military dialogue on security and defence. Acknowledging our different security affiliations, we pursue an agenda based on joint security perspectives, efficient and cost-effective cooperation to strengthen our national defences and the ability to act together. In order to operationalize the vision, we will strive towards the following targets: By 2025, we have:\nMinimal restrictions on movement and storage of military units and equipment, between and through the nations in support of national and multinational activities, operations and deployments.\nIncreased cooperation in total defence, military security of supply and civil-military cooperation.\nImproved regional and common situational awareness in peace, crisis and conflict, in all relevant domains, through real-time information- and data sharing.\nEnhanced NORDEFCO as a platform for crisis consultation and established mechanisms for that purpose.\nImproved our readiness and sustainability in order to improve our ability to act together.\nCoordinated relevant training and exercises between the Nordic countries, and we have improved interoperability.\nEnhanced our transatlantic relations by seeking closer cooperation in areas such as training, exercises and other activities, and improved cooperation with our European partners.\nContinued to strengthen our dialogue and cooperation with Estonia, Latvia and Lithuania.\nImproved our resilience in light of the dangers posed by hybrid threats and growing cyber threats. Coordinated our international operations with a focus on national contributions, command and control and joint logistics, when possible.\nEnhanced logistical cooperation where possible and desirable, with mutual measures to support national needs in crisis and conflict. Established a strategic dialogue to enhance capability development in order to meet the requirements needed to address the security environment\nAn active and flexible partner in the Nordic defence industry in developing capabilities and finding new solutions to armaments and total defence requirements, including through utilization of the possibilities inherent in the proposed European Defence Fund as well as other relevant for a and instruments.\nWe will continue to explore and adopt new beneficial possibilities for cooperation which may emerge\nEstablished options for common education and training to maximize effectiveness and availability in all Nordic development and procurement programs\nEnhanced our armaments coordination and cooperation\nWe will continue to explore and adopt new beneficial possibilities for cooperation which may emerge.\nDanish Chairmanship Priorities and Results\nNordic-Transatlantic relations A key priority of 2020 has been to strenghten the Nordic-Transatlantic relations as one of the targets in Vision 2025. To achieve this, the Nordic Ministers of Defence agreed on a joint approach to continued U.S. engage- ment in the Nordic region. A key deliverable in this respect has been to ensure that the Nordic-led Arctic Challenge Exercise (ACE) maintains so-called flag-level according to the agreed-upon criteria, and U.S. participation in the exercise is essential in this regard. The U.S. Air Force Europe is already deeply engaged in ACE 2021. This was consolidated in the first two ever meetings between the US European Command (USEUCOM) and the NORDEFCO Military Coordination Committee (MCC).\nTo support the strong Nordic-Transatlantic Military cooperation on ACE, the political level in NORDEFCO has initiated the development of a political Letter of Intent to ensure future Nordic-Transatlantic engagement in ACE in order to maintain it at flag level, with a signing ceremony scheduled for February 2021.\nWhile the COVID-19 pandemic did lead to cancellations, postponements and downscaling of both national and multinational exercises, a positive development included the establishment of new procedures for further synchronization and coordination of military exercises and training activities with both Transatlantic and Baltic partners.\nThe Arctic An important theme during the Danish chairmanship was to strengthen the Nordic se-curity policy dialogue, in particular in regard to the Arctic. In light of emerging security challenges, the Arctic has become of increasing interest. During 2020, the Danish chairmanship facilitated discussions on security and defence related aspects of the situation in the Arctic and information exchanges in regard to national Arctic strategies in the Nordic countries. The work of a military NORDEFCO Working Group on increased information sharing in the Arctic was delayed due to COVID-19, but is scheduled to be completed by spring 2021. Supplementing this, information sharing on joint Nordic military winter training and exercises will also be strengthened and a course catalogue on activities has been made available at the NORDEFCO webpage.\nCyber security and resillience The cyber security threat knows no borders, and to build resilience, nations need to work together. This is why strengthened Nordic cooperation on cyber security and resilience is one of the goals in Vision 2025. To achieve this, a Danish-led working group explored possible areas for Nordic cooperation within cyber security, exchanged information and shared best practices during 2020. Specifically, the working group explored best practices in regard to cyber security education in the area of the respective MoDs, including a general map- ping of challenges and lessons learned of respective national cyber security education initiatives. A similar working group on the military level focused on establishing methods and procedures of information sharing related to cyber operations, which led to enhanced coordination regarding participation in exercises and courses. The work showed that existing frameworks for collaboration could be utilized further, for example to exercise scenarios for experts already working together in the Nordic military CERTs (Computer Emergency Response Teams). During the Chairmanship, a Maritime Hybrid Warefare Webinar was held, at which the European Hybrid Centre of Excellence presented current scenarios. The purpose was to bring awareness to the risks and dangers of this type of warfare, and it showed that preparedness within a broad range of fields such as legal, technical, tactical, political as well as interagency and international cooperation would be necessary to counter this type of threat.\nInternational operations A new NORDEFCO concept for ‘information sharing about ongoing and coming operational planning’ was developed and implemented during 2020. The concept aims to enable identification of cooperation possibilities in the first phases of operational planning. A concrete example has been to explore possible cooperation in Iraq within the framework of Operation Inherent Resolve (OIR). It led to the conclusion that support from a small number of instructors on established bases could be facilitated. The question of possible NORDEFCO or Nordic-Baltic joint ventures within the framework of the NATO Mission in Iraq (NMI), as well as possible cooperation in Sahel, will be further explored.\nImplementing NORDEFCO’s Vision 2025 As regards the most central goal of NORDEFCO’s Vision 2025 – to improve the ability to act together in peace, crisis and conflict – Finland will continue the work in accordance with the decisions made during the Danish chairmanship. Last spring, military mobility was selected as the test project for examining possibilities for future cooperation during crises and conflicts. The first NORDEFCO Table Top Discussion was organised during Finland’s previous chairmanship in 2017. In 2020, a plan for similar exercises in the coming years was made and adopted. According to the plan, the Finnish chairmanship will organise a Table Top Discussion (TTD) at senior official level. This will be NOR- DEFCO’s second Table Top Discussion.""\nLet us know what you think of this report here: https://www.biedsociety.com/forum/european-union/sub-regional-european-entities', 'Topic brief: Military and the environment\nDownload as PDF · Published: March 26, 2018 · Categories: Publications, Military and the environment\nTopic brief: Military and the environment\nMilitary activities have the potential to harm the environment, and human health, in a number of highly visible ways. The tendency to focus on environmental damage caused during active hostilities often diverts attention from the impact of regular operational activities, whether at home or abroad, from operational practices during periods of occupation, and from the environmental costs of demilitarisation and legacy wastes.\nMilitaries worldwide have been slow to develop environmental policies and, where they have, they have often had to face the legacy of their own activities that in many cases took place with little regard for their environmental consequences. Efforts to determine the uptake of environmental policies globally is complex, and hampered by a lack of transparency. It is also skewed by the historical activities of different states, for example the extent of their overseas bases or the nature of their armed forces. For large militaries, the process is made more complex with elements of the organisation progressive on particular issues but regressive on others.\nMotivations and regulation\nThe development of environmental standards in military practices is ultimately constrained by the inevitable footprint of their operations. Emissions from vehicles and aircraft that are not designed for fuel economy, landscape damage and contamination from training areas, and high resource consumption in settings with minimal logistical or waste management infrastructure, are just a few of the factors that have historically underpinned arguments that military activities should be exempted from environmental protection norms.\nWestern militaries have faced increasing domestic environmental regulation in the last two decades, particularly over emissions from training areas and facilities. Frameworks governing the use and management of chemicals are also slowly forcing changes in standards, as are measures to reduce the exposure of personnel to toxic substances. Fuel and energy consumption are also being addressed, although economic and operational factors are the primary drivers. But as with all large entities, institutional inertia over accepting environmental standards is a significant barrier to change, even where environmental sustainability has demonstrable benefits for operational success.\nNevertheless the fact that many traditional military activities are inherently unsustainable ensures that greenwashing is commonplace, for example in promoting conservation narratives around military estates or overseas bases to deflect scrutiny of environmental damage. Similarly a high reliance on particular substances or practices has often led to political interference in environmental and public health assessments. Finally, the enormous costs often associated with remediation, and with it the fear of setting precedents over liability – particularly for overseas facilities – continues to undermine environmental conduct.\nIn recent years, work has been undertaken to enhance the environmental sustainability of peacekeeping operations mandated by the UN Security Council. Research, and high profile incidents such as the outbreak of cholera in Haiti in 2010, had shown that managing the environmental footprint of UN operations was vital for ensuring positive relations with host communities. Effective waste management and efforts to minimise the use of water resources in arid regions were viewed as a priority.1 The principle is now broadly accepted by UN Security Council members although implementation is still subject to cost benefit analysis.2 Consideration is also given to historical and cultural heritage in the areas of deployment and how segments of the population may be differently affected by environmental degradation.\nMilitary environmental standards have improved markedly during the last two decades, and it seems likely that standards will improve further in response to increased environmental regulation in the territories in which they operate, triggered in part by political pressure due to the military’s historical legacy of harmful practices. However the inherently unsustainable nature of many military activities, when combined with weak transparency and scrutiny, a tradition of self-policing and a persistent view of environmental exceptionalism, may place a ceiling on this process. As will the military’s view that operational success will always be more important than environmental considerations. In recent years, an expansion in the use of weakly regulated Private Military and Security Contractors to support operations has also created new challenges for environmental regulation and oversight.3\nRAND (2008) Green Warriors – Army Environmental Considerations for Contingency Operations from Planning Through Post-Conflict:\nTRW Project (2014) Pollution Politics – Power, accountability and toxic remnants of war:\nUNEP (2012) Greening the Blue Helmets: Environment, Natural Resources and UN Peacekeeping Operations:\nJohn Hamilton (2016) Contamination at U.S. Military Bases: Profiles and Responses:\nPro-Publica (2017) Bombs in Our Backyard series:\nProceedings of the European Conference of Defence and the Environment:\n- UN DPKO Environmental impact and sustainability http://peacekeeping.un.org/en/environmental-impact-and-sustainability\n- UNSC (2017) Security Council Press Statement on Environmental Management of Peacekeeping Operations:\n- Onita Das and Aneaka Kellay (2017) Private Security Companies and other Private Security Service Providers (PSCs) and Environmental Protection in Jus Post Bellum: Policy and Regulatory Challenges:']"	['<urn:uuid:e718d7f4-05b0-4147-a4e1-4b114e59b9b6>', '<urn:uuid:6b3640d1-62d2-4b55-8b2d-1a6911492cf7>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T03:34:55.284799	14	100	2594
97	What are the standard methods for calculating bond coupon payments, and how does the Green Bond Pledge initiative incorporate environmental considerations into bond financing?	Bond coupon payments are calculated by multiplying the coupon rate by the bond's face value. For instance, a bond with an 8% coupon rate and $1,000 face value would pay $80 annually. The Green Bond Pledge initiative extends beyond traditional bond calculations by requiring that all infrastructure and capital projects financed through bonds must address environmental impact and climate risk. This pledge ensures that traditional infrastructure projects like transportation, water, and energy are both adaptive and resilient to climate-related risks, while maintaining standard financial characteristics. The initiative is supported by various organizations including the Climate Bonds Initiative and follows established protocols for green bond certification.	"['How To Find A Bonds Coupon Rate\nListing Websites about How To Find A Bonds Coupon Rate\nCoupon Rate of a Bond (Formula, Definition) Calculate\n(8 days ago) The coupon rate of a bond can be calculated by dividing the sum of the annual coupon payments by the par value of the bond and multiplied by 100%. Therefore, the rate of a bond can also be seen as the amount of interest paid per year as a percentage of the face value or par value of the bond. Mathematically, it is represented as,\nWhat Is Coupon Rate and How Do You Calculate It\n(9 days ago) How Bond Coupon Rate Is Calculated Coupon rate is calculated by adding up the total amount of annual payments made by a bond, then dividing that by the face value (or “par value”) of the bond. For example: ABC Corporation releases a bond worth $1,000 at issue. Every six months it pays the holder $50.\nCoupon Rate Formula Step by Step Calculation (with Examples)\n(1 days ago)\nHow can I calculate a bond\'s coupon rate in Excel\n(5 days ago) In cell B2, enter the formula ""=A3/B1"" to yield the annual coupon rate of your bond in decimal form. Finally, select cell B2 and hit CTRL+SHIFT+% to apply …\nHow to Calculate a Coupon Payment: 7 Steps (with Pictures)\n(Just Now) If you know the face value of the bond and its coupon rate, you can calculate the annual coupon payment by multiplying the coupon rate times the bond\'s face value. For example, if the coupon rate is 8% and the bond\'s face value is $1,000, then the annual coupon payment is.08 * …\nCoupon Rate Calculator\n(5 days ago) Coupon Rate = (Coupon Payment x No of Payment) / Face Value Note: n = 1 (If Coupon amount paid Annual) n = 2 (If Coupon amount paid Semi-Annual) Coupon percentage rate is also called as the nominal yield. In other words, it is the yield the bond paid on its issue date.\nBond Price Calculator\n(7 days ago) Coupon rate is the annual rate of return the bond generates expressed as a percentage from the bond’s par value. Coupon rate compounding frequency that can be Annually, Semi-annually, Quarterly si Monthly. Market interest rate represents the return rate similar bonds sold on the market can generate.\nHow to Calculate Bond Value: 6 Steps (with Pictures)\n(4 days ago) Determine discount rate. Divide the discount rate required by the number of periods per year to arrive at the required rate of return per period, k. For example, if you require a 5% annual rate of return for a bond …\nHow to Find the Interest Rate on a Bond Budgeting Money\n(8 days ago) Multiply the coupon rate by the face value if the coupon rate is listed as a percentage. For example, a bond with a face value of $5,000 and a coupon rate of 6 percent pays a coupon rate of $300 per year. Sometimes the coupon rate is stated in dollars. If so, you can skip this step.\nCoupon Rate Calculator\n(Just Now) How to calculate coupon rate? First, determine the face value of the bond. Calculate or determine the market price of the face value of the bond. Next, determine the annual coupon payment received.\nHow to Calculate Coupon Rates Sapling\n(9 days ago) Most bonds pay the same coupon on a set schedule until the bond matures, which is when the issuer pays back the bond\'s face value and any remaining interest. You calculate a coupon rate by dividing the annual coupon payments by the bond\'s face value.\n(2 days ago) A bond\'s coupon rate can be calculated by dividing the sum of the security\'s annual coupon payments and dividing them by the bond\'s par value. For example, a bond issued with a face value of $1,000\nHow can I calculate a bond\'s coupon rate in Excel\n(1 days ago) A bond’s coupon rate is just the rate of curiosity it pays annually, expressed as a proportion of the bond’s par worth. (It’s known as the coupon rate as a result of, in days of yore, traders truly had possession of bodily paper bonds, which had literal coupons connected to them.\nBond Present Value Calculator\n(8 days ago) Annual Coupon Rate is the yield of the bond as of its issue date. Annual Market Rate is the current market rate. It is also referred to as discount rate or yield to maturity. If the market rate is greater than the coupon rate, the present value is less than the face value.\n(9 days ago) A coupon is stated as a nominal percentage of the par value (principal amount) of the bond. Each coupon is redeemable per period for that percentage. For example, a 10% coupon on a $1000 par bond is redeemable each period. A bond may also come with no coupon.\nZero Coupon Bond Calculator – What is the Market Price\n(6 days ago) Zero Coupon Bond Calculator Inputs Bond Face Value/Par Value ($) - The face or par value of the bond – essentially, the value of the bond on its maturity date. Annual Interest Rate (%) - The interest rate paid on the zero coupon bond. Years to Maturity - The numbers of years until the zero coupon bond\'s maturity date.\n25% OFF How To Find Bond Coupon Rate Verified\n(6 days ago) (2 days ago) To calculate bond coupon rates, use the formula C = i/P, where ""C"" represents the coupon rate, ""i"" represents the annualized interest rate and ""P"" represents the par value, which is the principal amount (or face value) of the bond. The coupon rate is based on a bond\'s face value, not current yield.\nBond Yield to Maturity (YTM) Calculator\n(2 days ago) On this page is a bond yield to maturity calculator, to automatically calculate the internal rate of return (IRR) earned on a certain bond.This calculator automatically assumes an investor holds to maturity, reinvests coupons, and all payments and coupons will be paid on time.\n(8 days ago) The Bond Calculator can be used to calculate Bond Price and to determine the Yield-to-Maturity and Yield-to-Call on Bonds Bond Price Field - The Price of the bond is calculated or entered in this field. Enter amount in negative value. Face Value Field - The Face Value or Principal of the bond is calculated or entered in this field.\nCalculate Coupon Rate Bond\n(1 days ago) Coupon Rate On A Bond Calculator. CODES (29 days ago) Coupon Rate Formula | Calculator (Excel Template) CODES (2 days ago) Below are the steps to calculate the Coupon Rate of a bond: Step 1: In the first step, the amount required to be raised through bonds is decided by the company, then based on the target investors (i.e. retail or institutional or both) and other parameters face value or par\nSolving for the coupon rate of a coupon bond\nHow to Calculate Reinvested Bond Interest Finance\n(Just Now) Subtract 1 and divide by the YTM rate. Multiply the result by the coupon payment amount and subtract the total amount of payments. As an example, if a bond offers a 10 percent YTM rate with 20\nVariable Coupon Renewable Note (VCR)\n(1 days ago) Coupons are expressed in the form of a percentage of the face value and are paid for the duration of the bond’s validity, which is from the issue date until the date of maturity. The sum of all coupons paid to the investor over a year, when divided by …\nBond Valuation: Formula, Steps & Examples\n(5 days ago) Bond Terms. Horse Rocket Software has issued a five-year bond with a face value of $1,000 and a 10% coupon rate. Interest is paid annually. Similar bonds in the market have a discount rate …\nHow Interest Rates Changes impact Bond Prices\n(6 days ago) A treasury bond is available at ₹1,000 and offers a 6% coupon rate for the next 10 years. You go ahead and purchase it. There are two things you need to understand here. 1. A bond’s coupon rate is the interest you will receive on an annual basis for holding the bond. This coupon rate is irrespective of whatever is the price of the bond.\n20% OFF Bond Coupon Rate Calculator Verified\n(5 days ago) BAII Plus Bond Valuation | TVMCalcs.com. COUPON (6 days ago) Draw a time line for a 3-year bond with a coupon rate of 8% per year paid semiannually. The bond has a face value of $1,000. The bond has three years until maturity and it pays interest semiannually, so the …\nCalculate Coupon Rate Of Bond\n(4 days ago) Coupon Rate of a Bond (Formula, Definition) Calculate . CODES (8 days ago) Formula The coupon rate of a bond can be calculated by dividing the sum of the annual coupon payments by the par value of the bond and multiplied by 100%. Therefore, the rate of a bond can also be seen as the amount of interest paid per year as a percentage of the face value or par value of the bond.\nCalculate Price of Bond using Spot Rates CFA Level 1\n(2 days ago) Spot rates are yields-to-maturity on zero-coupon bonds maturing at the date of each cash flow. Sometimes, these are also called “zero rates” and bond price or value is referred to as the “no-arbitrage value.” Calculating the Price of a Bond using Spot Rates. Suppose that: The 1-year spot rate is 3%; The 2-year spot rate is 4%; and\nHow to Price a Bond Using Spot Rates (Zero Curve\n(3 days ago) A better way to price the bonds is to discount each cash flow with the spot rate (zero coupon rate) for its respective maturity. Example 1. Let’s take an example. Suppose we want to calculate the value of a $1000 par, 5% coupon, 5 year maturity bond. We also have the following spot rates for the next 5 years:\nHow to Convert Bond Price to Yield Finance\n(Just Now) Step 1. Find out a bond’s price, coupon rate and par value from your broker, in a financial newspaper or in the Market Data section of the Financial Industry Regulatory Authority’s website.\nHow to Calculate the Break-Even Interest Rate on Bonds\n(3 days ago) To calculate the break-even interest rate, take (1 + 0.02) ^ 5 for the five-year bond, and (1 + 0.03) ^ 10 for the 10-year bond. The resulting numbers are 1.10408 and 1.34392, respectively.\n85% OFF what is bond coupon rate Verified CouponsDoom.com\n(8 days ago) How to Calculate Bond Discount Rate: 14 Steps (with Pictures) COUPON (3 days ago) A bond discount is the difference between the face value of a bond and the price for which it sells. The face value, or par value, of a bond is the principal due when the bond matures.\nHow to Calculate Spot Rate From Government Bonds Pocketsense\n(3 days ago) How to Calculate Spot Rate From Government Bonds. Calculating the implied spot rate on a coupon paying government-issued bond is not a complicated calculation if you have all of the necessary information. The spot rate refers to the theoretical yield on a zero-coupon Treasury security. Coupon paying government bonds\nHow to calculate bond yields\n(1 days ago) There are three main yields applicable to dated bonds: Coupon rate. This is the interest rate the bond initially pays on issue. It’s invariably given in the name of the bond. For instance Treasury 5% would have a coupon of 5%. i.e. If you bought it when it was first issued — before the price began to fluctuate in the market — you’d get\nHow to Value a Bond Using Forward Rates\n(5 days ago) We also saw that forward rates can be derived from spot rates. If so, we can also value a bond using forward rates instead of spot rates. Let’s take a specific cash flow in a bond to understand this. Say, a bond is going to pay $100 as coupon after 2 years. s 2 is the 2-year spot rate is 6%. The present value of this cash flow will be:\nHow to Calculate a Coupon Payment Sapling\n(6 days ago) In finance, a coupon payment represents the interest that\'s paid on a fixed-income security such as a bond. Par value is the face value of a bond. Calculate the annual coupon rate by figuring the annual coupon payment, dividing this amount by …\nZero Coupon Bond Value Calculator: Calculate Price, Yield\n(1 days ago)\nCoupon Rate Of Bond Formula\n(2 days ago) Coupon Rate of a Bond (Formula, Definition) Calculate . CODES (8 days ago) Formula The coupon rate of a bond can be calculated by dividing the sum of the annual coupon payments by the par value of the bond and multiplied by 100%. Therefore, the rate of a bond can also be seen as the amount of interest paid per year as a percentage of the face value or par value of the bond.\nHow to calculate yield to maturity in Excel (with Template\nHow to Calculate the Price of a Bond With Semiannual\n(7 days ago) Therefore, the example\'s required rate of return would be 2.5 percent per semiannual period. To convert this to a coupon payment, or the amount of money you\'d actually receive each period, multiply the face amount of the bond by the required rate of return.\nWhat is yield and how does it differ from coupon rate\n(3 days ago) There are two ways of looking at bond yields - current yield and yield to maturity. Current Yield. This is is the annual return earned on the price paid for a bond. It is calculated by dividing the bond\'s coupon rate by its purchase price. For example, let’s say a bond has a coupon rate of 6% on a …\nHow to calculate bond price in Excel\n(1 days ago) For example there is 10-years bond, its face value is $1000, and the interest rate is 5.00%. Before the maturity date, the bondholder cannot get any coupon as below screenshot shown. You can calculate the price of this zero coupon bond as follows: Select the cell you will place the calculated result at, type the formula =PV(B4,B3,0,B2)\nHow To Calculate YTM With Coupon Rate Our Deer\n(8 days ago) Settlement date is the purchase date of the bond. Coupon rate is expressed as an annual percentage. The bond price and face value should be expressed as a percentage of par. Face value is usually 100, and price will look like 98.472 if you paid $98,472 for a $100,000 bond. Payment frequency for most bonds is twice a year, so the value will be 2.\nCorporate Bond Coupon Rate\n(1 days ago) Find the best Corporate Bond Coupon Rate. Save today with new coupon codes and shop the latest offers available online and in stores\nBAII Plus Bond Valuation TVMCalcs.com\n(4 days ago) Draw a time line for a 3-year bond with a coupon rate of 8% per year paid semiannually. The bond has a face value of $1,000. The bond has three years until maturity and it pays interest semiannually, so the time line needs to show six periods. The bond will pay 8% of the $1,000 face value in interest every year.\nWhat is Coupon Rate\n(9 days ago) The coupon rate is calculated on the bond’s face value (or par value), not on the issue price or market value. For example, if you have a 10-year- Rs 2,000 bond with a coupon rate of 10 per cent, you will get Rs 200 every year for 10 years, no matter what happens to the bond price in the market.\nHow is the coupon rate of a bond calculated?\nThe formula for coupon rate is computed by dividing the sum of the coupon payments paid annually by the par value of the bond and then expressed in terms of percentage. Coupon Rate = Total Annual Coupon Payment / Par Value of Bond * 100%\nWhat is coupon rate and how do you calculate it?\nA bond\'s coupon rate can be calculated by dividing the sum of the security\'s annual coupon payments and dividing them by the bond\'s par value. For example, a bond issued with a face value of $1,000 that pays a $25 coupon semiannually has a coupon rate of 5%.\nHow can I calculate a bond\'s coupon rate in Excel?\nMost bonds have a clearly stated coupon rate percentage. However, calculating the coupon rate using Microsoft Excel is simple if all you have is the coupon payment amount and the par value of the bond. The formula for the coupon rate is the total annual coupon payment divided by the par value.\nHow to calculate coupon bonds?\nThe formula for coupon bond calculation can be done by using the following steps:\n- Step 1: Firstly, determine the par value of the bond issuance, and it is denoted by P.\n- Step 2: Next, determine the periodic coupon payment based on the coupon rate of the bond based, the frequency of the...\n- Step 3: Next, determine the total number of periods till maturity by multiplying the...', ""The Pledge is a joint initiative developed and designed by international climate finance and environmental groups including the Climate Bonds Initiative, Mission 2020, CDP, Ceres, Citizens Climate Lobby, California Governor's Office, California Treasurer's Office, Global Optimism, NRDC & The Climate Group.\nClimate change threatens our quality of life and poses material risk to our communities. As we build and finance new infrastructure and renovate and maintain our existing infrastructure, we must address the challenges posed by climate change.\nThe Green Bond Pledge is a simple declaration with broad and far-reaching impact. All bonds that finance long-term infrastructure and capital projects need to address environmental impact and climate risk. Green bonds contribute to beneficial environmental and climate outcomes and signal that these imperatives have been deliberately incorporated into the planning and deployment of infrastructure projects.\nLong-term infrastructure and capital projects, which are typically financed with bonds, are increasingly scrutinized by investors for sustainability. Such traditional infrastructure as transportation, water, wastewater, buildings, energy and other projects need to be adaptive and resilient to climate related risks ‚Äì and not create unintended climate problems. In addition, natural infrastructure, including forest and wetlands restoration, wildland preservation, and levees to address sea level rise, will require new attention.\nBecause green bonds have similar yields, ratings and return profiles to other fixed income investments, they provide investors with a clear way to get both economic and environmental returns without additional risk. For issuers, green bonds signal that the projects being financed have accounted for climate and other environmental risk factors.\nThe green bond market started in 2007 with initial bonds from the European Investment Bank and the World Bank. The market has grown steadily since then, with over $160 billion in green bonds issued in 2017, doubling the amount issued during the prior year. In 2017 California and New York each issued over $4 billion in 2017 to finance green projects for mass transit, green schools, clean water, land preservation and green housing; and companies as diverse as Apple, Kaiser Permanente and Southern Power Company have issued green bonds. This market is growing in size and breadth.\nSeveral resources exist to address what constitutes a green bond, including those under the certification protocols of the Climate Bonds Initiative Climate Bonds Standard and International Capital Markets Association Green Bond Principles. Other financial market participants have green bond definitional products, including Moody's, S&P Global Markets, and Bloomberg/MSCI/Barclays.\nThe Green Bond Pledge - March 2018\nIssuer Pledge: Agencies, Government and Companies\nWe agree that all infrastructure and capital projects will need to be climate resilient and where relevant, support the reduction of greenhouse gas emissions.\nWe welcome the role that green bonds can play in helping to achieve the financing of that infrastructure.\nAs a signatory to this pledge, we support the rapid growth of a green bonds market, consistent with global best practices that can meet the financing needs we face, and will issue, whenever applicable, bonds for infrastructure as green bonds.\nWe pledge to support this goal by establishing a green bonds strategy that will finance infrastructure and capital projects that meets the challenges of climate change while transforming our community into a competitive, prosperous and productive economy.""]"	['<urn:uuid:e24e1a45-8c9d-4b05-bb80-2c73ad3d835c>', '<urn:uuid:3a8ccafe-5185-4161-88a7-5ebb2c9eec02>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T03:34:55.284799	24	105	3414
98	dental crown regular cleaning appointments vs deep cleaning which is better for crown longevity	For crown longevity, regular cleaning appointments (every 6 months) are better as they are preventive maintenance that helps protect the crown. Deep cleanings are only therapeutic procedures done to address existing problems. To maximize crown lifespan, which is typically 5-15 years, patients should maintain regular cleaning appointments and examinations every six months.	"['PREVENTION TIPS and FREQUENTLY ASKED QUESTIONS\nThese prevention tips will help you have a healthy smile for life! The average patient that does everything on this page will save ten\'s of thousands of dollars and countless hours of dental chair time over their lifetime. Tips are in order of importance!\nCLEAN: Become an expert at cleaning your mouth. Have you noticed the amount of dental products in store shelves now? We can customize a simple plan for you. One that shows you what techniques, tools, and products are best for you. Remember that technique is more important than products or tools.\nREPAIR: Let us completely repair any problems that exist in your mouth so there are no gum infections, tartar, rough fillings, cavities or areas that trap plaque or food. This will also make step #1 easier.\nLIVE HEALTHY: There is a huge relationship between your general health and periodontal disease, cavities, and cancer. Improve your sleep, reduce stress, get more aerobic exercise, improve your nutrition and have regular check ups by your physician.\nALIGN & PROTECT THE BITE: Adjust your bite and/or use a night guard. Adjusting the bite, or ""equilibration"", improves the way teeth come together. This has a massive impact on the longevity of your teeth, gums and dental work. Aligning and balancing car tires is a good analogy. The night guard adds another level of protection.\nMONITOR & MAINTENANCE: Increase the frequency of your preventive maintenance appointments (""cleanings"" and check ups). For most people, we recommend cleanings every 6 months and a check up yearly. Higher risk patients may need cleanings every 3 to 4 months.\nSUGAR: Decrease or eliminate the amount and frequency of sugary foods & drinks.\nHow much does it cost? We understand that cost is a big concern for some people. Every situation is unique. The best way to know what something will cost is to get a complete treatment plan and estimate from Dr. Marquze. Also keep in mind that we do have financing available with approved credit and will happily give you all the information you need to make the necessary investments in your oral health.\nDo you take my insurance? We are always looking out for you, and that includes helping you maximize your benefits. As long as your insurance gives you the freedom to pick your dentist, you should be able to use your benefits with us. We will be glad to file for you. HMO plans prevent you from picking your dentist. We are not an HMO provider, however we are providers with select PPO plans.\nWhat is a crown? A custom made replacement for your enamel that is made of porcelain or metal. It is generally used when the tooth is too weak for a filling.\nWhat is a root canal? The disinfection of the internal part of the tooth. It does NOT involve removing the roots. It is necessary when patients let cavities get too deep, the tooth fractures into the nerve, or the nerve dies.\nWhat are dental implants? Anchors for either non-removable crowns or removable dentures. They are made of titanium alloy so they cannot get cavities. They help you chew better, look better and keep your bite from shifting by filling in missing spaces.\nHow long does an implant take? The surgical placement about 20 minutes. The healing wait time before you can begin chewing on them, 3-6 months.\nDo implants hurt? The procedure itself is painless. There is always some post operative discomfort associated with any surgical procedure, everyone is different. Some of our patients may need a 4 day supply of a prescribed pain reliever, however it is rare that one of our patients has to use pain medicine beyond 12 hours after their appointment. All patients have felt up to going to work the next day.\nWhat is the difference between a cleaning and a ""deep cleaning""? The first is preventive and the other therapeutic. Cleanings are done to keep you out of trouble and focus mainly on cleaning the teeth above the gum line. ""Deep cleanings"" are done to get you out of trouble and focus on cleaning the roots below the gum line.\nWhat are veneers? Strong, thin facings that bond onto your teeth to permanently change color or shape.\nDo I need braces? Upon your exam we will let you know what treatment is needed to correct any problems with overcrowding, underbite or overbite and more. While some people do go the route of traditional braces, many of our patients choose to enjoy the same results offered by Invisalign ""invisible"" braces. This is somethign we can discuss in detail at your appointment.\nIs bleaching safe? Yes. It does not harm your enamel or gums when used as directed, and we do have a few different options we would love to tell you more about during your visit.', 'Patients with cracked, broken, or damaged teeth might be advised by their oral health professional to get dental crowns. This procedure can dramatically help patients when it comes to easing pain and discomfort, protecting teeth from further damage, and completing smile restoration or enhancement. This guide will give patients the information they need to know if they are considering dental crowns.\nWhat are Dental Crowns?\nDental crowns are artificial teeth installed in the mouth. They can be used to complete a dental implant or can be bonded to the top of an existing tooth. Dental crowns work to protect the tooth by covering any damage to it and preventing worse damage from occurring. There are many different types of dental crowns, and they can be made from different materials to fit each patient’s needs and budget.\nDental Crowns Before and After 1\nHow Long do Dental Crowns Last?\nDental crowns usually last between five and 15 years. It is important to note that the way a dental crown is cared for will dramatically impact its lifespan. Patients looking to get the most time out of their dental crowns should minimize sticky foods and be sure to brush and floss regularly, as well as visiting the dentist at least every six months for an examination and cleaning. Additionally, habits like chewing ice or fingernails, or using teeth to open a package can also reduce the lifespan of dental crowns.\nCan Anyone Notice Dental Crowns?\nDental crowns are usually designed to not be noticed. Crowns that go in the back of the mouth might be made of gold or stainless steel, but crowns in the visible areas of the mouth will usually be made of porcelain or resin that closely resembles the appearance, color, and texture of a patient’s natural teeth. As a result, someone you know well could have dental crowns and you might not ever realize it.\nWhat Kind of Results Can Patients Expect?\nPatients should always ask to see pictures of past results when they are talking to their dentist about dental crowns. This will give patients a good idea of what to expect when it comes to their dental crown procedure. Most patients are happy with their dental crowns, and have reported less pain and discomfort, a better smile, more confidence, an easier time chewing food, and fewer complications with their oral hygiene as a result of their dental crowns.\nDental Crowns Before and After 1\nHow Are Dental Crowns Cared For?\nDental crowns are cared for just like regular teeth. Patients should brush and floss regularly, and see an oral health professional at least every six months for routine cleaning and examinations. Patients can increase the lifespan of their crowns by avoiding very hard foods, sticky foods, and destructive oral health habits like smoking, eating sticky foods, chewing on finger nails, or opening packaging with teeth.\nAre there Any Alternatives to Dental Crowns?\nThere are some alternatives to dental crowns for some patients, but, for the most part, dental crowns are the least invasive procedure that will resolve many oral health issues. Alternatives include dental implants, dentures, fillings, and other options that might not be as effective or may be much more involved when compared to dental crowns.\nAs you can see, dental crowns are a safe, effective option for many patients suffering from broken, cracked, discolored, stained, or otherwise damaged teeth. The result is less pain and discomfort and an easier time eating food. Patients also report feeling better about their smile, and an increased sense of overall well-being. The flexibility when it comes to materials and the procedure means that there is a dental crown solution for many patients.']"	['<urn:uuid:c4cef1d6-fd65-4e58-92c0-b5d165edb47c>', '<urn:uuid:eab99c28-06eb-465c-80c3-6f9567cdbdf9>']	factoid	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-13T03:34:55.284799	14	52	1425
99	Why do AIDS patients usually die from other illnesses?	Because AIDS destroys natural body protections against diseases, patients suffer from chronic illnesses that accumulate one after another. People usually don't die directly from HIV infection; rather, invading diseases gang up to waste away their bodies until they can no longer survive.	"['It\'s All In the Numbers\nDepending upon students\' grade and maturity levels, the essay, ”It’s All In the Numbers,"" may be used as teacher background information or as a student reading assignment. It is especially effective when read aloud.\nAcquired Immunodeficiency Syndrome (AIDS) is not a disease like the measles or flu, and there is no cure. It is the result of a long-term viral infection. A person with AIDS no longer has natural body protections against many diseases that circulate through the human population. People usually don’t die directly from HIV infection; rather, AIDS patients tend to suffer from chronic illnesses that accumulate one after another. Invading diseases gang up to waste away their bodies and cause great suffering until they no longer can survive. Once a person has AIDS, treatment options are mostly reactive. If a person with AIDS has pneumonia or cancer, doctors employ pneumonia or cancer treatments. Often, AIDS patients have multiple illnesses, challenging doctors to find treatments that are effective and compatible. Regardless, over time the battle will be lost.\nBecause there is no cure or vaccine, worldwide efforts are focused on preventing AIDS from spreading from one person to the next. As noted earlier, AIDS results from infection by the Human Immunodeficiency Virus (HIV), an almost unimaginably small particle of genetic material more than 800 times smaller across than a human hair. HIV is passed from human to human only through body fluid transfer. Blood transfusions, breastfeeding, and sharing of needles among drug users are common routes of transfer of HIV virus particles.\nHIV/AIDS: A Numbers Game\nOnce inside the bloodstream, the virus particle attaches itself to cells that have a particular kind of molecule, called CD4, on their surface. T cells, the white blood cells responsible for directing the body’s defense against invaders, have CD4 receptor molecules. In fact, T cells also are referred to as CD4+ cells. After attaching, the HIV virus particle injects its contents into the cell. The viral material may lay dormant for years but, eventually, it begins to multiply. Actually, the host cell does the multiplying. The particle simply provides the cell with a genetic “how-to” manual for creating copies of the virus. Each new virus particle triggers the formation of more particles. Their numbers grow until millions of HIV particles are released into the bloodstream to interact with (infect) more CD4+ cells. Once infected, CD4+ cells are less able to defend the body against disease; sometimes, they are simply overwhelmed and die. As the immune system gradually fails, the disease known as AIDS results.\nAnti-HIV treatments usually rely on a combination of three different medications that target the HIV virus itself. Because HIV is capable of rapid genetic change (mutations), it can become resistant to the treatment drugs if medications are not taken on schedule as prescribed. HIV also is difficult to treat because its genetic material becomes incorporated into the DNA of cells within the human immune system. Once inside the nucleus of a CD4+ cell, for example, HIV can remain inactive and unaffected by drugs for years. HIV’s ability to “hide” within cells makes it impossible to eliminate completely. If treatment is stopped or disrupted for any reason, HIV is able to emerge from hiding and multiply within the body again.\nKeywords: AIDS | blood | capsid | CDC | disease | DNA | epidemic | epidemiologist | epidemiology | HIV/AIDS | illness | immune system | infection | microbe | microbiology | microscope | pandemic | pathogen | replication cycle | retrovirus | RNA | SEM TEM | T-cell | viral assembly | virion | virus | white blood cell | WHO | HIV\n- Vogt, G., and Moreno, N. (2012) The Science of HIV/AIDS Teacher’s Guide. Baylor College of Medicine: Houston. ISBN: 978-1-888997-62-0\n- Scanning electron microscopic image courtesy of the National Institute of Allergy and Infectious Diseases, NIH.\nYour slide tray is being processed.\nFunded by the following grant(s)\nGrant Number: 5R25RR018605']"	['<urn:uuid:6925704b-ab8f-46b7-a6b3-5c5a5a193c6f>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T03:34:55.284799	9	42	660
100	How should business owners evaluate and select short-term insurance plans?	To choose the right short-term health insurance plan, you should assess your healthcare needs considering age, health status, and expected medical expenses. Then compare quotes from multiple insurance providers, looking at coverage options, deductibles, and premiums. Finally, verify that your preferred healthcare providers accept the short-term insurance plan you're considering.	['Short-Term Health Insurance for Small Business Owners: A Comprehensive Guide\nSmall business owners face unique challenges when providing healthcare coverage for themselves and their employees. While larger companies often have the resources to offer comprehensive health insurance plans, small business owners may need help. This is where short-term health insurance can be a valuable option. This comprehensive guide will explore the world of short-term health insurance for small business owners. We’ll discuss short-term health insurance, its benefits, and limitations, how to choose the right plan, and provide insights into the regulatory landscape.\nSection 1: What is Short-Term Health Insurance?\nShort-term health insurance, also known as temporary health insurance or STM, is a type of health coverage designed to provide individuals and families with temporary medical benefits. These plans are typically purchased for short durations, ranging from one month to a year, and are often used as a bridge to cover gaps in healthcare coverage. Small business owners may opt for short-term health insurance between jobs, waiting for their group health insurance plan to start, or simply looking for a cost-effective healthcare solution.\nSection 2: Benefits of Short-Term Health Insurance\nShort-term health insurance offers several benefits for small business owners:\n- Affordability: Short-term plans are generally more affordable than traditional health insurance, making them an attractive option for budget-conscious small business owners.\n- Flexibility: These plans allow for flexibility in choosing coverage periods, making aligning with your business’s specific needs easier.\n- Customization: Small business owners can tailor short-term plans to include the specific benefits they need, such as doctor visits, prescription drug coverage, or maternity care.\nSection 3: Limitations on Short-Term Health Insurance\nWhile short-term health insurance can benefit small business owners, it’s essential to be aware of its limitations.\n- Limited Coverage: Short-term plans may not cover pre-existing conditions, preventive care, or essential health benefits required under the Affordable Care Act (ACA).\n- No Guaranteed Renewal: These plans do not guarantee renewal, and policyholders may need to reapply and pass medical underwriting to continue coverage.\n- No Federal Subsidies: Short-term plans do not qualify for federal premium subsidies, potentially making them more expensive for some individuals.\n- State Regulations Vary: Short-term health insurance regulations vary by state, affecting plan availability and terms.\nSection 4: How to Choose the Right Short-Term Health Insurance Plan\nSelecting the right short-term health insurance plan requires careful consideration.\n- Assess Your Needs: Evaluate your healthcare needs, considering age, health status, and expected medical expenses.\n- Compare Plans: Obtain quotes from multiple insurance providers, comparing coverage options, deductibles, and premiums.\n- Check Provider Networks: Ensure that your preferred healthcare providers accept the short-term insurance plan you are considering.\nSection 5: The Regulatory Landscape\nThe regulatory environment for short-term health insurance varies from state to state and may change over time. While the federal government has relaxed short-term plan regulations, some states have implemented stricter rules to protect consumers. Small business owners must stay informed about local regulations that may impact their insurance choices.\nShort-term health insurance can be a valuable option for small business owners seeking affordable and flexible healthcare coverage. However, it’s crucial to understand the benefits and limitations of these plans and carefully assess your specific needs before making a decision. Doing so can ensure that you and your employees have access to the healthcare coverage that best suits your circumstances. Stay informed about the regulatory landscape in your state, and remember that short-term health insurance is not a long-term substitute for comprehensive coverage.']	['<urn:uuid:f3a3c940-b073-41cb-86fc-e472e466e1f9>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T03:34:55.284799	10	50	581
