qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	Work is overwhelming me. What are signs of chronic stress, and how can I manage my workload?	Signs of chronic stress include inability to concentrate, frequent headaches, heightened irritability, low self-esteem, and digestive issues. These can develop into serious physical problems like memory issues, heart disease, and extreme weight changes. To manage workload, you need to review your tasks and determine which can be removed from your to-do list. Focus on what matters most, break down larger tasks into smaller ones, and work during your peak 'thinking time' - whether morning or night. If stress becomes severe, professional help through Cognitive Behavioral Therapy (CBT) or lifestyle changes may be necessary.	['Time management is an important aspect of anyone’s life, even for students, professionals or business owners. Managing one’s time properly can increase productivity in work, save on company expenses, and find balance from work/school and other things in life. Hundreds of books, workshops, planners, to-do-list programs and classes are developed to train people about proper time management, yet people still feel the stress of deadlines, cramming for studies, and other shortcomings that may occur due to lack of time.\nThe secret of time management boils down to realizing 5 important points:\n1. Time is limited – You may be the best in your class, or a CEO for a big-time company, but you can never buy more time. You have to understand that time lost can never be made up, even if you work twice as hard the next day. You have to realize that time cannot be stretched; it only needs “managing” for you to find enough time to do all tasks you need done. The more you look for time, the more time is lost. The best thing to do to “make” time is to decide taking time from one activity and giving it to another. Setting priorities is your only option to make time work for you.\n2. Time Scheduling is Your Answer – Everyone is given 24 hours a day, subtract 14 hours for sleep, eating, preparing for school/work and travel, then you’re left with 10 hours or less to set a schedule for work/school and other activities. Depending on what works for you, notepads, calendars, journals, MS Office programs, to do lists, and other planning materials or programs. To schedule time effectively, you must put important tasks at the top of your list and other tasks that can be delegated on the very bottom.\n3. Determine when Time is not the Problem – Many people complain of lack of time to do things they need done, but sometimes, time is not the problem. People often make excuses for not doing other tasks due to other priorities. For instance, spending more time for work or school can be an excuse for not being able to exercise. Your goal is to determine when these instances occur, acknowledge it and choose priorities wisely.\n4. Take Everything One Step at a Time – The single factor that makes success a possibility is setting goals and meeting them. When creating a list of goals, include a doable timeline. Success in school or work doesn’t come overnight, so it is important to realize that time management has a huge part in everything you do. Doing small tasks that require a few minutes to complete take up space in your 24-hour schedule, so make sure to make every minute count.\n5. Quality Work Requires More Time – Whether you’re a student or an employee, competence of work requires more time. When balancing tasks, you have to schedule your work so you could accomplish more within a shorter amount of time. As you get used to scheduling priorities, your work or school grades will improve and you’ll be able to consistently provide efficient results.\nTime management shouldn’t be a pain. You just have to make a few changes and make these time management techniques a part of your day-to-day routine:\nKnow Your “Thinking Time” – Some people work well in the mornings, while others excel at night. It is important to acknowledge the time of the day when you study/work more comfortably and look for a job that offers graveyard shifts. If you’re still a student, study at night or in the morning. Save the hardest tasks for your “thinking time.”\nUse time Wisely – If you take the train to school or work, you can take advantage of free time to complete your work or studies. Extra time also comes at lunchtime and other breaks, so make sure to use it wisely. This helps in reducing time spent for work or studies, giving you more time for other activities.\nUse to-do Lists – It’s never too late to start using to-do lists. These lists may seem useless, but it helps you remember tasks that you might forget otherwise. Instead of cramming to do a task when you suddenly remember the deadline is fast approaching, it is best to keep your tasks organized in a small to-do list. Be aware that it is important to have some leeway when planning your tasks; life interruptions may occur and it is best to be flexible should any unavoidable incidents happen that could affect your schedules.\nWork Smartly – One of the most important things in time management is to develop a plan where you work within a short period, but complete more tasks. To do this, determine if you’re more comfortable in working with harder tasks than easier ones, or the other way around. Break down larger tasks into small, easy tasks to avoid getting overwhelmed with a single, hard task. Make a schedule for the tasks, set deadlines and follow it religiously.\nRemove Workload – Sometimes, people take on too much work with their available time. You have to accept that you can’t do everything all at once. Review the tasks you regularly work on and determine which tasks can be removed on your to-do list. Remember that you may feel each task you include is a priority, but with proper reflection and assessment, you can decide which tasks can go. Do what matters most first, then move on to the next task on your list.\nComplete a Long Overdue Task – If you’ve been putting off a particular task on your list, make sure to have it done once you have extra time. This will help in shortening your to-do list and free up time and mental space to focus on other tasks. Reward yourself a few minutes to enjoy the feeling of getting things done.\nAnyone who has been suffering from stress due to plenty of workload, or complain that they don’t have enough time to enjoy life must acknowledge that they have poor time management. One of the most common and serious effects of poor time management is chronic stress, which can also cause skin problems, cardiovascular disease, muscle pain, and health conditions related to the reproductive organ, stomach, lungs and the immune system. Over time, these conditions add up and may lead to heart attacks, diabetes, obesity, chronic fatigue and other serious health problems.\nStatistics show that about 90% of all doctors’ visits are related to stress. If you can acknowledge the five points listed above and perform some time management techniques that work for you, you can reduce stress dramatically. Aside from having peace of mind that the day’s work is complete, effective time management allows you to have more energy and free time for hobbies, exercise, vacations and time with family and friends. Once you’ve implemented a time management plan, it would give you a feeling of being control over your life and a sense of satisfaction and achievement.', 'At any point in our daily lives, we might experience stress. It’s quite common to the human experience. Sometimes, though, people experience stress so severe that it stops them from going about their daily lives. In these cases, mental health professionals can step in, providing guidance and intervention to help individuals deal with stress.\nWhy We Get Stressed\nStress developed early on in human existence as a way for humans to defend themselves against predators. A strong stress response acted as an alarm, warning early humans when danger was near.\nToday, we do not necessarily need this stress response as often as we did 30,000 years ago. For some of us, however, it is still ever-present, triggering us even when danger is not near. This can lead to a permanent state of stress that makes life for some very difficult.\nHow We Can Manage Stress\nThose looking to manage stress effectively can benefit greatly from turning to our clinic and one of our mental health professionals. They can develop a plan of intervention for you that involves any number of things, including therapy, medication, and more. i\nOne form of therapy that works very well in dealing with stress is Cognitive Behavioral Therapy, or CBT. As a talk therapy, CBT allows patients to explore their lives and identify ways in which their stress response gets triggered. Patients can then learn coping mechanisms for dealing with these triggers effectively.\nMaking Changes to Your Lifestyle\nIn some cases, a therapist may recommend making changes to your lifestyle. There are many things in our lives that can trigger stress, including relationships, school, and work. Sometimes something as small as an adjustment to a work schedule can make a big difference. You cannot always eliminate stressors entirely, but small changes can help.\nWhen stress is very intense or affecting your physical health, your mental health professional may recommend medication. Medications for stress can be addictive, so engaging in ongoing medication management with your psychiatrist is the best approach.\nDifferent Forms of Stress\nStress can come in chronic, acute, or episodic acute forms. Each of these has their own symptoms that manifest in very different ways.\nAs discussed above, there are some aspects of life that come with stress but that we cannot eliminate from our lives, including work and money. When stress stems from issues such as these, it can manifest as chronic stress, lasting for long stretches of time.\nChronic stress can have a strong impact on one’s physical health. Experiencing stress day in and day out can lead to insomnia and serious physical conditions.\nChronic Stress Symptoms\nSymptoms of chronic stress include:\n- An inability to concentrate\n- Frequent headache\n- Feeling you have lost control\n- Heightened irritability\n- Low self-esteem\n- Stomach/digestive issues\n- Feelings of hopelessness\nThe Effect of Chronic Stress on Physical Health\nSerious physical or comorbid problems that can come with chronic stress include:\n- Memory issues\n- Anxiety disorders\n- Heart disease\n- Extreme weight gain or loss\nHow to Treat Chronic Stress\nMany therapists will recommend a combination of therapy and lifestyle changes when a patient is dealing with chronic stress. One of the most popular therapies used is CBT. Lifestyle changes may include beginning an exercise regime or maintaining a healthy sleep schedule.\nAcute Stress Disorder\nSometimes, when an individual experiences an unexpected trauma, they may develop an acute stress disorder as a result. An acute stress disorder is a short term response to the trauma, but can still last for weeks or even months. It will not, however, remain in the long term as chronic stress would.\nAcute Stress Disorder Symptoms\nSymptoms patients can experience in acute stress include:\n- Scaring or startling easily\n- Blocking out the trauma or experiencing flashbacks\n- Panic attacks\n- Avoiding others, social settings, or triggers\n- Being unaware of surroundings\n- Remaining emotionally distant from others\nTreating Acute Stress Disorder\nRuling out any comorbid conditions may be the first step. Once a therapist determines that an acute stress disorder is the correct diagnosis, they will develop a therapeutic approach that suits your needs and many include both CBT and medication.\nEpisodic Acute Stress Disorder\nMany people who have perfectionist tendencies or have a Type A personality sometimes experience something known as Episodic Acute Stress Disorder. In this form of stress, the individual has a significant stress reaction to a comparatively small trigger.\nThe brief but intense episodes of stress in this disorder are not related to external triggers. Rather, the patient themselves is triggering the stress with their own internal thinking or expectations. While sometimes mischaracterized as “dramatic”, these episodes are very real and intense.\nThe Symptoms of Episodic Acute Stress Disorder\nSymptoms that someone with Episodic Acute Stress Disorder may experience include:\n- Muscle aches and tightness with no physical trigger\n- Digestion problems\n- Panic attacks\n- Increased heart rate\n- Uncontrollable anger\nPatients with untreated Episodic Acute stress can experience physical symptoms, including:\n- Chronic headaches\n- High blood pressure\n- Heart disease\nTreatment for Episodic Acute Stress\nTreating episodic stress often involves some form of therapy in tandem with recommended lifestyle changes. Sometimes, medication may be necessary, as well, in order to reduce anxiety.']	['<urn:uuid:7cfb53b6-65fd-4bda-8b41-3622f6b9320d>', '<urn:uuid:5ac49053-cc87-44e7-af29-7518ab6dbaf6>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T23:00:04.658849	17	93	2043
2	How is China's education system changing to accommodate new approaches, and what role does the hukou permit play in controlling urbanization?	China's education system is evolving through initiatives like the Waldorf movement, which has spread to over 200 schools nationwide, combining international education methods with Chinese classical systems. However, the hukou permit system acts as a government tool to control the pace of urbanization by restricting access to urban services. While some cities are beginning to experiment with reforms, such as offering permits based on a points system, the system remains largely intact due to powerful interests who benefit from maintaining these restrictions, including business owners who benefit from cheap labor and urban residents who don't want to share their services.	"['Few things strike fear into the hearts of Chinese parents like schooling. More than a billion people compete for space in this country, and education is the key to grabbing a piece of it. But the Chinese school system is dysfunctional at best, and its numbing reliance on rote learning and standardized tests – not to mention skyrocketing costs – has eroded society’s confidence in it. The best kindergartens in Chengdu charge anywhere from $4,000 to $10,000 USD per year, and even at that price many students languish on multi-year waiting lists. A few years ago, an administrator for the Chengdu International School confided to me that Chinese mothers were purchasing fake Zambian passports for their children in order to bypass the Chinese government’s law barring international schools from accepting Chinese students.\nDisillusionment with this system and its constraints has led to an increasing number of parents in Chengdu eschewing the standard system altogether. A decade ago, two entrepreneurs opened the first Waldorf School in Chengdu with no license, little money and almost zero knowledge of what a Waldorf education actually meant.\nFounded by German philosopher Rudolph Steiner, Waldorf works on the premise that education should create spiritually healthy individuals with a strong sense of purpose and freedom. These ideas resonate strongly with Chinese, who believe their current state-run education system does just the opposite. In 2004 the school was established in a small farmhouse in a field on the outskirts of the city. Today, it is the nexus of a movement that has spread across the nation, spawning more than 200 Waldorf schools in every major city in China, and countless “family kindergartens,” small one-room affairs that are often put together by parents stuck on waiting lists.\nLuo Xuan, 31, was one of the first volunteers at the Chengdu Waldorf School. He arrived at the school in 2004 with two suitcases holding all of his belongings, and for years taught for pennies. Last year he attended the annual Waldorf Training Seminar in Chengdu, with participants from all over the world, as well as hundreds of Chinese from all over China.\n“We don’t have the environment, or all of the necessary tools,” he says. “But we have the enthusiasm, passion and a belief that what we are doing is revolutionary. We have the feeling that we are changing something, changing the world we live in.”\nThe Waldorf movement in China, and the family kindergartens and spin-offs it has helped found, are at the front lines of China’s attempt to change the way its children learn. The state has taken some steps to revamp the system, but a top-down approach has so far produced disappointing results. Beijing’s school system is straining under crushing demand from a growing population. Shanghai has long since made it illegal for anyone but a resident of Shanghai to get a seat at a local school. There aren’t enough spaces, let alone quality choices, for clamoring middle and upper-middle class families. (Forget about the poor – or even worse, migrants – at the fringes of China’s urban spaces.)\nChengdu, however, is a perfect laboratory for reform. It’s a rich, rapidly growing city whose peripheral location keeps it somewhat shielded from the central government’s prying eyes. City leaders here are able to take risks and experiment, as the authorities in Beijing are more tolerant of trial-and-error out in the western hinterlands. Whether those experiments pan out is another matter.\nSo far, several initiatives have fallen flat. Attempts to internationalize Chengdu’s school system involved little more than sending Chinese teachers abroad for a semester, or inviting foreign teachers to lecture in Chengdu. The key, says Richard Mast, former principal of the Leman International School in Chengdu, is to understand that the school system is a reflection of the culture. There are no quick fixes. If the government simply takes an international system and plops it down onto Chinese schools, teachers will be lost, children confused and parents increasingly frustrated that the price they pay for internationalized education is just the Chinese system in Western clothing.\nThat’s why Waldorf is gaining traction in China. Chinese see a reflection of their own culture in the system. Steiner studied Buddhism and other Eastern philosophies, and it shows in little ways, such as Steiner’s belief in reincarnation, and his adoption of Buddhist ideas like karma and the Eight-Fold Path throughout his writings.\nAt the same time, many young Chinese teachers are going back to the Chinese classical system known as guoxue, emphasizing art, calligraphy, music and philosophy. Guoxue puts them in touch with the classic teachers, Confucius and Mencius, sages like Lao Tzu and traditional Chinese medicine. This dovetails with Waldorf philosophy, which incorporates esoteric elements such as teaching math through art, language through dance and psychology through colors. Together this disparate community is working to concoct an alternative that is international, yet essentially Chinese.\n“We do not want to transform the culture,” says Benno Nijhuis, a Dutch teacher and trainer for the Waldorf school system who has spent time training teachers in China. “We bring a new way of thinking that can cooperate with the philosophies of China, to make the culture stronger. Chinese art, for example, is a core part of our curriculum, as are many Taoist ideas. The systems complete each other.”\nIt’s a work in progress. Family kindergartens still lack licensing and funding, and the parents involved in these small enterprises have little real pedagogic knowledge. And affordability isn’t a foregone conclusion – the Waldorf School in Chengdu raised its tuition from $500 USD in 2008 to more than $3,500 USD today. A new Waldorf school, built on several acres of forest land outside of the Chengdu city limits, currently charges around $9,000 USD. What began as a middle-class response to a lack of options already risks being taken over by the wealthy.\nBut in this regard, the schools have little choice. Without rich, well-connected parents, many of them would be shut down by paranoid authorities afraid of losing control over the education system. The original Waldorf school in Chengdu managed to obtain a license from the government, and get a proposed rail line moved that would have bisected the school. “Waldorf in China is now a blazing fire,” says Liu Li, the Chengdu Waldorf school’s principal at a recent seminar. “The key now is to determine how it is blazing, blazing at what level, and where it is blazing to.”', 'A hukou is a permit given by the Chinese government which entitles people to live in a certain area and receive government services. It\'s also one of the most significant social issues in contemporary China, branded by some as a form of apartheid. Under the system, every person is broadly classed as either a rural or an urban resident, and is registered to access government services in the area of their birth and nowhere else. This means that roughly two hundred million of the migrant workers who live in cities and manufacture goods for export to the western world live as second-class citizens, unable to access schools, hospitals or public housing.\nChina\'s economy has developed rapidly as it has implemented market reforms, and with this development has come a growing divergence between the standard of living and opportunities available to people in the cities and the countryside. There are many advantages to dwelling in the cities, not the least of which is the opportunity to work in the modern part of the economy. Chinese factory jobs may be low-paid and hard, but clearly they\'re appealing to rural residents or China wouldn\'t have urbanized so rapidly in the last decade. On top of access to these jobs and a more modern way of life, government services also tend to be better in the cities. Far fewer rural Chinese attend university than do the children of city-dwellers, for instance.\nFactors such as these have underpinned the enormous urbanization that has taken place in China in recent decades. Roughly 50% of the country\'s population, who themselves make up about one-sixth of humanity, now live in cities. The hukou system is an attempt by the Chinese government to exert some control over the movement of people between the cities and the countryside by obliging rural residents who want to move to the cities to apply for residency permits.\nWe often think of China as having ""gone capitalist"", and it has in many ways. But restrictions such as these on the free movement of people within the country, which are unthinkable somewhere like Britain or the United States (but existed in the Soviet Union), highlight the hybrid system that in fact exists. The Chinese government is trying to manage an enormous process of social and economic change while maintaining Communist Party rule, and it still subordinates the free market to the interests of social stability.\nThe rationale for the hukou system is to discourage too rapid a rate of urbanization and to keep a lid on the cost of government services in the urban areas. China is undergoing one of the largest migrations in the history of humanity, and if the government had to expand its service infrastructure at the same rate then it would quickly collapse. Furthermore, it would be politically difficult to explain to long-term residents of somewhere like Beijing or Shanghai why the quality of their services and the resources available should be rapidly diluted in an attempt to cater to outsiders. Excluding hundreds of millions of new arrivals in the cities from the housing market also keeps a lid on house prices. By making life in the cities harder, it discourages too rapid a rate of migration which might lead to pervasive unemployment and corresponding instability.\nIn reality, however, the hukou system has started to fray at the edges. Technically, the hukou system mandates that Chinese live in the area for which they have a residency permit, but this system has broken down due to the need for cheap labour in the cities. Cities won\'t grant hukou, but nor will they enforce the residency critieria. Hundreds of millions of Chinese live on the margins of urban society, powering the factories that are pushing the economy forward but unable to access services and living in a precarious legal status that makes them liable to sanctions should they incur the displeasure of the authorities. As their children cannot attend schools in the urban areas (hukou is hereditary, meaning even if you have a child in a city - and good luck finding a hospital, by the way - they still have a rural hukou), they must either stay with relatives in the village, be home-schooled, or attend an illegal school liable to be shut down by the authorities.\nThe hukou hence also serves as a means of social control; urban migrants, a major source of potential social unrest in China, are discouraged from strikes or other actions because they can so easily be sent back from where they came. Furthermore, the Chinese authorities have used hukou as a means to blackmail people into ""good behaviour"" - after riots in the city of Zengcheng earlier this year, the authorities offered hukou to anyone informing on the rioters. The police in Beijing recently temporarily shut down many of the illegal schools used by the children of migrant workers in an apparent attempt to remind them of the precariousness of their existence and encourage them to return to rural areas as the country\'s economy slows, rather than have them knocking about in the cities where they might be a force for instability.\nReform of hukou has been promised for years, and the need has only grown greater as urbanization continues apace and capitalism continues to generate winners and losers and corresponding inequalities. Nor has it escaped anyone\'s notice that the class system hukou creates is so at odds with Communist ideology. Some cities are beginning to experiment with reform, offering permits based on a points system. As with so many things in contemporary China, however, the interests weighing against reform - business owners who benefit from cheap labour, residents with urban hukou who don\'t want to share their services - are formidable. As the system generates an increasing amount of social unrest and ever-more blatant inequalities, eventually something will have to give - the question is what, when, and with what consequences for China\'s development. Challenges of this magnitude took western economies decades to overcome; don\'t expect China to be any quicker.']"	['<urn:uuid:93407ae6-9a60-41a3-9468-5be075bc8174>', '<urn:uuid:973083e4-4c29-40c3-9b61-ffdfbfe8c0c0>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T23:00:04.658849	21	100	2078
3	How did the Battle of Midway impact Japanese naval strength and American morale?	The Battle of Midway severely weakened Japanese naval strength by causing them to lose four fleet carriers along with their crews, as well as three battleships, three transports, and four cruisers. While Japan still maintained a larger naval force than the U.S., their reduced fleet and expanded territory made it nearly impossible to concentrate their navy for a single, concentrated attack. For American morale, this battle represented the first great Allied victory since Pearl Harbor and was seen as partial revenge for the December 1941 attack. The success at Midway both halted Japanese advances and lifted the spirit of the whole nation, especially significant as it occurred near the six-month anniversary of Pearl Harbor.	"['USS GEORGE H.W. BUSH (CVN 77) Hosts 68th Anniversary of the Battle of Midway Commemoration\nFrom USS GEORGE H.W. BUSH Public Affairs\nNORFOLK, Va. (NNS) -- USS GEORGE H.W. BUSH (CVN 77) hosted two World War II veterans and hundreds of Sailors from across the Hampton Roads area, as they observed the 68th anniversary of the historic Battle of Midway on the flight deck of the aircraft carrier June 4.\nThe commemoration, hosted by Commander, United States Fleet Forces, Adm. J.C. Harvey Jr., was attended by Battle of Midway veterans William Eckel and Howard Snell. Both men served aboard ships during the battle, and traveled to Norfolk to attend the ceremony.\nThe Battle of Midway, which is often referred to as the turning point of World War II, took place June 4-7, 1942, when the Japanese sent the majority of their naval force to capture Midway Island, which was being used by U.S. forces as an airfield. The battle was primarily fought by aircraft launched from aircraft carriers. By the battle\'s end, the Japanese had to retreat after losing vital air superiority. The U.S. lost the carrier Yorktown while four Japanese fleet carriers were lost along with their crew.\nThe commemoration featured musical selections played by Fleet Forces Band, a moment of silence, remarks from Harvey and Kilcline, and an invocation and benediction led by Cmdr. Cameron Fish, command chaplain. A vintage TBM-1 Avenger conducted a fly over during the ceremony. The Avenger got its combat debut during the decisive battle and is also the same model of aircraft flown by the aircraft carrier\'s namesake George H.W. Bush.\nThe ceremony also featured a wreath-laying presentation, as Harvey was joined by Eckel and Snell to pay homage to the brave men who lost their lives during the battle.\n""It was important to me that this event be by Sailors, about Sailors,"" Harvey said. The event that took place 68 years ago today was done by young American Sailors and pilots wearing dungarees and khakis. So to make that connection with them we are wearing our flight suits, our flight deck jerseys along with our Navy working uniforms, to pay honor to those young Sailors who stared everything in the face, when everything was on the table and rose to the occasion.""\n""We come together today to honor the brave men who fought to defend Midway, they were just ordinary Americans who responded to the call to serve their nation, but at Midway they were heroes,"" said guest speaker Commander, Naval Air Forces Pacific, Vice. Adm. Thomas J. Kilcline.\nFish, whose father served in the Navy during World War II, recalled the huge impact that one naval battle had on not just the Navy, but countless individuals as well.\n""The battle changed the tide of the war,"" he said. ""I remember my father saying that things looked bleak. The success at Midway both halted the advance of the Japanese and lifted the spirit and morale of the whole nation. The Battle of Midway affects me very personally because it affected my father and mother. If it were not for that victory, I might not be here today,"" he said.\nKilcline said that our victory was not without a cost, we lost the aircraft carrier Yorktown along with 145 of our aircraft and 307 Americans lost their life in that battle while paying the ultimate cost for victory.\nFish also explained the significance of having two veterans on board for the commemoration.\n""Having the veterans in attendance was absolutely wonderful,"" he said. ""It\'s an honor and a privilege and extremely humbling to have them. These men were there, during the climax of World War II, and now they\'re here today.""\nHarvey discussed the importance of the commemoration being held on the Navy\'s newest aircraft carrier.\n""It was important to me that we commemorate this day from the modern version of the Hornet, Yorktown and enterprise, which struck the decisive blow to the Japanese fleet and tie together what those Sailors did then to what our Sailors are doing now,"" Harvey said.', 'Six months after Japan’s brutal attack on Pearl Harbor, the U.S. Navy has defeated the Japanese fleet at Midway Island in the Pacific. The battle started on June 4 and lasted four days, after which the U.S. was able to claim victory.\nOnly limited details of the battle are available at this time, however it has been confirmed that three Japanese aircraft carriers have been sunk; another two carriers were reported damaged. Japan also lost three battleships, three transports, four cruisers and several planes. These naval casualties, taken together with the losses Japan has suffered in other recent battles (in the Coral Sea and elsewhere in the Pacific), mean the Japanese navy has taken a hard hit.\nA German Transocean broadcast recorded a Tokyo newspaper stating: “One cannot always expect victories, but must also be able to stand losses. The Japanese Commander in Chief… received the news of the loss of two of his vessels quite calmly. Every leader of a great fleet must show similar calm.”\nEven with the recent destruction of various warships and planes, the Japanese still have a larger naval force than the U.S. However, with more territory to cover and fewer vessels available, it would be nearly impossible for Japan to concentrate the entirety of its navy on a single, concentrated attack against the Allies. This helps put the U.S. in a much stronger position.\nThis battle is being viewed (and celebrated) as the first great victory for the Allies since the U.S. entered the war in December 1941. What makes the battle even more significant is that it took place relatively close to Pearl Harbor, and culminated on the six-month anniversary of the surprise attack by the Japanese on the U.S. Since that day at Pearl Harbor, many Americans have been waiting for the chance to strike back – and the victory at Midway proved to be the retribution the country needed.\n“A momentous victory is in the making,” said Admiral Chester W. Nimitz, commander of the United States Pacific Fleet. “Pearl Harbor has now been partially avenged.”\nAfter the recent string of Allied losses, a big win is exactly what was needed to raise morale – for those actively involved in the war and those watching (and waiting) desperately from the sidelines. The U.S. citizens at home finally get to see their country hit back and succeed after months of defeat.\nIt will take the Japanese months, if not longer, to replace all the ships and planes lost in the battle at Midway Island. Perhaps equally important, this battle proves that Japan is not invincible—this American victory in the Pacific may very well be a turning point in the Second World War.\n“A MOMENTOUS VICTORY.” (1942, June 8). New York Times, p. 14.\nClausen, W. B. (1942, June 7). “One U.S. Carrier Damaged; Our Forces Still Battering Fleeing Enemy; Wounded Jap Vessels Far From Base.” The Washington Post, p. M1.\nLindley, E. (1942, June 10). “Midway Battle: Small Aircraft Carriers.” The Washington Post, p. 9.\n“Midway Victory.” (1942, June 8). The Washington Post, p. 8.\nNorris, J. G. (1942, June 6). “Enemy Repulsed, Says Nimitz but Great Victory Is Not Yet Won.” The Washington Post, pp. 1-2.\n“TOKYO PRESS HINTS AT MIDWAY DEFEAT.” (1942, June 10). New York Times, pp. 1-6.']"	['<urn:uuid:ea8f8a3f-607e-4502-b8d3-d0499f0b504c>', '<urn:uuid:0a697412-a3c0-4109-9d04-9d40660b9787>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T23:00:04.658849	13	114	1226
4	How do opt-out requirements differ between TCPA and CAN-SPAM?	Under TCPA, opt-out requests must be honored but no specific timeframe is mentioned. In contrast, CAN-SPAM requires opt-out mechanisms to remain active for at least 30 days after sending a message and opt-out requests must be honored within 10 business days. Additionally, CAN-SPAM specifies that opt-out processes must be free, require only an email address, and be limited to sending a reply email or visiting a single webpage.	"['Telephone Consumer Protection Act (TCPA) FAQs for Insurance Agents\nQ. What is the Telephone Consumer Protection Act?\nA. The Telephone Consumer Protection Act, or TCPA, is a federal consumer privacy law. It regulates telephone calls to landlines and mobile phones, facsimiles, and text messages. The TCPA applies to calls to both individuals and businesses, including telemarketing calls and calls made for other business purposes, like informational calls. The legal requirements are more restrictive for certain types of calls, such as prerecorded calls, artificial voices, SMS text messages, and calls made to mobile phones.\nQ. Why do producers need to comply with the requirements of the TCPA?\nA. The TCPA applies to any businesses making outbound calls, faxes, or text messages. Violations of the TCPA are subject to lawsuits with damage awards of $500 to $1,500 for each individual call, text, or fax. There is no statutory cap on these damages. In 2014, a TCPA lawsuit settlement involving a credit card issuer totaled $75.5 million. In addition, the Federal Trade Commission (FTC), Federal Communications Commission (FCC), and state attorneys general, as well as other enforcement agencies, can enforce violations of the TCPA.\nQ. When do producers need consent from the recipient to make a call to a landline (such as a residential or business line)?\nA. If a call to a landline uses an artificial or prerecorded voice and is a telemarketing message, you must have the recipient’s prior express written consent.\nIf a call to a landline does not use an artificial or prerecorded voice in whole or in part consent is not required. Or, if a call to a landline does not include a telemarketing message in whole or in part, no consent is required.\nQ. When does a producer need consent from the recipient to make a call to a mobile or cellular phone?\nA. It depends on whether the call is for a telemarketing or informational purpose.\nIf a call to a mobile or cellular phone contains a telemarketing message and uses an artificial or prerecorded voice, you must have the recipient’s prior express written consent. Similarly, if a call is a telemarketing message and is placed with an “automatic telephone dialing system,” or ATDS, you must have the recipient’s prior express written consent.\nIf a call to a mobile or cellular phone does not contain a telemarketing message, but uses an artificial or prerecorded voice or is placed with an ATDS, you must have the recipient’s prior express consent.\nQ. When does a producer need consent from the recipient to send a text or SMS message to a mobile or cellular phone?\nA. A text message is subject to the same requirements as a prerecorded voice call or a call placed to a mobile phone with an ATDS. In other words, if the text message constitutes telemarketing, the recipient must provide prior express written consent. If the text message does not constitute telemarketing, the text still is subject to prior express consent.\nQ. When does a producer need consent from a recipient to send a fax?\nA. If the fax constitutes telemarketing, the recipient must provide prior express consent or there must be an “established business relationship” with the recipient. If the fax does not contain any telemarketing content, no consent is required.\nQ. How does the TCPA define “telemarketing?”\nA. Telemarketing calls include those that offer, market, or promote products or services to consumers or that have a telemarketing purpose. If a call is made to induce the purchase of goods or services, then or in the future, it is a telemarketing call.\nThe TCPA telemarketing rules apply to both “telemarketing” and “advertisements.” Federal regulations define “advertisement” as “any material advertising the commercial availability or quality of any property, goods, or services.” In turn, “telemarketing” means “the initiation of a telephone call or message for the purpose of encouraging the purchase or rental of, or investment in, property, goods, or services, which is transmitted to any person.”\nAs a general rule, calls that are not purely informational in purpose and message constitute telemarketing. This means that dual-purpose calls (i.e. calls that have both an informational component and a telemarketing purpose) are telemarketing. Also, even if a consumer is called with a non-telemarketing purpose, once a sales component or upsell is introduced on the call, it becomes a telemarketing call.\nQ. What does “prior express written consent” mean?\nA. The term “prior express written consent” means an agreement, in writing, bearing the signature of the person called. The agreement must clearly authorize the receipt of advertisements or telemarketing messages using an automatic telephone dialing system or an artificial or prerecorded voice. In addition, the agreement must include the telephone number authorized to receive such advertisements or telemarketing messages. For additional information, including the validity of electronic consent and the required contents of a valid, written consent, see the enclosed Manual.\nQ. What does “prior express consent” mean?\nA. Neither the TCPA nor the FCC Rules expressly define “prior express consent,” but the FCC has ruled that “persons who knowingly release their phone numbers have in effect given their invitation or permission to be called at the number which they have given, absent instructions to the contrary.”\nOn the basis of this definition, consumers who voluntarily provide their mobile telephone number may be called on their mobile phone for non-telemarketing calls without providing that consent in writing. On the other hand, telephone numbers purchased from a third party (such as a data broker) or the Internet (i.e., Google), do not constitute prior express consent.\nQ. What is the “Do Not Call” Registry mentioned in the Attestation and how does it apply to all of this?\nA. The National Do Not Call Registry is a list of phone numbers from consumers who have indicated their preference to limit the telemarketing calls they receive. The Registry is managed by the Federal Trade Commission (FTC), the nation’s consumer protection agency. It is enforced by the FTC, the Federal Communications Commission (FCC), and state officials.\nThe DNC Registry applies to any telemarketing call, including telemarketers who solicit consumers on behalf of third-party sellers. It is against the law to call any number on the National DNC Registry unless the consumer agreed in writing to receive calls placed by or on behalf of the seller or unless there is an existing business relationship with the consumer.\nIt is also against the law to call any person whose number is within a given area code unless the telemarketer has paid the annual fee for access to the portion of the National DNC Registry that includes telephone numbers within that area code. If you are required to use the Registry, you have to update your records by comparison with numbers on the Registry at least every 31 days.\nTo access the DNC Registry, go to www.telemarketing.donotcall.gov. To learn more, the FTC maintains an FAQ website for telemarketers at https://www.ftc.gov/tips-advice/business-center/guidance/qa-telemarketers-sellers-about-dnc-provisions-tsr.\nQ. For calls and texts to mobile phones, what is an automatic telephone dialing system or ATDS?\nA. TCPA regulations define an ATDS as “equipment which has the capacity:\n(A) to store or produce telephone numbers to be called, using a random or sequential number generator; and\n(B) to dial such numbers.”\nThe FCC has declared that predictive dialers are “automatic telephone dialing systems.”\nIf the dialing platform in use has the “capacity” to store/produce numbers in random or sequential order, whether utilized or not and has the “capacity” to dial without human intervention, whether utilized or not, it is generally considered an ATDS. As a result, nearly all modern phones or dialing platforms are an ATDS.\nQ. Are there other requirements to the TCPA of which producers should be aware?\nA. Yes. This Frequently Asked Questions document is merely an introduction to the TCPA. You must familiarize yourself with and ensure your organization and employees’ compliance with all of its requirements. Among those requirements are restrictions or prohibitions on outreach to numbers registered with the TCPA’s National Do-Not-Call Registry, Opt-Out Requirements for recipients, recordkeeping rules, and restrictions on the times of day and frequency with which telephone calls, text messages and/or faxes may be initiated.\nQ. In addition to the TCPA, are there other laws with which producers are required to comply?\nA. Yes. There are additional federal laws governing outreach to consumers, such as the CAN-SPAM Act, which regulates both text messages and emails. Further, there may be state laws applicable in a particular state, which may also regulate telephone calls, text messages, and faxes in a manner that is more restrictive than the TCPA.\nQ. How can I learn more? What if I have questions?\nA. For any questions about your legal obligations under the TCPA or other telemarketing laws, you should consult your lawyer. In addition, the FCC publishes guidance at https://www.fcc.gov/general/telemarketing-and-robocalls.', 'The CAN-SPAM Act:\nThe CAN-SPAM Act (Controlling the Assault of Non-Solicited Pornography and Marketing Act of 2003) supercedes the various conflicting state laws for the regulation of email. When sending emails you need to make sure you comply with the CAN-SPAM Act of 2003. For more information on the CAN-SPAM Act, go to: https://www.ftc.gov/tips-advice/business-center/guidance/can-spam-act-compliance-guide-business\nWe are completely permission-based. All of our services are in full compliance with CAN SPAM laws. We supply a CAN-SPAM compliance guarantee on all orders. We can help you maintain comprehensive opt-out lists, properly identify and clean bounce-back e-mail addresses, as well as implement and manage permission and frequency rules for your e-mail database.\nNevertheless, the CAN-Spam Act contains requirements that must be met by all mailers regardless of existence of a prior business relationship with the recipient. All companies that send commercial email must:\n• Do Not use subject headings intended to mislead the recipient into opening the message.\n• Use a reply address that will be active for at least thirty days following the transmission of an email message.\n• Include a physical postal address in the body of each message.\n• Include a clear notice that the message being sent is an advertisement or solicitation.\n• Include clear instructions in the body of the message detailing how to opt-out of subsequent mailings\n• Honor all opt-out requests within ten days and not transfer, sell, lease, or exchange the email address of any recipient that has made an opt-out request.\nAll of the above apply to both solicited and unsolicited commercial mailings with one exception. Mail sent to recipients at their consent (opt-in newsletters, alerts, etc…) does not need to contain the disclaimer labeling the message as an advertisement or solicitation. Damages under this Act can be reduced if policies and procedures designed to prevent such violations have been established and implemented, and a violation occurred despite reasonable effort intended to maintain compliance with the aforementioned policies.\nSince most legitimate email marketers honor removal requests and do not send mailings by hijacking open relay servers or write misleading subject lines, the two key issues to address before the New Year are the inclusion of a physical postal address in the message, and the inclusion of a disclaimer identifying the message as a solicitation or advertisement, should one be required.\nInt\'l data compile clients or clients\' users may not use Int\'l data compile system to send unsolicited emails (SPAM) for any purpose commercial or non-commercial. An email will be considered spam if the list member has not specifically granted permission for the list owner to send them email. If your email list is derived from harvesting, purchase (even with the original list owner\'s permission), or any other means other than direct subscription, any email sent to the list will be considered unsolicited (SPAM).\nInt\'l data compile will investigate all complaints of unsolicited email. Int\'l data compile reserves the right to suspend or cancel your list hosting service for any email we determine to be unsolicited. It is the list owner\'s responsibility to keep documentation of all opt-in email subscriptions as proof of request.\nIf you are a list member and feel that you have received unsolicited email, please complete our complaint form.\nDouble Opt-In Policy:\nInt\'l data compile requires a double opt-in subscription method for all new list members. A new member shall not receive email from the list until the member receives and replies to a single subscription request from the requested member address. The original subscription request may not contain advertising or information other than the subscription request and list owner contact information.\nAll list messages must contain unsubscribe instructions in the email headers and/or the body or footer of the email message itself. Any member who requests to be removed manually from your list must be removed within 10 business days of the removal request.\nCAN-SPAM Law: A Brief History\nCAN-SPAM Law is a shortened version of the name of Public Law No. 108-187, which was signed into Law by President George W. Bush on December 16, 2003. The full name of the Law and bill was Controlling the Assault of Non-Solicited Pornography And Marketing Act of 2003. Of course, the name was also a play on the fact that unsolicited and unwanted email is typically referred to as ""spam."" The Law gives the U.S. Federal Trade Commission the right to enforce the standards of the law (which we\'ll discuss in detail below). You may think, based on the amount of unwanted and unsolicited email that you receive daily, that the Law is not particularly effective. However, as a sender of email, you are still governed by it. If the proper complaints are filed against you and you are found to be in violation of CAN-SPAM Law, then you are subject to large fines. Fortunately, being in compliance of CAN-SPAM Law is quite simply if you follow a few basic rules.\nWhat Type of Email Sending Does CAN-SPAM Law Cover?\nIt\'s also misleading to think that CAN-SPAM Law only applies to large bulk email sends. CAN-SPAM Law covers all commercial email messages. What does that mean? According to the wording of the Law, it means ""any electronic mail message the primary purpose of which is the commercial advertisement or promotion of a commercial product or service."" That includes email that promotes content on a commercial website, so if your website makes any money at all and your email links back to it, you are liable under CAN-SPAM Law.\nDo you use email in your business? The CAN-SPAM Act, a law that sets the rules for commercial email, establishes requirements for commercial messages, gives recipients the right to have you stop emailing them, and spells out tough penalties for violations.\nDespite its name, the CAN-SPAM Act doesn’t apply just to bulk email. It covers all commercial messages, which the law defines as “any electronic mail message the primary purpose of which is the commercial advertisement or promotion of a commercial product or service,” including email that promotes content on commercial websites. The law makes no exception for business-to-business email. That means all email – for example, a message to former customers announcing a new product line – must comply with the law.\nEach separate email in violation of the CAN-SPAM Act is subject to penalties of up to $40,654, so non-compliance can be costly. But following the law isn’t complicated. Here’s a rundown of CAN-SPAM’s main requirements:\n1.) Don’t use false or misleading header information. Your “From,” “To,” “Reply-To,” and routing information – including the originating domain name and email address – must be accurate and identify the person or business who initiated the message.\n2.) Don’t use deceptive subject lines. The subject line must accurately reflect the content of the message.\n3.) Identify the message as an ad. The law gives you a lot of leeway in how to do this, but you must disclose clearly and conspicuously that your message is an advertisement.\n4.) Tell recipients where you’re located. Your message must include your valid physical postal address. This can be your current street address, a post office box you’ve registered with the U.S. Postal Service, or a private mailbox you’ve registered with a commercial mail receiving agency established under Postal Service regulations.\n5.) Tell recipients how to opt out of receiving future email from you. Your message must include a clear and conspicuous explanation of how the recipient can opt out of getting email from you in the future. Craft the notice in a way that’s easy for an ordinary person to recognize, read, and understand. Creative use of type size, color, and location can improve clarity. Give a return email address or another easy Internet-based way to allow people to communicate their choice to you. You may create a menu to allow a recipient to opt out of certain types of messages, but you must include the option to stop all commercial messages from you. Make sure your spam filter doesn’t block these opt-out requests.\n6.) Honor opt-out requests promptly. Any opt-out mechanism you offer must be able to process opt-out requests for at least 30 days after you send your message. You must honor a recipient’s opt-out request within 10 business days. You can’t charge a fee, require the recipient to give you any personally identifying information beyond an email address, or make the recipient take any step other than sending a reply email or visiting a single page on an Internet website as a condition for honoring an opt-out request. Once people have told you they don’t want to receive more messages from you, you can’t sell or transfer their email addresses, even in the form of a mailing list. The only exception is that you may transfer the addresses to a company you’ve hired to help you comply with the CAN-SPAM Act.\n7.) Monitor what others are doing on your behalf. The law makes clear that even if you hire another company to handle your email marketing, you can’t contract away your legal responsibility to comply with the law. Both the company whose product is promoted in the message and the company that actually sends the message may be held legally responsible.']"	['<urn:uuid:90479884-93b7-4f32-8241-779475d5087b>', '<urn:uuid:bb886c9e-ce40-4353-9056-155b59c39686>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T23:00:04.658849	9	68	3007
5	web development expert here what responsibilities ux developers take on besides visual design	UX developers have several key responsibilities beyond visual design. They create interactive prototypes, ensure user interface accessibility, optimize page performance to reduce bounce rates, conduct testing and quality assurance, and stay current with the latest UX trends and technologies. They also work closely with UX designers and front-end developers to understand project goals and technical requirements.	['The success of any digital product or service depends on the seamless integration of design and development, with a user-centric approach being the key to success. Designers focus on the visual aesthetics and user experience of a product. UX developers, on the other hand are responsible for creating the functionality and features.\nHowever, this division of labor can sometimes create a gap in back-end web developers that affect the final product’s usability and success. This is where UX developers play a critical role in bridging the gap between the two teams.\nUX developers act as a liaison between designers and developers, ensuring that the final product is user-friendly and functional. They have a skill set with a deep understanding of both design principles and technical development requirements. This enables them to translate design ideas into functional code. They work closely with designers to understand the user experience and develop solutions that meet both design and technical requirements.\nThis article will explore the critical role of UX developers in bridging the gap between design and development in UX\nTABLE OF CONTENTS\n- Evolution Of UX Developers\n- Understanding the Role Of UX Developer\n- Challenges Of Design And Development Gap\n- Benefits Of Bridging the Gap Between UX Designers and UX Developers\n- Strategies to Bridge the Gap\n- Role of UI Developers in UX\n- Relationship Between UX and UI\n- Vital Role of Front-End Developers\n- Final Thoughts\n- Frequently Asked Questions\nEvolution Of UX Developers\nThe field of UX and web development has evolved significantly over the years as the importance of user-centered design in web development has become increasingly recognized by the tech industry. Here is a brief overview of the career path and evolution of UX developers:\nIn the early days of the web, web developers were responsible for both design and development. There was no clear separation between the two roles, and user-centered design was not yet a priority.\nThe Emergence of Web Designers\nAs the web grew in popularity, designers began to emerge in separate roles from developers. They were responsible for creating the look and feel of a website but often had limited understanding of development.\nRise of UX Designers\nIn the early 2000s, user-centered design began to take hold, and UX designers emerged in a separate role from web designers. They were responsible for designing the user experience of a website or application, including the user interface, information architecture, and interactions.\nIntroduction of UX Developers\nAs websites and applications became more complex, the need for developers with a strong understanding of user experience grew. UX developers emerged in a separate role from web developers, with a focus on building user interfaces and interactions that were optimized for the user experience.\nIntegration of UX Design and Development\nIn recent years, there has been a growing trend towards integrating UX design and development, with designers and developers working closely together throughout the design and development process.\nToday, UX developers are an essential part of any team focused on user-centered design. They bring together the skills of both UX designers and front-end developers, creating interfaces that are not only visually appealing but also optimized for the user experience.\nAs user research in the field of UX continues to evolve, the role of UX developers will likely become even more critical in building effective, user-centred digital experiences.\nUnderstanding the Role of a UX Developer\nA UX developer is a professional who bridges the gap between design and development in the user experience (UX) design software development process. The role of a UX developer is to ensure that the user interface (UI) is not only visually appealing but also optimized for the user experience.\nThe specific skills and responsibilities of a UX developer can vary depending on the organization and the project, but typically include the following skills:\nUX developers work closely with UX designers, front-end developers, and other stakeholders to understand project goals, user needs, and technical requirements.\nThese professionals are responsible for creating interactive prototypes to test and refine user interfaces and interactions.\n3. Front-End Development\nUX developers also ensure that user interfaces are accessible to all users, including those with disabilities.\nPage performance is a crucial part to work on in order to ensure a reduced bounce rate on a landing page. UX developers optimize user interfaces for performance, ensuring that pages load quickly and interactions are smooth.\nUX developers are involved in testing and quality assurance, ensuring that user interfaces are bug-free and meet technical requirements.\n7. Continuous Learning\nUX developers stay up-to-date with the latest trends and technologies in UX design and development and continuously improve their skills and knowledge. Read here- ‘Best Free Online Courses for Aspiring UX Designers’.\nIn addition to these technical skills, communication and problem-solving skills are also critical for a UX and front end developer too. UX developers must be able to effectively communicate with UX designers, front-end developers, and other stakeholders to ensure that everyone is working towards the same goals and that the end user and experience are optimized.\nThe Challenges Of Design And Development Gap\nThe visual design and development gap refers to the disconnect between the graphic design and development teams in the UX design process of mobile apps. This gap can lead to several challenges and issues that can impact the overall quality of visual design and the user experience.\nSome common issues that arise from the design and development gap include:\n1. Communication Breakdown\nThe design and development teams may not communicate effectively, leading to misunderstandings and delays in the project timeline.\n2. Lack of Alignment\nThe design and development teams may have different goals or priorities, leading to suboptimal outcomes that do not fully meet the user’s needs.\n3. Inefficient Processes\nThe design and development teams may not have clear processes in place for collaboration and handoff, leading to inefficiencies and rework.\n4. Technical Limitations\nThe design team may create designs that are not technically feasible or that are difficult to implement, leading to delays and compromises in the final product.\n5. Design Compromises\nThe development team may be forced to make compromises in the design to meet technical requirements, leading to suboptimal outcomes that do not fully meet the user’s needs.\nThese issues can lead to inefficient processes and suboptimal outcomes. For example, if the design and development teams are not aligned on project goals, the final product may not fully meet the user’s needs.\nThis can lead to negative user feedback, lower engagement, and decreased revenue. In addition to user and usability testing itself, if the design team creates designs that are not technically feasible or that are difficult to implement for real users, this can lead to delays in the project timeline and increased costs.\nBenefits Of Bridging the Gap Between UX Designers and UX Developers\nBridging the gap between UX designers and developers can bring many benefits to the UX design process. When designers and developers work closely together and collaborate effectively, the results of interaction design, are often better than when they work in silos.\nHere are some of the benefits of bridging the gap between UX designers and developers:\n1. Improved Collaboration\nThe UX designers and developers can share ideas and feedback more effectively. This collaboration can lead to better designs and more efficient development processes.\n2. More Efficient Processes\nBridging the gap between UX designers and developers can streamline the development process. When designers and developers work together from the beginning, they can identify potential technical limitations and design solutions that are easier to implement. This can save time and resources in the long run.\n3. Higher Quality Outcomes\nWhen designers and developers work together, the final product is often of higher quality. This is because the design team can create designs that are optimized for the user experience, and the development team can implement those designs in a way that is technically feasible and efficient.\n4. Faster Time to Market\nWith an effective collaboration between the designers and the developers, the development process can be more efficient and streamlined. This can lead to faster time to market and a competitive edge in the marketplace.\n5. Better User Experience\nBoth the professionals can create user interfaces that are optimized for the user experience. This can lead to higher engagement, increased user satisfaction, and better business outcomes.\nStrategies to Bridge the Gap\nHere are a few tips/strategies to help effectively bridge the gap between design and development in UX:\n- One of the critical elements of a successful project is regular communication. UX designers and developers should communicate regularly to ensure that they are on the same page and that the project is progressing as planned.\n- Collaboration from the outset is also important. The UX designer and developer should work together from the beginning of the project to ensure that everyone has a clear understanding of the project goals and objectives.\n- Mutual respect and understanding are critical components of successful UX development. UX designers and developers should respect each other’s areas of expertise and understand how their respective roles contribute to the success of the project.\n- Productive use of prototyping is also a strategy that can help bridge the gap between UX designers and developers. Prototyping allows UX designers to visualize and test their designs in a way that developers can understand.\n- Developers can use prototypes to gain a better understanding of the UX designer’s vision and create a development plan that takes into account the design’s nuances.\nThe Role Of UI Developers in UX\nUI developers, also known as front-end developers, play a crucial role in the UX development process. UI developers are responsible for prototyping tools and for implementing the user interface design created by the UX designers, ensuring that the user feels that it functions correctly across different devices and platforms.\nThe specific skills key roles and responsibilities of UI developers include:\n- HTML/CSS: UI developers are experts in HTML and CSS, which are the building blocks of the user interface. They use these skills to create the layout, styling, and overall look and feel of the website or application.\n- Responsive Design: UI developers ensure that the user interface is responsive, meaning that it adapts to different screen sizes and devices.\n- Performance Optimization: UI developers optimize the user interface to ensure that it loads quickly and performs well.\n- Accessibility: UI developers ensure that the user interface is accessible to all users, including those with disabilities.\nThe Relationship Between UX and UI\nThe relationship between UI designer and UX is that UX (User Experience) focuses on designing the overall experience of the user, while UI designer(User Interface) focuses on the visual and interactive elements that make up that experience.\nUX designers are responsible for researching and understanding user needs, creating user personas, developing user flows, and wireframing designs.\nUI designers, on the other hand, are responsible for designing the look and feel of visual elements of the user interface, including colours, typography, and interactive elements.\nThe two are closely related and often work together to create a seamless and positive user experience. Good UX design requires good UI design, and vice versa, as both UI and UX designers are essential components of creating a successful product or application.\nUX (User Experience) and UI (User Interface) developers work together closely to create a seamless and intuitive digital experience for users. While UX and UI are related, they are distinct roles, each with their own specific focus and skill set.\nUX developers focus on the overall positive experience that a user has when interacting with a digital product or service. This involves research, user testing, and design thinking to create a user-centric product that is easy to use, efficient, and satisfying.\nUI developers, on the other hand, focus on the visual and interactive elements of a digital product. They work on designing the interface that users see and interact with, including the layout, typography, and visual elements.\nThe Vital Role of Front End Developer Job\nSpecifically, front-end developers are responsible for creating the layout, navigation, and functionality of a website or application. They work closely with designers, user experience (UX) specialists, and user interface (UI) designers to build prototypes and ensure that the website or application meets user needs and is easy to use.\nFront-end developers play a crucial role in bridging the gap between UX and UI design and development. They take the designs created by UX and UI designers and turn them into functioning websites and web applications.\nCollaboration between UX, UI, and front-end developers is important because it ensures that the website or application is user-friendly, visually appealing, and functional.\nIn the world of user experience (UX) design, there is often a gap between the design and development stages of a project. UX developers play a crucial role in bridging this gap by bringing both design and development skills to the table.\nThey work closely with UX designers to ensure that designs are practical and feasible, while also working with developers to ensure that the end product meets user needs and is easy to use.\nIn doing so, UX developers help create a seamless user experience that is both visually appealing and functional. They also play a key role in ensuring that design changes and updates are implemented correctly and efficiently.\nFrequently Asked Questions\nWhat is the role of UX developers in bridging the gap between design and development in UX?\nUX developers play a crucial role in ensuring that the design and development processes work together smoothly. They use their knowledge of both design and development to create user-centered interfaces that are both visually appealing and functionally efficient.\nHow do UX developers work with designers and developers?\nUX developers work closely with both designers and developers to ensure that the design is implemented accurately and effectively. They act as a bridge between the two teams, translating design requirements into functional code that meets user needs.\nWhat skills are required to become a UX developer?\nHow can UX developers help improve the user experience?\nUX developers can improve the user experience by ensuring that the design is implemented correctly and efficiently. They can also contribute to the design process by offering feedback and suggestions based on their understanding of user needs and technical constraints.\nWhat is the career outlook for UX developers?\nThe demand for UX developers is expected to continue growing as companies place increasing emphasis on creating user-centered interfaces. Salaries for UX developers are generally competitive and can vary depending on location, experience, and industry.']	['<urn:uuid:5939fc27-030e-49c2-96c4-3a67be3b5a4b>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T23:00:04.658849	13	56	2433
6	How did they fix the problem of cork-tainted wine?	The cork-tainted wine problem was largely brought under control by stopping the use of chlorine bleach for cork treatment, which was previously used to whiten corks and as an antimicrobial agent. Now, corks are treated with peroxide instead and undergo steam cleaning, rinsing, and heavy sulfuring processes.	"['Spume the question of corked Italian wines appeared too, confirming an old observation. But, this is not some funky Portuguese conspiracy because the manner in which these batches of wine are corked points to a slightly different offender.\nTCA (2,4,6-Trichloroanisole) is well known as the primary perpetrator in corked wine. Or, should say, known to be. The ""chloro"" part gives away that it had to do with chlorine and it often originated in bleached corks. It was thought to be a good way to make those dark corks whiter and a good antimicrobial agent. Whoops. That problem is largely under control now as most corks are peroxided instead and usually subjected to some sort of steam cleaning, rinsing and heavy sulfuring. A bit of time in a wine shop though showed that indeed a truly abnormal number of Italian wines were corked - whole lots, not just individual bottles. Three bottles in a row come to mind of both very reputable Brunello and Chianti Classico, and a high number of Alsatian whites also incidentally. Multiple bottles opened - all bad, though at slightly varying levels of corkiness.\nThe down and dirty truth is frightening. Basically, there are four related compounds all with similar effects if different sensory thresholds: TCA, TeCA, PCA and TBA. TCA is pretty clear in corks, but the frightening thing is that these compounds are alive and well in the wine cellar, TBA (2,4,6-tribromoanisole) in particular is more ubiquitous and just as potent. And, the sensory thresholds are in the parts per trillion (ppt) for the worst offenders - a few drops in an Olympic swimming pool sized. Observation shows that the sensory defects are similar but not exactly the same, and it seems that the threshold for recognition is more widely variable than for TCA. Long arguments have taken place over who can recognize it and who can\'t more than once.\nFor a long time everything was blamed on poor old TCA, which as a cork-based defect is isolatable and usually limited to a cork or a bale of corks. TBA is a taint that physically infects wine in the cellar, meaning large lots, like a tank or a barrel that inevitably is mixed in for bottling all at once. TBA is often used in wood treatment, ending up frequently in the lowly wooden pallet that necessarily appears everywhere. But wood pallets are not the only danger as TBA\'s precursor TBP is often present as a fire retardent ingredient in several carriers, in fungicides, paints, antiseptics, and bromine-containing detergents. No way, detergents unsafe? Yup, just like bleach. It can be in barrels, infect oak chips, it can show up in plastics, and it can be in the paint inside the cellar.\nWe have all experienced the flattened, fruitless bottle that does not seem corked or overtly musty and danky, but sure seems to suck. Chances are that the bad bottle was at 1-2 ppt TBA, just below threshold but clearly hurting quality. Sometimes it ""blows off"" a little, a curious thing that may indicates other problems as well - often a reduction issue. But when multiple bottles of the same wine show the same defect, TBA taint in the cellar is the suspect. These Italian wines are most likely not victims of bad corks, but either tainted wood or inappropriate cleaning compounds or paint in their cellars or coops.']"	['<urn:uuid:b5379cbf-78be-40d8-b747-5d45b0f1e9fd>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T23:00:04.658849	9	47	563
7	lstm vs rnn parameter estimation capabilities	While both LSTM and RNN are types of recurrent neural networks, LSTMs are more sophisticated and computationally effective. For parameter estimation tasks, document 1 shows that LSTM models with MFB-based feature extraction and spectral augmentation achieve good performance for acoustic parameter estimation, offering a good trade-off between performance and complexity. LSTMs contain specialized gates (input, output, and forget gates) that regulate information flow better than standard RNN units, which helps avoid the vanishing gradient problem that standard RNNs suffer from.	"['When a signal is acquired in a room with reflective surfaces and noise sources, the acquired signal can be modeled by convolution with the room impulse response (RIR) plus an additive noise component. The level of reverberation can be characterized by an RIR, from which the Clarity Index (C50) metric can be obtained which has been shown to be well correlated with ASR performance. The level of additive noise can be modeled by the Signal-to-Noise Ratio (SNR). The combined effects of noise and reverberation can be modeled by the Perceptual Evaluation of Speech Quality (PESQ) algorithm, which is an intrusive method requiring the clean reference signal. Another important aspect of the signal is estimating the segments where speech is present, a task commonly referred to as voice activity detection (VAD).\nIn our paper “Non-Intrusive Estimation of Speech Signal Parameters using a Frame-based Machine Learning Approach”, to be presented at EUSIPCO 2020, we propose a multi-task machine learning framework called NISA for non-intrusive acoustic parameter estimation that includes VAD, C50, PESQ and segmental SNR. While most recent methods for non-intrusive speech quality and reverberation parameter estimation operate on a large temporal window size (more than a second long) or at an utterance level, our framework is able to reliably estimate a number of parameters in short windows of length 300ms. The ability to estimate these parameters in short-time windows enables the use of these parameters as additional features in speaker diarization, signal quality assurance and for signal selection in multiple microphone ASR.\nIn this paper we explore three different feature extraction front-ends (MMF, MFB and PASE) and two different deep learning architectures (CNN and LSTM) for the joint estimation of the acoustic parameters.\nThe three feature extraction front ends we explore are as follows:\n- MMF – this is a combination of MFCCs and Modulation domain features as described in \n- MFB – this refers to the use of Mel Filter Bank features (similar to MFCCs in  but without the DCT). Spectral Augmentation (SA)  is further applied (randomly drop 3% of the Mel Channels for a given block of frames).\n- PASE – a Problem-Agnostic Speech Representation (PASE)  feature set that is extracted from the waveform of speech using a self-supervised encoder discriminator architecture with several target encoders (including MFCC features). Here we use the pre-trained model provided by the authors.\nWe explore two deep learning frameworks, trained with an RMS cost function and the Adam optimizer \n- LSTM – a recurrent structure , which has been shown to be a powerful architecture for modelling time varying features. Our model is composed of three hidden layers, arranged in a 108x54x27 cell topology (for each time step).\n- CNN – a compact Convolutional Neural Network model based on the SwishNet  architecture with important modifications:\n- Front end feature MFCC -> MFB + SA\n- Experimented with different filter sizes\n- Added a dropout layer in the architecture\n- Output layer modified for regression: linear layer with four output nodes\nThe different combinations of feature front ends and machine learning architectures provide tradeoffs in performance in terms of computational complexity and estimation accuracy. The systems are trained on simulated data with clean speech from the training partition of the Wall Street corpus, convolved with simulated RIRs and additive noise (ambient, babble, domestic and white). The RIRs are sampled with C50 in the range [0 to 30 dB]. We use two simulated test sets for evaluating the performance of the proposed framework with base speech material from two different sources to the training data (to avoid any overlap in words, speaker or recording system) and the RIRs and noise sources are also separate with no overlap with training data.\n|Mean Absolute Error||F1 Score|\n|Feature Type||Spectral Augmentation||Model Type||C50||SNR||PESQ||VAD||Trainable Parameters|\nWe show that a low complexity MFB-based feature extraction with spectral augmentation and an LSTM model achieves good performance for acoustic parameter estimation and provides a good trade-off in performance and complexity. We show the performance of this system is stable across different noise types and how different ground truth and estimated parameters correlate with ASR performance (in terms of WER as well as the sub-components such as insertions, deletions etc.). We show, firstly that of the three reverberation level estimation parameters, C50 is the most correlated with WER, and in addition, SNR and PESQ are also highly correlated with WER. Lastly, we show that the C50, SNR and PESQ estimated by the NISA model are also highly correlated with ASR performance.\nLucia Berger, Carl Quillen and Patrick A. Naylor contributed to the paper and this blog post.\n D. Sharma et. al. “Non-intrusive polqa estimation of speech quality using recurrent neural networks,” EUSIPCO, 2019\n S. Pascual et. al. “Learning problem-agnostic speech representations from multiple self supervised tasks,” INTERSPEECH, 2019.\n D. S. Park et. al., “SpecAugment: A simple data augmentation method for automatic speech recognition,” INTERSPEECH, 2019.\n D. P. Kingma et. al. “Adam: A Method for Stochastic Optimization,” vol. abs/1412.6980, 2014. [Online]. Available: http://arxiv.org/abs/1412.6980\n S. Hochreiter et. al. “Long Short-Term Memory,” Neural Computation, vol. 9, no. 8, pp. 1735–1780, Nov. 1997.\n M. Hussain et. al. “Swishnet: A fast convolutional neural network for speech, music and noise classification and segmentation,” 2018, [Online]. Available: https://arxiv.org/abs/1812.00149', 'What\'s the difference between LSTM and RNN? I know that RNN is a layer used in neural networks, but what exactly is an LSTM? Is it also a layer with the same characteristics?\nRNNs have recurrent connections and/or layers\nYou can describe a recurrent neural network (RNN) or a long short-term memory (LSTM), depending on the context, at different levels of abstraction. For example, you could say that an RNN is any neural network that contains one or more recurrent (or cyclic) connections. Or you could say that layer $l$ of neural network $N$ is a recurrent layer, given that it contains units (or neurons) with recurrent connections, but $N$ may not contain only recurrent layers (for example, it may also be composed of feedforward layers, i.e. layers with units that contain only feedforward connections).\nIn any case, a recurrent neural network is almost always described as a neural network (NN) and not as a layer (this should also be obvious from the name).\nLSTM can refer to a unit, layer or neural network\nOn the other hand, depending on the context, the term ""LSTM"" alone can refer to an\n- LSTM unit (or neuron),\n- an LSTM layer (many LSTM units), or\n- an LSTM neural network (a neural network with LSTM units or layers).\nPeople may also refer to neural networks with LSTM units as LSTMs (plural version of LSTM).\nLSTMs are RNNs\nAn LSTM unit is a recurrent unit, that is, a unit (or neuron) that contains cyclic connections, so an LSTM neural network is a recurrent neural network (RNN).\nThe main difference between an LSTM unit and a standard RNN unit is that the LSTM unit is more sophisticated. More precisely, it is composed of the so-called gates that supposedly regulate better the flow of information through the unit.\nHere\'s a typical representation (or diagram) of an LSTM (more precisely, an LSTM with a so-called peephole connection).\nThis can actually represent both an LSTM unit (and, in that case, the variables are scalars) or an LSTM layer (and, in that case, the variables are vectors or matrices).\nYou can see from this diagram that an LSTM unit (or layer) is composed of gates, denoted by\n- $i_t$ (the input gate: the gate that regulates the input into the unit/layer),\n- $o_t$ (the output gate: the gate that regulates the output from the unit)\n- $f_t$ (the forget gate: the gate that regulates what the cell should forget)\nand recurrent connections (e.g. the connection from the cell into the forget gate and vice-versa).\nIt\'s also composed of a cell, which is the only thing that a neuron of a ""vanilla"" RNN contains.\nTo understand the details (i.e. the purpose of all these components, such as the gates), you could read the paper that originally proposed the LSTM by S. Hochreiter and J. Schmidhuber. However, there may be other more accessible and understandable papers, articles or video lessons on the topic, which you can find on the web.\nLSTMs also have recurrent connections!\nGiven the presence of cyclic connections, any recurrent neural network (either an LSTM or not) may be represented as a graph that contains one or more cyclic connections. For example, the following diagram may represent both a standard/vanilla RNN or an LSTM neural network (or maybe a variant of it, e.g. the GRU).\nWhen should you use RNNs and LSTMs?\nRNNs are particularly suited for tasks that involve sequences (thanks to the recurrent connections). For example, they are often used for machine translation, where the sequences are sentences or words. In practice, an LSTM is often used, as opposed to a vanilla (or standard) RNN, because it is more computationally effective. In fact, the LSTM was introduced to solve a problem that standard RNNs suffer from, i.e. the vanishing gradient problem. (Now, for these tasks, there are also the transformers, but the question was not about them).']"	['<urn:uuid:ca1ef2be-3b85-4d18-b41d-9b4942b8d756>', '<urn:uuid:60b02de3-bd54-4b36-970f-3820f6e01d11>']	factoid	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-12T23:00:04.658849	6	80	1518
8	How does urban sprawl affect air pollution outdoors and indoors?	Urban sprawl increases outdoor air pollution through increased automobile dependency, with cars accounting for 30% of nitrogen oxides, 30% of hydrocarbon emissions, and 32% of carbon emissions. In sprawling areas like Atlanta, vehicles account for even higher percentages - 58% of nitrogen oxides and 47% of hydrocarbon emissions. For indoor environments, air pollution comes from multiple sources including tobacco smoke, cooking appliances, vapors from building materials, and volatile organic compounds from products like furniture polish and air fresheners, which can cause eye/nose/throat irritation, headaches, and potential long-term liver damage.	"['Presentation on theme: ""Hazardous to our Health: The Effects of Urban Sprawl on the Environment and its Inhabitants Erin Anderson-Ruddon Kayla Arslanian.""— Presentation transcript:\nHazardous to our Health: The Effects of Urban Sprawl on the Environment and its Inhabitants Erin Anderson-Ruddon Kayla Arslanian\nHazardous to our Health: The Effects of Urban Sprawl on the Environment and its Inhabitants Sprawl has four dimensions: a population that is widely dispersed in low density development, rigidly separated homes, shops and workplaces, a network of roads marked by huge blocks and poor access, and a general lack of well-defined thriving activity centers or downtowns (Ewing, Pendall, and Chen 2002). We hypothesize that urban sprawl is harmful to the environment and that the environmental degradation suffered increases health risks in these areas of sprawl.\nUrban Sprawl & Air Pollution Air quality is greatly affected by sprawl. Motor vehicles are the leading source of air pollution. In areas of sprawl, destinations are far apart, creating a dependence on automobiles to get from destination to destination. The need to make more car trips increases the average person’s daily miles driven.\nAir Pollution & The Sprawl Index The sprawl index categorizes areas according to their level of sprawl. A standard deviation of 50 separates a high area of sprawl, from an average area of sprawl, from a low area of sprawl (Sturm and Cohen 2004). For example, Atlanta, GA, an area of very high sprawl has an index of 57.7 whereas Chicago, IL, a denser city has an index of 121.2. For every 50 unit decrease in standard deviation on the sprawl index (more sprawling), there is a 1.96 miles/day increase in driving per person. For every 25 unit increase in standard deviation on the sprawl index (less sprawling), there is a 5.4 miles/day decrease in driving per person. This shows that people in more sprawling areas drive longer distances, more often than those living in areas of relatively low sprawl.\nAutomobiles & Air Pollution Because inhabitants in areas of great sprawl have to drive longer distances more frequently, they burn more fuel than persons living in areas of lesser sprawl. The advent of sprawl could threaten the current positive trend of cleaner air.\nAutomobiles & Air Pollution Automobiles account for: 30% of all oxides of nitrogen (any gaseous form of nitrogen) emitted into the air. 30% of all hydrocarbon emissions. The main component of fossil fuels, hydrocarbons combust when fuel is burned, releasing chemicals such as methane and benzene into the atmosphere. Benzene is a known carcinogen. 32% of all carbon emissions.\nAutomobiles & Air Pollution Automobiles account for: 30% of all oxides of nitrogen (any gaseous for of nitrogen) emitted into the air. 30% of all hydrocarbon emissions. The main component of fossil fuels, hydrocarbons combust when fuel is burned, releasing chemicals such as methane and benzene into the atmosphere. Benzene is a known carcinogen. 32% of all carbon emissions. In areas where sprawl is high, cars account for a greater number of these pollutants. For example, in Atlanta, vehicles account for 58% of all nitrogen oxides and 47% of all hydrocarbon emissions (Frumkin, 2002).\nUrban Sprawl & Ozone Sprawl affects a region’s ozone levels. Ozone is a photochemical smog created then organic gases (specifically VOCs, nitrogen oxides, heat and sunlight interact (Goldman 2001)). Though ozone is necessary in the stratosphere to protect earth from the sun’s UV rays, in the troposphere it degrades air quality, and is considered a pollutant. Currently, over 90 metropolitan areas regularly exceed ozone standards. The EPA attributes 50% of smog precursors to motor vehicles (Benfield, Raimi and Chen 1999).\nUrban Sprawl & Ozone Land use patterns affect levels of ozone. In sprawling environments, individuals release more VOCs and nitrogen dioxides into the atmosphere, both of which are components of ozone. Table 1: Average daily VOC and NOx emission rate for compact and disperse cities (Borrego, Martins, and Tchepel et al 2006).\nSprawl Index & Ozone Every 25 unit increase in the sprawl index (less sprawling) relates to a 7.5 parts per billion decrease in maximum ozone levels. Ozone levels between the most sprawling regions and the least sprawling regions differ by 41 parts per billion (Ewing, Pendall, and Chen 2002).\nUrban Sprawl & Ozone Urban sprawl not only contributes to higher ozone levels, but ozone also covers more area in sprawling regions: Figure 2: Ozone concentrations (relative to background ozone levels) at 2pm in alternate city structures (Borrego, Martins, and Tchepel et al 2006).\nUrban Sprawl & Ozone Ozone affects more people in areas of greater sprawl: Table 2: Population affected by ozone concentrations (Borrego, Martins, and Tchepel et al 2006). Again, because individuals living in sprawl produce more VOCs and nitrogen oxides daily, as a result their ozone levels are higher, cover more area, and affect more people than compact cities. This is because individuals in compact cities produce less VOCs and nitrogen oxides. Thus, their ozone levels, area and population affected by ozone are not as greatly impacted by ozone pollution.\nCarbon Monoxide Analysis Sprawling regions also emit an alarmingly large amount of carbon monoxide. Combining demographic information and data collected on how much carbon monoxide an average car emits, we can see how dangerous sprawl is for air quality. The EPA estimates that the average car annually produces 22g CO for every 12,500 miles driven (EPA, 1997). This means that the average car produces.00176g CO for every mile driven (22g CO/12,500).\nCarbon Monoxide Analysis It is also recorded the average person in the Atlanta metropolitan area travels 34.1 miles/day, whereas the average person in the Philadelphia metro area dives 16.9 miles a day, and the average Chicago metro inhabitant dives 19.9 miles per day (Frumkin, 2002). On the sprawl index, Atlanta’s score is has a 57.7 (very sprawly), Philadelphia’s is 112.6 (average sprawl), and Chicago’s is 121.2 (higher sprawl).\nThe Atlanta metro area has nearly two million less people then the Philadelphia metro area, yet Atlanta emits around two billion more pounds of carbon monoxide yearly. Chicago’s metro area has twice the population of the Atlanta metro area, and Atlanta still produces more than one billion pounds of carbon per year. The differences seen are attributed to sprawl: individuals living in sprawl drive more frequently and drive than those living elsewhere, and the impact this car dependency has on the environment will be detrimental.\nWater Pollution Urban development in watersheds (an area drained by a river or some other body of water) can greatly alter the composition of a river, which can in turn affect water quality This section focuses on two watersheds—the Schuylkill watershed and the Waquoit Bay watershed—and examines how sprawl impacts these two bodies of water.\nSchuylkill River, PA The Schuylkill River has some of the highest dissolved solute concentrations of all water sources in the northeast—it has the highest nitrate levels and the second highest chloride levels (Interlandi and Crockett 2003). The Schuylkill also had the fastest increase for nitrate, chloride and residuals over all other watersheds in the northeast (Interlandi and Crockett 2003). This suggests that development in the Schuylkill watershed negatively impacts the river.\nSchuylkill River, PA From 1982 to 1997, developed land in the Schuylkill watershed increased from 21.5% of the total watershed land area to 28.5%—over 34,000 hectares of forest and agricultural land were urbanized. This change in land use resulted in a 31% increase in developed land. When sprawling occurred in the watershed and developed land increased by 31%, chloride levels increased 37% in that same time period (Interlandi and Crockett 2003).\nWaquoit Bay, MA The Waquoit Bay is located in Cape Cod, MA, and like the Schuylkill watershed, the Waquoit Bay watershed has seen a recent burst in urban development. The predominant source of nitrogen in the water comes from atmospheric deposition, fertilizer use and wastewater disposal. in the 1980s, the major source of nitrogen changed from atmospheric deposition to wastewater, reflecting the increase in urbanization in the Waquoit watershed (Bowen and Valiela, 2001).\nWaquoit Bay Between 1938 and 1990, when most urbanization occurred in the area, nitrogen inputs into the Waquoit bay watershed increased twofold (Bowen and Valiela, 2001). In 1990, wastewater accounted for 22% of the bay’s total nitrogen delivery, whereas in the mid-century, it only accounted for 2%. Thus, nitrogen from wastewater increased ten-fold during the period of urban sprawl in the watershed.\nHealth Hazards of Sprawl The harmful effects that sprawl has on air and water quality have implications for human health. AIR Nitrogen oxides can react with numerous compounds to form acid and other particles Particles can inflict damage to lung tissue which can cause or worsen respiratory diseases such as emphysema and bronchitis\nHealth Hazards of Sprawl - Air VOCs release hydrocarbons, some of which are carcinogenic Carbon monoxide affects the central nervous system, causing visual problems, reduced ability to work or learn, and difficulty performing tasks At extremely high levels, carbon monoxide is poisonous and can cause death (EPA, 2006).\nWater Related Hazards Nitrogen Through water pollution, increased nitrates in the water can cause methmeglobenemia in infants under six months. Blue baby syndrome because the hemoglobin goes from being ferrous to ferric (FE 2 + to FE 3 + ). This is an oxidizing reaction—thus, the hemoglobin’s ability to carry oxygen is lowered, making the infants blue. This syndrome is potentially fatal\nWater Related Hazards Storm runoff currently 5,529 water bodes in the US are impaired by pathogens (Gaffield, Goo, Richards et al 2003). The insecticides in runoff that infect water have carcinogenic effects in humans. The amount of disinfectant (such as chlorine) can also have a carcinogenic effect. The EPA estimates that ingestion of drinking water with disinfectant byproducts is responsible for 1100-93000 cases of bladder cancer each year (Gaffield, Goo and Richards et al 2003).\n(Gaffield, Goo and Richards et al 2003). Impaired Water Bodies in the USA\nGeneral Health Hazards Sprawl categorically makes people less healthy. The overall sprawl index significantly predicts the number of chronic medical conditions and of physical health-related quality of life of the residents in the area. Greensboro, NC San Fran, CA 46.8 148.6 190 181 46.8 44.2 1.35 1.14\nSprawl Index (Abridged) As a region’s sprawl index increases by 25 points, the number of residents suffering from various chronic conditions decreases (Sturm and Cohen 2004). Sprawl Score5075100125150 Arthritis291274259243228 Trouble Breathing 6459544945 Abdominal/Diges tive Problems 9285797368 Migraine/Chroni c Headaches 145137130123116 Urinary Tract Issues 6660545945\nCONCLUSION America will undoubtedly continue to develop, thus, action must be taken to ensure that further development is not hazardous to the environment and to its inhabitants. It is necessary that that future designs must be eco-friendly, and moreover, must consider the health of its residents.\nWorks Cited: An Introduction to Indoor Air Quality: Organic Gases (Volatile Organic Compounds - VOCs). Environmental Protection Agency. 5 December 2006. Air Emissions Trends - Continued Progress Through 2005. Environmental Protection Agency. 5 December 2006. Atlanta Metropolitan Area. Wikipedia. 5 December 2006. Benfield, F. Kaid, Matthew D. Raimi, and Donald D.T. Chen. Once There Were Greenfields: How Urban Sprawl is Undermining America’s Environment, Economy, and Social Fabric. New York: Natural Resources Defense Council, 1999. Borrego, C., H. Martins, O Tchepel, et al. (2006) “How Urban Strucutre Can Affect City Sustainability from an Air Quality Perspective.” Environmental Modeling and Software. 21: 461-467. Bowen, Jennifer L. and Ivan Valiela. (2001). “The Ecological Effects of Urbanization of Coastal Watersheds: Historical Increases in Nitrogen Loads and Eutrophication of Waquoit Bay Estuaries.” Canadian Journal of Fisheries and Aquatic Sciences. 58: 1489-1500. Chloride. Wikipedia. 5 December 2006.\nChicagoland. Wikipedia. 5 December 2006. Delaware Valley. Wikipedia. 5 December 2006. Ewing, Richard, Rolf Pendall, and Don Chen. “Measuring Sprawl and its Impact.” Smart Growth America. 24 October 2006. Gardener, Sarah. “The Impact of Sprawl on the Environment and Human Health.” Urban Sprawl: A Comprehensive Reference Guide. Ed. David C. Soule. Westport, CT: Greenwood Press, 2006. 240-260. Gaffield, Stephen J., Robert L. Goo, Lynn A. Richards et al. (2003) “Public Health Effects of Inadequately Managed Stormwater Runoff.” American Journal of Public Health. 98: 1527-1531. Goldman, Todd. ""Consequences of Sprawl: Threats to California\'s Natural Environment and Human Health"" eScholarship Repository. Institute of Urban & Regional Development at UC Berkley. 24 November 2006. Health and Environmental Impacts of CO. Environmental Protection Agency. 6 December 2006. http://www.epa.gov/air/urbanair/co/hlth1.html Health and Environmental Impacts of NOx. Environmental Protection Agency. 6 December 2006. Frumkin, Howard. (2002) “Urban Sprawl and Public Health.” U.S. Department of Health and Human Services: Public Health Reports. 117: 201-217. Interlandi, Sebastian J. and Christopher. S. Crockett. (2003) “Recent Water Quality Trends in the Schuylkill River, Pennsylvania, USA: A Preliminary Assessment of the Relative Influences of Climate, River Discharge and Suburban Development.” Water Research. 37: 1737-1748. Nitrate. Wikipedia. 5 December 2006.\nWhat are the Six Common Air Pollutants? Environmental Protection Agency. 5 December 2006. Wolfe, Amir H. and Jonathan A. Patz. (2002) “Reactive Nitrogen and Human Health: Acute and Long-term Implications.” Ambio. 31: 120-125. Images: Massachusetts Estuaries Project. 5 December 2006. Schuylkill River: National State and Heritage Area. 5 December 2006. Waquoit Bay: National Estuarine Bay Research Reserve. 5 December 2006.', 'Sunday, 26 January 2014\nAIR POLLUTION AND HEALTH PROBLEMS\nAir is the mixture of gases that surround earth due to its gravitational pull. It is colourless, odourless and tasteless mixture that forms earth’s atmosphere. It contains Nitrogen (79.1%), Oxygen (20.0%), Carbon dioxide (0.03%) and traces of inert gases like argon, krypton, xenon, neon, helium, ammonia, ozone, water vapour and suspended particles (0.07%).\nWe need air to perform certain vital life functions. All human beings and animals breathe in Oxygen and breathe out Carbon dioxide. Plants breathe in Carbon dioxide and breathe out oxygen in the day time to manufacture their own food through Photosynthesis. But they follow the pattern of animals at the night. Without Air there would be no transmission of sound and radiation waves, no burning of fire and cycle of seasons.\nWhen the natural composition of air is altered to such an extent, so as to make it harmful for living creatures primarily due to human activities, air is said to be ‘polluted’. Any substance that causes pollution is called pollutant. Pollutants can be solid, liquid or gaseous substance.\nAir is polluted by natural ways of volcanic eruptions, forest fires, pollen from plants etc. However, nature has a way of balancing itself. Our concern here is man caused pollution which is looming largely as a threat to modern civilization. Pollution can be INDOOR as well as OUTDOOR:\nINDOOR AIR POLLUTION refers to the physical, chemical, and biological characteristics of air in the indoor environment within a home, building, or an institution or commercial facility. Indoor air pollution is a concern in the developed countries, where energy efficiency improvements sometimes make houses relatively airtight, reducing ventilation and raising pollutant levels. Indoor air problems can be subtle and do not always produce easily recognized impacts on health. Tobacco smoke, cooking and heating appliances, and vapors from building materials, paints, furniture, etc. cause pollution inside buildings.\nVolatile organic compounds originate mainly from solvents and the main indoor sources are perfumes, hair sprays, furniture polish, glues, air fresheners, moth repellents, wood preservatives, and many other products used in the house. Their main health effect is the imitation of the eye, nose and throat. In more severe cases there may be headaches, nausea and loss of coordination. In the long term, some of the pollutants are suspected to damage to the liver and other parts of the body.\nTobacco smoke generates a wide range of harmful chemicals and is known to cause cancer. It is well known that passive smoking causes a wide range of problems to the passive smoker (the person who is in the same room with a smoker and is not himself/herself a smoker) ranging from burning eyes, nose, and throat irritation to cancer, bronchitis, severe asthma, and a decrease in lung function.\nBiological pollutants include pollen from plants, mite, hair from pets, fungi, parasites, and some bacteria. Most of them are allergens and can cause asthma, hay fever, and other allergic diseases.\nFormaldehyde is a gas that comes mainly from carpets, particle boards, and insulation foam. It causes irritation to the eyes and nose and may cause allergies in some people.\nAsbestos is mainly a concern because it is suspected to cause cancer.\nRadon is a gas that is emitted naturally by the soil. Due to modern houses having poor ventilation, it is confined inside the house causing harm to the dwellers.\nOUTDOOR AIR POLLUTION is the pollution that occurs outside homes through automobiles, industrial emissions etc.\nMAIN AIR POLLUTANTS AND THEIR IMPACT ON HEALTH\n1) Oxides of sulphur 2) Oxides of Nitrogen 3) Suspended Particulate Matter 4) Carbon Monoxide 5) Lead 6) Benzene 7) Hydrocarbons\nHealth impacts of air pollution depend on pollutant type, its concentration in air, interaction with other pollutants, and length of exposure and individual vulnerability.\nAir Pollution has both acute and chronic effects on Human health. It has both long term and short term effects. It may range from minor irritation to most chronic respiratory problems.\nIMMEDIATE HEALTH PROBLEMS FROM AIR POLLUTION\nØ Aggravated cardiovascular and respiratory problems\nØ Burden on heart and lungs, causing them to work harder to supply the body with oxygen\nØ Damage the cells in respiratory organs\nØ Damage to deeper portions of lungs, even after symptoms of coughing or sore throat disappear\nØ Wheezing, chest pain, dry throat, headache or nausea\nØ Increased reactivity to allergens and particles\nØ Eye irritation\nØ Reduced body immunity to infection and increased fatigue\nLONG TERM EFFECTS OF AIR POLLUTION AS PERMANENT HEALTH PROBLEMS:\nØ Accelerated ageing of lungs and loss of lung capacity\nØ Decreased lung function\nØ Diseases like, asthma, bronchitis, emphysema and cancer\nØ Is a potential cause of human mortality\n1. Acute respiratory infections (ARI) in children: ARI in children under 5 years in\ncause: 13% of deaths; 11% of NBD; 24% of NBD for children under 5 years\n(NBD- National Burden of Disease) India\nARI is the largest single disease category for\n, accounting for about\none-ninth of the national burden. For the world as a whole, ARI is\nalso the largest category, accounting for about 8.5% of the global\nAcute respiratory infections as pneumonia, is one of the chief killers of children in Developed Countries. It is well known to be enhanced by exposure to urban air pollutants and indoor environmental tobacco smoke at levels of pollution that are some 10-30 times less than that typically found in villages.\nA recent study of 642 infants conducted in urban slums of\n. The incidence of acute respiratory\ninfection was more in highly polluted areas than less polluted areas. In New Delhi , children\nhave to bear the double the burden of diseases that have persisted for\ngenerations as well as of new diseases caused by various environmental factors. India\nAsthma: Asthma causes: 0.2% of deaths; 0.5% of NBD in\nAlthough Asthma rates are officially low in India , there is some recent\nevidence that the true prevalence is higher than previously thought.\nAssociated with urban outdoor pollution and ETS (Environmental Tobacco smoke),\ntypical solid-fuel indoor smoke exposures are much higher.\nUndoubtedly, the rates of Asthamatic patients have been increasing; it might be\ndue to increase in the environmental pollution.\n2. Chronic obstructive pulmonary disease: Today in developed countries, nearly all cases of COPD are attributable to tobacco smoking. Undoubtedly, smoking is also a significant factor in COPD incidence among LDC (Less Developed Countries) men. In\n, even though\nrelatively few rural women smoked during the past decades, COPD in\nrural women today is not uncommon. Chronic obstructive lung disease, for which\ntobacco smoking is the major risk factor remaining in the developing countries\nis known to be an outcome of excessive exposure to air pollution. India\n3. Cancer: There are many chemicals in biomass smoke, which are carcinogenic in nature. In recent study in Japan on the other hand, found that women aged 30 years old cooking with wood fuel have an 80% increased chance of having lung cancer in later life.\nhas a larger\nfraction of its national burden of disease attributable to TB than any other\nregion, although the actual risk per person is less than that in\nSub-Saharan Africa. A large scale survey in India reported that women using bio\nfuels were three times more likely to have tuberculosis than women using\ncleaner fuels. India\nhas a larger burden of\nblindness than any other major region of the world. Indeed, globally, one out\nof three cataracts occurs in India where they are responsible\nfor 80% of blindness in the country. One case-control study in India found an excess\ncataract risk of about 80% among people using biomass fuel. Delhi\nThe Health Information of India reports show that environmental reasons are increasingly responsible for increased mortality in women and children (see graphs: What kills India’s children). According to the report, 55 percent of child mortality in\nis due to conditions originating in the prenatal period. A significant\nproportion of the tables are strongly related to environmental causes. India']"	['<urn:uuid:7daff17b-004a-4103-8901-550bab085d6f>', '<urn:uuid:942ceabb-30d0-410d-a36e-2802ecd24954>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T23:00:04.658849	10	89	3509
9	What methods do these organizations use to promote collaboration between different groups and sectors in their communities?	These organizations employ several methods to promote collaboration. They actively bridge activities between academia, government, and private industry to facilitate knowledge and data transfer. They create environments that bring together diverse participants including technologists, planners, designers, and members of the public. Both organizations emphasize active involvement of community groups and subject matter experts in decision-making processes. They host events and conferences that serve as forums for exchanging ideas, techniques, and experiences among various stakeholders. They also prioritize activities that foster public use rather than private gain, while facilitating access for those traditionally excluded from civic processes. Through these methods, they create platforms where different sectors can collaborate effectively while maintaining respectful and productive relationships.	['Anti-Disctimination and Harassment Policy\nOpen Data Delaware is committed to encouraging a safe, diverse community. It is our goal to provide a welcoming place free of any type of discrimination and harassment. As a result, Open Data Delaware strives to uphold the following standards at all Open Data Delaware events and expects the same of its members and attendees:\n- A community comprised of technologists, planners, designers, members of the public, and the collaborative implementation of civic technology. This includes encouraging participation from women, minorities, and any other actual, perceived or traditionally marginalized groups.\n- An environment of respect and encouragement for each member or attendee’s ideas, skills, and contributions.\n- An environment that encourages social entrepreneurship, curiosity and does not assume another has or should have equal skill, experience, or education.\n- An environment that inspires productivity, regardless of skill, using encouraging language and communications, i.e. “yes/and”, not “no/but.”\n- An environment that encourages members and participants to listen as much as they speak.\n- An environment that strives to build tools that are open and free technology for public use.\n- Priority for activities that aim to foster public use, not private gain.\n- Facilitating access and encouraging input from those who are traditionally excluded from the civic process.\n- Active involvement of community groups and those with subject matter expertise in the decision-making process.\n- Relationships and conversations between community members, local government staff and community partners remains respectful, participatory, and productive.\n- Open Data Delaware reserves the right to ask anyone in violation of these policies not to participate in Open Data Delaware events or network activities.\nAnti-Discrimination and Harassment Policy Comments and Considerations\n- Our anti-discrimination and harassment policy aims to ensure an environment free from harassment of any attendee on the basis of his or her race, color, creed, religion, national origin, alienage or citizenship, sex, gender identity and expression, age, disability, sexual orientation, domestic partnership status, medical condition (including having or being perceived as having HIV/AIDS-related conditions), marital status, family status, genetic predisposition or carrier status, status as a victim of domestic violence, sex offenses, or stalking, military status, for having made a complaint or participating in an investigation under this policy or any other characteristic protected by law.\n- Harassment includes but is not limited to: offensive verbal or written comments related to gender, gender identity and expression, sexual orientation, disability, physical appearance, body size, race, religion; sexual images in public spaces; deliberate intimidation; stalking; following; harassing photography or recording; sustained disruption of talks or other events; inappropriate physical contact; unwelcome sexual attention; unwarranted exclusion; and patronizing language or action.\n- We also strive to maintain an environment free from retaliation against anyone because he or she complains of, reports or provides information or assistance related to any complaint of discrimination or harassment.\n- Anyone in violation of these policies may expelled from Open Data Delaware events or network activities, at the discretion of the event organizers.\nThese policies are based, in part, on the example policy from the Geek Feminism wiki, created by the Ada Initiative and other volunteers.They are also based on several other policies, including the Ohio LinuxFest anti-harassment policy, written by Esther Filderman and Beth Lynn Eicher, the Con Anti-Harassment Project, Code for Philly and Code for America. Mary Gardiner, Valerie Aurora, Sarah Smith, and Donna Benjamin generalized the policies and added supporting material; and the example policy from the Geek Feminism wiki, created by the Ada Initiative and other volunteers. Many members of LinuxChix, Geek Feminism and other groups contributed to this work.\nAll Open Data Delaware events and their staff, presenters, and participants are held to the anti-harassment policy. In addition to governing our own events by this policy, Open Data Delaware will only lend our brand and fund groups that offer an anti-harassment policy to their attendees. Open Data Delaware is dedicated to providing a harassment-free experience for everyone regardless of gender, gender identity and expression, sexual orientation, disability, physical appearance, body size, race, age, or religion. We do not tolerate harassment of staff, presenters, and participants in any form. If a participant engages in harassing behavior, the event organizers may take any action they deem appropriate, including warning the offender or expulsion from Open Data Delaware events and network activities.\nIf you are being harassed, notice that someone else is being harassed, or have any other concerns, please contact a member of the event staff immediately. Event staff will be happy to help participants contact hotel/venue security or local law enforcement, provide escorts, or otherwise assist those experiencing harassment to feel safe for the duration of the event.\nIf you cannot reach an event organizer and/or it is an emergency, please call 911 and/or remove yourself from the situation.\nYou can also contact Open Data Delaware about harassment at firstname.lastname@example.org and feel free to use the email template below.\nOpen Data Delaware staff acknowledge that we are not always in a position to evaluate a given situation due to the number of events and the fact that our team is not always present. However, we are hopeful that by providing these guidelines we are establishing a community that jointly adheres to these values and can provide an environment that is welcoming to all.\nWe value your attendance and hope that by communicating these expectations widely we can all enjoy a harassment-free environment.\nEmail Template for Anti-Harassment Reporting\nSUBJECT: Safe Space alert at [EVENT NAME]\nI am writing because of harassment at an Open Data Delaware Communities event, (NAME, PLACE, DATE OF EVENT).\nYou can reach me at (CONTACT INFO). Thank you.', 'What is CaGIS?\nThe mission of the Cartography and Geographic Information Society is to support research, education, and practice to improve the understanding, creation, analysis, and use of maps and geographic information to support effective decision-making and improve the quality of life. The society serves as a forum for the exchange of original concepts, techniques, approaches, and experiences by those who design, implement, and use cartography, geographical information systems, and related geospatial technologies.\nThe Cartography and Geographic Information Society supports research, education and practice to improve the understanding, creation, analysis, and use of maps and geographic information to support effective decision-making and improve the quality of life.\nCaGIS members are scholars, practitioners, and policy makers who rely on and care about the quality of mapping and geographic information. They include researchers, professionals, and students in Cartography and GIScience.\nThe major goals of CaGIS are to:\n- Represent mapping and geographic information activities in the U.S. to the International Cartographic Association (ICA).\n- Bridge the activities of academia, government, and private industry to facilitate the transfer of knowledge and data between and within these sectors.\n- Promote research and educational practices in Cartography and GIScience.\n- Collaborate with governments and organizations at the national, state, regional, and local level that use mapping and GIScience.\n- Promote the scholarship and professional career development of students in mapping and GIScience.\n- Promote best professional practices, standards, and tools to create, use, and visualize geographic information.\nFive Things to Know About CaGIS\n- It is THE international network of developers, researchers and educators in the field of Cartography and GIScience.\n- It publishes research at the forefront of developments in Cartography and GIScience in the Society’s CaGIS Journal.\n- It hosts AutoCarto, the biennial conference that brings together the international community to present current research developments in Cartography and GIScience.\n- It is the United States representative to the International Cartographic Association (ICA), and participates in the biennial ICA conference that is hosted by different Countries around the world.\n- It sponsors an annual map competition that recognizes excellence in map design with recognition awards from National Geographic, ESRI, and Avenza.\nStatement on Diversity, Empathy, and Inclusion (DEI)\nCaGIS is committed to promoting diversity, inclusion and empathy throughout the cartographic and GIScience research, education, and professional communities. We strive to maintain an environment in which all are welcome to collaborate, contribute and support the advancement of GIScience. We strongly reject all forms of racism, sexism, ageism, homophobia, biphobia, transphobia, queerphobia, religious intolerance, ableism and any actions that infringe on the dignity of any individual.* Recognizing the importance and power of maps and GIS and acknowledging that every map involves choices that reflect particular perspectives and biases, we take responsibility as cartographers and GIScientists to ethically address the challenges of our time, respect the viewpoints of others and create an open community that is inviting and inclusive for all.\n*This sentence was taken with permission from the DEI statement developed by the North American Cartographic Information Society. The CaGIS statement was also informed by policies and statements from other organizations in the geospatial community.\nCaGIS Code of Conduct\nThe following Code of Conduct applies for all CaGIS conferences (both virtual and in person), related social events, and other Society activities. The purpose of the Code is to enhance every participant’s experience and increase the value of events while respecting the people and places where we meet.\nFirst and foremost, each person is responsible for their own actions. Expectations are that participants will:\n- Be kind and considerate\n- Respect each person and exhibit common courtesy rules for personal interaction\n- Be aware of your surroundings\n- Be mindful of others around you to ensure a great experience for your fellow participants\n- Refrain from displaying any demeaning, harassing, aggressive, or intimidating materials or behavior. Use welcoming and inclusive language and be respectful of differing viewpoints and experiences.\n- Respect property\n- Do not damage private or communal property\n- Obey the rules and regulations\n- Report inappropriate behavior\nInappropriate behavior related to harassment, aggression, and intimidation of event participants, venue staff, or comm unity members will not be tolerated. Harassment includes offensive verbal comments related to gender, gender identity and expression, sexual orientation, disability, group status, physical appearance, body size, race, religion, sexual images in public spaces, deliberate intimidation, stalking, following, unwelcome photography or recording, sustained disruption of talks or other events, inappropriate physical contact, and unwelcome sexual attention. Participants asked to stop any harassing behavior are expected to comply immediately.\nPresenters and exhibitors should not use sexualized images, activities, or other material, or otherwise create a sexualized environment. However, within the context of academic, artistic, and professional practices of cartography, critical examination of beliefs and viewpoints does not, by itself, constitute hostile conduct or harassment. Similarly, use of sexual imagery or language in the context of a professional discussion or presentation might not constitute hostile conduct or harassment.\nIf a participant engages in behavior outside of the expectations outlined above, event organizers may take any action they deem appropriate, including warning the offender or expulsion from the conference with no refund.\nIf you are being harassed, notice that someone else is being harassed, or have any other concerns, please contact the session chair, email the CaGIS Executive Director, firstname.lastname@example.org, or reach out to a member of the conference staff in person\nby going to the registration desk.\nThe CaGIS President, Executive Director (email@example.com), or the conference chairperson(s) will communicate immediately with anyone who reports an issue and will work quickly to address the problem.']	['<urn:uuid:5b8e823e-6f46-4f12-b4e7-0ba65efeb90d>', '<urn:uuid:71f448f7-9316-4c73-acc4-2708a5823043>']	open-ended	direct	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-12T23:00:04.658849	17	114	1867
10	high mix low volume production capacity planning challenges methods predict bottleneck formations	High-mix, low-volume production faces complex capacity planning challenges due to rapidly changing product mix and resource loads. Document 1 indicates that finite capacity scheduling (FCS) software is essential for predicting bottleneck formations and order completion times. Document 2 shows how modern ERP systems like Microsoft Dynamics 365 address this through features such as demand forecasting, capacity planning, and finite loading, which help track capacity constraints and balance workloads across work centers.	['Capacity Planning in High-Mix, Low-Volume, Make-To-Order Production\nCapacity planning is a planning exercise for correctly determining in advance the required resource hours for meeting fluctuating demand. According to Wikipedia, capacity planning is the process of determining the production capacity needed by an organization to meet changing demands for its products. This process is usually needed for any production system that has to handle spikes in demand using limited resources. Capacity planning is a challenging task in any order-driven, high-variety production.\nThe nature of make-to-order production\nA make-to-order (MTO) production unit makes a product only after it receives a customer order for the product. Such type of production is necessary when products are to be made to specifications given by customers. Custom manufacturing also involves MTO production. A high-mix, low-volume (HMLV) production unit has the capability to make a large variety of products for meeting diverse needs of customers. For most products in such high-variety systems, usually order quantities are small and the system makes a product only after receiving a customer order for the product. This is because maintenance of final goods inventory (FGI) for every product is prohibitive physically and economically. For this reason, most high-mix, low-volume production units usually operate like MTO units. Nevertheless, they maintain FGI for a few products which customers frequently need.\nThe complexity of make-to-order production\nMost MTO production units are job shops which are small in size and revenue. They keep receiving a stream of diverse orders to make a variety of products using very limited resources. But, they receive orders without any predictability. Process and resource requirements for fulfilling an order are also unknown until the manufacturer receives the order and therefore, advance preparation for the order is not possible. Customers want deliveries of their products within stipulated lead times. The production system is often more complex than a simple production line that makes products one after another. Many job shops simultaneously make several diverse products using common resources.\nImpact on production lead times, throughput and customer satisfaction\nIn MTO production units, product mix keeps changing rapidly and consequently, the workload levels for resources will keep changing and bottlenecks also keep shifting over time. If we cannot enhance resource capacities at right times even temporarily, the changing and heterogeneous workload in the system will have major impact on production lead times of orders and factory throughput. Also bottlenecks will increase those lead times and adversely affect customer satisfaction. Enhancement of resource capacities at right times will be helpful to control production lead times, increase customer satisfaction and increase throughput.\nOptimal determination of resource capacity enhancements\nIn HMLV and MTO production, capacity planning is the determination of resource capacity enhancements at right times for this purpose. The enhancements include overtime for workers, running machines for more hours, outsourcing bottleneck operations, etc. But, managers usually decide resource capacity enhancement in real time for quick inventory reduction at bottlenecks. This is a real-time, reactive approach to controlling inventory and job lead times. Some instantaneous decisions may quickly reduce inventories in real time but their impact on the overall on-time delivery performance may not be significant in MTO systems. There is relatively less flexibility with instantaneous decisions for enhancing resource capacities based on the existing situation in production. Such decisions may even amount to firefighting. Advance planning for resource capacity enhancements at right times as required will be very effective. This is known as proactive capacity planning.\nShort-term capacity planning involves running machines for a few extra hours, providing overtime for workers or running production for extra shifts on some days. Capacity planning may also involve outsourcing some operations or even entire production of some selected products. On the other hand, long-term capacity planning is done by hiring workers, training workers for additional skills, adding more machines or running extra shifts on long term basis.\nThe purpose of capacity planning\n- Expedite some jobs without any significant adverse impact on completion times of other jobs\n- Reduce the risk of delays for orders\n- Shorten production lead times for some orders\n- Overcome the adverse impact of major interruptions on job completion times\n- Mitigate the adverse effect of shifting bottlenecks\n- Accept more orders\n- Accept a new order with shorter lead time stipulated by customer\n- Accept and manage a rush order without adverse impact on other orders.\nProactive capaicty planning is very useful to:\nIn job shops engaged in MTO production, optimal proactive capacity planning is to plan in advance for minimal increase in resource capacities for meeting the above mentioned objectives. This is a real challenge regularly faced by many job shops engaged in MTO production. Those shops cannot easily do it for high-variety production without rigorous calculations.\nFinite Capacity Scheduling for Prediction of Work Flow and Bottleneck Formations\nIn make-to-order production, there is neither a simple formula nor an easy method for such capacity planning. Moreover, there is no unambiguous definition of capacity in such production. In many industries, capacity planning is done in Excel using experience, commonsense and crude calculations. However, the result is unsatisfactory in some cases, particularly in MTO environment. Rough-cut capacity planning which converts a master production schedule into requirements for key resources is not effective for MTO production with limited resources because it may not be possible to increase capacity of some resources temporarily as much as required.\nFor MTO production, capacity planning will be effective and dependable when it is based on (1) reliable prediction of work flow and bottleneck formations and (2) fast, accurate and extensive what-if analysis of production with the help of work flow prediction. But, the prediction and what-if analysis are quite difficult due to rapidly changing product mix and rapidly changing loads of limited resources. This is where production scheduling methodology can provide a lot of help in capacity planning. Even discrete event Monte Carlo simulation of production helps with capacity planning.\nThe effectiveness of capacity planning in high-variety production increases with the efficiency of production scheduling. Truly powerful scheduling software based on finite capacity scheduling (FCS) logic help predict (1) bottleneck formations in HMLV environment and (2) progress and completion time of each order.\nWhat-If Analysis for Proactive Capacity Planning\nWith their predictive capability, finite capacity scheduling software support fast, accurate and extensive what-if analysis for optimal, proactive capacity planning. FCS-based capacity planning is done by what-if analysis of finite capacity production schedules with respect to changes in resource capacities. What-if analysis is usually done by FCS for changes like:\n- Addition of a new machine with specific functionality\n- Hiring a new worker with certain skills\n- A worker acquiring an additional skill\n- Running an extra shift for some period\n- Overtime for some workers\n- Removal of a machine from production\n- A machine in non-operating condition while waiting for repair\n- A worker of certain skills leaving the system\n- A worker becoming absent for one or more days.\nIn summary, managers can easily perform proactive capacity planning in make-to-order production by performing what-if analysis of finite capacity schedules with the help of FCS. Powerful, low-cost FCS software are now easily available for this purpose.\nAdvantages of Capacity Planning:\n- Reduction in rejection of orders due to capacity concerns\n- Reduction of anxiety to meet due dates\n- Increased revenue due to acceptance of more demand and efficient management of demand spikes\n- Improved customer satisfaction with increase in on-time delivery and acceptance of rush orders without affecting other orders\n- Judicious and cost-effective investments on new resources for increasing production, revenue and profit.\nSoftware Tools for Capacity Planning in MTO Environment\nThere are some powerful, low-priced software tools for scheduling MTO production. These tools are also very useful for efficient capacity planning also. Unlike production scheduling, capacity planning does not demand job status updates from shop floor very regular because capacity planning is not required to be done as frequently as production scheduling. Some difficulties faced in using software for production scheduling disappear while doing the same for capacity planning. Production managers in some industries are using powerful scheduling tools exclusively for capacity planning and getting a lot of benefit from this exercise alone. Optisol software tools,Schedlyzer and Schedlyzer Lite are very appropriate for efficient, proactive capacity planning in a wide range of job shops engaged in MTO production. Both of them all support fast, accurate and extensive what-if analysis as part of proactive capacity planning in make-to-order production.', 'Microsoft Dynamics 365 Business Central offers a full set of business management capabilities. These include a wide range of features for manufacturing management. Manufacturers are increasingly finding the Dynamics cloud ERP solution to be an effective way to operate a manufacturing business while cutting the costs of running ERP on-premises.\nManufacturing Management at the Sales Stage\nSome of the most important work in managing a manufacturing operation occurs before any actual manufacturing takes place. Accurate sales forecasting leads to effective capacity planning, staffing, inventory ordering and more. Dynamics 365 Business Central’s Demand Forecasting allows manufacturing managers to predict manufacturing volumes based on sales forecasts from sales management functions in the broader system.\nA Sales and Inventory Forecast extension offers insights into potential sales as well as expectations of stock-outs. The toolset can look at historical sales data to offer stock recommendations and suggestions for replenishment. These features connect directly to purchase order generation and materials requirement planning in the solution.\nThe Bill of Materials and Production Orders\nDynamics 365 Business Central offers extensive features for creating a Production Bill of Materials (BOM). The BOM lists all the materials and other inputs needed to manufacture a product. The tool also provides for version management. This is useful because manufacturers often have multiple versions of BOMs in use. Managers can use the tool to generate Production Orders. These include instructions for manufacturing a particular order or batch. An Order Planning window gives managers visibility and the tools they need to plan for demand from sales forecasts.\nThe term “agile manufacturing” refers to tools, processes and training that enable a manufacturing organization to respond rapidly to market shifts and customer needs—while still maintaining quality and controlling costs. This is largely a matter of software. Dynamics 365 includes an Agile Manufacturing module that enables agile manufacturing by means of dynamic supply and capacity planning.\nSupply planning makes it possible to map out requirements for materials based on demand, working in tandem with master production scheduling. Dynamics allows for automatic production order and purchase order creation as well as messaging and other tools for balancing supply and demand. This includes reordering processes for internal and third-party materials orders.\nCapacity planning and finite loading round out the agile manufacturing capabilities of Dynamics 365 Business Central. Manufacturing managers can use the software to add work centers to the manufacturing operation. This feature also includes setting up routing for production orders and material requirements planning (MRP). The application provides visualizations of loads the tasks. Finite loading involves tracking capacity constraints on work centers to avoid overloading capacity.\nA Machine Center is a locus of manufacturing operations built around related pieces of equipment. Dynamics 365 enables managers to establish machine centers as part of the manufacturing capacity planning and production management processes. The tool lets managers see the functioning of machine centers at the individual level and with a consolidated view. With machine centers, it is possible to monitor manufacturing activity, collect information on productivity as well as scrap percentages, and so forth.\nManufacturing management can be challenging, especially as product development and marketing cycles accelerate. The right manufacturing management software can make a difference in terms of productivity and profitability of a manufacturing operation. Microsoft Dynamics 365 Business Central offers a comprehensive feature set for effective manufacturing management. For businesses exploring the potential of technology to improve manufacturing management, this software is worthy of strong consideration.']	['<urn:uuid:e6fd63af-a300-4d10-a0b4-6c5f2e051b00>', '<urn:uuid:6f4b76db-279d-453a-9aac-bb7b0b488ffe>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T23:00:04.658849	12	71	1973
11	rescue damaged historical films techniques physical digital transfer	For damaged films, there are several rescue methods. During early deterioration stages, content can be transferred to new film stock. For sheet films with intact emulsion, solvents can be used to separate the emulsion from the degraded base and transfer it to a new support, though this is delicate and expensive. While digitization seems ideal, current scanning technology cannot match the original picture and sound quality. Therefore, the National Film Preservation Institute recommends film-to-film transfer as the best preservation method. This is crucial given historical cases like William Desmond Taylor's films, which were cut into pieces and burned when they degraded, or how only one reel of 'Flaming Youth' survives in the Library of Congress.	"['Along with Marilyn Ferdinand of Ferdy on Films, Greg Ferrara of Cinema Styles, and more than fifty other bloggers from around the world, the Siren urges anyone reading us this week to give something for our film heritage.\nPlease also check with your employer; some corporations match funds donated to the NFPF. If you know of such companies, please post them in comments.\nTo start things off, the Siren offers part one of a primer on Why We Fight (which film series, by the way, is on the list of films preserved by NFPF).\nFrom A Cast of Killers, Sidney D. Kirkpatrick\'s book about King Vidor\'s quest to film, and solve, the William Desmond Taylor murder. It\'s 1967 and Vidor is visiting the Paramount lot:\nIn front of the white post-production building, teeming with activity from the Christmas releases, he saw an old friend, an attractive blonde, who worked in the film library. He complimented her miniskirt, then asked if he could take a look at some of William Desmond Taylor\'s films. He was particularly interested in Taylor\'s last production, The Top of New York. She went to the office to see what she could do.\nOver the years Vidor and [Colleen] Moore had seen many of their own films destroyed. In the early sixties, silent films were thought to have no commercial value. They were difficult to store, dangerous to handle, and a fire hazard. Vidor\'s had been destroyed in a Bekins Storage Company fire, and Colleen\'s entire collection had burned up in a fire at Warner Brothers.\nVidor\'s friend brought him the news of Taylor\'s films. What titles had not disintegrated by the fifties were in such bad condition that they were taken out of the vaults, cut into small pieces with a chain saw, then burned, to salvage the silver content of the film stock.\nThe picture above is from the set of The Sky Pilot, the film Moore and Vidor made together; according to the Silent Era website, a print survives. Of Flaming Youth, the movie that made Moore\'s name and gave the world the flapper, only one reel survives in the archives of the Library of Congress.\nFrom Anita Loos\'s screenplay for A Virtuous Vamp, starring Constance Talmadge; included as an appendix to The Talmadge Girls. The movie opens just after the San Francisco earthquake of 1906. Gwen (Constance Talmadge), whose sole salvageable possession was a mirror, has just found her mother wandering near their former home, carrying a bicycle wheel and a garbage can.\nThe mother finally comes to and realizes what she has been carrying all the while, looks at the bicycle wheel and the garbage can and then at Gwen and says:\nSP. HAVE I BEEN CARRYING THESE THINGS ALL THE WAY FROM THE HOUSE?\nGwen says yes, evidently she has. Mrs Armitage shakes her head in despair, looks up at Gwen and says, ""Whatever will become of us?"" and begins to weep. Gwen puts her arms about her and comforts her. The mother then looks up at Gwen and says:\nSP. DO YOU REALIZE, CHILD, THAT ALL WE HAVE IN THE WORLD IS A BICYCLE WHEEL AND A GARBAGE CAN?\nGwen then holds out her mirror and says, ""But we have a mirror,"" and then looks at [her brother] Eddie who enters with his contribution, saying, ""Look what I got."" The mother takes it and looks at it:\nSP. YOUR FATHER\'S FAMILY CREST! THAT WILL HELP A LOT.\nShe takes the crest, lifts the garbage can lid, and drops it in. The kid, resenting this, goes around and fishes it out. The mother turns to Gwen and says:\nSP. WHY COULDN\'T ONE OF US HAVE SAVED A CAN OF SOUP?\nThe Silent Era website lists the survival status of A Virtuous Vamp as ""unknown.""\nFrom Peter Bogdanovich\'s interview with Raoul Walsh, in Who the Devil Made It?:\nNo print of The Life of General Villa is known to have survived.\nRaoul Walsh: (discussing The Life of General Villa) I spent four or five months with Villa, photographing some of the battles..Then I\'d photograph all the executions. In the morning they\'d line up these Federales against the wall and shoot them. I remember the first morning we set up to photograph this and I looked around: there was a bunch of Mexicans there, some with rocks in their hands and another bunch there with knives. I said, ""Jesus, this doesn\'t look too good."" The cameraman was Dutch (I can\'t think of his name); I said, ""You\'d better be able to run, Schultz--I don\'t like the look of things."" So this guy sat there, looked at us and said, ""We are ready now."" They shot these guys, and the bastards with the rocks ran in, opened the guys\' mouths and knocked the gold teeth out. And the others with the knives went in and started to cut their pants and take their boots off.\nBogdanovich: Did you photograph all that?\nWalsh: Yes. When I got home, [D.W.] Griffith said, ""My God, Raoul, you\'ve scared us all out of the projection room."" I said, ""Well, you wanted the life of Villa--you got it.""\nFrom Betty Blythe\'s interview with Kevin Brownlow in The Parade\'s Gone By:\n[Director J. Gordon] Edwards returned to this country, and his whole flash of genius as a director just went to ashes...Now I just carry in my heart the memory of this beautiful thing--The Queen of Sheba.\nThe Queen of Sheba is a lost film.\nJohn Ford, quoted in Bogdanovich\'s On Ford:\nIt was quite novel at the time--instead of riding to the rescue through Western scenery--they rode down Broadway at full tilt, weaving in and out. We went to downtown Los Angeles and rode the cowboys down the streets with a camera car ahead of them. And not a horse slipped.\nBucking Broadway was presumed lost until 2002, when it was located in the archives of the French National Center for Cinematography. The film was restored by Archives Françaises du Film in cooperation with the Museum of Modern Art.\nFrom The Los Angeles Times, Jan. 6, 1985; ""Old Films Are Getting a New Look,"" by Kevin Thomas:\nWhen construction was started on a skating rink on the site of a long-filled-in swimming pool in Dawson, the historic old gold-rush town in the Yukon, workers struck another kind of lode-a whopping 550 reels of film preserved half a century beneath the permafrost.\nIt seems that Dawson had been the end of the line on the old movie circuits-and the local bank, which had been storing the film in its basement, early on ran out of space for so many cans. Among other treasures, the trove yielded Douglas Fairbanks\' long-lost The Half Breed (1916).\nMore recently, a couple remodeling their porch found two reels of film buried under it that turned out to be the Civil War drama An Angel in Contention (1914) with Lillian Gish. But in the couple of months that it took for the American Film Institute to get hold of it, the nitrate had disintegrated so rapidly that now only seven frames exist.\nAfter publication of this article, it turned out that only two reels of The Half Breed, which was directed by Alan Dwan, had survived.', 'Cellulose acetate film\nCellulose acetate film, or safety film, is used in photography as a base material for photographic emulsions. It was introduced in the early 20th century by film manufacturers as a safe film base replacement for unstable and highly flammable nitrate film.\nBeginning with cellulose diacetate in 1909, this innovation continued with cellulose acetate propionate and cellulose acetate butyrate in the 1930s, and finally in the late 1940s, cellulose triacetate was introduced, alongside polyester bases. These less flammable substitutes for nitrate film were called safety film.\nThe motion picture industry continued to use cellulose nitrate supports until the introduction of cellulose triacetate in 1948, which met the rigorous safety and performance standards set by the cinematographic industry. The chemical instability of this material, unrecognized at the time of its introduction, has since become a major threat for film collections.\nDecay and the ""vinegar syndrome""\nThe first instance of cellulose triacetate degradation was reported to the Eastman Kodak Company within a decade of its introduction in 1948. The first report came from the Government of India, whose film was stored in hot, humid conditions. It was followed by further reports of degradation from collections stored in similar conditions. These observations resulted in continuing studies in the Kodak laboratories during the 1960s.\nBeginning in the 1980s, there was a great deal of focus upon film stability following frequent reports of cellulose triacetate degradation. This material releases acetic acid, the key ingredient in vinegar and responsible for its acidic smell. The problem became known as the ""vinegar syndrome.""\nThe progression of degradation\nIn acetate film, acetyl (CH3CO) groups are attached to long molecular chains of cellulose. With exposure to moisture, heat, or acids, these acetyl groups break from their molecular bonds and acetic acid is released. While the acid is initially released inside the plastic, it gradually diffuses to the surface, causing a characteristic vinegary smell.\nThe decay process follows this pattern:\n- Acetic acid is released during the initial acetate base deterioration, leading to the characteristic vinegar odor. This signal marks the progression of deterioration.\n- The plastic film base becomes brittle. This occurs in the advanced stages of deterioration, weakening the film and causing it to shatter with the slightest tension. These physical changes happen because cellulose acetate consists of long chains of repeating units, or polymers. When the acetic acid is released as these groups break off, the acidic environment helps to break the links between units, shortening the polymer chains and leading to brittleness.\n- Shrinkage also occurs during this process. With the cellulose acetate polymer chains breaking into smaller pieces, and with their side groups splitting off, the plastic film begins to shrink. In advanced stages of deterioration, shrinkage can be as much as 10%. A 1% reduction in size renders motion picture film unusable.\n- As the acetate base shrinks, the gelatin emulsion of the film does not shrink, because it is not undergoing deterioration. The emulsion and film base separate, causing buckling, referred to by archivists as \'channelling.\' Sheet films are often severely channelled in the later stages of degradation.\n- Crystalline deposits or liquid-filled bubbles appear on the emulsion. These are evidence of plasticizers, additives to the plastic base, becoming incompatible with the film base and oozing out on the surface. This discharge of plasticizers is a sign of advanced degradation.\n- In some cases, pink or blue colors appear in some sheet films. This is caused by antihalation dyes, which are normally colorless and incorporated into the gelatin layer. When acetic acid is formed during deterioration, the acidic environment causes the dyes to return to their original pink or blue color.\nTesting for degradation\nA testing product developed by the Image Permanence Institute, A-D, or ""acid-detection"" indicator strips change color from blue through shades of green to yellow with increasing exposure to acid. According to the test User\'s Guide, they were ""...created to aid in the preservation of collections of photographic film, including sheet and roll films, cinema film, and microfilm. They provide a nondestructive method of determining the extent of vinegar syndrome in film collections.""  These tools can be used to determine the extent of damage to a film collection and which steps should be taken to prolong their usability.\nPreservation and storage\nCurrently there is no practical way of halting or reversing the course of degradation. While there has been significant research regarding various methods of slowing degradation, such as storage in molecular sieves, temperature and moisture are the two key factors affecting the rate of deterioration. According to the Image Permanence Institute, fresh acetate film stored at a temperature of 65°F (18°C) and 50% relative humidity will last approximately 50 years before the onset of vinegar syndrome. Reducing the temperature 15°, while maintaining the same level of humidity, delays the process by 150 years. A combination of low temperature and low relative humidity represents the optimum storage condition for cellulose acetate base films, however, in practice temperatures of 55°F (12°C) and a relative humidity of 35% are now being used.\nMicroenvironments—the conditions inside an enclosure—can also have an impact on the condition of cellulose acetate film. Enclosures that are breathable or that contain an acid absorbent are instrumental in reducing the rate of decay due to vinegar syndrome. Sealed metal containers can trap the decay products released by the film, promoting the spread of vinegar syndrome.\nRescuing damaged film\nDuring early stages of decay, the film content can be rescued by transferring it to new film stock. Once the film becomes brittle it cannot be copied in its entirety. Because the gelatin emulsion usually stays intact during the degradation process, it is possible to save the image on sheet film using solvents to dissolve the emulsion away from the shrunken base. Once the emulsion has been freed from the shrunken support, it can be photographed or transferred to a new support. Because of the solvents used, this is a delicate and potentially hazardous procedure and is an expensive process for a large collection. Degraded motion picture film cannot be restored in this way, but sheet films often can.\nWhile digitization would be an ideal way to preserve the contents of cellulose acetate film, current standards do not allow for scanning at sufficient resolutions to produce a copy of the same picture and sound quality as the original. Currently, the National Film Preservation Institute advocates film-to-film transfer as the best method for film preservation, with the copies stored in proper environmental conditions.\nCellulose acetate film is also used to make replicates of materials and biological samples for microscopy. The techniques were developed for metallographic needs to examine the grain structure of polished metals. Replication can be used to understand the distribution, for example, of different types of iron in carbon steel samples, or the fine distribution of damage to a sample subject to mechanical wear.\n- ^ National Film Preservation Foundation. The Film Preservation Guide: The Basics for Archives, Libraries, and Museums. San Francisco: National Film Preservation Foundation, 2004, 9.\n- ^ Ram, A. Tulsi. “Archival Preservation of Photographic Film-A Perspective.” Polymer Degradation and Stability 29 (1990), 4.\n- ^ Adelstein, P.Z., J.M. Reilly, D.W. Nishimura, and C.J. Erbland. ""Stability of Cellulose Ester Base Photographic Film: Part I-Laboratory Testing Procedures."" SMPTE Journal 101 no.5 (1992): 336.\n- ^ James M. Reilly. ""Basic Strategy for Acetate Film Preservation."" Microform and Imaging Review 31 no.4 (2002), 117.\n- ^ Image Permanence Institute. User\'s Guide for A-D Strips: Film Base Deterioration Monitor. Rochester: Image Permanence Institute, 2001.\n- ^ Allen, N.S., M. Edge, C.V. Horie, T.S. Jewitt, and J.H. Appleyard. “Degradation of Historic Cellulose Triacetate Cinematograph Film: Influence of Various Film Parameters and Prediction of Archival Life.” The Journal of Photographic Science 36 no. 6 (1998), 194.\n- ^ Reilly, James M. IPI Storage Guide for Acetate Film; Instructions of Using the Wheel, Graphs, and Table; Basic Strategy for Film Preservation. Rochester: Image Permanence Institute, 1993.\n- ^ Adelstein, P.Z., J.M. Reilly, D.W. Nishimura, and C.J. Erbland. “Stability of Cellulose Ester Base Photographic Film: Part II-Practical Storage Considerations.” SMPTE Journal 101 no. 5 (May 1992): 353.\n- ^ ""Film and Media Storage"". http://www.bonded.com/storage.php.\n- ^ J.L. Bigourdan and J. Reilly, “Effectiveness of Storage Conditions in Controlling the Vinegar Syndrome: Preservation Strategies for Acetate Base Motion-Picture Film Collections”, Image Permanence Institute, Rochester Institute of Technology.\n- ^ Reilly, James M. “Basic Strategy for Acetate Film Preservation.” Microform and Imaging Review 31 no. 4 (2002): 118.\n- ^ www.filmpreservation.org/preservation/film_guide.html\n- ^ [http://www.sciencedirect.com/science/article/pii/0026080084900028 Measurement of the interlamellar spacing of pearlite ]\n- ^ Mechanisms of wear of the metal surface during fretting corrosion of steel on polymers\n- Adelstein, P.Z., J.M. Reilly, D.W. Nishimura, and C.J. Erbland. “Stability of Cellulose Ester Base Photographic Film: Part I-Laboratory Testing Procedures.” SMPTE Journal 101 no. 5 (May 1992): 336-346.\n- Adelstein, P.Z., J.M. Reilly, D.W. Nishimura, and C.J. Erbland. “Stability of Cellulose Ester Base Photographic Film: Part II-Practical Storage Considerations.” SMPTE Journal 101 no. 5 (May 1992): 347-354.\n- Adelstein, P.Z., J.M. Reilly, D.W. Nishimura, and C.J. Erbland. “Stability of Cellulose Ester Base Photographic Film: Part III-Measurement of Film Degradation.” SMPTE Journal 104 (May 1995): 281-291.\n- Adelstein, P.Z., J.M. Reilly, D.W. Nishimura, C.J. Erbland, and J.L. Bigourdan. “Stability of Cellulose Ester Base Photographic Film: Part V- Recent Findings.” SMPTE Journal 104 no. 7 (July 1995): 439-447.\n- Allen, N.S., M. Edge, C.V. Horie, T.S. Jewitt, and J.H. Appleyard. “The Degradation and Stabilization of the Historic Cellulose acetate/ Nitrate Base Motion-picture Film.” The Journal of Photographic Science 36 no.3 (1988): 103-106.\n- Allen, N.S., M. Edge, C.V. Horie, T.S. Jewitt, and J.H. Appleyard. “Degradation of Historic Cellulose Triacetate Cinematograph Film: Influence of Various Film Parameters and Prediction of Archival Life.” The Journal of Photographic Science 36 no. 6 (1998), 194-198.\n- Allen, N.S., M. Edge, C.V. Horie, T.S. Jewitt, and J.H. Appleyard. “The Degradation Characteristics of Archival Cellulose Triacetate Base Cinematograph Film.” The Journal of Photographic Science 36 no. 6 (1998), 199-203.\n- Allen, N.S., M. Edge, T.S. Jewitt, and C.V. Horie. “Initiation of the Degradation of Cellulose Triacetate Base Motion Picture Film.” The Journal of Photographic Science 38 no. 2 (1990): 54-59.\n- Allen, N.S., J.H. Appleyard, E. Edge, D. Francis, C.V. Horie, and T.S. Jewitt. “The Nature of the Degradation of Archival Cellulose-Ester Base Motion-Picture Film: The Case for Stabilization.” The Journal of Photographic Science 36 no.2 (1988): 34-39.\n- Allen, N.S., M. Edge, T.S. Jewitt, and C.V. Horie. “Stabilization of Cellulose Triacetate Base Motion Picture Film.” The Journal of Photographic Science 30 no.1 (1990):26-29.\n- Bigourdan, Jean-Louis and James M. Reilly. “Effectiveness of storage Conditions in Controlling the Vinegar syndrome: Preservation Strategies for Acetate Base Motion-Picture Film Collections.” In Michelle Aubert and Richard Billeaud. Archiver et communiquer l\'image et le son :les enjeux du 3ème millenaire : actes du Symposium Technique Mixte—JTS Paris 2000, 14-43. Paris: CNC, 2000.\n- Edge, M. and N.S. Allen. “Fundamental Aspects of the Degradation of Cellulose Triacetate Base Cinematograph Film.” Polymer Degradation and Stability 25 no. 2-4 (1989): 345-362.\n- Horvath, David G. (1987). The Acetate Negative Survey Final Report. Louisville, KY: Ekstrom Library Photographic Archives, University of Louisville. \n- Meyer, Mark-Paul and Paul Read. “Restoration and Preservation of Vinegar Syndrome Decayed Acetate Film.” In Michelle Aubert and Richard Billeaud. Archiver et communiquer l\'image et le son :les enjeux du 3ème millenaire : actes du Symposium Technique Mixte—JTS Paris 2000, 54-65. Paris: CNC, 2000.\n- National Film Preservation Foundation. The Film Preservation Guide: The Basics for Archives, Libraries, and Museums. San Francisco: National Film Preservation Foundation, 2004.\n- Ram, A.T. “Archival Preservation of Photographic Films-A Perspective.” Polymer Degradation and Stability 29 no. 1 (1990): 3-29.\n- Ram, A.T., D.F. Kopperl, and R.C. Sehlin. “The Effects and Prevention of Vinegar Syndrome.” The Journal of Imaging science and Technology 38 no. 3 (1994): 249-261.\n- Reilly, James M. “Basic Strategy for Acetate Film Preservation.” Microform and Imaging Review 31 no. 4 (2002): 117-130.\n- Reilly, James M. IPI Storage Guide for Acetate Film; Instructions of Using the Wheel, Graphs, and Table; Basic Strategy for Film Preservation. Rochester: Image Permanence Institute, 1993.\nWikimedia Foundation. 2010.\nLook at other dictionaries:\nCellulose acetate — Cellulose acetate, first prepared in 1865, is the acetate ester of cellulose. Cellulose acetate is used as a film base in photography, and as a component in some adhesives; it is also used as a synthetic fiber.Acetate fiber and triacetate… … Wikipedia\ncellulose acetate — n. chemical used in photographic film and the yarn and textile inustry … English contemporary dictionary\nCellulose triacetate — Cellulose triacetate, also known simply as triacetate, is manufactured from cellulose and acetate. Triacetate is typically used for the creation of fibres and film base.It is similar chemically to cellulose acetate, with the distinguishing… … Wikipedia\nacetate — ► NOUN 1) Chemistry a salt or ester of acetic acid. 2) fibre or plastic made of cellulose acetate. 3) a transparency made of cellulose acetate film … English terms dictionary\nFilm preservation — Stacked containers filled with reels of film stock. The film preservation, or film restoration, movement is an ongoing project among film historians, archivists, museums, cinematheques, and non profit organizations to rescue decaying film stock… … Wikipedia\nacetate — [ asɪteɪt] noun 1》 Chemistry a salt or ester of acetic acid: lead acetate. 2》 textile fibre or plastic made of cellulose acetate. ↘a transparency made of cellulose acetate film. ↘a direct cut recording disc coated with cellulose acetate … English new terms dictionary\nfilm — [ film ] n. m. • 1889; mot angl. « pellicule » 1 ♦ Pellicule photographique. Développer un film. Rouleau de film. ♢ (1896) Plus cour. Pellicule cinématographique; bande régulièrement perforée. Film de 35 mm (format professionnel). Films de format … Encyclopédie Universelle\nCellulose — is an organic compound with the formula chem|(C|6|H|10|O|5|)|n, a polysaccharide consisting of a linear chain of several hundred to over ten thousand β(1→4) linked D glucose units.cite book author=Crawford, R. L. title=Lignin biodegradation and… … Wikipedia\nfilm — /fɪlm / (say film) noun 1. a thin layer or coating. 2. a thin sheet of any material. 3. Photography a. the sensitive coating, as of gelatine and silver bromide, on a photographic plate. b. a strip or roll of cellulose nitrate or cellulose acetate … Australian English dictionary\nFilm base — A film base is a transparent substrate which acts as a support medium for the photosensitive emulsion that lies atop it. Despite the numerous layers and coatings associated with the emulsion layer, the base generally accounts for the vast… … Wikipedia']"	['<urn:uuid:dab06378-d0f8-4490-83b5-25bd0e933818>', '<urn:uuid:acc84dc5-1fbf-469d-a0db-cbc58b28cbd5>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T23:00:04.658849	8	115	3634
12	What makes old coins more valuable than others?	Several factors determine a coin's value. The mint year and mint location (marked with letters like P for Philadelphia, D for Denver, S for San Francisco) are crucial since different mints produced varying quantities. The coin's condition and grade (degree of wear) significantly impact value. The country of origin can affect value, with non-U.S. coins sometimes prized higher by U.S. collectors due to rarity. Damage, including improper cleaning, corrosion, dents, or holes, can make a valuable coin nearly worthless. For U.S. coins specifically, certain types are particularly collectible, such as Flying Eagle Cents (1856-1858), Mercury Dimes (1916-1945), and Morgan Dollars (1878-1921). The coin's authenticity is crucial - while some counterfeits may have collector value, most are worthless.	['How to Detect Counterfeit Coins\nCounterfeit coins are nothing new to the world of coin collecting; they have been around for centuries (if not millennia). To avoid getting duped by an unscrupulous coin dealer or collector into unknowingly purchasing a counterfeit coin, there are a few basic techniques and tools that can help. In this article, I present some basic ways to evaluate your coins to determine if they are genuine or counterfeit. The methods presented below are not foolproof but can help identify the more obvious counterfeit coins.\nDetermine If the Coin Is Magnetic\nSince silver, gold, and copper coins are not attracted to a magnet, this can be a simple test to tell if the coin has some iron content. Ferrous materials, such as iron and steel, will attach to a magnet and are not part of nearly any United States coins. A good example is a 1943 steel cent that has been copper coated to be passed off as a rare 1943 copper cent. The genuine copper version of the coin will not be attracted to the magnet; however, a copper coated steel cent will be strongly attracted to the magnet.\nAll United States silver and gold coins are not attracted to a magnet. Many Chinese counterfeit coins are magnetic, and it’s a dead giveaway if a silver-looking coin is attracted to a magnet. A strong Alnico magnet (see the figure) works well for counterfeit detection and can be purchased online for a few dollars. Be cautious with the magnet when you place it near a coin or object that has iron in it; the object will literally leap at the powerful Alnico magnet, and you don’t want to be in the way.\nDetermine the Weight of the Coin\nMany counterfeit coins do not weigh the correct amount, and weighing a coin is a simple method to determine if it was struck on a planchet (or blank) made from the wrong alloy. A weight measurement and comparison to the weights is listed in A Guide Book to United States Coins (normally called the Red Book); this will give valuable information to help you determine the authenticity of the coin. If the coin does not weigh the proper amount, then it is probably counterfeit. A more complicated situation arises when a coin weighs the correct amount but still may be a counterfeit. An electronic scale capable of measuring up to 50 grams with 0.01-gram accuracy can be purchased online for less than $25.00. They are typically used in the jewelry business.\nOne point to consider when using weight as a discriminator is that when genuine coins circulate, very small quantities of the metal wear off. Silver and gold are soft metals and tend to wear more rapidly than a copper or nickel alloy coin. As a result, your genuine coin with lots of wear will weigh less than the specified value by a few tenths of a gram, sometimes more if the wear is excessive. If the weight of your coin is more than the specified amount, then you probably have a fake.\nMeasure the Diameter of the Coin\nCounterfeit coins may not have the correct diameter, which is a good indication that the coin may not be genuine. A Vernier Caliper is a simple device used to measure the diameter of coins to +/- 0.1 mm. As with weight, the correct diameter is not a guarantee the coin is genuine, but it is a good data point to help you determine if the coin is real. Plastic versions of this measuring device cost less than $10.00.\nVisual Examination Using a 10x Magnifying Glass\nUsing the 10x magnifying glass to carefully examine the coin’s surfaces and edge to look for irregularities can be very helpful in your quest to detect a counterfeit. Some things to check for: seam around the edge indicating a cast coin, small raised spots or depressions in the field of the coin, and unrealistic or flawed styling of devices and legends. Compare the coin to the picture in the Red Book or online photos of the genuine coin to look for obvious mistakes made by counterfeiters. Pay careful attention to the date and mint mark of the coin; an added or removed mint mark or altered date can make a large difference in the value of a coin.\nOne of the more common types of counterfeit coins is cast copies. These coins generally are manufactured not for the purpose of deception, but rather to create a copy of a rare or beautiful coin as a souvenir or promotional giveaway. Many times, the word “copy” or “replica” will appear on the reverse (back) of the coin. Cast copies are normally identified by a seam that runs around the outside edge or circumference of the coin. The seam is a result of metal that flows at the point where the two molds, obverse and reverse, come together. Most cast counterfeit coins are underweight when compared to the genuine coin. Cast coins typically have less detail than expected for a genuine coin of the same grade. Base metals, such as copper, nickel, zinc, and iron, are normally used as a substitute for the precious metal, such as gold, silver, or platinum, that would we used in genuine rare coins.\nDetermination If a Coin Is Made of Silver\nThere are different metal alloys that counterfeiters use that appear to be silver. A “Ring Test” is a simple test that can be performed to determine if a coin is silver. The potential counterfeit coin is balanced on the tip of a finger then struck lightly with a known silver coin on the edge. If the coin is silver it will “ring” for several seconds with a sweet, high-pitched sound. The higher the silver content, the longer the ring will last. A base metal coin will sound like a “thud” or a “clunk” when struck with the other coin. Be careful with this test to not drop the coin being tested when you strike it with the silver coin. I recommend practicing with known genuine coins before you evaluate potential counterfeit coins. The more you practice the test with genuine coins, the more easily you will be able to determine the difference between the sound of silver and non-silver coins.\nThere are other methods to identify counterfeit coins, such as chemical acid testing, an electronic tester, specific gravity testing, laboratory material properties analysis, and die variety studies. These are more advanced topics to be covered elsewhere.\nOne way to turn a genuine common coin into a valuable rarity is by altering the coin in a specific way. One of the most common alterations is the addition of a mint mark to a coin that did not originally have a mint mark. For example, by adding an “S” mint mark to a genuine 1909 VDB Lincoln cent (a $10 coin), the forger makes the valuable ($500 plus) 1909-S VDB cent. The mint mark is removed from a common “S” mint coin and epoxied or soldered onto the 1909 VDB coin—and voila!—an instant rarity is created. Another example of where adding a mint mark greatly increases the value of a coin is the 1916 Mercury dime. By adding a “D” mint mark to the reverse of a 1916 dime minted at Philadelphia, which has no mint mark, the value of the coin goes from a few dollars to a few hundred or more.\nOne other type of alteration is changing the date of the coin. For example, a counterfeiter will start with a common 1944-D Lincoln cent, worth a few cents, and alter the first “4” in the date to make it look like a “1.” This turns a 1944-D into a rare 1914-D cent worth a hundred dollars or more. This alteration can be spotted by noting the improper spacing between the digits in the date. Other such alterations are made by changing a 1941 Walking Liberty half dollar into a 1921, or manipulating the date of a common Indian Head cent to the rare 1877.\nThe way to detect if a coin has been altered is by knowing the characteristics of a genuine coin. Since most collectors don’t have a safe full of genuine rare coins for comparison, the next best thing is to look at pictures of genuine coins and their specific die characteristics. The books and the websites listed in the Reference section of this article contain many details you will need to use to evaluate a potentially altered coin.\nProfessionally Graded and Authenticated Coins\nThe methods presented in the guide will help you identify most of the more obvious counterfeit coins. If you are spending several hundreds or thousands of dollars for a coin, it is recommended that you purchase the coin from a very reputable coin dealer with a return policy or purchase a coin that has been professionally graded and encapsulated by services such as PCGS, ANACS, NGC, ICG, or NCS. Also, professionally graded coins are in inert and sealed holders that are excellent for long term storage and are much easier to sell.\nKnow Your Coin Dealer\nLike nearly every other area of commerce, coin collecting has moved to the internet. Though this change has offered the collector a much larger array of coins to collect, it has also widened the gap between the coin collector and the coin dealer. If you buy your coins from a local person, for example, at a coin club meeting, a coin show, or at a coin shop, try to find out if the dealer is reputable. Most coin dealers are honest, but there are bad actors that you should avoid—ask around for references. The same advice applies when you purchase coins from a dealer on the internet.\nFor example, on eBay, check to see if the dealer has a lot of positive feedback or if they are a novice. If a dealer is a member of a professional organization like the American Numismatic Association (ANA) or the Professional Numismatists Guild (PNG), that helps build their credibility. When spending your hard-earned money on rare coins, the old saying caveat emptor applies, or “Let the buyer beware.” Just be a little cautious; do your homework on the rare coin you are interested in and the dealer selling the coin.\nFivaz, Bill. United States Gold Counterfeit Detection Guide. Whitman Publishing. 2005.\nLonesome John. Detecting Counterfeit Coins Book 1, Heigh Ho Printing Company. 1985.\nCounterfeit Detection: A Reprint from The Numismatist, American Numismatic Association. 1988.\nCounterfeit Detection: A Reprint from The Numismatist Volume II, American Numismatic Association. 1988.\nTravers, Scott A. (editor). Official Guide to Coin Grading and Counterfeit Detection. The House of Collectables. 1997.\nYeoman, R.S. and Jeff Garret (editor). A Guide Book of United States Coins 2020. Whitman Publishing. 2019.\n“Counterfeit Coin Detection – The Top 10 Most Common Counterfeit US Coins” https://coinweek.com/education/coin-grading/ngc/couinterfeit-coin-detection-counterfeit-detection-the-top-10-most-common-counterfeit-us-coins/\n“The Fundamentals Of Counterfeit Detection - Part 1” https://www.pcgs.com/news/the-fundamentals-of-counterfeit-detection--part-1\n“Counterfeit Detection” https://www.ngccoin.com/resources/counterfeit-detection/', 'Value Collectible Coins\nA look at the criteria used to value collectible coins.\nBefore a collectible coin can be valued, a coin dealer or collector will examine it closely. They will then categorize the coin by placing it into one of the obvious categories by denomination or type.\nThis is not always as easy as it seems. Every now and then a mint will strike odd coins, medals or tokens aimed primarily at collectors.\nCoin sets commemorating the Olympics or a significant historical event such as the first Apollo moon landing or unusual denominations are the most common examples.\nMints have produced half cent coins, large-sized cents, three cent nickels, seated quarters, bust dollars and many others over the decades. All complicating how to value collectable coins.\nThese commemorative sets help serious collectors collect full sets of coin types. Often full or complete sets are too expensive for the average collector to purchase, so they’ll try to get one of each type over a period of time.\nSo categorizing a coin is important in assessing the coins value depending possibly, on how rare it may be and how difficult it is to complete a set.\nAmerican Coins: Common types of collectible American coinage includes.\nCents – which have been minted since 1856 in the U.S. break down into several common collectible categories. The Flying Eagle Cent, minted from 1856-1858 is one. The Indian Head Cent – 1859-1909 is another. More recent mints are the Wheat Back Lincoln Cent (1909-1958).\nNickels – which are often actually made of a nickel-copper alloy, are 1/20th of a dollar by definition. Historical examples are the Liberty Head ‘V’ produced from 1883 to 1913. An authentic 1913 is especially rare and valuable, though many counterfeits exist. The Buffalo Nickel then took over from 1913 to 1938. Many nickels minted during WWII were a composite of copper, silver and manganese, showing that a nickel isn’t always made of nickel.\nDimes – initially 90% silver and 10% copper, dimes have seen their silver content reduced since first established in 1792. Beginning in 1965 the dimes value was debased and the coins made of 75% copper and 25% nickel, chosen specifically since they were not precious metals. As a result older dimes are generally worth more on the open market.\nAmong the more common collectibles is the Mercury Dime minted from 1916-1945, but the older Barber Dime – minted from 1892-1916 can still be found by diligent amateur collectors.\nQuarters – Valuable quarters are even harder to find, with the Standing Liberty, minted from 1916-1930 being a popularly sought item. The Barber, minted from 1892-1916 is still more so.\nHalf-dollars – are becoming increasingly rare. Finding a Franklin Half Dollar produced from 1948-1963, is always a happy target despite its relative youth.\nSilver Dollars – Genuine silver dollars are fairly easy to find, though their silver content makes them priced higher than the denomination.\nMorgan dollars minted from 1878-1921 are common collectibles, but the Peace Dollar produced from 1921-1935 is an excellent addition to the knowledgeable collectors set.\nSpeciality coin value categories, at least for US Collectors\nThis would include foreign coins that are of sufficient age not to be in general circulation.\nWhen determining how to value collectable coins, see our Collecting Rare Coins and Coin Reviews section for examples.\nAlso in those sections are some commemorative editions, as well as several gold coins. Examples of these are the Indian Princess Head Dollar, the Saint Gaudens Twenty Dollar Gold Piece and British Gold Sovereigns.\nTokens and medals categories are pieces that only peripherally qualify as coins though they often serve the purpose, when used on subways and buses, and will sometimes form part of a coin collection.\nWhen coins are rare and note a significant historical event they can be quite valuable. Once a coin has been categorized, the dealer or collector will then grade the coin.\nCoin Value Factors: Miscellaneous Factors in How to Value Collectible Coins\nThere are a host of other factors that determine the value of a collectible coin.\nThere are the obvious ones; mint year (when it was produced), grade (the general condition and degree of wear) and there are several others that coin dealers and collectors take into account when determining the coins value including …\nWhat is the Country of Origin?\nCountry of origin will play a role in determining the worth of a coin. Non-U.S. coins are prized slightly higher by some U.S. collectors since, from the collector’s perspective, the coin is rarer. See the examples and values in our Collecting Rare Coins page.\nWhere was the coin Minted?\nAnother element is the mint mark. There are several mints around the U.S.A and each mint stamps the coin with a letter designating the origin. These include :\n- P – Philadelphia Mint\n- D – Denver Mint\n- S – San Francisco Mint\n- O – New Orleans Mint\n- CC – Carson City Mint\nThe mint mark plays a part in determining the value since different mints produce different amounts of a specific design and each mint came into being in a different year. Whether the mint mark is visible is another element in determining wear and therefore the grade.\nIs the Coin Counterfeit?\nWhether the coin is authentic clearly plays a major role in value. Being counterfeit doesn’t automatically make a coin worthless however, since some, such as the 1913 Liberty Head V nickel are borderline. The Liberty Head V nickel was produced illegally by a legal mint, as it was only authorized up to 1912, but the mint manager produced several anyway.\nA counterfeit coin may also be rare and sought after by collectors for itself. The open market determines the value and sometimes being different or unusual is enough of itself, to bring a high price. In the majority of cases, however, a counterfeit coin is worthless since, by definition, it is a false piece which gas no value.\nHow Much Has the Coin Beeen Cleaned?\nCleaning can considerably alter the value of any coin and most collectors should avoid doing it. Cleaning often produces an unnatural appearance in the coin, which is discounted by collectors and dealers. Coin cleaning carried out without expert skill, can introduce scratches, produce wear and obscure important highlights on the coin.\nIs the Coin Damaged in any way?\nDamage beyond that introduced by cleaning, corrosion, dents, artificial coloring, holes drilled, etc, can often make a valuable coin near worthless. In rare cases the coin may fetch a little money, but usually this is a heavily discounted price. More often damaged coins are viewed as a tiny amount of scrap metal.\nPrice Guides: What Do The Price Guides Say on How to Value Collectible Coins?\nIn past decades price guides were a good beginning for the amateur collector to determine the approximate value of a coin.\nSuch books as The Standard Catalog of World Coins by Chester L. Krause and Clifford Mishler and ‘The Red Book’ (officially titled ‘A Guide Book of United States Coins’) were commonly used by collectors and dealers.\nMore up to date information was often found in magazines such as Coin World and Coin Age, which were useful supplements.\nBut since the rise of the information age and increased use of the Internet and websites devoted to coin collecting, the situation has changed somewhat.\nOnline versions of the Greysheet (used by dealers buying from other dealers), the ‘Blue Book’ (A Handbook of United States Coins) and dozens of other sources are now widespread.\nBooks are still valuable, but in order to keep up to date coin collectors should supplement their knowledge using online resources.\nIn any case, whatever the coin, the ultimate price guide is the collector or dealer, who will say what they are willing to pay for a particular coin.\nSo yes, you can learn how to value collectable coins and you can build up a reasonably accurate estimate of a coins value using the criteria above. But remember, only a ready and willing buyer will truly value collectable coins for you.']	['<urn:uuid:275b82d7-03b3-4af8-a73d-d6b756a01ece>', '<urn:uuid:3bef46a8-9926-49af-9600-9a881cdae85f>']	open-ended	direct	concise-and-natural	distant-from-document	three-doc	novice	2025-05-12T23:00:04.658849	8	117	3174
13	beginner friendly courses compare islamic culture art versus photography classes nyc requirements	Both programs are explicitly designed to welcome beginners. The Islamic art course emphasizes that no prior knowledge of Islam or its culture is required, as fundamental concepts will be explained accessibly. Similarly, PhotoManhattan offers beginner-level courses marked with 'I' designation, teaching photography fundamentals from the ground up. Both programs provide certificates - the Islamic art course offers a Certificate of Attendance from Oxford International Collaboration Centre, while PhotoManhattan provides certificates of completion for all classes.	"['This is a 10 session virtual professional module introducing the artistic culture of Islam from the viewpoint of its aesthetics and meaning. It is conceptualized like a forum for both learning and appreciating the rich and beautiful arts of Islam so that the audience will get a sense of what these arts mean and express for the Muslim faithful. The program aims to cover the most important artistic art forms and visual concepts that make the uniqueness of this culture in both aspects of its unity and diversity. The Muslim world is indeed vast and encompasses an immense variety of local cultures. It is also spiritually shaped by different interpretations of the Islamic faith. Yet, this world remains united through the shared acknowledgment of the truth of God’s words in the Qur’an and of God’s messenger and representative, Prophet Muhammad. This acknowledgment constitutes the metaphysical core of Islam that informs Islamic art making.\nClasses will be based on visual materials displayed on screen. Fundamental spiritual concepts will be explained in an accessible manner. Therefore, there is no need to be familiar with Islam or knowledgeable in its culture, as the course proposes above all stimulating insights.\nThe audience will be able to ask questions as the lecturer will be talking or at the end of the presentations, during a questions and answers session.\nDr. Valerie Gonzalez\nResearch Associate, SOAS, University of London\n- Introducing the world of Islam through both its spirituality and visuality\n- Stimulating appreciation of the Islamic arts and their meaningful beauty\n- Showing the richness of Islamic artistic creation throughout history\n- Giving a sense of the conceptual foundations of Islam as both a faith and a way of life\nTwo sessions per week\nDate and time of first class\nTuesday 18 and Thursday 20, January 2022, 4 pm to 6.30 pm GMT.\nAll classes every Tuesday and Thursday at 4 pm.\nJanuary 18: ‘What is Islamic art?’\nJanuary 20: ‘Islamic Logocentrism as the Metaphysical Foundation of Islamic Artistic Creation’\nJanuary 25: ‘The Qur’an as Aesthetic Paradigm’\nJanuary 27: ‘The Multiple Functions of Calligraphy in Islamic Visuality’\nFebruary 1: ‘The Uniqueness of Islamic Ornament’\nFebruary 1: ‘Examining the Space and Visuality of the Mosque Through a Few Key Buildings’\nFebruary 8: ‘A Masterpiece of Medieval Islamic Architecture: The Alhambra, Granada, Al-Andalus (Present-day Spain)”\nFebruary 10: ‘Islamic Book Culture I : Aesthetics of Persian Painting’\nFebruary 15: ‘Islamic Book Culture II: Aesthetics of Persianate Painting in South Asia in the Medieval period of the Sultanates and in the early Modern Mughal Empire’\nFebruary 17: ‘The Beautiful World of the Islamic Portable Objects: Ceramics, Metalwork, Glasswork, Ivories, Textiles’\nWho should attend?\nAnyone unfamiliar with Islam and interested in discovering it thanks to the appeal of its superb arts, students and professionals alike.\nAny Muslim student or professional who would be enticed by the perspective of revisiting their own culture and learning more about its artistic aspects.\nYou should have an intermediate level of English language proficiency for enrolment in this course.\nMethod of Course Delivery\nThis course will be held online in the English language.\nUpon completion of the programme, you will receive a Certificate of Attendance from the Oxford International Collaboration Centre.\nThe registration fee for Professionals is £175 and for Students is £115.\nRegister immediately online\nClick ‘Discussing Islamic Art, Aesthetics, and Visuality’ to book your place. You will receive a confirmation email from the admin team.', 'Photography Classes in NYC | Online Photo courses, in-person workshops and Remote tutoring\nOur photography classes in Manhattan, New York City, are made fun and affordable while providing expert training in photography since 2003, for all levels — beginners, amateurs and advanced. PhotoManhattan also provides\nonline photography classes via our Virtual classroom, private lessons with automated booking, certificates of completion for all classes, student ID’s with Photography discounts at major photo stores such as Adorma, free classes (Makeup class), long term free photography classes via our Work Study Exchange program, and much more.\nIn this section you\'ll find complete guides and courses from 2-9 weeks. Join in!\nPhoto classes are offered during weekends, evenings, and summers. Conveniently located at 14th street between 5th and 6th Avenues in Manhattan, if you are coming from Brooklyn, Queens, or the Bronx, exit at the 14th St train stop. Please review PhotoManhattan\'s list of photography classes below. Courses titled with ""I"" are beginner photography classes.\nScroll down to see course descriptions. ↓\nCamera Basic, Photo I, and Video\nLearn Photography from the ground up with our introductory photography classes. Get a solid review of the fundamentals of photography, including camera and exposure settings, framing, motion blur, focus control, night photography, Video and much more. Courses are for all cameras, DSLR, Point and shoot, Mirrorless, and Phone cameras. Starts $310.\nCreative Photography - Color, lighting and beyond\nYou know the ""rules"" of photography, now we break them. Experiment and expand your creativity to develop your own style. This course will explore advanced lighting, color filters, pattern, texture, image design, history of photography and more. Starts $325.\nManual Mode Shooting - Maximize your camera\'s potential with Manual Mode\nDid you know? Now iPhones and latest smartphones have Manual Mode! Find out all that can be done with Manual Mode beyond Bokeh background blur. No need to be afraid of the “M” on your camera anymore! With ""M"" mode you are able to unleash the full potential of your camera\'s technical ability and do amazing effects! Starts $310.\nTravel Photography, Street, Journalism, Documentary, Cityscapes\nShoot on the ground in New York City in a course built on intensive hands-on photo field trips. From street photography to documentary, photojournalism and Travel Photography. Sessions will consist of realtime, outdoor\nshooting. Starts $325.\nPortrait, Headshots, Selfies\nWelcome to portraiture. Learn the basics of posing, composition and lighting, as well as creative portrait, and the art of Self portrait. View several course options. Starts $225.\nBlack and white AND Film Photography\nIf you are looking for film photography classes in NYC, or a Black & White photo course, look no further! Learn analog photography, the basics of printing, B&W photography and all of the fundamentals in one place. Starts $325\nWedding Photography, Events, and TTL FLash\nBe guided through the entire process of shooting wedding photography. Topics include wedding and location preparation, working with equipment, shooting techniques and styles, lighting & posing and the business end of the event. Learn to use your Speed-light in TTL or manual mode and enrich your experience. Content covered will also be applicable to event photography. Starts $365.\nStudio & Lighting\nManipulate lighting to achieve precise results. Experiment and understand the techniques for creative imagery in portraiture and still life and product photography. Professional lighting equipment will be provided each class of this tutorial. Starts $365\nThis crash course will provide you with quick and efficient tricks to make the best product photo images using simple equipment, including iPhone-only. Recommended for those of you looking to improve the photos of your a blog, website or presentation, as well as for any size business looking to produce great images of small products such as crafts, apparel and textile, jewelry and flowers, food items, or services. Topics will include DYI home studio setup, lighting and shooting as well as tools to expand your branding and marketing, expedite productivity, and save time and production costs in the short and long term. Starts $225.\nFor an ever-increasingly popular genre of photography, this course will take you through the techniques to create beautiful and delicious food photographs. Learn the do’s and don’ts of what might seem like a simple shot, but is actually a thought-out and meticulous process. Starts $225.\nEditing: Photoshop & Lightroom\nLearn how to fine-tune your pictures through post-processing using Adobe Photoshop and Lightroom. Start your editing journey with topics including brightness and contrast, color balance, levels, curves, clone stamp, filters and more. Starts $365.\nTo know which Photography course is best for you, scroll up and click on the course descriptions.\nScroll down to see upcoming dates. ↓']"	['<urn:uuid:ad7b5a38-504d-4f4c-a5c7-a1c598c6a9d3>', '<urn:uuid:43386416-8ae4-4d49-8741-3085914e7689>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-12T23:00:04.658849	12	75	1343
14	How do Florida and Hawaii handle damaged coral reefs?	Both regions have developed coral restoration programs but take different approaches. In Florida, Nova Southeastern University transplants laboratory-raised staghorn coral colonies to threatened reefs, using both offshore and on-land nurseries. They take small branches, secure them to a substrate with cement, and grow them in 400-gallon tanks before transplanting. In Hawaii, the Department of Land and Natural Resources primarily sources corals from harbors, considering them more resilient and avoiding impact on natural reefs. Their shore-based nursery uses a Fast-Growth Protocol where coral fragments are exposed to optimal conditions and re-aggregated into larger colonies before transplantation.	['In a delicate operation at sea, 28 healthy laboratory-raised staghorn coral colonies were transplanted last month by our Nova Southeastern University Oceanographic Center researchers to a threatened reef off the Fort Lauderdale coast. This will – we expect – initiate a restoration process for a formally healthy reef that was recently devastated by disease.\nStaghorn coral, a fast growing branching variety that look something like deer antlers, were one of the first coral species to be listed as threatened by the Endangered Species Act in 2006. Florida and Caribbean reefs have been ravaged by a number of stresses, including global threats such as rising sea temperatures and ocean acidification as well as more local and human threats, like pollution, over fishing, and development.\nNOAA says staghorn coral populations have declined “up to 98 percent throughout their range” since 1980. Some of our local staghorn reefs have been wiped out nearly completely. It’s bad news for the fish who call the reefs home, but also for the state economy. Florida has 84 percent of the potential coral reef area in the United States, generating more than $6 billion a year and over 71,000 jobs.\nWe are looking for solutions, including raising corals in places where babies and juveniles can be nurtured in a relatively safe place before being moved to a location where we hope to restore the coral population. Offshore nurseries are a great idea, although those can be subject to the same threats – like hurricanes or disease epidemics – as natural reefs.\nA corollary technique is to grow corals under more controlled conditions in on-land nurseries where they are less subject to stress and variation. Once they grow to the right size, the next step is to transplant them. Nurturing corals in a land-based lab and then transplanting them to the ocean floor is a fairly new idea – and one among others that we hope will lead to increasing the tools in the toolbox for in restoring coral reefs.\nThe method sounds easy to do. Corals are famous for their asexual reproduction; a single branch can break off, fall into a crack or cement itself to the bottom, and begin to create a new colony. That’s essentially what we’re doing, although we hope we are doing it efficiently, correctly, and productively. The scientists take a small bit of branch – maybe an inch or two – and secure it to a substrate with a small amount of cement. In 400 gallon tanks, with access to good light and seawater, the corals grow very fast. In our on-land nursery, they grew about an inch per month, about twice as fast as they grow in the wild.\nAfter a year or so of careful tending, the corals are ready to be released into the wild. You have to find the right place to put them on the ocean floor; you need to find the right reef, and you need to identify the substrate that will receive them. It needs to be cleared of algae and have a place where the cement will stick.\nOn February 17, the delicate corals were packed into coolers and ferried offshore. Great care was taken not to abrade or injure them in the process. Diving into a warm, calm Atlantic, the research team carefully transplanted the two-dozen branching coral colonies to a decaying natural reef line about fifteen feet deep, relatively close to shore, just off the coast of Fort Lauderdale beach.\nNSU Oceanographic Center research assistants and graduate students Abby Renegar and Keri O’Neil are keeping close tabs on the transplanted coral to monitor their success rate and survivability. Is the coral still intact? Are there signs of breakage or disease? Which specimens of various genetic stocks are faring better? So far, the outlook is good and the coral appears to be doing well in their new home.\nDr. Dave Gilliam, assistant professor, is in charge of our off-shore nursery partnership with many other researchers. He is conducting similar growth and transplant activities utilizing our offshore nurseries.\nThere is a network of offshore nurseries that have been established up along the Florida coast from the Florida Keys to Ft. Lauderdale. On-land nurseries are still uncommon and can play a significant role. We’re expanding our grow-out facilities. With the addition of our new coral reef research center, we’ll triple the size of our nursery operation. We’re looking forward to having more room to grow coral for restoration purposes. In addition, we will have state of the art facilities for study of coral stressors in controlled fashion in order to better understand the impacts of these threats and how to better take corrective actions.\nThough our transplants have worked well so far, we still have many unanswered questions. For example: What size of coral survives transplants better? When is the best time to conduct the transplantation? What genetic strains of a single species survive best? How do other species compare? How long does it take to restore a reef?\nWe want to figure out how to stop the stresses that are killing off coral in the wild. But, because of bigger global problems that are not easily or quickly fixed, we must also learn how to help restore our reefs and to make them more resilient.\nCoral reefs are the rainforests of the sea. They create vast ecologies which result in healthy reefs and flourishing economies. Reefs are under extreme threat today and have experienced significant degradation. Finding ways to stop and minimize stresses on reefs is paramount. Research to understand how to best help restore reefs is a worthy endeavor and one in which we feel we are making a difference.', 'The state Department of Land and Natural Resources has found an effective way to safeguard Hawai‘i’s native, slow-growing coral reefs — a nursery away from the ocean.\nIn one corner of the Anuenue Fisheries Research Center on O‘ahu’s Sand Island, a modest collection of buildings has become home to the DLNR Division of Aquatic Resources Coral Restoration Nursery.\nPart neonatal intensive care unit and part bank for Hawai‘i’s native corals, the new nursery aims to use professional-level coral husbandry techniques to grow small fragments of a coral colony, recombine them into large colonies, and then transplant them into the field in a fraction of the time it would take these corals to grow naturally.\n“Most coral nurseries around the world are in situ, meaning they are in the field. These types of nurseries excel at raising naturally fast-growing species of corals, which are not components of major reefs in the Main Hawaiian Islands. At our Coral Restoration Nursery we’re focusing on ex situ or a shore-based nursery where we can grow large-size, adult colonies of coral for restoration purposes in a little more than one year.” DAR Coral Biologist David Gulko said.\nThe Hawai‘i nursery primarily uses corals for transplantation from harbors as they have lower ecological value compared to corals from natural areas; may be more resilient to disturbances and environmental changes; do not impact our natural reefs, and helps with upkeep and maintenance of manmade structures.\nThe Coral Restoration Nursery will also provide coral colonies for multiple restoration projects under the country’s first Aquatic Mitigation Bank, which primarily focuses on near-shore coral reef resources.\n“The mitigation bank, is akin to companies gaining carbon credits, in that costs recouped through the selling of coral restoration credits are based on lost ecological services from incidents like boat groundings and spills into the ocean,” DLNR Chair Suzanne Case said.\nNearly a quarter of coral species found in Hawai‘i are unique to the islands and are also among the slowest-growing corals on the planet, according to DAR Administrator Dr. Bruce Anderson.\n“This means it could take well over a decade for corals to get big enough to reproduce. The Coral Restoration Nursery is using techniques that will reduce the time it takes to grow transplantable corals to about one year,” Anderson said. “We are hopeful this will help recover reefs which have been seriously degraded by human impacts like coastal development, vessel groundings, pollution events, along with environmental factors such as climate change.”\nDAR’s Fast-Growth Protocol begins with the removal of a small coral from somewhere like a harbor piling. It is then quarantined in the nursery before being fragmented into a small living pieces. Each of these genetically-identical fragments are then exposed to optimal light, water and nutrient conditions before being re-aggregated together to create at least a 40 cm colony.\nPrior to being transplanted back into the ocean it is put in an acclimation tank, which duplicates the conditions it will experience once it’s transplanted onto the target restoration reef.\n“The technique makes use of non-coral reef source material (harbors, etc.), and provides protection from disease, water quality issues, aquatic invasive species, predation and competition to create re-combined coral colonies in a fraction of the time it would take to grow these corals naturally,” Gulko said.']	['<urn:uuid:5313ff71-6929-4b99-90f7-5f4bc334a01e>', '<urn:uuid:c0e5b4e2-d738-4a0c-88c5-a6e98d32775f>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T23:00:04.658849	9	95	1493
15	royal irish academy book series trials important people details	The Royal Irish Academy has published a series of books under its Prism imprint that puts various Irish historical figures on trial. The series includes books titled Judging Dev, Judging Cosgrave, and Judging Redmond and Carson, followed by an examination of George Bernard Shaw.	['Various Irish notables have recently been put on trial in a sparky series of books published by the Royal Irish Academy under its Prism imprint: so far we have had Judging Dev, Judging Cosgrave, and Judging Redmond and Carson. Now it is George Bernard Shaw’s turn in the dock. He could have no more sympathetic an advocate than Fintan O’Toole, the subtlest brain and the sharpest pen in Irish letters today, who makes his position clear in his dedication: “To my father Samuel O’Toole, Shavian, man and superman.” This father was a man to whom, adds O’Toole elsewhere, “Shaw was a pure delight…a pathfinder who had opened the way to the rough but exhilarating terrain of thinking for yourself.”\nO’Toole wants “to try to restore at least a little of that admiration for what Shaw did and what he got away with.” That last phrase assures us that he will approach Shaw in no great spirit of reverence (though he makes some pretty big claims for him along the way). He makes no pretension to the inclusiveness of Michael Holroyd’s richly detailed and leisurely four-volume biography, but he covers a remarkable amount of ground. This is no mean feat. Shaw’s longevity, coupled with his heroic productivity, demands an epic approach. O’Toole’s purpose, he says, is “to restore [Shaw]…to the twenty-first century.”\nWhen I started reading seriously, Shaw was very much a force to be reckoned with. In 1965, the socialist millionaire publisher Paul Hamlyn brought out two volumes, one containing all the plays and the other all the prefaces, in affordable editions. They sold like hotcakes; the plays themselves were still regularly performed in London’s West End and across the UK. A mere twenty years later, Shaw had all but faded from view, represented on the English stage by a mere handful of his sixty-two published plays: Heartbreak House, the most regularly revived, with very occasional productions of Saint Joan, Major Barbara, and the shockingly trenchant Mrs. Warren’s Profession.\nHe had vanished, too, as an intellectual influence, the books once read by anyone who could read—The Intelligent Woman’s Guide to Socialism and Capitalism and Everybody’s Political What’s What, to say nothing of The Quintessence of Ibsenism and The Perfect Wagnerite, his masterful exegesis of The Ring of the Nibelung—consigned to oblivion. As early as 1973, Martin Seymour-Smith was writing in his Guide to Modern World Literature: “[Shaw] was a superficial thinker and a third-rate writer. His career as a whole is a monument to the failure of human reason alone to solve human problems…he undoubtedly belongs to the history of the theatre; his place in literature, however, is a very minor one.”\nAnd of course he had completely disappeared as a personal presence. His ubiquity during his lifetime was unparalleled: in print, on stage,…\nThis article is available to subscribers only.\nPlease choose from one of the options below to access this article:\nPurchase a print subscription (20 issues per year) and also receive online access to all articles published within the last five years.\nPurchase an Online Edition subscription and receive full access to all articles published by the Review since 1963.\nPurchase a trial Online Edition subscription and receive unlimited access for one week to all the content on nybooks.com.']	['<urn:uuid:a288481e-217d-4ef4-bb1c-52cfb3a8a753>']	open-ended	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T23:00:04.658849	9	44	542
16	How does Nominatim handle the processing of independent places differently from dependent places when computing addresses?	Independent places (like streets, parks, water bodies, suburbs, cities, and states) receive a full address computation where Nominatim searches for all places that cover them at least partially. For places with an area, that area is used to check coverage, while for place nodes, an artificial square area is computed based on the rank. In contrast, dependent places (rank 30 places like house numbers and POIs) skip the full address computation for performance reasons and instead just find a parent place to attach themselves to, typically the closest street.	"[""In Nominatim, the word indexing refers to the process that takes the raw OpenStreetMap data from the place table, enriches it with address information and creates the search indexes. This section explains the basic data flow.\nAfter osm2pgsql has loaded the raw OSM data into the place table, the data is copied to the final search tables placex and location_property_osmline. While they are copied, some basic properties are added:\n- country_code, geometry_sector and partition\n- initial search and address rank\nIn addition the column\nindexed_status is set to\n1 marking the place as one\nthat needs to be indexed.\nAll this happens in the triggers\nThe main work horse of the data import is the indexing step, where Nominatim takes every place from the placex and location_property_osmline tables where the indexed_status != 0 and computes the search terms and the address parts of the place.\nThe indexing happens in three major steps:\nData preparation - The indexer gets the data for the place to be indexed from the database.\nSearch name processing - The prepared data is given to the tokenizer which computes the search terms from the names and potentially other information.\nAddress processing - The indexer then hands the prepared data and the tokenizer information back to the database via an\nINSERTstatement which also sets the indexed_status to\n0. This triggers the update triggers\nosmline_updatewhich do the work of computing address parts and filling all the search tables.\nWhen computing the address terms of a place, Nominatim relies on the processed search names of all the address parts. That is why places are processed in rank order, from smallest rank to largest. To ensure correct handling of linked place nodes, administrative boundaries are processed before all other places.\nApart from these restrictions, each place can be indexed independently from the others. This allows a large degree of parallelization during the indexing. It also means that the indexing process can be interrupted at any time and will simply pick up where it left of when restarted.\nThe data preparation step computes and retrieves all data for a place that might be needed for the next step of processing the search name. That includes\n- location information (country code)\n- place classification (class, type, ranks)\n- names (including names of linked places)\n- address information (\nData preparation is implemented in pl/PgSQL mostly in the functions\naddr:* tag inheritance\nNominatim has limited support for inheriting address tags from a building to POIs inside the building. This only works when the address tags are on the building outline. Any rank 30 object inside such a building or on its outline inherits all address tags when it does not have any address tags of its own.\nThe inheritance is computed in the data preparation step.\nSearch name processing\nThe prepared place information is handed to the tokenizer next. This is a Python module responsible for processing the names from both name and address terms and building up the word index from them. The process is explained in more detail in the Tokenizer chapter.\nFinally, the preprocessed place information and the results of the search name processing are written back to the database. At this point the update trigger of the placex/location_property_osmline tables take over and fill all the dependent tables. This makes up the most work-intensive part of the indexing.\nNominatim distinguishes between dependent and independent places. Dependent places are all places on rank 30: house numbers, POIs etc. These places don't have a full address of their own. Instead they are attached to a parent street or place and use the information of the parent for searching and displaying information. Everything else are independent places: streets, parks, water bodies, suburbs, cities, states etc. They receive a full address on their own.\nThe address processing for both types of places is very different.\nTo compute the address of an independent place Nominatim searches for all\nplaces that cover the place to compute the address for at least partially.\nFor places with an area, that area is used to check for coverage. For place\nnodes an artificial square area is computed according to the rank of\nthe place. The lower the rank the lager the area. The\ntables are there to facilitate the lookup. All places that can function as\nthe address of another place are saved in those tables.\nisin:* tags are taken into account to compute the address, too.\nNominatim will give preference to places with the same name as in these tags\nwhen looking for places in the vicinity. If there are no matching place names\nat all, then the tags are at least added to the search index. That means that\nthe names will not be shown in the result as the 'address' of the place, but\nsearching by them still works.\nIndependent places are always added to the global search index\nDependent places skip the full address computation for performance reasons. Instead they just find a parent place to attach themselves to.\nBy default a POI or house number will be attached to the closest street. That can be any major or minor street indexed by Nominatim. In the default configuration that means that it can attach itself to a footway but only when it has a name.\nWhen the dependent place has an\naddr:street tag, then Nominatim will first\ntry to find a street with the same name before falling back to the closest\nThere are also addresses in OSM, where the housenumber does not belong\nto a street at all. These have an\naddr:place tag. For these places, Nominatim\ntries to find a place with the given name in the indexed places with an\naddress rank between 16 and 25. If none is found, then the dependent place\nis attached to the closest place in that category and the addr:place name is\nadded as unlisted place, which indicates to Nominatim that it needs to add\nit to the address output, no matter what. This special case is necessary to\ncover addresses that don't really refer to an existing object.\nWhen an address has both the\naddr:place tag, then Nominatim\nassumes that the\naddr:place tag in fact should be the city part of the address\nand give the POI the usual street number address.\nDependent places are only added to the global search index\nthey have either a name themselves or when they have address tags that are not\ncovered by the places that make up their address. The latter ensures that\naddresses are always searchable by those address tags.""]"	['<urn:uuid:4ee83bb9-badd-4966-b004-b1903a140e49>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T23:00:04.658849	16	89	1093
17	Can I use my microwave to dry my garden herbs?	Yes, you can use a microwave to dry herbs, but it requires patience. First air dry the herbs for a few days, then dry them in the microwave in short 1-2 minute increments. Alternatively, you can spread the leaves in a single layer on a paper towel, microwave on low power level for 3 minutes, check them, and then microwave for another 3 minutes until they become shriveled and crispy.	"[""Share on ThriftyFunThis guide contains the following solutions. Have something to add? Please share your solution!\nMost herbs dry easily, and if properly preserved, will retain their flavor and aroma long enough to carry you into the next growing season. As with a lot of gardening, timing is everything. Here are some tips for gathering, drying and keeping them fresh while in storage.\nEach herb is different, but here are a few general rules for drying and storing herbs:\nAs a general rule, you can start harvesting when the plant has enough foliage to sustain growth. Harvest in the early part of the day and harvest no more than 2/3 of the plant at one time. Perennial herbs should not be harvested later than one month prior to the first frost to ensure they have time to store resources for winter.\nThe best way to dry herbs is to hang them and let them air dry. Clean herbs under running water if necessary. Shake off excess water and remove any dead or damaged foliage. Tie stems into small bundles and hang upside down to dry in a warm, dry place out of direct sunlight. To catch seeds or petals, cover the flower end of the stems with a paper bag (with air holes). Other methods of drying include laying herbs across screens or drying them on the lowest settings of a food dehydrator or conventional oven or if you have the time and patience, drying them for short 1 to 2 minutes increments in a microwave (after a few days of air drying). When dry, leaves will be brittle and crumble easily. Store them in airtight glass or plastic containers (or bags) in a dark, dry location like a pantry or cupboard.\nThis is a guide about using a microwave for drying herbs. There are a number of ways you can make your own dried herbs.\nAsk a QuestionHere are the questions asked by community members. Read on to see the answers provided by the ThriftyFun community or ask a new question.\nI grew some wonderful lemon basil this summer. How do I dry it so I can enjoy it in the winter time? Many thanks for this great site. I have learned so much!\nHardiness Zone: 5a\nBy Paula from Niagara Falls, Ontario\nPaula, I dry a variety of herbs and Lemon Basil is one of them. I pick the leaves early in the day after all dew or moisture is gone. I place a paper towel in the microwave on the turntable. Next, I spread out the leaves into one layer and try to not put one on top of the other. My microwave has different power levels and I use level #3. Start with 3 minutes then check. They should begin to look wilted. Microwave again for another 3 minutes on level 3. By this time, mine is usually done. It will be shriveled and crispy. I then let cool and then store in clean pint jars with a tight fitting lid. Moisture is the enemy of dried herbs and can cause mold to form so be sure to keep airtight. You might have to experiment with different time amounts and different temperatures. But this is what I do and it works well for me. Good Luck, Banty\nI have grown sweet basil, purple basil and the very flavorful Thai basil, all of which are really yummy. I have a dehydrator, and have tried them that way, on the temp recommended, which is about 105, as i recall, same as my Greek oregano, but if you want it to taste really good, and have an idea how much you will generally use at a time, take the clean, dry, basil, put the fresh leaves, as many as you expect to use, in a fold of alum. Foil. Make as many folds of alum. Foil as you need, but work fast, and get them in a zip type bag, lay them flat in the freezer, and as you pick, just keep making small packets of leaves, put them in the bag, and you will have really fresh tasting basil when you pull it out. You have to get it out and cut or tear it quickly, and put it in the food, or use it as a garnish, but the scent, and flavor will still be fresh, and no dulling of color like there is when it is dried.\nAll you really need to do is set them out and let em dry naturally the best type of drying out herbs and em upside down on a piece of string on something up high like a lamp or something else that's able to be high enough so the water can drip out\nHardiness Zone: 8a\nBecky from Charlotte, NC\nIt depends on what you're growing. I'm not sure if you're talking about harvesting the leaves, stems and seeds, or bringing the entire plant in for the winter. Some herbs, like basil for example, tend to taste bitter if harvested after the plant sets seeds. The same is true for most herbs that are harvested for their foliage. Once they bolt (set seed) their flavor is altered. As a general rule, herbs that will be used for their foliage should be harvested before they bolt. Herbs grown for seed should be harvested as their seedpods change from green to brown, but before the seeds drop or shatter. Herbs that will be harvested for their flowers should be harvested just before the flowers are fully open, and herbs that will be harvested for their roots can be harvested in the fall as soon as the foliage dies back. If your herbs still have foliage, try harvesting a few and drying them or adding them to dishes to test their flavor. If they taste fine to you, there is no harm in harvesting them this late.\nIf they have gone to seed, there might be a bitter taste.\nLikely. If drying for storage the best time to harvest is when leaves are large and green, not dry. Wrap a medium bundle in large twist tie and hang upside\ndown in shade for about 10 days, then strip best\nleaves and seed off stems inside large white plastic new garbage bag. After tossing all stems, pick out leaves and place in airtight containers for storage, sealing with scotch tape, marking date on top and labeling.\nThe residue is usually the seeds in the bottom of the sack, mixed with remnant leaf/stem particles. If possible, separate them by sifting in a flour sifter or finer sifter.\nStore seeds in old envelopes, seal, lable and\ndate, writing any special instructions, descriptions you might not remember.\nIf all you got is seed and tiny dried up leaves, just use them anyway, being extra careful to remove all stems, and learn for the next year's use.\nStore seed in a cool,dry place, not plastic bag. Beware of bugs attracted and able to eat into the paper.\nMy experience is mostly with Tarragon, Basil,\nGarlic, Oregano, Lemon Balm, Curry, and the more common herbs over the years. I keep only what will grow with the least care. I've had the most luck here in N. Texas with these I've listed.\nGood luck and God's blessings to you.\nI harvest my herbs all through the season for drying. I also cut off the flowering stem to promote more leaf growth, however I do leave some of the flowers so the butterflies and bees have something to feed on.""]"	['<urn:uuid:4e341f00-f249-4444-a9b6-1567702fcd50>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T23:00:04.658849	10	70	1260
18	Is pacemaker implant surgery risky and what happens to its battery waste?	Pacemaker implantation carries some risks: pneumothorax occurs in 1 in 100 patients, infection in 2 in 100 patients, and wire displacement in 2 in 100 patients, though death is extremely rare. Regarding battery disposal, their lithium-ion batteries require specialized recycling processes due to their hazardous nature. They need careful handling during collection and storage, with companies like Li-Cycle developing technologies to safely recover 80-100% of battery materials.	['What is a pacemaker?\nA pacemaker is an implanted device that monitors your heartbeat and prevents your heart from beating too slowly. It consists of a box (‘pulse generator’) attached to one or two wires (‘leads’). The pulse generator is usually placed underneath the skin or muscle beneath your collarbone. The lead or leads pass inside the large veins of your chest from the generator to your heart. The leads transmit information about your heartbeat to the generator. If your heart beats too slowly the pacemaker sends out a tiny electrical impulse to ‘pace’ the heart, i.e. to make it beat at an appropriate speed. If your heart then speeds up again, this is detected by the pacemaker, which then stops pacing your heart.\nWhat does pacemaker implantation involve?\nA pacemaker is usually implanted under local anaesthetic; you can also have some sedation to make you feel sleepy and more relaxed. The procedure generally takes about an hour and is performed in a ‘cardiac catheterisation laboratory’. Your heart rhythm, blood pressure and the oxygen level in your blood are monitored continuously. The doctor puts local anaesthetic under the skin below your collar bone. The anaesthetic stings for a few seconds as it gets to work but the area rapidly goes numb. An incision about 3-4 cms long is made below the collarbone. The next stage of the procedure is for the doctor to find a vein through which the leads are passed to the heart. Once the leads are in the veins they can be positioned in the heart. This is not in any way uncomfortable although you may feel a few extra heart beats for a short period. Care is taken to find the ideal position for the lead or leads and you will hear numbers being called out, referring to measurements made through the leads. When the doctor is happy that the leads are in the best position, they are stitched in and a pocket is created to house the pulse generator. This pocket may be underneath the skin or deeper, under the muscle of the chest wall. Finally the incision is closed up with stitches. For the skin stitch some doctors use a dissolvable suture while others prefer a suture that is taken out at your doctor’s surgery after 7 days. You will be told which you have.\nCan complications occur?\nThis procedure cannot be performed without some element of risk; you need to understand the risks before you sign the consent form. It is important that you understand that these risks are potential and unpredictable. The majority of patients, however, have no problems. The potential risks are listed below for your information:\nPneumothorax (air leak around the lungs): this occurs in approximately 1 in 100 patients. The veins used to get the pacemaker wires to your heart lie very near to the lung. It is possible to nick the lung on the way past causing an air leak. Sometimes this resolves on its own. Occasionally a chest drain is needed; if so your stay in hospital may be delayed by several days.\nPericardial effusion (blood leak around the heart): this occurs in approximately 1 in 500 patients. When the wire is positioned in the heart it can cause a fluid collection around the outside of the heart. Sometimes this resolves on its own but occasionally a drain is needed; very rarely an operation is needed. If so your stay in hospital may be delayed by several days.\nInfection: this occurs in approximately 2 in 100 patients. Antibiotics are given routinely to help prevent this. In the unlikely event that the pacemaker does become infected, it will probably need to be removed and a new pacemaker implanted.\nHaematoma (severe bruising): this occurs in approximately 4 in 100 patients. Rarely this bruising may need drainage. Patients taking blood thinning drugs are more prone to haematoma formation.\nWire displacement: this occurs in approximately 2 in 100 patients. This causes the pacemaker to pace inappropriately. In most cases the wires need to be repositioned requiring further surgery.\nDeath: this is fortunately an extremely rare complication\nWhat to expect after pacemaker implantationYou can expect your shoulder and arm to be sore for a few days afterwards. Simple pain-killers like paracetamol are usually helpful. It is important to keep the arm and shoulder moving, to avoid a ‘frozen shoulder’; however you should be gentle and not use the arm for heavy lifting for a few weeks, until your first pacemaker check. Try not to lift your arm above shoulder height again until the first check. You should avoid getting the wound wet for about a week after the implant. The wound usually appears a little bruised for several days afterwards. Although the bruising may become more prominent after you go home, any discomfort and redness around the wound should get better rapidly. If you have concerns about the wound it is important that you contact us directly.\nFollow-up after pacemaker implantationAfter discharge from hospital you will need to be followed up regularly. The first visit is usually about six weeks after implant and usually every 9-12 months thereafter although there is some variation. At these visits your pacemaker is checked to make sure that it is working well and to make any adjustments necessary to the way it is programmed. This visit usually lasts about twenty minutes.\nWhat happens when the battery runs down?Many factors determine how long a pacemaker battery will last. Currently available generators typically last about 7-10 years under standard conditions. The pacemaker battery behaves in a predictable fashion. By following you up regularly we can determine accurately when the pacemaker generator will need replacing. The pacemaker will be functioning perfectly normally throughout this time and you will be readmitted in a planned fashion to replace the pacemaker generator. When the generator is replaced the procedure is similar to that of the original pacemaker implant although we aim to use the original leads and simply attach a new generator.\nIs there anything you should avoid with a pacemaker?In almost every respect life with a pacemaker is completely normal. Household appliances and mobile telephones do not interfere with your pacemaker’s function, although you should not keep your mobile telephone in a pocket directly over the pacemaker. You will receive a pacemaker card giving full information about your pacemaker. If you are flying you should hand this card to security to show you have a pacemaker and you will not have to go through the security gate. If you are concerned about interference from a particular piece of equipment at home or at work, ask your consultant or the cardiac physiologists in the pacemaker clinic.\nDriving after a pacemakerYou should not drive for one week after pacemaker implantation if you hold an ordinary (group 1) licence or six weeks if you hold a vocational (group 2) licence. The DVLA rules are revised periodically and the current rules can be viewed at http://www.dvla.gov.uk/drivers/dmed1.htm.\n- Pacemaker (PM)\n- Implantable Cardioverter Defibrillator (ICD)\n- Cardiac Resynchronisation Therapy (CRT)\n- AF Ablation\n- Atrial Flutter Ablation\n- WPW Ablation\n- AVNRT Ablation\n- AT Ablation\n- VT Ablation\n- AV Node Ablation', 'In 30 years since commercialisation, lithium-ion (li-ion) batteries have been used in an increasingly diverse range of products, starting from early generation handheld electronics to powering cars and buses. Additionally, these batteries are increasingly sought after for utilisation in energy storage applications, often paired with renewable energy generation. The continued decline in battery prices combined with the global trend toward energy grids being powered by renewable energy sources is predicted to increase the world’s cumulative energy storage capacity to 2,857GWh by 2040 , a substantial increase from the current capacity of ~545MWh , according to recent estimates by Bloomberg New Energy Finance.\nThese staggering projections paint an encouraging picture for how prominent li-ion-driven energy storage applications will become in the future as the world increases usage of renewable, clean energy sources to power energy grids worldwide. Driven increasingly by electro-mobility as well as grid-scale energy storage applications, the volume of li-ion battery cells being sold is set to surge. The graph in Figure 2 contextualises the relative volume (in tonnes) of new li-ion battery cells forecasted to be sold through to 2025. The growing quantities of li-ion batteries being placed on the markets accelerates the urgency with which the world must find an economically viable, commercial-scale recycling solution for end-of-lifecycle li-ion batteries to be recycled at a ‘mega’ scale. This article will take a closer look at some of the challenges that exist today within the li-ion recycling sector and where opportunities exist to overcome the current roadblocks.\nLi-ion recycling industry challenges\nSecondary resource recovery (i.e. recycling) has a set of unique operational challenges that need to be addressed concurrent to the development of an economic, advanced technology. For the purpose of recycling, feed materials are typically inherently distributed, making it difficult to collect a high volume of feed for a processing plant. Although the collection supply chains for some analogous industries such as lead-acid battery recycling are well-established and mature by comparison, the li-ion battery recycling supply chain continues to be fluid. Spent li-ion battery sources can be broadly segmented into portable/’small format’ and ‘large format’ sources, which corresponds to the relative voltage of li-ion batteries (i.e. low voltage and intermediate to high voltage, respectively). Each of these types of batteries has a diverse group of stakeholders – from manufacturers, to the dealer network, recycling programmes, electronics and vehicle recyclers. In the context of the energy storage sector, its own diverse group of stakeholders exists – battery technology provider, energy storage integrator, project developer and asset owner. Managing the inherently heterogenous nature of li-ion batteries from a wide range of stakeholders remains a central challenge for companies in the li-ion resource recovery industry.\nLogistics and regulations\nLi-ion batteries are currently classified as Class 9 Dangerous Goods due their dual chemical and electrical hazard. Li-ion batteries can possibly undergo thermal runaway, typically resulting from internal shorting, leading to fire or explosion. There are numerous factors that can cause thermal runaway, including but not limited to overcharging, environmental conditions (e.g. extreme external temperatures) and manufacturing defects. At the onset of thermal runaway, the battery heats in seconds from room temperature to above 700°C. As part of this complex set of chemical reactions, the electrolyte solvent in lithium-ion batteries – typically alkyl carbonate-based – acts as a ‘fuel’ source for combustion.\nAdded care must also be taken when handling critical or damaged/defective batteries as there is an increased risk of thermal runaway. Specialised systems (e.g. Genius Technology’s LionGuard container for intermediate to high voltage lithium-ion batteries ) are typically used in tandem with non-flammable packing material to safely transport these batteries. As the overall volume of li-ion batteries increases, the quantity of critical or damaged/defective batteries is expected to increase across a broad swath of applications. As the li-ion battery resource recovery industry is still maturing, regulations vary significantly around the world. These regulations can also change significantly from year to year, as new industry and research reports are released. As a result, it is important to keep close track of regulatory (including logistics) considerations concurrent to process development.\nSafety and storage\nThe challenges of logistics and changing regulations typically revolve around one key factor – safety. Safety is paramount for those who handle, transport, store and process li-ion batteries, as there is a risk of thermal runaway. This raises another unique challenge for processors and consolidators, relative to the primary production of commodities and specialties. Specifically, the safest approach is to have the lowest amount of spent li-ion batteries on site as possible, in order to mitigate the risk of a thermal runaway event occurring. However, this is contradictory to the requirement to secure significant amounts of feed for processing purposes. The development of safe storage is further complicated by the currently prominent format factor of spent li-ion batteries, i.e. portable/small format batteries (e.g. from mobile phones, laptops and other consumer products). Portable li-ion batteries are typically consolidated in drums and could be mixed with other battery types. Upon an initial inspection, the state of all collected batteries within a single drum is not always clear (i.e. whether undamaged or damaged) and often only becomes apparent when the drums are tipped for sorting or processing. As a result, strict protocols must be implemented regarding the pallet/container spacing, total storage density and application of appropriate fire suppression systems within any li-ion battery storage space in order to mitigate the risk associated with thermal runaway and fire.\nSecondary resource processing challenge\nFrom a process development standpoint, the recovery of constituents from li-ion batteries presents a unique challenge compared to traditional primary metal resources due to the highly heterogeneous nature of the feed material. Currently, there are at least 14 different types of li-ion battery cathode chemistries currently existing in the market , each of which has even further permutations when considering specific constituents. With traditional metal resources the primary concentrate stream might have 1-4 elements to be recovered (e.g. copper, gold, silver and platinum). Li-ion batteries may however contain over 20 elements that demand consideration for recycling as illustrated by the example composition in Figure 3 . In addition, the metal values are typically contaminated with inorganic materials, organic materials and plastics, further complicating the recycling process. To be able to separate out the valuable constituents typically requires complex process flowsheets with many individual unit operations. Under this scenario, it is critical that the physical test-work required to develop the process flowsheet is well focused and driven by techno-economic analysis.\nLi-Cycle is one company with a strong focus on technology for resource recovery of end-of-lifecycle li-ion batteries. Since incorporation in 2016, Li-Cycle has developed and validated a unique process to recover 80-100% of all li-ion battery constituent materials using a two-step mechanical and hydrometallurgical system. This advanced resource recovery process, alongside concentrated efforts focused on battery sourcing from various supply chain players and a continuous prioritisation of safety, are fundamental elements supporting Li-Cycle’s goal of global commercialisation of Li-Cycle Technology.\nOpportunities and future outlook\nIt is evident that the global volume of li-ion batteries deployed in energy storage and other applications is set to increase steadily over the next two decades, underscoring the necessity for a sustainable end-of-life pathway for these batteries both now and into the future. Li-Cycle is on a mission to leverage its innovative solution to address an emerging and urgent global challenge. Li-ion batteries are increasingly powering our world and there is a need for improved technology and supply chain innovations to better recycle these batteries, and to meet the rapidly growing demand for critical and scarce battery-grade materials. Scalability, low-cost, safety and environmental sustainability are core tenets of commercialising Li-Cycle Technology. In turn, Li-Cycle seeks to enable the global transition to electro-mobility and reduce greenhouse gas emissions worldwide. Lithium-ion batteries will continue to electrify our world, now and into the foreseeable future. As a key driver of the transition away from a carbon-based economy, li-ion batteries are integral to the opportunity to drastically reduce greenhouse gas emissions worldwide. However, to ensure a truly positive impact over their lifecycle, we must ensure a closed-loop system is in place to safely handle and recycle spent li-ion batteries at scale. This will enable the reintegration of critical battery materials into the li-ion battery supply chain and the broader economy, while preventing negative environmental and safety impacts.\nThis article first appeared in PV Tech Power, Vol20, which is available now to download for free, here.\n- Munuera, Luis (2019) Energy Storage – Tracking Clean Energy Progress https://www.iea.org/tcep/energyintegration/ energystorage/\n- Bloomberg New Energy Finance (2018) Energy Storage is a $620 Billion Investment Opportunity to 2040 https://about.bnef.com/ blog/energy-storage-620-billion-investment-opportunity-2040/\n- Nitta N, Wu F, Lee JT, Yushin G (2015) Li-ion battery materials: present and future https://www.sciencedirect.com/science/article/pii/ S1369702114004118. Accessed 10 September 2018 • Lithium Collect (2018) https://ecobatgroup.com/occ-liion/en/ ss.php. Accessed 10 September 2018.\n- Diekmann J, Hanisch C, Frobose L, Schalicke G, Loellhoeffel T, Folster AS, Kwade A (2017) Ecological recycling of lithium-ion batteries from electric vehicles with focus on mechanical processes Journal of The Electrochemical Society 168 (1): A6184-A6191\nStay up to date with the latest news, analysis and opinions. Sign up here to the Energy-Storage.news Newsletter.']	['<urn:uuid:01083671-1769-49f9-b5fc-305124ee8ce8>', '<urn:uuid:9c9e2902-e591-4e64-b4f3-05dfa51a0e53>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T23:00:04.658849	12	67	2718
19	I have trouble with plant diseases in my garden - what's the difference between using nitrogen fertilizer versus coffee grounds for preventing plant problems?	These have opposite effects on plant diseases. Excess nitrogen makes plants more susceptible to insects and diseases. In contrast, coffee grounds have been shown to suppress common fungal rots and wilts (Fusarium, Pythium, and Sclerotinia) as well as some bacterial pathogens (E. coli and Staphylococcus). However, coffee grounds should be used carefully - they should make up no more than 20% of compost or be applied in thin layers covered by other mulch.	['- Does lime lower pH in soil?\n- Do coffee grounds make soil more acidic?\n- What happens if plants have too much nitrogen?\n- Why is my soil pH so low?\n- What should be added in the soil when it is too acidic?\n- Does nitrogen raise or lower pH?\n- Does nitrogen affect water pH?\n- What happens when soil pH is too low?\n- What causes a high pH in soil?\n- What is the fastest way to lower pH in soil?\n- What do you do if your soil has high pH?\n- What causes low pH in soil?\n- Is low pH bad for plants?\n- How do you lower pH in potting soil?\n- Does urea increase soil pH?\nDoes lime lower pH in soil?\nWhat is lime.\nLime is a soil amendment made from ground limestone rock, which naturally contains calcium carbonate and magnesium carbonate.\nWhen lime is added to soil, these compounds work to increase the soil’s pH, making soil less acidic and more alkaline..\nDo coffee grounds make soil more acidic?\nUsed coffee grounds are neutral.” If you rinse your used coffee grounds, they will have a near neutral pH of 6.5 and will not affect the acid levels of the soil. To use coffee grounds as fertilizer, work the coffee grounds into the soil around your plants.\nWhat happens if plants have too much nitrogen?\nWhen plants receive too much nitrogen (N), they become more attractive to insects and diseases. It can also cause excessive growth and reduce the strength of the stems.\nWhy is my soil pH so low?\nCOMMON CAUSES FOR LOW SOIL pH In soils, intensive fertilization with ammonium-based fertilizers or ammonium-forming fertilizers (urea) may lower soil pH. … Parent material – type of rocks from which the soil developed. Rainfall – soils under high rainfall conditions are more acid than soils formed under dry conditions.\nWhat should be added in the soil when it is too acidic?\nThe most common way to raise the pH of soil is to add pulverized limestone to the soil. Limestone acts as a soil acid neutralizer and consists of either calcium and magnesium carbonate or calcium carbonate. These are called dolomitic limestone and calcitic limestone respectively.\nDoes nitrogen raise or lower pH?\n– Of all the major fertilizer nutrients, nitrogen is the main nutrient affecting soil pH, and soils can become more acidic or more alkaline depending on the type of nitrogen fertilizer used. … Phosphoric acid is the most acidifying phosphorus fertilizer. – Potassium fertilizers have little or no effect on soil pH.\nDoes nitrogen affect water pH?\nNitrate nitrogen works differently by causing the release a negatively charged OH- or HCO3– anion when it is taken up by the plant root. These negatively charged anions are bases, and when they react with the growing medium, they cause the growing medium pH to increase.\nWhat happens when soil pH is too low?\nA pH level that is too low also liberates aluminum—not a plant nutrient—in amounts that can stunt root growth and interfere with a plant’s uptake of nutrients. At a high pH level, the plant nutrient molybdenum becomes available in toxic amounts.\nWhat causes a high pH in soil?\nSoils may be alkaline due to over-liming acidic soils. … Iron chlorosis in plants, caused by inadequate iron, is a common problem in alkaline soils. Phosphate, a macronutrient, may also be limited in these high pH soils due to its precipitation in the soil solution.\nWhat is the fastest way to lower pH in soil?\nTwo materials commonly used for lowering the soil pH are aluminum sulfate and sulfur. These can be found at a garden supply center. Aluminum sulfate will change the soil pH instantly because the aluminum produces the acidity as soon as it dissolves in the soil.\nWhat do you do if your soil has high pH?\nTreatment of High pH Soil Fertilizers and chelates can be added to soil to increase concentrations of plant nutrients. It is important to note that addition of phosphate fertilizer alone will further reduce the availability of other nutrients. Lowering the pH of alkaline soils, or acidifying the soil, is an option.\nWhat causes low pH in soil?\nSoil pH is affected by land use and management. … These changes are caused by a loss of organic matter, removal of soil minerals when crops are harvested, erosion of the surface layer, and effects of nitrogen and sulfur fertilizers. Addition of nitrogen and sulfur fertilizers can lower soil pH over time.\nIs low pH bad for plants?\nLow pH levels also create mineral levels in the soil that can be toxic, thus harming plants. Low pH can increase the solubility of minerals such as aluminum. Plant growth can be repressed in soils that have severely high levels of aluminum or manganese .\nHow do you lower pH in potting soil?\nSphagnum Peat Moss Adding peat moss to your garden soil can also help to lower the pH of your soil gradually. Peat moss is an excellent soil amendment for acid-loving plants and is easy to incorporate into the soil. Simply add two to three inches to the top of the soil and work it into the layers of topsoil underneath.\nDoes urea increase soil pH?\nWhen anhydrous ammonia (NH3) is applied to the soil, it reacts with water to form ammonium-N and the hydroxide ion, which is basic. This reaction initially raises the pH of the soil. … In soil solution, urea first reacts with water and free H+ ions to form ammonium-N and bicarbonate.', 'Using Coffee Grounds in the Garden - October 31, 2018\nJeff Schalau, Agent, Agriculture & Natural Resources\nUniversity of Arizona Cooperative Extension, Yavapai County\nDr. Linda Chalker-Scott, Extension Urban Horticulturist and Associate Professor at Washington State University, has taken it upon herself to explore anecdotal gardening “miracle products”. Along the way, she has provided excellent product reviews and dispelled various gardening myths. Chalker-Scott has also written an excellent book called The Informed Gardener. In an article from 2009, she wrote: Coffee Grounds – Will They Perk Up Plants? Below I will share some of her observations about the use of coffee grounds in home gardens and landscapes.\nIncreasing numbers of people are using spent coffee grounds as mulch and these people are claiming they repel cats, kill slugs, prevent weeds, aerate and acidify the soil, provide nitrogen, attract earthworms, and more. There is a body of research about the uses for the byproducts of coffee processing (husks, hulls, and waste water), but little about using actual coffee grounds in gardens and landscapes. Coffee grounds are often available in large quantities from coffee venders and many people use them as mulch (applied to the soil surface), for a direct soil amendment, and add them to their compost.\nCoffee beans are seeds that contain nitrogen-rich proteins needed for germination and growth. Protein comprises over 10% of coffee grounds. In fact, the carbon-to-nitrogen ratio of coffee grounds can be ideal ratio for plant and soil nutrition (as low as 11:1). Since coffee is extracted in water, the compounds that are not water soluble (oils, lipids, triglycerides, and fatty acids) remain in the grounds along with cellulose and indigestible sugars. Lignin, phenolics, and essential oils are also left over from the brewing process and these compounds are reported to have antioxidant and antimicrobial properties.\nSoil-borne bacteria and fungi break down the various chemical components of coffee grounds after several months. Earthworms are also able to use this food source. Earthworms consume coffee grounds and deposit them deep in soil. This may account for noted improvements in soil structure such as increased aggregation. Humic substances, which are important chemical and structural soil components, are ultimately produced through organic matter degradation – this includes degradation of coffee grounds.\nMany gardeners assume that coffee grounds are acidic, but this does not hold true experimentally. The pH of decomposing coffee grounds in these experiments ranged from 4.6 (mildly acidic) to 8.4 (somewhat alkaline). The pH also changes over time and you should not assume that it will always be acidic. As for soil-borne diseases, coffee grounds do appear to suppress some common fungal rots and wilts (Fusarium, Pythium, and Sclerotinia) as well as some bacterial pathogens (E. coli and Staphylococcus). Coffee ground composts and mulches enhanced germination of some seeds while inhibiting germination of others.\nDr. Chalker-Scott has synthesized coffee ground research results to make the following recommendations. In compost, limit coffee ground content to no more than 20% of the total compost volume – more than 30% has often been detrimental. Additions of diverse raw materials to compost should ensure a diversity of microorganisms. Don’t assume coffee grounds will make an acidic compost; pH levels will fluctuate over time.\nIn mulch, since coffee grounds are finely textured and easily compacted, they can create a barrier to moisture and air movement, especially when applied in thick layers. Dr. Chalker-Scott recommends against using pure coffee grounds as mulch. Instead, try using a thin layer (no more than half an inch) of coffee grounds and cover with a thicker (four inches) layer of coarse organic mulch like wood chips.\nThis is not the first time I have referenced Dr. Chalker-Scott’s work in the Backyard Gardener. Visit the Backyard Gardener website and access the on-line version of this column for a link to Dr. Chalker-Scott’s website and a direct link her article: Coffee Grounds – Will They Perk Up Plants? I have also linked other coffee ground articles below.\nFollow the Backyard Gardener on Twitter – use the link on the BYG website. If you have other gardening questions, call the Master Gardener help line in the Camp Verde office at 928-554-8992 or e-mail us at firstname.lastname@example.org and be sure to include your name, address and phone number. Find past Backyard Gardener columns or provide feedback at the Backyard Gardener web site: http://cals.arizona.edu/yavapai/anr/hort/byg/.\nCoffee grounds in worm compost (Penn State Extension).\nCoffee Grounds and Composting\nOregon State University Extension Service\nCoffee as Fertilizer?\nUniversity of Illinois Extension\nRecycling Coffee Grounds in the Garden\nPenn State Cooperative Extension\n| Arizona Cooperative Extension\n840 Rodeo Dr. #C\nPrescott, AZ 86305\nLast Updated: October 23, 2018\nContent Questions/Comments: email@example.com<firstname.lastname@example.org']	['<urn:uuid:752ee150-4188-4788-a0c8-008b6c338d16>', '<urn:uuid:81f5f996-de28-444d-b295-b9906b3f3e1e>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T23:00:04.658849	24	73	1710
20	compare biology physics fundamental study goals	While physics seeks to answer 'Why?' by describing all things and their interactions with the ultimate goal of creating a 'theory of everything', biology aims to study life at all scales from cellular mechanisms to ecosystems, with no mention of seeking a unified theory.	"['Physics seeks to answer the question ""Why?"" - to describe all the things we see, as well as their motion, behaviour and interactions. The science covers the entire history and future of the universe, and all objects from the fundamental matter we’re made from to the structure of galaxies.\nPhysicists look for patterns by making observations of the natural world, and models are developed to describe and predict them. Instruments, observations and experiments are then used to test and refine these models. Physicists sometimes aim to describe new things, and other times to improve the accuracy of an existing model. Based on the approach taken, each branch of physics can be divided into two aspects: theoretical and experimental. A key goal is often to combine several models into one - the ultimate goal of physicists in this respect is to complete ""the theory of everything"", though some believe this to be impossible.\nWhile Theoretical physicists use mathematics to first describe the observed behaviour and then to explain it, experimental physicists perform experiments to test their hypotheses. Scientists believe that no hypothesis can ever be proven, so experiments are instead designed to disprove one - and ""failing"" to disprove it strengthens our confidence in it. Once a hypothesis is supported by several different tests and becomes accepted, it gets called a ""theory"" - but experimental physicists will continue to try to find problems with it, so that the theorists can further improve their models.\nOur latest physics articles\nOur latest physics blog posts\nOur blog posts focus on a specialist topic.\nMany are written by scientists about their ongoing research, others by the TWDK team.\nDelve deeper into Physics\nCan\'t find what you\'re looking for? Browse the branches of physics that interest you most.\nAstronomy is one of the oldest subjects of study. It seeks to understand everything outside of our world and explain how it got there.\nAtomic Physics is all about the interactions between matter (the stuff everything is made from) and more matter, or between matter and light.\nBiophysics uses the principles of physics to study life at every level, from atoms and molecules, to cells, organisms and environments.\nChemical Physics is the study of chemical processes in terms of the atoms and particles that make up a system.\nClassical Physics deals with the physics of everyday things - from ice cubes to bicycles, or cups of tea. Things we take for granted, but often don\'t really understand.\nElectromagnetism is the study of one of the four fundamental forces in nature - the electromagnetic force, which mediates interactions between charged particles.\nGeophysics refers to the study of the physics of the Earth and its environment in space, including its gravitational and magnetic fields, and its atmosphere.\nMaterials Science deals with the structure and properties of materials.\nMedical Physics ranges from enhancing diagnostic imagery to developing new techniques and equipment with which to treat patients.\nParticle Physics is the study of the smallest things in the universe and the forces that govern their behaviour.\nOr try our advanced search.', 'TNPSC BIOLOGY (ENGLISH)\nWHAT IS STUDY OF LIFE?\nBiology is a branch of science and it is divided into various branches like Ecology, Embroyology, physiology, External morphology, anatomy, Histology, Evolution, Genetics, etc.,\nBiology literally means “the study of life”. It is such a broad field, covering the minute workings of chemical machines inside our cells, to broad scale concepts of ecosystem and global climate change. Biologists study intimate details of the human brain, the composition of our genes and even the functioning of our reproductive system.\nCharles Darwin, studied adaptations and gathered a great deal of proof he later incorporated into his theory of evolution. Darwin began to catalogue his collections and ponder the seeming “fit” of organisms to their mode of existence. He eventually settled on four main points of a radical new hypothesis.\n✦ Over Reproduction\n✦ Natural Selection\nBiology is a study of plants and animals. Biology can be divided into two main branches\n✦ Botany- study of plants\n✦ Zoology- study of animals\nSome of the sub branches are\n✦ Bacteriology- study of bacteria\n✦ Virology- study of virus\n✦ Phycology- study of algae\n✦ Mycology- study of fungi\n✦ Ethology- Study of animal behavior\n✦ Ecology- Study of Environment\n✦ Genetics- A science that deals with heredity\nSome of the Important terms coined by,\n✦ Biology- Lamarck and Treriranus\n✦ Cell- Robert Hooke\n✦ Protoplasm- Purkinjee\n✦ Genetics- W.Bateson\n✦ Vaccine- Edward Jenner\nConsidered as father of\n✦ Aristotle- Father of biology and zoology\n✦ Theoprastus- Father of botany\n✦ Gregor Johann Mandel- Father of Genetics\n✦ Carl Linnaeus- Father of Modern Taxonomy\n✦ Living organisms show a great degree of diversity in their size.\n✦ They are measured in microns and milli microns. Example: Virus, bacteria, algae, fungi and protozoan.\n✦ Virus is a Latin word which means poison. Viruses are the smallest and simplest of all living organisms.\n✦ The study of viruses is called Virology.\n✦ The curd contains Lacto Bacillus bacteria which helps to change the milk into curd.\n✦ Bacteria are unicellular and microscopic, belonging to the world microbes. The study of bacteria is called Bacteriology.\n✦ Bacteria are measured in microns. 1 micron = 1/1000 millimeter.\n✦ Four types of bacteria are recognized based on shape. They are\n○ Cocci (Spherical shaped)\n○ Bacilli (Rod shaped)\n○ Spirillum (Spiral or cork screw)\n○ Vibrio (Comma Shaped)\n✦ On the basis of the number and arrangement of the flagella, bacteria are classified as\n○Monotrichous (Single flagellum at one end)\n○ Amphitrichous (Tuft of flagella arising at both ends)\n○ Atrichous (without any flagella)\n○ Lophotrichous (Tuft of flagella at one ends) -\n○ Peritrichous (Flagella all around)\n✦ Algae are a unicellular green algae. It is spherical or oval in shape.\n✦ The cell wall may have a pectic sheath around it. There is a single large cup-shaped Chlamydomonas chloroplast\n✦ There are narrow end of the cells which helps in locomotion.\n✦ The study of algae is called phycology.\n✦ The conversion of sugar solution into alcohol and liberation of carbon dioxide is known as fermentation.\n✦ Wine, alcohols are prepared from the molasses by the fermentation activity of the yeast. etc.\n✦ Fungi do not ‘possess chlorophyll. Hence they are incapable of photosynthesis.\n✦ The study of fungi is called Mycology.\n✦ Protozoans are unicellular organisms.\n✦ Protozoan’s show mainly two modes of life, free living and parasitic.\n✦ Free living organisms inhabit fresh and salt water. Parasitic forms lives as ectoparasites or endoparasites. They cause diseases.']"	['<urn:uuid:86aacc8c-fca0-401c-be1e-7c44374d497b>', '<urn:uuid:bf7a137f-d1e2-4c85-8418-eb69be131eb5>']	factoid	direct	short-search-query	distant-from-document	comparison	expert	2025-05-12T23:00:04.658849	6	44	1099
21	As a volcanic researcher, what can Mars calderas tell us about Earth's?	The Arsia Mons caldera on Mars provides valuable comparative data for understanding caldera formation and evolution. It spans 110 km across and contains 29 volcanic vents, with lava flows covering 6700 km² and volumes between 67-540 km³. Activity occurred between 300 and 10 million years ago. On Earth, the Caetano caldera in Nevada offers a different perspective, formed by a supervolcano eruption of over 1100 cubic kilometers of tuff about 33.8 million years ago. The Caetano caldera was smaller, measuring 20 km long by 12-18 km wide and up to 1 km deep, but unique faulting and tilting have exposed features from surface to 5 km depth, providing exceptional three-dimensional views of caldera formation processes.	"['The Tharsis Volcanic Province covers roughly a quarter of the surface of the planet Mars, with 12 major volcanic edifices and over a thousand smaller vents. The southernmost of the major volcanic structures is Arsia Mons, a shield volcano over 300 km which has a caldera 110 km across. Within this caldera, a total of 29 small volcanic vents have been observed, each with its own lava flow extending over part of the caldera floor. No impact craters larger than 1 km across have been observed inside the caldera, leading to the conclusion that it is relatively young; crater dating techniques (which estimate the age of surfaces on other Solar System bodies based upon the number of observable craters upon them, from the assumption that the random distribution of impacts proceeds at a constant rate), have suggested an age of no more than 130 million years.\nIn a paper presented at the 48th Lunar and Planetary Science Conference at The Woodlands, Texas, in March 2017, and accepted for publication in the journal Earth and Planetary Science Letters, Jacob Richardson of the Planetary Geology, Geophysics, and Geochemistry Lab at NASA’s Goddard Spaceflight Center, James Wilson of the School of Geosciences at the University of South Florida and DigitalGlobe, Charles Connor, also of the School of Geosciences at the University of South Florida, and Jacob Bleacher, also of the Planetary Geology, Geophysics, and Geochemistry Lab at NASA’s Goddard Spaceflight Center describe the results of a study of the nature and age of the volcanic vents and lava flows within the Arsia Mons Caldera.\nRichardson et al. used Context Imager data to construct an ArcGIS baseamap with a 6 m/pixel resolution. Using this they were able to determine boundaries between the lava flows, enabling them to judge the extent of the lava flows derived from each vent; lava flows which could not be associated with a vent (probably because they have been overlaid by later flows) were excluded from the study. As all the images were taken from more-or-less directly above, it was impossible to determine the exact thickness of each lava flow; to model the approximate volume of the lava, a maximum thickness of 80 m and a minimum thickness of 10 m were used. Ages of the lava flows were estimated using crater density methods.\nThe Arsia Mons Caldera features 29 cataloged volcanic vents (black circles). Each vent effused lava flows 10s of km downhill. Flows are shaded by strati-graphic height, with uppermost flows lightly shaded and lower flows shaded dark red. Richardson et al. (2017).\nThe measured lava flows cover approximately 6700 km², roughly 70% of the caldera floor. They are estimated to have volumes of between 0.04 and 69 km³, with a total lava volume of 67-540 km³. Based upon the date estimates obtained, Richardson et al. estimate that the crater had a sharp increase in activity between 300 and 150 million years ago, and that activity declined to a halt between 90 and 10 million years ago, with an average of one major flow-producing event every 2.5 million years during this period, or an average of 1 to 8 km³ of lava being produced each million years.\nFollow Sciency Thoughts on Facebook.', 'Highlights are provided below. Representatives of the media may obtain complimentary copies of articles by contacting Ann Cairns at email@example.com. Please discuss articles of interest with the authors before publishing stories on their work, and please make reference to GEOSPHERE in articles published. Contact Ann Cairns for additional information or other assistance.\nNon-media requests for articles may be directed to GSA Sales and Service, .\nLarge-magnitude Miocene extension of the Eocene Caetano caldera, Shoshone and Toiyabe Ranges, Nevada\nJoseph P. Colgan et al., U.S. Geological Survey, Menlo Park, California 94025, USA.\nKeywords: Basin and Range Province, Miocene, extension tectonics, calderas.\nThe late Eocene Caetano caldera formed about 34 million years ago during eruption of the Caetano Tuff (described in a companion paper by John and others in this issue of Geosphere). Remnants of the caldera are presently exposed in a series of north-trending, east-tilted (about 40 degrees), fault-bounded blocks that crop out across 40 km (east-west) of the Shoshone and Toiyabe Ranges in north-central Nevada. Restored geologic cross-sections indicate that the caldera was originally about 12-18 km (north-south) by 20 km (east-west) and has therefore been stretched to about twice its original width. The authors interpret Miocene sedimentary rocks exposed between these fault blocks to represent material shed from rising mountain ranges into adjacent basins while the faults were moving, indicating that deformation began about 16 million years ago and continued until 10-12 million years ago. These older basins and ranges were broken up by younger, locally active faults that formed the modern basins and ranges seen in north-central Nevada today. It is likely that Miocene faulting and tilting was not confined to the former caldera, but also affected surrounding Paleozoic rocks-and, potentially, large, nearby Carlin-type gold deposits.\nHistory of Quaternary volcanism and lava dams in western Grand Canyon based on lidar analysis, 40Ar/39Ar dating, and field studies: Implications for flow stratigraphy, timing of volcanic events, and lava dams\nRyan Crow et al., Department of Earth and Planetary Sciences, University of New Mexico, Albuquerque, New Mexico 87131, USA.\nKeywords: Grand Canyon region, Uinkaret, basalt flows, lava dams, volcanic history.\nJohn Wesley Powell wrote in 1895: “...what a conflict of water and fire there must have been [in western Grand Canyon]! Just imagine a river of molten rock running down over a river of melted snow.” Over 110 years later, a synthesis of new and existing dates on these lava flows shows that many are significantly younger than initially thought and all are less than 725 thousand years old. The geochronology data indicates four major episodes when lava flows either erupted into the canyon or flowed over the rim into it: 725-475 thousand years ago (ka), 400-275 ka, 225-150 ka, and 150-75 ka. These flows formed lava dams in western Grand Canyon that had dramatic impact on the Colorado River. This paper presents light detection and ranging (lidar) data to establish the elevations of the tops and bottoms of basalt flow remnants along the river corridor. These data show the original extent of now-dissected intra-canyon flows and aid in correlation of flow remnants. From 725 to 475 ka, volcanism built a high edifice within Grand Canyon in the area of the Toroweap fault, with dike-cored cinder cones on both rims and within the canyon itself. These large-volume eruptions helped drive the far-traveled basalt flows which flowed down-canyon over 120 km. A second episode of volcanism, from 400 to 275 ka, built a 215-m-high dam along the Hurricane fault, about 15 km downstream. The ca. 200 and 100 ka flows (previously mapped as Gray Ledge) were smaller flows and lava cascades that entered the canyon from the north rim between the Toroweap and Hurricane faults. The combined results suggest a new model for the spatial and temporal distribution of volcanism in Grand Canyon in which composite lava dams and edifices were generally leaky in proximal areas. Available data suggest that the demise of volcanic edifices may have involved either large outburst-flood events or normal fluvial deposition at times when the river was established on top of basalt flows. These data highlight complex interactions of volcanism and fluvial processes in this classic locality.\nReelfoot rift and its impact on Quaternary deformation in the central Mississippi River valley\nRyan Csontos et al., University of Memphis, Ground Water Institute 300 Engineering, Memphis, TN 38125, USA.\nKeywords: Reelfoot rift, Mississippi embayment, New Madrid seismic zone, Mississippi River alluvium, geomorphology.\nThis article presents research into the Reelfoot rift within the Mississippi embayment of the central United States. The project is timely and relevant to a wide audience in light of the seismic risk and increased interest in petroleum potential within the Mississippi embayment. The northern end of the rift near the town of New Madrid, Missouri, was the site of the great 1811-1812 New Madrid earthquakes, and it remains the most seismically active area east of the Rocky Mountains. This research utilizes existing and new data sets which better define the structure and stratigraphy within the Reelfoot rift and presents them as a 3-D model for interpretation and visualization. This allows the generation of a unique picture and new understanding of the central Mississippi River Valley. Quaternary reactivation of Precambrian basement faults is of wide international interest in developing a better understanding of rift systems and their earthquake threat. Within the Mississippi embayment it is important to understand the source of the current and historic seismicity. Delineation of basement structures within the Reelfoot rift is also of interest to the petroleum industry, because the authors have mapped basement blocks that may help define petroleum targets. This work substantially improves the interpretation of the geologic history, formation, and development of the Reelfoot rift and its impact on Quaternary deformation.\nAutomated extraction of data from text using an XML parser: An earth science example using fossil descriptions\nGordon B. Curry et al., Digital Geosciences Laboratory, Dept of Geographical and Earth Sciences, University of Glasgow, Gregory Building, Lilybank Gardens, Glasgow G12 8QQ, Scotland, UK.\nThis paper describes a method of automating the computerization (digitization) of important sections of earth science information that is currently only available in printed text. At the present time, full digitizing of printed information requires manual entry of data into a database, which is slow, unrewarding, and likely to introduce mistakes. This paper demonstrates a method of automatically digitizing descriptions of fossil species, which are an immense source of information on the history of life on the planet. These descriptions are written in a very regular way, to such an extent that they can be read by computers using new software (a parser) that creates markers, or tags, around segments of text. Once tagged in this way, the information can be analyzed much more thoroughly that was previously possible. The digitized information is also much more complete than was previously available, as the entire species description is tagged, including all the features of the fossil, its stratigraphic distribution, and geographic location.\nLidar mapping of faults in Houston, Texas, USA\nRichard M. Engelkemeir and Shuhab D. Khan, Department of Geosciences, University of Houston, Houston, Texas 77204-5007, USA.\nKeywords: lidar, Houston, faults, subsidence\nThis paper uses LIDAR for mapping active surface faults in Houston, Texas. These faults result in damages to houses, pipelines, roads and other constructions. Accurate mapping of their locations therefore aids in hazard mitigation.\nAsh-flow tuffs and paleovalleys in northeastern Nevada: Implications for Eocene paleogeography and extension in the Sevier hinterland, northern Great Basin\nChristopher D. Henry, Nevada Bureau of Mines and Geology, University of Nevada, Reno, Nevada 89557.\nKeywords: paleogeography, extension, Eocene, ash-flow tuff, Nevada\nThe distribution of distinctive volcanic and sedimentary rocks indicates that northeastern Nevada 40 million years ago was a high plateau, possibly 4 kilometers (13,000 feet) high, incised with deep valleys. Rivers in the valleys on opposite sites of a “paleo-continental divide” that ran approximately north-south slightly west of Elko, Nevada, drained either westward to the Pacific Ocean or eastward to large basins in Utah. The high plateau probably resulted from thickening of the crust following a long period of folding that ended about 60 million years ago. The plateau was probably similar to parts of the modern Andes Mountains of South America or Tibet.\nMagmatic and tectonic evolution of the Caetano Caldera, north-central Nevada: A tilted, mid-Tertiary eruptive center and source of the Caetano Tuff\nDavid A. John et al., U.S. Geological Survey, 345 Middlefield Road, Menlo Park, California 94025, USA.\nKeywords: calderas, ash-flow tuff, magma resurgence, Basin and Range Province, extensional tectonics\nThe Caetano caldera in north-central Nevada formed during a supervolcano eruption of greater than 1100 cubic kilometers (greater than 270 cubic miles) of Caetano Tuff about 33.8 million years ago. The caldera formed by when the roof of the magma chamber collapsed as the Caetano Tuff erupted from the chamber. Collapse left an ovoid depression, a caldera, about 20 kilometers long by 12 to 18 kilometers wide and up to 1 kilometer deep. Due to younger faulting and tilting (see companion paper by Colgan and others in the same issue of Geosphere), the caldera was broken into several blocks that expose caldera features from the former surface to a depth of more than 5 kilometers. This extraordinary three-dimensional view allows a far more detailed analysis of caldera formation and evolution than is available for almost any other caldera in the world. Reconstruction of the Caetano caldera also constrains nearby Carlin-type gold deposits, presently the largest producing gold deposits in the United States, to have formed at depths of less than or equal to 1 kilometer.\nOutcrop fracture characterization using terrestrial laser scanners: Deep-water Jackfork sandstone at Big Rock Quarry, Arkansas\nMariana I. Olariu et al.; John F. Ferguson, corresponding author, Geosciences Department, University of Texas at Dallas, P.O. Box 830688, Mail Station FO21, Richardson, Texas 75083-0688, USA.\nKeywords: cluster analysis, fractures, turbidite, outcrop, lidar, laser scanner, laser.\nA new way of doing geology is being invented that is sometimes called ""cyber geology."" This paper is a pioneering example of this new approach to geologic mapping. Laser scanners and GPS are used to produce high-resolution (approaching one sample per square centimeter) maps of the outcrop of the Jackfork sandstone in the Big Rock Quarry near Little Rock, Arkansas. A three-dimensional virtual model of the outcrop is produced on a computer from this very large (millions of samples) data set. A processing scheme has been implemented that automatically identifies distinct surfaces and their orientations from the three-dimensional point cloud of sample points. These surfaces can be interpreted in terms of fractures and bedding planes in the Jackfork sandstone. The large number of virtual strikes and dip measurements permit a statistical characterization of the fracture orientations as well as an analysis of the special variability of the fractures in different locations within the quarry.\nA prominent geophysical feature along the northern Nevada rift and its geologic implications, north-central Nevada\nDavid A. Ponce and J.M.G. Glen, MS989, U.S. Geological Survey, 345 Middlefield Rd, Menlo Park, CA 94025, USA.\nKeywords: gravity and magnetic anomalies, northern Nevada rift, epithermal gold deposits, Battle Mountain-Eureka mineral trend, Basin and Range, Nevada.\nThe origin and character of a prominent large-scale geophysical feature in north-central Nevada is considered. This crustal-scale fault is coincident with the western margin of the northern Nevada rift—a mid-Miocene rift that includes mafic dike swarms and associated volcanic rocks and is partly coincident with the central part of the Battle Mountain-Eureka mineral trend. Geophysical evidence suggests that the northern Nevada rift partly followed this pre-existing feature in north-central Nevada. If the crustal fault along the northern Nevada rift and a previously inferred crustal fault along the central part of the Battle Mountain-Eureka mineral trend are in fact the same feature, deposits at a greater distance from the crustal feature associated with the northern Nevada rift at Battle Mountain could be explained by post-emplacement tectonic events. In any case, these large-scale crustal features are important to understanding the metallogeny, tectonics, magmatism, and water resources of the Great Basin.\nIdentification of quartz and carbonate minerals across northern Nevada using ASTER thermal infrared emissivity data—Implications for geologic mapping and mineral resource investigations in well-studied and frontier areas\nBarnaby W. Rockwell and Albert H. Hofstra, U.S. Geological Survey, Box 25046, MS 973, Denver Federal Center, Denver, Colorado 80225, USA.\nKeywords: remote sensing, ASTER, Thermal infrared, quartz, carbonate\nOutcrops and detritus composed of quartz and carbonate (calcite and dolomite) were identified and mapped across a 400 km by 400 km area of northern Nevada, USA, using thermal infrared data collected by the ASTER sensor aboard the EOS Terra satellite platform. This paper shows that such data can be used to generate accurate and cost-effective maps of these minerals at regional to local scales. Well-mapped rock types include: thick sequences of quartz sandstone (or quartzite) and conglomerate, bedded radiolarian chert, rhyolite, and diatomite as well as thick sequences of dolomite, limestone, and marble. Alluvial fan surfaces, sand dunes, and beach deposits composed of quartz and/or carbonate are prominent map features. Also detected were small hot spring silica sinter and travertine deposits in geothermal areas and quartz deposited from ancient hydrothermal systems that formed large deposits of gold and silver.\nGondwanan/peri-Gondwanan origin for the Uchee terrane, Alabama and Georgia: Carolina zone or Suwannee terrane(?) and its suture with Grenvillian basement of the Pine Mountain window\nMark G. Steltenpohl et al., Department of Geology and Geography, Auburn University, Auburn, Alabama 36849, USA.\nKeywords: Uchee terrane, Carolina zone, Gondwana, peri-Gondwana, southern Appalachians\nIn 1964, an exploratory petroleum well was drilled through one mile of Gulf Coastal Plain sediments in southeastern Alabama. The retrieved core included Silurian to Devonian sedimentary rocks that surprisingly contain fossils provincial to proto-Africa (i.e., Gondwanan), rather than proto-North America (Laurentian), setting off a scientific debate about how these rocks got there. At the same time, discoveries made in studies of the ocean floor were being formulated into the sea-floor spreading mechanism that would explain why and how continents drift around the surface of Earth, the missing link that had escaped Alfred Wegner’s conceptualization of continental drift. The Suwannee terrane, as the Alabama rocks became known, figured prominently in J. Tuzo Wilson’s 1966 article in Nature (“Did the Atlantic close and reopen?”), which led Kevin Burke to later coin the term “Wilson Cycle.” This orphaned block of Gondwanan crust is now known to extend in the subsurface beneath southern Alabama, Georgia, and South Carolina. A collage of deformed and metamorphosed fragments of ancient volcanic island arcs, known collectively as Carolinia (i.e., Carolina Superterrane), form the most eastern exposures of the Appalachian orogen and also project to depth beneath the Atlantic coastal plain. These arc terranes formed in an ancient ocean peripheral to western Gondwanaland prior to its climactic Appalachian collision with Laurentia that consolidated the supercontinent Pangaea. Due to the lack of fossils and reliable isotopic dates within these terranes, the nature and timing of their docking with Laurentia are controversial; hence, one of the most significant events in Appalachian history is also one of the least understood. Steltenpohl and others report new isotopic and structural information on the poorly known Uchee terrane in Alabama that bears on the problem. The Uchee terrane occupies a particularly critical tectonic position, being sandwiched between Laurentian continental basement exposed in the Pine Mountain window and Gondwanan crust of the overlying, albeit buried, Suwannee terrane. Steltenpohl and coauthors confirm that the Uchee is an “exotic” peri-Gondwanan arc terrane -- not part of Laurentia as was previously believed -- and thus provide a new puzzle piece that helps to constrain models for plate tectonic development of the Pangaean suture.\nLate Cenozoic paleogeographic evolution of northeastern Nevada: Evidence from the sedimentary basins\nAlan R. Wallace, et al., U.S. Geological Survey, MS 176, Mackay School of Earth Sciences and Engineering, University of Nevada, Reno, Reno, Nevada 89557, USA.\nKeywords: sedimentary basins, tectonics, geomorphology, Nevada, Miocene, Pliocene, gold, Humboldt River.\nNew geologic and dating studies along a 200-km-long transect across northeastern Nevada has led a better understanding of how the landscape has changed over the past 20 million years (m.y.). The studies focused on four sedimentary basins along the transect (Chimney, Ivanhoe, Carlin, and Elko, from west to east) because they record events, such as the formation of mountains and erosion by streams, that produced the landscape changes over time. The landscape was a subdued, eroding upland until about 16 m.y. ago, when volcanic eruptions in the western half of the transect and faulting in the eastern half dammed streams and formed the sedimentary basins. These basins gradually filled up and partially buried adjacent highlands. The dams for the western three basins failed after about 2 m.y., and the streams integrated into the beginnings of the Humboldt River drainage system. The Elko basin remained isolated until about 10 m.y., at which point it began to drain to the west into the Humboldt system. The river system flowed westward into northwestern Nevada, where active faulting was forming deepening valleys. As a result, the basin sediments in the upstream areas have been eroding and carried to the new downstream basins. This erosion has re-exposed the pre-basin highlands, so some modern ranges are fairly old rather than young as previously thought. Similarly, major mineral deposits in the upstream areas gradually have been exposed, weathered, or eroded, whereas those in downstream areas have been covered up. This concept is very important for the evaluation of and exploration for mineral deposits in the region.']"	['<urn:uuid:1ae186df-b5b2-4133-943b-a1444e928563>', '<urn:uuid:f501d403-2943-405d-b986-f522dae3a3c6>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T23:00:04.658849	12	115	3437
22	when calculate focus adjustment dof guides versus scheimpflug technique	The calculation methods for focus adjustment differ significantly between depth of field guides and the Scheimpflug principle. Depth of field guides use a step-by-step process involving setting the sensor size, lens focal length, and subject distance to determine the required f-stop for desired depth of field. In contrast, the Scheimpflug principle requires calculating the angle of the plane of focus based on geometric relationships - when the lens is tilted by an angle θ relative to the image plane, the angle ψ between the image plane and plane of focus must be calculated using specific mathematical formulas involving the focal length and focus distances. With the Scheimpflug principle, adjusting focus rotates the plane of focus rather than displacing it along the lens axis, while with depth of field guides, the focus planes remain parallel to the image plane.	"[""By Ron Eggers\nOne difference between a serious photographer and a casual photographer is the level of expertise honed from years of shooting experience. A variety of tools are available to help you sharpen photographic skills and insights. The depth-of-field guide, which has been around for some 30 years, has been revised recently with the introduction of ExpoAperture2 Depth-of-Field Guides from ExpoImaging.\nFor too many photographers, depth of field is a relatively vague concept of what's in focus and what isn't. In fact, you can determine depth of field very precisely, mathematically. Many fixed-focus lenses and some zoom lenses have depth-of-field guides marked on their barrels. Some cameras also have depth-of-field preview capabilities. These work well enough while shooting. But they aren't much help in planning a shoot.\nExpoDisc inventor George Wallace (not the governor, nor the comedian) devised a depth-of-field calculator in 1978, making it possible for photography students and other photographers interested in expanding their photographic abilities, to know exactly what would be in focus and what wouldn't.\nThat was a time in photography when there was a minor resurgence of the objectives of the Group f64, and the style of photographers like Ansel Adams and Edward Weston, where everything in a frame was as sharp as a tack. To ensure those results, some photographers turned to depth-of-field guides.\nDOF guides were also employed to control depth of field creatively. With it, a photographer could determine what would be sharp and what would be softer in a composition and set the exposure accordingly.\nThe two ExpoAperture2 Depth-of-Field Guides are refined versions of that early DOF guide. Both are dual sided. The original disk covers lens focal lengths from 15mm to 135mm. The second covers 70mm to 600mm. With either disk, a good starting point is to find the aperture for the desired depth of field.\nThe first step is to set the sensor size on the lower side of the outer dial. Both disks include preset format sizes range from 1.7X (with 35 mm being 1X) to 6x6 cm. A reference card is included that lists many of the popular DSLR bodies and their sensor factor, which is the same as the lens conversion factor when shooting with that body and 35mm lenses. A sensor factor of 1X indicates that the sensor size is the same as a 35mm frame. The card, and the ExpoAperture2 guides, include half-frame 35mm, APS, 35mm, 120, 220 and 620.\nUsing the first disk, the 35mm setting is used here as an example. It works the same with either 35mm film or a full-frame sensor. After setting the format size on the outer ring, set the focal length of the lens being used in the yellow window on the inside of the dial. Since a 50mm lens is the standard lens of a 35mm camera, that focal length serves as a good example.\nThen flip the disk over to the Distance Dial and find the distance that the primary subject is going to be away from the lens. For this example, try 7.5 feet. The next step is to figure out how much depth of field is required to come up with the desired composition. There might be numerous elements in a frame, but only a few of those elements should be sharply in focus.\nThat might be from 6 to 10 feet. Then count the number of alternating gray and white segments inside the outer distance dial. That would be two zones. Flip the disk over again. This time, on the gray inner dial, find the number of zones. The aperture that's on the blue dial above zone number, in this case f9, is the f/stop required to achieve the desired depth of field.\nConversely, it's also possible to find the resulting depth of field from a specific aperture. The first two steps of setting the sensor size and lens focal length are the same. Then find the f/stop that's going to be used on the outer blue ring. Find the number of zones beneath the aperture, on the gray dial. For f/9, that would be two. Flip the disk over and find the distance to the primary subject. Using 7.5 feet again, two zones indicate everything from 6 to 10 feet would be in sharp focus.\nBe careful not to move the disks as you turn the guide from one side to the other to read the settings. It's particularly easy to accidentally shift the format adjustment without noticing it.\nThe ExpoAperture2 Depth-of-Field Guides are useful tools for any photographer interested in working more effectively or more creatively with depth of field. They are particularly useful as learning tools for photography students."", 'This article includes a list of references, related reading or external links, but its sources remain unclear because it lacks inline citations. (June 2013) (Learn how and when to remove this template message)\nThe Scheimpflug principle is a geometric rule that describes the orientation of the plane of focus of an optical system (such as a camera) when the lens plane is not parallel to the image plane. It is commonly applied to the use of camera movements on a view camera. It is also the principle used in corneal pachymetry, the mapping of corneal topography, done prior to refractive eye surgery such as LASIK, and used for early detection of keratoconus. The principle is named after Austrian army Captain Theodor Scheimpflug, who used it in devising a systematic method and apparatus for correcting perspective distortion in aerial photographs.\nDescription of the Scheimpflug principleEdit\nNormally, the lens and image (film or sensor) planes of a camera are parallel, and the plane of focus (PoF) is parallel to the lens and image planes. If a planar subject (such as the side of a building) is also parallel to the image plane, it can coincide with the PoF, and the entire subject can be rendered sharply. If the subject plane is not parallel to the image plane, it will be in focus only along a line where it intersects the PoF, as illustrated in Figure 1.\nBut when a lens is tilted with respect to the image plane, an oblique tangent extended from the image plane and another extended from the lens plane meet at a line through which the PoF also passes, as illustrated in Figure 2. With this condition, a planar subject that is not parallel to the image plane can be completely in focus.\nScheimpflug (1904) referenced this concept in his British patent; Carpentier (1901) also described the concept in an earlier British patent for a perspective-correcting photographic enlarger.The concept can be inferred from a theorem in projective geometry of Gérard Desargues; the principle also readily derives from simple geometric considerations and application of the Gaussian thin-lens formula, as shown in the section Proof of the Scheimpflug principle.\nChanging the plane of focusEdit\nWhen the lens and image planes are not parallel, adjusting focus[a] rotates the PoF rather than displacing it along the lens axis. The axis of rotation is the intersection of the lens’s front focal plane and a plane through the center of the lens parallel to the image plane, as shown in Figure 3. As the image plane is moved from IP1 to IP2, the PoF rotates about the axis G from position PoF1 to position PoF2; the ""Scheimpflug line"" moves from position S1 to position S2. The axis of rotation has been given many different names: ""counter axis"" (Scheimpflug 1904), “hinge line” (Merklinger 1996), and ""pivot point"" (Wheeler).\nRefer to Figure 4; if a lens with focal length f is tilted by an angle θ relative to the image plane, the distance J[b] from the center of the lens to the axis G is given by\nIf v′ is the distance along the line of sight from the image plane to the center of the lens, the angle ψ between the image plane and the PoF is given by[c]\nEquivalently, on the object side of the lens, if u′ is the distance along the line of sight from the center of the lens to the PoF, the angle ψ is given by\nThe angle ψ increases with focus distance; when the focus is at infinity, the PoF is perpendicular to the image plane for any nonzero value of tilt. The distances u′ and v′ along the line of sight are not the object and image distances u and v used in the thin-lens formula\nwhere the distances are perpendicular to the lens plane. Distances u and v are related to the line-of-sight distances by u = u′ cos θ and v = v′ cos θ.\nFor an essentially planar subject, such as a roadway extending for miles from the camera on flat terrain, the tilt can be set to place the axis G in the subject plane, and the focus then adjusted to rotate the PoF so that it coincides with the subject plane. The entire subject can be in focus, even if it is not parallel to the image plane.\nThe plane of focus also can be rotated so that it does not coincide with the subject plane, and so that only a small part of the subject is in focus. This technique sometimes is referred to as “anti-Scheimpflug”, though it actually relies on the Scheimpflug principle.\nRotation of the plane of focus can be accomplished by rotating either the lens plane or the image plane. Rotating the lens (as by adjusting the front standard on a view camera) does not alter linear perspective[d] in a planar subject such as the face of a building, but requires a lens with a large image circle to avoid vignetting. Rotating the image plane (as by adjusting the back or rear standard on a view camera) alters perspective (e.g., the sides of a building converge), but works with a lens that has a smaller image circle. Rotation of the lens or back about a horizontal axis is commonly called tilt, and rotation about a vertical axis is commonly called swing.\nTilt and swing are available on most view cameras, often on both the front and rear standards, and on some small- and medium format cameras with special lenses that partially emulate view-camera movements. Such lenses are often called tilt-shift or ""perspective control"" lenses.[e] For some camera models there are adapters that enable movements with some of the manufacturer’s regular lenses.\nDepth of fieldEdit\nWhen the lens and image planes are parallel, the depth of field (DoF) extends between parallel planes on either side of the plane of focus. When the Scheimpflug principle is employed, the DoF becomes wedge shaped (Merklinger 1996, 32; Tillmanns 1997, 71),[f] with the apex of the wedge at the PoF rotation axis,[g] as shown in Figure 5. The DoF is zero at the apex, remains shallow at the edge of the lens’s field of view, and increases with distance from the camera. The shallow DoF near the camera requires the PoF to be positioned carefully if near objects are to be rendered sharply.\nOn a plane parallel to the image plane, the DoF is equally distributed above and below the PoF; in Figure 5, the distances yn and yf on the plane VP are equal. This distribution can be helpful in determining the best position for the PoF; if a scene includes a distant tall feature, the best fit of the DoF to the scene often results from having the PoF pass through the vertical midpoint of that feature. The angular DoF, however, is not equally distributed about the PoF.\nThe distances yn and yf are given by (Merklinger 1996, 126)\nwhere f is the lens focal length, v′ and u′ are the image and object distances parallel to the line of sight, uh is the hyperfocal distance, and J is the distance from the center of the lens to the PoF rotation axis. By solving the image-side equation for tan ψ for v′ and substituting for v′ and uh in the equation above,[h] the values may be given equivalently by\nThus at the hyperfocal distance, the DoF on a plane parallel to the image plane extends a distance of J on either side of the PoF.\nWith some subjects, such as landscapes, the wedge-shaped DoF is a good fit to the scene, and satisfactory sharpness can often be achieved with a smaller lens f-number (larger aperture) than would be required if the PoF were parallel to the image plane.\nThe region of sharpness can also be made very small by using large tilt and a small f-number. For example, with 8° tilt on a 90 mm lens for a small-format camera, the total vertical DoF at the hyperfocal distance is approximately[j]\nAt an aperture of f/2.8, with a circle of confusion of 0.03 mm, this occurs at a distance u′ of approximately\nOf course, the tilt also affects the position of the PoF, so if the tilt is chosen to minimize the region of sharpness, the PoF cannot be set to pass through more than one arbitrarily chosen point. If the PoF is to pass through more than one arbitrary point, the tilt and focus are fixed, and the lens f-number is the only available control for adjusting sharpness.\nDerivation of the formulasEdit\nProof of the Scheimpflug principleEdit\nIn a two-dimensional representation, an object plane inclined to the lens plane is a line described by\nBy optical convention, both object and image distances are positive for real images, so that in Figure 6, the object distance u increases to the left of the lens plane LP; the vertical axis uses the normal Cartesian convention, with values above the optical axis positive and those below the optical axis negative.\nThe relationship between the object distance u, the image distance v, and the lens focal length f is given by the thin-lens equation\nsolving for u gives\nThe magnification m is the ratio of image height yv to object height yu :\nyu and yv are of opposite sense, so the magnification is negative, indicating an inverted image. From similar triangles in Figure 6, the magnification also relates the image and object distances, so that\nOn the image side of the lens,\nThe locus of focus for the inclined object plane is a plane; in two-dimensional representation, the y-intercept is the same as that for the line describing the object plane, so the object plane, lens plane, and image plane have a common intersection.\nA similar proof is given by Larmore (1965, 171–173).\nAngle of the PoF with the image planeEdit\nFrom Figure 7,\nwhere u′ and v′ are the object and image distances along the line of sight and S is the distance from the line of sight to the Scheimpflug intersection at S. Again from Figure 7,\ncombining the previous two equations gives\nFrom the thin-lens equation,\nSolving for u′ gives\nsubstituting this result into the equation for tan ψ gives\nSimilarly, the thin-lens equation can be solved for v′, and the result substituted into the equation for tan ψ to give the object-side relationship\nthe relationship between ψ and θ can be expressed in terms of the magnification m of the object in the line of sight:\nProof of the “hinge rule”Edit\nFrom Figure 7,\ncombining with the previous result for the object side and eliminating ψ gives\nAgain from Figure 7,\nso the distance d is the lens focal length f, and the point G is at the intersection the lens’s front focal plane with a line parallel to the image plane. The distance J depends only on the lens tilt and the lens focal length; in particular, it is not affected by changes in focus. From Figure 7,\nso the distance to the Scheimpflug intersection at S varies as the focus is changed. Thus the PoF rotates about the axis at G as focus is adjusted.\n- Strictly, the PoF rotation axis remains fixed only when focus is adjusted by moving the camera back, as on a view camera. When focusing by moving the lens, there is a slight motion of the rotation axis, but except for very small camera-to-subject distances, the motion is usually insignificant.\n- The symbol J for the distance from the center of the lens to the PoF rotation axis was introduced by Merklinger (1996), and apparently has no particular significance.\n- Merklinger (1996, 24) gives the formula for the angle of the plane of focus as\n- Strictly, keeping the image plane parallel to a planar subject maintains perspective in that subject only when the lens is of symmetrical design, i.e., the entrance and exit pupils coincide with the nodal planes. Most view-camera lenses are nearly symmetrical, but this is not always the case with tilt/shift lenses used on small- and medium-format cameras, especially with wide-angle lenses of retrofocus design. If a retrofocus or telephoto lens is tilted, the angle of the camera back may need to be adjusted to maintain perspective.\n- The earliest Nikon perspective-control lenses included only shift, hence the designation “PC”; Nikon PC lenses introduced since 1999 also include tilt but retain the earlier designation.\n- When the lens plane is not parallel to the image plane, the blur spots are ellipses rather than circles, and the limits of DoF are not exactly planar. There is little data on human perception of elliptical rather than circular blurs, but taking the major axis of the ellipse as the governing dimension is arguably the worst-case condition. Using this assumption, Robert Wheeler examines the effect of elliptical blur spots on DoF limits for a tilted lens in his Notes on View Camera Geometry; he concludes that in typical applications, the effect is negligible, and that the assumption of planar DoF limits is reasonable. His analysis considers only points on a vertical plane through the center of the lens, however. Leonard Evens examines the effect of elliptical blur at any arbitrary point in the image plane, and concludes that, in most cases, the error from assuming planar DoF limits is minor.\n- Tillmanns indicates that this behavior was discovered during the development of the Sinar e camera (released in 1988), and that prior to that, the DoF wedge was thought to extend to the line of intersection of the object, lens, and image planes. He does not discuss the rotation of the PoF about the apex of the DoF wedge.\n- Merklinger uses the approximation uh ≈ f 2/Nc to derive his formula, so the substitution here is exact.\n- Strictly, as the focus distance approaches infinity, v′ cos θ → f; hence, the approximate formulas differ by a factor of cos θ. At small values of θ, cos θ ≈ 1, so the difference is negligible. With large values of tilt, as occasionally might be needed with a large-format camera, the error becomes greater, and either the exact formula or the approximate formula in terms of tan θ should be used.\n- The example here uses Merklinger’s approximation. For small values of tilt, sin θ ≈ tan θ, so the error is minimal; for large values of tilt, the denominator should be tan θ.\n- Carpentier, Jules. 1901. Improvements in Enlarging or like Cameras. GB Patent No. 1139. Filed 17 January 1901, and issued 2 November 1901. Available for download (PDF).\n- Larmore, Lewis. 1965. Introduction to Photographic Principles. New York: Dover Publications, Inc.\n- Merklinger, Harold M. 1996. Focusing the View Camera. Bedford, Nova Scotia: Seaboard Printing Limited. ISBN 0-9695025-2-4. Available for download (PDF).\n- Scheimpflug, Theodor. 1904. Improved Method and Apparatus for the Systematic Alteration or Distortion of Plane Pictures and Images by Means of Lenses and Mirrors for Photography and for other purposes. GB Patent No. 1196. Filed 16 January 1904, and issued 12 May 1904. Available for download (PDF).\n- Tillmanns, Urs. 1997. Creative Large Format: Basics and Applications. 2nd ed. Feuerthalen, Switzerland: Sinar AG. ISBN 3-7231-0030-9\n|Wikimedia Commons has media related to Scheimpflug principle.|\n- View Camera Geometry (PDF) by Leonard Evens. Analysis of the effect of elliptical blur spots on DoF\n- Depth of Field for the Tilted Lens (PDF) by Leonard Evens. A more practical and more accessible summary of View Camera Geometry\n- How to Focus the View Camera by Quang-Tuan Luong. Includes discussion of how to set the plane of focus\n- The Scheimpflug Principle by Harold Merklinger\n- Addendum to Focusing the View Camera (PDF) by Harold Merklinger\n- Unilateral Real-time Scheimpflug Videography to Study Accommodation Dynamics in Human Eyes (PDF) by Ram Subramanian\n- Notes on View Camera Geometry (PDF) by Robert Wheeler\n- Tilt and Shift Lenses: Tailored towards small-format tilt-shift lenses, but principles apply to any format']"	['<urn:uuid:6f83078b-dceb-4d00-ba85-ab7a6f412e14>', '<urn:uuid:057328b0-918b-452a-bde7-97f7b232d15e>']	open-ended	direct	long-search-query	distant-from-document	comparison	expert	2025-05-12T23:00:04.658849	9	138	3448
23	Do Aliso Viejo and Layton give the same advice about how long to prepare supplies for?	No, they give different recommendations. Layton City advises citizens to prepare to be on their own for at least 3-5 days, specifically mentioning having supplies to last for at least 72 hours (3 days). Meanwhile, Aliso Viejo recommends preparing supplies that will sustain you for 5 to 7 days after an emergency or disaster.	"[""Emergency Management Map\nVolunteer Application Forms\nWelcome to Emergency Management\nLayton City’s Emergency Management program is responsible for coordinating the combined efforts and resources from all levels of government and various volunteer organizations to assist and protect the citizens of Layton from the effects of disasters, both natural and man made. Layton’s program supports the community before, during and after unusual events and major disasters by focusing on prevention, protection, response, recovery and mitigation activities.\nThe Assistant City Manager functions as a part-time Emergency Program Manager, and serves as the City’s liaison on emergency preparedness, response and recovery issues. Duties include:\n- Coordinate training and exercises across City departments, and assist departments with their departmental emergency plans;\n- Oversee the operation of the City's Emergency Operations Center (EOC) when necessary to manage the distribution of city services and resources to respond and recover from a disaster.\n- Work closely with the Fire Department, Police Department, Public Works and other city, county and state agencies to identify and analyze the hazards that may face the community and to develop, implement and update the City’s Emergency Operations Plan (EOP).\nThe City’s Emergency Operations Plan (EOP) identifies hazards the City is exposed to and provides guidelines to accomplish the primary objectives of saving lives and protecting public health and property. This plan is updated regularly based on lessons learned during training, exercises and actual emergencies. Questions about the plan may be directed to the Emergency Program Manager.\nAnother important part of the City’s Emergency Management program is the establishment of the Layton Citizen Corps Council (LCCC) to promote community preparedness through an organizational structure dividing the City into Districts, Areas and Blocks. Volunteers serve as District and Area Coordinators, and Block Captains. The Emergency Preparedness Manager trains District Coordinators to work with citizens, faith based organizations, businesses, and other community groups to promote citizen preparedness, including Citizen Emergency Response Team (CERT) training. CERT teams in Layton are organized and coordinated under the direction of District and Area Coordinators. The LCCC also encourages citizen participation in Amateur Radio Communications, Neighborhood Watch, FireCorps and Volunteers in Police Services.\nLayton City is committed to prepare for emergencies. This includes not only preparing and training City employees, but also encouraging businesses and citizens to prepare as well.\nWhy should citizens prepare? During routine emergencies, we have become accustomed to thinking of public safety response times in terms of minutes. However, when a major disaster strikes, like a serious earthquake, response times will extend to at least several days. It is critical that Layton citizens be prepared to be on their own for at least 3 - 5 days. This is because in a major disaster, the City’s emergency services will be overwhelmed and responses will be prioritized, and it will likely take much longer before emergency services are available to everyone. Therefore it is very important that all citizens prepare in advance to take care of themselves, their families and their businesses for such an occurrence.\nWhat should citizens prepare for? Because of its varying climate and terrain, Utah can experience a variety of disasters. Layton City has identified some of the potential hazards that either have occurred, or could likely occur in the Layton area. This can serve as a useful guide as citizens develop individual, family and business preparedness plans. There are actions that should be taken before, during and after an event that are unique to each hazard. For more information, visit http://www.utah.gov/beready/family/be-informed.html\nHow will citizens be notified about a disaster? Warnings may be given to the public using an alert and notification system, AM/FM radio, television, public address systems and mobile sirens via police or fire vehicles. In addition, the Layton Citizen Corps Council has established a localized community system whereby the City provides notification through District Coordinators as a vital means for disseminating warnings. At the present time, there are four (4) established systems to give broad scale warnings to citizens. They are:\n- The Emergency Alert System (EAS) – KSL is the Local Primary broadcast station for the Wasatch Front.\n- Layton’s Emergency Notification System (Geocast Web). Requires registration. To learn more, go to: Emergency Notification System\n- Davis County Sheriff’s Alert and Notification System\n- Calling tree through the Neighborhood Network (Districts – Areas – Blocks). Learn more by viewing Community Orgaziation at Layton Citizen Corps Council.\nGet an emergency supply kit. An emergency supply kit is simply a collection of basic items your household may need in the event of an emergency. Try to assemble your kit well in advance of an emergency. You may need to evacuate at a moment’s notice and take essentials with you. You will probably not have time to search for the supplies you need or shop for them. You may need to survive on your own after an emergency. This means having your own food, water and other supplies in sufficient quantity to last for at least 72 hours. Local officials and relief workers will be on the scene after a disaster. Additionally, basic services such as electricity, gas, water, sewage treatment and telephones may be cut off for days, or even a week, or longer. Your supplies kit should contain items to help you manage during these outages. Additional information is available at http://www.ready.gov/build-a-kit and http://www.utah.gov/beready/family/get-a-kit.html.\nMake a family emergency plan. Your family may not be together when a disaster strikes so it is important to plan in advance: How will you get to a safe place? How will you contact each other? What will you do in different situations? Disaster can strike quickly and without warning. It can force you to evacuate your neighborhood or confine you to your home. What would you do if basic services, like water, gas, electricity, and telephones were interrupted? Local officials and relief workers will be on the scene, but cannot reach everyone right away. Families can cope with a major disaster by preparing in advance and working together as a team. Preparing in advance and knowing what to do when an emergency strikes is each family’s best protection and each family’s responsibility. Additional information is available at http://www.ready.gov/make-a-plan and http://www.utah.gov/beready/family/make-a-plan.html.\nGet Involved. Layton residents are encouraged to become part of their organized neighborhood network, under the direction of the appointed volunteer Block Captain. See the Mayor’s letter encouraging citizen participation www.laytoncity.org/downloads/LCCC/MayorLetter.pdf. One of the best ways to prepare to care for your family and to help your neighbors when professional emergency services are unavailable is to take a CERT class. For more information see Cert Page. Additional information is available at http://wwww.utah.gov/beready/family/get-involved.html.\nHazards in Layton\nThe first step in being prepared for disasters is knowing what to be prepared for. Identifying what hazards can occur, how often they occur, how severe the situation is likely to get and how these hazards can affect the community should be the basis for your personal and family disaster plan.\nA “hazard analysis” considers all types of hazards that have occurred, or could potentially occur. Categories of hazards include natural hazards, such as storms, floods, and seismological events, and technological, or “man-made” hazards, such as an aircraft accident or failure of oil and gas pipelines. Some hazards may be the result of civil or political issues such as a neighborhood that has been the scene of rioting or large demonstrations. Cascading emergencies, or situations when one hazard triggers others in cascading fashion, should also be considered. For example, an earthquake that ruptured natural gas pipelines could result in fires and explosions that dramatically escalate the type and magnitude of events.\nThe following is a list some of the hazards that have occurred, or could potentially occur in Layton City. Identified hazards include both naturally occurring and “man-made” events:\n- Landslides / Debris Flow\n- Soil Liquefaction\n- Pandemics / Epidemics\n- High Winds\n- Winter Storms\n- Dam Failure\n- Hazardous Materials Incidents\n- Fallen Aircraft\n- Power Outages\n- Civil Disturbance\n- Terrorism / Criminal Acts\nMake a plan, get a kit, be informed and get involved are the basic steps to prepare yourself and your family for a disaster. Here are some resources to help you learn how!\nGeneral Preparedness Information\n- 20 to Ready Preparedness Videos\n- American Red Cross\n- American Red Cross Fire Safety\n- Centers For Disease Control\n- Utah Extension Disaster Education Network (U-EDEN)\n- Federal Emergency Management Agency (FEMA)\n- FEMA - Until Help Arrives\n- Just In Time Disaster Training - Video Library\n- Kitsap County Emergency Preparedness Videos\n- Kitsap County Emergency Preparedness Resource Library\n- The Humane Society of the United States\n- American Veterinary Medical Foundation\n- Layton City Emergency Preparedness Manual\n- Food Safe During an Emergency\n- Nuclear War Survival Skills\n- Shelter In Place\n- BRU Disaster Preparedness For Seniors\n- How to Prepare for a Flood\n- FEMA - Are you ready?\nEarthquake Preparedness Information\n- SEVEN STEPS TO EARTHQUAKE SAFETY - Earthquake Country Alliance\n- Putting Down Roots in Earthquake Country - Handbook\n- Preparing your family for an earthquake\n- Reducing Earthquake Hazards In Your Home\n- Duck, cover, and hold\n- How to secure your furniture\n- Organize your neighborhood\n- Tips for pet owners\n- Tips for apartment and mobile home managers\n- Tips for the elderly\n- Tips for preparing children\n- Tips for physically challenged"", '- Emergency Services\n- Disaster Preparedness\nDisaster Preparedness Backpacks\nKick start or enhance your disaster preparedness by purchasing a Disaster Preparedness Backpack today! The City of Aliso Viejo sells prefilled, customized backpacks for $50. Click here to purchase.\n- N95 masks (x2)\n- Nitrile Gloves (x2 pairs)\n- 12-Hour Light Stick (x2)\n- 4-in-1 Gas & Water Shut Off Tool\n- Bio Hazard Bag\n- 54 Piece First Aid Kit\n- Roll of Duct Tape - 10 Yards\n- Adult Emergency Poncho (x2)\n- Large Solar Blanket - 84’ x 52’ (x2)\n- Personal Hygiene Kits (x2)\n- 4-in-1 AM/FM Radio, Flashlight, Siren, & Phone Charger\n- Folder For Critical Paperwork\n- Fillable Household Emergency Plan\n- Aliso Viejo Specific Hazard & Preparedness Info\n- Extra pockets to add items specific to your household\nThe purchase price reimburses the City of Aliso Viejo for the costs associated with the procurement of the products included in your backpack. Email questions to email@example.com or call 949-425-2598.\nHazard Specific Preparedness Flyers\nDisaster Preparedness for Your Business\nAccording to the Federal Emergency Management Agency (FEMA), roughly 40% to 60% of small businesses never reopen their doors following a disaster (FEMA), but it doesn’t have to be like this in Aliso Viejo! The price of preparing your business is far less than the cost of being unprepared. These websites have free tools and templates to assist with creating your business continuity plans, evacuation and shelter-in-place plans, preparing and training your employees and protecting your investments:\nYour business can also partner with the City in disaster preparedness, response, and recovery! If you’re interested in being a resource to the City of Aliso Viejo in a disaster, email Disaster Preparedness to find out how.\nDisaster Preparedness Presentations in the Community\nThe City of Aliso Viejo offers Disaster Preparedness presentations for groups of 10+ people to familiarize the community with local hazards, how to prepare, alert and warning systems, and what to do when disaster strikes. The presentation length and information can be tailored to the specific audience. We regularly do presentations for:\n- Boy Scout Troops\n- Faith-Based Organizations\n- Girl Scout Troops\n- Neighborhood Watch Groups\n- Schools (Students and Staff)\nEmail Disaster Preparedness to set up a presentation.\nDisaster Preparedness for those with Disabilities, Access & Functional Needs, Seniors\nIf you or someone close to you has a disability and other access and functional needs, you may need to take additional steps to protect you and your family in an emergency. Below are links to free resources to assist you in preparing for disasters:\nDisaster Preparedness for Faith-Based Organizations\nFaith-based organizations are an integral part of disaster preparedness, response, and recovery in every community. It’s critical that houses of worship in Aliso Viejo are prepared. Provided are free tools, templates, and resources to prepare your facilities and your members:\nYour faith-based organization can also partner with the City in disaster preparedness, response, and recovery! If you’re interested in being a resource to the City in a disaster, email Disaster Preparedness to find out how.\nDisaster Preparedness for Your Home\nThere is nothing more important than preparing you and your family for the unfortunate possibility that disaster can strike at any time. The City of Aliso Viejo is susceptible to damaging earthquakes and unpredictable wildfires. You can lessen the impact of a disaster by preparing you and your:\n- Anywhere You Spend Your Time\n- House of Worship\n- Place of Business\nGet a Kit\nPut together a kit that includes basic necessities for survival that will sustain you for 5 to 7 days after an emergency or disaster.\nGet a Plan\nThink about all the ways you and your family can stay safe, prepared, and together during an emergency.\nAlertOC is a mass notification system to keep Orange County, California residents and businesses informed of emergencies and certain community events. AlertOC can send time-sensitive texts and voice messages to your home, cell, and business phones, and emails with important information during an emergency.\nFor more detailed information about building your emergency kits, what to include in your emergency plan, and other ways to stay informed, please visit the ReadyOC website.\nDisaster Preparedness for Kids\nOne of the best ways to help kids feel less vulnerable in disasters is by engaging them in the preparedness process. Here are resources to help prepare kids and schools for disasters:\nYour school can partner with the City in disaster preparedness, response, and recovery! If you’re interested in being a resource to the City in a disaster, email Disaster Preparedness to find out how. Emergency planning and preparedness as well as alert and warning for public schools in Aliso Viejo is managed by the Capistrano Unified School District.\nDisaster Preparedness for Pets\nDon’t forget to prepare your pets for disasters! You should have emergency supplies and a “go bag” for each of your pets in the event you have to evacuate or shelter in place for 5 to 7 days. Here are preparedness tips and checklists for emergency supplies:']"	['<urn:uuid:6254c7ca-fea5-4327-9cef-bd4fc15ed181>', '<urn:uuid:6e05ba78-4435-4e1a-ab1b-d8c0665588fb>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T23:00:04.658849	16	54	2398
24	nuclear suppliers group australia group india pakistan membership requirements comparison	India and Pakistan have different statuses regarding membership in nuclear control groups. India has made progress in joining some groups without committing to international agreements - it recently joined the Missile Technology Control Regime (MTCR) and the Wassenaar Arrangement (which has 42 members and controls arms/technology transfers). However, both India and Pakistan remain outside the Nuclear Suppliers Group (NSG) and Australia Group. One proposal suggests offering both countries NSG membership in exchange for ratifying the Comprehensive Nuclear-Test-Ban Treaty, though India has shown little interest since it has achieved progress without such commitments. Notably, China, which opposes India's NSG entry, is not part of the MTCR or Australia Group, potentially making India's entry into the Australia Group easier.	['Although considered a nearly dead issue a few years ago, the Comprehensive Nuclear-Test-Ban Treaty (CTBT) has received new attention on its 20th anniversary because of several recent developments. These include the recent North Korean nuclear test, and today’s United Nations Security Council resolution, championed by the United States, reaffirming support for the CTBT— this after President Obama failed to muster enough support from the U.S. Senate to ratify the CTBT. Thus, with the CTBT again receiving high level coverage, it is important to consider the case of India and Pakistan, which have neither signed nor ratified the CTBT. This is especially relevant considering the Obama administration has requested India and Pakistan to ratify the CTBT.\nAlthough Pakistan developed the capability to manufacture and assemble nuclear weapons by the late 1980s, it had no immediate plans to test them because of perceived costs and consequences. Nevertheless, Pakistan tested its nuclear weapons in May 1998 in response to Indian nuclear tests earlier in the month. These tests came at the cost of international sanctions against India and Pakistan. In an effort to demonstrate its commitment to the nuclear nonproliferation regime after its nuclear test, Pakistan proposed that both India and Pakistan ought to simultaneously sign the CTBT, but India rejected the offer. Regardless, both India and Pakistan individually affirmed a self-imposed moratorium on nuclear testing, which neither country has broken.\nOne factor that could explain India’s reluctance to sign the CTBT is that India considers China as its principal strategic competitor and rival, which introduces a more complex strategic calculus (in contrast to Pakistan’s India-centric deterrence posture) that could impact its nuclear decision-making. Several other important factors could include India’s desire to demonstrate its technological prowess, seek regional hegemony, and attain global prominence. These factors have thus far prevented India from formalizing self-imposed restrictions through signing treaties like the Nuclear Non-Proliferation Treaty (NPT), the Fissile Material Cut-Off Treaty (FMCT), and, of course, the CTBT. In turn, the prevailing wisdom in India that it must not forgo the option of nuclear testing in the future will always prevent Pakistan from unilaterally signing the CTBT.\nThus, potential incentives for India and Pakistan to sign and ratify the CTBT would likely have to be coupled together. For example, India and Pakistan could be offered membership into the Nuclear Suppliers Group (NSG) in return for their ratification of the CTBT. However, one could argue that India has little incentive to agree to such rewards because it has made great progress in achieving them without committing to international agreements. For example, few have criticized India’s failure to credibly separate its civilian and military nuclear programs even though that was a condition of the Indo-U.S. nuclear deal. On the contrary, India has continued to negotiate several nuclear cooperation agreements with other states, such as Australia and Japan. Further offering exceptions to India, such as membership into the NSG without signing the NPT, would only erode the faith in the nuclear nonproliferation regime for both India and Pakistan.\nPakistan’s recent proposal to India of translating the self-imposed moratorium on nuclear testing into a mutually binding agreement could serve as an important milestone towards both countries signing the CTBT in the future. However, it is very unlikely that India would respond positively to this proposal for it has already been sufficiently rewarded in nuclear deals and membership into international regimes, such as the Missile Technology Control Regime (MTCR), without pre-conditions. Thus, for the present, it seems that the best that one can hope for is that both India and Pakistan would continue to comply by their self-imposed moratoriums on nuclear testing.\nThrough this UNSC resolution, President Obama is presumably still trying to live up to his envisioned goal of a nuclear weapons free world. However, a point to note is that this initiative comes at a time when the U.S. administration is currently pursuing a nuclear stewardship program which involves spending almost a trillion dollars on the modernization of nuclear weapons over the next three decades. This is in addition to the U.S. Air Force’s plans to develop a new long-range cruise missile and a land-based intercontinental ballistic missile. Most importantly, the resolution is not legally binding for any U.S. government, if it decided in favor of nuclear testing in future. Under these circumstances, it would be a very challenging task to convince states which have yet to either sign or ratify the CTBT, especially India and Pakistan.\nEditor’s note: With the 20th anniversary of the Comprehensive Nuclear-Test-Ban Treaty (CTBT)’s entry into signing on September 24 and in light of the United States’ upcoming United Nations resolution, SAV contributors explore whether the CTBT is still relevant today and why, how the stance of India and Pakistan–two of only three Annex 2 states that haven’t signed the treaty—has evolved over the years, and circumstances under which India and Pakistan would sign the treaty. Read the entire series here.\nImage 1: The White House, Flickr\nImage 2: India’s Kundankulam Nuclear Power plant under construction (IAEA Imagebank, Flickr)', 'Is India a nuclear power?\n- Before its nuclear weapons test in 1998, India was considered a nuclear capable country but was not recognised as one. This meant that there was no formal recognition that India was capable of producing both nuclear power and nuclear weapons.\n- The reason for this was mainly because India had not acceded to the Nuclear Non-Proliferation Treaty which aims to prevent spread of nuclear weapons and weapons technology and promote nuclear disarmament, which India considered discriminatory since it could do so only as a non-nuclear power.\n- That would mean that the five permanent members of the UN Security Council — the U.S., the U.K., China, France and Russia — coincidentally all nuclear powers, would in effect continue to deny India the recognition as a nuclear power as well, even though it was widely acknowledged as a nuclear capable country after the 1974 “peaceful nuclear explosion” at Pokhran.\nWhat is India’s stand?\n- India wanted to break out into the open and be recognised as a nuclear power like the other nuclear countries and be part of the global nuclear order, not as a nuclear hold out. For that to happen, India needed to be recognised as a nuclear power and be made part of the global non-proliferation architecture. There are four groupings of countries that multilaterally work to prevent and address proliferation of nuclear weapons, the technology that enables making of nuclear weapons and the systems that are capable of delivering those weapons. The Wassenaar Arrangement is one such. The Missile Control Technology Regime (MTCR) is another; India became a member of this grouping last year. The Australia Group and the Nuclear Suppliers Group are the other two; India is not a member of either yet.\nWhat is the Wassenaar Arrangement?\n- The Wassenaar Arrangement is a grouping of 42 countries, of which India is the latest entrant (on December 8) that seek to bring about security and stability, by fostering transparent practices in the process of sale and transfer of arms and materials and technologies that can be used to make nuclear weapons with a view to prevent any undesirable build-up of such capabilities. By doing so the grouping hopes to stymie destabilising developments.\n- A further aim is also to prevent these proscribed items and technologies from falling into the hands of terrorists as well. Significantly, one of the purposes of the arrangement is to “enhance co-operation to prevent the acquisition of armaments and sensitive dual-use items for military end-uses, if the situation in a region or the behaviour of a state is, or becomes, a cause for serious concern to the Participating States.”\nHow do Wassenaar countries do this?\n- They periodically exchange detailed and specific information on transfer or denial of items or technology, both conventional and nuclear capable, to countries outside the grouping. This is done through maintenance and updating of detailed lists, of dual use and munitions that are considered militarily significant.\n- There are various classifications of the sub-sections of the lists. The dual use list has for example, the sensitive list and the very sensitive list. The disclosure of such sale or transfer is voluntary.\nWhat are the advantages?\n- India will be able to more easily access dual use technologies and materials and military equipment that are proscribed for non-participating members. India will also be able to sell its nuclear reactors and other materials and equipment indigenously produced without attracting adverse reactions. It will also be in a better position to collaborate with other countries in developing such capabilities.\nWill it help to join other groups?\n- There are more or less the same countries in all these groupings, with one crucial exception. China, which has been opposed to India’s entry into the NSG, is not part of both the MTCR as well as the Australia Group. So it should be easier to get into the Australia Group.']	['<urn:uuid:de666047-b03a-40e3-beb5-840690fe5910>', '<urn:uuid:fcca27f3-5e99-4ec1-9cd8-23689eb5f6fa>']	open-ended	direct	long-search-query	similar-to-document	comparison	novice	2025-05-12T23:00:04.658849	10	117	1489
25	systolic diastolic blood pressure measurements frequency monitoring guidelines	Blood pressure should be checked at least every two years according to health organizations like the American Heart Association. Blood pressure is measured using two numbers: systolic (pressure during heartbeat) and diastolic (pressure between beats). Normal blood pressure is 130/85 or below, with 120/80 being optimal, while 140/90 or above is considered high blood pressure.	['Original source: https://pennstatehershey.netreturns.biz/HealthInfo/Story.aspx?StoryId=32a654d8-1370-4d02-91c6-92c71af34e1f#.Wx_iRVyplBw\nScreening tests can catch serious diseases in very early stages, before they do major damage to your health.\nYou may think of the doctor’s office as someplace you go when you’re sick. But it’s important to have an occasional visit when you’re well too.\nAll men should have a few routine screening tests. These tests can help catch health problems early, before they’ve progressed enough to cause symptoms. And treatment in these early stages is more likely to be fully effective.\nYou can get screened for lots of diseases, but most men only need screening for a few. The diseases you need screening for, and how frequently you should be screened, varies according to your health and risk factors. The following tests are recommended for most men:\n- Blood pressure tests\nA number of health organizations, such as the American Heart Association (AHA), recommend blood pressure checks at least every two years and more often if it is high. Keeping your blood pressure at safe levels will reduce your risk of heart disease, the leading killer of American men.\n- Cholesterol screening\nThe AHA recommends cholesterol checks once every four to six years starting at age 20.\nHigh blood cholesterol is a major risk factor for coronary artery disease.\n- Colorectal cancer screening\nRegular screening for this cancer should start at age 45, and maybe sooner if you’re at high risk.\nYour doctor can help you decide on the best type of screening test for you.\n- Prostate exams\nThe American Cancer Society (ACS) recommends talking to your doctor about prostate cancer screening when you turn 50 years old. The prostate specific antigen (PSA) test and digital rectal examination (DRE) can uncover this cancer at an early stage. But screening has drawbacks, too, and some groups don’t recommend routine screening for this cancer. Your doctor can help you decide if it’s right for you.\nFor men at high risk, such as African-American men and men with a close family member who had prostate cancer at an early age, the ACS recommends the discussion with your doctor start at age 45 or earlier.\n- Blood glucose testing\nAll men age 45 and older should think about getting screened for diabetes, says the National Institutes of Health. A simple blood test can reveal if you have diabetes or its precursor, prediabetes. Identifying and treating these conditions in their early stages helps prevent serious damage to organs all over the body.\nThis test is even more important for men with risk factors such as a family history of diabetes, high blood pressure, high blood cholesterol or high body weight. You’re also at higher risk if your family background is African American, American Indian, Asian American, Pacific Islander or Hispanic.\n- Skin exams\nThe ACS recommends monthly self-exams to look for growths or changes that could be skin cancer. Signs to look for include any type of change in a mole or freckle, or a growth with uneven colors, borders or shape, or that is larger around than a pencil eraser. Check your whole body, head to toe, and consult your doctor about anything suspicious.\n- Depression screening\nScreening for this serious, treatable disease should be a part of everyone’s regular healthcare, according to Mental Health America.\nIf you’ve felt down, hopeless or uninterested in the things you usually enjoy for two weeks straight, it’s even more important to ask your doctor about screening for depression.', 'What is blood pressure?\nAs the blood circulates around the body, it is under pressure. The blood pressure is a result of the pumping action of the heart and the resistance of the vessels, through which the blood flows.\nWhen blood pressure is high it puts an unnecessary strain on the heart and blood vessels.\nBlood pressure may rise temporarily due to exercise, excitement, anger or anxiety making your heart beat faster.\nWhat is normal blood pressure?\nYour blood pressure levels will vary during the day and will normally be highest around lunchtime or after exercise and lowest when you are resting or sleeping. Blood pressure is quoted as 2 numbers, the first number known as systolic pressure, is the blood pressure during each heartbeat. The second, known as diastolic, is the blood pressure between beats.\nThe latest guideline from the British Hypertension Sociey (2004) defines blood pressure of 130/85 or below as normal but say that 120/80 is the optimal. 140/90 is the level used to diagnose high blood pressure. If your blood pressure reading is consistently 140/90 or above this is considered high blood pressure and treatment may be required. The high blood pressure reading can be either a systolic reading of 140 or above, a diastolic reading of 90 or above, or both.\nWhat causes blood pressure to be high?\nAnyone can develop high blood pressure and it doesn’t always have an obvious cause. If you have persistent high blood pressure, the condition is called hypertension.\nHigh blood pressure tends to run in families. Blood pressure will also increase, as you get older. This may be why heart attacks are more common in older people. In a few cases high blood pressure can be related to another medical condition.\nThere are many other elements that contribute to high blood pressure that can be avoided, such as being overweight, drinking too much alcohol, smoking, lack of exercise and a poor diet.\nHow can I tell if I have high blood pressure?\nYou may feel perfectly well, as high blood pressure usually causes no symptoms and generally will go undiscovered until a doctor happens to take your blood pressure. If you have very high blood pressure you may experience dizziness, headaches, blurred vision and possible breathlessness. The only way you can tell for sure if you have high blood pressure is to have it measured by your doctor, or purchase one of the many blood pressure monitors that are available from most pharmacies.\nWhat are the dangers of having high blood pressure?\nHow can I reduce the risk of high blood pressure?\nA healthy lifestyle can help prevent high blood pressure. For a healthy lifestyle you should cut down on salt intake and watch your weight, find a weight that is right for you and try to stick to it. You should also give up smoking, drink less alcohol, eat a balanced healthy diet and get regular exercise.\nIf these measures are not successful in lowering blood pressure, then there is a wide range of drugs available to treat high blood pressure. Your doctor will help you decide what treatment is right for you. Some women experience high blood pressure because of the contraceptive pill, your doctor will monitor your blood pressure if you are on the pill. High blood pressure can also occur during pregnancy and usually returns to normal after the birth, always consult your doctor if you are worried.']	['<urn:uuid:ac181618-d61b-4838-a2e9-9f1c5de579e6>', '<urn:uuid:c3fbce36-c9f1-4809-8d6f-9c131c877fde>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T23:00:04.658849	8	55	1148
26	what is the origin word hurricane ancient god	The word 'hurricane' is derived from Huracán, who was the god of wind and storm. Huracán was a deity who could command the natural elements of the earth and played an important role in creating humankind, particularly in creating the third and final version of the human race out of corn.	"['A ""myth"" is a traditional story without an author that is usually intended to:\nIn ancient times, something like a clap of thunder must\'ve caused quite a fright. They didn\'t understand the inner workings of meteorology. Imagine the panic a hurricane or earthquake would\'ve incited.\nIn order to understand these phenomena and, perhaps, bring comfort to people, myths were created as explanations for the many mysteries of life. Even our ancient ancestors contemplated the creation of earth, natural disasters, flaws in humanity, death, and love.\n\'Myth\' is derived from the Greek work \'mythos\', which means \'story\'. Myths are usually very old and came about during a time when science, philosophy, and technology were not very precise.\nMyths exist in every culture and country. Most cultures have their own creation myth. The majority of myths, regardless of their premise, tend to involve the exploits of gods or heroes who possess god-like qualities.\nYou\'ll find that many myths are set in a timeless place and involve a plethora of symbols that have multiple meanings. This is what makes myths such an interesting study: decoding their symbols.\nRa was the sun god, often regarded as the most important of all Egyptian gods. Ra emerged from the chaos at the beginning of time and created the world. As well as being a god, Ra also ruled as Pharoah. Many Egyptian rulers claimed they were descended from Ra, to give credence to their seat on the throne.\nAccording to the myth, Ra eventually began to grow weary of his duties. This made his people question whether he should be their ruler. When Ra learned this, he sent his daughter, the \'Eye of Ra,\' to kill those who doubted him. She went on a rampage and Ra took pity on the humans. He tricked her into getting so drunk she couldn\'t continue killing. After this, Ra left the earth for the sky.\nThis myth can be interpreted as a reflection on the flaws of humanity.\nPoseidon was the god of the sea, son of Kronos and Rhea. He became ruler of the sea when the universe was divided after the fall of the Titans. Poseidon was bad tempered and when he became angry, he would cause earthquakes. He managed to upset even Hades, the ruler of the underworld. Poseidon was most often depicted with his trident, and sometimes riding a horse (he is sometimes claimed to be the creator of horses).\nNatural disasters still leave us flummoxed, even today. So, how could earthquakes be explained in ancient times? An angry god who shook the earth is one way.\nA bean sídhe was a fairy spirit, that would wail whenever a family member was about to die. (In English, we refer to them as banshees.) The bean sidhe didn\'t cause death but announced the death or warned loved ones that death was near, so they could prepare. Some believed she escorted their loved ones safely to the afterlife.\nDeath - like creation - is one of those inexplicable facets of life. Many cultures employed myths in order to wrap their minds around how and why these things occurred.\nIzanagi and Izanami are Shinto creator gods. Izanagi and Izanami created the Japanese islands and the deities of the sea, wind, mountain, river, trees, and rice. When Izanami gave birth to a fire god, it fatally burned her.\nIzanagi resolved to have his wife returned to him and sought her out in the underworld. Izanami could not return because she\'d eaten \'the dark food,\' but said she would beg the gods to let her go. After waiting a long time Izanagi lit a torch to go find her and when he saw her rotting body he fled and sealed the door to the underworld. Izanami then vowed to kill 1,000 people each day and Izanagi promised to create 1,500 to replace them.\nThis myth would once been used to explain how everything came into being and the cycle of life and death.\nHuracán was the god of the wind and storm, and could command the natural elements of the earth. He played a large role in the three attempts to create humankind, destroying the second generation that displeased the gods and creating the third and final version of the human race out of corn. The word \'hurricane\' is derived from the god Huracán.\nLike earthquakes, hurricanes are often unpredictable. Eons before the advent of meteorology, gods like Poseidon and Huracán were created to help people understand these phenomena.\nMarduk was possibly the most important god in Mesopotamian mythology. Marduk led the new gods in battle against the old gods. Marduk and his army defeated the old gods and he became the supreme god. After this, Marduk created the sky and earth, as well as the first human beings. Marduk decreed that humans would do the work gods had no time for and in return the gods would care for them.\nEven today, we debate the origins of the earth. This myth is another example of a culture\'s attempt to explain how we all came to be and out purpose in life.\nThor is the hot-tempered god of thunder in Norse (Germanic, Nordic, or Scandinavian) mythology, the son of the chief god, Odin, and goddess of the earth, Fjorgyn. As a strong and courageous warrior, Thor was protector of both gods and humans. Lightening bolts flew from his hammer Mjöllnir as he slew enemies, but it could also be used to heal and revive. Thor was so revered that when the Anglo Saxons adopted the Roman calendar, they named the fifth day of the week \'Thursday\', after him.\nThunder and lightening can be powerful and frightening. So, believing it was Thor defending his people probably soothed some terrified souls.\nThis is one we all know, illustrating the magnitude of mythology. Cupid was the god of love and desire. He was the son of Venus and usually depicted as a winged child carrying a bow and arrows. His arrows could either draw people together, or pull them apart.\nThis myth helps to explain why one person can be so intensely drawn to another and even why relationships don\'t always work out the way you hoped and shows that myths didn\'t only cover natural events but also day-to-day life.\nThe Cailleach is the winter goddess who created the mountains and hills that speckle the Scottish Highlands. Some myths say she created them as her stepping stones. Others say she accidentally created them when she dropped rocks out of her apron. Her staff froze the ground but she kept the seeds safe for spring.\nHer story encompasses the power and beauty of nature and the changing of the seasons. Of course, now we have the benefits of science. But, back then, it might\'ve been helpful to attribute such glory to the hands of a god.\nHere we are, in the 21st century, and many of us still enjoy mythological tales. Courses are taught at universities. Tomes have been written on the subject. Even blockbuster movies relive the adventures found within mythological tales. Isn\'t it fun, just for a moment, to imagine a god of thunder who fearlessly protects his people from fates worse than death with a magical hammer?\nToday, we don\'t need a storm god to explain wind and rain. However, you can see why myths were once created. Ancient people wanted to wrap their minds around mystifying acts of nature they couldn\'t otherwise comprehend. Perhaps the human condition hasn\'t changed much because they, too, couldn\'t make much sense of love. Maybe it\'s time for Cupid to step up his game.']"	['<urn:uuid:22714232-d830-4ba3-93dd-8fb9d8d95138>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T23:00:04.658849	8	51	1269
27	new south wales role australian unification	New South Wales played a crucial role in advancing federation when it finally supported the cause. The colony had previously refused to participate in the Federal Council of Australasia (which met in 1886, 1888 and 1889). When NSW Premier Henry Parkes called for a convention following his Tenterfield speech, it led to the 1890 Federation Conference where colonies agreed to unite as one nation under the British Crown.	"[""|Title:||Australasian Federation Conference|\nNames of the attendees photographed:\n|Keywords:||Australian history, federation, government, Constitution|\n|Record creator:||Department of Information, Central Office|\nThis is a black-and-white group photograph showing the secretary and 13 colonial delegates to the Australasian Federation Conference. It shows the men seated or standing in front of Parliament House in Melbourne before or after one of the sessions of the conference, held between 6 and 14 February 1890.\nThis photograph shows the politicians from the six Australian colonies and the colony of New Zealand who, meeting in Melbourne in February 1890, agreed that the interests of the Australian colonies would be best promoted by uniting as one nation under the British Crown. They transformed the federation question from whether it should occur to how it would occur, agreeing to meet to draw up a draft constitution the following year.\nThe Australasian Federation Conference was a significant event and each of the colonies sent delegates who were described at the time as 'representative public men' – public figures who were members of colonial parliaments. Nine were ministers in colonial governments. Three were premiers: Duncan Gillies (1834–1903) of Victoria, Henry Parkes (1815–96) of New South Wales and John Cockburn (1850–1929) of South Australia. Debate among these 'public men' was vigorous.\nThe conference met from 6 to 14 February 1890, and between 10 and 13 February debated Parkes' motion that the colonies ought to unite, finally amending it to remove any reference to Australasia. On 13 February, it also agreed to a motion moved by the NZ delegates that the remoter Australasian colonies would be able to join any union of Australian colonies, and to a motion moved by Alfred Deakin (1856–1919) that a national constitutional convention be called.\nThis conference was not the first intercolonial gathering to debate a union or federation of the Australian colonies. Significant among the early meetings was the Australasian Intercolonial Conference held in Sydney in November and December 1883. This conference resulted in the ineffectual Federal Council of Australasia, which met in 1886, 1888 and 1889. The council was handicapped by the non-participation of New South Wales, New Zealand and South Australia (until 1889).\nThe 1890 conference was triggered by two events in October 1889 – a report on the defence of the Australian colonies and Parkes' Tenterfield Address. The defence report prepared by James Bevan Edwards (1834–1922) found that the colonial forces would be unable to defend the continent from attack and recommended that the colonies combine for mutual defence – federating their forces, using the same weapons and establishing a military college for officers.\nFollowing Edwards' report, Parkes made a famous speech in Tenterfield, New South Wales, on 25 October 1889, in which he argued for federation, stating that he hoped to live long enough to see 'the dawning of a great new day in our beautiful land when all matters of national importance are considered and acted upon by a national government'. With New South Wales now supporting federation, Parkes' calls for a convention resulted in the 1890 conference.\n© Education Services Australia Ltd and National Archives of Australia, 2007-10""]"	['<urn:uuid:b48e2032-ea72-4dc5-b7e1-a14fdcb5ce31>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T23:00:04.658849	6	68	516
28	I'm designing a comprehensive pool safety system. What are the three most critical components needed: a safety cover, rescue equipment, and lighting/alarms?	A safety cover is essential as it provides a safety barrier and prevents debris from falling into the pool during off-season months. It comes in two main types: solid covers that allow minimal sunlight and mesh covers that allow water drainage while catching debris. For rescue equipment, you need a first aid kit for emergency response, life jackets for learners, a spine board for injuries, and a life hook for quick drowning rescues. As for lighting and alarms, you should install pool alarms in strategic locations like the pool deck, entrance, windows, and doors to prevent unauthorized access, along with energy-efficient LED lighting systems that serve both safety and decorative purposes.	"['by Sean Griffin, October 2, 2009\nA safety cover is ideal for the homeowner with a swimming pool in their backyard. It adds a safety barrier and keeps debris from falling into the pool during those months when the pool is not in use. It makes spring cleanup easier and is visually more appealing than any tarp style cover that has the potential of turning into a backyard swamp.\nThere are several different manufacturers who offer a wide variety of covers. Different standards of mesh and different styles of solid covers allow you to choose the cover that suits you best.\nThe first step is deciding which cover is best for you. Features & Benefits to compare include warranty, amount of sunlight that penetrates cover, spring capability, quality of craftsmanship, and of course pricing. Colors are also an option on any safety cover and availability varies from manufacturer. Covers range from a standard green to a kaliedoscope cover, using multiple colors.\nSolid Covers- Allows minimal to zero sunlight to reach water. Tend to be more expensive and are heavier than mesh. Can have drain panels or be completely solid and utilize a cover pump to remove standing water.\nMesh covers- Allows water to drain into the pool while catching fallen debris. Mesh weaves and fabrics vary. Lightweight and easy to put on and remove.\nTake note of when you open and close your pool and your surrounding climate. Pool owners that are prone to algae outbreaks and open later in the season should consider a solid cover. If you open early, close late and monitor water chemistry - a mesh cover is ideal.\nMEASURING FOR A CUSTOM MESH SAFETY COVER-\nThis will depend on the shape of your pool. For simple rectangular pools we can measure the pool perimeter using inside dimension (waterline). Take note of all non removable objects, including diving board, ladders, handrails, and note the amount of decking around the pool to ensure anchors will align on deck. Also note the type of coping you have. If you have flagstone or any stone that has a sharp edge, additional padding might be necessary to prevent premature wear. Form fit pools, kidney shape pools, ellipse and round pools might require an A-B measurement. This is entered into a CAD program to triangulate the points and design the cover to follow the curve of your pool. To get forms for AB measurement you can follow this link... http://www.poolcenter.com/measure_guide.pdf\nAn A-B measurement is taken by first marking points around the inside perimeter of the pool. I have used chalk and I’ve used masking tape, but anything that you can mark with a number and stay in place to take a measurement from will work. Depending on the curve or if it is a tight angle you will space your point about 2-3 feet apart. For tighter turns you would want to mark off more points and for broader curves you can space farther apart. For straight walls, your points can be up to 5 feet apart. Next you will set up your A-B line. This line should be approximately 2/3 the length of the pool and at least three feet back from any point on the pool line. You will then take a measurement from both A & B to all points around the pool. With this information, and the known distance between A and B, a series of triangular measurements accurately plots your pool shape. You will also take a few cross dimensions for further confirmation.\nORDERING A MESH SAFETY COVER-\nOnce you’ve’ decided on what cover you like you can check our site for a list of stock covers to see if your pool qualifies. If your pool cover size is not considered ""stock"" you can submit your measurement to Poolcenter.com by Fax, email, or mail to receive a custom quote. Some homeowners opt to use a stock size cover to avoid price increase that accompanies customized covers. Be aware that on the underside of the pool cover, wear strips are sewn in to deal with the friction that occurs at the edge of the coping. A improperly fitted cover may not have the wear strips crossing the coping stones. However, some folks still use a larger rectangle to cover a free form, or an odd sized rectangular pool. Although this voids the warranty, if you are willing to pad the cover in certain areas, using carpet strips or other suitable material, these stock covers can still perform well on your pool.\nIf you are ordering a replacement cover, anchor locations will need to be measured to ensure correct panel spacing. Some manufacturers are also willing to use old cover as a template. Simply clean and remove springs first, and send the old cover directly to the designers. Contact your dealer before sending, so the manufacturer will know who to send the quote to.\nTurnaround time varies between manufacturers. Custom covers typically take a little longer than stock covers. Once order is placed a confirmation will be issued with approximate ship date. When it is shipped a racking number will be applied. Most mesh and solid safety covers ship UPS or FedEx.\nINSTALLING A MESH SAFETY COVER-\nThere are several different methods for installing a safety cover. The easiest is to float the cover on top of the pool water to position. Then you would drill anchors in at the four corners and continue working your way around the cover. The straps are adjustable and should be in the position to allow spring to compress 2/3 of the way. I have used water buckets to hold the cover in position on up and over cover installs. I have also heard of setting up a lattice to hold cover.\nObviously some installs are more complex than others. For custom covers, the trick is constant centering and re-centering. Start from the inside and work toward the outside (opposite from rectangle covers). Keep modest tension on the springs until all anchors are drilled, then adjust if needed.\nIf you don\'t own a Rotary Hammer drill, you can rent one at any good rental shop. You\'ll need a 3/4"" masonry drill bit. A sharp bit will speed things up, at around 30 seconds per hole, but a dull bit can take several minutes per hole. A nice hammer drill will also speed things up. They are rated at BPM (blows per minute), and nicer brands include Milwaukee, Bosch and Hilti. It should look like a small jackhammer. If it looks too small to do the job, it probably is. Handheld drills and cheap bits will take a long time to get your cover installed.\nAfter installing your cover, make adjustments to all the straps after a day or so has gone by, to allow for small amounts of stretching. The springs should be adjusted less than 1/2 of their compression. Not too tight, not too loose. In areas of water features, raised walls or steps, work with the tension to close up any gaps that might allow debris, animals or small children to slilp through.\nFor more information on mesh safety pool cover installation, check our Pool Info page regarding.', 'Rescue EquipmentThere are many essential swimming pool safety accessories that you should have within the swimming pool area, such as:\n- A first aid kit: to help you easily and efficiently respond to any emergency before seeking proper medical attention.\n- Life jackets: for those who are learning how to swim to remain floating on the water surface when they become helpless.\n- Spine board: to carry swimmers with injuries of the pool.\n- Life hook: a quick way of rescuing people from drowning by pulling them off the pool.\nAlarms and LightingSwimming pool alarms are an important security feature, rendering it nearly impossible for any intruder to access the swimming pool area. These alarms can be installed in strategic locations such as the pool deck, entrance, windows, and doors. Swimming pool lights, mainly LED systems that are energy-efficient, will not only work as a safety measure but also double up as an aesthetically pleasing decoration.\nCleaning AccessoriesCleaning is an important process in ensuring the pool maintains the required standards of hygiene and reduce the risk of infections due to a build-up of algae or bacteria. Common cleaning accessories include:\n- Chemical floater: an accessory that floats on the water surface, slowly releasing chemicals.\n- Swimming pool broom: for the easy removal of algae and leaves in water.\n- Pool vacuum cleaner: This helps reach algae or some stains are hard to remove manually.\n- Filter cartridge cleaner: for the easy maintenance of pool hygiene.\nMaintenance AccessoriesMaintenance accessories, such as a maintenance kit can be extremely handy to have around, as they come well-equipped with all the accessories you need when opening bolts, cutting pipes, testing PH, or unblocking pipes. Another important maintenance accessory is the electric swimming pool drain pump, which is important when draining pool water. A pump removes any solid contaminants that might be in the water, emptying them into a skimmer basket or filter sump, and it keeps the water fresh by circulating it in and out of the pool. A filter is attached to a pool pump and it should run between 4 and 8 hours every day unless the pool is closed off during the winter months.\nEntertainment AccessoriesThese are vital, in a way, particularly if you have children looking to enjoy their time in the family pool. There are many high-quality accessories on the market that vary in design and brand that can be great ways for kids to have fun, including:\n- Inflatable boats/rafts: which come in all shapes and sizes and can carry anything between 3 and 10 people, and enable kids to use their imaginations, as though they are in a lake of the sea. Some inflatable boats and rafts come with handles and chairs.\n- Pool toys: there are so many inflatable pool toys to choose from, including beach balls, water slides, baby floaters, beach toys, pool noodles, water guns (known as blasters or soakers), diving toys, and snorkeling apparatus.\nSwimming Pool Steps and LaddersJumping into and out of the swimming pool can be very dangerous. Therefore, you need to make sure that you purchase a step or a ladder that is strong and corrosion-free, to make it easier for children to access the swimming pool. Some swimming pool ladders are made from strong materials, such as stainless steel or resin\nAccessories for Everyday Pool OperationsA swimming pool is a huge system assembled from so many different components. Depending on the design and purpose of your pool, you may need to look at buying some of these accessories listed below:\n- Swimming pool heater\n- Filters such as rapid or high rate sand filter\n- Recessed automatic skimmers\n- Floor inlet fittings\n- Drain covers\n- Gas chlorinators\n- Chemical feeders, generators, and controllers']"	['<urn:uuid:3a13e951-ff01-464e-8638-501c282c5760>', '<urn:uuid:d986c031-5f90-4ee1-aecc-ff2f165fbe81>']	open-ended	with-premise	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-12T23:00:04.658849	22	111	1823
29	I work in cross-cultural communication and often encounter different behavioral expectations. Could you explain how ancient societies distinguished between universal and culture-specific rules, and how does this compare to modern sociological classifications of behavioral norms?	Ancient societies, particularly Romans, distinguished between 'jus gentium' (universal law) and 'jus civile' (laws specific to each state). The jus gentium represented principles they believed were common to all civilized nations, such as basic concepts of marriage and property ownership. However, they recognized that some practices, like the specific form of Roman marriage, belonged to their particular civil law. In modern sociology, this distinction is reflected in the classification of norms into different categories: folkways (everyday conventions), mores (moral guidelines), laws (written and officially enforced rules), and taboos (strongly prohibited behaviors). Like the Romans acknowledged variations in civil law, modern sociology recognizes that norms can differ significantly between societies and even between groups within the same society. Both systems acknowledge the need for enforcement - ancient societies through legal systems and modern societies through social control mechanisms including both formal sanctions and informal social pressure.	"[""« PrejšnjaNaprej »\nthe science of navigation ought to teach what are the essential conditions of a ship; the science of architecture, what are the essential conditions of a house, a bridge, or any other structure; the science of banking, what are the essential conditions of a bank, and so on-before they respectively proceed to consider what it is that constitutes the excellency or defects of a ship, a building, or a bank.\n$ 3 11. After we leave the certain and universally true propositions of positive politics, we come to another class of general propositions, lying on the confines of positive politics, but nevertheless essentially distinct in their nature. These are the propositions which may be styled the principles of universal jurisprudence.\nThese principles were called by the Greeks unwritten laws.(0) That is to say, they were political or moral maxims, which were recognised as having a binding force almost equivalent to a positive law, but which had not been reduced into writing by any lawgiver. They were conceived as rules obligatory upon mankind, of universal application, emanating from a Divine source, and sometimes too strong to be repealed by any human ordinance. Thus Antigone, in the tragedy of Sophocles, describes herself as bound, by the unwritten and immutable laws of the\n(6) The expression unwritten laws (ãypadol vóuoi) first occurs in the funeral oration of Pericles, in Thucyd. ii. 37, where it appears to denote those laws of the state which are corroborated by the moral sanction. It next occurs in a conversation between Socrates and Hippias, reported in the Memorabilia of Xenophon, iv. 4, § 19-25. In this passage,\nit signifies moral rules of universal application, such as the duty of honouring parents. Compare Xen. (Econ. 7, § 30-1. The expression was doubtless adopted by Socrates from popular usage. Thus, Plato speaks of tà καλούμενα υπό των πολλών άγραφα νόμιμα.-Leg. vii. p. 793. It occurs frequently in this sense in the Greek writers, see Demosth. cont. Aristocrat. p. 639; and the passages cited by Ast ad Plat. Rep. viii. p. 563. In Plato, Politicus, c. 37, ãypapa márpia con are opposed to written laws. Dionysius, Ant. Rom. vii. 41, speaks of ãypadov kal ávouodérntov púoews dikalov. On the opposition of con and vópoi, i. e. of customs and written laws, see Ast ad Plat. Leg. i. 8, p. 636. Aristotle, however (Rhet. i. 13, § 2), makes unwritten law a species of jus civile, or of the law peculiar to each state ; and opposes it to the universal law of nature. See the notes to the passage in Dr. Gaisford's collection. This use of the word agrees exactly with that of the Roman law. See Inst. i. 2, § 3, where reference is made to the Greeks.\ngods, to bury the dead body of her brother, notwithstanding the prohibitory decree issued by the king. (*)\nAs examples of these universal principles of government, we may take the seven axioms laid down by Plato in his Laws, which he declares to be applicable to every state, great or small, and likewise to families; to be of necessary obligation, and to be according to nature. These are\n1. That parents should rule over children. (0)\nmiss it.('') Of these seven principles, Plato declares the sixth to be the most important, in which view of government he is supported by the general voice of the ancient philosophers. His seventh principle affords a remarkable instance of the danger of generalizing institutions with which the writer himself is familiar ;(12) for election by lot, though universal in the Greek democracies,(13) is rarely used in the free governments of modern\n(7) Antig. 454.\n(8) It is laid down to be a law of nature (κοινός ανθρώπων νόμος, ον η púois čowkev ätaoiv), that progenitors should rule over descendants, in the speech of Fufetius in Dion. Hal. Ant. Rom. iii\n. 10. The existence of any such necessary institute of nature is denied by Tullus Hostilius, ib. c. 11. Fufetius afterwards (c. 23), in a speech to the Alban officers, describes the Romans as violating the natural law, common to Greeks and Barbarians, that fathers should rule over their descendants, and mother-states over their colonies. He treats their conduct as an attempt to supplant divine by human laws.\n(9) Servius Tullius, in Dionys. iv. 23, denies that the distinction between master and slave is natural. Compare the doctrine of the Roman law, cited above, vol. i. p. 429, n. 115.\n(10) The same law of nature is laid down by Dionysius, ib. i. 5. (11) De Leg. iii. 10, p. 690. He calls these principles átlápata.\n(12) For Plato's more detailed doctrine respecting appointment by lot, see Leg. vi. 5. He considers it in the light of a necessary evil.\n(13) In the Rhetoric, Aristotle defines democracy to be a government in which the offices are distributed by lot (i. 8, § 4). Concerning the natural law of Zeno, see Minucius Felix, c. 19.\ntimes. A similar remark applies, though in a less degree, to the subject of slavery.\nThe Romans called those legal principles and institutions, which were deemed to be common to all nations, by the appellation of jus gentium, as distinguished from the jus civile, or the law peculiar to each state. As the Romans extended their conquests, and incorporated into their empire a larger number of independent states, each possessing a peculiar system of law, they formed, by this experience, an idea of certain abstract principles and rules of jurisprudence, which were common to all, or most of these states. To this abstract collection of rules they gave the name of jus gentium—the law common to all nations. Afterwards, as their legal relations with their subjects increased, it acquired in some measure a positive character, and was engrafted into the practical jurisprudence of the empire. It became a sort of lingua franca-a medium of communication between persons of different countries—in which the peculiarities of their respective languages were rubbed off, and they were thus enabled to meet on a common ground.\nThere were certain principles or institutions which all systems of law, in countries with which the Romans came in contact, agreed in recognising, such as dominion or ownership, buying and selling, hiring, partnership, marriage, slavery. These, independently of the peculiar form in which they were clothed by the jurisprudence of each state, were juris gentium ; any peculiar form of them, such as the Roman marriage, belonged to the jus civile.(-4)\nThe jus gentium of the Roman jurists was equivalent to their jus naturale, to those positive institutions and laws, which\n(14) Inst. i. 2; i. 1, § 11. See the subject explained with remarkable clearness and precision by Savigny, System des heut. Römischen Rechts, vol. i. p. 108-116, and aph. i. p. 413-20. As to Ulpian's fanciful doctrine of a law common to men and animals, see above, vol. i. p. 16. Compare also Austin, Prov. of Jurisp. p. 188-91.\nThe patria potestas is an example of a right peculiar to the jus civile of Rome, and not a right juris gentium. Jus autem potestatis, quod in liberos habemus, propriumi est civium Romanorum. Nulli enim alii sunt homines, qui talem in liberos habeant potestatem, qualem nos habemus.' -Inst. i. 9, § 2. Compare Dig. i. 6, § 3.\nwere substantially common to all nations enjoying a certain degree of civilization.(15) Taking man, with the conditions of his nature, physical and mental, and of society, as they exist, he naturally establishes certain laws and institutions; and, hence, the law common to all nations and natural law become equivalent.(16) Thus, as soon as regular judicatories are in operation, the principle of hearing the defence of an accused man before he is condemned,(7) and the principle of hearing both parties in a suit before the decision is given in favour of either, can scarcely fail to be recognised. Other elementary principles of this sort, having a very wide application, (such as that ‘no man ought to be a judge in his own cause,') might be adduced.\nSuch practical principles, however recommended by an obvious and strong utility, are not acknowledged by all nations; and Savigny properly remarks, that when the Romans speak of the jus gentium as being recognised by all men,' and 'all nations,'(18) they were doubtless aware that their induction was\n(15) · Neque vero hoc solum naturâ, id est jure gentium, sed etiam legibus populorum, quibus in singulis civitatibus respublica continetur, eodem modo constitutum est, ut non liceat sui commodi caussâ nocere alteri.'—Cic. de Off. ii. 5. Societas est enim latissime quidem quæ pateat, hominum inter homines ; interior eorum, qui ejusdem gentis sunt; propior eorum, qui ejusdem civitatis. Itaque majores aliud jus gentium, aliud jus civile esse voluerunt. Quod civile, non idem continuo gentium ; quod autem gentium, idem civile esse debet.-Ib. c. 17. Æquissima vox est, et jus gentium præ se ferens, Redde quod debes.'--Seneca de Ben. iii. 14.\nGentium jus est quod acceperis reddere,' in a passage attributed to Seneca, vol. v. p. 427 ; ed. Rukkopf.\n(16) Cicero, referring to the belief in the existence of the gods, says: • Omni autem in re consensio omnium gentium lex naturæ putanda est.' Tusc. Disp. i. 13. Also in the speech, De Harusp. Resp. c. 14: 'Quanquam hoc si minus civili jure perscriptum est, lege tamen naturæ, communi jure gentium, sancitum est,' &c. Commune jus gentium.'-Nepos, Themist. 7. (17) μηδε δίκην δικάσης πριν αμφοίν μύθον ακούσης,\nAp. Pseudo.-Plat. Demodoc. $ 4. Aristoph. Vesp. 725. Compare Param. Gr. vol. ii. p. 759; ed. Leutsch. In Latin, Audi alteram partem. The putting of persons to death without a trial (Kreivelv åkpitous) is always spoken of with severe reprobation by the Greek writers ; and as if, in condemning such a practice, they appealed to a recognised principle.-See, e.g., Dion. Hal. A. R. . 22.\n(18) • Omnes homines,' omnes gentes,' 'gentes humanæ.'- Gaius, Savigny, ib. p. 110. Thus Ovid said of himself\n• Mihi fama perennis\nQuæritur, in toto semper ut orbe canar- -(Amor. i. 15) yet he knew that there were parts of the world, inhabited by savage\nincomplete. In the first place, they were not acquainted with all nations; and, in the next, they did not inquire very narrowly if a certain law was really common to all the nations with which they were acquainted.\nAs soon as we travel beyond the limits of the nations partaking of the Greek and Roman civilization in antiquity, and those partaking of the civilization of Western Europe in modern times, we arrive at nations which adopt different practical principles in the fundamental departments of civil law, and reject those principles of jurisprudence which theoretical writers denominate universal. All nations above the savage state recognise a right of possession; but tribes in the nomad state do not recognise a right of property or ownership in land ;(19) and there\ntribes, where his name would be unheard. The whole world is a hy. perbole, as in the account of the temptation in St. Matthew, iv. 8—the high mountain from which all the kingdoms of the world' could be seen. • All Asia and the world' is said to worship Diana, in the speech of Demetrius the silversmith, Acts xix. 27. Gibbon (c. 15) cites a passage of Justin Martyr, which describes Christianity as having spread over the whole world in the first century. Speaking of the 'gentium consensus tacitus,' Pliny says, that its first object was the use of the Ionic letters, the second was the practice of shaving the beard, the third was the division of hours. -N. H. vii. 58-60. From the instances given, it is clear that he means to limit himself to the states enjoying the Hellenic civilization. By a natural hyperbole, the orbis Romanus has often been identified with the entire inhabited world. Thus Florus : Ita late per orbem terrarum arma cir. cumtulit, ut, qui res ejus legunt, non unius populi, sed generis humani facta discant (i. 1, § 2). Compare Gibbon: • Dazzled with the extensive sway, the irresistible strength, and the real or affected moderation of the emperors, they permitted themselves to despise, and sometimes to forget, the outlying countries which had been left in the enjoyment of a barbarous independence; and they gradually usurped the licence of founding the Roman monarchy with the globe of the earth.'— Decl. and Fall, c. i. ad fin., who refers to Bergier, Hist. des Grands Chemins de l'Empire Romain, liv. ii. c. 1. (19) See Horace, Carm. iii. 24:\nCampestres melius Scythæ\nQuorum plaustra vagas rite trahunt domos,\nImmetata quibus jugera liberas\nNec cultura placet longior annuâ. Speaking of the Suevi, Cæsar says: 'Privati ac separati agri apud eos nihil est ; neque longius anno remanere uno in loco, incolendi causâ, licet.'\n-B. G. iv. i. Arva per annos mutant, of the Germans.' Tacit. Germ. 26. On the nomad state of society, see Comte, Traité de Législation, 1. iv. c. 14; Gibbon, c. 26; Volney, Egypte et Syrie, tom. i. p. 351."", 'Every society has expectations about how its members should and should not behave. A norm is a guideline or an expectation for behavior. Each society makes up its own rules for behavior and decides when those rules have been violated and what to do about it. Norms change constantly.\nNorms differ widely among societies, and they can even differ from group to group within the same society.\nExample: The way we are expected to behave in church differs from the way we are expected to behave at a party, which also differs from the way we should behave in a classroom.\nExample: In some African countries, it’s acceptable for people in movie theaters to yell frequently and make loud comments about the film. In the United States, people are expected to sit quietly during a movie, and shouting would be unacceptable.\nExample: In the United States in the 1950s, a woman almost never asked a man out on a date, nor did she pay for the date. While some traditional norms for dating prevail, most women today feel comfortable asking men out on dates and paying for some or even all of the expenses.\nSociologists have separated norms into four categories: folkways, mores, laws, and taboos.\nA folkway is a norm for everyday behavior that people follow for the sake of convenience or tradition. People practice folkways simply because they have done things that way for a long time. Violating a folkway does not usually have serious consequences.\nExample: Holding the door open for a person right behind you is a folkway.\nA more (pronounced MORE-ay) is a norm based on morality, or definitions of right and wrong. Since mores have moral significance, people feel strongly about them, and violating a more usually results in disapproval.\nExample: Parents who believe in the more that only married people should live together will disapprove of their son living with his girlfriend. They may consider their son’s action a violation of the moral guidelines for behavior.\nA law is a norm that is written down and enforced by an official agency. Violating a law results in a specific punishment.\nExample: It is illegal in most countries to drive a car while drunk, and a person violating this law may get cited for driving under the influence (DUI), which may bring a fine, loss of driver’s license, or even jail time.\nA taboo is a norm that society holds so strongly that violating it results in extreme disgust. The violator is often considered unfit to live in that society.\nExample: In most countries, cannibalism and incest are considered taboo. In some Muslim cultures, eating pork is taboo because the pig is considered unclean.\nWhere there are rules, there are rule breakers. Sociologists call the violation of a norm deviance. The word deviant has taken on the negative connotation of someone who behaves in disgusting or immoral ways, but to sociologists, a deviant is anyone who doesn’t follow a norm, in either a good way or a bad way. See Chapter 6 for more about deviance.\nExample: Most people don’t graduate from college with a 4.0 grade point average, so sociologists view someone who does graduate with a 4.0 as deviant. Likewise, most Americans get married at some point in their lives, so someone who chooses not to marry is sociologically a deviant.\nAlthough deviance can be good and even admirable, few societies could tolerate the chaos that would result from every person doing whatever he or she pleased. Social control refers to the methods that societies devise to encourage people to observe norms. The most common method for maintaining social control is the use of sanctions, which are socially constructed expressions of approval or disapproval. Sanctions can be positive or negative, and the ways societies devise to positively or negatively sanction behaviors are limited only by the society’s imagination.\nA positive sanction rewards someone for following a norm and serves to encourage the continuance of a certain type of behavior.\nExample: A person who performs well at his or her job and is given a salary raise or a promotion is receiving a positive sanction. When parents reward a child with money for earning good grades, they are positively sanctioning that child’s behavior.\nA negative sanction is a way of communicating that a society, or some group in that society, does not approve of a particular behavior. The optimal effect of a negative sanction is to discourage the continuation of a certain type of behavior.\nNorms and Consequences\nExample: Imprisoning a criminal for breaking the law, cutting off a thief’s hands for stealing, and taking away a teenager’s television privileges for breaking curfew are all negative sanctions.\nConsequences for violation\nWearing a suit to an interview\nOnly married couples should live together\nConflicts with family members, disapproval\nLaws against public nudity\nImprisonment, monetary fine\nEating human flesh\nVisible signs of disgust, expulsion from society']"	['<urn:uuid:85b03647-fde3-4ee1-be9e-509fcdd58b67>', '<urn:uuid:9b735bcc-9d74-400f-9a3a-ac50a43a2e12>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T23:00:04.658849	35	145	2997
30	What causes withdrawal symptoms, and how can they be managed naturally?	Withdrawal symptoms manifest differently in the brain and body. Research shows they involve serotonin pathways, as evidenced by studies of the median raphe nucleus which influences reward and aversion behaviors. Physical withdrawal symptoms often include stomach issues like nausea, vomiting, and abdominal cramps. These can be managed through natural approaches including nutrition education to restore essential vitamins and minerals, Qigong therapy combining mental focus with breathing techniques, and mindfulness practices that help reduce withdrawal symptoms while significantly lowering stress levels.	['Research Press Release | December 28, 2022\nJoint press release by Hokkaido University and Kyoto University.\nNew insights into the opposing actions of serotonin-producing nerve fibres in mice could lead to drugs for treating addictions and major depression.\nScientists in Japan have identified a nerve pathway involved in the processing of rewarding and distressing stimuli and situations in mice.\nThe new pathway, originating in a bundle of brain stem nerve fibres called the median raphe nucleus, acts in opposition to a previously identified reward/aversion pathway that originates in the nearby dorsal raphe nucleus. The findings, published by scientists at Hokkaido University and Kyoto University with their colleagues in the journal Nature Communications, could have implications for developing drug treatments for various mental disorders, including addictions and major depression.\nPrevious studies had already revealed that activating serotonin-producing nerve fibres from the dorsal raphe nucleus in the brain stem of mice leads to the pleasurable feeling associated with reward. However, selective serotonin reuptake inhibitors (SSRIs), antidepressant drugs that increase serotonin levels in the brain, fail to exert clear feelings of reward and to treat the loss of ability to feel pleasure associated with depression. This suggests that there are other serotonin-producing nerve pathways in the brain associated with the feelings of reward and aversion.\nTo further study the reward and aversion nerve pathways of the brain, Hokkaido University neuropharmacologist Yu Ohmura and Kyoto University pharmacologist Kazuki Nagayasu, together with colleagues at several universities in Japan, focused their attention on the median raphe nucleus. This region has not received as much research attention as its brain stem neighbour, the dorsal raphe nucleus, even though it also is a source of serotonergic nerve fibres.\nThe scientists conducted a wide variety of tests to measure activity of serotonin neurons in mice, in response to stimulating and inhibiting the median raphe, by using fluorescent proteins that detect entry of calcium ions, a proxy of neuronal activation in a cell-type specific manner.\nThey found that, for example, pinching a mouse’s tail—an unpleasant stimulus—increased calcium-dependent fluorescence in the serotonin neurons of the median raphe. Giving mice a treat such as sugar, on the other hand, reduced median raphe serotonin fluorescence. Also, directly stimulating or inhibiting the median raphe nucleus, using a genetic technique involving light, led to aversive or reward-seeking behaviours, such as avoiding or wanting to stay in a chamber—depending on the type of stimulus applied.\nThe team also conducted tests to discover where the switched-on serotonergic nerve fibres of the median raphe were sending signals to and found an important connection with the brain stem’s interpenduncular nucleus. They also identified serotonin receptors within this nucleus that were involved in the aversive properties associated with median raphe serotonergic activity.\nFurther research is needed to fully elucidate this pathway and others related to rewarding and aversive feelings and behaviours. “These new insights could lead to a better understanding of the biological basis of mental disorders where aberrant processing of rewards and aversive information occur, such as in drug addiction and major depressive disorder,” says Ohmura.\nHiroyuki Kawai, Youcef Bouchekioua, et al. Median raphe serotonergic neurons projecting to the interpeduncular nucleus control preference and aversion. Nature Communications. December 22, 2022.\nThis work was supported by the Japan Society for the Promotion of Science (JSPS) through Grants-in-Aid for Scientific Research from (JP20H04774, JP20K07064, JP18H04616, JP20H00491, 21K07473, 21H02668, JP22K11498) and Grants-in-Aid for JSPS Fellows (JP20J12341, JP21J14215, JP21J21091); a Grant-in-Aid for Nagai Memorial Research Scholarship from the Pharmaceutical Society of Japan (N-184403); the Japan Agency for Medical Research and Development (AMED) (JP20ak0101088h0003, JP21ak0101153h0001, JP21wm0525026, JP20lm0203007); the Smoking Research Foundation; The Shimizu Foundation for Immunology and Neuroscience Grant; The Uehara Memorial Foundation; The Lotte Foundation; Takeda Science Foundation; and the SENSHIN Medical Research Foundation.\nDr. Yu Ohmura\nDepartment of Neuropharmacology\nFaculty of Medicine and Graduate School of Medicine\nDr. Kazuki Nagayasu\nDepartment of Molecular Pharmacology\nGraduate School of Pharmaceutical Sciences\nSohail Keegan Pinto (International Public Relations Specialist)\nPublic Relations Division\nDavid Hajime Kornhauser\nDirector, Global Communications', 'If you or a loved one is struggling with addiction, you may be looking for effective treatment strategies for a complete and lasting recovery. Here we check out some alternative addiction treatment methods in 2021.\nAddiction is a severe public health concern characterized by an intense, uncontrollable desire for a particular substance or drug even when such use can adversely affect the individual. Addiction affects not just the individual’s physical and mental health, but it also impacts social health and life as a whole.\nAddiction treatment aims to limit or stop the dependence on this substance and guide the person towards full recovery and rehabilitation. Using proven treatment options gives a more significant opportunity to improve the individual’s health and ensure a drug-free lifestyle.\nAccording to the Substance Abuse and Mental Health Services Administration (SAMHSA), 41.1 million Americans require addiction treatment in 2020. Of these numbers, around 2.7 million individuals receive pharmacotherapy, behavioral therapy, and a combination of medications and behavioral therapy. Typically, these patients receive addiction treatment at a specialty facility.\nUnfortunately, addiction is a chronic condition that can go on a cycle of remission, recovery, and relapse. Due to the complexity of this health condition, there is no one-size-fits-all approach when it comes to addiction treatment. Hence, it is not uncommon for people struggling with their drug-seeking behaviors to seek alternative treatment strategies to improve their mental health and ensure long-term recovery.\nHere are some alternative addiction treatment programs and therapies you can explore.\nHypnosis is an alternative addiction treatment that promotes relaxation and reduces cravings. A trained hypnotherapist assists you during recovery. Hypnosis is often used as an adjunctive therapy and instills the belief that you can overcome the addiction behaviors.\nMost people who struggle with addiction feel hopeless, and they think that they cannot get out of their situation. Mindfulness teaches these patients a new perspective on their condition. It aims to reduce cravings and prevent withdrawal symptoms. When mindfulness is instilled into the individual’s life, it significantly reduces stress and improves the chances of full recovery.\nThis alternative therapy is associated with mindfulness. Qigong combines several therapeutic approaches, such as mental focus, breathing techniques, and choreographed postures. With Qigong, mindfulness is coupled with slow, controlled movements. The participants also engage in guided imagery and positive visualization that restores health.\nDerived from a Western African shrub called iboga, ibogaine is a psychedelic compound used to prevent cravings and withdrawal symptoms. Anecdotal evidence points to the benefits of ibogaine treatment for drug cessation programs, and the compound adjusts brain chemistry to prevent intense withdrawal symptoms. However, ibogaine hasn’t been approved by the US Federal Drug Administration (FDA) and is classified as a Schedule I drug in the US. Individuals who want to try this alternative treatment method go abroad in jurisdictions where ibogaine treatment centers legally operate, like Mexico.\nNutritional imbalance is one of the significant adverse effects of addiction. Nutrition education aims to restore essential minerals and vitamins of the body, and it teaches the patient the importance of a healthy diet in the fight against substance abuse. The early stages of the withdrawal process can be accompanied by stomach symptoms, such as nausea, vomiting, upset stomach, abdominal cramps, and diarrhea. Correcting these nutritional deficiencies can help the patient recover faster and avoid potential relapse.\nProlonged dependence on addictive substances can result in anxiety, anger, stress, and tension. In turn, these psychological symptoms can produce physiological changes. Biofeedback, particularly neurofeedback, helps participants gain control over their brainwaves.\nDuring a biofeedback session, an electroencephalogram (EEG) machine monitors the changes in the patient’s brain waves. The patient is asked to visualize different scenarios, which help the therapist understand various reactions. Neurofeedback therapy is thought to prevent drug relapse.\nJust like conventional addiction treatment programs, alternative addiction treatment methods are not a one-size-fits-all strategy. It may have worked for some patients but does not guarantee it will work for everyone. Hence, it is essential to understand every therapy. Some people try more than one therapy to determine what treatment works best in their case. Better yet, when used alongside conventional treatments, patients can experience long-term success.\nOverall, these addiction programs are aimed to restore physical well-being, build life coping mechanisms and life skills, and give hope to people struggling with this condition.']	['<urn:uuid:2586a492-2f11-4bcb-9bdd-42b8385f3544>', '<urn:uuid:b3fbdb16-6014-49a3-93d6-876dd1fd4921>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T23:00:04.658849	11	80	1376
31	cooling tower valves pipes regular maintenance vs dam operation valves pipes regular maintenance	Both cooling towers and dams require regular maintenance of their valves and pipes, but dams are more critical and hands-on. Cooling towers need regular maintenance for efficiency and effectiveness, while dams require continuous upkeep to prevent failure - without maintenance, dam inner chambers can begin to flood within 3-5 years, leading to spillway failures and potential catastrophic water releases.	"['I didn\'t know the answer to this, but knew it was important, so I researched it. I\'m not an expert, so I\'ll give you quotes from some who know more about the subject.\nThe general impression I get is that dams will more or less remain in the flow position they are in when the dam is abandoned. If the electronic controls go out, there are manual controls, but they take a really long time to adjust. If the waters rise to the point where the dam will overflow, there are usually, but not always overflow spillways, which will allow the excess water to go over, through, or around in a controlled manner to do little damage. The concrete structures themselves are very very strong, however dams need continual upkeep and maintenance. One source says that if left alone the inner chambers will begin to flood in a few years and that will result in the dam failing not too long after.\nSo, I don\'t know about you, but I\'m just gonna avoid dams and being downstream of them in a post-collapse scenario just to be safe.\nHere are some quotes:\n""It all depends if the gates automatically close or open when electricity fails.\nOverflow or flood gates that aren\'t able to open might create a problem of excessive stress on the dams.""\n""Actually there was something about this on Discovery. That statistics on dams in the US as far as deficiencies is amazing. For instance, the number of High Hazard Dams, those whose failure would result in significant loss of life, in need of repair was 364 in 2001. That number rose the 1,743 a mere six years later in 2007. Now this is a relatively low number considering Oregon has about 84,000 dams alone, 225ish are ranked ""high hazard"" but, when you see a 500% increase in dams that have a high probability of failure and causing a significant loss of life in a mere 6 years that\'s a little unnerving.""\n""According my friend who worked the dams as an engineer for most of his adult life .\nIn 3-5 years the dams would flood internally and the results would be a failure of the spillways causing a huge release of water that would casade down the river taking out lower dams after 2-3 go the whole mess below would end up failing. He says a couple bad winters could even speed that up.\nHe says the dams are very hands on and if left on their own for as much as a winter they would be on the path to failure\nmight not be a worry in the first couple years but soon.""\n""Without continuous maintenance, they will eventually fail. The upkeep is critical. There is one on the Cumberland River in KY that is leaking around the edges NOW and they are working on a fix for it but without that work that dam will fail.""\n(This next gentleman works at a large dam in Washington and is very knowledgeable. It gets technical though.)\n""This is an interesting question. I\'m writing this from the control room of Ross Powerhouse were I began my career on Monday April 29, 1974. I\'ve been in Hydro Operation for a while. The generating plants that were here in the 1960\'s and some in the early 70\'s would be recoverable after an EMP. Today\'s plants would be toast. For decades now, I\'ve watched bad Engineering decisons wreck the reliability of generating plants. I\'ve seen processor driven automated systems replace manual controls, the manual controls removed, then the only Engineer that understood the new system at all leave the utility to go to another state.\nGenerally, gates do not open automatically. And, given that emergency generators at the dams have electronic controls these days, getting power to open them would be a challenge after an EMP. It\'s theoretically possible to crank open the two 72"" butterfly spill valves in the center of Ross Dam and the three 78"" butterfly valves in the center of Diablo Dam, using hand wheels. But it would take a good crew a full day of constant cranking for some of them. For the spillgates you need power. For 16 of the 19 gates at Diablo you need to push a 5,300 lb. ""mule"" onto the dam, hook it up to power and use its hydraulic grapples to lift the gates. THe two 47 by 500 ft. gates at Gorge need power for a 15hp three phase motor, and even its lower outlets are gates that need power.\nMost concrete masonry dams are well constructed and can withstand a certain amount of overtopping, but it won\'t do them any good or make recovery any easier.\nAs to the generating plants, if we have battery power or other stored energy (sometimes a pressure accumulator) to close the penstock valves, or opportunity to put gate locks in place, we can keep the generators from spinning up and damaging themselves as all power and pressure on the gate servos is lost. If the plant has a functioning water powered sump eductor and we turn it on, we should be able to keep the powerhouse from flooding as long as there is water in the power tunnel.\nThe dams themselves are built to last. Concrete is weird stuff, it never really stops curing, only slows down. At 50-80 years, good concrete is very good, indeed. At 150 years, it may actually be getting a bit brittle.\nI don\'t see these plants as recoverable from EMP in less than... well years, if major equipment is damaged and more has to be ordered, built and transported. Working around delicate electronics to get something running could be done in weeks or months, but that requires some materials, too. And it requires a full time operating staff that no longer exists. The solid state exciters common on modern generators would not fare well in an EMP event. No excitation means that big alternator does not make much power.\nWith the generators down for the count, our biggest immediate challenge would be configuring the dams to not be overtopped and perhaps to function as some degree of flood control. Example: the water behind Ross is 82 feet below full right now, because the storage reservoir cycles seasonally. If we could get enough lower outlets opened, leaving space for the lakes to fill before getting close to the tops of the dams, we could still use the dams to even out some of the peaks during flood season. But as far as power, don\'t hold your breath.\nFederal law requires us to have an ""Emergency Action Plan"" for the event of a dam breaking or threatening to break. It does not require us to have a plan that would work in an earthquake. We are not required to plan for an EMP event. If we have a solar event like the one in 1859, I\'d say our society is back to about the year 1900.\nI am my utility\'s most senior Hydroelectric Operator. I collect kerosene lamps. Think about it.\nIn most of our critical infrastructure, not just the electrical industry, we see the skilled blue collar people who keep it working gettting older and older and finally retiring. Very few youngsters are coming up through the ranks. Recently, it gave me pleasure to see two bright young Apprentices ""top out"" and become Journeyman Electrician Constructors. But this was in a time and place where it should have been a dozen, not two. Physical assets are not being adequately maintained. ""Managment"" is more and more the political type who will postpone maintenance and upgrades to cut costs. The term ""crumbling infrastructure"" is accurate. But perhaps equally important is the loss of institutional knowledge and experience as the most experienced people are leaving, or in some cases being driven out, without adequately trained and experienced replacements coming aboard.\nWe live in a nation of MBA\'s who cannot wire a house, fall a tree, or even change a tire or drive a nail. Brace yourselves.""\nIn recent years, much as been done in the way of recalculating Possible Maximum Floods and finding methods to ""harden"" dams in the event of overtopping. But they weren\'t really designed for the water to just flow over them. Oh sure, you can design for ""surcharge."" The full elevation of Gorge Dam is 875 ft. above sea level; that\'s the top of the gates. But those gates are designed for 5 feet of surcharge. That is, you can let the lake ""pile up"" as high as 880 ft (but not more, because the roadway on the dam is at 880). Water flows over the top of the gates, but it\'s not a huge flow compared to opening the gates because it\'s not under much pressure.\nBTW, if you\'re planning a micro hydro, it maybe be useful to you to know that with fresh water you get 0.434 psi for every foot of head. Take water from 100 feet above your site, and you have 43.4 psi at your site - until you start it flowing and have to deal with pressure losses due to friction in your supply line.\nI really don\'t want to think about the headaches we\'ll face at our dams and powerhouses if we get hit with a violent EMP event. The solar event of 1859 had telegraphers thinking their equipment was possessed, and where they had instruments with rolls of paper and a pen to record dots and dashes it actually set the paper on fire. I can\'t even imagine the damage such an event would do to today\'s society.\n(another said that you would think spillways would always being built into the top of the damn) ""You\'d think an open spillway could be built with its sill at ""full"" elevation, ""just in case."" But not really. The flows in a Possible Maximum Flood would be very large, the dam\'s gates are designed with that as the maximum they can pass with all gates open. But that is with water level well above the sills of the spillways. A simple open spillway at ""full"" level would have to be too large and would still ""pile up"" water well above normal full level, and you can\'t have that. Instead, along much of the top of the dam you have a bunch of gates that can be opened when you see the flood building and realize you won\'t have room in your reservoir to hold it.\nOver the years, they\'ve recaculated PMF for a number of dams, coming up with high figures, and worked on modifying the dams to survive the new PMF. I\'ve seen a number of ideas. Sometimes it\'s hardening the face of an earthen dam with roller compacted concrete. Sometimes it\'s modifying a concrete dam with a lot of the crest being ""one time use"" emergency breakaway gates that can be cut released in an emergency and allow the dam to spill a lot more water - but not refill its reservoir until the emergency gates are replaced.\nSometimes modifications to dams to hold back a but more water are poorly thought out. Imagine adding splashboards so that dam could hold back a few more feet of water, but also making its new height at bit more than the height of the bank near the intake for your plant - and placing your emergency generator to power spillgates in an emergency, out on the dam with vulnerable remote control cabling located so it can be easily damaged by flooding - by which time people can no longer access the dam at all. Look into the Upriver Spokane Dam washout of 1986 sometime.\nLook into the much more recent and totally preventable Taum Sauk disaster the overtopped the reservoir of a large pumped storage plant.\nMore and more, I\'m coming to agree with Murphy about things going wrong, and to believe if there\'s a way for people to screw it up, sometime, somewhere, they will.\nThe Chernobyl disaster was because a new know-nothing Junior Operator came up a a ""better way"" to do a shutdown from full power. All experienced hands said he was nuts. But he was the son of the installation\'s Political Officer, who is the most powerful person there, more so than the site Superintendent. So they tried it his way - once...""\nBack on the original topic: There are dams that have spillways designed to simply start flowing at a certain water level. But lots of dams are NOT designed that way. More typical is something like our dams, where you have spillgates and often smaller outlets located lower than the gates, but they need to be operated. And that dam is not designed to handle overtopping without damage.\nYou would think. But they aren\'t designed that way. ""Full"" will be the top of the gates. The reservoir is seldom run more than a few feet lower than that, usually not down near the gate sills - a typical taintor gate might be 20 feet tall (they can be much larger). The exception is a seasonal storage reservoir where the lake might drop many feet in winter, or maybe a pumped storage reservoir where it cycles daily but high flow flooding isn\'t an issue. The road on top of the dam is just a few feet above that ""full"" level. ""\nSure, an open channel emergency spillway is a good idea. But lots of dams are not built that way. If designing one that way, you\'d calculate how high the water in the lake would get with Possible Maximum Flow through the emergency spillway for a number of hours, and you\'d have to design your dam to handle water to that level - but in normal operation you\'d never run with water level above the sill of the opening to the emergency spillway.', ""Along with pressure comes the capacity of the system.Consider the following three options when examining the pressure level and relative capacity of the cooling system Small-diameter pipe (0.5 to 2) globe control valve.Mid-diameter pipe (3 to 6) segmented ball valve. results for this questionWhat are the different types of cooling pipes?What are the different types of cooling pipes?Consider the following three options when examining the pressure level and relative capacity of the cooling system Small-diameter pipe (0.5 to 2) globe control valve Large-diameter pipe (8 and above) high performance butterfly valve (HPBV) Small-diameter pipe two- or three-piece ball valve Mid-diameter pipe three-piece or flanged ball valveBest Practices Pipe and Valve Selection for a Cooling results for this questionWhat is a cooling system?What is a cooling system?Regarding cooling systems,the valves that are selected typically are going to be flow control valves and isolation (shutoff and bypass) valves.There are important questions to consider before picking the valve for the cooling system.Best Practices Pipe and Valve Selection for a Cooling\nresults for this questionWhat to consider before picking a cooling valve?What to consider before picking a cooling valve?There are important questions to consider before picking the valve for the cooling system.The first thing to consider is the flow capacity.When considering the size required by the cooling application,it is easy to narrow down the valve that should be chosen.An important thing to remember in valve selection is the pipe size.Best Practices Pipe and Valve Selection for a Cooling 12345NextEnclosure Cooling Selection - AutomationDirect\nValve Evaporator Compressor Condenser Enclosure Cooling Selecting a Fan or Cooling Device Fan selection To select the proper size (CFM) fan for your forced air cooling solution,you need to determine the amount of heat to be removed (in watts) and determine the Delta T (Max.allowable internal enclosure temperature °F Max.outside ambientAuthor Ray HerreraValves Improve Process Cooling 2015-08-11 Process CoolingAug 11,2015·The wine cooling process itself is fairly straightforward,but it may highlight how valves can function as surprisingly important ingredients in improving a variety of cooling applications.Consider a typical wine cooler process and learn about suggested tradeoffs and advantages involved in optimum valve selection.\nThis guidebook covers the best practices in piping systems with a primary view of reducing energy cost,keeping in mind the safety and reliability issues.The basic elements of best practice in piping systems are 1.Analysis optimum pipe size selection for water,compressed air and steam distribution systems 2.Good piping practices 3.Best Practices Engineering GuideBest Practices Engineering Guide Installation Recommendations for FCI Single-Point, and selection of thermal dispersion flow meters simple and effective. Recommendation for Flow Controlling Valves Upstream or Downstream Pipe Size Upstream Downstream < 6 inches [150 mm] 20Best Practices Pipe and Valve Selection for a Cooling Apr 01,2017·Best Practices Pipe and Valve Selection for a Cooling System Valves,pipes and controls throughout an industrial process cooling system are all critical to its overall efficiency and effectiveness.Within the industrial processes that require a cooling system,where a significant amount of cooling is needed,cooling towers often are used.\nBest Practices Pipe and Valve Selection for a Cooling System Valves,pipes and controlsthroughout an industrial process cooling system are all critical to its overall efficiency and effectiveness.Within the industrial processes that require a cooling system,where a significant amount of cooling is needed,cooling towers often are used.Best Practices Pipe and Valve Selection for a Cooling\nWas this helpful?People also askShould pipes and valves be included in a cooling system?Should pipes and valves be included in a cooling system?When selecting the pipes and valves to be included in a cooling system,it is important to understand the options available and the possible outcomes associated with each selection.The selection process is often overlooked or,at least,not given proper attention by designers and contractors when putting together a cooling system.Best Practices Pipe and Valve Selection for a Cooling Best Practices for Strainer Location Pumps SystemsSystem designers frequently place strainers on the suction side of a pump.This practice,ostensibly based on good intentions,is rarely a good idea and will create serious issues for the pump from the aspect of reduced and turbulent flow,inadequate net positive suction head available (NPSHa) and eventually blocked flow.The consequential reduction in NPSHa will create deleterious effects for Choosing The Right Seat Material is the - Flo-Tite ValvesValve Seat Seal Selection Guide Choosing The Right Seat Material is the Most Challenging Decision in Ball Valve Selection.VALVE REPAIR KITS INCLUDE ALL SOFT PARTS Seats - Body Seals - Stem Packing - Thrust Washer Flanged End 3 Piece C Valves Valve Soft Parts Flo-Tite ball valves are available with a variety of seat seal options to allow\ncooling capacity.All after-coolers need a separator with auto drains with a check valve.Air Receivers Air receivers contribute greatly to any system by reducing pulsations,separating entrained moisture due to velocity change and improve response times by acting as a reservoir for sudden air demands.It is important that compressedCooling Tower - an overview ScienceDirect TopicsStephen Hall,in Branan's Rules of Thumb for Chemical Engineers (Fifth Edition),2012.Mechanical Draft Cooling Towers.Most packaged cooling towers use fans to force or draw air through the tower.Countercurrent-induced draft towers are the most prevalent in the process industries,capable of cooling water within just over 1 °C (2 °F) of the wet-bulb air temperature.Cooling Tower Basics Piping and Controls 2016-03-01 Mar 01,2016·For most industrial manufacturing,electric power generation and even air-conditioning needs,cooling towers are a critical element of the design.Cooling towers work on the principle of heat rejection They extract heat to the atmosphere through evaporative cooling.The towers themselves are enclosed,steady-flow devices for cooling water by evaporation through direct contact with air.\nof static heads for pump selection because of a bal-ance or cancellation of static heads between the supply and return risers.Static head lost by water flow to any height in the supply piping is cancelled by a static head regain as water flows down the return piping.TheCooling water pump KSB - Pumps,Valves and Service KSBCooling water pumps are used for supplying heat exchangers with cooling water.Their flow rate varies depending on the heat flow to be dissipated.The required head is determined by the type of cooling system.A distinction is made between wet cooling and dry cooling processes.Range of head and flow rate The required heads for fresh water operation usually lie within 5 to 15 m.In cooling Coolselector®2 free cooling calculation software DanfossFeb 24,2020·We did complex you do awesome.Coolselector®2 helps you optimize energy consumption and increase efficiency in any HVACR system.Run unbiased calculations based on a set of operating conditions such as cooling capacity,refrigerant,evaporation,and condensation temperature and then select the best components for your design.\nChoosing the right cylinder heads for an engine build can make all the difference in an engine that delivers and one that falls short of its potential.Horsepower,torque and throttle response all depend on how well the cylinder heads,camshaft and induction system work together.Choose the right combination and youll build a winner.DeltaPValve System Design Manual - Flow ControlControl Valves,Not the Coil,Set the Delta T.Delta T is the temperature difference between supply and return water across a cooling or heating coil.The only point where the coil sets the water delta T is at design conditions at all other loads the control valve sets the delta T.Design Chilled Water Flow Rate and Cooling Load -I was using the water flow rate of 24.48usgpm (based on Total Cooling Load of 10.2 x 2.4,and 10degF delta T) to do the pipe sizing and valves selection.My friend commented that because of low SHF,I should have using the water flow rate of 16.08usgpm (based on Sensible Cooling Load of 6.7 x 2.4) otherwise my system will be oversized.\nOct 29,2017·When it comes to radiators the answer will depend on your situation,but for the most part,the answer is usually yes.However,there are some instances whereEngineering Specification for the Selection,Installation Engineering Specification for the Selection,Installation and Maintenance of Pipeline Strainers Prepared by a blowdown valve and some modification of the straining element of a basket strainer,the marine oil burners,lubricating lines on ships,cooling towers,continuously running chemical operations,and many industrial water intake Fail-Safe Position Selection of Control ValveDec 30,2016·In designing a processing facility,we usually use control valve to manipulates the temperature,pressure,level,or fluid flow rate in process system.The selection of control valve,which is air-to-open or air-to-close,is base on safety consideration.In case of power loss or air failure ,control valve should move in safe position (fail-safe).\nOct 14,2019·An argument could be made that the most important element to implementing an effective heat-trace system is attention to detail during the installation.There are a multitude of factors at play when installing heat trace,and each must be carefully considered.Too often,heat trace is installed improperly or in an inconsistent manner,resulting in a system that is more prone to failure.Managing Pressure Regulator - Air Best PracticesPressure regulators are everywhere compressed air is used.These simple devices,essential for safe and steady equipment operation,can be a big waster of compressed air.This article shows how with proper regulator selection,installation and setting managementPrevious123456Next\nJun 11,2016·Valve design and selection of materials of construction must account for these types of media characteristics.Temperature of the flow media and the operating environment is another factor to be considered.Corrosion will occur at in increased rate at higher temperatures.In addition,temperature can cause different materials to expand and Process Piping Fundamentals,Codes and Standardsused materials,criterial for material selection,associations involved in generating piping codes,design factors depending on fluid type,pressure,temperature and corrosion,roles and valves,and specialty components.All piping systems are engineered to transport a fluid or gas safely and reliably from one piece of equipment to another.Resources Articles ValinBest Practices Pipe and Valve Selection for a Cooling System.Submitted by Ray Herrera |Valin Corporation on Mon,04/03/2017.When selecting the pipes and valves to be included in a cooling system,it is important to understand the options available and the possible outcomes associated with each selection.The selection process is often\nTop ten worst practices cooling towers The poor design and installation of pipe work associated with cooling towers continues to be a concern across the industry.Despite best practice guidelines available,it remains common to find examples of poorly designed and executed piping,which create a variety of hazards.Figure 2.Section Vb1 Valves - Johnson ControlsThree Way Valves 23 Valve Selection Summary 25 Maximum Temperature and Pressure Ratings 29 Glossary 30 Section Vb1 Valves.2 Vb1 Engineering Data Book. total pump head will appear across the valve when it is closed,the best way to keep the relative change as small as possible is to size the valveSteam Piping Best Practices CleanBoilerFailed Valves.This valve is obviously stuck partly open.However,often valves are installed with pipe on the down-stream side.As in the picture,steam still blows past the valve,but since its inside of pipe,its not obvious.Valves fail due to wear,corrosion and dirt/contaminates stuck in the valve seat.\nFeb 24,2021·Titanium valves will work with stock valve guides and seats,but for the best results they should be used with steel beryllium seats (to improve heat transfer and cooling) and manganese or silicone bronze valve guides.Performance Coatings.Valves often have stem and/or head coatings to enhance performance.Valve Selection Hot Valve Materials for Hot Engines Feb 24,2021·Titanium valves will work with stock valve guides and seats,but for the best results they should be used with steel beryllium seats (to improve heat transfer and cooling) and manganese or silicone bronze valve guides.Performance Coatings.Valves often have stem and/or head coatings to enhance performance.Valves Selection Guide - Engineering ToolBoxAn applications guide for selecting valves.Related Topics .Piping Systems - Dimensions of pipes and tubes,materials and capacities,pressure drop calculations and charts,insulation and heat loss diagrams; Control Valves - Control Valve terminology,bodies,trim,flow characteristics,Cv and Kv sizing,noise,actuators and positioners; Valve Standards - International standards for valves in\nValve selection can make the difference in boiler and chiller efficiency; in maintaining flow regardless of pressure fluctuation and in achieving the right comfort level throughout the building.Discover the difference with Siemens in maximizing comfort to make your perfect place a reality.""]"	['<urn:uuid:e3b1976e-2e0a-4373-a719-c2c5bb33a442>', '<urn:uuid:a9014618-d5b9-4e39-ac86-14505b0d524a>']	factoid	direct	long-search-query	similar-to-document	comparison	novice	2025-05-12T23:00:04.658849	13	59	4267
32	What defines Dominica geographically and how does it face natural disasters?	Dominica is a volcanic island in the Windward Islands of the Caribbean, historically a stronghold of the Carib Indians with its capital in Roseau. As part of the Caribbean archipelago, it faces severe hurricane threats, as demonstrated in 2017 when Hurricane Maria hit the island with winds exceeding 160-miles-per-hour, tropical force winds extending 230 miles in diameter, and heavy thunderstorms spanning 40 miles from the hurricane's eye, which was approximately 9 miles wide.	"['Definition of Dominica\n1. Noun. A country on the island of Dominica.\nGroup relationships: Oas, Organization Of American States\nGeneric synonyms: Country, Land, State\nGroup relationships: Caribbean\nTerms within: Roseau\n2. Noun. A volcanic island in the Windward Islands that was once a stronghold of the Carib Indians.\nTerms within: Commonwealth Of Dominica\nGeneric synonyms: Island\nDefinition of Dominica\n1. Proper noun. A country in the Caribbean. Official name: Commonwealth of Dominica. ¹\n¹ Source: wiktionary.com\nMedical Definition of Dominica\n1. An island republic of the west indies. Its capital is roseau. It was discovered in 1493 by columbus and held at different times by the french and the british in the 18th century. A member of the west indies federation, it achieved internal self-government in 1967 but became independent in 1978. It was named by columbus who discovered it on sunday, domingo in spanish, from the latin dominica dies, the lord\'s day. (12 Dec 1998)\nClick the following link to bring up a new window with an automated collection of images related to the term: Dominica Images\nLexicographical Neighbors of Dominica\nLiterary usage of Dominica\nBelow you will find example usage of this term as found in modern and/or classical literature:\n1. A Dictionary of Christian Antiquities: Being a Continuation of the by Samuel Cheetham (1880)\n""dominica prima post Natale Domini. dominica prima, etc. poet ... dominica iv» in Quadragesima (<>r the Sunday of the Blin.i Man). ...""\n2. A Dictionary of Christian Antiquities: Comprising the History, Institutions by William Smith, Samuel Cheetham (1880)\n""dominica prima post Natale Domini. dominica prima, etc. post ... dominica in Quadragesima. dominica prima mensis primi. dominica iiia, iva, v*, ...""\n3. Guide to the Materials for American History, to 1783 by Charles McLean Andrews (1912)\n""Report of the Board of Trade on the king\'s title to dominica, with illustrative ... Including printed description of dominica with lists of freehold lands, ...""\n4. History of the Late War Between the United States and Great Britain by Henry Marie Brackenridge (1844)\n""Both vessels continued to manoeuvre for two or three hours; the dominica ... The dominica not being able to disengage herself, dropped alongside, ...""\n5. The American Naturalist by American Society of Naturalists, Essex Institute (1880)\n""THE ISLAND OF dominica. BY FM ENDLICH. dominica has always been \\ .iled in a halo of mystery. Vague rumors of "" smoking mountains,"" of springs flowing ...""\n6. Reports of Cases Argued and Determined in the Court of King\'s Bench: With by Great Britain Court of King\'s Bench, George Mifflin Wharton (1845)\n""Where a ship was chartered on a voyage from London to dominica and back to London, at a certain rate of freight upon the outward cargo ; and after ...""\n7. Sea Power in Its Relations to the War of 1812 by Alfred Thayer Mahan (1905)\n""One of the most noted of these encounters, that of the schooner "" Decatur,"" of Charleston, with the man-of-war schooner "" dominica,"" can hardly be claimed ...""\n8. The Auk: Quarterly Journal of Ornithology by American Ornithologists\' Union, Nuttall Ornithological Club (1891)\n""Alpheus H. and George E. Verrill, who collected in dominica for several months during 1890. Among the rarities of the collection are two specimens of a ...""', 'Scene of devastation in Roseau, September 19, 2017 (Credit: Unknown)\nOriginally published as a Facebook note on September 25, 2017\nAs we contemplate the future post-HURRICANE MARIA, if we are to step into the future with confidence, we should learn the LESSONS from the past. In some respects, traumatic experiences such as weathering a natural disaster on the scale of which Dominican nationals and residents have experienced can leave you feeling hopeless and uncertain of the future few of my insights into how we can benefit and learn from this welcomed event that has turned the lives of all upside down, and caused great emotional pain to loved ones, at home and abroad. Hurricane Maria was the 13th named hurricane in the Atlantic tropical and subtropical region for the 2017 season. (Old Farmer’s Almanac. 2017 ) The hurricane season runs roughly from June 1st to November 30 each year (En.wikipedia.org 2017). Beginning early Monday evening, of September 18th , Dominica, also known as The Nature Isle, endured the brutal onslaught of Hurricane Maria, that carried winds in excess of 160-miles-per-hour, with higher gusts. Tropical force winds extended 230 miles from the diameter, in every direction. Moreover, a spiral band of ‘heavy thunderstorms’ with a diameter of 40 miles extended from the hurricane’s eye. The eye was estimated to be about 9 miles in diameter. (Weather Underground. 2017)Because the storm made landfall at night, and did not leave the island until later the following day, one one could say that the storm affected the island over two calendar days.\n“ I call upon those with substantial military capacities to lend us the rescue and rebuilding equipment that may be standing idle waiting for a war; Let Dominica today be that war. ….because currently, our landscape reflects a zone of war.”\nPrime Minister Dr. Roosevelt Skerrit\nLESSONS LEARNED FROM HURRICANE DAVID\nIn 1979 when Hurricane David devastated the island, I was only 16 years old. When the tempestuous storm passed and I emerged from what was left of our home on Kennedy Avenue, Roseau, my first thoughts were that Dominica would never be the same again. I won’t go into the graphic details of what a category 5 hurricane can do. Because, YOU are veteran. a hurricane survivor. One who lived to tell the story! 38 years later, after getting over that overwhelming shock, and sorrow of hearing the plight of our relatives and loved ones back home. It suddenly dawned on me how the days following the disaster, turned into weeks. Weeks turned in to months. Months into years and finally decades. Until the mountains were dressed in their resplendent finery, the tropical environment was tidy, and the memory of that experience truly became history. All that remained to remind us of David was a school bus lying crushed under a huge African Bao Boa (Adansonia digitata) tree in the Botanic Gardens.\nIncidentally, when David struck in 1979, Dominica was barely one year as an independent nation. The country severed ties with the United Kingdom to become the world’s newest nation on November 3rd, 1978 under the premiership of Patrick John. However, political unrest, led to mass demonstrations and the eventual ousting of the John government in a popular uprising on May 29th that year. (Biography.jrank.org. 2017) Oliver Seraphin , who was a member of the Committee for National Salvation, was ‘invited to lead an interim government to prepare the way for full elections in 6 months. (The Commonwealth 2017) Then hurricane David struck! Despite the hurricane, and all the devastation it caused, Dominicans were exercising their democratic right, and overwhelmingly voted The Freedom Party, led by Dominica’s first female Prime Minister,Dame Mary Eugenia Charles on July 1980, less than one year after the hurricane. (Biography.jrank.org. 2017) In 2003, while I was a reporter with the Chronicle, Seraphin would tell me that I was the only journalist that had interviewed him about his experience of the passage of Hurricane David and the aftermath under his watch.\nThe 2017 Atlantic hurricane season has been particularly brutal this year. There are 25 nations in the Caribbean archipelago (TravelRepublic Blog. 2016) Of which nearly half of them have been devastated by hurricanes Irma and Maria. Others fared better suffering only mild damage by inclement weather this hurricane season. The cost of damages to infrastructure and livelihoods, of all islands north of Dominica is huge. Experts estimate that the cost of destruction by Hurricane Irma by itself, is a staggering £7.6billion (US$10 billion) (BBC News. 2017) Prime Minister Skerrit, Went to the United Nations last Friday. There he presented a moving and eye-opening speech at the 72nd Session of General Assembly highlighting the avoidable plight of victims of severe hurricanes attributed to climate change on developing nations like ours. “ I come to you straight from the front line of the war on climate change, “ he charged in his opening salvo. The Prime Minister used the analogy throughout his speech of Dominica being a battered and war-weary nation. Determined to fight against the enemy. He said, “ With physical and emotional difficulty I have left my bleeding nation to be with you here today because these are the moments for which the United Nations exists! “Dr Skerrit took the opportunity to offer his condolences on behalf of the Dominican people to sister islanders who had suffered recent ravages by hurricane Irma less than two weeks prior to Dominica,” To our brothers and sisters in Antigua and Barbuda, Anguilla, Cuba, St. Maarten, St. Thomas, the British Virgin Islands, the Turks and Caicos islands, Puerto Rico and the Dominican Republic and also to those who have suffered losses in our host country, the United States – in Texas and Florida,” He remarked.More importantly, he pointed out that these natural disasters were the result of global warming that is causing the seas and oceans to warm. “Mr. President, he warned, “ to deny climate change is to procrastinate while the earth sinks; it is to deny a truth we have just lived!” he said, adding, “ I call upon those with substantial military capacities to lend us the rescue and rebuilding equipment that may be standing idle waiting for a war; Let Dominica today be that war. ….because currently, our landscape reflects a zone of war. (“Facebook.com. 2017)\nWHERE DO WE GO FROM HERE?\nThis morning in a 9am Security Update for Dominica press briefing at the Press Room at the Office of the Prime Minister, Dr Skerrit urged his countrymen for patience and understanding as the government puts in place the necessary mechanisms and procedures to bring much-needed humanitarian relief to the people. He concluded his message with the hopeful clarion call to his audience.“ I just want all of us to understand to be united in purpose, “ he urged, adding, “Do what you can in the circumstances, and the authorities will get to you at the appropriate time. So when we get to you you will already be mobilised doing things for yourself and the rest of us.”\nOld Farmer’s Almanac. (2017). List of Names for the 2017 Hurricane Season. [online] Available at: https://www.almanac.com/content/list-names-2017-hurricane-season [Accessed 25 Sep. 2017]. En.wikipedia.org. (2017).\nAtlantic hurricane season. [online] Available at: https://en.wikipedia.org/wiki/Atlantic_hurricane_season [Accessed 25 Sep. 2017]. Weather Underground. (2017). Category 5 Hurricane Maria Hits Dominica. [online] Available at: https://www.wunderground.com/cat6/category-5-hurricane-maria-hits-dominica [Accessed 25 Sep. 2017].\nThe commonwealth. (2017). Dominica : History | The Commonwealth. [online] Thecommonwealth.org. Available at: http://thecommonwealth.org/our-member-countries/dominica/history [Accessed 25 Sep. 2017].\nBiography.jrank.org. (2017). Mary Eugenia Charles Biography – Learned Values from Family, Jumped into Politics, Elected to Lead, Formed Tough Government Policies. [online] Available at: http://biography.jrank.org/pages/2901/Charles-Mary-Eugenia.html [Accessed 25 Sep. 2017].\nTravelRepublic Blog. (2016). How Many Of The 26 Caribbean Countries Have You Heard Of? – [TravelRepublic Blog]. [online] Available at: https://www.travelrepublic.co.uk/blog/26-countries-of-the-caribbean [Accessed 25 Sep. 2017]. BBC News. (2017).\nCaribbean counts cost of Hurricane Irma. [online] Available at: http://www.bbc.co.uk/news/business-41192158 [Accessed 25 Sep. 2017].Facebook.com. (2017).\nRoosevelt Skerrit. [online] Available at: https://www.facebook.com/SupportRooseveltSkerrit/?hc_ref=ARR5sRCshW9lwwuLBAsOjesM1-9bv3ncl1ovEIVTLjgblx381wE9LmIECmzTYsDxe2w&fref=nf [Accessed 25 Sep. 2017].Bibliography\nPrime Minister, Hon Dr Roosevelt Skerrit’s statement to the UNGA 72nd session Gadebate.un.org. (2017). [online] Available at: https://gadebate.un.org/sites/default/files/gastatements/72/dm_en.pdf [Accessed 26 Sep. 2017]\nDisaster Reports : The Effects of Hurricane David. 1979, on the Population of Dominica.\nMona.uwi.edu. (2017). [online] Available at: https://www.mona.uwi.edu/cardin/virtual_library/docs/1266/1266.pdf [Accessed 25 Sep. 2017].\nHurricane David’s impact on Dominica.A description of activities related to Hurricane David and the extent of its impact on Dominica and its residents between Tuesday, August 28, and Sunday September 2, given below:']"	['<urn:uuid:8dbbca9c-bfea-490d-b14b-4a2909c95a22>', '<urn:uuid:943c962c-6306-4b22-9005-beeed88aade5>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T23:00:04.658849	11	73	1943
33	I love eating steak but sometimes worry about food safety - what happens if you eat meat that isn't properly cooked?	Eating undercooked meat can lead to parasitic infections, particularly from tapeworms and trichinosis roundworms. Tapeworms can embed their heads into the intestinal wall and produce eggs that mature into larvae that migrate to other parts of the body. With trichinosis, the larvae mature in your intestines and as they reproduce, they can travel outside the intestines into muscle and other tissue.	['Parasites are organisms that live in and feed off a living host. There are a variety of parasitic worms that can take up residence in humans. Among them are flatworms, thorny-headed worms, and roundworms.\nThe risk of parasitic infection is higher in rural or developing regions. The risk is great in places where food and drinking water may be contaminated and sanitation is poor.\nRead on to learn more about parasitic worms, plus how to avoid becoming an unwitting host.\nWhen it comes to parasitic infection, flatworms and roundworms are likely culprits. These two types of parasitic worms can be found in a variety of habitats. They aren’t always visible to the naked eye.\nYou can get a tapeworm, which is a type of flatworm, by drinking water contaminated with tapeworm eggs or larvae. Raw or undercooked meat is another way tapeworms can find their way into people.\nTapeworms embed their heads into the intestinal wall and remain there. From there, certain types of tapeworms can produce eggs that mature into larvae that migrate to other parts of the body.\nFlukes are a type of flatworm. People are less likely than animals to contract flukes. Raw watercress and other freshwater plants are the main sources of flukes in humans. You can also get them when you drink contaminated water.\nThey make their home in your intestines, blood, or tissues. There are many varieties of flukes. None reach more than a few inches in length.\nHookworms are transmitted through feces and contaminated soil. The most common way to make contact with this type of roundworm is to walk barefoot on soil infested with hookworm larvae. They can pierce through the skin.\nHookworms live in the small intestine, where they attach themselves to the intestinal wall with a “hook.” They’re usually less than half an inch long.\nPinworms are tiny, fairly harmless worms. They’re more common in children. These roundworms, when fully matured, live in the colon and rectum. The female lays eggs around the anus, usually during the night.\nThe eggs can survive on bedding, clothing, and other materials. People contract them when they touch the eggs and end up putting them in their mouths. The eggs are so small you can even breathe them in if they become airborne. They’re easily passed among children and caregivers or in institutions.\nAlthough pinworm infections are usually harmless and easily treatable, there have been less common cases of pinworms in the appendix that, when present, are usually in children and rarely in adults. One journal article found pinworms to be a rare cause of acute appendicitis.\nAnother journal article noted that pinworms in the tissues of a surgically removed appendix are an infrequent finding, and researchers also state that parasite infections only rarely cause acute appendicitis.\nHowever, these articles note that the symptoms of an intestinal parasite infection may mimic symptoms one would see in acute appendicitis, although appendicitis may or may not actually be occurring.\nTrichinosis roundworms are passed among animals. The most common way humans get trichinosis is by eating undercooked meat that contains the larvae. The larvae mature in your intestines. As they reproduce, those larvae can travel outside the intestines into muscle and other tissue.\nIt may be hard to believe, but you don’t always know when you have an uninvited guest inside you. You may not have any symptoms, or they may be quite mild.\nThe symptoms you could have include:\n- lack of appetite\n- abdominal pain\n- weight loss\n- general weakness\nIn addition, tapeworms can cause:\n- lumps or bumps\n- allergic reaction\n- neurological problems such as seizures\nIt may take weeks or months to notice additional symptoms of fluke infection. These may include:\nAdditional symptoms of hookworms include:\n- itchy rash\nAs trichinosis worms travel through the bloodstream and enter other tissue or muscles, they can cause:\n- swelling of the face\n- muscle pain and tenderness\n- light sensitivity\nIf you’re experiencing any unusual symptoms, especially if you’re returning from a trip to another country, consult your doctor. They’ll work with you to determine the cause of your symptoms.\nDiagnostic tests will be necessary to identify the culprit:\n- A fecal test involves checking a stool sample for parasites, larvae, or eggs.\n- A colonoscopy can be useful when stool samples turn up no evidence of parasites as a cause of diarrhea. They may also help eliminate other causes for your symptoms.\n- A blood test can be used to detect some types of parasites in the blood.\n- Imaging tests like MRI, CT scan, or X-rays can be used to detect organ injury caused by parasites.\n- A tape test involves placing clear tape around the anus. The tape can be examined under a microscope for the presence of pinworms or their eggs. But even with the naked eye, sometimes you may be able to see evidence of pinworms around a child’s anus within the first few hours of falling asleep.\nThe main treatment is prescription antiparasitic medication. This family of drugs can kill parasites and help pass them through your system.\nThe antiparasitic medication you’ll receive, doses schedule, and duration of treatment will depend on the type of parasite infection you have. Don’t stop taking the medication in the middle of the course, even if you feel better.\nIn very severe cases in which parasites have invaded other parts of the body, additional treatments like surgery and other medications to address additional problems caused by the parasites may be necessary.\nAsk your doctor if you should follow a special diet or take nutritional supplements during this time. Follow up with your doctor as advised.\nMost people respond well to treatment and feel better within a few weeks. A full recovery can be expected in most cases.\nIt may take longer to recover if you have:\n- a severe case\n- compromised immune system\n- a coexisting health condition\nThe following tips can often help prevent parasitic worm infection:\n- Never eat raw or undercooked meat, fish, or poultry.\n- Avoid cross-contamination during food prep by keeping meat separate from other foods.\n- Disinfect all cutting boards, utensils, and countertops that touched raw meat.\n- Don’t eat watercress or other freshwater plants raw.\n- Don’t walk barefoot in places where soil may be contaminated by feces.\n- Clean up animal waste.\nAlso be sure to give your hands a good scrubbing with soap and water at these times:\n- before eating\n- before food prep\n- after touching raw meat\n- after using the toilet\n- after changing a diaper or caring for someone who’s sick\n- after touching an animal or animal waste\nIt’s more difficult to prevent parasitic worm infection when you’re traveling to foreign countries, especially in regions where sanitation is a problem. That’s when you should be extra vigilant.\nWhen traveling, be sure to:\n- Be aware of how your food is prepared.\n- Drink only bottled water.\n- Carry hand sanitizer. Soap and water is best, but if you don’t have access to soap and running water, it can help prevent parasitic worm infection.\nHealthline and our partners may receive a portion of revenues if you make a purchase using a link above.']	['<urn:uuid:3c711f53-eabc-4758-ad03-2014a96ba5e1>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T23:00:04.658849	21	61	1207
34	difference between risk assessment hazard analysis	Risk assessment and job hazard analysis (JHA) are related but distinct safety evaluation processes. Risk assessment is a broader legal requirement that examines what could harm people by evaluating risk (severity x probability) and determining if existing precautions are adequate. It follows 5 steps: identifying hazards, determining who might be harmed, evaluating risks, recording findings, and reviewing. JHA, on the other hand, is a more focused technique that specifically analyzes job tasks to identify hazards before they occur, examining the relationship between worker, task, tools and work environment. JHA is used to prioritize workplace safety efforts and is typically performed before creating Job Safety Analyses (JSAs).	"['Presentation on theme: ""Risk Assessment. Objectives By the end of this presentation you will know: What risk assessment is; Where the need for risk assessment comes from; and.""— Presentation transcript:\nObjectives By the end of this presentation you will know: What risk assessment is; Where the need for risk assessment comes from; and The principles behind carrying out a risk assessment.\nRisk Assessment - What Is It? A careful examination of what could cause harm to people, so that you can weigh up whether you have taken enough precautions or should do more. The aim is to make sure that no one gets hurt or becomes ill. It is a legal requirement.\nWhat is a Hazard? Something with the potential to cause harm Examples: Electricity, Hazardous substances, Working at Heights, Vehicles.\nWhat is Risk? The likelihood of harm resulting from a hazard Risk = Severity x Probability\nRISK COST The Legal Position Health & Safety At Work Etc Act 1974 – General duty to ensure health, safety and welfare at work of all employees so far as is reasonably practicable.\nManagement of Health & Safety at Work Regulations 1999 Legal duty on employers to carry out risk assessments as first step in ensuring safe and healthy workplace Written record (5 or more employees) Identify preventive and protective measures Review assessments if no longer valid Risk assessments must be Suitable and Sufficient\nWhat Is Suitable & Sufficient? Identify all hazards Identify specific regulations Systematic approach Include non-routine operations Identify who is at risk Take existing control measures into account\n5 Steps to Risk Assessment 1. Look for and list the hazards 2. Decide who might be harmed and how 3. Evaluate risks arising from hazards and decide whether existing controls are adequate 4. Record the findings 5. Review assessment when necessary\nStep 1: Identify the hazards Walk around the workplace and look afresh at would could cause harm Ignore the trivial, and concentrate on significant hazards Ask your employees what they think Use manufacturers instructions or data Refer to accident/incident records\nStep 2: Decide who can be harmed Employees are the main group, but dont forget: – Young workers, trainees, new and expectant mothers etc. may be at particular risk; – Cleaners, visitors, contractors, maintenance workers etc. who may not be in the workplace all the time; and – Members of the public, or people you share a workplace with if there is a chance they could be hurt by your activities.\nStep 3: Evaluate the risk Consider how likely it is that each hazard could cause you harm Even after all precautions have been taken, some risk usually remains Decide for each significant hazard whether this remaining risk is high, medium or low Have you done all the things the law requires you to do? Are there industry standards?\nStep 3: Evaluate the risk The aim is to make all risks small In taking action, the aim is: a.) Get rid of the hazard altogether b.) Control the risk so that harm is unlikely When controlling risks, you need to follow the principles of prevention\nPrinciples of Prevention Try a less risky option Prevent access to the hazard (e.g. by guarding) Organise work to reduce exposure to the hazard Issue personal protective equipment Provide welfare facilities (e.g. washing facilities to remove contamination, first aid etc.\nCommon Problems Variation of work; Employees who move from one site to another: – Identify the hazards you can reasonably expect and assess the risks from them. After that, if additional hazards are spotted when you get to site, get information from others on site, and take what action seems necessary\nCommon Problems Sharing a workplace: – Tell the other employers and self-employed people there about any risks your work could cause them, and what precautions you are taking. – Also think about the risks to your own workforce from those who share your workplace\nCommon Problems Some risks have already been assessed: – If, for example, you have already assessed the risks from hazardous substances and have taken necessary precautions, you do not have to repeat this information\nStep 4: Record significant findings If you employ five or more people then you must record the significant findings of your assessment This means recording the significant hazards and conclusions You must also tell your employees about your findings\nStep 4: Record significant findings You need to be able to show that: – A proper check was made; – You asked who might be affected; – You dealt with all the obvious significant hazards, taking into account the number of people who could be involved; – The precautions are reasonable, and the remaining risk is low\nStep 4: Record significant findings Keep the record for future reference or use - it can help if an Inspector asks what precautions you have taken or if you become involved in any action for civil liability You can refer to other documents (e.g. manuals, H&S policy, company rules, manufacturers instructions etc.). These may already list hazards and precautions - there is no need to repeat all that.\nStep 5: Review Sooner or later, new machines, substance or procedures will be introduced which could lead to new hazards If there is any significant change, add to the assessment to take account of the new hazard Dont amend the assessment for every trivial change It is good practice to review the assessment from time to time to ensure precautions are still working effectively\nSummary Risk assessment is a legal requirement Follow the 5 steps to Risk Assessment; – Identify the hazards – Decide who can be harmed – Evaluate the risk – Record significant findings – Review and revise Risk assessment is a means to an end, not an end in itself - the aim is to keep people safe, not have good paperwork', 'Presentation on theme: ""What is a JHA and How Do I Use It? Presented by Bill Jividan, CSP.""— Presentation transcript:\nWhat is a JHA and How Do I Use It? Presented by Bill Jividan, CSP\nPurpose To inform safety proponents about a very useful tool that can be used to indentify hazards and to have a plan on how to control or eliminate hazards.\nWhat is a hazard? A hazard is the potential for harm. In practical terms, a hazard often is associated with a condition or activity that, if left uncontrolled, can result in an injury or illness.\nJHA Definition OSHA defines a Job Hazard Analysis as: a technique that focuses on job tasks as a way to identify hazards before they occur. It focuses on the relationship between the worker, the task, the tools, and the work environment. A JHA is on component of the larger commitment of a safety and health management system.\nWhats the difference between JHA and JSAs? A Job Hazard Analysis should be the preliminary step to evaluating hazards associated with any job category or function. The JHA assigns risk while the JSA identifies specific hazards and provides tools to remediate the hazards.\nContinued Performing a JHA prior to a JSA is critical because it allows a company to prioritize the implementation of its workplace safety efforts.\nWhy is a JHA important? One of the best ways to determine and establish proper work procedures is to conduct a JHA. A JHA is one component of the larger commitment of a safety and health management system\nWhat value does a JHA have? JHAs help identify, eliminate and prevent hazards in the workplace. This will likely result in fewer worker injuries and illnesses, safer, more effective work methods and reduced workers compensation costs and increased worker productivity.\nWhat jobs do I use JHA on? Jobs with the highest injury or illness rates. Jobs with the potential to cause severe or disabling injuries or illness, even if there is no history of previous accidents. Jobs in which one simple human error could lead to a severe accident or injury. Jobs that are new to your operation or have undergone changes in processes and procedures. Jobs complex enough to require written instructions.\nOK, Where do I begin? Involve your employees Review your accident history Conduct a preliminary job review List, rank, and set priorities for hazardous jobs. Outline the steps or tasks.\nHow do I identify workplace hazards? What can go wrong? What are the consequences? How could it happen? What are other contributing factors? How likely is it that the hazard will occur?\nGrinding Iron Castings: Job Steps Step 1. Reach into metal box to right of machine, grasp casting, and carry to wheel. Step 2. Push casting against wheel to grind off burr. Step 3. Place finished casting in box to left of machine.\nExample Job Hazard Analysis Form Job Location: Metal ShopAnalyst: Joe Safety Date: Task Description: Worker reaches into metal box to the right of the machine, grasps a 15 pound casting and carries it to grinding wheel. Worker grinds 20 to 30 castings per hour. Hazard Description: Picking up a casting, the employee could drop it onto his foot. The casting\'s weight and height could seriously injure the worker\'s foot or toes. Hazard Controls: 1. Remove castings from the box and place them on a table next to the grinder. 2. Wear steel-toe shoes with arch protection. 3. Change protective gloves that allow a better grip. 4. Use a device to pick up castings.\nJob Location: Metal Shop Analyst: Joe Safety Date: Task Description: Worker reaches into metal box to the right of the machine, grasps a 15 pound casting and carries it to grinding wheel. Worker grinds 20 to 30 castings per hour. Hazard Description: Castings have sharp burrs and edges that can cause severe lacerations. Hazard Controls: 1. Use a device such as a clamp to pick up castings. 2. Wear cut-resistant gloves that allow a good grip and fit tightly to minimize the chance that they will get caught in grinding wheel.\nJob Location: Metal ShopAnalyst: Joe SafetyDate: Task Description: Worker reaches into metal box to the right of the machine, grasps a 15 pound casting and carries it to grinding wheel. Worker grinds 20 to 30 castings per hour. Hazard Description: Reaching, twisting, and lifting 15 pound castings from the floor could result in a muscle strain to the lower back. Hazard Controls: 1. Move castings from the ground and place them closer to the work zone to minimize lifting. Ideally, place them at waist height or on an adjustable platform or pallet. 2. Train workers not to twist while lifting and reconfigure work stations to minimize twisting during lifts. Repeat similar forms for each job step.\nSample Job hazard Analysis Form Job Title:Job Location:Analyst:Date: Task #Task Description: Hazard Type:Hazard Description: Consequence:Hazard Controls: Rational or Comment:\nQuestions? Presented by Bill Jividan CSP firstname.lastname@example.org']"	['<urn:uuid:b5cbee21-9ee4-4370-ad6e-296ad4e78a22>', '<urn:uuid:33477bac-b10f-4f57-996a-31137b43edec>']	open-ended	direct	short-search-query	similar-to-document	comparison	expert	2025-05-12T23:00:04.658849	6	106	1797
35	digital pressure gauge vs analog gauge advantages disadvantages operation differences explain	Digital pressure gauges are more accurate and easier to read than analog gauges, but require an additional power source like batteries to operate. They use sophisticated sensors and microprocessors, working through either strain gauge technology (measuring changes in electric resistivity) or piezoelectric technology (measuring electric charges). In contrast, analog gauges operate mechanically using an elastic chamber connected to the pressure being measured, translating pressure-induced motion into needle movement across a scale through a system of gears. While analog gauges are simpler and don't need external power, they're increasingly being replaced by digital gauges due to superior accuracy.	['Many types of pressure measurement devices convert their readings into electric signals that can be displayed by a data acquisition device (or DAQ). Although pressure gauges are able to display their readings digitally, they are notable for being able to directly measure and display pressure readings without necessarily converting such information electronically. Pressure gauges are valued in the industrial world for their simplicity, accuracy, cost-effectiveness, and low maintenance.\nIn a certain sense, the origin of pressure gauges can be traced back to the early modern era and the scientific discoveries of Evangelista Torricelli, a mathematician and physicist from Italy. In 1644, Torricelli discovered the existence of vacuums in nature as well as the fact that air carries weight. Other scientists like the Frenchman Blaise Pascal continued to build on Torricelli’s discoveries. Pressure gauges as we know them today, however, did not truly come in to being until the Industrial Revolution. During the 1840s, Frenchman Eugene Bourdon began searching for a solution to the disturbing rate of fatalities involving highly-pressurized locomotive engines. The result of his efforts was the invention of the Bourdon gauge in 1849. Although initially designed for railroad application, Bourdon inadvertently made a much larger contribution to the field of industry as a whole. The Bourdon gauge enabled industrialists of all types to measure much higher-pressure levels than had previously been possible and opened the way for further gauge development. Today, the Bourdon gauge (discussed further below) continues to be the most commonly used type of pressure gauge.\nThe measurement of pressure is crucial to the safe and proper functioning of many types of industrial systems (e.g. water-based systems, oil-based systems, gas-based systems), and corresponding industrial products (e.g. water heaters, fire extinguishers, medical gas cylinders, etc.) Entire fluid power systems would be unpredictable and unreliable (and thus, useless) without an accurate way to measure and adjust pressure. Measuring pressure is important not only to maintain mechanisms that directly run on proper pressure control, but to properly operate mechanisms that depend on values tied to pressure control (e.g. flowmeters where pressure levels affect flow rates). In fact, pressure is so important to modern industry that it is one of the most commonly measured phenomena in commerce as a whole. Only temperature is more commonly sampled.\nPressure gauges may be fabricated with a number of materials, depending on the demands of the specific application. A couple of examples follow below.\n• Many gauges are exposed to harsh substances or chemicals including those that operate in the chemical, petrochemical, refining, power, and pharmaceutical processing industries. Such gauges must be corrosion-resistant; most likely, they be made of stainless steel. On the other hand, for pressure gauges that will encounter only non-corrosive fluids or gases, brass or bronze construction will probably be adequate.\n• The specific pressure conditions a gauge will operate in constitute another factor in material selection. High pressure scenarios generally require gauges made of very durable material, such as steel. In contrast, low pressure scenarios may be well served by gauges made of bronze, or similar material.\nThe internal mechanisms of gauges are usually made of materials such as Beryllium copper, phosphor bronze, various steel alloys, etc. Typically, the internal tubes inside pressure gauges (discussed in the following section) receive a specific form of heat treatment know as spring tempering. Such treatment increases the tube’s elasticity while (more or less) retaining its original shape.\nWith regard to actual size, pressure gauges display a marked degree of variability. Regardless of their specific material makeup, standard and custom pressure gauges are designed to be able to fit into any number of tight spaces or (on the other hand) to sufficiently span an uncommonly large tank.\nHow They Work\nThe (relative) simplicity of pressure gauges derives from the fact that the pressure measured by the gauge is the only energy source necessary for the gauge to work. At the end of the production process, pressure gauges are calibrated according to the pressure readings of a “master” gauge that already exists. Once this is accomplished, the gauge is ready for use. Gauges are typically able to be installed at various points on a fluid power system (e.g. near a hydraulic pump’s pressure port, at a stand-alone regulator in a pneumatic or compressed air system, etc.). Sometimes, pressure gauges are even able to measure “sub-circuits” within a fluid power system that operate at different pressures than the rest of the overall system (e.g. a circuit that occurs after a reducing valve).\nTwo major groups of pressure gauges exist: analog pressure gauges and digital pressure gauges. This division is important to take note of since these two types of gauges operate and display information somewhat differently. Analog gauges are “traditional” pressure gauges which display information with a needle that changes position on a meter’s clock-like face (in proportion to changes in pressure).\nAnalog Pressure Gauges\nThe key component of analog gauges is the “tube” alluded to in the previous section. These types of gauges contain some type of inner, elastic chamber that is somehow connected to the pressure being measured – and thus deforms, or otherwise moves, as the pressure acting on it changes. Through an intricate system of gears (known as the movement), the chamber within an analog gauge is able to translate pressure-induced motion into the movement of a needle across a scale.\nElastic chambers typically come in three forms:\n• Bourdon tubes are the most common type of elastic analog chamber. A Bourdon tube is an elastic, C-shaped chamber consisting of one of the metals described in the previous section (e.g. copper, steel). When pressurized fluid enters this tube, it causes the tube to uncoil or straighten. This uncoiling of the Bourdon tube activates the gear and shaft assembly – which, in turn, moves the needle on the gauge’s clock-like display. Bourdon tubes may also come in the form of spirals or helices. Overall, this type of elastic chamber comprises a simple yet effective mechanical method for converting changes in pressure to quantitative readings on a scale.\n• Bellows are elastic chambers that expand and contract rather than uncoiling in response to pressure changes. They consist of some type of thin-wall tubing and are almost always supplemented by a coil spring that strengthens its fatigue life.\n• Diaphragms or stacks (single or multi) Elastic chambers consisting of thin metal sheets within a cup are known as diaphragms or stacks. This type of chamber moves as pressure is applied to its interior. Unlike bellows, diaphragms do not utilize supporting spring structures.\nDigital Pressure Gauges\nAlthough analog gauges are still very popular, they are increasingly being replaced by digital gauges, which are easier to read and more accurate. Unlike analog gauges, digital pressure gauges require another power source (e.g. batteries) to operate. They are attached to an additional measuring device, which measures pressure using sophisticated sensors and microprocessors. Once this additional measuring device feeds results back to the gauge, the gauge is able to display a numerical reading.\nDigital gauges operate either using strain gauge technology or piezoelectric technology. Strain gauge sensors indirectly measure mechanical pressure by measuring changes in the electric resistivity of conducting materials. (Most often, this material is silicon, bonded metal foil, or some type of film – polysilicon film, thick film, sputtered thin film, etc.) When pressure mechanically deforms the chamber (usually a diaphragm) within the gauge, changes in resistivity also occur. These resistivity changes are then electronically converted and subsequently displayed. Piezoelectric type digital gauges work in a similar fashion. However, instead of measuring changes in resistivity, piezoelectric sensors measure electric charges that develop across them, proportional to mechanical changes in pressure.\nTypes and Applications\nTo accommodate many different applications, manufacturers make many different types of pressure gauges. Examples of these include water pressure gauges, air pressure gauges, oil pressure gauges, temperature gauges, gas pressure gauges, fuel pressure gauges, differential pressure gauges, and vacuum pressure gauges. Some of the uses of these gauges are more obvious than others.\nGauges Classified by the Substance They Measure\nWater pressure gauges (naturally) monitor the pressure of any water-based system. Quite commonly, they are found attached to tanks, where they monitor the pressure of the water within.\nGas pressure gauges measure and display gas pressure. They are especially prevalent in factories and manufacturing facilities, where they keep track of the flow rate of both high and low pressure natural gas and propane-based systems. Fuel pressure gauges also check gas pressure levels, but they do so in the context of automobiles. They measure and display the fuel supply, or amount of gas, left in a vehicle tank.\nAir pressure gauges measure pneumatic pressure in air-operated equipment.\nOil pressure gauges measure the pressure of oil circulating in a lubricating system.\nGauges Classified by Certain Conditions They Are Designed For\nVacuum pressure gauges measure and display the pressure within vessels or systems that are immersed in a sub-atmospheric or vacuum environment. Vacuum environments are particularly used to create cold temperatures.\nSeal gauges are made specifically to stop and avoid potential leaks. As such, they are designed with the addition of a diaphragm seal isolator. Most of their applications are found in the process, pharmaceutical, chemical, petrochemical, and sanitation industries.\nSome pressure gauges may be designed to specifically measure substances flowing at either exceptionally high velocities or exceptionally low velocities. High pressure gauges are important to manufacturing and industrial applications, particularly those related to high pressure hydraulic technology (e.g. hydrodemolition, hydroblasting pumps, and water cutting machines). Low pressure gauges are extremely accurate and sensitive, generally measuring pressures between ten and fifteen PSI. They are especially important to applications that take place in environments with frequent pressure fluctuation.\nClassification by Different Standards of Pressure\nThus far, only pressure gauges that operate with reference to gauge pressure have been discussed. A pressure gauge that measures gauge pressure uses the surrounding atmospheric pressure as the standard by which it measures. However, this is not the only way for a pressure gauge to operate. The main exceptions follow below.\nAbsolute pressure gauges measure pressure with reference to a perfect vacuum. This means that absolute pressure gauges include surrounding atmospheric pressure within their overall pressure reading. Due to the name of the standard by which they operate, absolute pressure gauges are commonly misidentified as being identical to vacuum pressure gauges. Diaphragm type chambers are typically used in absolute pressure gauges.\nSealed pressure gauges operate in a manner more similar to normal gauge pressure mechanisms. However, instead of using the surrounding atmosphere as the standard by which to measure pressure, sealed pressure gauges simply use some fixed pressure quantity – that may not necessarily correspond to the surrounding atmosphere – in order to make measurements.\nDifferential pressure gauges are a bit different than other gauges. Instead of measuring pressure as a whole, they measure the difference in pressure between two points of contained liquid or gas. They are popular for filtration applications.\nGauges Classified by Types of Applications\nYet another way to approach gauge classification is focusing on the specific application a gauge is used for. To provide just one example, magnehelic gauges are a special type of pressure test gauge that uses a diaphragm and measures static pressure within the HVAC industry. Many gauges are designed for use with a specific type of product. A single example is the use of dial gauges or weighted gauges for use with pressure canners.\nMore so than other industrial devices, pressure gauges are frequently used in conjunction with complementary instruments, like pressure sensors, pressure transmitters, pressure transducers, and switches. With the addition of these devices, gauges increase in accuracy and precision, displaying more specific readings with smaller margins of error. They may also be equipped with electric contacts that sound alarms, turn on signal lights, or control a valve or pump. Two specific examples of accessories used to increase the functionality of gauges are test point adapters and gauge isolators. Test point adapters fit to gauges and allow them to screw onto various points within the system, enabling a wide range of test pressure measurements without the purchase of multiple, separate gauges. Gauge isolators turn a pressure gauge into an “on/off” mechanism by mounting between a gauge and its circuit; unless a button is activated, the gauge will not be exposed to and will not read fluid pressure.\nMany accessories for gauges have protective purposes. As sophisticated industrial instruments, gauges face a variety of threats such as pipe vibration, water condensation, etc. The aforementioned gauge isolator also serves a safety function by protecting a gauge’s internal mechanism from sudden pressure spikes. Snubbers provide a similar function by damping intense fluctuations in pressure. A variety of case protectors can be purchased to protect the exterior of a gauge from hard shocks, while chemical seals are available to protect gauges from being corroded or plugged up. While pressure gauges generally do not require a high level of maintenance, investing in protective accessories is one of the best things gauge users can do to protect and extend the life of a gauge.\nWhen choosing a pressure gauge for a specific application, several factors should be taken into account. Some of the major considerations include the size of the dial, the size of the connection or port the gauge will be using, units of measurement that the gauge is capable of (e.g. PSI, mmHg, PA, etc.), compatibility of gauge material with the operating conditions (including temperature, corrosiveness, etc.). and whether the gauge should be dry or liquid filled (the latter tends to have longer life due to shock absorption).\nOne of the most important factors to consider is the pressure range of the gauge. Generally speaking, you should use a gauge that is able to read at least twice the amount of the expected working pressure. This provides a reasonable margin of safety for using the gauge. As a rule of thumb, operating pressure should never go above three-quarters of a gauge’s reading range.\nA corollary of the rules of thumb involving pressure is the importance of choosing application-specific pressure gauges. Hydraulic systems, for example, should only utilize hydraulic gauges designed for operating under normal conditions in hydraulic environments.\nIt is important to note that the different types of chambers utilized by pressure gauges correspond to different ideals concerning pressure conditions. Bourdon type gauges are especially useful for medium to high pressure environments. However, they do not work well for low pressure scenarios. Gauges which utilize bellows and diaphragm chambers, on the other hand, are well suited to sensing low amounts of pressure and incremental changes within them.\nFor even greater accuracy – as well as greater speed, reliability, and durability – digital pressure gauges should be used in place of analog devices (despite their greater expense). Keep in mind, however, that the limitations of analog devices can often be overcome with the variety of accessories available for pressure gauges. (For example, some analog gauges possess temperature compensation equipment and multiple dial sizes to enhance the accuracy of their readings.)\nSince so many factors constitute proper pressure gauge selection, investing in a quality pressure gauge provider is one of the best choices that pressure gauge consumers can make. You should focus on finding a reputable provider that offers a broad array of gauge products and/or services (e.g. recalibration services.) Working with a quality supplier will ensure that you are able to maximize the usefulness and effectiveness of your pressure gauges for your particular application.']	['<urn:uuid:c83b4e8e-c128-4dc8-bc4c-21013bf552c3>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T23:00:04.658849	11	97	2574
36	How has software development changed from waterfall to modern practices?	Software development has evolved from the sequential waterfall model, where each step had to be completed before the next could begin and changes were difficult to implement, to modern agile and DevOps approaches. Today's practices use automation through CI/CD pipelines, where code changes are continuously integrated, tested, and deployed. This modern approach enables faster time to market, more frequent updates, and improved product quality. While the traditional waterfall method required starting over when problems were found, current automated systems allow for rapid builds, tests, releases, and revisions.	['In the early days of software development, projects were developed sequentially in a series of steps which was called ?The Waterfall Model.? It was called the waterfall because once you got past a step, you couldn?t climb back up. Here is a typical waterfall model for software development:\nEach step needed to be completed before the next could begin. It took time to develop and changes were difficult to include. When problems were found, developers often had to start all over at the top of the waterfall.\nSay Goodbye To The Waterfall\nIn today?s continuous development cycles, the waterfall method just is not relevant anymore. Thus, the agile methodology was born. It values individuals and interactions over process and tools. It taught developers to break software down into smaller chunks, develop these chunks, and accelerate feedback loops. It provides for quicker design, collaboration, and a faster launch cycle.\nDevOps teams have taken this agile concept to heart by merging software development (the Dev part) and IT operations (the Ops part) into a cohesive team to accelerate delivery. It created agile teams that could work together cross-functionally and iterate quickly.\nContinuous integration (CI) streamlines the system by using automation to drive products through the system. Each chunk is tested and integrated continuously. This allows for smaller and more frequent updates. Continuous delivery (CD) allows for rapid release software delivery. Tools like Jenkins provide DevOps teams with the ability to automate each stage of the delivery pipeline. It enables a consistent and reliable method to regularly update apps and software.\nUsing these methods, agile methodologies have been extended across development, integration, testing, and delivery cycles. While agile methods provide the underpinning for DevOps teams, the collaboration of development and operations teams, coupled with CI and CD, greatly enhances the efficiency of the operation.\n- Faster time to market\n- Ability to build and evolve products\n- Improved efficiency\n- Reliable releases and deliver\n- Improved quality product\n- Improve customer satisfaction\nDevOps Builds On Agile Methodologies\nYou will find many similarities between the traditional agile model and how DevOps teams work together. DevOps is holistic and collaborative with continual feedback loops. Automated systems like JFrog Artifactory and Go Registries allow for rapid builds, tests, releases, monitoring, and revisions.\nHowever, there are differences as well. Agile focuses on the development phase and tends to ignore the operations phase. DevOps provides end-to-end operations from development through delivery. It treats the code no differently whether it is in the development phase, QA, or on a production server. DevOps tends to embrace a more inclusive and wider culture.\nDevSecOps teams use the same techniques to remove silos in organizations by folding in other functional areas that play a role. One area that?s emerging is folding in security. So-called DevSecOps teams function as a unit and drive security functions throughout the end-to-end process as part of the culture.\nBuilding on agile methods, the DevOps model breaks down barriers between functional units so that development and operations can work collaboratively across the entire product life cycle. It smooths out hand-off friction and creates operational efficiency. This accelerates delivery and innovation.', 'Agile is widely used in development environments, however, DevOps is required if continuous integration and delivery (CI/CD) is to be adopted. In older times many teams may be unaware that much of the process from development coding to testing and ultimately production may be automated.\nAccording to AWS documentation, DevOps is a combination of cultural concepts, processes, and technologies that increase an organization’s ability to develop rapid applications and services: updating and improving products at a quicker rate than conventional management approaches. Implementing a CI/CD pipeline in the development environment eliminates confusion, and developers can maintain speedy delivery of fixes and updates during the software lifecycle, which was previously significantly slower due to the lack of automation.\nCI/CD is the Backbone of the DevOps Concept\nAgile is a concept that allows development and testing iterations in the software development process. Unlike the Waterfall approach, development and testing operations are carried out concurrently under this methodology. This procedure improves communication among clients, developers, managers, and testers.\nDevOps Methodology added automation to the SDLC process with the help of cloud computing and DevOps tools. Developers can use CI/CD to distribute software updates more quickly and lower the number of errors in design and specifications when presenting new modifications to stakeholders. Automation with CI/CD accelerates the process and can considerably shorten software development time.\nSo What is CI/CD?\nContinuous Integration and Continuous Delivery shortly CI/CD is the process of automating the SDLC process. Commonly referred to as the CI/CD pipeline it is a series of procedures that must be followed in order to deploy new software releases.\nIt gets the generated codes, deploys to development and test environments, and makes it easy to find and fix bugs in similar production environments that assist you in properly deploying a new version of the product. CI refers to continuous integration while CD is continuous delivery.\nContinuous integration is a software development technique in which code changes are integrated into a shared repository, then build and test phases are triggered and performed automatically. The primary aims of continuous integration are to find and resolve flaws quicker, improve software quality, and reduce the time it takes to verify and deploy new software updates.\nContinuous delivery is a technique that automates the creation, testing, and deployment of code changes. It stands for one step beyond continuous integration by deploying all code changes beyond the build step to a test or production environment. When continuous delivery is effectively implemented, developers will always have a deployment-ready build artifact that has passed a standardized test process.\nContinuous Delivery vs Continuous Deployment\nIf someone says CI/CD, they usually mean continuous delivery rather than continuous deployment. As it is explained in the previous section, Continuous Delivery is the technique of automatically pushing code updates into the “dev” and “test” (staging) environments so that the dev team can examine the product increment and QA testers can make their tests.\nContinuous Deployment is when code updates go beyond Continuous Delivery and are automatically deployed to the production environment on a regular basis. The presence of a manual approval process to move to production distinguishes continuous delivery from continuous deployment. Continuous deployment automates production and eliminates the need for explicit approval.\nHow to Implement CI/CD Pipeline\nMost development teams now use the DevOps approach, but there is no single way to execute CI/CD because most teams tailor the software automation to their specific needs and interests. The secret to effective CI/CD is to use the appropriate automation tools and methods. Developers must identify how to coordinate changes and select methods that work with the environment.\nTeams should implement CI/CD into the SDLC process through a combination of version control, build, test, orchestration, and configuration management tools.\nCI starts with centralized repositories, where the development team works on code using version control systems such as Git. A version control system (VCS) keeps track of every change and makes it easy to return when anything goes wrong. It also supports configuration as code, allowing teams to handle testing, infrastructure, and other components as versioned artifacts.\nCI build tools package files into deployable components. After passing unit test checks, CD tools deliver artifacts to the operations team for further testing and preparation.\nThe term “environment” refers to a deployment infrastructure such as “prod” (production), staging, or “dev” (development) for the deployable components. CI/CD pipeline builds, tests, and deploys code in contexts ranging from where developers write code to where operations teams make apps publicly available.\nAny CI/CD workflow relies heavily on testing. Automation facilitates effective CI/CD adoption. Tests must be done as quickly as feasible so that feedback reaches the developer as soon as possible. This enables the discovery of bugs at an early stage. The final result is a bug-free and more accurate application. All of this will be feasible only with test suite automation.\nIf all tests pass so the code components are regarded as ready for deployment in a production environment. It is transmitted to human stakeholders, authorized, and then deployed in a continuous delivery pipeline.\nSoftware configuration management is the work of tracking and regulating changes in software in software engineering, and it is part of the wider cross-disciplinary topic of configuration management. Configuration management is frequently used in conjunction with version control and CI/CD pipelines.\nBenefits of CI/CD\nIn an organization that doesn’t use DevOps methodology, the release of software requires a long time for manual deployments and testing, while a mistake that can stop the process can bring everything back to the beginning. In such an environment, due to the effort and time required to get code ready for release, changes are only delivered every few months or so.\nBut with DevOps, Continuous Integration and Delivery (CI/CD) pipelines automate the build, test, and deploy process and produce several advantages. When done effectively it may reduce development time, manual integrations, and administrative resources. Implementing CI/CD can save thousands of dollars in development costs. Here are the other benefits of CI/CD:\nProductivity and Speed\nWhat are the Best CI/CD Tools?\nDepending on your goals and, of course, your budget, CI/CD solutions are available in both free and paid models. You can develop your custom automation scripts, but using the correct tools is a lot more effective approach to dealing with automation. Jenkins, GitlabCI, Bamboo, TravisCI, CircleCI, and AWS CodePipeline are some of the most popular CI/CD tools.\nCI/CD tools are essential for organizing and automating the pipeline’s many steps, from starting the process after a commit to managing the build, triggering automated tests, publishing artifacts, and aggregating and relaying feedback. Once you’ve decided on a tool, it needs to be set up to work with your version control system.\nWithout DevOps tools, the deployment process is complex. The CICD implementation completely solves this complexity as automation happens with one click in the CICD pipeline.\nCombined, CI/CD gives the benefit of faster software releases that are reliable, and of highest quality through test automation and continuous service provision. Due to the reasons above, CI/CD is currently being adopted rapidly in the software engineering industry. It is predicted that worldwide DevOps software market to reach $8 billion by end of 2022.\nWhether you aim to optimize your own software development life cycle or build from start with CI/CD automation tools, having a system that can grow to meet your business goals is critical. Nioyatech’s CI/CD Implementation Service allows enterprises to work together to accomplish those common goals.\nAWS Documentation, https://aws.amazon.com/devops/what-is-devops/#integration\nRajasinghe, Maneka (2021): Adoption challenges of CI/CD methodology in software development teams. TechRxiv. Preprint. https://doi.org/10.36227/techrxiv.16681957.v1']	['<urn:uuid:0c2d2c33-abd3-4a5c-85d9-60e4d1fea0f5>', '<urn:uuid:144c2861-161e-40da-a850-e6cbfacba683>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T23:00:04.658849	10	87	1780
37	As a naval historian studying WW2 convoys, what challenges did ships face in rough weather?	Ships faced severe challenges in rough weather during WW2 convoys. Records show gales reaching 80 mph that caused extensive damage - smashing carley floats, swamping lifeboats, and flooding mess decks. Ships experienced dangerous rolls up to 57 degrees, steering gear failures, and multiple fire outbreaks from damaged wiring. The conditions were so severe that convoys would get scattered - for example, convoy SC.78 was reduced from 47 ships to just 12 after a major storm. Ships also had to deal with practical issues like fuel shortages from searching for scattered convoy members and rationed drinking water.	"['Commodore was Captain W. H. Lawrence C.B.E. (in Briarwood?). For info, this ship is listed in Convoy JW 51A from Loch Ewe to Russia, Dec. 15-1942 - ref. this external link.\nShips in the Convoy:\nThe British Briarwood, Empire Sky, Empire Galliard, Empire Gilbert, Empire Scott, Empire Clarion, Chulmleigh, Daldorch, Dover Hill.\nFor info, Dover Hill and Daldorch are listed in Convoy JW 51B to Russia - again, ref. external link provided above, and see also the earlier ""Operation FB"" on that site (go to ""Independent""), in which Briarwood, Daldorch, Chulmleigh, Empire Sky, Empire Gilbert, Empire Galliard, Empire Scott and Richard H. Alvey took part.\nThe American Richard H. Alvey, John Emerson.\nAlso, 2 more un-named ships (the report below mentions a York, Oct. 21).\nDon was scheduled, but failed to join. (There was a Norwegian Don, but this must have been the British ship by the same name).\nRescue Vessels Rathlin and Toward.\nToward was on her 22nd voyage as Rescue Vessel, having started this voyage from Clyde on Oct. 17, to Reykjavik Oct. 22. Used as accommodation vessel at Iceland. Returned to Loch Ewe with Convoy RU 50, Nov. 26-Dec. 1-1942.\nRathlin was on her 9th voyage as Rescue Vessel, having started this voyage from Oban on Oct. 17, to Reykjavik Oct. 22. Used as accommodation vessel at Iceland for spare crews, then returned to Clyde with Convoy RU 49, Nov. 18-Nov. 23-1942. (""Convoy Rescue Ships 1940-1945"", Arnold Hague).\nReport on Passage:\n05:00 - Anchor weighed.\n05:46 - Passed gate.\n06:10 - ? Buoy.\n07:13 - Altered course 358°.\n07:15 - Flags HZ 6 also by flashing.\n07:45 - Flashing K 6.\n08:20 - From S.O. 13 is missing.\n08:22 - To S.O. Thank you, am doing 6 until they are in station.\n09:16 - K 8.\n09:18 - From S.O. What portion is missing.\n09:19 - To S.O. Think all here except 13.\n09:45 - From S.O. Please tell Empire Gilbert to keep guard on D/F 335 instead of 342.\n09:40 - To Dover Hill Please hurry up.\n10:20 - S 010°.\n10:35 - From S.O. What was last course.\n10:37 - To S.O. Course 010°.\n11:33 - From S.O. NE gale SW of Iceland. Depression 500 mls SW. Pressure m.b.(?) moving NE.\n12:30 - General signal Pass out and back course will be altered 20° at a time to 300° commencing about\n13:02 - S 2 a/c 350\n13:09 - S 2 a/c 330\n13:18 - S 2 a/c 310\n13:28 - S 300 a/c 300°\n13:26 - K 8 Speed of convoy 8 knots.\n13:43 - From 52 What is our course please\n13:44 - To 52 300°\n16:50 - From 31 Are you going to stream fog buoys tonight\n17:10 - To 31 & 51 Please pass back ? clocks to GMT(?) at midnight.\n17:40 - illegible (WUT?), T2400 illegible clocks one hour at midnight.\n17:50 - JG1 Load all P.A.C.(?) projectors with snowflakes.\nMonday, Oct. 19:\nTime not visible - Dense fog.\nTime not visible - Fog clearing. From S.O. Good morning. Do you intend altering at noon and is there anything you want\n10:40 - To S.O. Propose altering to 312° at noon. Will you signal with your siren when I have (done so?).\n10:45 - To 31 Alteration will be made at noon.\n10:41 - From S.O. Would rather not use siren. I am in touch with you all the time. Am instructing escorts to inform convoy of your alteration.\n10:58 - To S.O. Thank you.\n11:00 - Fog cleared. No. 12 1 mile ahead of station.\n11:10 - S 312°.\n11:20 - From 11 What is your signal.\n11:22 - S 312.\n11:30 - To Escort Please tell S.O. alteration has been made? (word missing in margin).\n11:40 - From 62 Will there be any more alterations.\n11:48 - To 62 No more alterations until tomorrow.\n12:04 - From S.O. Have just told No. 12 to resume position.\n!2:15 - From S.O. Formation of American aircraft bound for ?(missing in margin, looks like U.K.).\n12:20 - From S.O. Formation of aircraft from Iceland to U.K.\n13:25 - General signal 1st sub VIHG\n13:28 - To S.O. My position by observation at ? 60 30N 12 26W.\n17:13 - From S.O. No submarines reported in the vicinity. Gale off SW Iceland now E to SE. Do you still intend altering at 07:30 please.\n17:20 - To S.O. Will not cancel the signal tonight but will do it tomorrow morning if necessary if it is thick will alter course as arranged it will make little difference.\n17:35 - From S.O. We will probably make the difference in leeway anyhow.\n17:28 - To S.O. Agree. That is what I was thinking of (times seem a little mixed up here).\n17:30 - General signal. TG 1 Load P.A.C. with snowflake rockets.\n08:10 - From 22 What is your true course.\n08:12 - To 22 True course 290°.\n08:21 - From S.O. Air escort will be circling convoy today, Catalinas.\n08:34 - From S.O. What is your present course please.\n08:36 - To S.O. Am now steering 285 to make 290°.\n08:53 - From S.O. Air escort is sighted astern of convoy.\n08:54 - General signal A.I. Aircraft is friendly.\n09:15 - From S.O. What was last hoist.\n09:17 - To S.O. A.I.\n09:45 - General signal A.I. Catalina sighted.\n17:00 - General signal T.G.I.(?) and I.M. hoisted.\n17:10 - To S.O. Will continue on this course until tomorrow. When the weather may be a little better(?).\n17:17 - From S.O. Agree. Consider depression is now over Iceland. Will let you know the weather report if I get it before dark.\n?:35 - British cruiser approaching.\n?:40 - From S.O. Gale from the SE later on.\n?:41 - F.W. Approaching vessel is friendly.\n?:00 - From S.O. Regret have orders for Saladin and myself to leave. Lady Madeleine will take over Senior Officer. Goodbye and good luck.\nTime not visible - To S.O. Thank you for your assistance. Good luck.\n?:05 - S 310°\n?:10 - To S.O. Is aircraft friendly.\n?:12 - From S.O. Aircraft is friendly.\n?:13 - A 1 Aircraft is friendly.\n?:20 - A/C 310°.\n?:30 - To S.O. Do you know whether Reykjanes light is burning.\n?:44 - From S.O. I think so.\n?:15 - To No. 12 At noon take station astern of 32.\n?:20 - To No. 11 At 12:30 take station ahead of 23.\n?:25 - To No. 23 No. 11 is taking station ahead of you at 12:30.\n?:30 - From No. 11 Does that apply to whole column.\n?:35 - To No. 11 No, have ordered 12 to take station astern.\n? - To No. 62 At noon take station 53.\n? - To No. 53 Please make room for 62 to come ahead of you at noon.\n? - To No. 52 Please close up to allow York to come in astern of you at noon.\n? - To No. 62 You may take station now while the going is good.\n? - To No. 61 At noon drop back and take station astern of my column.\n? - From S.O. At 11:00Z aircraft gives out position 62 45N 19 49W. By D.F. I make (? looks like Portland or similar) abeam 016° and Reykjanes 305°.\n11:40 - To S.O. Have ordered convoy into four columns at noon.\n11:42 - From S.O. What do you make out estimated position.\n11:43 - To S.O. We agree with you but hoping to get a position off the island of ? (looks like Gerfuglasket or similar)\n11:46 - From S.O. I have a radio D.F. Will try to get a bearing.\n13:28 - From S.O. Rock bearing 359°.\n13:34 - To S.O. Do you mean ? (Gerfuglasket or similar).\n13:39 - From S.O. Yes.\n13:50 - To S.O. Please repeat message I cannot see the island yet.\n13:55 - From S.O. Was bearing 359 but I cannot see it now.\n14:44 - To S.O. Gerfuglasket(?) bearing 000° now.\n15:10 - I.E. T1900 Both wing coulmns are to form astern of their next inner coulmns at 19:00 hours.\n15:45 - S 300.\n15:45 - To Dover Hill Are you having trouble with your steering gear.\n15:48 - Reply Just a bad ?\n15:53 - To S.O. S 300.\n16:00 - To S.O. Suggest reducing to 7 knots at 18:00 hours to make E.T.A. 06:30.\n16:30 - From S.O. Yes, I agree. Shall I make E.T.A. Suggest two columns when passing Reykjanes.\n16:35 - To S.O. Yes please make E.T.A. Convoy ordered into four columns at 19:00 hours.\n17:05 - From S.O. Have made E.T.A. Suggest burning navigation lights on approaching Reykjanes and onwards.\n17:30 - To S.O. Suggest leading ships only burn navigation lights.\n17:48 - From S.O. I agree.\n17:50 - K 7 Speed of convoy 7 knots.\n(This is all that\'s available of the report).\nHMS Scimitar, Saladin and 3 armed trawlers (the trawler HMS Lady Madeleine is mentioned in above report).\nBack to Arctic Convoys page\nTo Main Index Page', ""For Posterity's Sake\nA Royal Canadian Navy Historical Project\nWartime Diary of Robert V. RICHMOND, Tel, RCNVR\nCourtesy of Bob Richmond\nHMCS ST FRANCIS I93\nApril 7, 1942 - Heavy fog, fail to locate sub. Turn about for completion of our manouvers. Gun fire from corvette perfect. Ours also. Anti aircraft guns next but torpedo run called off. Steaming in the coast stat signals us bad news. One of our sparkers father has died. Church was one of (5) five survivors off a torpedoed liner and a good head. Corvette bumps into us in fog knocking us over two feet. Use fog lights all night. Corvette DUNVEGAN had joined in manouvers.\nApril 8, 1942 - Weather fair. Slip out tomorrow on convoy at 6 and half knots for 2500 miles. Snow and cold again today. In harbour until tomorrow. War correspondent BOB BROWN on board us, he’s going with us for passage and material for speech. He goes to Nfld Broadcasting Stat where he mentions he is leaving shortly aboard Canadian Warship which is us. 6 ratings also coming as passage. RESTIGOUCHE with Universal Newsman also coming along to take notice of our work. (click here to read the summary of Bob Brown's report of the crossing)\nApril 9, 1942 - Get flu and heavy cough. Zero weather and high wind. At 0630 our sirens whail and we slip to a sea that nearly capsizes us as soon as we hit open sea. Go over on a roll recorded at 57o. Corvette BUCTOUCHE forced to turn back. Mess decks already swamped and everything torn lose. Sea comes in soaking our blankets. Noise of this crate would make tug boat Annie take a stiff drink. Were on rolls now and please believe me!\nWebmaster's note: The convoy was SC.78 from Halifax to Liverpool - Departed Halifax 05 Apr 1942, arrived Liverpool 22 Apr 1942 - RCN escorts from St. John's joined convoy on 09 Apr 1942.\nApril 10, 1942- Gale hammering us at 80 mph ripping the sea lose. Our carley floats have been smashed off and life boats swamped. Stock on (in) galley blown into sea. Steering-gear busted, using emergency. Flares on Carley floats set off at night causing immediate action. Officers, Petty Officers and our own mess decks swamped. Sweet language not being used about these modern destroyers. Officer thrown across bridge on belly. Racket from wind is beyond a humans’ imagination. Four fires break out over our ship from wiring. Generator burns up! Ships condensers out of action! Signalman Murphy nearly taken overboard. Dangerous to venture on upperdeck in gale. At 2100 wind decreases slightly but sea still playful. Out of our whole convoy we find one corvette with Greek steamer searching for others now.\nApril 11, 1942- Heavy seas and cold as hell. Locate another corvette with her asdic gear out of action but guarding a lone merchant ship! Locate lone tanker, pull as close alongside as possible in heavy sea and give her orders. Wind decreasing getting warmer.\nApril 12, 1942 - Fuel rather low after battling gale and searching for scattered convoy. Locate RESTIGOUCHE with 3 merchantmen and later COBALT with 3. Wind dying down-cool-heavy swell. Late afternoon cold-bleak-and blowing up for another storm. Never noticed crew so fed up and disgusted before. Everyone hard to get along with and no one has humor. Our collection of stray merchant ships is now eleven. COBALT, NAPANEE and RESTIGOUCHE with us and all steaming at 6 and half knots except us who steam a little more to buzz around convoy sweeping. One more merchant ship just joined us which makes 12 ships out of 47 at the start! H.M.S. WANDERER joins us but leaves later. Heavy sea and fierce rain storm arises.\nApril 13, 1942 - Weather fine cool wind rough sea. At dawn we find ourselves 5 miles astern of convoy due to poor visibility at night. We have a heavy stbrd list making it damn hard for us to live in comfort, ha- ha as if we could! Our 5th day at sea and our fuel is far from where it should be due of course to hunting convoy in gale. Asdic gear repaired being off. In gale Friday night one of our depth charges skid out of rack into sea. Our drinking water is rationed and to wash you are very lucky!\nApril 14, 1942 - Weather fair, sea fair, convoy in order. Turns black cold and choppy.\nApril 15, 1942 - Snow rain and wind. Get first taste of U boats this trip and fire pattern of charges in heavy sea. Steering gear again out of order, use emergency. Our stern wake tonight is well lit up from phosphorous in water and quite visible. April 16- Weather fine sea flat. Leave convoy at 0820 in morning to run for Irish port-very short of fuel. Get taste of sunshine today for change.\nApril 17, 1942 - Weather fine sea flat. Sight suspicious vessel at 1200 on horizon but turns out to be H.M.S. ROCKINGHAM. About 800 miles of Irish coast. Asdic gear working off and on.\nApril 18, 1942 - Weather fair. Pass US destroyers MAYO - SIMPSON - HMS ACANTHERS - ROSE - MIMOSA - PONTENTILLA - EGLANTANA - schooner READY and US merchant ship IMPULSE at 0630 in morning. SURCOUF just announced as sunk in med. British aircraft looks us over then disappears.\nApril 19, 1942 - Weather fine sea same. Ship very high from water due to fuel shortage. Pass HMS CASTLETON - 3 new yank destroyers escorting liner with catapult plane amidships just steaming into Lough Foyle. Check on our reception of w/t mesgs this trip and find we are a little better than 99%. Arrive at port about 0730 evening where we refuel. This crossing makes my 6th complete crossing of Atlantic [since coming off our Halifax-Newfoundland and Iceland run] Submarine N50 alongside. L.Sig of Canadian corvette alongside us [Barry] dives over to save chum but drowns himself.\nWebmaster's note: The sailor that drowned was L/Sig Harold Lorne Bullock, V2314, RCNVR of HMCS Barrie\nApril 20, 1942 - Go up before skipper to obtain leave and at 1200 I get it. By 1300 I am on my way by train to Belfast where I catch channel steamers for Glasgow Scotland.\nApril 21, 1942 (Tuesday) - Arrive at Glasgow 1100 and take train to Gourich Scotland where I clean up at Bay Hotel. At about 1400 I go to Greenock by train and see Mrs. Mulholland. Quite a treat to see her after such long absence. Go down to Massey’s Shop where I surprise Margaret.\nWebmaster's note: Robert Richmond got married while ashore on leave\nApril 25, 1942 - Arrive Belfast 0700 morning after one grand-glorious binge aboard steamer. Remember nothing until I awake at berthing. Take train to Londonderry and report aboard ship at 1330 being one and a half hours adrift.\nApril 26, 1942 - Spend day in harbour after going shore for church service. Get general razzing and best wishes from boys.\nApril 27, 1942 - Weather fine. Slip out to sea with three corvettes. COBALT - DUNVEGAN and NAPANEE for submarine trials and gunnery shoot. Accuracy of our gunfire nearly perfect at extreme range. Few army fellows aboard this trip for experience. Pull in at 1800. Leaving harbour in evening we full astern and ram into corvette DUNVEGAN just below bridge tearing our port depth charge rack into scrap and putting hole in our stern plate causing us to leak and damaging our rudder. Corvette is ripped open in three places causing injury to two seamen below deck. We attempt to carry out night shoot but steam back in to Londonderry where we wait repairs.\nApril 28, 1942 - Weather fine in harbour. Harbour routine and corvette DUNVEGAN has temporary repairs made to her. Our damage so far not being touched. (They ignore us.)\nApril 29, 1942 - Weather cool with stiff wind. Holes in plate being temporary repaired until we arrive back in home port. Buffer on DUNVEGAN has fractured skull as a result of our accident. Health today is rotten-heavy cough still after seven days and continued headaches. We slip for sea tomorrow with Canada bound convoy.\nApril 30, 1942 - Weather fine. Health poor. Tonsils, sore throat heavy cough still after eight days. Medical officer treating me is going to put me ashore at Nfld RCNH. Condition due to travelling across from Belfast to Glasgow on damn cattle boat. Repair damage to stern plates with cement and steel. Take torn depth charge rack off altogether leaving us short of pattern. Slip downstream at 1700 and refuel at tanker. Slip away from tanker and head around coast of Ireland west to locate our convoy. Sea flat, weather warm. Our escort this trip is HMS BROADWAY - ST. FRANCIS - COBALT - DRUMHELLER - NAPANEE and DUNVEGAN out.\nWebmaster's note: The convoy was ON.90 from Liverpool to Halifax. Departed Liverpool on 28 Apr 1942, arrived Halifax on 15 May 1942.""]"	['<urn:uuid:a9222571-669d-4b87-b1d1-fb14aaf4d24d>', '<urn:uuid:4b64e46c-d292-431c-9028-87af56a48fa4>']	open-ended	with-premise	concise-and-natural	similar-to-document	three-doc	expert	2025-05-12T23:00:04.658849	15	96	3020
38	What happened at Pearl Harbor from both sides' views?	From the Japanese perspective, Air Strike Leader Mitsuo Fuchida led the attack, radioing the code 'to to to to' (meaning 'charge') as bomber regiments from six aircraft carriers approached Pearl Harbor. The attack devastated all eight U.S. Pacific Fleet battleships. From the American perspective, as described by survivor Ewalt Shatz who was just 18 at the time, the attack began as a normal day until he noticed a building exploding on North Ford Island. Despite initial disbelief from his fellow sailors, they sprang into action when Japanese planes appeared. Shatz, who had never fired a real gun before, manned a 50-caliber machine gun and kept shooting to stay calm during the attack.	"['HONOLULU (HawaiiNewsNow) - Ewalt Shatz was 17-years-old when he enlisted in the Navy and just 18 when Pearl Harbor was attacked. 72 years later, he says he remembers most of it clearly.\n""Like Roosevelt said, it\'ll last forever – the story of this attack – and we should never forget it,"" said Shatz.\nShatz says December 7, 1941 started out as any other day aboard the USS Patterson, until he noticed something in the distance.\n""I happened to look over to North [Ford] Island and I saw this building that just kind of – all of a sudden it was just getting bigger and bigger. It was just exploding,"" described Shatz.\nHe says that\'s when he and others spotted a Japanese plane flying above them and sprung into action.\n""Guys were running down there telling everybody to get up out of bed because we were being attacked, and everybody just thought they were joking. They paid no attention \'til the first guns started firing,"" explained Shatz, who was behind one of those 50-caliber machine guns.\n""I\'d never fired that gun until that day,"" recalled Shatz, who says he knew how to operate it because he had done dry runs without ammunition.\n""As long as I kept shooting that machine gun, I was fine. I mean, I was calm, but as soon as I stopped I got scared,"" Shatz described.\nShatz says he simply had reported to his general quarter\'s station, but there were no gunner\'s mates around – so he started firing away. Just last year, during a Pearl Harbor remembrance ceremony at the USS Arizona, he was heralded with shooting down a Japanese plane.\n""Well, I don\'t know about that,"" Shatz said, smiling shyly. ""It\'s possible I did shoot at some planes, but whether I hit any or not that\'s another story. I don\'t take any credit for any of that.""\nShatz humbly shrugs off any suggestion he\'s a World War II hero.\n""No, I\'m not a hero. I was just there. I just happened to be there, just like if you were there – that\'s what you would do. When you have something to do, you just do it and that\'s what I did like everybody else on the ship,"" Shatz said.\nThe 90-year-old says he\'s proud of the men and women serving today, but doesn\'t give much thought to the legacy he played an important role in.\n""I\'m just trying to survive, that\'s about it. As far as leaving anything behind or anybody – no, I\'m just trying to survive,"" Shatz said.\nThis is the fourth year Shatz has traveled to Hawai\'i for the remembrance ceremony at Pearl Harbor, but getting here this time wasn\'t that easy.\nShatz was on his way to Hawai\'i Wednesday when he was bumped from his United Airlines flight from Los Angeles to Honolulu because he was told the plane was overweight.\nAccording to Shatz, he explained to United employees that he was on his way to the USS Arizona memorial to mark the 72nd anniversary of the attack on Pearl Harbor, but still wasn\'t allowed to board.\n""I thought that they just overbooked the deal and they were trying to get rid of a couple of customers and somebody had my seat -- and that\'s what made me mad, because I paid for that seat. I paid for that seat and somebody else is sitting in it,"" said Shatz.\nUnited Airlines issued the following statement to Hawaii News Now:\nThanks for reaching out. Inclement weather in the path of United flight 1226 required the flight to carry additional fuel and, as a result, reduce the number of passengers on board by 41. United agents in Los Angeles rebooked those customers, including Mr. Shatz, on the best available alternate flights on United and other airlines. We look forward to speaking with Mr. Shatz and the other affected customers.\nShatz says he hasn\'t heard from United since, but never plans to travel with them again.\nInstead, he\'s looking forward to Saturday\'s ceremony and says he\'s hoping to run into a buddy he used to play cards with every week for 20 years.\nShatz, who served 6 years in the Navy, says he knows there will be fewer familiar faces in the crowd. According to the association he\'s a member of, 60 fellow Pearl Harbor survivors have died this past year.\n""I think it\'s a great day and I like to come out. I would like to continue it as long as I\'m healthy and I can make it over here,"" Shatz said with a smile.', 'Japan entered the Pacific War in December 1941 (the 16th year of the Showa Era), plunging into a “sea of battles” for three years and nine months. What on earth was the purpose of this war? How did Japan end up headed for catastrophe after an initial series of victories? This section will mainly examine the Battle of Midway, a decisive battle which set the course for Japan’s defeat.\nAmbiguity in war goals\nAdmiral Isoroku Yamamoto was the Commander-in-Chief of the Combined Fleet. (See Footnote.)\nOn December 8, 1941, Air Strike Leader of the aircraft carrier Akagi of the Combined Fleet, Mitsuo Fuchida, radioed the following coded message when his plane started to fly over the U.S. Pacific fleet anchored at Pearl Harbor in Hawaii: “To to to to.... (All units, charge).” “To” stood for totsugeki, meaning “charge.”Attack bomber regiments that took off from six aircraft carriers dealt devastating blows to all eight battleships belonging to the U.S. Pacific Fleet.\nMeanwhile, in operations southwest of Japan, the Twenty-fifth Army commanded by Tomoyuki Yamashita landed on the Malay Peninsula while regiments of Zero fighters based in Taiwan took control of the air over the Philippines within a few days.\nOn December 10, two days after the opening of the Pacific War, more than 80 Japanese attack planes succeeded in sinking the British battleship HMS Prince of Wales and the battle cruiser HMS Repulse just prior to the Battle of Malaya. In February 1942, Japan conquered and occupied the British colony of Singapore. This turn in events was soon followed in March by the evacuation of U.S. General Douglas MacArthur, Commander of U.S. Forces from the Philippines to Australia.\nIndeed, Japan was able to take control of the sea and air in the Western Pacific all at once.\nAfter a series of victories, however, Japan started on the a path to its defeat.\nHere, we would like to examine Japan’s purpose in this war—whether it went to war for “self-preservation and self-defense” or whether it aimed to create the Greater East Asia Co-prosperity Sphere.\nIndeed, the very ambiguity of Japan’s war goals led to the failure of its war management.\nOn November 2, 1941, Prime Minister Hideki Tojo had an audience with Emperor Showa to inform him of the government’s decision to go to war. The Emperor asked Tojo, “What do you think is the just cause of starting this war?” Tojo replied, “We are in the process of studying that, and I will report to you on that soon, Your Majesty.”\nAt the Imperial Headquarters-Government Liaison Conference, where the Imperial policy decision was made to wage war earlier, no substantial discussion was conducted on the “just cause” of going to war. Within the Navy, the dominant opinion was that the purpose of the war should be “self-preservation and self-defense.” The Navy insisted that Japan had no choice but to go to war because it had no oil available after the United States imposed an oil export embargo.\nAlso within the Army, the majority supported the notion that Japan would go to war for “self-preservation and self-defense.”\nBut Tojo, who also held the War Minister portfolio, made it clear that the creation of the Greater East Asia Co-prosperity Sphere was the “basis” of Japan’s policies every time he was briefed on the process of planning national policies. Kenryo Sato, chief of the Military Affairs Section of the War Ministry, constantly insisted that Japan’s goal was to create the Greater East Asia Co-prosperity Sphere while ignoring the arguments for “self-preservation and self-defense.”\n“[The Army is not so enthusiastic about] a war with the United States as the Navy and the government. Nevertheless, the Army is willing to fight a 100-year war if that’s what it takes to complete the Second Sino-Japanese War.” The Army obviously was preoccupied with the war with China.\nFor Tojo and others, the establishment of the Greater East Asia Co-prosperity Sphere through war with the United States and Britain was Japan’s last resort to make China surrender.\nThe Principles for Implementation of the Imperial Policy, which was officially approved at a meeting in the presence of the Emperor on November 5, 1941, stated that “the Empire of Japan decided to wage war against the United States, Britain and the Netherlands to make a breakthrough in the current crisis, realize self-preservation and self-defense and build a new order in the Greater East Asia.”\nHowever, the outline merely put in writing the arguments of both the Army and Navy. Later, the phrase “Greater East Asia Co-prosperity Sphere” developed a life of its own, and the Army and Navy made separate plans for operations without trying to coordinate their moves.\nThe Daikairei 1-go, Navy Operation Order Number 1, issued on November 5 in preparation for attacking the southern areas (Southeast Asia), stated the purpose of the war was for “self-preservation and self-defense.” It was written on the assumption that the war would be short. On the other hand, the Dairikumei 564-go, Army Operation Order Number 564, which cited both “self-preservation and self-defense” and “building a new order in the Greater East Asia” as pretexts for going to war, envisioned a long, drawn-out war toward the goal of the reorganization of the Asian colonies held by Britain, the Netherlands and the United States.\nThe Imperial Rescript on the War issued on December 8, underlined that the war was started for self-preservation and self-defense. On December 10, however, the Imperial Headquarters-Government Liaison Conference decided to call the war the “Greater East Asia War.” According to the Cabinet’s Information Department, the war was so named to indicate its purpose of building a new order in the Greater East Asia.\nThe central concept of “Greater East Asia Co-prosperity Sphere” was to geopolitically divide the world into four major blocs: Greater East Asia, the Americas, the Soviet Union and Europe, with Japan becoming the leader of the Greater East Asia Co-prosperity Sphere. Under this scenario, the Kwantung Army was beefed up in strength to prepare for a possible war with the Soviet Union even after Japan started the war against the United States; the Army also maintained the strength of Japan’s China Expeditionary Army. The Army intended to let the Navy engage in battle with the United States in the Pacific.\nAfter the war ended, Tojo said, “The motivation behind resorting to the use of force was the desire for self-preservation and self-defense.” He also said, “Once the war started, however, we saw to it that the policies aimed at realizing the Greater East Asia Co-prosperity Sphere could be implemented.” Thus the creation of the Greater East Asia Co-prosperity Sphere became Japan’s national policy without due political process.\nMeanwhile, the final goal for Japan’s southward advancement—that was launched to attain self-sufficiency—was the Dutch East Indies (presently Indonesia). The Netherlands, however, was excluded from the enemy countries listed by the Imperial Rescript on the War. The Netherlands was regarded by Japan as a “quasi-enemy,” and the idea was floated to negotiate with the Dutch East Indies once the war started so that Japan would be able to bloodlessly invade and occupy the archipelago under control of the Dutch East India Company. It was not until January 1942 that Japan launched attacks on the Dutch East Indies.\nAt the time the war started, the United States and Britain were alarmed by the possibility of Japan’s gaining access to oil and other resources in the Dutch East Indies by controlling Southeast Asia. They feared Japan was prioritizing gaining a foothold in Asia to fight a long drawn-out war, and avoiding direct confrontation with the United States and Britain and carefully watching their war against Germany in Europe. But Japan defied their assumptions and chose to directly confront the United States and Britain with an attack on Pearl Harbor.\nJapan’s Combined Fleet\nThe core of the Imperial Japanese Navy consisted of two or more fleets. The Combined Fleet was formed before the opening of the first Sino-Japanese War in 1894. Based on the Navy’s tradition of commanders taking the lead and setting the example for subordinates, the Fleet Commander was based aboard his flagship. The Commander-in-Chief was assisted by his Chief of Staff and a large number of staff officers. The Combined Fleet Commander-in-Chief received orders for administration and for operations including battle campaigns from the Chief of the Naval General Staff. But, on many occasions, once the Combined Fleet commander was entrusted with his assignment as the supreme commander of naval operations, he would increasingly clash with the Naval General Staff.']"	['<urn:uuid:96e3b89a-8480-4110-9bf0-9eb066522da3>', '<urn:uuid:68f157d8-b12f-4126-8331-3c4f85740fa3>']	open-ended	with-premise	concise-and-natural	distant-from-document	three-doc	novice	2025-05-12T23:00:04.658849	9	112	2183
39	nft vs dutch bucket hydroponic systems	NFT (Nutrient Film Technique) and Dutch Bucket systems serve different purposes. NFT uses a shallow stream of continuously-flowing nutrient solution and is best for fast-growing crops with compact root systems, like lettuce and herbs. Dutch Bucket systems, which use multiple growing buckets connected by irrigation and drain lines, are specifically designed for large plants that demand high nutrients, such as tomatoes or pumpkins.	['13 DIY Hydroponic Plans Made Easy!\n(This post may include affiliate links. While buying items through these links won’t increase your cost at all, we may receive a small commission that helps keep this site up and running. See our Terms and Conditions page for more details)\nHydroponics is the future of growing food, but it’s hard to know where to start.\nYou can’t buy a hydroponic system at your local garden store, and you don’t want to spend $200 (or more!) on an off-the-shelf solution online. The DIY approach sounds like the right way to go, but what do you need to get started? How do you build one? What equipment does it take?\nHydroponic growing systems work by transferring liquids through grow media to create nutrient-rich environments, often using simple components like plastic buckets and PVC piping. The most common hydroponic plans fall into 6 main categories:\n- The Kratky method\n- Wick hydroponics\n- Deep water culture (DWC)\n- Ebb and flow (Flood and Drain)\n- Nutrient film technique (NFT)\nMultiple variations also exist within these broad categories. Given the number of techniques and ways to modify them, growers in almost any situation should be able to find a set of hydroponic plans that works for them.\nTo give you some ideas on how you can get started, we’ve compiled a list of 13 hydroponic plans. These plans cover all the major categories and a few different ways to customize some of them. And the best part- you can construct them all yourself, saving you some cash and letting you build skills at the same time.\nSo let’s get started!\n1. Mason Jar Hydroponics\nDid you know that hydroponic growing doesn’t have to be a big, complicated, expensive system? Even if all the space you have is a single bright windowsill or a little bit of countertop room, you can have a hydroponic garden that produces fresh greens, lettuce or herbs all year long.\nOne way to do this is by converting a plain old Mason jar (or any glass jar) into a hydroponic bed. This project uses the Kratky method, a passive hydroponic growing technique that doesn’t use any electricity.\nRELATED: Visit our post on what to grow using the Kratky method to get some ideas of crops to grow and learn more details of how the Kratky method works.\n- A glass jar that holds at least 24 ounces\n- Black paint or duct tape (to block light from entering your system)\n- Net cups\n- A sheet of thin plastic (a plastic salad greens container works great)\n- Sharp scissors or a craft knife\n- Nutrient solution\n- Filtered water\nTo read the full instructions and get your own mini garden set up, visit our post on Mason Jar Hydroponics.\n2. Large-scale Kratky Tank\nIf you’ve got a little more space to work with but love the simplicity of the Kratky method, just scale your build up a little bit!\nThis video from MIGardener explains the process of transforming heavy-duty plastic totes into a Kratky hydroponic tank that can house up to 5 plants. Just like with the small-scale Mason jar technique, larger Kratky tanks are ideal for fast-growing plants that have a compact root structure:\n- Greens (spinach, bok choy, kale, etc)\n- Compact herbs (basil, cilantro, parsley, etc)\nIn a larger Kratky tank like this, some growers have had success growing tomatoes and peppers. However, you’ll need to change the water and monitor your tank more closely, so you will lose some of the wonderful set-it-and-forget-it benefits of Kratky growing.\n3. Wick Hydroponics\nAnother passive hydroponic method is a wick system. This method uses a grow bed suspended over a nutrient solution reservoir, with a strand of cotton or another absorbent material stretched between the two. The cotton string uses capillary action to draw moisture from the reservoir and deliver it into the grow bed, keeping your plants watered and nourished.\nJust like the Kratky Mason jar, a wick system is the perfect way to get your feet wet with hydroponics or turn that sunny windowsill into a garden of healthy, fresh foods. No electricity needed!\nThis video from Green Our Planet shows how to upcycle an empty water bottle into a mini hydroponic system. It’s a super fun idea that doesn’t require much time, experience or expensive supplies. It doesn’t get much easier than this when it comes to DIY hydroponic plans:\n4. Deep Water Culture (DWC) Set-Up\nThis is one of the most popular hydroponic growing methods since it strikes an awesome balance between simplicity and the ability to support larger plants, like peppers and tomatoes.\nThe basic elements of a DWC system are:\n- A reservoir tank\n- Air pump\n- Nutrient solution\n- Net cups\n- Growing media\nDWC uses a high water level to submerge the plant roots in nutrient solution, and an air pump/air stone combo produce highly-oxygenated water to further nourish your plants.\nSince it needs electricity to power the air pump, DWC is an active system. And it will need some attention on your part to monitor water conditions, nutrient levels and reservoir refills.\nHere’s what the system looks like:\nRELATED: There are many air pumps and air stones out there to choose from, so how can you know which one you should get? Stop by our posts on the top hydroponic air pumps and air stones for hydroponics to get some guidance!\nAnd they’re not hard to get set up! DWC is perfect for hydroponic growers who are ready to move beyond the Kratky method and onto a more advanced technique.\nThis video from Tikki O does an outstanding job of explaining how to set up a DWC system from inexpensive components, and she also offers some great insights on how to care for your system.\nRELATED: For a more detailed explanation of DWC growing, stop by our post on the basics of Deep Water Culture!\n5. Recirculating Deep Water Culture (RDWC)\nRecirculating Deep Water Culture (RDWC) uses the same basic principles of standard DWC:\n- Submerging plant roots in nutrient solution\n- Using an air pump to oxygenate the solution\nBut when you want to expand your hydroponic system for maximum yield, RDWC is the way to go.\nThis method uses a single batch of nutrient solution to circulate through two or more reservoirs, and a single air pump supplies the oxygen needed. Here’s an illustration that shows the system:\nAnother benefit to a RDWC system is that you only have to monitor/adjust nutrient levels and pH for one water sample to run the entire system. This is as opposed to managing nutrients/pH for each individual DWC reservoir you’re running. Also, with a larger volume of water to work with, you’ve got a little more tolerance for error.\nHere’s a detailed video showing a RDWC build from PA Hydroponics:\n6. Dutch Bucket System\nThe Dutch Bucket (also called Bato Bucket) hydroponic system uses multiple growing buckets connected by a single irrigation line and a single drain line. At set intervals throughout the day, the water pump sends nutrient solution through the irrigation line to each bucket. Then the excess solution is drained away to be recirculated on the next cycle.\nThe Dutch bucket system is a fantastic way to grow large plants that demand a lot of nutrients, like tomatoes or pumpkins. And it does all its work with just the bare minimum of water consumption.\nThis video from MHPGardener does an awesome job explaining what the Dutch bucket system is, why it’s great and how to set one up for yourself:\n7. A-Frame Hydroponic Set-Up\nDo you have a sunny backyard or a large deck/patio to work with? Then an A-frame outdoor hydroponic garden could be a perfect fit.\nLike all A-frame structures, this build type has a wide, stable base, and depending on how you reinforce your build, they can be a sturdy, long-lasting setup for years to come.\nAnother perk to an A-frame build is that you can grow plants on both side of the frame, and you can also take advantage of vertical space as well. So you’ve got the capability to support a large number of plants with a relatively small footprint.\nThis video from Useful Videos shows you from start to finish how to gather the necessary supplies, build your frame and get it up and running.\n8. A Simple Aquaponics System\nAquaponics takes basic hydroponics and goes one step further: Adding a fish tank underneath a hydroponic growing bed to create a self-sustaining ecosystem that produces a harvest of both vegetables and fish.\nSounds expensive and complicated, right? It doesn’t have to be! Here’s a video from Rob Bob’s Aquaponics and Backyard Farm to show you how to get one set up:\nRELATED: We’ve dedicated a post to outlining the differences between aquaponics and hydroponics. Check it out if you want more information!\n9. Vertical Hydroponic Tower\nVertical hydroponics is a fantastic option if you’re extra short on space or just want to maximize your harvest with the smallest footprint possible.\nYou have plenty of pre-made hydroponic garden towers to choose from, but they can be very expensive. Fortunately, you don’t need to shell out a ton of cash to take advantage of this technique if you build one yourself.\nThis video from Chris Oh shows you how to construct your own closed hydroponic garden tower using:\n- PVC piping with some extension and cap pieces\n- Appropriate hand or power tools\n- A small water pump\n- A plastic bucket reservoir\n- 2-inch net cups\n- Grow medium\nSince the plants are packed in pretty densely, it’s ideal for small plants like lettuce, herbs and greens. And it looks great!\n10. Nutrient Film Technique (NFT)\nNutrient film technique (NFT) relies on a balance between nutrients and oxygen. Your plants need each of these elements for healthy, speedy growth, and NFT’s goal is to provide both simultaneously.\nIt does this by suspending plants in a net cup above a shallow stream (or film) of continuously-flowing nutrient solution. This keeps the root tips in a constant flow of nutrient solution while allowing the majority of the root ball to be exposed to oxygen.\nNFT is outstanding for fast-growing crops with compact root systems, like lettuce, leafy greens and bushy herbs. And it only requires a small amount of water and nutrients to run the system since the entire technique is based on a shallow stream of water.\nThis video from Hoocho lays out the technique for building a home NFT system:\n11. Aeroponics Hydroponic System\nIn aeroponics, your plants are suspended in a mostly-empty reservoir and the roots are intermittently or continuously sprayed with a nutrient solution. The idea is to give plants high exposure to oxygen while still providing nutrients, thereby speeding up growth.\nIt’s a highly-efficient technique that uses a minimal amount of water and nutrients. In fact, the astronauts on the International Space Station use an aeroponic system to fresh produce in space! If you’d like to see some photos of the space garden, check out NASA’s website.\nThere are high-pressure and low-pressure aeroponics systems.\nHigh-pressure systems use a powerful air pump to produce a fine nutrient-solution mist. The mist fills the reservoir and nourishes the plants.\nLow-pressure systems use a standard water pump to force water through a multi-outlet manifold in the reservoir. This system produces more of a sprinkler-head water flow that runs over the plants’ roots.\nHere’s a walk-through of a high-pressure aeroponics build from Aeroponic Growers:\nAnd here is a low-pressure aeroponics build, laid out by Peace Love Garden:\nAeroponics is unique in that you can buy a small pre-built system, an Aerogarden, if you’d like to get your feet wet before building your setup. There are Aerogardens of various sizes along with seed kits and other accessories. Stop by the Aerogarden website to take a look at their full offerings if you’re interested!\nRELATED: We’ve gone into much more depth on the differences between aeroponics and hydroponics in a different post. If you’d like to dive into it a little deeper, check out our post.\n12. Ebb and Flow (Also Called Flood and Drain)\nEbb and flow is a hydroponic system that uses a grow bed situated on top of a reservoir, and the two are connected via a set of pipes.\nOne pipe is connected to a water pump which periodically fills the grow bed with nutrient solution. When the pump turns off, the second pipe drains the nutrient solution back into the reservoir to be re-used on the next cycle.\nThis system gives your plants a great balance of oxygen exposure, nutrients and moisture for fast, healthy growth.\nThis video from Hoocho shows you how to set up a small-scale ebb and flow (he calls it flood and drain) system that’s perfect for small spaces:\n13. Drip Hydroponic System\nThis one is a twist on standard hydroponic systems. Instead of providing your plants with nutrient solution from a bottom reservoir, drip systems sit above the growing medium and “rain” down nutrient solution from above.\nYou can use a drip system for any plants you’d like. But if you want to start seeds or clone plants hydroponically, you must have an overhead drip system in place.\nHere’s a great video from FenderGibsonWashburn that lays out how to set up a drip system on a single 5-gallon bucket:\nIf you’d like a larger-scale drip system, Everest Fernandez has an informative (and entertaining!) video that demonstrates the setup:\nFrequently Asked Questions about Hydroponic Plans\nAs you can see from this small sampling, there are many hydroponic plans and growing methods that can suit almost anyone.\nNo matter if all you have room for is a Mason jar or you wind up creating a large-scale hydroponic garden, any food you produce yourself increases your independence and sustainability. And of course, there’s nothing quite like the satisfaction of eating food that you grew yourself!\nWhich of these hydroponic plans are you most interested in? Do you have any other questions or suggestions to offer?\nWe’d love to hear your thoughts in the comments!']	['<urn:uuid:27c2b957-17ca-48f6-9b15-83480a55718d>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T23:00:04.658849	6	63	2362
40	Why do radio experts say there is more background noise in the FM band compared to nearby frequency ranges?	Scientists at USA Digital Radio observed that the noise floor in the FM band is about 10 dB higher than it is on the lower adjacent band or the upper adjacent aviation band. Much of this noise, particularly in urban areas, is manmade, with an additional component resulting from FM co-channel signals adding asynchronously in a manner that resembles Gaussian noise.	['Mention noise interference to radio reception, and most people’s thoughts immediately jump to the AM band. But noise is also an issue for FM reception, the subject of Glynn Walden’s NAB Show presentation, “Noise Contributions to Recovered Analog FM Reception.”\nWalden, now a consultant for Entercom, says FM noise is a different animal.\n‘‘When we listen to AM radio, it’s easy to distinguish between noise and a weak signal. With FM, the limiter in the receiver masks most of the noise, and what’s left might sound like just slightly more background hiss.’’ Although this noise is largely indistinguishable, it nevertheless becomes a limiting factor in FM reception.\nNOT WELL DOCUMENTED\nWhile noise in the AM band has been researched and discussed, much less is really known about how it works on FM.\n‘‘When I first became interested in this topic,’’ says Walden, ‘‘I worked for iBiquity, now Xperi. I did extensive research on existing noise levels in the FM band, and discovered that neither the FCC nor the radio consulting community knew what the noise level was.\n“What we do know is that the total noise is the sum of atmospheric noise, the co-channel interference within some finite distance from the station and the noise figure of the FM receiver.’’\nWalden adds that when evaluating received signal-to-noise ratios in car radios, he generally ignores the noise floor of the receiver, as it is too low to be a contributing factor.\nWe also know that, comparatively speaking, the FM band is increasingly noisy. Walden notes that scientists at iBiquity’s predecessor USA Digital Radio observed that noise floor in the FM band is about 10 dB higher than it is on the lower adjacent band or the upper adjacent aviation band. That observation led them to explore why this is so.”\n“A lot of the noise, particularly in urban areas, is manmade,’’ says Walden, ‘‘and there is an additional component that is simply a result of the number of FM co-channel signals adding asynchronously in a matter that resembles Gaussian noise.’’\nThe session will focus on the effects of IBOC transmissions, at –10, –14 and –20 dBc on the received signal-to-noise ratio at the –60 dBu contour. Field measurements that Walden made were compared with FCC F 50:50 contours and Longley Rice matrixed predicted contours. The actual measured signal levels, at the point identified as the FCC F 50:50 60 dBu contour, were used to calculate the signal-to-noise ratios with IBOC power levels at –20, –14 and –10 dBc.\nHD Radio has been a large part of Walden’s career for the past 25 years. As the senior VP of engineering for CBS Radio, he was responsible for numerous technology rollouts, including the CBS conversion to HD. He helped found USA Digital Radio in 1991; that consortium of broadcasters was responsible for developing in-band, on-channel technology. Before that, he was VP of engineering for Westinghouse Broadcasting’s Group W Radio, where he was involved with numerous improvements to AM and FM radio, including digital broadcasting.\nOver the years, he continued to work on IBOC technology, and currently holds four patents involving digital radio. For those efforts, he received the Westinghouse Signature of Excellence. He was presented with a Lifetime Achievement Award at the 2004 NAB convention for his contributions to the industry and its transition to digital radio.\nWalden’s session “Noise Contributions to Recovered Analog FM Reception” will be presented on Sunday April 8 at 2:10 p.m. in N255.']	['<urn:uuid:65e93fc3-c56e-4245-9a42-44e31026df1e>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T23:00:04.658849	19	61	575
41	global monitoring invasive species methods challenges	Global monitoring of invasive alien species involves three essential variables: alien species occurrence, alien status of species, and species impact on biodiversity and ecosystems. While this monitoring framework exists, there is currently no global, systematic evaluation process in place. The challenges are significant, as invasive species enter through various means, particularly international shipping - including ballast water discharge, shipping containers, and wood packaging materials. Countries must provide some monitoring data themselves, while additional supporting information comes from other sources. The impacts are severe, affecting biological diversity, agricultural productivity, and causing hundreds of billions of dollars in economic losses annually.	['There is a key set of measurements, or essential variables, that are needed to monitor biological invasions. Some data must be provided by countries themselves, whereas a wealth of additional supporting information is available from elsewhere to support national and global monitoring efforts.\nInvasive alien species are well known as being a great risk to the environment and economies. However, there is currently no global, systematic evaluation and monitoring process in place for invasive alien species. Also, the variables and measures for monitoring invasions in a standardised way – needed to underpin a robust observation system – have until now not been fully developed or adopted. Here we present the Essential Biodiversity Variables for global monitoring of biological invasions.\nAn Essential Biodiversity Variable Approach to Monitoring Biological Invasions: Guide for Countries\nThe variables for invasion monitoring fit within the broader framework of Essential Biodiversity Variables (EBVs; Pereira et al., 2013). The EBV framework underpins the development of a global system of harmonized observations on biodiversity change. Similarly, essential variables for invasion monitoring underpin the development of a global system of harmonised observations to assess and track the status of biological invasion (McGeoch et al., 2006; McGeoch et al., 2010).\nThe key set of measurements, or essential variables that are needed to monitor biological invasions are (1) alien species occurrence, the (2) alien status of a species and the (3) impact that a species has on biodiversity and ecosystems (McGeoch et al., in review; GEO BON, 2015).Relationship between Alien species occurrence and EBVs\n“Species occurrence” – including alien species occurrence – belongs to the class of Essential Biodiversity Variables called ‘Species populations’ (Pereira et al. 2013) and forms part of the ‘Species distributions’ EBV within this class. Observation data on alien species occurrence therefore contribute directly to the Species distributions EBV.\nThe status of a species as either alien or native at any particular locality can be considered to be an attribute of the species occurrence variable, or information that is ancillary to species occurrence. It is not itself an Essential Biodiversity Variable (EBV; Pereira et al. 2013). Nonetheless, in the context of biological invasions, this information takes on special significance. It is the essential basis upon which appropriate policy and management decisions are taken. There is also the need to prioritize collection and collation of these data, which are currently inadequate (McGeoch et al. 2012), to support effective invasion management.\nThe ‘alien species impact’ variable is a higher level, composite variable that includes information from multiple Essential Biodiversity Variables (EBVs), such as species traits and ecosystem structure and function. It is not itself an Essential Biodiversity Variable (EBV) (Pereira et al. 2013). Alien species impact, quantified using the standardised method for classifying alien species according to the size of their impacts can be used as an indicator of the change of ecosystems and economies by biological invasion (Hawkins et al., 2015; Blackburn et al., 2014). This variable is therefore not ‘observation’ based but rather the outcome of a semi-quantitative, analytical and stakeholder involved process.\nSome of these data must be provided by countries themselves, whereas a wealth of additional supporting information is available from elsewhere to support national and global monitoring efforts (GEO BON, 2015). Together these essential variables provide the minimum information set needed to assess and track the status of biological invasion (McGeoch et al., in review; GEO BON, 2015). This will enable countries with a range of baseline data adequacies and capacities to report on the status of biological invasions and to work towards achieving Aichi Target 9 of The Convention on Biological Diversity’s Strategic Plan for Biodiversity, 2011-2020.\nBlackburn, T. M. et al. (2014) A unified classification of alien species based on the magnitude of their environmental impacts. PLOS Biology 12 | doi: 10.1371/journal.pbio.1001850\nGEO BON (2015) An Essential Biodiversity Variable Approach to Monitoring Biological Invasions: Guide for Countries. http://www.geobon.org/downloads/biodiversity-monitoring/technical-reports/GEOBON/2015/Monitoring-Biological-Invasions.pdf\nHawkins, C.L., et al. (2015) Framework and guidelines for implementing the proposed IUCN Environmental Impact Classification for Alien Taxa (EICAT). Diversity and Distributions | doi: 10.1111/ddi.12379\nMcGeoch, M.A., Chown, S.L. and Kalwij, J.M. (2006) A global indicator for biological invasion. Conservation Biology 20, 1635 – 1646. | doi: 10.1111/j.1523-1739.2006.00579.x\nMcGeoch, M. A., Butchart, S. H. M., Spear, D., Marais, E., Kleynhans, E. J., Symes, A., Chanson, J. and Hoffman, M. (2010) Global indicators of biological invasion: species numbers, biodiversity impact and policy responses. Diversity & Distribution, 16, 95–108. | doi: 10.1111/j.1472-4642.2009.00633.x\nMcGeoch, M.A., Spear, D., Kleynhans, E.J. and Marais, E. (2012) Uncertainty in invasive alien species listing. Ecological Applications, 22, 959-971. | doi: 10.1890/11-1252.1\nMcGeoch, M. A., Pyšek, P., Jeschke, J. M., Blackburn, T. M., Bacher, S., Capinha, C., Costello, M. J., Fernández, M., Gregory, R. D., Hobern, D., Hui, C., Jetz, W., Kumschick, S., Latombe, G., Pergl, J., Roy, H. E., Scalera, R., Squires, Z. E., Wilson, J. R. U., Winter, M. and Genovesi, P. (in review) A vision for global monitoring of biological invasions.\nPereira, H.M. et al., (2013) Essential Biodiversity Variables, Science 339, 277-278 | doi: 10.1126/science.1229931', 'Murder Hornets, Really?\nAs if 2020 could get any worse, enter the “murder hornet”. Measuring around two inches, the Asian giant hornet is a particularly nasty variety. They have longer stingers than the honeybee and their venom is more toxic – and they can sting repeatedly.\nSadly, these predators are known to decimate honeybee colonies. A cool fact from National Geographic is that Japanese honeybees have learned to protect themselves by surrounding the hornets and cooking them alive through intense flapping that reaches temperatures of over 115 degrees Fahrenheit. Japanese honeybees developed this defense as they co-evolved with the Asian giant hornet in a common native habitat.\nFirst spotted in the state of Washington last December, the Asian giant hornet is thought to have entered the United States through shipping containers. As an invasive alien species to the United States, our honeybees are defenseless against this hornet.\nStowaways and Hitchhikers\nHuman travelers and importers sometimes intentionally transplant species to new locations for food, economic, or environmental purposes, such as for use as biological control agents. Though such approaches should be approved by regulators, illicit trade in plants, seeds and wildlife is a significant problem in global trade. And if you’ve seen those adorable agriculture-sniffing beagles in airports, their job is to catch illegal importation of fruits, vegetables, animal products, soil and samples people try to stow in their personal baggage.\nThese are all vectors for the introduction of invasive alien species, but the much bigger cause of the spread of non-native species throughout the world is international shipping in global trade.\nAlien species hitch rides on agricultural commodities, stow away in shipping containers, get ejected into new waters through the purging of ship ballast water or embed themselves in wood packaging materials, among other modes of unintentional introduction through shipping.\nInvasive weeds threaten fishing livelihoods and trade in aquatic goods.\nNot Wanted: Moths, Mollusks and Beetles\nCargo ships carry water as ballast. At the end of an ocean voyage, freighters jettison the ballast water they took on at the port of origin but in so doing, they can introduce a non-native and sometimes aggressive and invasive aquatic species like zooplankton into new waters at the port of destination. Other unwelcome travelers include aquatic plants, algae or small animals that attach themselves to the hull of a ship.\nGrowing up in Michigan, I recall the invasion of Zebra mussels, native to the Caspian Sea region of Asia, that were transported to the Great Lakes region through ballast water discharge. They quickly spread throughout the United States causing infrastructure damage and economic losses along the way, not to mention being unsightly along the Great Lakes beaches.\nAsian gypsy moths are another example of a pest whose eggs are easily transported in international shipping containers. USDA’s Animal and Plant Health Inspection Service (APHIS) closely monitors incoming vessels from the gypsy moth’s native lands of Japan, China, Korea and Far East Russia. During the annual Asian gypsy moth infestation season, APHIS requires that vessels from high-risk Asian ports bound for U.S. ports provide pre-departure certifications that they are free of the moths. APHIS also inspects for moth egg masses on incoming ships.\nThe Asian longhorn beetle is anathema to many species of broadleaf trees in North America and Europe but is suspected to have been introduced through infested wood packing material made of unprocessed raw wood. Because of the pest risk, international standards have been developed to require heat treatment or fumigation of wood packaging materials used in international trade.\nComing and Going\nInvasive alien species can present a major threat to biological diversity by disturbing native ecosystems and habitats, causing native species to decline. The introduction of foreign pathogens and infectious diseases poses a threat to livestock and human health. Trade is the route by which many invasive alien species are introduced to new environments, and ultimately, the disruption to agricultural productivity can end up costing hundreds of billions of dollars every year through lost opportunities to trade.\nFor example, rats transported on ships are estimated to consume as much as 50 percent of Madagascar’s annual rice production. Fruit fly infestations have spread rapidly in West Africa, devastating mango, citrus and other tropical fruit production for export. The spread of pig disease like the swine flu has required farmers to cull herds and sacrifice exports.\nInvasive grasses can ruin pastures important to animal grazing. Harmful algal blooms like the “red tide” along Florida’s Gulf Coast can deplete oxygen in waters and release toxins that kill fish and make shellfish dangerous to eat. Beyond agriculture, invasive alien species have caused the spread of infectious diseases, hampering business travel and tourism, a key economic driver for many countries – just as we’re witnessing now with COVID-19.\nPrevent Invasive Species without Unduly Restricting Trade\nThe Convention on Biological Diversity requires countries to prevent the introduction of invasive alien species, as feasible and appropriate, and to control or eradicate them if introduced.\nBecause the introduction of alien species occurs largely through trade, the measures governments take to prevent their introduction will – by definition – be trade restrictive to some degree. They often involve controls at ports of entry, appropriate use of quarantine and remediation procedures.\nInternational conventions on biodiversity, plant protection, prevention of animal disease and related trade agreements, preeminently the WTO Agreement on Sanitary and Phytosanitary Measures (SPS), are designed to achieve the objectives of protecting plant, animal and human health without unnecessarily restricting trade. WTO members are encouraged to adopt the guidelines and recommendations developed in global bodies specializing in plant and animal health and to make best efforts to harmonize SPS measures to facilitate trade.\nTo achieve the twin goals of protecting against the introduction of harmful species while facilitating trade, governments need to have in place transparent standards and procedures based on evidence and science-based risk assessments. Regulatory authorities must have expertise and competencies at the borders as well as phytosanitary and veterinary infrastructure such as diagnostic laboratories and proper storage to conduct inspections at entry points. And, ideally, more governments will implement IT systems to ensure that SPS procedures and certifications are integrated with other border systems. Scientific and regulatory cooperation across agencies and across governments is critical for effective monitoring, prevention and control of the global spread of harmful invasive alien species, which is in everyone’s interest.\nWhere Trade and Nature Intersect\nA 2017 study in the journal Nature Communications found that the problem of invasive alien species has continuously increased, with more than a third of all new introductions recorded between 1970 and 2014. Introductions of algae, mollusks and insects in particular increased steeply after 1950, mostly likely as a consequence of the growth of global trade.\nThe arrival of murder hornets on the west coast is just the latest reminder that increased trade volume, changes in trade routes, and the expansion of airport and seaport capacity around the world means having to deal with the unwelcome stowaways in global trade.\nAndrea Durkin is the Editor-in-Chief of TradeVistas and Founder of Sparkplug, LLC. Ms. Durkin previously served as a U.S. Government trade negotiator and has proudly taught international trade policy and negotiations for the last fifteen years as an Adjunct Professor at Georgetown University’s Master of Science in Foreign Service program.']	['<urn:uuid:06ad6990-7536-4d3e-84f9-2487853c3e6a>', '<urn:uuid:d5b309ca-ea3e-4bcf-ba41-44e1fe94b293>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T23:00:04.658849	6	99	2046
42	hiv caused cell death vs infection stages compared	HIV affects CD4+ T cells in different ways across infection stages. In early infection (acute stage occurring 2-4 weeks after infection), HIV multiplies rapidly and destroys CD4+ T cells, with very high virus levels in blood. During chronic infection (second stage), HIV continues multiplying but at very low levels, with gradual CD4+ T cell loss over 10+ years. The cell death occurs through both direct viral infection and indirect mechanisms - in GALT (gut tissue), SIV/HIV triggers an apoptotic pathway leading to death of both infected and uninfected CD4+ T cells. Without treatment, CD4 counts eventually drop below 200 cells/mm3, marking progression to AIDS, the final stage where opportunistic infections occur due to severe immune damage.	"[""In early simian immunodeficiency virus (SIV) and human immunodeficiency virus-1 (HIV-1) infections, gut-associated lymphatic tissue (GALT), the largest component of the lymphoid organ system1, is a principal site of both virus production and depletion of primarily lamina propria memory CD4+ T cells; that is, CD4-expressing T cells that previously encountered antigens and microbes and homed to the lamina propria of GALT2,3,4,5,6,7,8,9. Here, we show that peak virus production in gut tissues of SIV-infected rhesus macaques coincides with peak numbers of infected memory CD4+ T cells. Surprisingly, most of the initially infected memory cells were not, as expected10,11, activated but were instead immunophenotypically ‘resting’ cells that, unlike truly resting cells, but like the first cells mainly infected at other mucosal sites and peripheral lymph nodes12,13, are capable of supporting virus production. In addition to inducing immune activation and thereby providing activated CD4+ T-cell targets to sustain infection, virus production also triggered14 an immunopathologically limiting Fas–Fas-ligand-mediated apoptotic pathway15,16 in lamina propria CD4+ T cells, resulting in their preferential ablation. Thus, SIV exploits a large, resident population of resting memory CD4+ T cells in GALT to produce peak levels of virus that directly (through lytic infection) and indirectly (through apoptosis of infected and uninfected cells) deplete CD4+ T cells in the effector arm of GALT. The scale of this CD4+ T-cell depletion has adverse effects on the immune system of the host, underscoring the importance of developing countermeasures to SIV that are effective before infection of GALT.\nSubscribe to Journal\nGet full journal access for 1 year\nonly $3.90 per issue\nAll prices are NET prices.\nVAT will be added later in the checkout.\nRent or Buy article\nGet time limited or full article access on ReadCube.\nAll prices are NET prices.\nMowat, A. M. & Viney, J. L. The anatomical basis of intestinal immunity. Immunol. Rev. 156, 145–166 (1997)\nVeazey, R. S. et al. Gastrointestinal tract as a major site of CD4+ T cell depletion and viral replication in SIV infection. Science 280, 427–431 (1998)\nKewenig, S. et al. Rapid CD4+ T cell depletion and enteropathy in simian immunodeficiency virus infected rhesus macaques. Gastroenterology 116, 1115–1123 (1999)\nSchneider, T. et al. Loss of CD4 T lymphocytes in patients infected with human immunodeficiency virus type 1 is more pronounced in the duodenal mucosa than in the peripheral blood. Gut 37, 524–529 (1995)\nMehandru, S. et al. Primary HIV-1 infection is associated with preferential depletion of CD4+ T lymphocytes from effector sites in the gastrointestinal tract. J. Exp. Med. 200, 761–770 (2004)\nBrenchley, J. M. et al. CD4+ T cell depletion during all stages of HIV disease occurs predominantly in the gastrointestinal tract. J. Exp. Med. 200, 749–759 (2004)\nGuadalupe, M. et al. Severe CD4+ T-cell depletion in gut lymphoid tissue during primary human immunodeficiency virus type 1 infection and substantial delay in restoration following highly active antiretroviral therapy. J. Virol. 77, 11708–11717 (2003)\nClayton, F., Snow, G., Reka, S. & Kotler, D. P. Selective depletion of rectal lamina propria rather than lymphoid aggregate CD4 lymphocytes in HIV infection. Clin. Exp. Immunol. 107, 288–292 (1997)\nLim, S. G. et al. Loss of mucosal CD4 lymphocytes is an early feature of HIV infection. Clin. Exp. Immunol. 92, 448–454 (1993)\nVeazey, R. & Lackner, A. The mucosal immune system and HIV-1 infection. AIDS Rev. 5, 245–252 (2003)\nVeazey, R. et al. Identifying the target cell in primary simian immunodeficiency virus infection: Highly activated memory CD4+ T cells are rapidly eliminated in early SIV infection in vivo . J. Virol. 74, 57–64 (2000)\nZhang, Z.-Q. et al. Sexual transmission and propagation of simian and human immunodeficiency viruses in two distinguishable populations of CD4+ T cells. Science 286, 1353–1357 (1999)\nZhang, Z.-Q. et al. Roles of substrate availability and infection of resting and activated CD4+ T cells in transmission and acute simian immunodeficiency virus infection. Proc. Natl Acad. Sci. USA 101, 5640–5645 (2004)\nBoirivant, M. et al. HIV-1 gp120 accelerates Fas-mediated activation-induced human lamina propria T cell apoptosis. J. Clin. Immunol. 18, 39–47 (1998)\nBoirivant, M. et al. Stimulated human lamina propria T cells manifest enhanced Fas-mediated apoptosis. J. Clin. Invest. 98, 2616–2622 (1996)\nDe Maria, R. et al. Functional expression of Fas and Fas ligand on human gut lamina propria T lymphocytes. A potential role for the acidic sphingomyelinase pathway in normal immunoregulation. J. Clin. Invest. 97, 316–322 (1996)\nTesti, R., Phillips, J. H. & Lanier, L. L. Constitutive expression of a phosphorylated activation antigen (Leu 23) by CD3 bright human thymocytes. J. Immunol. 141, 2557–2563 (1988)\nTesti, R., Phillips, J. H. & Lanier, L. L. Leu 23 induction as an early marker of functional CD3/T cell antigen receptor triggering. Requirement for receptor cross-linking, prolonged elevation of intracellular [Ca + + ] and stimulation of protein kinase C. J. Immunol. 142, 1854–1860 (1989)\nPhillips, A. N. Reduction of HIV concentration during acute infection: independence from a specific immune response. Science 271, 497–499 (1996)\nReilly, C. S. et al. The clustering of SIV infected cells in lymphatic tissue. J. Am. Stat. Assoc. 97, 943–954 (2002)\nMothe, B. R. et al. Dominance of CD8 responses specific for epitopes bound by a single major histocompatibility complex class I molecule during the acute phase of viral infection. J. Virol. 76, 875–884 (2002)\nReynolds, M. R. et al. The CD8+ lymphocyte response to major immunodominant epitopes after vaginal exposure to SIV: too late and too little. J. Virol (in the press)\nPope, M. & Haase, A. T. Transmission, acute HIV-1 infection and the quest for effective vaccines, microbicides and other strategies to prevent infection. Nature Med. 9, 847–852 (2003)\nMiller, C. J. et al. Intravaginal inoculation of rhesus macaques with cell-free simian immunodeficiency virus results in persistent or transient viremia. J. Virol. 68, 6391–6400 (1994)\nPoppema, S., Lai, R. & Visser, L. Monoclonal antibody OPD4 is reactive with CD45RO, but differs from UCHL1 by the absence of monocyte reactivity. Am. J. Pathol. 139, 725–729 (1991)\nMiller, C. J. et al. Propagation and dissemination of infection after vaginal transmission of SIV. J. Virol (submitted)\nWe thank R. Veazey, L. Picker, J. Lifson, D. Douek and M. Roederer for discussions; L. Compton, D. Lu, B. Vang, K. Bost and R. Dizon of the Immunology Core Laboratory and Primate Services Unit at the CNPRC for technical assistance; and T. Leonard and C. O'Neill for help in preparing the figures and manuscript. This work was supported by grants from the National Institute of Allergy and Infectious Diseases and from the National Center for Research Resources.\nThe authors declare that they have no competing financial interests.\nAbout this article\nCite this article\nLi, Q., Duan, L., Estes, J. et al. Peak SIV replication in resting memory CD4+ T cells depletes gut lamina propria CD4+ T cells. Nature 434, 1148–1152 (2005). https://doi.org/10.1038/nature03513\nIntestinal intraepithelial lymphocytes: Maintainers of intestinal immune tolerance and regulators of intestinal immunity\nJournal of Leukocyte Biology (2020)\nAIDS Research and Human Retroviruses (2020)\nCurrent Treatment Options in Infectious Diseases (2020)\nAfrican green monkeys avoid SIV disease progression by preventing intestinal dysfunction and maintaining mucosal barrier integrity\nPLOS Pathogens (2020)\nInnate Lymphoid Cells: Their Contributions to Gastrointestinal Tissue Homeostasis and HIV/SIV Disease Pathology\nCurrent HIV/AIDS Reports (2019)"", 'Professional. Discreet . Friendly\nCD4 AND HIV\nFor the past three decades or so, we’ve all heard the same thing about HIV; that it affects the immune system and leads to an infected person contracting illnesses that would usually be thwarted by a healthy immune system.\nHIV is a unique human RNA virus, capable of infecting cells of the immune system. Specifically, HIV targets T helper cells (CD4 cells), leading to the eventual death of the cell.\nCD4 cells play a vital role in the regulation of immune responses to invading microorganisms. The cells act as an army against infective pathogens. In an untreated person with HIV, 10 billion to 100 billion new viruses are produced per day. This massive viral replication leads to a progressive loss of CD4 cells over a period of several years to as long as a decade. And the destruction of CD4 cells causes a patient vulnerable to unusual opportunistic infections (OIs) that are rarely seen in healthy humans.\nMost patients who die from AIDS succumb to one or more OIs.\nThe main aim of HIV management is prevention, namely prevention of viral replication and prevention of HIV opportunistic infections. This is usually done with the administration of anti- retroviral drugs and prophylactic antibiotics/antifungals if needed.\nAlthough a patient’s general condition is a good indicator of how far an HIV infection has progressed, a more definitive method is to regularly monitor blood CD4 counts. This way, the doctor is able to decide whether their treatment is adequate.\nWHAT IS CD4??\nCD4 is commonly referred to as a type of white blood cell that is an essential part of the human immune system. They are also known as CD4+ T helper cells, T-helper cells or T4 cells. They are made in the thymus gland and they circulate throughout the body and lymphatic system.\nThey are called helper cells because one of their main roles is to send signals to other types of immune cells, including CD8 killer cells, which then destroy the infectious particle.\nIf CD4 cells start depleting in number, for example in untreated HIV infection, or following immune suppression prior to a transplant, our body is left vulnerable to a wide range of infections that it would otherwise have been able to fight.\nWHY MONITORING CD4 COUNT IS IMPORTANT?\nThe CD4 count serves as the major laboratory indicator of our immune system functions. It is also the strongest predictor of subsequent disease progression and survival according to findings from clinical trials and cohort studies. As HIV infection progresses in different stages, the number of these cells declines. When the CD4 count drops below 200, a person is diagnosed with AIDS. A normal range for CD4 cells is about 500-1,500. Usually, the CD4 cell count increases when the HIV virus is controlled with effective HIV treatment. In fact, studies have found that patients who adhere to regular treatments can achieve a life span similar to persons who have not been infected with HIV.\nThere are three stages of HIV infection:\nAcute HIV Infection\nAcute HIV infection is the earliest stage of HIV infection, and it generally develops within 2 to 4 weeks after infection with HIV. During this time, some people have flu-like symptoms, such as fever, headache, and rash. In the acute stage of infection, HIV multiplies rapidly and spreads throughout the body. The virus attacks and destroys the infection-fighting CD4 cells of the immune system. During the acute HIV infection stage, the level of HIV in the blood is very high, which greatly increases the risk of HIV transmission.\nChronic HIV Infection\nThe second stage of HIV infection is chronic HIV infection (also called asymptomatic HIV infection or clinical latency). During this stage of the disease, HIV continues to multiply in the body but at very low levels. People with chronic HIV infection may not have any HIV-related symptoms, but they can still spread HIV to others. Without treatment with HIV medicines, chronic HIV infection usually advances to AIDS in 10 years or longer, though in some people it may advance faster.\nAIDS is the final, most severe stage of HIV infection. Because HIV has severely damaged the immune system, the body can’t fight off opportunistic infections. (Opportunistic infections are infections and infection-related cancers that occur more frequently or are more severe in people with weakened immune systems than in people with healthy immune systems.) People with HIV are diagnosed with AIDS if they have a CD4 count of fewer than 200 cells/mm3 or if they have certain opportunistic infections. Without treatment, people with AIDS typically survive for about 3 years.\nCONFIRMATORY HIV TESTING\nWestern Blot Assay is confirmatory testing for HIV I/II antibody. A positive Western Blot is generally regarded as conclusive for HIV infection. However, if the test turns out negative, it does not rule out the possibility of HIV infection as there is an interval between HIV infection and the appearance of measurable anti-HIV antibodies or so-called the ""window period"". The test should then be repeated within 2-3 weeks time.\nRead: STD & HIV Window Period\nThe ELISA test also called the EIA for enzyme immunoassay, is used to detect the HIV antibody. It checks for certain proteins that the body makes in response to HIV. An enzyme will bind with the HIV antigen or HIV antibody if the blood sample contains the HIV virus.\nTREATMENT MONITORING TESTS\nHIV -1 Viral Load Assay is a test done to measure the quantity of HIV-1 virus within the plasma. This test is usually done after the confirmation test turns out positive, to review the prognosis of the disease and to aid in assessing viral response to the antiviral treatment as measured by the levels.\nCD4 T lymphocyte (CD4) count with HIV-1 viral load assay are the two surrogate markers of antiretroviral treatment (ART) responses and HIV disease progression that have been used for decades to manage and monitor HIV infection. Measurement of CD4 is useful before the initiation of ART. The CD4 cell count provides information on the overall immune function of a person with HIV along with other general test and examinations.\nWHAT HAPPENS DURING A CD4 COUNT TEST?\nBasically, CD4 tests measure the number of these cells in the blood and, in conjunction with an HIV viral load test, help assess the status of the immune system in a person who has been diagnosed with HIV infection. Before your test, be sure to tell your doctor about all the medications you’re taking. This includes any over-the-counter (OTC) and prescription medications or herbal supplements.\nCertain medications can impact your T cell count, which will alter the results of your test. Your doctor may ask you to stop taking your medications for a little while, or they may change the dosage before your test.\nDuring your visit to the clinic, your doctor will take a blood sample from a vein in your arm, using a small needle. After the needle is inserted, a small amount of blood will be collected into a test tube or vial. This usually takes less than five minutes.\nUse of CD4 Count for Initial Assessment.\nThe CD4 count is one of the most important factors in determining the urgency of ART initiation and the need for prophylaxis for opportunistic infections. All patients at entry into care should have a baseline CD4 count.\nUse of CD4 Count for Monitoring Therapeutic Response.\nAn adequate CD4 response for most patients on therapy is defined as an increase in CD4 count in the range of 50 to 150 cells/mm3 per year, generally with an accelerated response in the first 3 months of treatment. Subsequent increases in patients with good virologic control average approximately 50 to 100 cells/mm3 per year until a steady state level is reached. Patients who initiate therapy with a low CD4 count4 or at an older age may have a lesser increase in their counts despite virologic suppression.\nHOW OFTEN IS THIS DONE?\nWhen you are first diagnosed with HIV infection to get a baseline assessment of your immune system; about every 3 to 6 months after starting antiretroviral therapy (ART) to check whether you are responding to treatment; if you have responded well to treatment, you may then be tested every 6 to 12 months. If you change antiretroviral therapy or if your viral load is consistently above an acceptable level despite being on ART, you may need to be tested every 3 to 6 months.\nWe at DTAP have a quick, easy, reliable and accurate way of measuring CD4 levels using a rapid detection method that we perform in the clinic.\nThis method garners results in a matter of 30 minutes instead of waiting days to weeks for CD4 counts.\nWhile there is no cure for HIV, there are different medicines you can take to protect your immune system and can prevent you from getting AIDS. Today, people with HIV are living longer, with a better quality of life than ever before. If you are living/ diagnosed with HIV, it\'s important to see your health care provider regularly and get your CD4 levels monitored to ensure optimal treatment.\nHIV & STD Medical Articles\nWhat Happens If I Forget To Take My HIV Medication?\nFor many people, remembering to take a daily medication can be the difference between life and death, especially if you...']"	['<urn:uuid:444bec59-e2cb-42df-bf00-2cc6d70985be>', '<urn:uuid:824d52b0-9609-4713-a5db-02d2e1d39fd8>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	novice	2025-05-12T23:00:04.658849	8	116	2748
43	How can medications and lifestyle changes treat Meniere's symptoms?	The vertigo in Meniere's can be managed through reductions in salt, caffeine, alcohol, nicotine and stress. Doctors often prescribe diuretics to reduce fluid retention, and antiemetics can help suppress vertigo. Corticosteroids can reduce inflammation and ease fluid pressure, while gentamicin injections can help reduce vertigo severity and duration. Additionally, many doctors prescribe diuretics to assist in reducing fluid retention in the inner ear, as the endolymph fluids naturally contain virtually no sodium.	"[""On the site\nwhat is it?\nTreatment (medical treatments not holistic)Diet & stress\nFor many people the vertigo in Meniere's can be managed by changes in diet, lifestyle and when necessary, with medicine. Reductions in salt, caffeine, alcohol, nicotine and stress may be all that is needed to dramatically reduce the volume and intensity of attacks and symptoms. For most the number one priority is managing the vertigo and as such treatments tend to focus on this.\nThe inner ear endolymph fluids contain virtually no sodium. Controlling salt in your diet should assist regulation of this, preventing swelling (hydrops) in the inner ear and subsequent attacks. Many doctors will prescribe a diuretic to assists the reduction of fluid retention.\nMany sufferers report that stress can exacerbate symptoms or even trigger vertigo attacks. It is known that many changes occur in the body when experiencing stress, which no doubt can influence all parts of the body including the inner ear.\nClick here for a comprehensive list of medicines used in USA.\nA vestibular retraining program may benefit those with ongoing dizziness. Cawthorne Cooksey exercises for example, are aimed at rehabilitating the balance system. Relaxation exercises may also assist patients with stress and there is good success in response to tinnitus. These exercises will not control vertigo.\nThis device works with similar principals to the above. A small ear piece placed in the ear, then a small portable device will send a pressure wave through a grommet. Click here for further information.\nEndolymphatic Sac Surgery / Shunt\nThe endolymphatic sac is located below the balance organ. This operation decompresses and opens the endolymphatic sac opening. The theory is that the sac duct may have become blocked, but once widened, can then absorb some of the endolymphatic fluid again. Occasionally a small shunt is placed to open the duct between the sac and the balance organ to allow endolymph to drain. Success in controlling vertigo is thought to be around 70-85% and the risk to hearing caused by the procedure is minimal, however their is the potential for blockages to reoccur. Some of the latest research suggests that it is better to remove the visible portion of the sac altogether. This operation can be performed in bilateral Meniere's.\nGentamicin is an antibiotic which is toxic to the cells in the inner ear, thus injecting into the inner ear will stop their functionality. In time, the healthy ear should do its best to compensate for loss of the affected ear, restoring balance. Injections are usually given in low doses on a weekly basis until success. Risk to hearing being affected by the treatment is 10% and can only be used in unilateral Meniere's (only one ear affected). more info\nSurgical labryinthectomy is used to treat Meniere's far less than the above. The operation will completely destroy the balance organ on the affected side and hearing is completely lost which is why this operation is usually only considered where hearing loss is already significant. In time, the healthy ear should do its best to compensate for loss of the affected ear restoring balance. This can only be used in unilateral Meniere's (only one ear affected).\nVestibular Nerve Section\nThe vestibular nerve carries the balance signals from the inner ear, to the brain. This operation cuts the nerve, therefore the signals do not reach the brain. The inner ear continues to function thus hearing will continue to be lost as the disease progresses. In time, the healthy ear should do its best to compensate for loss of the affected ear restoring balance. This can only be used in unilateral Meniere's (only one ear affected).\nTreatment for those with bilateral Meniere's is a challenge to the medical profession, especially when the above treatments have already been carried out. Research shows that Meniere's affected ears still have some small balance function even in later stages of the disease so destroying these early on could be problematic if bilateral Meniere's is later diagnosed. Endolymphatic sac surgery can be considered and Intratympanic steroid injections to reduce inflammation are possible."", 'There ear is divided into three parts – the outer, middle and inner ear. The first two parts are mainly responsible for hearing while the inner ear is responsible for both hearing and balance. Conditions affecting the inner ear will therefore lead to symptoms such as hearing loss, tinnitus and lead to balance and movement-related symptoms like vertigo. Meniere’s disease is one such condition.\nWhat is Meniere’s disease?\nMeniere’s disease is a condition where there is increased pressure within the inner ear. Since the inner ear is responsible for both the sense of hearing and for balance, Meniere’s disease will often affect these functions. Therefore symptoms such as hearing loss, tinnitus (ringing in the ears) and dizziness are often present in Meniere’s disease. It may be of varying intensity and fluctuate during the course of the disease.\nOverall Meniere’s disease is not very common but is not a rare condition either. It is known to affect about 1 in 100 people but the prevalence could be much higher as the condition may be under reported. Although Meniere’s disease has been reported in children as young as 4 years, it is most likely to occur in the 40 to 60 year age group. It tends to be more common among females.\nCauses of Meniere’s Disease\nThe cause of Meniere’s disease is not exactly known. In fact the medical term for Meniere’s disease is idiopathic endolymphatic hydrops, and the term idiopathic means of unknown cause. To some extent the exact mechanism is also not clearly understood but the condition appears to be due to increased fluid pressure within the structure of the inner ears.\nThe labyrinth is a hollow cavity within the ear which is filled with fluid. It has different parts, one of which is the cochlea and the other being the vestibule. The cochlea is the hearing apparatus while the vestibule is reponsible for balance. There are two types of fluid within the labyrinth – endolymph and perilymph – which are separate by a membrane.\nIn Meniere’s disease, it appears that there is an increase of endolymph which raises the pressure within the labyrinth of the inner ear. It is unclear as to whether this increase of fluid is the cause of the disease or occurs as a result of the disease. In fact, there are patients who have increased fluid pressure within the inner ear but do not experience any of the symptoms that are characteristic of Meniere’s disease.\nThe increased fluid pressure appears to cause disturbances within the organs of hearing and balance. Although the exact reason for this increase in fluid is unclear in Meniere’s disease, it can be occur with infections, allergies and trauma. Blocked drainage of fluid, immune disturbances, genetic factors, head injury and migraines may also play a role in a rise in fluid pressure within the ear but this may not always lead to Meniere’s disease.\nSigns and Symptoms\nThe symptoms of Meniere’s disease are usually episodic. These episodes come and go with symptom-free gaps that may last for months or even years. The three main symptoms of Meniere’s disease are vertigo, hearing loss and tinnitus. Other symptoms like a feeling of fullness in the ear may also be present. The auditory symptoms are usually one-sided but can sometimes affect both sides (bilateral).\nVertigo is a sensation of moving despite being stationary. Most people describe it as a spinning sensation. With Meniere’s disease, vertigo can come and go suddenly. It may last anywhere from a few minutes to several hours. These episodes of vertigo must last for at least 20 minutes for a diagnosis to be made. It may be accompanied by nausea and vomiting.\nThe hearing loss in Meniere’s disease may vary in intensity and duration. It is sensorineural hearing loss and usually only affects one ear. However, it can occur in both ears. The hearing loss is not always obvious. Often it is only the ability to hear lower frequencies that are normally audible to humans which is lost. In the long term there tends to be some degree of permanent hearing loss.\nTinnitus is the perception of sound despite an external source. It is often described as a ringing sensation but may also be a swooshing, roaring, whistling or buzzing sound in the ears. This abnormality usually correlates with the hearing loss and can be continuous or intermittent.\nTreatment of Meniere’s Disease\nAlthough there is no cure for Meniere’s disease, treatment can help control the symptoms. Some drugs may also reduce the frequency of Meniere’s disease attacks. In severe cases that do not respond to medication and other non-invasive techniques, surgery may be considered.\n- Antiemetics typically used for nausea and vomiting can help to suppress the vertigo. Drugs used for motion sickness may also be helpful in this regard.\n- Diuretics increase the passing of water through urine and can be helpful in reducing the fluid pressure within the inner ear. These drugs may be used for long term management.\n- Corticosteroids reduce inflammation and appear to help ease the fluid pressure in the inner ear. It does not resolve the condition but helps reduce the severity of symptoms.\n- Gentamicin is an antibiotic that may help reduce the severity and duration of vertigo. It may be administered as an injection.\n- Endolymphatic sac procedure helps reduce the pressure (decompression) within the inner ear and a shunt may also be done to drain excess fluid.\n- Vestibular nerve section involves cutting the vestibular nerve which relays signals about movement and balance back to the brain. This helps with reducing vertigo.\n- Labyrinthectomy involves removing a portion of the inner ear but impacts on hearing and will only be considered when there is complete hearing loss, or almost complete loss, in the affected ear.\nOther therapies focus on helping a person suffering with Meniere’s disease to cope with the symptoms of the disease. This may include:\n- Hearing aids increase the volume of environmental sound to help improve impaired hearing.\n- Meniett device to apply pressure to the middle ear which aids with fluid exchange in the inner ear. It helps with vertigo.']"	['<urn:uuid:ae6853c8-47ab-4828-9bfa-6e5561547dff>', '<urn:uuid:f8798068-6fa4-46a5-b439-9359d2bea29c>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T23:00:04.658849	9	72	1702
44	What preservation tools existed historically and today?	Historically, preservation occurred naturally in some cases, like the Herculaneum scrolls preserved by volcanic ash from Mt. Vesuvius. Modern preservation uses advanced tools including environmental monitoring systems that track temperature and humidity, bioluminescence tests for contamination detection, and non-destructive analysis techniques in state-of-the-art conservation laboratories.	"['When you start to think about the oldest books that a library might hold, there are any number of rabbit holes you can fall down. What’s the oldest book in any particular city? What’s the oldest book in the world? Well, what do you mean by “book”? The oldest written text? The oldest manuscript? The oldest printed material? The oldest bound book?\nLibrarians take these kinds of questions very seriously, so when Atlas Obscura contacted some of our favorite libraries to ask about the oldest books in their collections, we were treated to a wealth of information about the treasures they hold.\nThe New York Public Library, for instance, has not only cuneiform tablets and ninth-century gospels, but also a Gutenberg Bible and a copy of The Bay Psalm Book, one of the oldest books printed in America. In addition to its own cuneiform tablets and Gutenberg Bible, the Library of Congress holds one of the oldest examples of printing in the world, passages from a Buddhist sutra, printed in A.D. 770, as well as a medieval manuscript from 1150, delightfully titled Exposicio Mistica Super Exod.\nIn the history of writing, bound books as we know them today arrive fairly late, so there are no actual “books” on this list. Instead, this is a wondrous collection of illuminated manuscripts, papyrus scrolls, and clay tablets. Some of these items you can even see in person, if you pay a visit.\nApicius, De re culinaria\nCreated: A.D. 830, Germany\nSynopsis: The earliest surviving cookbook in the West, this Latin manuscript contains recipes that date all the way back to the fourth century B.C. These were recipes meant for average Roman households, although they included non-native spices that would have had to travel far to reach the Mediterranean. Some of the highlights, according to the library, are “roast lamb with coriander, deep-fried honey fritters, and cucumber with mint dressing.”\nProvenance: The manuscript was originally created in a German monastery in the ninth century. It was later held in Rome and then Paris, where it was sold in 1824 to noted bibliomaniac Sir Thomas Phillipps. From Phillipps, it went to Margaret Barclay Wilson, a teacher and librarian, who donated her extensive collection of cookbooks and medicinal recipe books to the New York Academy of Medicine in 1929.\nConstantinus Africanus, Viaticum\nCreated: No later than 1244, Italy\nSynopsis: In the 10th century A.D., Ibn al-Jazzar, an Muslim physician, wrote a book titled Provisions for the Traveler and the Nourishment of the Settled, a compendium of the medical knowledge of the day, which focused on the interaction of humours and elements in the human body. A few decades later, Constantinus traveled from North Africa to a monastery in southern Italy, where he adapted and translated Ibn al-Jazzar’s work into Latin.\nProvenance: A reader in 1429 made notes in the margins. By the 16th or 17th century the book had a new and still unidentified owner, who added chapter titles. It later passed through the hands of booksellers in Lugano, Switzerland, to the College of Physicians library.\nCreated: Before A.D. 79, Herculaneum, Italy\nSynopsis: When Mt. Vesuvius erupted in the first century A.D., it covered the town of Herculaneum in ash, gas, and other volcanic material that preserved organic matter for hundreds of years. These carbonized papyrus scrolls came from the town’s library. Archaeologists have struggled to find a way to read the damaged scrolls, so there’s little known about their contents—though progress is being made with advanced imaging techniques.\nProvenance: These scrolls were discovered in Herculaneum in 1750s. In 1810, George, Prince of Wales, received 18 of them and presented four to the library in Oxford as a gift.\nCreated: A.D. 330–60, possibly Rome, Italy\nSynopsis: A handwritten version of a Greek Bible, the codex is one of the oldest copies known today, and the oldest surviving version of the complete New Testament.\nProvenance: The codex was kept in St. Catherine’s Monastery in Egypt before the wider scholarly world came to know about it in 1844, when a scholar removed 43 folios from the monastery library. Parts of the codex are now kept at four different libraries, but St. Catherine’s still holds a small fraction of the work—12 pages and 24 fragments.\nChicago Botanic Garden\nTheophrastus, Historia Plantarum\nCreated: 1483, Treviso, Italy\nSynopsis: Back in the third century B.C., Theophrastus, one of the first botanists of the Western world, set out to catalogue the the plants of ancient Greece and created the first known classifications of plants in his part of the world. He covered a range of trees, shurbs, dwarf shrubs, and herbs and examined how they grew and were used in his own time.\nProvenance: This Latin translation of the original text was printed in northeast Italy in the 15th century, making it an incunable, or a printed book that predates 1501. In 1664, the then-owner added a title page and doodles that Leora Siegel, the library’s senior director, describes as “a woman’s anatomy, but poorly done.” In the early 20th century, it was owned by an orchid specialist who donated it to the Massachusetts Horticultural Society. The Botanic Garden purchased the society’s rare book collection in 2002.\nAmerican Museum of Natural History\nAlbertus Magnus, De animalibus\nCreated: 1495, Venice, Italy\nSynopsis: Albertus Magnus spent his life studying and commenting on the works of Aristotle. Without his work, much less knowledge of the Greek philosopher would have made it to future generations of scholars. Albertus, who died in 1280 and was later canonized, wrote widely about the scientific and natural worlds. This volume collects his work on the animal kingdom.\nProvenance: By the 19th century, the book had entered the holdings of the collection now named the Berlin State Library. The natural history museum in New York bought the book in 1923 from Paul Gottschalk, a German book dealer.\nLibrary of Congress\nCuneiform Accounting Tablets\nCreated: 2050 B.C., Sumeria\nSynopsis: The Library of Congress’ oldest written material dates far back, beyond the founding of this nation, to more than 4,000 years ago. The collection of cuneiform tablets dates back to the reign of Gudea of Lagash, in the 2100s B.C. The tablets recorded bills of sale, receipts, ledgers, and other accounting tasks.\nProvenance: In 1929, Kirkor Minassian, a dealer of Islamic and Near Eastern art, visited the library and was inspired to send a suite of gifts, which included these tablets.\nFolger Shakespeare Library\nCreated: Compiled 1325, England\nSynopsis: The Magna Carta was the first English statute, but it wasn’t given that name until after 1217, when it started being issued along with the Charter of Forests. To distinguish the original statutes from the forestry code, they were given the name the Great Charter, or the Magna Carta. This copy is in Anglo-Norman, translated from Latin.\nProvenance: A note in one leaf indicates that the book was given as a gift in 1821. Henry Clay Folger acquired it from an “E. Williams of Hove” in 1922.\nEgyptian Artifact Album\nCreated: circa 1500 B.C., Thebes, Egypt\nSynopsis: This book isn’t exactly the oldest book in the Athenaeum’s collection. It was bound in the 1910s and is “more an album of archaeological artifacts than a rare book in the traditional sense,” the library writes. It contains three shrine hangings describing the worship of the goddess Hathor, and a series of inscribed mummy bandages, which are some of the oldest examples of painting on cloth.\nProvenance: The artifacts were discovered in 1905 by archaeologist Robert de Rustafjaell, and a bindery in London collected them into a book sometime between 1913 and 1916. The library bought the album that year in Boston, from Goodspeed’s Book Shop.\nBook of the Dead\nCreated: 15 century B.C., Egypt\nSynopsis: Sesostris was a cattle counter and writer in ancient Egypt, and he was well-off enough to own his own copy of the Book of the Dead. In contrast with its name, the book is actually a papyrus scroll, 20-foot long, that contains magical spells to help the recently deceased make their way to the afterlife.\nProvenance: The Court Library, the predecessor to the National Library, acquired the papyrus collection of Archduke Rainer in 1899.\nCreated: Circa ninth century, Brittany, France\nSynopsis: Like the Library of Congress, the New York Public Library’s oldest written material is a collection of cuneiform tablets. Its oldest manuscript, however, is a ninth-century copy of the Gospels, made in a French monastery, but showing Celtic influence in its imagery.\nProvenance: The manuscript once belonged to the Marquis of Blandford, fifth Duke of Marlborough. It was later owned by Sir Thomas Phillipps (who also owned the New York Academy of Medicine’s copy of Apicius’s De re culinaria), who passed it to his grandson, FitzRoy Fenwick. A.S.W. Rosenbach bought it in 1926, and its last private owner was one Edward S. Harkness, who donated it to the library as part of a larger gift.\nAldhelm, De Virginitate\nCreated: Circa 899, England\nSynopsis: The oldest written items at the Free Library of Philadelphia are a set of cuneiform tablets—a common theme—but its oldest manuscript is a prose version of De Virginitate, About Virginity. Aldhelm, the Abbot of Malmesbury and Bishop of Sherborne, had written this treatise about early virgin martyrs, the value of purity, and its relationship to Christian virtue. This later Latin edition was written in an Anglo-Saxon script.\nProvenance: R. Contan gave this book to a new owner in March, 1855. J.F. Lewis acquired it in 1914, before it went to the library.\nCorrection: The original version of this article showed an image of the Codex Sinaiticus Syriacus, also held by St. Catherine’s Monastery, rather than the Codex Sinaiticus in Greek. The image has been updated.', ""Conservation and Preservation Services\nAll materials used in books and manuscripts are in danger as they are exposed to internal and external causes of deterioration. Moreover, most recent documents (e.g. modern books, audiovisual materials... etc.) are even more fragile than older ones.\nQatar National Library (QNL), a member of Qatar Foundation for Education, Science and Community Development (QF), specifically highlights in its mission statement the importance of preserving cultural heritage.\nQatar National Library offers the expertise of a fully trained international team and laboratories are equipeed with most recent techniques and mashines, to handle any kind of activities in the following areas:\n- Preventive conservation section\n- Bio deterioration analysis and treatment section\n- Conservation section\n- Materials analysis to support conservation actions as well as codicological studies\nRegular monitoring of all relevant information about the environmental condition is one of the first steps in the implementation of preventive conservation strategy.\nAt QNL, we carefully monitor the temperature and the relative humidity in several ways. These data are retrieved by a computer in order to get a periodic view of the climatic condition of the air quality in all of the storage rooms and showcases.\nMold spores, bacteria and insect eggs are always in the air and the dust, around us; in variable quantities in both clean and dirty environments. So it’s very important a regular monitoring that can allows us to detect biological infestation in advance and helps us take appropriate intervention measures.\nThe QNL Preservation and Conservation Center is well equipped with modern devices that allow personnel to check and manage factors related to the causes of bio deterioration.\nThe quality of the air and the environment is regularly checked with regards to bio-contamination. This consists of checking the amount of mold spores in the air and determining the acceptable quantity of spores. An increase in this quantity indicates a change in air quality in respect to the risk of mold development.\nSame care is reserved to the contamination that can come by contact. In order to do this, bioluminescence tests are used to check for bio-contamination related to the surfaces.\nThe staff of the conservation laboratory are fully trained and have advanced skills in various fields such as map conservation, paper and parchment conservation and bookbinding conservation. Each decision they take is carefully considered and supported by the analysis of the material of the documents as well as the quality of the modern products used for conservation treatments.\nPreservation and conservation processes have developed over time and are now a worldwide task for libraries. There are mainly two kinds of deterioration that conservators have to face:\nThe natural deterioration process of paper and of some inks is a slow but quite irreversible process. Paper, particularly paper produced since the 19th century, yellows and becomes very brittle. Some manuscript inks can cause paper and parchment to disintegrate making the text unreadable.\nBad environmental conditions such as high temperatures, high but also very low humidity, and fluctuations in both, as well as excessive light exposure, are damaging to paper and leather bindings. Also rodents, insects, molds, and bacteria can cause documents to deteriorate very rapidly. Disasters such as floods, fires, and earthquakes can dramatically endanger collections. However, we shouldn’t underestimate the potential for slow degradation of collections caused by bad handling and inappropriate repairs.\nYou or your family may have some old books and documents that you wish to preserve for future generations. Here is some quick advice: never use adhesive tape, remove dust periodically and do so gently, and make sure to store your documents in an air conditioned room in a place with low risk of water leakage.\nThe QNL Preservation and Conservation Unit is equipped with a state-of-the-art conservation laboratory which can analyze the quality of the material of the documents.\nUsing non-destructive, non-invasive modern analysis techniques, information related to the material aspects of books becomes more accessible. This information is very useful to enforce preservation strategies, support conservation decision and increase knowledge related to codicological studies.\nThe IFLA Strategic Programme on Preservation and Conservation (PAC) was officially created during the IFLA annual conference in Nairobi in 1984 to focus efforts on issues of preservation and initiate worldwide cooperation for the preservation of library materials.\nIn line with its mission, since 2015, the QNL's conservation laboratory is the IFLA/PAC (Preservation and Conservation) Regional Center for the Arab Countries and the Middle East, stressing the importance of international cooperation in this field. As a PAC Center, QNL offers the expertise of a fully trained international team and conservation facilities.\nOur objective is to create a professional network of collaborative assistance to improve the strategy of preservation and conservation in the Arab countries and in the region in general. Our IFLA/PAC regional center's role is to support the Arab libraries through a series of activities and initiatives to promote, disseminate and improve the practices in preservation and conservation.\n• creating an international network to share information and knowledge\n• supporting and assisting Arab libraries in conservation issues\n• producing and publishing information materials (in Arabic and English)\n• developing better disaster plan information\n• organizing courses, workshops and meetings\n• translating into Arabic IFLA's guidelines and documents related to preservation and conservation\nHere are some documents and materials related to preservation and conservation:\n- IFLA's Principles for the Care and Handling of Library Materials - English Arabic Translation\n- QNL's Preservation and Conservation Center Leaflet\nFor further information, especially if you are in one of the countries that are covered by our PAC regional center and you are interested in our activities, please do not hesitate to contact:\nPreservation and Conservation Manager,\nQatar National Library\nTel: +974 4454 8147\nPO Box: 5825, Doha – Qatar\nEmail: [email protected]""]"	['<urn:uuid:25d45c4c-e9c9-4636-84ea-70cc821546ff>', '<urn:uuid:5a921b5e-3771-4286-a140-f0f1fa9bc6cc>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T23:00:04.658849	7	45	2577
45	What eating habits do wild and pet fish have?	In the wild, swordfish are solitary hunters that feed on smaller fish and squid, attacking schools of fish. They sometimes jump to startle potential prey. As for pet fish like goldfish, while they are technically omnivores, their optimal diet should be about 70% plant matter and 30% high-protein foods. They do well with foods like green beans, peas, spinach, and certain types of algae. However, they shouldn't be fed land-based meats, bread, or wild insects as these can cause health problems. Both wild and captive fish tend to eat opportunistically, as they're naturally adapted to environments where food can be scarce.	['The maximum size of the swordfish is 177 in. (455 cm). The highest possible weight is 1400 lbs (650 kg). The heaviest swordfish on record is 535 kg. However, it is hard to find swordfish that is over 200 kg. The typical ones captured by fishery weighs around 100 to 150 kg. This fish s is the sole member of the Xiphiidae family.\nThe swordfish has large eyes and a heavy, somewhat round body. The dorsal fin at the back resembles a crescent, while the next one is quite small. The anal fins look like the dorsal fins but not as big.\nThere are no ventral fins. A keel is at the point where the fins emerge. Regardless of the size of the swordfish, they do not have scales or teeth.\nThe name of course, is derived from its beak which looks like a sword. The beak is about 1/3 of the body length. It is sharp and pointed. The swordfish has a small, sharp lower jaw and a wide mouth.\nThe colors vary. Many are a silver white with touches of dark brown. In young swordfish, the jaws are of equal length. Their bodies are also more snakelike.\nThe females are bigger than the males. Studies indicate that a one year old female will weigh 4 kg and triple that the following year. At 4 years the female will tip the scales at 70 kg. At five years of age, it will be over 100 kg. However there are males that can go over 300 lbs. Swordfish can swim over 50 mph.\nLifespan and Distribution\nWhatever the size of the swordfish, the lifespan is usually at least 9 years. They become sexually mature around 3 years of age.\nThe swordfish are found in the water column instead of the coastal sections. They are usually found 180 m to 580 m deep. They are found throughout the world in waters with temperatures over 15 C (58 F). They can however, swim at 10 C (50 F). Size seems to be a factor, because swordfish over 90 kg are never found in 18 C or below waters.\nSwordfish are solitary; they swim in solitude. Even in groups, they are separated by about 30 ft (30 m). The swordfish can also be found near the surface sometimes. The fish can also be seen jumping.\nThere are two theories as to why the fish jumps. The first is that it is trying to remove pests attached on its body. The second is that it is trying to startle smaller fish. By making smaller fish jump around, they will become easy prey.\nSwordfish feed on smaller fish and squid. They can be seen attacking schools of fish, killing and consuming the fish around them. Their natural enemies are the shark and the killer whale.\nThe size of the swordfish, not to mention its aggressive nature as a fighter, has made it popular among sports fishermen. Although its population has gone down due to overfishing, restrictions are helping increase its numbers once again.', 'When first owning a goldfish, it can be confusing to figure out how much to feed them. You may find that they gobble down food in seconds and look at you through the tank as if asking for more.\nMaybe they rush to the front of the tank begging for food as if they’ve never eaten before in their lives. But do not let this fool you!\nFish anatomy is very different from our human body, and so their digestive systems are completely different.\nDifferent fish have different digestive workings, but goldfish lack an acidic stomach like we have, instead, they digest and absorb nutrients solely through their intestines.\nThe bacteria in their gut does all the work, and so their bodies do not have a system like ours, which tells them they are “full” and many fish species can and will “eat themselves to death!”\nMany fish also live in an environment where food is scarce, and they may not eat for days or even months on end, so they make the most of food when they see it, as in their mind, it might be the last time they eat for a long while.\n- From this, how can I know how much to feed my goldfish?\n- How often do I need to feed them?\n- How much is too much?\n- How do I know if I’m underfeeding my goldfish?\nWe aim to answer these questions below:\nDo goldfish eat a lot?\nAll fish are cold blooded animals – this means that their body temperature is based on their surroundings. As an extension of this, the temperature they live at directly influences the metabolism of the fish.\nIn warmer temperatures, fish will have a faster metabolism and be able to expend more energy. In cooler temperatures, fish will have a slow metabolism and will not expend as much energy.\nThis means that in warmer temperatures, your goldfish will eat more than they would in cold temperatures. They will also burn off more weight in warmer temperatures and will be much hungrier.\nAs it stands, goldfish generally eat a good amount as far as fish go, but when compared to how much a human or mammal might eat, it is a tiny amount.\nGoldfish eat lots because they are active fish and so burn lots of energy; they also get quite large and have a more herbivorous diet, meaning they need to consume more algae and plant matter to get their energy.\nIn the typical aquarium, we usually say a small pinch of food, but this is again, dependant on temperature, size of the tank, amount of fish, size of fis, and so on. But for a general aquarium housing multiple fish, a small pinch, or one eighth of a teaspoon, 4 times a week, is ideal.\nFor big ponds with mature goldfish, usually give a pinch or palmful of flakes, pellets or sticks, once every two days.\nWill my goldfish eat everything I give them?\nGoldfish have very broad diets, and so they will eat a lot of different types of foods – this doesn’t mean they will eat everything however, and you should be responsible in what you choose to feed your goldfish.\nGoldfish explore the world with their mouths, and much like a child, will attempt to eat things they aren’t supposed to, so be careful when offering them things which could potentially get stuck in their throats (yes goldfish can choke to death), could injure them or that are indigestible.\nGoldfish will eat most fruit and veg, fish, meat, egg and tiny animals like insects, spiders and worms.\nWhile goldfish can eat all these things, not all of them are a great food source as they can fowl the water quickly if left uneaten, aren’t always healthy for goldfish and can carry disease or parasites.\nFoods that are good for goldfish\n✅Green beans (from a can): This is one of the best foods you can give to goldfish. It is natural, easy to digest and is an excellent source of nutrients for your fish; they are cheap and easily available, and don’t foul the water much. They are a highly recommended food for any fishkeeper.\n✅Frozen or sugar snap peas: These are a great food for goldfish, they even help with digestion and bowel movement. Simply boil the peas until they become soft, cool them down and offer them to your fish.\n✅Spinach: This is another great food when boiled, along with cucumber.\n✅Algae: Most types of algae are also eaten with gusto by goldfish, especially the softer types of green hair algae, which is a food they would eat if they were wild.\n✅Hard boiled egg: This is a good food, although be careful as it is a strong pollutant, you may want to do a water change after feeding it.\n✅Broccoli: This is a great food, along with orange peel and lettuce.\n✅Dried mealworms, salmon, muscle, prawn and crab: These are all superb sources of protein to give to goldfish.\nFeeding these types of foods is not only good for their nutrition, but also provides them with enrichment, as they can spend hours at a time picking away at these foods – it keeps their mind focused and stops them from picking on each other too!\nFoods to not give to goldfish\n❌Wild insects: While it is in a fish’s nature to eat bugs, catching wild insects that are potentially poisonous and carry harmful bacteria and parasites and feeding them to your pet is very irresponsible.\nIf you wish to feed your fish insects, do so by purchasing captive bred “live foods” from your local pet store; they are free from parasites and disease and are safe to feed, although for goldfish, while large fish can eat insects, we don’t really recommend it.\n❌Land based meats: As fish are aquatic animals, their digestive systems aren’t as accustomed to eating land animals as you or I. Meats like pork, chicken and beef are not good foods for goldfish as they contain amino acids which goldfish cannot break down, and that build up in their bodies.\nWhile meat can be fed very sparingly, it is not good long term, as the amino acids build up, causing things like fatty liver disease, dropsy, internal infections, gout and a host of other horrible health conditions you don’t want for your goldfish.\nIt is better to not feed your goldfish land based meat at all, as their diets are more on the plant based side anyway.\n❌Bread, grains and seeds: These are also bad foods for goldfish as they are not evolved to eat seeds, and all they will do is clog up their intestinal tract, causing colic and potentially killing your fish.\nWhat do goldfish eat in the wild?\nWild goldfish don’t really exist, as they are a man made animal, however, stray goldfish that have been irresponsibly released into the wild do exist and their diets are nearly the exact same to those of many of the world’s other carp species.\nThey feed on mainly algaes, aquatic plants and small insects and invertebrates.\nAll fish are omnivores, but they fit within categories. Some fish are more on the predatory side, and have a diet mainly focused on high protein, meaty foods like invertebrates, other fish, amphibians and carrion. Some species even eat reptiles, mammals and birds!\nThese predatory fish include things like:\n- Some cichlids\n- Most loaches\n… and a few others.\nOn the other side, you have fish which hang more on the herbivorous side, and have a more plant and fibre based diet. These fish feed mostly on algae, as well as any other plant matter that falls into their environment, aquatic plants, leaves, veg and fruit.\nFish with a more plant based diet are:\n- Nearly all of the Carp family\n- Most of the Barb family\n- Mbuna cichlids\n- Small Plecostomus species\n- Borneo Loach\n… and a few more.\nIn essence, the structure of predator and prey is pretty loose when applied to fish, as predatory fish still need a deal of plant matter in their diet, and herbivorous fish can and will eat other animals given the chance if they will fit in their mouths, and need protein.\nBut, the best diet for goldfish is one that is around 70% plant matter and 30% high protein foods like small invertebrates, bloodworms and daphnia.\nHow much should you feed goldfish?\nAs mentioned, it can be difficult to pinpoint exactly how much to feed your fish depending on your situation, but there are some things you should know that may help you to better understand how much to feed your tank or pond.\nThis decides the metabolic rate of the fish and its rate of food consumption. If you have your goldfish in a warm tank, that is 20C / 68 F or above, your goldfish will want to eat more.\nAlthough we don’t recommend it, if you keep your goldfish at 26 C / 78 F, then you should feed them every day, sometimes even twice a day as long as your filter can handle it.\nYou will find that at these temperatures, your goldfish burn off weight really fast, they will also be more energetic, will grow much faster and will have shorter lifespans as a result.\nGoldfish that live below 20 C / 68 F will not need to eat as much, and the lower the temperature goes, they will eat less and less, expend less energy, grow more slowly, and will live significantly longer lives.\nOnce the temperature reaches below 10 C / 50 F, the bacteria within the fish’s gut slows down, and can no longer break down high amounts of protein – this is why, in a pond, we switch to wheat germ during the autumn and early spring.\nWe should also feed less than we do in warmer months during this time, as it takes a lot longer for the food to be processed through the gut. If you feed lots during this time, blockage can build within the intestines and this causes bloat and dropsy.\nBelow 5 C / 41 F, the gut bacteria stops completely and the goldfish will enter a torpid state. Do not feed your fish at all during this time! Don’t worry, they expend almost no energy during this time and lose weight at an extremely slow rate.\n|26 C / 78 F||Around 2 times per day|\n|20C / 68 F or Above||Around 1 – 2 times per day|\n|Below 20 C / 68 F||Around 1 time per day|\n|Below 10 C / 50 F||Switch to wheat germ|\n|Below 5 C / 41 F||Do not feed|\nAmount of goldfish\nEveryone will have a different amount of fish – we always recommend you keep goldfish in groups as they are highly social fish, so some individuals may have many fish.\nA good technique is to first check over all your fish. Look at their bellies, if you notice some look sunken in, that means they aren’t getting enough food and you perhaps aren’t giving enough food to get around to them all.\nIf this is the case, feed small amounts over a longer time, keep giving little bits of food over the course of about 5 minutes. If any food is left behind, stop feeding.\nThis should ensure a fair chance that all your fish get at least some food.\nSize of the tank\nRemember that when you feed, you are adding biological waste to the tank, and your filter can only handle so much before the ammonia skyrockets and crashes the tank.\nIt is important to know your limitations – this is one of the reasons we recommend large tanks for goldfish as a small tank won’t be able to handle the bioload necessary to feed a goldfish.\nIf you do have a smaller tank, you should feed less, maybe only feed 3 times a week or once every other day, to allow the beneficial bacteria to keep on top of the waste.\nSize and age of goldfish\nThe age of the goldfish is also a factor in how much to feed, as a young growing goldfish will eat a lot more than an adult which expends less energy and nutrients.\nGoldfish fry will need feeding 3 times a day.\nSince they are so small, they haven’t the fat reserves to survive without food for a number of hours. This is why it’s important to do lots of water changes when you have fry.\nWhile larger goldfish will eat less, they need larger foods like pellets and vegetables, as opposed to tiny crumbles of fish flakes.\nHow to tell if you are underfeeding goldfish\nIt is quite easy to tell if your goldfish are underfed – simply look at their bellies; if they are sunken in and have a concave arch shape to them which folds inward, they are underweight and need more food.\nFish should always have their bellies bow out slightly, but should still look streamlined. Their gills and head should be aligned with the rest of their body. If they have what seems like a big head or gills which protrude from their sides, they are again, underweight.\nThe peduncle, which is the space between the tail fin and body should also have some weight to it – it should be straight and have thickness to it; if it bows inwards on top and bottom, and looks very slim, your fish is underweight and you should increase the amount that you feed.\nIf you find your fish are underweight, you should also give them health checks. Look for any signs of lesions or parasites, it could be that they are harbouring internal and or external parasites which are sapping away nutrients, causing them to lose weight.\nAlthough this isn’t very common in goldfish as they are a domestic animal, it does happen to a lot of people. You can easily deworm your fish with a chemical antibiotic treatment (just be careful when treating as meds that kill parasites will also kill snails and shrimp you may want in your tank).\nUnderweight Goldfish Checklist:\n⬜ Sunken bellies\n⬜ Big head or gills which protrude from their sides\n⬜ Space between the tail fin and body bows inwards on top and bottom\nIf your goldfish has any of the above, they may be underweight.\nHow to tell if you are overfeeding goldfish\nIf you are overfeeding your goldfish, the first thing you will notice is their bellies are round and bow out. They will look fat, not only from underneath, but the sides will also bow out.\nIf your goldfish does this, you should fast them and reduce your feeding. Continuing feeding when a fish has a swollen belly will quickly lead to bloat, dropsy and eventually death.\nAnother thing you will notice is your water quality will foul quickly. If you notice a sudden spike in ammonia or nitrites, then it could be that you need to reduce your feeding schedule.\nThis is another reason why it’s important to test your water regularly, as you could be doing something harmful to the water chemistry without realising, such as overfeeding.\nYou will also be able to tell if you are overfeeding if there is food left behind – sometimes there is so much food that your goldfish will miss some and it will rot at the bottom of the tank.\nIf you don’t have a cleanup crew, like snails, then remove this uneaten food as soon as possible as it is a source of harmful ammonia.\nOnce you get a good schedule going and know how much to feed, deviate from it very little, as the stability of a good routine is key to the success of an aquarium.\nOverweight Goldfish Checklist:\n⬜ Round bellies and sides that bow out\n⬜ Water that fouls quickly\n⬜ Food left behind\nIf your goldfish or tank has any of the above, you may be overfeeding and they may be overweight.']	['<urn:uuid:de09286e-512e-4f5b-bb86-b922b3456a56>', '<urn:uuid:bdbd7c5f-0216-4bdb-81fa-c6f0ea8eb5d3>']	open-ended	direct	concise-and-natural	distant-from-document	three-doc	novice	2025-05-12T23:00:04.658849	9	101	3191
46	statistics chemical substances exponential growth historical trend 1800 1995	During the past 200 years from 1800 to 1995, the number of chemical substances has shown approximately exponential growth without saturation. This growth pattern applies to both organic and inorganic chemistry, following at least a two-phase model, with some balancing phenomena observed during war and post-war times.	"['English Abstracts of Selected Publications\n(Joachim Schummer\'s Homepage /\nJoachim Schummer: ""Color\nfield painting als \'reine Malerei\': Die Etablierung der Kunstkritik\nim Modernismusentwurf"", in: Kunsthalle Karlsruhe (ed.), Farbfeld\nund Philosophie, Karlsruhe 2000.\nAbstract: The paper analyzes the interrelated establishment\nof art criticism and Modern Art in the U.S.A. in period from the\n1930s to the 1960s. Picking out Clement Greenberg as the central figure,\nI describe how he first adopted European ideas of modernity and transformed\nit to his normative concept of \'pure art\', particularly exemplified by\nhis visions of color field painting. Though at first his approach did not\nfit at all the ideas of most abstract painters, Greenberg soon became most\ninfluential in establishing both modern American painting and art criticism.\nIn conclusion I argue that, while the intercontinental transfer of ideas\nnecessarily depended on concepts of art, it is far from clear what the\ndescriptive concepts used by art critics refer to. The establishment of\nart criticism by way of establishing a normative talk of \'modernity\'\n(as well as the recent talk of \'post-modernity\') seems to be, by their\nown measure, a pre-modern process without much reflection on their own\nKeywords: History of Theory of Art, Color Field Painting, Modernity,\nJoachim Schummer: ""Aristotle\non Technology and Nature"", Philosophia Naturalis, 38\nAbstract: Due to the rapid development and ubiquitous impact\nof modern technology, many people feel that nature is in danger of becoming\nextinct. From the 13th century until today, philosophers and theologians\nhave been seeking advice from Aristotle to define both nature and technology\nin a way that the former restricts the latter. In this paper, I reconsider\nthree corresponding theses usually attributed to Aristotle. 1) Technology\nimitates nature, such that there is no place for authentic human creativity.\n2) Technology in supplementing and completing nature fulfils but the inherent\naims of nature. 3) There is an ontological hiatus between natural things\nand artifacts such that technology cannot reproduce or change natural things.\nI argue that 3) is inconsistent with 1) and 2) and that Aristotle’s writings\nsupport none of the three theses in general. Instead, his proper concept\nof technology places little restrictions on technological innovation. While\nthe putative ontological hiatus has been most influential in the history\nof chemistry/alchemy, Aristotle himself rather holds a relative distinction\nthat he levels out just in the realm of chemistry. Moreover, the case of\ngenetic engineering shows that current problems are beyond the scope of\nAristotelian theory. Rather than presenting solutions, I argue that claiming\nAristotle’s authority to support criticism of modern technology does justice\nneither to Aristotle nor to the complexity of today’s problems.\nKeywords: Consciousness of Time, Temporality, Ecology, Environmental\nJoachim Schummer: ""Zeitbewußtsein,\nÖkologie und Ethik"", Antemnae (Roma), 2 (2000).\nAbstract: (""Consciousness of Time, Ecology, and Ethics"")\nThe paper investigates the impact of global environmental issues on\nour consciousness of time and vice versa. In Part I, I first analyze in\ndetail how various kinds of temporal structure, such as linear progress,\ncircularity etc., are built and what makes us select one of them as being\nuniversal. Part II discusses types of confusion and distortion of our consciousness\nof time due to global environmental issues. Since our consciousness of\ntime is fundamental to our way of perceiving and dealing with the environment,\nthe topic has also an ethical dimension. In Part III, following Hans Jonas’\napproach of ethics, I ask what kind of a consciousness of time is adequate\nfor responsibly treating our environment.\nKeywords: Aritotle, Nature, Technology.\nJoachim Schummer: ""Coping\nwith the Growth of Chemical Knowledge: Challenges for Chemistry\nDocumentation, Education, and Working Chemists"", Educación\nQuímica, Vol. 10 (1999), No. 2 (Special 10th Anniversary Edition),\npp. 92-101 .\nAbstract: Chemistry is by far the most productive science concerning\nthe number of publications. A closer look at chemical papers reveals that\nmost papers deal with new substances. The rapid growth of chemical knowledge\nseriously challenges all institutions and individuals concerned with chemistry.\nChemistry documentation following the principle of completeness is required\nto schematize chemical information, which in turn induces a schematization\nof chemical research. Chemistry education is forced to seek reasonable\nprinciples of selectivity, although nobody can have an overview any more.\nPhilosophical evaluation of the growth of chemical knowledge proves that\nat the same time chemical ‘nonknowledge’ increases more rapidly. An analysis\nof reasons, why chemists are making new substances at all, shows that the\nproliferation of new substances is for most part an end in itself. The\npresent paper finally argues for the need of a rational discourse among\nchemists on the aims of chemistry.\nKeywords: Growth of Knowledge, Chemistry Documentation, Chemistry\nEducation, Aims of Chemistry.\nChemical Core of Chemistry I: A Conceptual Approach"", HYLE--An International\nJournal for the Philosophy of Chemistry, 4 (1998), 129-162.\nAbstract: Given the rich diversity of research fields usually\nascribed to chemistry in a broad sense, the present paper tries to dig\nour characteristic parts of chemistry that can be conceptually distinguished\nfrom interdisciplinary, applied, and specialized subfields of chemistry,\nand that may be called chemistry in a very narrow sense, or ‘the chemical\ncore of chemistry’. Unlike historical, ontological, and ‘anti-reductive’\napproaches, I use a conceptual approach together with some methodological\nimplications that allow to develop step by step a kind of cognitive architecture\nfor chemistry, which basically contains: (1) systematic chemical knowledge\non the experimental level; (2) clarification of chemical species; (3) chemical\nclassification systems; (4) theoretical foundation through the chemical\ntheory of structural formulas. In a succeeding paper the results will be\nchecked for resisting physicalistic reduction.\nKeywords: chemical properties, logical structure of chemical\nknowledge, pure substances, chemical classification, theory of structural\nJoachim Schummer: ""Epistemology\nof Material Properties"", in: Proceedings of the 20th World Congress\nof Philosophy, Boston/MA, USA, August 10-16, 1998 (to be published 1999).\nAbstract: The paper presents an epistemological approach to\nthe investigation of material properties that is opposed both to phenomenalistic\nepistemology and recent linguistical and ontological accounts of matter/mass\nterms. Emphasis is laid on the inherent context dependence of material\nproperties. It is shown that, if this is taken seriously, some deep epistemological\nproblems arise, like unavoidable uncertainty, incompleteness, inductivity,\nnonderivableness. It is further argued that some widely held epistemological\naccounts, namely that of essentialism, constructivism, and pragmatism,\nall reveal some serious defects if related to the recognition of materials.\nIn order to responsibly manage our material environment, a more realistic\nestimation of our epistemic abilities and prospects is suggested.\nKeywords: Epistemology, Material Properties, Context Dependence,\nJoachim Schummer: ""Physical\nChemistry: Neither Fish nor Fowl?"", in: P. Janich, N. Psarros (eds.),\nAutonomy of Chemistry. Königshausen & Neumann, Würzburg\n1998, pp. 135-148.\nAbstract: It is the aim of this paper to clarify the role of\nphysical chemistry establishing various relations between physics and chemistry.\n(1) According to a systematics of material investigation, physical chemistry\nhas a research field of its own that lies in between physics and chemistry,\nboth taken in a narrow sense. (2) In so far as chemistry in a broader sense\nis concerned with material differences, physical chemists treat chemical\nproblems by applying physical concepts, methods and instruments. (3) While\neliminative discipline reduction can be rejected for several reasons, there\nare some non-eliminative reductive approaches within physical chemistry,\nwhich are quite different from that of physics. (4) A closer look on theoretical\nreasoning reveals some further methodological characteristics of physical\nchemistry supporting the idea of an emerging discipline: (a) Physical chemists\ntry to explain material diversity in quantitative terms. They use (b) a\npragmatic pluralism of models, which are (c) rather subject to refinement\nand adaptation than to crucial tests. There is (d) an extensive use of\nsemi-empirical approaches and (e) the emergence of new types of models\nfrom combining physical and chemical concepts and theories.\nKeywords: Physical Chemistry, Chemistry and Physics, Interdisciplinary\nJoachim Schummer (Ed.): Glück\nund Ethik. Königshausen & Neumann, Würzburg 1998.\nAbstract: The anthology provides a comprehensive survey of recent\napproaches in the field of philosophy of happines by outstanding\nscholars of German speaking philosophy. Contributors are: Günther\nBien, Dieter Birnbacher, Maximilian Forschner, Malte Hossenfelder, Hans\nKrämer, Gregor Paul, Annemarie Pieper, Wilhelm Schmid, Joachim Schummer,\nand Martin Seel. Topics included are: the limits of philosophy of happiness\n(historical, intercultural, and systematical); philosophy of happiness\nas ars vivendi, and the methodology for philosophical counceling;\nphilosophy of happiness as a pre-moral theory of goodness and good life;\nthe relationships between happiness and morality with special regard to\nfoundational problems of moral philosophy.\nKeywords: Philosophy of Happines, Morality, Ethics.\nJoachim Schummer: ""Challenging\nStandard Distinctions between Science and Technology: The Case of Preparative\nChemistry"", HYLE. An International Journal for the Philosophy of\nChemistry, 3 (1997), 81-94.\nAbstract: Part I presents a quantitative-empirical outline of\nchemistry, esp. preparative chemistry, concerning its dominant role in\ntoday\'s science, its dynamics, and its methods and aims. Emphasis is laid\non the poietical character of chemistry for which a methodological model\nis derived. Part II discusses standard distinction between science and\ntechnology, from Aristotle (whose theses are reconsidered in the light\nof modern sciences) to modern philosophy of technology. Against the background\nof results of Part I, it is argued that all these distinctions fail, because\nthe underlying concepts of science are either out-dated, one-sided, or\narbitrary. A deeper understanding of today\'s sciences requires, in particular,\na philosopical investigation of chemistry.\nKeywords: Chemistry, Science and Technology, Poietical Science,\nJoachim Schummer: ""Towards a Philosophy\nof Chemistry"", Journal for General Philosophy of Science, 28\nAbstract: The paper shows epistemological, methodological and\nontological peculiarities of chemistry taken as a classificatory scinece\nof materials using experimental methods. Without succumbing to standard\ninterpretations of physical science, chemical methods of experimental investigation,\nclassification, reference, theorizing, prediction and production of new\nentities are developed one by one as first steps towards a philosophy of\nchemistry. Chemistry challenges traditional concepts of empirical object,\nempirical predicate, reference frame and theory, but also the distinction\ncommonly drawn between natural science and technology. Due to its many\npeculiarities, I propose to treat chemistry philosophically as a special\ntype of science, apart from other sciences.\nKeywords: Chemistry, Epistemology, Experiment, Classification,\nReference, Prediction, Technology, Matter, Material Properties.\nJoachim Schummer: ""Scientometric Studies\non Chemistry II: Aims and Methods of Producing New Chemical Substances"",\n39 (1997), 125-140..\nAbstract: Chemistry, as today\'s most active science, has increased\nits substances exponentially during the past 200 years without saturation.\nTo get more insight why and how chemists produce new substances, a content\nanalysis of 300 communications to the \'Angewandte Chemie\' of the years\n1980, 1990, and 1995 is carried out regarding aims and methods of preparative\nresearch. In the most productive field of organic chemistry production\nmainly occurs to improve abilities for further production, while the less\nproductive field of inorganic chemistry has more diverse aims. Methodological\ndifferences between organic and inorganic chemistry are discussed in detail\nas well as the relationship between pure and applied science.\nKeywords: Chemistry, Scientometrics, Production of Substances,\nPreparative Aims and Methods, Organic and Inorganic Chemistry, Pure and\nJoachim Schummer: ""Scientometric Studies\non Chemistry I: The Exponential Growth of Chemical Substances, 1800-1995"",\n39 (1997), 107-123.\nAbstract: The number of chemical substances is considered as\na cumulative measure of the cognitive growth of preparative chemistry.\nDuring the past 200 years there is approximately exponential growth without\nsaturation. Separate analysis of organic and inorganic chemistry suggests\nat least a two-phase modell either. Detailed discussion of the results\n(considering also the growth of chemists, chemical papers, patents, and\nchemical elements) reveals that an external (socio-economical) explanation\nis insufficient. Instead, an internal (methodological) approach is suggested\nto explain the exponential growth as well as ballancing phenomena in war\nand post-war times.\nKeywords: Chemistry, Scientometrics, Growth of Substances, Growth\nof Knowledge, Externalism vs. Internalism.\nJoachim Schummer: Realismus und Chemie.\nPhilosophische Untersuchungen der Wissenschaft von den Stoffen. Königshausen\n& Neumann, Würzburg 1996 (English title: Realism and Chemistry.\nPhilosophical Investigations of the Science of Substances).\nAbstract: Against the background of recent concepts of scientific\nentity realism and experimentalism, esp. those of Rom Harré, this\nbook provides a non-physicalistic philosophical approach to chemistry.\nChemistry is considered to be the fundamental science of substances looking\nfor a theoretically supported classification that enables to predict and\ncreate new substances. The development and methodological refinement of\nthis scientific approach is dealt with along the whole history of chemistry,\nfrom Aristotle to modern synthetic chemistry. Fundamental concepts, classificatory\napproaches, and theoretical reasoning of chemistry are shown to be inherently\nconnected to experimental operations.\nKeywords: Chemistry, Realism, Matter, Experiment, Classification,\nJoachim Schummer: ""Die stoffliche Weltveränderung\nder Chemie: Philosophische Herausforderungen"", in: C. Hubig, H. Poser (eds.),\nhumana - Dynamik des Wissens und der Werte, vol. 1, Leipzig, 1996,\nAbstract: Chemistry is substantially changing our material world\nby currently producing a million new substances a year. Philosophers, who\nseem to be unaware of that hitherto, are challenged in ontological, epistemological\nand ethical respect. Against physicalistic ontology as well as essentialistic,\nconstructivistic, and pragmatistic myths of epistemology it is argued for\na more modest and sensible epistemology of substances that is in need for\ndealing with chemicals in a more responsible way. A proposal for risk assessment\nof chemicals is made.\nKeywords: Chemistry, Substances, Ecology, Ethics, Risk\nNikos Psarros, Klaus Ruthenberg, Joachim Schummer\n(eds.): Philosophie der Chemie - Bestandsaufnahme und Ausblick.\nKönigshausen & Neumann, Würzburg 1996.\nAbstract: The German working group ""Philosophy and Chemistry""\n(founded in 1993) has collected 9 articles that provide a comprehensive\nsurvey of what have been done in the past and what should be done in a\nfuture philosophy of chemistry. Since many authors find a notorious neglect\nof chemistry in the past, they try to explain this neglect from various\naspects among which reductionism seems to be the most important.\nKeywords: Chemistry, History of Philosophy of Science, Reductionism\nJoachim Schummer: ""Philosophie der Stoffe,\nBestandsaufnahme und Ausblicke. Von der Philosophischen Entstofflichung\nder Welt zur ökologischen Relevanz einer Philosophie der Stoffe"",\nin: N. Psarros, K. Ruthenberg, J. Schummer (eds.): Philosophie der Chemie\n- Bestandsaufnahme und Ausblick. Königshausen & Neumann, Würzburg\nAbstract: The first part of this essay tells the story of an\nold struggle between philosophy of matter and philosophy of form in broad\noutline, from the presocratics to analytical philosophy, leading to a philosophical\nand metaphorical ""dematerialization"" of the world (ontology), knowlege\n(epistemology) and language (mass terms). The second part argues, against\nthe background of ecological crisis, for the need of sophisticating both\nour ordinary and scientific concepts of materials, and it gives an outline\nfuture philosophical contributions to these problems.\nKeywords: Chemistry, Matter, Mass, Ecology\nJoachim Schummer: ""Zur Semiotik der chemischen\nZeichensprache. Die Darstellung dynamischer Verhältnissse mit statischen\nMitteln"", in: P. Janich, N. Psarros (eds.), Die Sprache der Chemie,\nKönigshausen & Neumann, Würzburg 1996, pp. 113-126.\nAbstract: After giving a survey of recent literature on the\nlanguage of chemistry attention is drawn to the semiotics of the chemical\nsign language. Against the background of Peirce\'s theory of signs, it is\nargued that structural formulas should be interpreted neither as symbols,\nnor as icons, let alone indices. Instead, the systemic character of structural\nformulas (in the sense of Peirce\'s super-icons) is pointed out enabling\nchemists to represent chemical changeabilities by static means. In conclusion\nthe current misleading pictographic paradigm is shown to be at the expense\nof chemical information.\nKeywords: Chemistry, Semiotics, Sign, Peirce\n""Ist die Chemie eine schöne Kunst? Ein Beitrag zum Verhältnis\nvon Kunst und Wissenschaft"", Zeitschrift für Ästhetik und\nAllgemeine Kunstwissenschaft, 40 (1995), no.2, 145-178.\nAbstract: Chemistry is a highly productive enterprise currently\nproducing some 1 million new substances a year. Since many chemists have\nrecently claimed for the beauty of their products (the most famous is the\nbuckminster fullerene), it is time to take this claim seriously. Against\nthe background of several aesthetical theories (idealistic aesthetics from\nPlato to Kant, psychological approaches of Gombrich and Arnheim, Godman\'s\nsymbolic and Eco\'s semiotic approach) chemical products are analyzed after\ndistinguishing three ontological categories: chemical substances, molecules,\nmolecular models. It is argued that none of these theories can explain\nthe putative beauty of chemical products. But Eco\'s approach is able to\nexplain recent dynamics in supramolecular chemistry being motivated by\nKeywords: Chemistry, Aesthetics, Art, Symmetry, Eco\nJoachim Schummer: ""Zwischen Wissenschaftstheorie\nund Didaktik der Chemie. Die Genese von Stoffbegriffen"", chimica didactica,\nAbstract: In reviving the common roots of philosophy and didactics\nof science (19th century\'s propaedeutica) a theory of recognition\nand concept building in the material realm is provided. Clarification of\nthe stepwise development of material concepts and knowledge, from ordinary\nexperience to experimental science, aims at two different but related things:\nFirst, it helps to understand the logical structure of chemical knowledge;\nand secondly, it rules out barriers of understanding caused by misleading\nKeywords: Chemistry, Didactics, Matter, Substances.\nJoachim Schummer: ""Die Rolle des Experiments\nin der Chemie"", in: P. Janich (ed.), Philosophische Perspektiven der\nChemie, Bibliographisches Institut, Mannheim, 1994, pp. 27-51.\nAbstract: While experimentation has been nearly neglected by\nmodern epistemology and philosophy of science until recently, it has always\nplayed a central and manifold role in chemistry. Experimentation is shown\nto be fundamental in concept building and concept refinement by specifying\nexperimental contexts, and in various chemical approaches to substance\nclassifications. Chemical experimentation links science to technology and\nallows an experimentalist interpretation of entity realism.\nKeywords: Chemistry, Experimentation, Classification, Realism,']"	['<urn:uuid:6d6680f7-ac56-46df-b4bd-4ef38daf9f08>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T23:00:04.658849	9	47	2860
47	rise in violence after 1950s explain main reasons causes changes society	The rise in violence since the mid-20th century was caused by major societal changes including deindustrialization, decolonization, and neo-liberalism. These changes made the source of social problems more difficult to identify and created a general sense of unease. This led to an increase in chronic violence without resolution, unlike earlier periods where conflicts were mainly ritualized encounters that reinforced group identities. The change in the nature of society's problems contributed to individuals feeling they had no other options but to turn to violence.	['Ten years ago, two major work about violence came out: “Violence: A Micro-Sociological Theory”, by Randall Collins (Princeton University Press, 2008) and “Violence: A New Approach”, by Michel Wieviorka (Sage, 2009). The two sociologists meet today to discuss their theories and renew the debate for The Conversation France.\nAn article byThe Conversation on June 17, 2018, published in\nCan you tell us more about your earlier approach and how you look at it today?\nRandall Collins: Michel Wieviorka explains well the historic shift in violence since the mid-20th century. Previous conflicts were mainly ritualised encounters that reinforced group identities.\nThen came deindustrialisation, decolonisation, and neo-liberalism, all of which made the source of troubles amorphous and created a malaise manifested in the rise of chronic violence without closure. Wieviorka connects structural change with the phenomenology of individual motivation for violence. My micro-situational approach focuses on the pragmatics and emotions of violence-threatening encounters.\nThese are above all the inhibitions to effective violence in face-to-face communication with one’s opponent, an emotion I have called “confrontational tension/fear”. Humans, when directly focused on each other, become involved in each other’s bodily rhythms. Violence as action is in tension with the tendency toward Durkheimian solidarity through interaction rituals.\nIn such moments, adrenaline and a racing heart tend to incapacitate one’s ability to be effectively violent, leading most often to standoffs with no more than angry gestures, or to wild shooting or hitting. Doing damage in such an encounter depends on establishing emotional domination and setting the rhythm of action while the opponent is incapacitated by such high tension.\nThe historical evolution of weapons has created some alternatives to direct confrontation – the ability to fire at a distance, or by clandestine tactics such as suicide bombers who pretend that there is no confrontation until the bomb goes off.\nThe nature of violence changes as new weapons and social techniques spread, and these drive changes even if the motivation that Wieviorka describes continues to define the malaise of the last 60 years. I agree that our approaches are complimentary; my focus on the micro-situation is pragmatic, seeking the eye-of-the-needle that turns motivation into action.\nWhy violence is chosen?\nMichel Wieviorka: My analysis insisted on the processes through which some people end up as terrorists, or, more generally, act violently. These processes have a lot to do with their subjectivity, with their previous difficulties in life.\nI would say that violence appears and develops when processes of subjectivation and desubjectivation lead the person in question feel that there’s no other way – or no best way – to act. And Randall insists differently on interaction, on processes where violence appears in the relation, in the face to face between individuals and its evolution.\nOne approach is to try and understand how an individual or a group chooses violence on the basis of subjectivity: is it because they look for a meaning in life? Or to modify a given situation? Protest against a situation they view as unfair, or to impose one? Such individuals encounter difficulties transforming meaning into action.\nAnother approach is to understand how through concrete interactions violence may appear and extend.\nRandall Collins, as a sociologist and a novelist, you are now developing analysis on the role of Internet and new technologies in contemporary war and terrorism. How are these new aspects of your work connected with the previous ones? And what would be Michel Wieviorka’s answer?\nR.C: After finishing Violence: A Micro-sociological Theory in 2008, questions remained. Have the macro-dynamics of violence changed, especially geopolitics and war, in an era of computerised high-tech? Is it true, as some have claimed, that Clausewitzian friction has been overcome? Certainly not entirely.\nWestern soldiers who fought in Iraq and Afghanistan report that advanced weapons are not always available when needed – limited by refuelling, repairs, logistics and expense – and so ground troops often rely on old-fashioned tactics.\nMoreover, long-distance weapons guided by targeting information from satellites and drones force enemies to disperse, concealing themselves in civilian populations, emerging for guerrilla attacks on isolated bases and vehicles. Seeking even easier targets, they launch terrorist attacks on civilians. As a consequence, rising civilian casualties establish an atmosphere of moral atrocity, especially when Western soldiers respond ferociously to such tactics. Emotional dimensions of war have not disappeared in the high-tech era.\nTerrorists use mobile-phone communications and GPS to aim and trigger their improvised explosive devices (IEDs), routed through Internet cafes in neutral countries. Cyber-war plays heavily on emotions for sustaining motivation on both sides, making it into a war of competing atrocities.\nUntil now we have seen only asymmetric conflicts, where wealthy, high-tech powers fight militarily weaker forces, albeit under the shared umbrella of the Internet. What would happen in a symmetrical war between two equally advanced forces? This may be the United States versus China in coming decades.\nWhat if “the US military divides and fights against itself”?\nFor now, I have made a thought-experiment: the US military divides and fights against itself. This is my novel Civil War Two (2018). The election of a divisive president splits the United States into north vs. south, and the American Civil War of 1861-65 is replayed, this time with today’s weapons.\nDoes the emotional dimension – the fog of war – disappear? No: because mutual attrition due to advanced weapons forces a return to low-tech forms of war; because cyber-war inside a divided organisation with shared codes causes intense paranoia; over-centralised computer controls become overloaded and break down; massive columns of armoured vehicles become stranded without fuel in a huge traffic jam of urban refugees [an imaginary invasion of New York]; small groups of soldiers can crawl inside the defensive skin of long-distance weapons bases. Victory continues to hinge on breaking down the morale and social coordination of the enemy. This applies also to nuclear weapons, which are above all a form of emotional threat, as the melodramatic conclusion of my novel illustrates.\nImagining a world without Internet or new technologies\nM.W.: Carlos Fuentes in his novel The Eagle’s Throne (2003) imagines Mexico in a situation where there is suddenly no more access to satellites – no more television, telephone, fax, Internet, e-mails… The framework is domestic, and not geopolitical, and what Fuentes proposes is a mixture of very parochial political struggles and sex or love affairs.\nThe sociological lesson is great : let us just imagine our world without Internet and the new technologies, and we can understand better the gap with the previous era. This new civilisation is global, and as Randall demonstrates, military forces don’t have now to be face to face. You can kill at distance. In this global world, states have no more the monopoly of technological or scientific power, the whole civil society is able to use the more modern technologies of communication – and often, innovation, in this field, cannot be controlled by the state, and comes from social actors.\nBut if we entered this new civilisation, what about interactions, what about explaining violence by analysing interactions between those far from each other, when a terrorist cell somewhere in the Middle or the Far East can kill some people in Paris or London without having to be on the spot?\nHave we really entered this new era? And what can we do about it?\nR.C: Are we in a new era of civilisation? Yes and no. The Internet penetrates much of daily life all over the world, because it is one of the cheapest and most easily diffused products of capitalism. Digital technology accelerates the tendency – ever since mass troop charges proved impossible in World War I – to disperse the battlefield, now overturning the distinction between war zone and civilians.\nBut hackers of all kinds may go too far – not just amateurs causing mischief and criminals seeking money, but terrorists hiding in Internet cafes, and governments threatening to destroy the opponent’s electronics-based economy, and to turn the enemy’s computer-controlled weapons against them. A solution to these problems is now being considered in the Western militaries: to shut down the Internet in time of war. This would return us to an older form of living – pre-1980, let us say. History does not always go forward or in a straight line.\nM.W: When 9/11 happened, everyone was dumbfounded. Experts in Washington had imagined many technological terrorist threats – biological, chemical, nuclear. But they apparently didn’t think that potential attackers could buy business-class airline tickets, look respectable, board a plane only armed with very small cutters and be ready to give up their lives the way they did.\nIn this regard, 9/11 taught us that not only do we have to take new technological possibilities as violence tools very seriously, but also that states must be ready to anticipate, keeping in mind human efficiency and attacks free from any form of modern technologies. Recent attacks in Europe where vehicles were used have proven in many ways as efficient as bullets when it comes to spreading terror and death.\nAre you optimistic or pessimistic about possibilities for exit from violence?\nR.C: On the micro level, I am optimistic. Face-to-face, humans are not good at violence. They bluster and threaten and curse, but most small-scale violence – whether in quarrels or in protest demonstrations – ends in stalemate.\nPhysical damage happens when one side achieves emotional domination, confronting a weak or momentarily passive victim whom they can attack without resistance. When both sides mirror each other, maintaining a steady face and voice, replying without escalating, threats dissipate. Prospects are good that more people will learn techniques of keeping anger and fear from escalating, and thus cooling down the possibility of violence. Knowledge of the social psychology of interpersonal conflict is now spreading – in business corporations, in schools, hopefully among police and the people who encounter them. On the micro-level we may get a more peaceful everyday life.\nThis will not come because the world has solved the structural problems that cause the malaise and desubjectivation that Wieviorka has described. Causes for anger remain, but we can make the situational eye-of-the-needle into violence even narrower.\nOn the macro level, I am more pessimistic. In asymmetric war between rich states and embittered insurgents, the cat-and-mouse game continues. Rich states devise more and more electronic surveillance tools and more precise remote-controlled weaponry.\nInsurgents respond with electronic hacking and hiding in the civilian population awaiting the moment to commit atrocities against other civilians; anonymous attacks and counter-measures make life more unpleasant for all of us. The politics of would-be charismatic leaders and routinising bureaucrats keeps stirring up political disputes. International crises are repetitive because they are de-escalated only after they become too costly to continue, and crises reappear because perceptions of the evil done by the enemy stirs up cries for intervention and revenge. Perhaps my macro-analysis is too pessimistic. In any case, it is a reason why I focus on micro-analysis, with its elements of optimism.\nM.W: Randall Collins is right, and we must distinguish at least between the micro and the macro level. But in the former, I am more pessimistic than he is. In fact, I consider that violence when used as a military or political tool is also, in some cases, barbaric, loaded with various forms of cruelty, cases of violence for violence.\nSuch is the case of jihadist violence. Disembodiment of human bodies contributes to a dehumanising process for all actors, performer as well as the victim. And today, this is done publicly as such acts are displayed and staged publicly through social media. In doing so, in order to exist, sustain themselves and feel empowered, some humans destroy others, including their humanity, negating their sense of belonging to the human species itself, not only for them but also for those who are watching.\nAt the macro level, I think it is and will be always very difficult to end with violence. For instance, while a specific form of violence disappears or fades away, other forms can emerge, and take over. Such is the case of Colombia, where the peace agreements put an end to the FARC guerrillas. Yet, in some territories that were under FARC’s control, new expressions of violence are now developing. Some are connected to organised crime cartels and implemented by armed groups that have no political project and which appeared in the vacuum caused by the departure of FARC.\nToday we see new surge of violence everywhere through populist, extremist or nationalist movements. But also an authoritarian tendency in society that could announce new explosions of violence.\nMichel Wieviorka is Director of studies at EHESS. His research focus on violence, conflict, terrorism, racism, antisemitism, social movements, democracy, and cultural differences issues.\nMichel Wieviorka is & Scientific director of the IPEV project and a member of the steering committee.']	['<urn:uuid:baee295f-845c-4cb7-87d3-d13050b97f1c>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T23:00:04.658849	11	83	2139
48	how hot mechanical dishwasher vs cooking meat temperature	A high temperature dishwasher must reach 82°C (180°F) for sanitizing, while meat must be cooked to specific temperatures - ground meat should reach 160°F and poultry needs to reach 165°F to kill bacteria.	"['Food safety: Fact versus fiction\nOver the years we’ve all heard advice related to food safety. Some of this advice is true, but some of it is just plain wrong. Here are some food safety facts versus fiction.\nWe do what we can to help keep ourselves and our families healthy. We wear our seat-belt in the car and have smoke alarms in our homes. But do we handle our food with the precaution so we don’t get sick?\nThe Partnership for Food Safety Education has created a quiz that helps us debunk common home food safety myths and recognize safe food handling procedures. Take the below quiz and see how you score:\n1. Once a hamburger turns brown in the middle it is cooked to a safe temperature.\n2. Plastic or glass cutting boards don’t hold harmful bacteria on their surfaces the way that wooden cutting boards do.\n3. We should not put hot food in the refrigerator.\n4. Freezing food kills bacteria that can cause food poisoning.\n5. We don’t need to wash produce if we peel it.\n6. Rinsing chicken with water in the kitchen sink will remove bacteria like salmonella.\n7. Locally grown or organic foods will never give people food poisoning.\n8. Leftovers are safe to eat, until they smell bad.\nCheck your answers:\n1. No! You can’t look at food and determine whether it is cooked to a safe temperature. The only way to know is to use a food thermometer - ground meat (like hamburger) should be cooked to 160 degrees Fahrenheit.\n2. Any type of cutting board can host harmful bacteria on its surface. Regardless of the type of board, it should be washed and sanitized after each use. Make a sanitizing solution consisting of one tablespoon of bleach per gallon of water, you can use it on counter-tops and other surfaces as well and it will last for a week.\n3. You can put hot food directly into the refrigerator. But, a large pot of food like stew or soup should be divided into smaller portions and put into shallow containers for quicker cooling. Remember that food isn’t safe after sitting out at room temperature for more than two hours (one hour if it’s 90 degrees or warmer).\n4. Absolutely not. Bacteria can survive freezing temperatures and they continue to multiply when the food is thawed. Cooking food to the proper temperature (and using a thermometer) is the best way to kill bacteria.\n5. You should always wash fresh fruits and vegetables before eating, cutting or cooking. Harmful bacteria could be on the outside and your knife or hands could transfer it to the part you eat.\n6. No, rinsing poultry won’t remove bacteria. In fact it can spread raw juices around your sink and onto countertops, ready-to-eat food and other surfaces. Bacteria are killed when poultry reaches 165 degrees Fahrenheit on a food thermometer.\n7. Any food, organic or not, can contain harmful bacteria, either from the growing site or introduced at any point from the farm to our homes. That’s why it’s important to reduce foodborne illness by practicing the four steps: Clean, separate, cook and chill.\n8. No, there are all kinds of bacteria, some of which don’t affect the taste, smell or appearance of food. That’s why it’s important to freeze or toss leftovers within three to four days. Remember the motto: “When in doubt, throw it out!”\nFor more information on preventing foodborne illnesses and other issues of interest to Michigan families, please contact a Michigan State University Extension educator in your area.', 'Presentation on theme: ""Sanitation In an Industrial Kitchen Student Need-to-know Information.""— Presentation transcript:\nSanitation In an Industrial Kitchen Student Need-to-know Information\nRole of the Public Health Inspector They are employed by your local health unit to: inspect food premises examine records close a food premise investigate complaints educate people other public health matters\nPremises awaiting First FoodCheck Peel Inspection Premises in substantial Compliance with the Food Premises Regulation Minor non-compliance Reinspection at next Scheduled inspection Disclosure Signs\nSignificant non-compliance With the Food Premises Regulation Threat to food safety with reinspection within 72 hours Immediate health hazard observed in the premises. Reinspection on a daily basis\nMICROORGANISMS NEED… 1. Food Hazardous – protein moist Non-Hazardous – low in protein, dry, high acid 2. Temperature Danger Zone 3. Time Bacteria double every 10 – 20 minutes. Maximum amount of time in the danger zone is 2 hours.\nSAFE FOOD TEMPERATURES -18 o C (0 o F) Freezer Temperature 4 o C (40 o F) Refrigerator Temp. 60 o C (140 o F) Hot Holding Temperature 74 o C (165 o F) Cooking /Temperature Max. time in Danger Zone = 2 hrs DANGER ZONE 4 o C (40 o F) to 60 o C (140 o F)\nHow does time affect bacteria? Bacteria double in number every 10 to 20 minutes in the temperature danger zone. 00:00 = 1\nCROSS CONTAMINATION Cross contamination means something that is already contaminated transfers that contamination to a food This can happen in 3 ways: 1) food to food cross contamination 2) equipment to food cross contamination 3) people to food cross contamination\nPREVENTING CROSS-CONTAMINATION… PREVENTING CROSS-CONTAMINATION… Prevent food-to-food cross contamination by: storing food correctly Prevent equipment-to- food cross contamination by: cleaning and sanitizing Prevent people to food cross-contamination by: practicing good hygiene\nThe Greatest Reason for Foodborne Illness is … Other causes of foodborne illness: Cross contamination Poor hygiene Improper cleaning & sanitizing of food contact surfaces UNSAFE FOOD TEMPERATURES\nINTERNAL COOKING TEMPERATURES How do you know when meat & poultry are cooked? The safest way to know when meat and poultry are cooked is by checking the internal temperature. Food can look done but not have reached a hot enough temperature to kill bacteria.\nSafe Ways to Defrost Potentially Hazardous Foods Microwave Refrigerator Cold running water\nREHEATING/HOT HOLDING Remember to keep hot foods hot! At 60°C or hotter Reheat food to its original cooking temperature as quickly as possible, then keep in a hot holding unit at 60°C or hotter\nCOOLING Cool foods as quickly as possible by: Cooling smaller batches Using shallower aluminum or stainless steel pans Ice baths Remember to keepcold foods cold! at 4°C or colder\nREVIEW QUESTIONS What temperature is the danger zone? 4°C - 60°C\nWhat do microorganisms need to live and grow? 1) Food 2) Warm temperatures 3) Time\nWhat type of law is the Food Premises Regulations? Provincial\nWhat do you use this to check the internal temperature of food? An internal probe thermometer.\nRECEIVING Food must be from government inspected sources Keep receipts Check all deliveries for signs of spoilage, damage, dirt, insects and rodents Check the temperature of hazardous foods\nMILK and DAIRY PRODUCTS All milk and milk products must be pasteurized, which means it has been heated to kill pathogens Milk must be delivered at a temperature of 4°C or colder and well before the best before date Creamers and milkettes must be kept refrigerated.\nEGG and EGG PRODUCTS Eggs must be clean, uncracked, grade A and received refrigerated at 4°C or less They must be delivered well before the use by or best before date Ungraded and cracked eggs are not permitted in food service establishments because they are a source of salmonella\nDRY STORAGE Foods in dry storage must be: - kept covered to prevent contamination - stored in its original, unopened container - stored in a clean, covered, labelled container Dried fruits, cereals, grains, sugar, flour and rice must be received in dry, unbroken packaging Check for dampness, mould, holes and tears in packaging\nSAFE STORAGE Practice the FIFO method of stock rotation: First In First Out Dry foods must also be stored food 15cm off the floor, because: 1. it is the law 2. makes it easier to clean the floor 3. makes it easier to see insects and rodents\nHACCP H azard A nalysis C ritical C ontrol P oint A food safety inspection system that review all food handling process in a food business. Also called an audit. Purpose - Identify poor food handling processes -Make corrections to the processes Goal - Prevent foodborne illness -Make the safest food possible\nHACCP cont… The HACCP system breaks a recipe into steps from receiving to service Identifies critical control points Sets food safety standards and corrective actions for each step in the recipe Helps to: find problems, correct problems and educate staff\nPERSONAL HYGIENE Remember the safest way to prevent people- to-food cross contamination is to practice good personal hygiene Practice hand washing in a separate hand washing basin If you are sick, do not work with food Avoid: coughing near food, smoking near food, putting fingers in mouth, nose, hair; sneezing near food, scratching ; chewing gum\nCLEANING & SANITIZING Post a cleaning schedule What is to be cleaned When it is to be cleaned Who is responsible What to use How to clean and sanitize For sanitizing surfaces, cutting boards, equipment and dishcloths: add 5ml of bleach to 750ml of water Replace wiping cloths and sponges on a regular basis Mix a fresh batch of sanitizing solution each day Put in a labeled spray bottle for easy use\nDISHWASHING When using a mechanical dishwasher: Wash temperature is 60 o C - 71 o C (140 o F-160 o F) Sanitizing Rinse temperature High temperature dishwasher is 82 o C (180 o F) for at least 10 seconds. Hot water is used for the sanitizing rinse. Low temperature dishwasher is 24 o C (75 o F) for at least 45 seconds. Chemicals are added.\nSANITIZE IN THE 3rd SINK FOR AT LEAST 45sec Use clean hot water at least 77 o C (170 o F) OR Chlorine bleach at least 100 ppm OR Quaternary ammonium (quats) at least 200 ppm OR Iodine at least 25 ppm Bleach']"	['<urn:uuid:91afb6e0-598e-4175-8a7f-63ed62fa7eae>', '<urn:uuid:73dbc9a2-8883-4abc-a163-9a3b1bca5cc4>']	factoid	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-12T23:00:04.658849	8	33	1644
49	how body image distortion affects chronic pain and what are psychological treatment options for body image problems	Body image distortion can significantly affect pain perception - when a limb appears magnified, patients experience more pain and slower recovery, while making it appear smaller reduces pain. For psychological treatment of body image problems, cognitive behavioral therapy is effective, helping people break existing thought patterns and learn new habits, while also monitoring and avoiding triggering experiences.	"['The term body image was coined by the great neurologist Henry Head and refers to a mental representation of one\'s physical appearance. Constructed by the brain from past experience and present sensations, the body image is a fundamental aspect of both self-awareness and self-identity, and can be disrupted in many conditions.\nDisruption of the body image can have profound physical and psychological effects. For example, body image distortion is implicated in eating disorders such as anorexia nervosa, and also leads to phenomena such as phantom limb syndrome and body dysmorphic disorder; extreme cases of the latter can lead some people to request amputation of what they perceive to be a supernumery limb.\nA new study published in Current Biology now shows that visual distortions of the body image in patients suffering from chronic pain can significantly affect their perception of painful sensations. The findings could lead to improvements in the treatment of, and rehabilitation regimes for, a wide variety of neurological and psychiatric disorders.\nTen patients suffering from chronic pain in one arm participated in the new study. They were asked to watch their own arms while they performed a standardized repertoire of ten hand movements.These tests were carried out under four different and randomized conditions. In the ""clear"" condition, the participants looked at their arms through a pair of binoculars with no magnification; in the ""magnified"" condition, they looked through binoculars with 2x magnification; and in the ""minified"" condition they looked through inverted binoculars which made their arm appear smaller than it actually was. In the control condition, they looked at their arms without any visual manipulation.\nWhile they performed the tests, the participants were asked to rate how much pain they felt, on a scale of 0-100, which was displayed on a computer screen and could be adjusted with a mouse controlled by the unaffacted hand. After each movement was executed, the researchers also assessed the extent of swelling, by measuring the circumference of the digits of the affected hand and comparing them to those on the unaffected hand.\nIn all the tests, the participants reported that the pain was worse immediately after the movements than it was before. Unexpectedly though, the extent to which the movements increased the participants\' pain depended on how their vision had been manipulated. They reported the greatest increase in pain when the image of their arm was distorted by the magnifying binoculars, and the least increase when they viewed their arms through the minifying binoculars.\nThe visual manipulations also affected the time taken for the participants\' pain to return to the pre-task level - the recovery was slowest during the ""magnified"" condition and fastest during the ""minified"" condition. Furthermore, the swelling that occurred following the movements increased more during the ""magnified"" condition than during the ""minified"", ""clear"" or control conditions. These findings therefore clearly demonstrate that making a limb look bigger than it actually is increases the pain and swelling evoked by movements and that, conversely, making a limb look smaller than it is reduces that pain.\nIt is well known that physical changes to the body alter the mental representation of it. Following amputation, for example, there is significant alteration of the somatosensory cortex, the part of the brain onto which the body is mapped. This relationship between the body and the brain is referred to as ""bottom-up"". This study, however, points to a powerful ""top-down"" effect, and therefore strongly suggests that this relationship is reciprocal.\nExactly how a distortion of the body image modulates painful sensations is unclear. It may be the case that magnifying the body leads to an enhancement of the sense of touch. Some evidence for this comes from a 2001 study which showed that the ability to discriminate between two tactile stimuli was improved by magnification of the arm, and a neuroimaging study published earlier this year, which showed that magnification of the arm alters its somatosensory representation. By the same token, minifying a part of the body would have the opposite effect on tactile sensations. In terms of cognitive function, the authors suggest that minifying the limb reduces the felt pain because it decreases the participants\' sense of ""ownership"" of it.\nRegardless of the mechanism, these findings have implications in the clinical setting, and could lead to improvements in the rehabilitation regimes administered to patients with a wide variety of conditions. Obviously, the finding that manipulating the body image can reduce pain could prove beneficial to patients with chronic pain. Less obviously, manipulating it so that body parts appear larger than they actually are could be useful for various psychiatric conditions, such as, for example, alien hand syndrome, in which a patient denies ownership of his limb and claims that it is being controlled by external forces.\nMoseley, G. et al (2008). Visual distortion of a limb modulates the pain and swelling evoked by movement. Curr. Biol. 18 DOI: 10.1016/j.cub.2008.09.031.\nMo, I\'ve tried to find your post on amputees with phantom limb pain who seemed to have some relief by watching their existing limb in a mirror but I\'ve had no success. Has any one done a study to see how that therapy alters the somatosensory cortex?\nMirror box therapy was developed by V.S. Ramachandran. I wrote about it in this post on the old blog, which includes a film clip. As far as I know, there have been no published studies on how this treatment alters the somatosensory cortex, but a quick search brought up this page about an ongoing NIH-funded clinical trial in which participants underwent fMRI before treatment, then again 2 and 4 weeks after it.\nAlas, Mo, your film clip mentioned in post #2 no longer is a live link.\nThank you for posting about the work of one of my favorite pain researchers.\n@Diane: here\'s another YouTube vid demonstrating use of the mirror box, although Ramachandran isn\'t in it.\nI\'d like to see that video you posted Mo (in post #4), but the link seems to go straight to the main blog page here, not a video, when I click on it...\nOops! Sorry. The link is fixed now.\nThere are a couple of papers on a similar topic that Patrick Haggard has been involved in (as well as the one you mention), such as Taylor-Clarke et al: they found that prolonged viewing of a distorted arm and hand altered the perceived distance between two points on that arm/hand. However, they found no difference in tactile acuity after distorted viewing, suggesting that their finding wasn\'t due to changes in primary somatosensory cortex.\nFor those who are interested in using mirrors to treat chronic pain - check out the series of postings I did on my blog - including references, books, scientific papers and videos.\nThank you for making this because I am doing a research project on pain and how it can be affected. This has helped a lot. Thanks!', 'When does a fixation on your appearance become body dysmorphic disorder (BDD)? Learn more about statistics and symptoms related to BDD.\nMany people wish to change aspects of their appearance, whether it be a desire to be thinner, taller, or have a differently shaped nose. However, when a focus on perceived flaws becomes obsessive or stressful, these may be signs of body dysmorphic disorder (BDD).\nBDD statistics show that symptoms of the condition can include spending hours grooming, checking mirrors or avoiding social situations that can trigger anxious feelings about one’s appearance. While seemingly minor or non-existent to outsiders, these perceived flaws can be intensely distressing and debilitating to someone with BDD. BDD facts and statistics reveal the widespread prevalence of this condition, as well as the groups it most commonly affects.\nPrevalence of Body Dysmorphia\nThe prevalence of body dysmorphic disorder, also known as body dysmorphia, is estimated to affect approximately 2.0% of the general population, but the condition may be more common in some groups, such as teens, women or people with a pre-existing psychiatric condition. A survey of American adults found that BDD prevalence may vary by gender, where women reported slightly higher rates of BDD than men. Several factors can also increase the risk of BDD, including experiencing childhood bullying, having a perfectionistic personality, or living with another mental health condition, such as anxiety or depression.\nBDD in Teens\nThe teenage years are marked by dramatic changes in physical appearance, which is likely why this developmental stage is when BDD symptoms tend to first emerge. BDD in teens appears to impact both boys and girls equally and often becomes more prevalent in the later years of adolescence.\nBody dysmorphic disorder in adolescence can be particularly distressing, given that the teenage years are a vulnerable time of many changes. However, in many cases, BDD symptoms in adolescence are temporary and subside as the person gets older. It is vital to monitor and provide adequate psychological support to teens with BDD, as untreated cases may persist into adulthood.\nBody Dysphoria in Men & Women\nBDD is experienced in both men and women, although women tend to have the condition at slightly higher rates. This slight discrepancy may be due to additional societal pressure placed on the importance of female appearance. However, the gender gap may be closing as men also feel increased pressure to maintain an attractive appearance. Interestingly, the presentation of BDD may be different in men and women. For example, women may be more likely to become preoccupied with their skin, stomach, weight or body hair, while men tend to fixate on body build, thinning hair or genitals.\nMen, particularly those who participate in an elite sport or appearance-driven competitions such as body-building, may be more susceptible to a subtype of BDD called muscle dysphoria. Muscle dysphoria relates to specific obsessions and concerns surrounding body size, shape or muscle mass. Muscle dysphoria can co-occur with social anxiety disorder or eating disorders.\nBDD and Co-Occurring Disorders\nWhile BDD can be diagnosed and treated on its own, it shares many features and symptoms with mood disorders, obsessive-compulsive disorder (OCD) and eating disorders. People with BDD demonstrate obsessive, neurotic, anxious and controlling characteristics, which are common in other mental disorders. People with BDD often also experience social anxiety related to how others will perceive their appearance, and may develop anorexia or bulimia in trying to alter their appearance. In rare cases, altered visual perception in BDD may be a symptom of schizophrenia, though these are typically considered separate disorders.\nExperiencing BDD in combination with a mood, obsessive or eating disorder could be additionally distressing and requires a different treatment strategy than those used when BDD is experienced in isolation.\nBody Dysmorphia and Suicidal Ideation\nThe intense distress and obsession that underlies body dysmorphic disorder can lead to suicidal ideation, as people with BDD may feel helpless in their ability to control their appearance. People with BDD are at increased risk of suicide. Fortunately, early intervention and treatment help reduce this risk.\nBody Dysmorphic Disorder and Cosmetic Surgery\nThe desire to alter or conceal one’s appearance is standard in cases of BDD. Perhaps unsurprisingly, BDD rates are reported to be much higher in those who have received plastic surgery or cosmetic dental work. Although people with BDD may feel initial relief following cosmetic surgery, appearance-related obsessions and anxieties often return if the underlying, psychological causes of BDD are left untreated.\nStatistics on Body Dysmorphic Disorder Treatment\nThe obsessions and distress experienced by those with BDD can be debilitating. With the support of medical professionals and loved ones, the prognosis of BDD can include a return to a fulfilling and obsession-free life. BDD treatment often includes:\n- Cognitive behavioral therapy to break existing thought patterns\n- Learning new habits and ways of thinking\n- Monitoring and avoiding triggering experiences\nWhile unlearning thought patterns and behaviors related to BDD can be challenging, these measures can significantly improve a person’s quality of life. This personal work is particularly important if a person lives with co-occurring BDD and addiction. If you or someone you care about is affected by BDD along with a substance use disorder, contact The Recovery Village today for more information about comprehensive treatment options.\nBjornsson, Andri S et al. “Age at onset and clinical correlates in body dysmorphic disorder.” Comprehensive Psychiatry, 2013. Accessed April 20, 2019.\nCerea, S., et al. “Muscle Dysmorphia and its Associated Psychological Features in Three Groups of Recreational Athletes.” Scientific Reports, 2018. Accessed April 20, 2019.\nHart, A. S. and Niemiec M. A.. “Comorbidity and Personality in Body Dysmorphic Disorder”. Oxford University Press, 2017. Accessed April 20, 2019.\nHiggins, S, and A Wysong. “Cosmetic Surgery and Body Dysmorphic Disorder – An Update.” International Journal of Women’s Dermatology, 2017. Accessed April 19, 2019.\nKoran, L. M., et al. “The prevalence of body dysmorphic disorder in the United States adult population.” CNS Spectr, 2008. Accessed April 18, 2019.\nPhillips, K. A et al. “Gender similarities and differences in 200 individuals with body dysmorphic disorder.” Comprehensive Psychiatry, 2006. Accessed April 20, 2019.\nPhillips, K. A. “Suicidality in Body Dysmorphic Disorder.” Primary Psychiatry, 2007. Accessed April 20, 2019.\nSchneider, S. C., et al. “Prevalence and correlates of body dysmorphic disorder in a community sample of adolescents.” Aust N Z J Psychiatry, 2017. Accessed April 18, 2019.\nSilverstein, S. M., et al. “Comparison of visual perceptual organization in schizophrenia and body dysmorphic disorder.” Psychiatry Res, 2015. Accessed April 19, 2019.\nStrother, E., et al. “Eating Disorders in Men: Underdiagnosed, Undertreated, and Misunderstood.” Eating Disorders, 2012. Accessed April 20, 2019.\nVeale, D., et al. “Body dysmorphic disorder in different settings: A systematic review and estimated weighted prevalence.” Body Image, 2016. Accessed April 18, 2019.\nThe Recovery Village aims to improve the quality of life for people struggling with substance use or mental health disorder with fact-based content about the nature of behavioral health conditions, treatment options and their related outcomes. We publish material that is researched, cited, edited and reviewed by licensed medical professionals. The information we provide is not intended to be a substitute for professional medical advice, diagnosis or treatment. It should not be used in place of the advice of your physician or other qualified healthcare providers.']"	['<urn:uuid:9bed7d21-af0d-4267-9a92-faad22b95bc9>', '<urn:uuid:27857f4d-37b3-4d4b-9bdd-c1815135f4c6>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T23:00:04.658849	17	57	2358
50	As a surgeon, I'm curious about the relationship between bowel obstructions and hernias - what are the diagnostic challenges in identifying bowel obstructions after duodenal switch surgery, and how do the symptoms compare to those of ventral hernias?	In bowel obstruction after duodenal switch, traditional diagnostic methods can be misleading because patients can have bowel obstruction while still having bowel movements and no vomiting. A CT scan with oral and IV contrast is necessary, showing dilated loops of bowel with no contrast within the lumen. For ventral hernias, diagnosis typically involves a visible bulge that increases with pressure, accompanied by pain at the hernia site. When either condition becomes strangulated, they share similar symptoms including sharp pain, nausea, vomiting, and intestinal obstruction.	['Tag: Bowel obstruction\nWe are all aware of the arterial and venous systems. Arteries take the oxygenated blood from the heart to the organs and the veins take the blood back to the lungs to unload the carbon dioxide and reload oxygen to be taken back to the organs. In addition to the arterial and venous vascular systems, we also have the lymphatic channels that flow into the lymphatic system.\nThe Lymphatic channels and system may be new to some, however, it is the third vascular network that is much less defined. The Lymphatic system collects fluids that has left the artierial/venous vascular system along their travel outlined above and take it back to the venous system. The lymphatic vessels transport this fluid to the lymph nodes throughout the body where the nodes filter the fluid of bacteria and harmful substances. Eventually, the fluid makes it way back to the venous system via the Superior Vena Cava. Additionally, Lymphatics collect the lipids within the GI tract and transport them to the venous system for metabolism. Most of the time these serosal lymphatic vessels are very small and hard to notice on the bowel.\nExample of Lymphatic channels\nThe following image is in a patient who had small bowel obstruction. The obstruction had resulted in vascular congestion at the base of the mesentery. The congestion had effected the low pressure system of the veins and the lymphatics disproportionately more that the arterial system. The white-milky tubular structures are the lymphatic channels filled with lipids.\nThere are three layers to the small intestinal lymphatic system, in the villi, submucosal and serosal layers and has the unique ability to transport absorbed intra-lumenal nutrients. There is a need for further research in the areas of health, obesity and disease in regards to the lymphatic system.\nWhenever there is a bowel resection with anastomosis made there will be a defect in the mesentery (the tissue that holds the blood supply and the nerves etc going to and from the bowel) that needs to be closed. In this particular case, the stitches that were used to close the defect were intact and yet the tissue had separated from it. The result is an internal hernia. This can cause bowel obstruction, where by a loop of the bowel can go through the defect and kink the bowel causing the blockage. In some cases, the internal hernia may reduce itself with intermittent symptoms of the bowel obstruction and in other cases it may require immediate emergent surgery. A CAT scan with oral and IV contrast is needed after Duodenal Switch to visualize the alimentary and bioliopancreatic limbs.\nSymptoms may include but are not limited to:\n- abdominal bloating\n- abdominal tenderness\n- cramping abdominal pain\n- diarrhea, constipation\n- feeling of inability to completely empty bowels\n- severe abdominal pain.\nOne of the potential complications of any abdominal surgery is Bowel Obstruction. If the treating physician (usually the primary care, or the emergency room doctor) is not absolutely clear of the anatomy of a patent post duodenal Switch or the Gastric bypass surgeries this will pose a diagnostic dilemma. In intact anatomy the GI tract start at the mouth and ends up at the rectum as a long tube. After the Duodenal Switch the small bowel has two parallel limbs, the alimentary limb brings the food down from the stomach, and the biliopancreatic limb brings down the biliopancreatic secretions. These two limbs join and form the common channel.\nIn normal anatomy, bowel obstruction may present with nausea, vomiting, abdominal cramps, inability to pass gas, and/or have bowel movements. In this case, the X-ray will also show dilated loops of bowel and if oral contrast is given with the X-ray, there will be no contrast past the obstruction. Think of it as a garden hose that has been kinked and no water is going thru.\nIn this upper GI- the contrast travels down the small bowel and the entire small bowel is the same caliber. This is normal study with no evidence of obstruction. In a patient with the DS, the patient my have the biliopancreaitc limb obstruction, with an identical X-ray as above, since the oral contrast given will never get to the biliopancreatic limb and it will not show if it is dilated or not.\nIn duodenal switch operation, a patient may have complete obstruction of the alimentary limb, with nausea, vomiting and abdominal pain, and yet have bowel movements because the content of the biliopancreatic limb is getting to the common channel. Similarly, a patient with biliopancreatic limb may have nausea, but no vomiting, because the obstructed biliopancreatic limb is not connected to the stomach and the content can’t not be expelled from the stomach.\nThe images of fluid filled loops of bowel are highly suspicious.\nIt is critical to make sure that a patient with a suspected bowel obstruction after the DS, is evaluated with the understanding that the common signs and symptoms, and the diagnostic workup will not provide an accurate picture. A patient with the DS or RNY, can have bowel obstruction and still have bowel movement, and no vomiting.\nA patient with suspected bowel obstruction should have CT scan of the Abdomen with oral and IV contrast. The cardinal findings will be “dilated loops of bowel with no contrast within the lumen of the bowel”. This is highly suspicious for bowel obstruction after DS, where the regular x ray will not pick this up. Additionally, abnormal liver function test may suggest biliopancreatic limb obstruction.', 'VENTRAL AND EPIGASTRIC HERNIAS\nVentral and Epigastric Hernia\nWhat is a Ventral Hernia?\nA ventral hernia is a bulge of tissue (internal fat called omentum, pre-peritoneal fat or intestines) through an opening in the muscles in the “ventral” or anterior part of the abdomen. When it occurs in the midline above the belly button is called epigastric hernia. Another type of ventral hernia that occurs in the area of a previous incision (incisional hernia) will be discussed in a different page.\nSymptoms of a Ventral Hernia\nPatients with ventral hernias usually have a visible bulge in the area that increases in size with coughing, straining, pushing, heavy lifting or any maneuver that increases the intra-abdominal pressure. Most of the times there is also pain or pressure at the hernia site. When the hernia is strangulated, sharp pain, redness of the surrounding skin, nausea, vomiting and intestinal obstruction can occur.\nDiagnosis of a Ventral Hernia\nMost ventral hernias are diagnosed on physical examination by an experienced physician or Hernia specialist. Sometimes with non-palpable hernias or in the obese patient additional tests are indicated to help in the diagnosis including ultrasound, CT scan of the abdomen and pelvis or MRI. A hernia is called a “reducible hernia” when the bulge can be pushed back into the abdomen. An incarcerated or a “non-reducible hernia” occurs when the contents inside the hernia sac cannot be pushed back into their normal anatomic position inside the abdomen. Incarcerated hernias can sometimes become “strangulated” when the blood supply to the organs that are entrapped (intestines or fat or other organs) becomes compromised and the tissue dies. This is a surgical emergency that requires immediate operation.\nTreatment of Ventral Hernias\nVentral hernias can be repaired either via an open approach or a laparoscopic / robotic approach. With an open approach a small incision is made in the area of the bulge and the hernia is repaired either using the placement of a prosthesis or mesh or using a “suture” technique, where no mesh is placed and the muscles and tissues are sutured together in order to close the defect. For laparoscopic and robotic approaches, three or four small incisions are made in the lateral part of the abdomen through which long instruments are placed and the hernia is repaired with mesh. At the Miami Hernia Center when we perform open repair of ventral and epigastric hernias with mesh, most of the time we place the mesh in the pre-peritoneal space, that way the mesh is not in contact with the intra-abdominal organs, decreasing the risk of complications. For very large hernias we used the retro-rectus technique where the mesh is placed “sandwiched” in between the muscles. This technique is considered the “gold standard” for open repair of large ventral and incisional hernias.\nMost open small ventral and epigastric hernias can be fixed with local anesthesia and intravenous sedation. Sometimes general anesthesia is required depending on the patient’s conditions and the size of the hernia. For laparoscopic or robotic repair, general anesthesia is required.']	['<urn:uuid:cf813f97-05a5-4edb-bee3-12932019fd66>', '<urn:uuid:d25884db-d87e-4518-9244-839c3e4a1a5c>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T23:00:04.658849	38	84	1427
51	inuit landmarks purpose archaeological significance	Inuit landmarks served dual purposes: as stone sentinels (called Inuksuit, meaning 'to act in the capacity of the human') constructed in Arctic regions, and as archaeological markers of significant settlements, as evidenced at Kittigazuit where they marked the largest seasonal gathering of Inuit known in the Canadian Arctic, occupied continuously from 1400 to 1900.	"['“…to act in the capacity of the human”\nJohn Luther Adams is one of the 21st Century’s most important composers. He has written a lot of extraordinary music for chamber ensembles, orchestras, solo instruments and specifically percussion ensembles. Most of his music draws inspiration from the outdoors, especially the landscapes of his home in Alaska where he has lived since 1978. Inuksuit (2009) was premiered at the Banff Centre in the Canadian Rockies of Alberta and received it’s US premiere on the campus of Furman University in South Carolina.\nI am fortunate to be performing the West Coast premiere at the 2012 Ojai Festival under the direction of Steven Schick. Over the past couple of months, I have been building a resource guide for percussionists who will be presenting future performances of Inuksuit. This guide is in no way complete. If you know of other resources, please let me know and I will add the links and resources to the site.\nJohn Luther Adams: The Music of a True Place\nInuksuit, with an introduction from composer John Luther Adams (Furman Concert)\nDoug Perkins Discusses the Individual Parts\nThanks to Dan Savell for letting me know about these videos. Highly recommended!\nNYC Park Avenue Armory’s Performance\nWQXR Interview, JLA, Douglas Perkins and Adam Sliwinski\nProgram Note (From Armory Performance)\nMy music has always been rooted in the earth. For over thirty-five years I’ve composed music inspired by the outdoors, to be heard indoors. After hearing my percussion cycle Strange and Sacred Noise performed in the Anza-Borrego desert, the New England woods, and on the tundra of the Alaska Range, I was moved to create a large-scale work conceived specifically to be performed outdoors.\nInuksuit is inspired by the stone sentinels constructed over the centuries by the Inuit in the windswept expanses of the Arctic. The Inuktitut word translates literally: “to act in the capacity of the human”. This work is haunted by the vision of the melting of the polar ice, the rising of the seas, and what may remain of humanity’s presence after the waters recede. How does where we are define what we do and who we are? How do we understand the brevity of our human presence in the immensity of geologic time? What does it mean to act creatively with and within our environment? The musicians of Inuksuit are dispersed over a large area. Listeners, too, are invited to move around freely and discover their own individual listening points. There is no preferred listening point, no “best seat in the house”. Rather, every listening point is potentially the best seat. You may choose to root yourself in a central location for the entire performance, listening as the music gradually expands to fill the site. Or you may choose to wander freely, following wherever your ears may lead you, discovering musical moments and spaces that no other listener may ever hear.\nInuksuit has been performed at the Banff Centre in the Canadian Rockies, on the campus of Furman University in South Carolina, and at the Round Top Festival in Texas. This performance at Park Avenue Armory, the first ever to be presented indoors, features seventy-two percussionists— fifty-four in the drill hall and eighteen in the smaller rooms on the west end of the building. Microphones located around the exterior bring the sounds of the surrounding streets into the space, turning the Armory inside out, as Inuksuit becomes part of the never-ending music of this singular city. —John Luther Adams\nOriginally posted on DrumChattr.com on June 4, 2012.\nThe photo in this post is used under the Creative Commons License: Attribution – NonCommercial – ShareAlike 2.0 Generic (CC BY-NC-SA 2.0) by thewoodenshoes’ on Flickr.com.\nYou must log in to post a comment.', ""Kittigazuit Archaeological Sites National Historic Site of Canada\nKittigazuit Archaeological Sites\nSites archéologiques de Kittigazuit\nLinks and documents\n1400/01/01 to 1900/01/01\nListed on the Canadian Register:\nStatement of Significance\nDescription of Historic Place\nKittigazuit Archaeological Sites National Historic Site of Canada is located on the island of Kittigazuit at the mouth of the Mackenzie River in the Northwest Territories. The site, set in and around the present-day village of Kitigaaryuk, encompasses a number of archaeological features, including the ruins of winter houses, a collapsed log cabin and ice house and remains from the old village of Kittigazuit. On the high ground to the north and south of the village, a number of traditional graves are visible by their marker stones. Official recognition refers to an irregular polygon around the archaeological site.\nKittigazuit Archaeological Sites was designated as a national historic site of Canada in 1978 because:\n- occupied continuously during the period 1400 to circa 1900, it was the location of the largest seasonal gathering of Inuit known in the Canadian Arctic;\n- for at least 500 years, it was used as a beluga hunting station by the Kitigaaryungmiut, the ancestors of today's occupants of Inuvik and Tuktoyuktuk.\nThe heritage value of Kittigazuit Archaeological Sites lies in its historical associations with the Inuit populations who used the site for beluga whale hunting, as illustrated by the site and its archaeological remains. Archaeological evidence shows that the Kittegaryumiut have occupied Kittigazuit continuously for over 500 years, between 1400 and 1900. The site was used for summer beluga whale hunting by Inuvialuit populations living in the area, who took advantage of the estuary as a natural trap for the whales. Approximately 1000 people used Kittigazuit and the neighbouring village of Kupuk during the summer months, making it the largest traditional Inuit community known in Arctic Canada. Remains of six large winter houses and the number of traditional aboriginal graves are still visible today. The site is also the place where Sir John Richardson, a member of Sir John Franklin’s second Arctic expedition, first met the Inuit. Additionally, it was the location of the first Hudson’s Bay Company trading post operated on the western Arctic coast. The remains of a log house and ice house used by the Hudson’s Bay Company are still visible.\nSources: Historic Sites and Monuments Board of Canada, Minutes, June 1978; December 2007.\nKey elements that contribute to the heritage character of the site include:\n- its location in and around the village of Kittigazuit, at the mouth of the Mackenzie River in the Northwest Territories;\n- its setting on a rocky island separated from the mainland by a narrow channel;\n- the integrity of any surviving or as yet unidentified archaeological remains relating to the Inuit occupation, including the visible remains of six winter houses, large cruciform-shaped houses, graves and their marker stones, cache pits, midden deposits, beluga whale skeletal parts, and other anthropogenic features of unknown function;\n- the integrity of any surviving or as yet unidentified archaeological remains relating to the Hudson Bay Company’s trading post, including the remains of the collapsed log cabin, ice house, and the company store;\n- the viewscapes from the site across the Mackenzie River.\nGovernment of Canada\nHistoric Sites and Monuments Act\nNational Historic Site of Canada\n1400/01/01 to 1900/01/01\nTheme - Category and Type\n- Peopling the Land\n- Canada's Earliest Inhabitants\n- Developing Economies\n- Hunting and Gathering\nFunction - Category and Type\n- Historic or Interpretive Site\n- Food Supply\n- Hunting or Resource Harvesting Site\nArchitect / Designer\nLocation of Supporting Documentation\nNational Historic Sites Directorate, Documentation Centre, 5th Floor, Room 89, 25 Eddy Street, Gatineau, Quebec\nCross-Reference to Collection""]"	['<urn:uuid:1b3be39a-a159-45eb-ab68-33e02f62df5b>', '<urn:uuid:e9bd698e-1146-407a-8fa6-4bfadaec3d26>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T23:00:04.658849	5	54	1237
52	what key differences between art objects and handmade craft items	Art generally refers to fine arts like glass creations, paintings and sculptures, created by people with formal art school training. These pieces tend to be expensive. Crafts, in contrast, are typically smaller-scale items made by people following passed down knowledge without formal education, often referred to as 'grassroots' in their creation.	['Nette und einfache Eiscreme-Stock-Handwerks-Ideen\nArts and Crafts\nWhat exactly are ‘arts & crafts’? Commonly, the time period ‘arts & crafts’ refers to handmade solutions that have been made independently Using the intent of creating a bit of artwork or even a decoration. Folks engage in arts & crafts as artisans and crafters – that is definitely, people that make their dwelling by generating and promoting handmade objects – together with a great number of hobbyists who get it done just for their very own pleasure. Naturally, there will also be a hidden benefit if you may make your own private items rather then owning to buy them. Arts & crafts are deeply rooted in regional and ethnic cultures, And so the details of what types of objects constitute arts & crafts differs enormously across the world.\nJust how long have arts and crafts existed?\nThe origin of arts & crafts can be found in simple bartering transactions; professional artisans and crafters would trade their talent for sustenance. The artisans and crafters would then go their know-how on to the next technology by way of an apprentice – an aid who grew up Together with the artisan for the sole objective of Finding out their trade. Artisans who passed their awareness on In this particular fashion contain ironworkers, glass blowers, weavers and blacksmiths.\nCertainly, this all transformed with the commercial revolution. A lot of the lovingly handcrafted merchandise which were formerly created by crafters and artisans could now be cheaply generated in substantial quantities. Because of this, arts & crafts as being a popular career immediately died out; unique artisans and crafters could just not contend with industrial efficiency. Right now, People people who focus on the previous staples including glass blowing or weaving are unusual, and typically generate large-priced items, in contrast to their utilitarian forbearers. Eventually arts & crafts is usually produced up by hobbyists and smaller suppliers.\nHow do you obtain arts and crafts?\nArts & crafts are generally not tough to locate. Artisans often take part at arts & crafts fairs to be able to have the opportunity to sell their wares, and permit consumers the opportunity to search and shop from many different different types of arts & crafts.\nFestivals and fairs are fantastic Places to search for arts & crafts. Quite a few sellers will hire booths at these functions so that you can reach a large audience. In addition to exhibits and fairs, lasting arts & crafts malls are also simple to search out in most metropolitan locations. Frequently, these malls include a group of semi-long term booths rented out by various artisans and crafters.\nOf course, if the buyer is familiar with precisely the sort of product they is seeking, the net generally is a excellent resource for locating the best arts & crafts product. A growing number of artisans and crafters are now posting their wares on the internet with the power look at shots and skim descriptions in their unique goods. Even so, although on-line retail revenue have taken off in other industries, arts & crafts consumers generally like the experience of obtaining and looking at the one particular-of-a-variety merchandise in individual, that has slowed down its adoption On this space.\n‘Art’ and ‘Craft’\nArtwork generally refers to good arts, including glass creations, paintings and sculptures. The people today powering the generation of those objects have normally received official coaching at art educational institutions or have researched art as a result of other courses. These artwork objects tend to be high priced.\nCrafts, Then again, are generally looked upon as scaled-down in scale, or colloquially often called ‘grassroots’ in their creation. People that make crafts are normally doing this according to passed down information with no formal education and learning. This naturally isn’t going to indicate that they are any considerably less talented than artists. ‘Arts & crafts’ then may be the merging of these two creative traits; grassroots objects made by remarkably experienced artisans and crafters.\nThe most typical arts & crafts objects\nAs stated higher than, you can find actually a great number of versions and iterations of arts & crafts objects on the globe. However, most arts & crafts could be equipped right into a handful of essential classes: handmade jewellery, pottery, woodworking, metalworking, and sculpture are some primary groups into which most arts & crafts may be assembled. Other common types of arts & crafts consist of cross-sew, weaving, crochet, knitting, embroidery, patchwork, quilting, macramé, and others. Nevertheless extra popular sorts include ceramics, wood, metal, needlepoint & textiles, and glass will work. However Many others are based upon paper and paper products, including scrapbooking, calligraphy, collages, paper mache, and origami.\nHow can you come across other arts and crafts collectors?\nThere are a selection of nationwide, regional, and local arts & crafts organizations that can provide equally artisans and collectors a place where they are able to network and achieve a way of Neighborhood. Some samples of well known corporations include things like the American Sewing Guild, the Arts & Crafts Society, the Black Crafters Guild, the house Sewing Association, the Nationwide Art Components Trade Association, the Canadian Craft & Interest Association, the American Quilter’s Modern society, the Craft & Interest Affiliation (U . s .).']	['<urn:uuid:2af7ac6b-e750-4ca4-95e4-e2cf0224b4fc>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T23:00:04.658849	10	51	879
53	What is the biggest issue in recruitment according to the text?	The biggest issue in recruitment is the 'big black hole' - when candidates apply for jobs but never hear back from the company, or only hear back months later.	"['When it comes to modern HR, there\'s no such thing as business as usual -- or, at least, there shouldn\'t be. The...\nexponential changes in culture, technology and the global economy are forcing companies and their HR departments to revolutionize how they relate to potential and current employees. Jeanne Meister, award-winning co-author of The Future Workplace Experience and founding partner of Future Workplace, an HR advisory and research firm, spends her workdays focused on just how to do that.\nMeister spoke about why AI for HR is such a disruptor, why creating an overarching HR strategy is non-negotiable and who HR should include on its change journey.\nThe great disruptor -- AI for HR\nWhich technology trends are most disrupting HR?\nJeanne Meister: Without a doubt, artificial intelligence -- especially machine learning -- is the single most disruptive technology affecting HR. The way we all work is going to change, regardless of our role. McKinsey estimates that about a third of the activities of most jobs can be automated with AI.\nSo, the question is: What are companies and individuals doing to upskill? Work is going to be radically different as organizations adopt AI to automate the … routine activities of many jobs. We\'ll each need to focus on what makes job roles uniquely human.\nSo, we have a new focus on creativity and critical problem-solving.\nWhat can companies and their workforce do today to be thinking of how AI might affect their particular job, their particular company?\nMeister: The call to action for employers and HR leaders is to provide employees with a wider array of upskilling opportunities.\nThe biggest trend in learning and development is the focus on curation rather than creation of proprietary content. Companies need to be smart and think about how to use machine learning platforms for recommending publicly available content to employees.\nAnd with some of this type of content [e.g., massive open online courses], you can get a specialization and a digital badge, and often, that only costs a couple of hundred dollars. It\'s not a big investment, and some companies are stepping up and saying, \'We\'ll pay for that.\'\nFrom customer experience to employee experience to AI\nBack to the concept of AI more broadly -- since it affects so many areas, where should companies start?\nMeister: You start by developing an AI strategy for HR. That strategy is going to impact all of the employee lifecycle. It\'s going to impact how you recruit. It\'s going to impact your strategy for learning and development. It\'s going to impact performance management and coaching because there are many new AI tools to give you that instant feedback on how you\'re doing on the job.\nHR [also needs to] develop a better and deeper relationship with the chief marketing officer and the marketing team to understand how they\'re using AI to create a more powerful customer experience.\nJust as employee experience is borrowing from customer experience, even with AI, you\'re looking to that customer focus to develop your tech focus.\nMeister: Yes, exactly. And the marketing department has the decades of experience on this as they\'ve really gone to digitalization and personalization of the marketing experience with customers. And ultimately, adopting an AI strategy for HR helps you create a much more personalized employee experience. You\'re going to understand the real needs of your employees, and you\'re better able to deliver on those needs.\nChatbots for better candidate and employee experience\nCan you provide a few illustrations of the use of AI for HR?\nMeister: The recruitment function is the first one to be massively transformed by artificial intelligence. There is a lot in that talent acquisition function that is manual and routine that immediately can be transformed with an artificial AI tool [like a] chatbot.\nSome of the new venture-funded chatbots -- one is called Mya -- say that they can automate 75% of the recruiting function. The biggest issue in recruitment is the \'big black hole.\' You go to the website, you see an awesome job, you send your resume -- and what happens? Basically, nothing. You never hear from that company, or maybe, you hear from them six months later.\nResearch tells us that that\'s impacting [potential candidates\'] perception of that employer as a brand.\nA chatbot, which some companies are developing their own … can communicate with the applicant. For example, \'Let me check the skills and competencies you have against what\'s required for the job, and make sure [we] didn\'t miss anything."" And these chatbots are giving you status updates. \'We\'re taking you to the next level.\' You don\'t expect to get an offer for every job you apply to, but you do expect to hear from the employer as to what happened to your application.\nSo, AI tools such as chatbots can really provide much more of a personalized, one-to-one experience between the company and the job candidate and lead to a higher satisfaction in that candidate experience.\nOnce the applicant gets into the pipeline and we\'ve identified skills and experience, what can we do to see: Does the candidate have an affinity for the organization [using various factors]? All of this can happen in the back end with machine learning. Now, you have vastly improved the list that you\'re now going to vet for that new opening.\nA new view of the right candidate, HR\nCurrently, there is a tendency for companies to look for applicants in terms of exact keyword matches or having done the exact same job before, a practice often executed with the help of applicant tracking systems. But such practices won\'t work as people change jobs a lot more, the gig economy grows and there is more of a need for horizontal and transferable skills. Do you think AI for HR will help solve some of these problems or just exacerbate them?\nMeister: I think it helps to solve the problems. For example, let\'s take a look at the accounting function. There are a lot of roles in that function that are ripe for automation, and that\'s one of the 10 functions listed by the World Economic Forum that is going to be massively automated between now and 2030.\nLet\'s say you\'re an accountant, and you\'re wondering, \'What does this mean for me?\' But you have deep relationship management skills. Those are horizontal skills: relationship-building and communication skills. If the company is identifying those skills, you may not think of them in relationship to an accountant, but machine learning can help you identify real core horizontal skills that can be transferable across roles and across business units and functions. And using those core skills -- and others -- that accountant could be identified as a [for example] customer success director candidate.\nJeanne MeisterFounding partner, Future Workplace\n[Core, transferable skills are what] companies will really want to recruit for. Because jobs are changing so fast, you\'re going to need talent with core skills and agility to pick up and go from one department and function to another. So, using AI for HR improves not only the candidate experience, but also the ability of the employer to identify individuals with these horizontal skills and bring them into the company and give them internal mobility opportunities.\nThat makes sense, but I think a real thinking shift will have to happen, no matter what the technology can do.\nMeister: That\'s why AI for HR strategy needs to be not only focused on the technology and whatever tools you [use], but it needs to be focused on the people, the impact on the people, the change. Change management is too weak a term here. It\'s a change evolution that\'s needed in the mindsets of companies.\nOr even revolution?\nMeister: Yes! I think that\'s what needs to shake people up, this concept of how do you [create] a revolution in your company. Think about: Who do you need to take on your [AI for HR] revolution journey? You\'re going to be surprised that it\'s often individuals and stakeholders you probably haven\'t worked with before. For example, what about your design thinkers? If you don\'t have them in your company, you need to identify them and bring them on your team. You need to start thinking from the point of view of the individual, from a human-centered point of view.\nWho else do you need? You need HR and IT, of course, and design thinking as mentioned, but also employer branding and marketing. Those are probably functions that you haven\'t developed deep partnerships with yet, but this is an opportunity to do that.']"	['<urn:uuid:3ecd1a08-bca8-45a0-a8f1-81b850ef8851>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T23:00:04.658849	11	29	1432
54	I've been researching web design psychology - how do the principles of visual design differ between minimalist long-scrolling sites and conventional grid-based layouts in terms of user engagement?	Minimalist long-scrolling sites and conventional grid layouts each approach user engagement through different psychological principles. Grid layouts work with users' habitual top-to-bottom, side-to-side scanning patterns, allowing them to quickly find content in an expected format. This aligns with users' need for familiarity and comfort with traditional standards. In contrast, long-scrolling minimalist designs create engagement through the Attractiveness Bias Principle, where users are drawn to aesthetically pleasing designs, while also providing smooth visual narratives that keep users immersed. The long-scrolling format allows content to be strategically spread out, preventing overwhelming users while maintaining their interest through interactive elements and special effects. Both approaches need to consider the Serial Positioning Effect for optimal information presentation, but they achieve this differently - grid layouts through structured positioning, and long-scrolling through careful content pacing and visual hierarchy.	['Long-scrolling, one-page websites are becoming commonplace. The above-the-fold versus the no fold debate appears to be tilting in favor of the latter approach. It has for a long time been taken for granted that a website’s most important content should be above the fold. The argument is that most users are reluctant to scroll down a page if they haven’t already found what they are looking for.\nThis argument does not hold water anymore, however. As long as the content of a homepage or landing page holds the promise of good things to come, users will gladly keep scrolling as long as the content is relevant to their needs.\nLong-scrolling and infinite-scrolling sites are sometimes portrayed as a new and modern web design trend. These scrolling techniques have actually been in use for many years. They have recently become more popular, which gives them the appearance of being new and innovative techniques.\nLong scrolling is here to stay, and for a good reason: users clearly like it.\nLong Scroll, Smart Navigation, Clever Special Effects\nThe increased use of mobile screens has definitely played a key role in the widespread acceptance of this technique. Attempting to navigate with buttons on a small screen can be cumbersome, and even more so as screens get smaller and smaller.\nHow It All Works\nLong-scrolling sites tend to work in one of two ways. Either the site has a larger homepage with links to other pages, or the site consists of a single, long page. Either setup creates a wealth of possibilities for a web designer, including the use of storytelling techniques.\nStorytelling can often be made more effective by incorporating subtle special effects, such as parallax scrolling, Ajax/jQuery, or scroll-triggered animations.\nLong Scrolling Used Primarily for Storytelling\nA Smooth Platform\nPage-by-page navigation and storytelling do not always make a good match. This type of navigation tends to be herky-jerky at best, whereas long scrolling provides a much smoother storytelling platform. Long scrolling also gives the user greater control over pacing. At the same time, its continual immersion capabilities avoids the lags between pages that can be detrimental in keeping a user-focused and fully engaged.\nGiving the User Control\nLong scrolling also allows creative special effects to be used more effectively. Parallax scrolling and scroll-activated animations are controlled, at least to some extent, by the user, giving the website an almost game-like feel; and definitely a friendlier one.\nWhy Most People Like Long-Scrolling\nMaking the Most of a Memorable Design\nYou could easily get the impression that long-scrolling pages opened the door to the use of highly innovative, memorable, visual designs. In truth, it’s the other way around. These cool design types have long been with us.\nTo put it simply, if you want your site’s visitors to keep scrolling down a long page for more content, you have to keep them interested. A memorable design will do that!\nLong Scroll, Animation, and Hover = A Potent Combo\nGetting back to the original debate, a long scrolling website will not turn users off, increase your bounce rate, or drop your conversions – but only if its design is solid and has been carefully executed. When that’s the case, bounce rates drop, and conversion rates go up.\nLong scrolling offers plenty of opportunities to discover ways to keep users fully engaged. The longer they stay on your site, the more likely they respond to your calls for action.\nManaging Lengthy Content\nOne major challenge is determining the right amount of content to be placed on a page. Too little, and it becomes harder to engage a user, while too much can overwhelm a user, cause a cluttered effect, and be detrimental to the user’s experience.\nLong scrolling offers a simple solution. Content can be spread out, thereby allowing a user to assimilate it at their own pace – and without disruptive page-navigation interruptions. This technique is ideal for storytelling, and also for disseminating information that cannot be neatly confined to a conventional web page.\nEffectively Using Horizontal Nav Bars\nHorizontal navigation bars tend to fit nicely onto most page types, but there is a limit to how many internal pages they can accommodate. Dropdown menus are one solution, but they have their own disadvantages as well.\nLong scrolling can significantly reduce the need to accommodate multiple internal pages. Instead, the nav bars can guide a user to the main portions of a site’s main page story.\nCreating Engaging Websites\nCool Uses of Metaphors\nMetaphors are not often thought of as web design elements, but they can provide a great way of keeping a user engaged throughout a lengthy story or explanation. Humans are not comfortable when faced with uncertainty, and this especially applies to website visitors.\nWeave a few simple metaphors into your long pages to counteract any feelings of uncertainty. This will prepare your visitors for what they may encounter as they proceed.\nPractical Uses of Interactivity\nAnother design technique users appreciate is strategically placing interactive elements throughout your site’s pages. These interactivities serve to give your users a mental break while at the same time maintaining their interest and keeping them focused and engaged.\nThis approach is somewhat akin to giving people a chance to get out and stretch their legs during a long road trip. It’s refreshing.\nWhy Some Users Don’t Like Long-Scrolling\nMany users, and especially mobile device users, like long scrolling offers. Some users, however, don’t particularly care for or don’t like it at all. What you need to bear in mind as a designer is that long scrolling can be ideal for some types of websites and ill-suited for others.\nDon’t just incorporate this capability because it is trendy; consider the objectives of your site, its limitations, and its overall structure – and then decide.\nSmaller Screens – Longer Scrolling\nSmall screens and long scrolling go hand in hand. Unless small screens suddenly go out of fashion, which is highly unlikely, long scrolling is probably here to stay.\nIt is quite likely that, as mobile devices evolve and their functionally improves, long scrolling techniques and functionalities will also evolve and improve – so stay tuned.\nSmall Screen Parallax Possibilities\nParallax is primarily a desktop effect at present, and it is not yet as mobile-friendly as designers and users would like it to be. Thanks to improvements in Ajax and other auto-loading strategies, this situation is bound to improve.\nParallax is more than an entertaining special effect. When properly used, it can provide a smooth visual narrative that will keep a user engaged.\nLong Scrolling & Minimalism – An Outstanding Combination\nLong scrolling can be expected to incorporate or act in concert with other trends as it increases in popularity. Take for example the trend toward minimalism. While minimalism and long scrolling do not appear to be particularly related, give a minimalistic site a long-scrolling capability, and you can realize some amazing results.\nAll of the examples above have been designed with mobile devices in mind. But they work equally well on desktops. Long scrolling techniques really do shine when touch gestures are used, but you can expect designers to find better ways to use these techniques with non-touchscreen applications.\n- Want UX Inspiration? Look To Classic Video Games\n- Critical Info: The Story Behind Building a Government COVID-19 Website\n- How to Use Negative Space in Web Design Effectively\n- Scrolling in Web Design: How Much Is Too Much?\n- Infinite Scrolling: Is it Helping or Hurting Your Business?\n- 10 Inspiring UX Design Portfolios\n- The Time-Saving Design Trend & How to Use It\n- The Rise of the Almost-Flat Design Web Trend\n- Exploring the Long Scrolling Web Design Trend\n- Exercise Caution When Using These Popular Web Design Trends', 'When it comes to website design, you want to strike the balance between conventional and unique. Meeting certain standards is important in order for your visitors to feel comfortable and use your site as they wish, but a little style and some modern features can help you stand out in the very crowded space that is the Internet. We take a look at how you can get your web design just right.\nWEB USERS OF HABIT\nWhy opt for conventional website design? Because web users are only human, and, after years of getting used to traditional web usability standards, they have certain expectations when they land on a website. Some common website elements that users expect to see are the top menu navigation bar or hamburger menu (complete with sections like “About Us” and “Products/Services”, a search bar (often contained in the top menu), and a standard grid layout of content.\nThat top menu is key because it’s the first place people look, and if they don’t find what they want quickly, they’re likely to bounce. The same can be said for the search bar; often visitors come to a website with something specific in mind to look for – so don’t make them work to find your search box. As for the grid layout, it allows a user to easily and quickly scan a webpage for content using top-to-bottom side-to-side eye movements.\nThis take on conventional design, while modern in style, gives the user the top menu navigation bar, the search bar icon at top right, and makes use of a grid layout.\nBREAKING THE MOLD\nAlthough it’s wise to adhere to certain design standards, intuitive web designers are now starting to exceed visitor expectations by providing some added value and positive user experience through unique web design. When you get it right, visitors remember you and come back, maybe even doing some of your marketing for you.\nIn some bold designs it can work to shift the positioning of the navigation bar, or to use images as navigational elements.\nThis design positions new content above the navigation bar (but still above the fold).\nGETTING DOWN TO IT: WHAT ARE USERS LOOKING FOR?\nWhen a user visits your website, they likely have in mind an action they are looking to complete, or some information they want to find. (In fact, you can actually see from a user’s behavior what their intentions on your site are, and optimize it accordingly.) Whether your design is attractive or not isn’t their first priority: what they want is a simple design that makes their experience easier. This applies to all website design elements from layouts to graphics and color schemes.\nAs important as simplicity is navigability. Users tend to move around different parts of your site while exploring or looking for something specific, and your site flows should be clear and sensical. In practice this probably means having a navigation menu at or near the top of your page as well as in the footer, and having a search bar on every page. It’s also good practice to use breadcrumbs so the navigation trail is visible.\nPeople are creatures of habit, and good web design should play into that. This means your web design should be somewhat consistent across all pages, meaning that as a user experiences your site they will get more comfortable using it. It makes no sense to surprise visitors later on and make the experience more difficult.\nPSYCHOLOGY OF WEB DESIGN\nLet’s start on the outside. The Attractiveness Bias Principle is the term given to the idea that people are drawn to and favour attractive things. So if your web design is slick and well thought-out, your users are more likely to feel positively towards it and want to stick around. But (and this is a big but), there’s only so long that this can last. If they soon realise there’s no substance beyond the surface, they’re not going to stay with you. Keep this in mind.\nVisual hierarchy is about working with your design tools (color, spacing, typography) to present information optimally. The Serial Positioning Effect tells us that our ability to remember some information accurately is subject to where it is placed in a list. You need to find helpful and effective ways to present information, especially the key information, so that it’s not lost on your users. This can be as simple as making your most important points bigger, bolder and brighter.\nClicktale was acquired by Contentsquare in 2019. Since then, tools and features mentioned in this blog may have evolved. Learn more about our Digital Experience Analytics Platform.\nBe In The Know']	['<urn:uuid:d686497e-a45b-4ef2-b997-d46c997e7a14>', '<urn:uuid:c258baa1-a937-4e84-a249-ec1c3a21efac>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T23:00:04.658849	28	133	2069
55	campus patrol methods uc villanova difference	Both institutions provide patrol coverage through similar methods - University of California Police Officers patrol on foot, bicycle, and motor vehicles, while Villanova's 75-member Public Safety team also patrols the campus on foot, on bicycle, and in vehicles.	['University of California Police Officers generally provide for the peace, safety, and security of persons and facilities on the university campus and those properties owned, controlled, or occupied by the University of California. It is the function of the patrol unit to provide twenty-four hour uniformed patrol coverage to all areas of the university. Officers provide preventive patrols on foot, bicycle, and motor vehicles; receive, process, and investigate complaints by citizens; respond to emergencies such as crimes, accidents, injuries, illness, and fires; protect persons and property through crime prevention; arrest offenders; appear in court; provide traffic control and enforcement; attend community meetings; perform miscellaneous services related to public health and safety; safeguard the custody and disposal of evidence and lost and found property.\nThe Rescue unit provides the university and surrounding community with\na paramedic ambulance service that operates twenty-four hours\na day. The unit is staffed with full-time Paramedics and part-time\nTechnicians that are also full-time UCSB students. The Rescue\nunit contributes to the medical readiness of the community by\ndelivering emergency medical\naid and teaching cardiopulmonary resuscitation classes to campus\ndepartments and the public. The unit actively participates in\nthe skill development\nof emergency medical technicians and paramedics by participating\nin training seminars and supporting the county’s annual Emergency\nMedical Services Conference. The Rescue unit responded to six\nhundred and sixty-four medical calls this year, transporting\nfour hundred and fifty eight patients to a medical facility.\nTwo hundred and six of the medical calls were for first aid assistance,\nnot requiring patient\nThe Rape Prevention and Education Program (RPEP) is a personal safety program jointly sponsored by the UCSB Police Department and the UCSB Women’s Center. Police and RPEP staff work together to educate the campus community about sexual assault prevention and to develop strategies for a safer environment. RPEP staff is also available to provide education, aid, and counseling to survivors of sexual assault.\nRape prevention workshops, discussions, and seminars are available to all students, staff, and faculty, both on-campus and in the surrounding community. Workshops focus on prevention, resources and assistance.\nThe Investigations unit fulfills the community’s need for advanced resources in criminal investigation, crime prevention, personal safety, dignitary protection, and liaison to outside agencies. The unit investigates all major felonies, crimes against persons, property, and general crimes. The supervisor of the unit acts as the coordinator for the hiring process for police officers.\nThis unit is responsible for managing the department’s in-service\ntraining, coordination of all crime prevention programs, approval\nof campus alarm systems, including the 5-point security inspections,\nand other staff\nprojects. This position is also responsible for media relations,\nupdating the department web page with logs and alerts, and acts\nas the Public Information Officer.\nThe Community Service Organization (CSO) consists of students who work\nwith the Police Department in serving the campus community. The\nCSO program began in 1972 as a communication link between the police and\nand has grown to fulfill safety and security needs of the university.\nCSOs provide public assistance and personal safety escorts, complete\nsecurity checks, and detect safety hazards in campus facilities. CSOs\nsecurity for campus events and residence halls, to meet the special\nneeds of these areas. In response to the needs of over fourteen thousand\nat UCSB, CSOs conduct a comprehensive safety program of bicycle\neducation, licensing, and enforcement. Throughout 2004, the CSOs have\nin a campaign to educate the public and regulate behavior pertaining\nStock” (motor vehicles, bicycles, skaters, in-line skaters and skateboarders)\nissues in the central campus.\nThe Isla Vista Foot Patrol (IVFP) was initiated in 1970 in the interest of developing “community based policing” for Isla Vista. The UCSB Police Department works with the Santa Barbara County Sheriff’s Department (SBSO) and the California Highway Patrol (added in 1995) to staff the IVFP. While the SBSO maintains overall jurisdictional responsibility for Isla Vista, the UCSB Police Department and California Highway Patrol provide a cooperative role in serving the special needs of a highly diverse population of over nineteen thousand people. The UCSB Police Department also serves an important role in representing the university’s interests in matters of safety and security. The IVFP provides police uniformed public safety services on foot, bicycle and in motor vehicles.\nThe Records unit provides information resources to university departments and law enforcement affiliates. The Records unit serves as a “clearing house” for information developed and recorded by all units of the Police Department. Services include police record maintenance and control; employment identification processing; records liaison with other university departments.\nThe Communications unit provides radio and telephone support services to police personnel and the public. Services include: dispatching police department response to all calls for emergency and routine services; providing information and assistance to the public; monitoring alarm systems for fire, hazard, security, and intrusion; and maintaining crime and traffic record data systems.', 'It is important to understand the procedures Villanova University has in place and how it proceeds and responds in the case of an emergency on campus.\nIn addition to day-to-day coverage by our 75-member Public Safety team, the University has an extensive Emergency Preparedness Plan that includes procedures to ensure a rapid response to an unanticipated emergency or threatening situation. Villanova takes all incidents affecting student, faculty, and staff safety as serious and time critical.\nProviding a Safe Campus\nVillanova University has devoted significant resources to providing a safe campus and assuring the safety of our community members. Our Public Safety department is responsible for campus security and consists of approximately 75 officers who provide on-campus coverage 24 hours a day, seven days a week. To assure the greatest breadth of coverage, the officers patrol the campus on foot, on bicycle, and in vehicles.\nThe Department of Public Safety employs both sworn police officers as well as non-sworn security personnel. University Police Officers have full arrest powers and several defensive weapons, including firearms. Public Safety has 23 sworn Police Officers, along with approximately 45 Security Officers. The officers are well-trained professionals, including some who are Emergency Medical Technicians (EMT’s). The Department also works closely with both the Lower Merion and Radnor Police Departments, as well as the Delaware County Office of Emergency Services.\nWe encourage members of the campus community or any visitor to immediately report suspicious activity or an emergency to the Public Safety department by calling 9-4444 from an on-campus telephone or 610-519-4444 from an outside telephone or cell phone. If an emergency requires the assistance of an outside EMS, Fire, or Police agency, the Public Safety department will contact the appropriate agency as the situation dictates.\nVillanova University Crisis Response\nIf a situation arises on or around campus that could potentially threaten the health or safety of University community members, senior officers of the University and members of Public Safety are immediately summoned as part of Villanova’s crisis response procedure. This group determines the appropriate actions to be taken, using Villanova University’s Emergency Preparedness Plan as its basis and guideline.\nThis action may include:\n- Deployment of Public Safety officers\n- Engagement of law enforcement officials, if appropriate\n- Cancellation of classes\n- Patrol of the campus perimeter\n- Closing of roads and entrances onto campus\n- Lockdown of campus buildings and residence halls\n- Evacuation of campus buildings and residence halls\nThe University has the capability via the Wildcard system to control access to virtually all buildings and entrance gates on campus. This is an access control system, not a security system, but it can regulate what buildings and rooms cardholders may enter.\nEmergency Communication Process\nIn the event of an emergency, an effective University-wide communications process is vital in order to provide the greatest safety possible for our students, faculty, and staff.\nWe will use all available means of communications in a crisis situation, including text messaging, telephone, the Web site, and e-mail. The University will also implement on-site personal contacts by utilizing Public Safety officers and other University staff and officials. The communications tools may include:\nVillanova Web Site\n- Critical information is posted on the University’s home page and may be viewed both internally (students, faculty, and staff) and externally (parents, alumni, and other constituents)\n- Broadcast e-mails are sent to Villanova students, faculty, and staff, as well as alumni, if appropriate\n- Broadcast voice mails are sent to faculty, staff and RA students, and can be accessed both on and off campus through their individual voicemail; Nova alert is used primarily to broadcast to students as well as faculty and staff; emergency phones are installed in all residence halls and are operational for campus calling, 800 number calling, 911 emergency and contacting public safety directly.\n- Critical information is posted on the Villanova Emergency Hotline 610-519-4505; this information is accessible from on or off campus to any constituent, including parents.\n- Important emergency alerts, notifications and updates are sent to all registered devices, including cell phone (via SMS/texting); email account (work, home, other); smartphone/PDA (iPhone, BlackBerry, and other handhelds devices)\n- This opt-in service requires registration.\n- Parents of Villanova students can also register for Nova Alert.\nPersonal Interaction and Connection\n- Public Safety personnel and other University staff can make personal contact with persons in buildings on campus\n- Many buildings on campus have a designated emergency response contact who is trained in responding to an emergency\nVillanova University Emergency Guidebook\nVillanova University developed an Emergency Guidebook to give members of the University community a ready-reference on how to respond in case of an emergency situation on or near the campus. Campus safety is a responsibility we all share. All of us play a critical role in keeping students, faculty, and staff safe on campus. Click here to view the University’s Emergency Guidebook.\nMoving Forward: Assessment of Our Emergency Plans\nVillanova routinely examines the Emergency Preparedness Plan and devises measures to respond to various incidents as quickly and effectively as possible. Villanova University President, the Rev. Peter M. Donohue, OSA, has asked the senior administration to regularly review and assess the Emergency Preparedness Plan, as well as our communication strategies to assure we have accounted and prepared for as many scenarios as possible.\nIn the event of an emergency on campus or off, your emotional well-being is also critically important to us. We provide full-service counseling services, as well as access to Campus Ministry staff, who are trained to provide grief counseling. If you would like further information about the specific services we make available to our students, faculty, and staff, you may find the following links helpful.']	['<urn:uuid:e1d8c8ad-4c29-47a7-bf8c-e5f8aed9dde1>', '<urn:uuid:54e90f17-6f89-4c95-91f8-e85f116822ce>']	factoid	direct	short-search-query	distant-from-document	comparison	expert	2025-05-12T23:00:04.658849	6	38	1751
56	What activities complement main sport training and prevent burnout?	The training program should include diverse supplementary activities like boxing, wrestling, karate, kayaking, water polo, mountain biking, yoga, basketball, and various football codes. These activities serve two purposes: they develop broad physical skills to supplement the main sport's specific skills (following the Soviet approach), and they introduce potential post-career interests. To prevent burnout, these activities should be scheduled as the last session of each training day. Additionally, this variety helps prevent overtraining by incorporating cross-training principles and providing different intensities of exercise, while also helping athletes maintain desire and enjoyment in their training.	"['Author Ashley Jones In season training is often problematic with trying to fit in all…\nIdeas for Off Season Training – Ashley Jones\nAuthor Ashley Jones\nAfter 15 off seasons in preparing players for the rigours of professional team sports competition, I would like to suggest a few ideas that may spark ideas for you to optimize the quality and quantity of your program for you and the players entrusted to you.\nI think that you need to be specific in your creative programming, I would divide the players into three (3) groups, based on there major priority for the upcoming training period:\nNeural Mechanical Metabolic\nFrom there you can framework the off season around achieving significant improvements in the primary area with the other areas becoming secondary and tertiary goals, this is irrespective of the position on the field that they play.\nSo key areas for each would be:\nNeural – speed, power, Olympic lifting focus, plyometrics\nMechanical – size, strength, technique, power lifting/bodybuilding focus\nMetabolic – aerobic, anaerobic, run/cross training focus, GPP\nFor neural I would reverse periodise and work short to long, whereas in metabolic I would classic periodise and work long to short but also use some concurrent training, and mechanical I would use a modified classic Westside protocol, the series of articles entitled “Westside for Skinny Bastards” on www.elitefts.com or www.t-nation.comis a great starting point.\nAlong with your specific groupings you may have groups of players who because of their respective training ages need to have a reduced training load, two groups which we have created is a 75% of total training time group which is less than or equal to 2 years of professional training and a greater than or equal to 8 years of professional training, for the second group the specific program is generated through one on one chats, as I feel they know how to get their body in the condition you need them to be to play the game you want from them. The rest of the squad will fall into the major training groupings.\nI would try and limit each session to a 30 or 60 or 90 minute time frame, since we need to add sessions such as stretching, yoga, pilates, prehab/rehab to the overall plan as well.\nTime allocation is critical and should reflect your main focus, I also like adding an activities plan to the off season for two reasons;\ni) Firstly, the development of a broad focus of physical skills from a variety of sports to supplement the specific skills they get from the main sport, a very much former Soviet approach where athletes are exposed to as many sports as possible in the developmental phases of their training; and secondly\nii) Exposure to a variety of activities will introduce players to possibilities for post career that they may wish to take up that otherwise they may have never had an opportunity or interest in.\nSome of these activities maybe boxing, wrestling, karate, kick boxing, kayaking, water polo, mountain biking, yoga, basketball, netball, korfball, volleyball, Gaelic football, handball, Australian Rules, football. All these sports/activities add to the physical development plan by challenging players both mentally and physically. I would select maybe 5 or 10 (if you want them to have a choice between activities) and schedule them for the last session on each day of training.\nSo depending on the priority of your program, the number of sessions for each type of specific training will be as follows:\nNeural – Intensity and recovery are the key, again with Olympic lifting you may go as high as 10 sessions per week but these may only be one or two exercises per session, speed sessions 3 – 5 per week.\nMechanical – Depending on your preference could be 4 sessions per week or up to 8 if you move to a more double day body building split style.\nMetabolic – 5 sessions per week, cross training (indoor rowing, versa climber, Grappler, bike, grinder, swim), anaerobic (fitness) games, hill repeats, repeated speed, cross country running, General Physical Preparedness (GPP) following weight training sessions, strongman circuits, weight training complexes and circuits, of course they will also gain in this area from the activities training they do as well.\nI also like to keep off season training from Monday to Friday only and not take any time on the weekend since we take most weekends during the season, leave that for recovery and family time.\nOur current program is attached to give you an idea how we put things together, enjoy, it is great fun putting all the pieces together and watching the plan come together.\nHello Ash, based on your off season programs (5 days of weight training) ,I would like to know if they could also be used by centres-winger? By the programing that I read in the crusaders off season, you only have “finishers” do a 3 day weight training program. I do not really need to put more then 2-5 Kg`s top, my real goal is to improve my conditioning.\nMy other question is, how do you set the “ideal playing weight”? I am 1.81 mts tall and weight 90 KG (my body fat is 10%) and I play centre, I would like to be at 93-95 but I don`t know if that would be good for me In season.\nHi Nico, the reason the outsides only do 3 weights workouts is because they do a lot more speed training and I see this as more important, you could do the 5 days per week weights but I think it would impact on your ability to recover and do the speed work at maximum intensity to ensure gains. I do not think there is a perfect weight for any player in any position and feel that the weight you are talking about sounds just right although some centres are over 100kg, so perfect is what is right for you and that is just a trial and error situation I had a player once who like to be closer to 90kg during Super 14 since it was a lot faster game and more expansive but wanted to be closer to 95kg for test match rugby, hope that helps, ash', ""How to Prevent Overtraining and Overreaching\nSerious athletes have no time to break from training, right? Wrong. Conditioning for your sport is not all about work, activity and movement. You also have to rest, relax, and recover (or restore, recuperate, and regenerate). Overtraining can lead to a number of serious consequences and set your training back weeks or even months.\nAre you overtraining (pushing your body too far over the course of a training program) or overreaching (going too hard in a single workout or series of workouts)? You've gone too far if you're experiencing any of these symptoms:\n- Sympathetic overtraining syndrome, where your resting heart rate, blood pressure, and metabolic rate are abnormally elevated\n- Parasympathetic overtraining syndrome, where your resting heart rate and blood pressure decrease abnormally\n- Emotional instability like fatigue, apathy, depression or irritability\n- Decreased desire for and enjoyment of training\n- Decline in performance\n- Loss of muscle strength\n- Weight loss and loss of appetite\n- Prolonged recovery from training sessions, which can include tenderness and soreness in muscles and joints\n- Sleep disturbances\n- Gastrointestinal disturbances\nOveruse injuries often result from repeated, abnormal stress applied to a muscle, tendon, ligament or bone—by doing too much, doing too much too soon, or not taking enough time to recover and recuperate (Baechle and Earle, 2008). They can also occur as a result of training errors, faulty technique, decreased flexibility or insufficient strength. Find out if you're overtraining or underrecovering.\nHow to Prevent Overtraining\nAre you guilty of overtraining or overreaching? Start incorporating some of these ideas into your training.\n- Design a good training program that incorporates sound exercise principles, including rest days\n- Design a program that is appropriate for your level of conditioning\n- Use the principles of cross training (variety of activity)\n- Use the principles of interval training (variety of intensities)\n- Learn to control your stress in daily life so your body can recover from exercise sessions\n- Get enough sleep to allow your mind and body to recover from workouts\n- Get a massage periodically\n- Use self-massage tools after a workout or on rest days—e.g., massage stick, foam roll, or small massage balls\n- Use a steam bath, sauna, or whirlpool, as needed\n- Eat a balanced healthy diet so that you replenish fluids and nutrients needed for recovery\n- Take a vacation several times a year to allow your mind and body to recharge; an often overlooked area in training is how your daily life impacts recovery from exercise, workouts, and sports competition\nYou can also prevent overtraining and overreaching by tweaking aspects of your personal life, such as:\n- Cutting back on smart phone time, including texting, web surfing, checking emails, and talking, to give your brain a break\n- Driving slower in your community so that your body is not on overdrive (not to mention that it's safer for you and other people on the road)\n- Keeping your personal finances simple and in order\n- Enjoying time with family and friends\n- Avoiding being a constant weight watcher. Don't micromanage every second of your life\n- Learn to complain less throughout your day\nAmerican College of Sports Medicine (ACSM). ACSM's Resource Manual for Guidelines for Exercise Testing and Prescription, 6th ed. Philadelphia; Wolters Kluwer Lippincott Williams & Wilkins, 2010.\nBaechle TR, Earle RW, eds. Essentials of Strength Training and Conditioning, 3rd ed. Champaign, Ill.: Human Kinetics, 2008.\nKreider RB, Fry AC, O'Toole ML, eds. Overtraining in Sport. Champaign, Ill.: Human Kinetics, 1998.\nRichardson SO, Andersen MB, Morris T. Overtraining Athletes: Personal Journeys in Sport. Champaign, Ill.: Human Kinetics, 2008.\nVerkhoshansky Y, Siff M. Supertraining, 6th ed. Rome, Italy: Verkhoshansky, 2009. www.verkhoshansky.com and www.melsiff.com.""]"	['<urn:uuid:94f13ee7-c169-456d-8c7d-43f196a75b48>', '<urn:uuid:eece625b-c80a-4ab3-9588-c1b3ad4c937a>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T23:00:04.658849	9	93	1663
57	How do the ideal temperature conditions differ when making regular milk kefir compared to water kefir for the best results?	For milk-based kefir, the fermentation temperature range is specifically studied between 25-35°C, with maximum pH reduction rate achieved at 35°C using semi-skimmed milk (1.7% fat). For water kefir, while specific optimal temperatures aren't directly stated, the culture generally requires temperatures between 68°F to 78°F (approximately 20-26°C) for proper fermentation. If temperatures are cooler, special care must be taken to maintain proper temperature range for the cultures.	"['Mathematical Modeling of pH Variation as a Function of Temperature and Time in Kefir Production\nA mathematical model was developed to describe the effect of temperature and fermentation time on the kinetic parameters of pH change by kefir grains yeast population, using whole milk, semi-skimmed milk and skim milk of cows. Fermentation temperature (25–35 ºC), total fat level (3.0, 1.7, 0.15 %) and inoculum level (2%) w/v had simultaneous effects on the acidification process in kefir fermantation. The changes in pH of pasteurized cow milk inoculated with 2 % culture were investigated during fermentation at 25-35 ºC. Measurement of pH change was followed first order kinetics during kefir fermentation. The optimal kinetics model for pH change during fermentation of kefir was the linear mathematical model. Furthermore, statistical analysis indicated that fermentation temperature and time significantly affected pH change of kefir. pH reduction rate of kefir was maximum at semi-skimmed milk (1.7 %) at 35 ºC.\nKefir, pH, Kinetic, Milk Fat, Fermentation, Time\nBosch, A., Golowczyc, M. A., Abraham, A. G., Garrote, G. L., De Antoni, G. L., & Yantorno, O. (2006). Rapid discrimination of lactobacilli isolated from kefir grains by FT-IR spectroscopy. International Journal of Food Microbiology, 111(3), 280-287.\nChen, H.-C., Wang, S.-Y., & Chen, M.-J. (2008). Microbiological study of lactic acid bacteria in kefir grains by culture-dependent and culture-independent methods. Food Microbiology, 25(3), 492-501.\nGüzel-Seydim, Z. B., Seydim, A. C., Greene, A. K., & Bodine, A. B. (2000). Determination of Organic Acids and Volatile Flavor Substances in Kefir during Fermentation. Journal of Food Composition and Analysis, 13(1), 35-43.\nIsmaiel, A. A., Ghaly, M., & El-Naggar, A. (2011). Some physicochemical analyses of kefir produced under different fermentation conditions. Journal of Scientific and Industrial Research (JSIR), 70(5), 365-372.\nLeite, A. M. O., Mayo, B., Rachid, C. T. C. C., Peixoto, R. S., Silva, J. T., Paschoalin, V. M. F., & Delgado, S. (2012). Assessment of the microbial diversity of Brazilian kefir grains by PCR-DGGE and pyrosequencing analysis. Food Microbiology, 31(2), 215-221.\nLevenspiel, O. (1974). Chemical reaction engineering. India: Wiley Eastern Publication, New Delhi\nMagalhães, K. T., Dragone, G., de Melo Pereira, G. V., Oliveira, J. M., Domingues, L., Teixeira, J. A., e Silva, J. B. A., & Schwan, R. F. (2011). Comparative study of the biochemical changes and volatile compound formations during the production of novel whey-based kefir beverages and traditional milk kefir. Food Chemistry, 126(1), 249-253.\nMotaghi M, Mazaheri M, Moazami N, Farkhondeh A, Fooladi MH, & EM., G. (1997). Short Communication: Kefir production in Iran. World J Microbiol Biotechnol, 13(5), 579-581.\nÖner, Z., Karahan, A., & Çakmakçı, M. (2010). Effect of different milk types and starter cultures on kefir. Gıda, 35(3), 177-182.\nÖzdestan, Ö., & Üren, A. (2010). Biogenic amine content of kefir: a fermented dairy product. European Food Research and Technology, 231(1), 101-107.\nTorriani, S., Gardini, F., Guerzoni, M. E., & Dellaglio, F. (1996). Use of response surface methodology to evaluate some variables affecting the growth and acidification characteristics of yoghurt cultures. International Dairy Journal, 6(6), 625-636.\nVarghese K, S., & Mishra, H. N. (2008). Modelling of acidification kinetics and textural properties in dahi (Indian yogurt) made from buffalo milk using response surface methodology. International Journal of Dairy Technology, 61(3), 284-289.\nWitthuhn, R. C., Schoeman, T., & Britz, T. J. (2005). Characterisation of the microbial population at different stages of Kefir production and Kefir grain mass cultivation. International Dairy Journal, 15(4), 383-389.\nZajšek, K., & Goršek, A. (2010). Mathematical modelling of ethanol production by mixed kefir grains yeast population as a function of temperature variations. Biochemical Engineering Journal, 49(1), 7-12.\nZanatta, P., & Basso, A. (1992). A new approach to the characterization of Streptococcus salivarius subsp thermophilus based on acidification rates. Lait, 72(3), 285-295.', 'We\'ve compiled this list of FAQs to help you learn more about water kefir! From water kefir basics to choosing ingredients to instructions for making water kefir, these FAQs cover nearly everything you need to know to start making water kefir at home, whether you are just starting to experiment with culturing or are experienced with fermented foods. If you still haven\'t found what you\'re looking for, be sure to check out our full collection of expert advice on making water kefir at home or reach out with more questions. We\'re here to help!\nWHAT IS WATER KEFIR?\nWater kefir is a probiotic beverage made with water kefir grains. Water kefir grains can be used to culture sugar water, juice, or coconut water. A powdered Kefir Starter Culture may also be used to culture coconut water or fruit juice.\nWhat are water kefir grains?\nWater kefir grains consist of bacteria and yeast existing in a symbiotic relationship. The term ""kefir grains"" describes the look of the culture only. Water kefir grains contain no actual ""grains"" such as wheat, rye, etc.\nWhat is the difference between water kefir grains and powdered kefir starter culture?\nThere are three primary differences between water kefir grains and powdered kefir starter:\n- Water kefir grains have a larger number of probiotics than the powdered starter culture.\n- With proper care, water kefir grains can be used indefinitely to make water kefir. Powdered starter culture can be reused for a number of batches, but will eventually stop culturing.\n- Powdered kefir starter culture has a smaller initial investment cost than do water kefir grains; however, you will need to continue to purchase new culture.\nWhat ingredients go into making water kefir grains?\nOur water kefir grains are grown in filtered water and organic sugar.\nDoes water kefir have the same benefits as milk kefir?\nWater kefir contains fewer strains of bacteria and yeasts than milk kefir. On the other hand, water kefir contains far more than other cultured products, like yogurt or buttermilk.\nWhat strains of yeast and bacteria does water kefir contain?\nWhile the probiotics can vary with each batch made with water kefir grains, a list of bacteria and yeasts generally found in water kefir grains can be found in our article, Composition of Water Kefir Grains: Bacteria & Yeasts.\nFor our powdered starter cultures, a list of ingredients can be found on each product page.\nI want to consume milk kefir but I\'m allergic to dairy. Is water kefir a good alternative?\nYes! Water kefir contains no dairy and are grown in filtered water and organic sugar.\nDoes water kefir contain gluten?\nNo, water kefir grains do not contain gluten.\nAre water kefir grains reusable?\nYes, water kefir grains are reusable. Once a batch of water kefir is finished culturing, simply remove the water kefir grains and place them in fresh sugar water, juice, or coconut water. (Although reusable, it is not advisable to use your only set of grains in juice/coconut water, as this can be very hard on them. It\'s best to use a separate set of grains.)\nThe powdered kefir starter culture may also be resued several times. Simply follow the instructions for Making Kefir with a Direct-Set Starter Culture.\nHow long do water kefir grains last? How long does a powdered kefir starter culture last?\nIf cared for properly, water kefir grains have an unlimited life span and can be used repeatedly to make water kefir.\nKefir made with a powdered kefir culture (direct-set) can often be recultured from 2-7 times. The exact number of successive batches will depend on the freshness of the kefir and hygienic practices employed.\nWhat is the process for making water kefir?\nWater kefir can be made using a starter culture or reusable water kefir grains. You find instructions for each method below:\n- How to Make Water Kefir Using Water Kefir Grains\n- How to Make Water Kefir Using a Kefir Starter Culture\nCan I rehydrate the water kefir grains longer than the recommended 3-4 days?\nWe strongly recommend moving the water kefir grains to fresh sugar water after four days, maximum. The grains can starve if left longer without fresh food.\nCan I drink the sugar water used to rehydrate my water kefir grains?\nWe don\'t recommend drinking the rehydration liquid. Since the water kefir grains spent their time rehydrating rather than consuming the sugar, the resulting solution will likely contain a large amount of sugar. Also, the sugar water may taste unpleasant, as part of the rehydration process involves the rebalancing of the yeast and bacteria that comprise the water kefir grains.\nHow long does it take to make water kefir?\nWater kefir generally takes 24-48 hours to culture. The exact time will vary depending on environmental factors, the most important of which is temperature. Allowing the water kefir grains to culture for longer than 48 hours puts you at risk of starving the grains and potentially damaging them.\nMy house is colder than 68ºF, how can I culture water kefir?\nMany homes maintain temperatures that are cooler, especially in the winter. For tips on keeping cultures within proper temperature range, see our article, Cold Weather Care for Starter Cultures.\nHow will I know if I\'ve successfully made water kefir? How do I know if I shouldn’t drink it?\nIf your grains are working, the liquid may lighten in color and turn cloudy. The flavor may become less sweet and it may have a slightly tangy or sour aroma and flavor. If your water kefir ever looks, smells, or tastes unpleasant, we always recommend that you refrain from consuming it.\nWhat does water kefir taste like?\nThe taste of finished water kefir varies greatly, depending on the sugar used and the culturing time. Water kefir can be fairly sweet and may have a flat taste unless bottled. Most people prefer to add flavoring to water kefir before consuming.\nHow long can I store the water kefir in the refrigerator?\nFinished water kefir can be stored as follows:\n- At room temperature (68° to 78°F): 3 to 4 days\n- In the refrigerator (40° to 45°F): 2 to 3 weeks\n- In the freezer (0° to 25°F): 1 to 2 months or longer\n- Storage recommendation: Refrigerate\nWhat amount of water kefir grains do I need to make water kefir?\nTo culture 1-2 quarts water kefir, use 3-4 tablespoons of hydrated water kefir grains.\nDo I need to rinse the water kefir grains off between batches?\nNo. There is no need to rinse the water kefir grains between batches, and regular rinsing may be detrimental to the health of the water kefir grains.\nDo I need to wash the jar/container between batches of kefir?\nWe recommend using a clean container for each batch of water kefir.\nCan I use a plastic container to brew water kefir and plastic bottles to store it?\nTheoretically food-grade plastic shouldn\'t cause any damage to the culture but we always recommend glass when working with starter cultures, due to the potential of plastic to leach undesirable chemicals.\nHow can I flavor water kefir?\nThere are endless ways to flavor water kefir. Our Ginger Water Kefir Kit includes everything you need to make ginger flavored water kefir at home. For more detailed instructions on flavoring water kefir, watch our How-To Video on How to Flavor Water Kefir.\nCan I add flavoring to water kefir while it\'s culturing?\nWe do not recommend adding fruit or other flavoring to the water kefir with the grains. Some fruits and other flavoring may be damaging to the water kefir grains.\nCan I use juice to make water kefir?\nYes, some fruit juices can be used to make water kefir. However, using water kefir grains in juice and then moving them to sugar water may result in unpleasant-tasting water kefir. We recommend maintaining two sets of water kefir grains instead, one for juices and one for sugar water. Alternately, add juice as flavoring after the grains have been removed.\nCan I use coconut water to make water kefir?\nYes, coconut water can be used to make water kefir. Before getting started, make sure your grains are fully hydrated and have been culturing in sugar water for at least 3 weeks prior to using coconut water. Alternate coconut water batches with sugar water batches to keep the grains healthy.\nCan I use honey to make water kefir?\nWe do not recommend using honey to make water kefir.\nWhat type of sugar should I use to make water kefir?\nA number of kinds of sugar can be used to make water kefir, however we do not recommend making water kefir with honey, molasses, or sugar substitutes like agave, stevia, monk fruit, or Splenda. These can either be damaging to water kefir grains or not provide them with the proper ""food"" to survive. Consult our tutorial Choosing Ingredients for Making Water Kefir for more information.\nDo I need to add sugar if I\'m using juice to make water kefir?\nNo, there should be enough sugar in the juice to feed the water kefir grains.\nHow much sugar does finished water kefir contain?\nThe remaining sugar in finished water kefir will vary depending on ingredients used and culturing conditions. Generally speaking, water kefir cultured for 48-hours will result in less remaining sugar than water kefir cultured 24-hours under similar conditions.\nWhat type of water should I use to make water kefir?\nUse water as free from contaminants as possible, for the health of the water kefir grains. For more information, consult our article, Choosing Ingredients for Making Water Kefir.\nHow much alcohol does water kefir contain?\nAs with all cultured and fermented foods, a small amount of naturally occurring alcohol is typically present in the finished product. Although the amount will vary from batch to batch, for the typical brewing period, the amount should be quite low.\nCan I use water kefir grains to make alcohol?\nYes. Culturing water kefir grains in 100% juice (especially with added sugar) for several days will result in a higher alcohol content. Also, it is possible to make a type of beer using water kefir grains.\nWill water kefir grains multiply?\nWater kefir grains are known to multiply, but at times they are reluctant to do so. Even if they do not multiply, with proper care, water kefir grains can be used repeatedly to brew water kefir. For more information, consult our article, Encouraging Water Kefir Grains to Multiply.\nWhat supplies do I need to make water kefir?\nMaking water kefir does not require any specialized equipment. The Water Kefir Starter Kit includes everything you need to get started aside from a glass jar and water. Read our tutorial How to Make Water Kefir for more information.\nCan I use a metal strainer with my water kefir grains?\nWhile a plastic mesh strainer is preferred, stainless steel is acceptable. Avoid all other types of metal when working with water kefir grains.\nHow do I take a break from making water kefir?\nDetailed instructions can be found in our article How to Take A Break from Making Water Kefir.\nIf I’m making other cultured foods (yogurt, sourdough, kombucha, etc.), how far apart do I need to keep the water kefir culture?\nWe suggest a distance of at least 4 feet between cultures. When stored in the refrigerator with a tight-fitting lid, there is no need to keep distance between cultures.\nMy water kefir grains are multiplying rapidly. Is there a point at which I must remove some of the grains?\nWe recommend using 3-4 tablespoons of water kefir grains to culture 1-2 quarts of sugar water. More grains cause the culturing process to progress very quickly, so it is best to remove any amount above 4 tablespoons, to avoid over-culturing and to impart the best flavor.\nExtra water kefir grains can be used to culture another jar of water kefir, shared with friends, eaten, blended into smoothies, or dried and stored in a sealed container in the fridge as backup.\nCan I keep my water kefir grains in a bag instead of letting them float free?\nThe water kefir grains may be contained in a muslin bag. Be sure the bag is submerged in the sugar water. If it floats it can mold.\nShould I add things like ginger juice, eggshell, and baking soda to my water kefir grains to keep them healthy?\nIf you are using a quality water and sugar source, additives should not be necessary.']"	['<urn:uuid:ba587455-5b28-408e-b7d2-18e4ed791458>', '<urn:uuid:ee815f27-6e22-4714-94ca-035a9c089fc2>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T23:00:04.658849	20	66	2702
58	what courses help career data analysis	Several foundational courses are essential for data science careers: differential calculus, Bayesian statistics, graph theory, and linear algebra. These courses form the basis for advanced areas like deep learning, computer vision, and sensor mining. Spending time mastering these fundamentals in college is crucial for preparing for a career in data science.	['For this ComputingEdge issue, we asked Naren Ramakrishnan—professor of engineering and director of the Discovery Analytics Center at Virginia Tech University—about big-data career opportunities. Ramakrishnan’s research interests include mining scientific datasets in domains such as systems biology, neuroscience, sustainability, and intelligence analysis. He was a co-guest editor for Computer’s April 2016 special issue on big data.\nComputingEdge: What careers in big data will see the most growth in the next several years?\nRamakrishnan: With this space maturing, more than seven of 10 organizations in the US are expected to have an in-house data science team by the end of this year. Demand for data scientists will grow in technical areas like deep learning, as well as in fields such as healthcare, the Internet of Things economy, finance, manufacturing, educational innovation, sustainability, and forecasting. You can keep track of what’s going on in data science forums such as KDnuggets (www.kdnuggets.com).\nComputingEdge: What would you tell college students to give them an advantage over the competition?\nRamakrishnan: Remember those courses you thought were boring and had nothing to do with real-life, like differential calculus, Bayesian statistics, graph theory, and linear algebra? They are the foundations of data science today! So spend time honing your fundamentals in college. It will prepare you for advanced courses and careers in areas such as deep learning, computer vision, and sensor mining. It’s also important to develop a portfolio of your data-analytics and visualization code, perhaps hosted on a GitHub page. Many prospective employers want to see examples of your big-data and data-analytics skills.\nComputingEdge: What should applicants keep in mind when applying for big data jobs?\nRamakrishnan: Just as data-science applications are varied, so are the job titles, responsibilities, and expectations. Find out how data science fits into a potential employer’s organizational structure. Do they have a CDO (chief data officer) or CIO (chief information officer)? Does data science play a supporting role or is it an integral part of the way they do business? How many business units within the organization rely on data science? These questions are important to understand how you will fit within the organization and how the organization will fit within your career objectives.\nComputingEdge: How can new hires make the strongest impression in a new position?\nRamakrishnan: There are significant open source software and frameworks for data analytics and visualization. Get familiar with these tools before joining your company. Once you start, you should be able to leverage this background to rapidly analyze data, perform exploratory or predictive analysis, and develop visual dashboards for demonstration to managers. Nothing conveys a stronger impression than a person who is able to complete the loop from data to insights to decisions. This will position you for interesting assignments and professional growth.\nComputingEdge: Name one critical mistake for young graduates to avoid when starting their careers?\nRamakrishnan: One common error is not taking the time to understand how data permeates your organization—for example, how data is produced and collected, who makes decisions based on data analytics, and what types of decisions these are. Understanding a bit about the larger picture will help you be a more effective data scientist. Get out of your comfort zone and speak to nontechnical professionals to understand the domain. Then when you speak as a data scientist, people will take your conclusions more seriously.\nComputingEdge: Do you have any learning experiences you could share that could benefit those just starting out in their careers?\nRamakrishnan: Data science is so pervasive today that it provides insights into situations and events in entirely unexpected ways. My favorite example has to do with the OpenTable website (www.opentable.com), which lets users make restaurant reservations in many cities worldwide. The site lets analysts download aggregate reservation data for specific cities on a daily—and even hourly—basis. In analyzing this data, my collaborators and I found that spikes in cancellations can correspond to health events, such as the flu season’s early onset or a food-contamination episode. The temporal profile of restaurant availability can help forecast flu seasonality. The lesson to take away from this is that data will show up in surprising places and can provide valuable insights if we just know where to look and how to relate disparate datasets. One should consciously think about data they encounter in real life and study its implications.\nAbout Lori Cameron\nComputingEdge’s Lori Cameron interviewed Ramakrishnan for this article. Contact her at email@example.com if you would like to contribute to a future ComputingEdge article on computing careers. Contact Ramakrishnan at firstname.lastname@example.org.']	['<urn:uuid:6c5aba3b-0a31-4b2b-b2cc-c1d58fef72ae>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T23:00:04.658849	6	51	753
59	looking for machine part stop emergency stop options	There are two emergency stop options for machines: 1) A latching valve with push/pull-button actuator that requires a secondary motion to reset, and 2) A relief valve that can automatically open at a specific pressure point to prevent component failure due to excess pressure.	['A relief valve that decelerates an actuator\nFigures 18-8 through 18-14 show normally closed, solenoid-operated relief valve B used to rapidly extend, then decelerate a free-falling cylinder. Deceleration takes place when the cylinder makes a limit switch that deenergizes the solenoid on relief valve B . Relief pressure should be set 150 to 200 psi higher than the pressure required to raise the cylinder. Any higher relief pressure shortens the deceleration stroke and increases shock.\nFigure 18-8 shows a cylinder with its rod port piped to tank through normally closed solenoid-operated relief valve B . Prefill valve F allows the cap end of the cylinder to fill during rapid advance. (See Chapter 7 for an explanation of the prefill valve’s function.) Check valve C at the rod port keeps cylinder flow from going to tank through directional valve A.\nTo extend the cylinder, energize solenoid A1 on directional valve A to pass oil to the cylinder’s cap end, as in Figure 18-9. Also energize solenoid C1 on relief valve B , venting it to tank and allowing the cylinder to fall freely. As the cylinder falls, the cap end fills from the pump and from tank directly through prefill valve F.\nAs the cylinder extends, high flow leaving the cylinder’s rod end goes to tank. Just before the rod contacts the work, a limit switch deenergizes solenoid C1 on relief valve B , Figure 18-10. As valve B tries to close, pressure increases in the cylinder’s rod end, keeping the valve partially open. Backpressure from relief valve B quickly and smoothly slows cylinder descent. The cylinder continues to slow while the relief valve shuts. The cylinder does not completely stop because the pump forces it to extend after free fall.\nAfter deceleration, relief valve B acts as a counterbalance valve, as in Figure 18-11, so the load cannot run away. The cylinder extends at pressing speed to the work. This part of the stroke should be as short as possible to save time. Prefill valve F closes as the cylinder decelerates and allows pressure to build in the cap end. The slowdown is smooth and controlled — without shock or bouncing. This circuit decelerates the cylinder when commanded by an electrical signal at any point in its stroke.\nFigure 18-12 shows the circuit while the cylinder is pressing. When the cylinder contacts the work, energize solenoid C1 on relief valve B again. Energizing the solenoid on the relief valve lets oil from the cylinder’s rod end flow to tank at minimal pressure. This allows the weight of the platen and tooling to add to the pressing force because they are no longer counterbalanced. Pressure increases in the cylinder’s cap end to perform the work.\nDeenergizing solenoid A1 on directional valve A lets it center and decompress the cylinder, Figure 18-13 shows directional valve A centered, blocking the cylinder’s cap-end port and unloading the pump. At the same time, a signal to single-solenoid valve E in the cap-end line shifts it open. Trapped pressurized oil in the cylinder’s cap end flows to tank through an orifice, thus lowering pressure without shock. Pressure switch D indicates when pressure is low enough to shift valve A to retract the cylinder. (See Chapter 7 for an explanation of a decompression circuit. A decompression circuit keeps the cylinder from rapidly losing pressure and shocking the system.)\nTo retract the cylinder, energize solenoid B1 on directional valve A to send oil to the cylinder’s rod end, as in Figure 18-14. Oil from the pump starts to retract the cylinder. Pilot oil opens prefill valve F to tank. Oil from the cylinder’s cap end flows to tank through the prefill valve and the main directional control valve. The cylinder retracts rapidly at low pressure.', 'Valves are used in pneumatic systems to control airflow by blocking or directing it to meet the needs of the system. A pneumatic valve can have many different configurations. Knowing how to specify which valve is necessary in a schematic is an important skill for any engineer working with pneumatic systems.\nSymbol standards for Pneumatic valves are found in the standard ISO1219-1:2012 Fluid power systems and components – Graphical symbols and circuit diagrams. The ISO Standard replaces ANSI Y32.10 Fluid Power Graphic Symbols.\nCommon Types of Pneumatic Valves\nMuch like specific electrical components are used to control and protect the circuit, pneumatic systems utilize a variety of mechanical components in order to control and protect the pneumatic system. There are many different types of valves used to control air flow by preventing flow, directing flow, controlling velocity, or relieving excess pressure. A few of the most common types are described below:\nCheck valves allow free flow of air in one direction but block flow in the opposite direction. They are passive because they require no external input to function. In a sense, they function like a diode in an electric circuit.\nSpring-Assisted Check Valve\nBasic Check Valve\nA pressure relief valve is used to relieve excess pressure in a pneumatic system. Their purpose is to open automatically at a specific pressure and to continue to remain open until the volume pressure drops below the set point. These valves are used as a fail-safe measure to prevent pneumatic component failure resulting from excess pressure due to a control valve malfunction, temperature increase, etc.\nA shuttle valve allows fluid to flow through it from two different sources, one at a time. It functions as a logical OR gate. When the flow is applied to one input the other input is closed.\nQuick Exhaust Valve\nA quick exhaust valve is similar in form to a shuttle valve but instead of two inputs the quick exhaust valve has one (1) input, one (1) outlet, and one (1) exhaust. When pressure is applied to the input, the air flows to the outlet. When the pressure is removed, air flows from the outlet to the exhaust.\nFlow Control Valve\nFlow control valves are used to control the velocity of air through a pneumatic system. These are often used to control the speed of a pneumatic cylinder to meet a specific application. Flow control valves can be either Fixed Orifice or Adjustable depending on the application. The adjustable version uses a needle valve to control the flow of air.\nFixed Orifice Flow Control Valve\nAdjustable Flow Control Valve\nAdjustable Flow Control Valve\nDirectional Control Valve\nSymbols representing Directional Control Valves contain information about the valve that they represent. They show the number of positions, the methods of actuation, the number of ports, and the paths that the air can take.\nDirectional Control Valves\nThe remainder of this post will concern directional air control valves. These are available in many configurations. The function of a valve is given by two numbers (e.g. 3/2). The first number indicates the number of ports, NOT including pilot feeds or signal ports. The second number indicates the number of valve positions (e.g. on, off, etc.). Because directional control valve symbols communicate more information than the previously mentioned valves, they are necessarily more complex.\nSymbols representing Directional Control Valves contain information about the valve that they represent. They show the number of positions, the methods of actuation, the number of ports and the paths that the air can take.\nThe number of points entering or exiting the flow box represents the number of physical ports on the valve. The number of flow boxes in a valve symbol represents the number of positions of the valve, and each box MUST have the same number of ports. A directional control valve is designated as shown in the example below:\nNote: directional control valves are often designated by the number of “ways” in the valve. This term is often misapplied to the last number in the valve designation (i.e. 3/2 way valve). This use is INCORRECT. Ways refer to the number of paths the fluid can take through the valve. For example, a 3-port valve has 2 ways or 2 paths that the fluid can follow (i.e. from the source to the output and from the output to the exhaust). Because the number of “ways” and the number of “valve positions” happens to be the same, the term gets confused. Regardless, it is often used incorrectly by manufacturers and distributors, so it is wise to be aware of what the terms may indicate, right or wrong. THE SAFE APPROACH is to determine the number of required ports and the number of required positions then, using the symbols associated with specific valves (manufacturer specification sheets), find the valve that performs to your specification.\nBelow are some examples of 2, 3, 4 and 5 port flow boxes:\nFlow Lines (Key)\nPorts are labeled on the “Normal” position flow box. The remaining flow box ports match the corresponding port location on the “Normal” flow box. Port labels can be letters as well as numbers. The following is a general list of what each letter means if used in a valve symbol:\n- (P) – Pressure Inlet Port\n- (A) – Outlet Port\n- (B) – Outlet Port\n- (R) – Exhaust Port\n- (S) – Exhaust Port\nUsing Directional Control Valves in a Schematic\nValve symbols in a schematic are drawn in their normal or default position. The default position can be either Normally Open (NO), or normally closed (NC). Normally open indicates that the default state of the valve will allow fluid flow. Normally closed means the opposite or that fluid flow is hindered in this state:\nNormal State (3/2, N.C. valve)\nActuated State (3/2, N.C. valve)\nFlow lines in the schematic must be connected to the ports in the normal position flow box, not the actuated position box. The valve symbol can be visualized as moving from one flow box to another when moving from one state to another. The port connections remain the same from one flow box to another and the flow lines within the flow box change to indicate the actuated flow of the valve.\nActuators are used to switch a valve from one position to another. There are many types of actuators, some of which can be combined on a single valve. Each actuator has its own symbol which attempts to illustrate its function. Actuator symbols are attached to the valve flow box that would become active if the actuator were triggered.\nNote: The actuators shown below do not represent a comprehensive list, nor does every actuator symbol shown match exactly the symbols used by manufacturers or schematic designers.\nThe manual symbol means that switching states or positions in the valve is a manual process. This might include a button, a switch, or a lever activated by direct human intervention.\nA pushbutton actuator is a specialized manual actuator. It is generally combined with a spring return so that the valve is default in one position and momentarily actuated (changes position) when the button is pressed.\nA special case of manual valves is a “latching” valve (generally with a push/pull-button actuator). These are often used as emergency stop switches and require a secondary motion to actuate or “reset” the valve.\nA lever actuator is a specialized manual actuator. A lever can have two positions (i.e. on/off) or have multiple selectable positions when paired with a detent feature.\nA pedal actuator is a specialized manual actuator intended to be activated with the operator’s foot.\nA mechanical actuator utilizes some form of mechanical actuation to operate the valve. The actuator might be a roller, cam, lever, piston, etc. that is triggered without direct human intervention. These are generally used as mechanical switches on pneumatic systems such as factory automation and conveyor systems. An example of a mechanically actuated valve is shown below:\nElectrically Controlled Valves\nAn electrically controlled valve is NOT activated by direct human input. It is tied to an external control system which activates the valves according to the requirements of the associated system and/or program.\nA solenoid actuator is a small electrical coil which uses an electromagnet to change the valve position.\nAn internal pilot actuator uses an internal pilot valve (like a diaphragm) to change or maintain the position of a larger valve. The larger valve remains actuated as long as the main air pressure is present. When the pressure is lost, the internal pilot changes the position of the main valve. This type of pilot actuator can serve as a passive means of actuating an in-line valve, generally as a failsafe.\nAn external pilot actuator also uses a smaller pilot valve to change the position of a larger valve. The larger valve changes position when a signal pressure or flow is reached. In this case, the pilot actuator is external to the system and can be physically located on or away from the larger valve.\nThe piloted solenoid is a common combination of actuators. In a piloted solenoid an electrical signal opens and closes the pilot valve which operates the larger valve using the pilot medium as the mechanism for forcing the actuation. This is commonly used to actuate large valves where a solenoid would have insufficient power to actuate the valve.\nThe following symbols are commonly used in combination with primary actuation symbols to indicate a more specialized function:\nA detent feature provides clear physical delineation between valve positions. It is usually paired with a lever actuator.\nA spring provides a restoring force to the valve. Generally, the spring side is the normal side, functioning to return the valve to “normal” in in a power-loss situation.\nValve Symbol Examples\nFollowing are a few examples of directional valve symbols. Notice the variation in how portions of the symbols are drawn:\n2/2 Valve with Lever Actuation\n3/2 Valve with Push-Button Actuation\n3/2 Valve with Pilot Actuation\n5/3 Valve with Pilot Actuation and Closed Center\nRecently Apollo designed a simple pneumatic system for a large prop on a game show. The system was designed to use large bore pneumatic cylinders that, when activated, engages the prop driving surface. The output force is proportional to the input pressure to the cylinders. The pneumatic schematic is shown below:\nIn the schematic above, there are 4 directional flow control valve symbols. Their purposes are as follows:\n- A 3/2 manual lever valve which isolates the system from the compressor when in its normal state. When activated it allows air to flow from the compressor to the cylinders. This valve also exhausts when in its normal state. Because the exhaust feature on this valve isn’t used in this system a 2/2 valve could have been selected. The reason this valve was chosen rather than a simpler 2/2 valve is because of its Lock Out Tag Out (LOTO) feature.\n- A 4/2 manual lever valve that, when valve 1 is activated, will supply air to the cylinders. When this valve is in its normal state, it will supply air to the extend side of the cylinders and allow air from the retract side to exhaust. When this valve is actuated, the path for the air is switched, the cylinders are retracted, and the extend lines are exhausted.\n- Valve 3 is a simple 2/2 lever valve that is normally closed. Its purpose is to allow a pressure transducer to be placed near the cylinder to monitor any pressure spikes that the system might see. When the transducer is in place, the valve is opened.\n- Valve 4 has the same purpose as valve 3 but is near the second cylinder.\nThis system also features flow control valves, pressure relief valves, and a spring assisted check valves. The function of each of these valve types is discussed earlier in this post.']	['<urn:uuid:ad546351-9b0f-4e3c-8b21-1f2704a158e7>', '<urn:uuid:b1d84c4e-365f-4538-a39d-e610e6541d91>']	factoid	with-premise	short-search-query	distant-from-document	three-doc	novice	2025-05-12T23:00:04.658849	8	44	2609
60	How do the roles and opportunities for women in ancient Han China compare to those in traditional Japanese theatre, particularly in terms of their ability to perform and participate in public life?	In Han China, despite ideological restrictions, women had significant public roles - they participated in agriculture, textile production, paid taxes equally to men, and were even drafted to build the Great Wall. They also wielded political influence, with some powerful women ruling behind young emperors. In contrast, traditional Japanese theatre strictly excluded women from performing. After women were banned from Kabuki theatre in 1659, only adult men were allowed to perform, with male actors (onnagata) specializing in female roles. The only exception came much later with Shimpa theatre in the early 1900s, which finally allowed women to play women's parts, but only in that specific theatrical form.	"['The Subordination of Women: Ideology versus reality\nProfessor Yi Shaoyin, Shanghai Normal University:\n""Han gender ideology can be summed up in three points. One is how women related to men. In strict description it was that the male is superior and the female is inferior. Second was the social space for women. They were supposed to remain in the interior of the household… while the exterior of the public space belonged to men. Women were excluded from public life.\nThirdly, how should women behave? There is the famous doctrine of the Three Obediences. As a young girl and a young married woman at home, women should be obedient to the father. Once married the wife should be obedient to the husband. And what if your husband dies? You should be obedient to your son”\n“But in reality, particularly in the Former Han Dynasty, this is not at all the case. In fact for the entire Han dynasty, it was not the case. First, in politics women exerted great influence; two or three most powerful women in the court ruled for years behind young or even child emperors. Further down in a society you see women did not remain in the household, but participated in agriculture, in economy particularly the production of textiles. Because all these contributions to the society, women were equal, or almost nearly equal, participants of economic life. Women actually paid tax, when the tax was collected based on the population, women paid equally.\nWomen were also drafted into building the Great Wall. For projects like this you know much later on you only see drafting of males, but in the Han women were drafted too. Moreover, widowed mothers actually had great influence in household affairs. Inheritance laws gave them certain rights to their husband’s wealth and property. In that era, it didn’t all go to the sons.\nSo you see the ideology and the reality of the Han Dynasty did not match. The subordination of women was meant to be descriptive of the social reality – and that is as later people tend to read them - but in fact it was prescriptive and meant to change reality.”\n- - -\nJakucho Setouchi, Novelist:\n""Until the Meiji period, women were not supposed to study. Men could study, but there was no need for women to study. The only purpose for women was to get married. That was a way of thinking in Japan. When Raicho Hiratsuka started the Seito movement, it was a wake-up call. It was the beginning of Women\'s Liberation in Japan. And now, 100 years later…\nThis is the age of women. There are not many female politicians yet. Maybe the world still sees Japan as a male-dominated society. But women have become stronger compared to the past. The number of working women has increased. Japanese men are falling down. There are young men called grass eaters. They are like plants.They have no sexual desire. So this is the age of women in Japan.""', 'early from of theatre; used techniques such as song, dance, and characterization; rooted in religion\nWhat two traits distinguish theatre from ritual?\n1. an actor portraying a character\n2. a story with conflict\ngrew out of Ancient Greece when Thespis stepped put of the chorus around 500 BCE; Greeks-Romans-medieval England\nall other forms of drama from ritual theatre in Africa to shadow puppets in the Muslim lands; does not attempt to recreate objective reality\nPrecolonial African Theatre\ngrew out of ritual; incorporated acting, music, storytelling, poetry, dance, costumes and masks; did not separate the audience and performers\nAfrican storytellers who used theatrical elements like song, mime, and impersonation in their folk tales\ntraditional African theatre mixed with Western Theatre; is associated with very political messages\nAfrican playwright; wrote Death and the King\'s Horseman; first African to win the Nobel Prize for Literature in 1986\nan encyclopedic book of dramatic theory and practice written sometime b/w 200 BCE and 200 CE; 37 chapters on every aspect of classical Indian drama, including costume design, theatre layout, playwriting, directing, acting, music, and philosophy\nnamed for the ancient Indian language is it performed in; based on Indian myths; always end happy; costumes and make-up are elaborate; take up to six hours to perform; not realistic; use of hand gestures\nmeans ""story play""; dramatized versions of Hindu epic poems ""Ramayana"" and ""Mahabharata""; highly stylized dance and gestures; elaborate costumes and make-up\nsynthesis of music, dance, acting, and acrobatics; piercing style of singing; precise stylized gestures; played types; born into it\ndifferent colors meant different personality traits; almost all plays had a clown with white make-up\nTwo types of Peking Opera\nCivil Plays (plots about imperial concubines, chivalry, romance)\nMilitary Plays ( often set in the Three Kingdoms period (220-265 CE) when China was divided into three rival kingdoms\nCommunists vs. Peking Opera\n1949 Communists began wiping out Peking Opera saying it preserved superstitions, perpetuated the backwards laws of the feudal society, and encouraged the class system\nHistory of Noh Theatre\ndeveloped from the dance-prayers of Buddhist priests; 1374 a shogun named Yoshimitsu invited the performers to his court where he added poetry, acting; singing; and dance all together to create No theatre\ndescribe Noh theatre\nactors (only men) move slowly and gracefully and chant poetic texts that follow the seven-five rhythm used for haiku poems; highly expressive masks and costumes\nDescribe the Noh stage\nbare stage except for a pine tree on the back wall of the stage that symbolizes eternal life; on the left is the Hashigakari, a narrow bridge that the principal actors use to make their entrances\nWhere did the meaning of the pine tree come from?\npossible a Shinto ritual- according to myth, deities descended to earth via the pine tree\nFive possible subjects of Noh play\ndeities, deeds of heroic samurais, women (played by men), insanity, famous legends\nchance meeting between two characters; introductions are made and question-answer sequence reveals the protagonist\'s concern\ndescribe an evening of Noh drama\nseveral plays with small farces between them (like the Greek satyr plays although any actual relationship between the two is debated)\nHow has Noh theatre changed over the year?\nNoh theatre has reached its present form in the 1600s and have remained practically unchanged ever since; still popular today; performed in the language of the 14th century shoguns-general audiences must bring translation to understand the text\nHistory of Kabuki\nOkuni owned a brothel and began having her girls perform Noh theatre to draw in customers. After the show men lined up to get some from the girls, and ended up fighting over them. The shogun banned to girls from performing so little boys took their place, but the men fought over them too! So in 1659 it was ruled that only adult men could perform Kabuki theatre\nName one modern day thing Kabuki has influenced in America\nPower Rangers perform ""mie"" poses as they get in their animal fighting things. Look it up on Google!\nWomen are allowed to participate in which types of theatre?\nAfrica (some countries; it varies)\nNOT IN JAPANESE THEATRE AT ALL!*\n*exception see ""Shimpa Plays""\nmen who play female roles in Japanese theatre; not considered in the peak of acting career until he has been playing women\'s roles for at least 20 years; feel they know women better than they know themselves\nDifference in Kabuki and Noh Theatre\nKabuki has a spectacular scenery with special effects, trap doors, and an exaggerated pine tree on the back wall; audience members are expected to participate; there is no curtain call; some melodramatic scenes; more characters\nSimilarties in Kabuki and Noh\nhighly stylized movements; colorfully embroidered kimonos; the pine tree on the back wall; men play women; dialogue is sung/chanted/shouted in a very stylized manner\nJapanese puppet theatre; life like puppets about 1/3 size of real life; multiple people to control all moving parts like eyebrows, joints, mouths and eyes\nmie pose (look up on Google)\nperformed at particularly intense or profound moments in the play; sudden, striking pose accompanied by several powerful beats of wooden clappers known as the ""Ki""; posture includes crossed eyes, sharp turned chin, big toe pointed to the sky\nThree Kinds of Kabuki Plays\nhistory plays-about major political events of the past\ndomestic plays-about the loves and lives of merchants and townspeople\ndance-dramas-about the world of spirits and animals\nhistory play; first staged in 1840; tells story of a famous Japanese warrior Yoshitsune (1159-1189 CE) who was forced to flee because of unjust accusations of disloyalty by his half-bother, the shogun; one of most popular Kabuki plays in repertoire\nBanraku Puppet Masters Hierarchy\nSenior Puppet Master-shows face; controls right hand and head\n2nd Highest Puppet Master- dresses all black including hood; controls the left hand\nApprentice-dresses all black including hood; controls the feet\n*takes at least ten years to become senior puppet master\none of most popular Kabuki and Bunraku playwrights; combined poetry and prose in dramatic tales of comedy and tragedy earning him the name ""Japanese Shakespeare""; wrote over 100 plays; based on scandalous gossip and actual events\nShimpa or ""New School of Movement""\nbeginning of 1900s; modified Kabuki theatre; toned down traditional Kabuki style and added some Western realism; told stories of everyday people particularly women; women could play women\'s parts\nWhy was Shimpa theatre important in Japan?\nWomen could play women\'s parts, but ONLY in Shimpa plays.\nJapanese Theatre Today\nKabuki is most popular traditional theatre.\nWestern theatre is also popular.\nModern drama focuses on social issues, suppression of individualism, and technology and humanity.\nprobably originated in China around 100BCE; candles light a screen from behind puppets make shadows on the screen; narrator tells story\nTa\'ziyeh Religious Dramas or Iran\npreformed outdoors or occasionally indoors on special stages; uses real horses and camels; compare to the religious dramas in England in the middle ages\nWhat do stories do the Ta\'ziyeh plays tell?\nMost tell of Imam Hussein, grandson of the Prophet of Islam, who died in battle in 680 CE outside of Baghdad; later plays tell of Moses and the Pharaoh and Solomon and the Queen of Sheba\nWho wrote the first European-style play in Arabic and what play was it based on?\nMarun al-Naqqash in 1847; based on Moliere\'s ""The Miser"" (1668)\n1915 one of the first women to appear on stage in Egypt; became first female artistic director in the Muslim world when she formed her own theatre company\n(b. 1944); Algerian-born French playwright; one of the most well-known Muslim playwrights; plays discuss violence against women, female sexual desire, and religious fanaticism\nEnglish director (b. 1925); staged ""The Mahabharata"" in 1985; Western retelling of the epic battle between two sets of cousins in an ancient Indian Dynasty; cycle of three plays took 12 hours to stage and featured actors from all over the world']"	['<urn:uuid:b811157a-ccb1-4c6a-96ff-c2a017d8b4af>', '<urn:uuid:339db09f-5682-4f2d-b7f9-9c18f088e62a>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T23:00:04.658849	32	107	1805
61	gas turbine corrosion prevention strategies efficiency degradation causes remedies	Gas turbine efficiency degradation occurs through clogged inlet filters, dirty compressor blades, damaged blading, and excessive clearances between casings. Prevention involves proper inlet filter maintenance, routine compressor washing, and careful monitoring. For corrosion prevention specifically, the main strategy is avoiding the establishment of electrical circuits between metals, as corrosion occurs through electrochemical reactions requiring moisture as an electrolyte. The presence of salt accelerates corrosion by increasing current flow between positively charged (anode) and negatively charged (cathode) areas.	['Several options to improve the operating characteristics of gas turbines do exist and, given that those few extra percentage points of operating efficiency are essential to maximizing the value of an installation, they should be taken up. Michael Gabriel reports.\nIn the present economic environment, owners and operators are seeking cost effective ways to expand gas turbine operability, improve efficiency, gain more output and extend the life of their existing equipment. The regulatory process for permitting new generation sources is slow and more demanding than ever before, making minor turbine improvements to existing equipment a more attractive option.\nMany things can be done to keep a turbine operating at peak efficiency. Attention to detail when observing and trending operating parameters can identify degrading turbine performance. Various types of monitoring packages exist to assist in this endeavour. Some are designed to warn of impending failures (i.e. bearing vibration), thereby averting costly forced outages. Other products are aimed at analyzing turbine or plant performance over time. Remote monitoring centers can do either or both.\n|Figure 1: Typical plant operational envelope|\nAn additional challenge to plant owners and operators are the variations in fuel composition needed over time. Combustion systems are designed and built to operate in a relatively narrow range of fuel quality. Significant fuel composition variation can cause considerable operational issues with regards to emissions, flame stability and combustor dynamics. These changes are most challenging to lean, premixed, ‘low NOx’ combustion systems.\n|Figure 2: ECOMAX plant operational envelope|\nTURBINE EFFICIENCY ” DEGRADATION AND RECOVERY\nThere are many ways a gas turbine experiences performance degradation over time. Some can be corrected or mitigated in an operational manner, while others require significant maintenance during a shutdown. Examples of common causes of turbine efficiency loss are clogged inlet filters, dirty compressor blades, damaged compressor and turbine blading, excessive clearances between casings and moving compressor or turbine blades, and sub-optimal combustor tuning.\nInlet filter cleaning and maintenance\nProperly maintaining the inlet filtration system is vital to maintaining overall turbine health. Poorly maintained inlets can result in damage to compressor blades, dirty, fouled compressor blades, and excessive pressure drop across the filters (with the resultant loss in turbine efficiency).\nPeriodic inspections of the clean side of the filters should be performed ” searching for ‘light leaks’ as well as material that is or could become loose and enter the compressor.\nSome turbines have a reverse air ‘puff’ cleaning system for the filters, which utilize the compressor discharge air (after cooling it) to reverse flow filters for a brief instant, with the intention of dislodging the contaminants entrapped on the dirty side of the filter. One of the largest drawbacks to this system is that the vast majority of the contaminants can be drawn back into the filters after the short puff, requiring many repeats of the process to significantly reduce inlet filter differential pressure. A simple improvement is to utilize the plant air system to puff the filters when the turbine is shut down. This method significantly reduces the time required and amount of air consumed in the cleaning process.\nRoutine compressor washing is essential to maintaining turbine output and efficiency. Significant performance degradation can be experienced due to dirty compressor blades, especially if the gas turbine is in, or near, an industrial environment.\nCompressor washes can take place with the unit in operation (online) as well as shut down (offline). Some turbines, especially those with low NOx combustors, do not allow detergent use during turbine operation. A typical regime for these turbines is to wash online once a day for a few minutes a day. When the unit can be taken offline, a more thorough wash is conducted, utilizing detergent, with wash and multiple rinse cycles.\nAs an example, Gas Turbine Efficiency model 600i compressor wash systems were installed on two 9E gas turbines in Malaysia. Implementing a proper routine of online and offline washes, the plant realized a heat rate recovery of up to 1.5%. This resulted in an economic payback period of two months. Additionally, the gas turbine output was increased by 8% to 13% during the online wash cycles.\nTypical output recovery achieved after conducting an offline wash can exceed 2% when compared to pre-wash turbine operation.\nOther benefits include redution in emissions. Results will vary based on turbine type, usage, and time since last washing.\nInlet conditioning ” reducing the inlet air temperature is a very common method for improving gas turbine output. There are three well known methods for achieving this output improvement: fogging, evaporative cooling, and chilling. These are summarized in Table 1.\nFogging involves injecting atomized water into the inlet air stream, downstream of the inlet filters. The system carefully controls the amount of water injected to ensure no large droplets of water are ingested into the compressor.\nEvaporative cooling equipment is installed in the inlet ducting, after the filtration system. The system consists of a media bank in the air stream, which is continually wetted by one or more water pumps. As the incoming air passes through the media it absorbs some of the moisture and at the same time loses heat (via water evaporation), resulting in cooler, denser air. This dense air then enters the compressor. Cooler, denser air allows the turbine to produce more output power with improved turbine efficiency.\nChilling the inlet air stream is an effective way to increase the air density and thus improve turbine output. The chiller ” typically a large mechanical refrigeration unit ” is adjusted to provide a controlled inlet air temperature to the compressor. No matter what the ambient air temperatures are, the chiller ensures that the compressor inlet temperature will not exceed a specified target temperature.\nPeak firing a gas turbine (above the target OEM design limit) is another option for producing additional output, but the additional maintenance demands imposed on combustion and turbine hardware make this economically feasible on rare occasions.\nHowever, there can be some opportunity to take a turbine from its current operating state and allow it to operate at the OEM defined design limit. It is not unusual to find a turbine slightly under fired as compared to OEM design limits. Increasing the turbine output in this manner must be undertaken with great care so as to not exceed the OEM limits under any, and all, ambient conditions.\nIMPROVING EMISSIONS COMPLIANCE\nEmissions compliance is a continual focus of operating plants, with the requirements only becoming more stringent over time. An example is the EU large machine directive, which is driving emissions of NOx and CO to less than 20 ppm for all plants greater than 20 MW. A similar programme in the US is pushing for requiring upgrades of all non attainment zone sited plants to the best available retrofit technology.\nEmissions compliance is not only better for the environment, but it will help avoid costly fines. Typically, gas turbine combustion systems are ‘tuned’ twice a year, once prior to the summer and once prior to the winter. Each tuning period is designed to optimize the turbine for the upcoming season while ensuring, using a significant safety margin, that there should not be excessive emissions, excessive combustor dynamics or loss of flame (due to lean blow out).\nUnfortunately, these tuning services can be time consuming, preventing the turbine from operating to its full potential during the tuning period. In addition, seasonal tuning inherently leaves ‘operational performance margin’ on the table, as the combustion system must be tuned to ‘safely’ allow for acceptable performance under all projected ambient conditions over the next six months (until the next tuning event). Fortunately, automated tuning systems are available which continually optimize the combustion system.\nOne such system is Gas Turbine Efficiency’s ‘ECOMAX’, which utilizes real time emissions and combustor dynamics data to continually determine the optimal operating point for the combustion system. The plant operators can select, in real time, to optimize emissions, combustor dynamics, and/or turbine output. This system also allows the operators to safely take the turbine up to OEM design firing limits, as mentioned in the previous section. Additionally, ECOMAX can be configured to optimize the part load and base-load heat rates, in simple cycle or combined cycle configurations. The system is illustrated in Figures 1 and 2.\nOperational experience with ECOMAX has shown that it has optimized the combustion system on one unit almost 300 times in a single month. Additionally, the unit had almost a 2% gain in output due to operation at or near the OEM design limit.\nAdditional challenges arise from the current trend of having more frequent variations in fuel quality/composition. The results can be flame-out turbine trips (lean blow outs of the combustion system), emissions violations as well as combustion dynamics excursions. These negative events are more prevalent in lean, premixed, low NOx combustors. Continually tuning a combustion system can enable the turbine to successfully operate in a much broader window fuel composition variation. This can also be turned into an economic advantage for the plant, allowing consumption of lower BTU content fuel at a significant cost savings. Essentially, why pay for ‘high test’ if your turbine can run on ‘regular’ with no ill effects?\nFinally, automated tuning systems can be adjusted to reduce emissions on a continual basis, setting targets below the accepted, standard emissions levels normally achieved.\nDOING MORE WITH LESS\nOnce degradation is noted, the first step is to find the cause, with the logical second step being rectifying the issue. Some issues will be internal to the turbine casing, and require deep turbine knowledge coupled with operating data to pinpoint the likely cause. Other issues may be external to the casing of the turbine, and can be discerned by a combination of operating data analysis coupled with experienced eyes and ears examining the turbine auxiliary components and systems.\nOne common issue is the loss of compressor efficiency due to air leaking by closed shut anti-surge (bleed) valves or bypass valves. Some plants may have installed instrumentation which show a temperature rise in the associated piping. If your lines are not instrumented, a thermal gun is all that is needed to discern which may be leaking. The same process can be used for the inlet air heating valve.\nIn the era of shrinking staffs coupled with more regulations and administration, the trend is that fewer eyes and ears are roaming the plant with the intention of finding issues before they become significant. One solution is to have a plant audit conducted by people familiar with the technology, but not too close to the operation of that particular plant. A questioning attitude by the auditors is vital. The most successful audits are thoroughly embraced by management and operational personnel before, during and after the audit.\nFor those owners/operators with a large fleet of similar units, significant savings can be realized with regards to proper outage scheduling and asset management, especially in the area of capital spares over the life of the turbine. A robust programme which manages hardware life and predicts fallout allows owners to properly allocate funding to purchase capital spares without overspending potentially millions of dollars. These services are commercially available for fleet operators who find themselves wanting to implement this type of program without expanding their already overburdened staff.\nIn summary, many options exist for owners and operators to improve the various operating characteristics of their gas turbines. They range from inexpensive and simple to costly and complex, and are marketed by OEMs and third party vendors alike.\nMichael Gabriel is the manager, Monitoring Centre, with Gas Turbine Efficiency, Orlando, Florida, US.', 'Corrosion endangers the gas turbine and its support\nequipment by reducing the strength and changing the\nstructural characteristics of the materials used in their\nconstruction. All such materials are designed to carry\ncertain loads and withstand given stresses and\ntemperatures, as well as to provide an extra margin of\nstrength for safety. Corrosion can weaken the structure,\nthereby reducing or eliminating this safety factor.\nReplacement or repair operations are costly, time\nconsuming, and restrict the usage of the equipment.\nCorrosion in electronic and electrical components can\ncause serious malfunctions. These malfunctions reduce\nthe effectiveness and reliability of the engineering plant\nand can often completely destroy these components.\nA thorough comprehension of the dangers of\ncorrosion and the ability to recognize and cope with the\nvarious types of corrosion should be included in the\nobjectives of any maintenance training program. As a\nwork center supervisor, you may find that corrosion\nprevention and control frequently turn out to be an\nall-hands evolution. To some extent you can avoid this\nsituation through frequent inspections, effective use of\navailable manpower, and proper training of your\nThe problem of gas turbine engines and support\nequipment protection is threefold: (1) prevention of\ncorrosion of the metal parts; (2) control of deterioration\nof nonmetallic materials; and (3) elimination of physical\ndamage during replacement, repair, and maintenance.\nOf the three basic problems, corrosion of metals is the\nmost difficult to control.\nMetal corrosion is the deterioration of a metal.\nWhen the metal is combined with oxygen, it forms\nmetallic oxides. This combining is a chemical process\nthat is essentially the reverse of the process of smelting\nmetal from ore. Very few metals occur in nature in the\npure state. For the most part, they occur as metallic\noxides. The refining process involves the extraction of\nrelatively pure metal from its ore and the addition of\nother elements (both metallic and nonmetallic) to form\nAfter refining, regardless of whether or not they are\nalloyed, base metals possess a potential or tendency to\nreturn to their natural state. However, this potential is\nnot enough in itself to initiate and promote this\nThere must also exist a corrosive\nenvironment in which the significant element is oxygen.\nIt is the process of oxidation that causes metals to\nIt is a well-known fact that the tendency to corrode\nvaries widely between various metals. For example,\nmagnesium alloys are very difficult to protect and have\na very low corrosion resistance. Copper alloys have\nrelatively good corrosion resistance and are very easy\nCorrosion may take place over the entire surface of\na metal by having a chemical reaction with the\nsurrounding environment. Or corrosion may be\nelectrochemical in nature between two different\nmetallic materials or two points on the surface of the\nsame alloy that differ in chemical activity. The presence\nof some type of moisture is usually essential for\ncorrosion to exist.\nPrevention and control of corrosion begins with an\nunderstanding of the causes and nature of this\nphenomenon. As stated earlier, corrosion is caused by\nan electrochemical or a direct chemical reaction of a\nmetal with other elements. In the direct chemical attack,\nthe reaction is similar to that which occurs when acid is\napplied to bare metal. Corrosion in its most familiar\nform is a reaction between metal and water and is\nelectrochemical in nature.\nIn an electrochemical attack, metals of different\nelectrical potential are involved and they need not be in\ndirect contact. When one metal contains positively\ncharged ions and the other metal contains negatively\ncharged ions and an electrical conductor is bridged\nbetween them, current will flow as in the discharge of a\ndry-cell battery. In this type of reaction, the conductor\nbridge may be any foreign material such as water, dirt,\ngrease, or any debris that is capable of acting as an\nelectrolyte. The presence of salt in any of the foregoing\nmedia tends to accelerate the current flow and hence\nspeed the rate of corrosive attack.\nOnce the electrolyte has completed the circuit (fig.\n2-32), the electron flow is established within the metal\nin the direction of the negatively charged area (cathode).\nThe positively charged area (anode) is eventually\ndestroyed. All preventive measures taken with respect\nto corrosion prevention and control are designed\nprimarily to avoid the establishment of an electrical\ncircuit. Or secondly, to remove electron flow as soon as\npossible after its establishment before serious damage']	['<urn:uuid:57bc500b-b28d-4c08-98ee-a898d60f8dbc>', '<urn:uuid:fc76a00f-a3f7-41b8-b015-9d6200e56e4f>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T23:00:04.658849	9	77	2660
62	What are the key differences between temperature control in long fermentation bread making and kettle souring of beer, and how does temperature affect the microbial activity in each process?	In long fermentation bread making, cold temperatures of about 5-6°C (41-43°F) are used to slow down yeast and bacterial growth, while promoting the production of fruity ester compounds and favoring heterofermentative bacteria. In contrast, kettle souring of beer requires warmer temperatures - 80-100°F for L. plantarum or 110-120°F for other lactobacillus cultures - to optimize bacterial growth and acid production. These higher temperatures in beer making favor rapid souring while inhibiting spoilage bacteria, whereas the cold temperatures in bread making enable a controlled 48-hour fermentation process that develops complex flavors.	['We all know the benefits of a long fermentation in our breads: more flavor, more aroma, more health. However, one has to take extra care to avoid over-fermentation and too much sourness, especially when using a sourdough starter and fermentations of up to 48 hours. Fermentation at cold temperatures will aid us to get wonderful breads with a juicy crumb and a flavorful crust.\nI started making long fermentation breads when Sébastien Boudet’s ‘Durum Pavé’ bread was proposed as the monthly challenge in the Spanish Bread Forum. This 48-hours fermentation bread became a favorite not only because of its flavor, but also for convenience: splitting the process into several days makes it easier to fit into a busy agenda.\nThe pros and cons of cold fermentation\nYeast and bacteria growth slows down or even stops at low temperature. At 5ºC (41ºF), yeasts will make higher amounts of some ester compounds that have a fruity and pleasant aroma, such as ethyl acetate, ethyl hexanoate and ethyl octanoate. Moreover, cold will favor heterofermentative over homofermentative bacteria, and hence the production of acetic acid, which acts as an aroma enhancer. In addition, heterofermentative bacteria also produce alcohols with high and pleasant odor activity, such as 2-methyl-1-pentanol, 3-hexen-1-ol or 1-octanol. Finally, a long fermentation with a sourdough starter will increase the concentration of free amino acids which are important flavor agents in the crust, such as ornithine, methionine, phenylalanine, leucine, isoleucine and valine. I still have to read more in especialized scientific journals, but it seems that long fermentations make gluten more digestible and lower the bread glycemic index.\nThe main drawback of long fermentations is the difficulty to build a starter powerful enough to proof, but still ‘young’ to avoid too much sourness in the bread. Another potential problem is our fridge’s temperature. When doing 48-hours fermentation breads, I found optimal temperatures of about 5-6ºC (41-43ºF).\nBoth bulk and final proof will be carried out mostly within the fridge (24 hours each). For the dough to withstand such a long time, we will take into account the following tips to minimize gluten degradation:\n1) We will use a sourdough starter to provide acidity. Mild acidity strengthens the gluten network.\n2) We will knead well to achieve a strong gluten network.\n3) Use a stiff starter rather than a liquid one. Stiff preferments increase dough’s tenacity.\n4) Moderate hydration to keep enzymatic activity low.\nThe overall flour composition will be 10% whole rye flour, 30% durum flour (i.e. rimacinata), 60% white wheat bread flour. Total amount of flour is 600 gr.\n1) Refresh your sourdough starter and allow it to peak.\n2) Refresh again using 50 gr whole rye flour + 50 gr rimacinata flour + 50 gr de water + 50 gr starter. Leave 8-10 hours a 22-24ºC.\n3) Build the preferment by mixing 50 gr of whole rye flour, 50 gr of rimacinata flour, 60 gr of water y 50 gr of the refreshment obtained in step 2). Allow it to rise until it doubles in size (use the tip of the dome as a reference, see picture). It will take around 7-8 hours at ~ 22ºC (72ºF). You can use a larger amount of preferment and less time, if you prefer.\nMix together the preferment, 120 gr of rimacinata flour, 360 gr of bread flour and 360 gr of water (70% hydration). Autolyse for 30 min. Add 12 gr of salt and knead well.\nLeave at room temperature for 1 hour, stretching and folding twice during this time. Transfer to an oiled plastic box with fitting lid. In summer or when the room temperature is high, we will place directly the container in the coldest zone of our fridge. In winter, however, the dough will cool down too fast and fermentation will be impaired. To avoid this, we will place the plastic box containing the dough inside another plastic container (like a matryoshka doll). In this way, the air between both containers will act as an insulator, and we will get a milder cooling slope.\nLeave for ~23 hours in the fridge.\nTake out the dough from the fridge, preshape it and allow it to rest for 30 minutes. Shape and place the dough into a banetton. Depending on the room temperature, leave it for 30 min in the bench (summer) or 60 minutes in a warm place (winter) before placing it again into the fridge for 23 hours more.\nScoring and baking\nThis kind of bread is prone to more decorative and complicated scoring patterns. Sébastien Boudet’s can be seen in this clip and in this nice scheme by Amadeu Sanz. Lately I have found myself inspired by like Hannah Page’s patterns, see for instance these pics.\nBake a bit longer tan usual to make a darker crust and take advantage of all the aroma and flavor compounds produced in such a long fermentation.\nAlternative approach using a stiff starter made with high-gluten flour\nIt will be easier to monitor both activity and acidity in this way. I cannot tell whether these breads have different taste and aroma than their rye/durum starter counterparts, since I have not tested them in parallel. Perhaps they are subtle differences, I don’t know, but the result is wonderful enough to give it a try.\n1) Early in the day, refresh your sourdough starter using high-gluten flour (I use Italian ‘Manitoba flour’, W=400) using a 1:1:0.5 ratio (i.e. 50 gr stiff starter, 50 gr flour, 25 gr water). Allow to triple in volume, it will take 3-4 hours at 25-26ºC (77-79ºF). Refresh again using the same ratio. An active starter will double in volume in 2 hours, and triple in 3-3.5 hours. If your starter is fed regularly, and therefore is active enough, you may not need to do these refreshments. Or refresh just once before the overnight refreshment.\n2) Do an overnight refreshment 0.5:1:0.5 (i.e. 25 gr stiff starter, 50 gr flour, 25 gr water) and leave for 12 hours at 16-18ºC (61-64ºF). If you cannot achieve this low temperature, use less starter (0.25:1:0.5 or even less). You can always place it next day in a warm water bath for an hour or so if it did not grow enough.\n3) Next day, build the preferment by kneading 80 gr of the overnight refreshment, 80 gr of high-gluten flour and 40 gr of water. Allow to triple in volume for 3-3.5 hours at 25-26ºC (77-79ºF).\n4) One hour before the preferment is ready, prepare the dough and allow it to autolyse. Mix 60 gr whole rye flour, 180 gr durum flour (i.e. Italian rimacinata), 240 gr bread flour and 360 gr of water. Leave for 45-60 min while the preferment is getting ready.\n5) Add 180 gr of starter (which contains 120 gr of flour and 60 gr of water), 12 gr of salt and 30 gr of extra water to the dough. Final hydration will be 75%. Knead well.\n6) Follow the procedure described above. Don’t forget to give a full bake. This bread deserves it.\nThis is how a long fermentation bread made with a high-gluten starter looks like:\nI would like to thank Jordi Mercade for his research on Sébastien Boudet’s ‘Durum Pavé’ bread, and to Adolf Peroy for his generosity in providing bibliographical resources.\nDurum Pavé. Spanish Bread Forum January 2015 challenge (in Spanish). Click here.\nBrich, A.N. et al. (2013). The aroma profile of wheat bread crumb influenced by yeast concentration and fermentation temperature. LWT – Food Science and Technology 480-488\nCalvel, R. (2001) The taste of bread. Aspen Publishers, Inc. ISBN: 0-8342-1646-9\nSuas, M. (2009) Advanced bread and pastry. A professional approach. 2009 Delmar, Cengage Learning . ISBN: 978-1-4180-1169-7\nThiele, C., et al. (2002). Contribution of Sourdough Lactobacilli, Yeast, and Cereal Enzymes to the Generation of Amino Acids in Dough Relevant for Bread Flavor. Cereal Chemistry Journal, 79, 45-51', 'by Todd Stephens\nBrewing sour beers has always been a challenge due to the potential of contaminating one’s brewing equipment with the microbes used to sour wort. Many brewers dedicate separate equipment for fermenting, transferring, bottling, or kegging clean and sour beers, which can get expensive and take up a lot of space. Although the risk of cross-contamination can be minimized by using strict sanitation procedures, it only takes a small number of living microbes from a sour batch to ruin a clean beer, so the margin of error is very small. Also, the time required for traditional development of acidity in a sour beer can range from several months to years, requiring space in a somewhat temperature-controlled area to store batches while they develop.\nThe technique of kettle souring has become more popular with its ability to turn around a batch of sour beer in just a few weeks without the risk of cross-contamination, using equipment most brewers already own. All the souring is done in the boil kettle prior to boiling, so the bacteria are killed before the wort is transferred into the fermenter. These quick-soured beers do not display the range of complexity of an aged Belgian Lambic or Flanders red, and do not evolve over time in the bottle, but sometimes a simple, light, tart beer can be the perfect thing on a hot summer afternoon.\nKettle souring is not an advanced or difficult process. There are a couple of tricks to improve the final product, but it’s really a fairly simple task. At a high level, the process of kettle souring starts with the production of wort like any other all-grain batch through mashing and sparging grains, or for an extract brewer, by dissolving the malt extract in the appropriate amount of water. The wort is then inoculated with lactic acid bacteria and allowed to sour until the desired sourness is reached over the course of several hours to a few days. The wort is boiled to kill off the lactic acid bacteria and finished in the fermentor with clean brewer’s yeast. Since the fermentation is done exclusively with brewer’s yeast, the beer can be packaged and served in the same time frame as any other clean beer. Also, the sourness level is locked in at the time of the boil, so the brewer has greater control over the finished product.\nBeer was traditionally soured pre-boil via sour mashing, which uses the naturally occurring bacteria on the grain husks to create lactic acid. This process has a well-deserved reputation for creating horrible smells, due to the multitude of other microbes present on the grain which could lead to spoilage before the lactic acid bacteria could drop the pH sufficiently to prevent off-flavors and toxins from forming in the wort. In the presence of oxygen, naturally occurring bacteria on the grain can produce isovaleric acid, which has a distinct aroma of parmesan cheese or sweaty socks, or butyric acid, which has an aroma of bile. If done properly these issues can be avoided, but the process of kettle souring wort eliminates these risks by controlling the microbes used to sour.\nFor quickly souring pre-boil wort, a culture of lactobacillus must be used. A culture can be made from grain, probiotic drinks and capsules, or a cultured dairy product containing live cultures, such as sour cream or yogurt. A pure culture can also be purchased from any of the major yeast laboratories. Commercially available lactobacillus species include lactobacillus delbruecki, lactobacillus buchneri, lactobacillus brevis, and lactobacillus plantarum. Of the four, l. brevis and l. plantarum are probably the most reliable for quick souring. The metabolism of lactic acid bacteria is not as well documented as for brewer’s yeast, but there is some evidence that lactobacillus multiplies until about pH 3.8, at which point the lactic acid inhibits cell growth. As such, making a small 1-liter starter to increase the number of active cells will help ensure quick souring of the wort.\nWhat do heteros and homos have to do with beer?\nLactic acid bacteria can be classified as heterofermentative or homofermentative. In reality it is a little more complicated than this, but as long as the bacteria are not exposed to oxygen during growth, then they can be considered one or the other. Homofermentative means that the bacteria eat sugar in the wort and produce only lactic acid. Heterofermentative means that the bacteria convert sugar to both lactic acid and ethanol. Many brewers have noticed large krausen and significant gravity drop from using pure cultures of lactobacillus, and it was assumed that this was due to heterofermentative lactobacillus. Some new research, however, has shown that pure cultures of heterofermentative lactobacillus under controlled conditions can only create fractions of a percent of alcohol. The natural explanation for this is that many commercial bacteria cultures available to homebrewers are often contaminated with yeast, which is responsible for the drop in gravity and the creation of alcohol.\nIt’s important to know whether your lactic acid bacteria culture creates alcohol, because if it does it will produce carbon dioxide and a large krausen in the souring vessel, and may require a blow-off tube. Also, ethanol produced by yeast in a contaminated lactobacillus culture will evaporate during the boil, so this may affect your decision to boil the wort after souring or just pasteurize it. If your bacteria starter shows any bubbling or airlock activity, then it is safe to assume that there is yeast in the culture. Although not guaranteed, lactic acid bacteria cultured from grain husks will more than likely be contaminated with wild yeast, so some ethanol will probably be produced during souring.\nPre-acidifying the wort to a pH of 4.5 has some advantages when kettle souring. This can be done with food-grade lactic acid, and should happen after collecting all the wort in the boil kettle so that the mash pH can be kept in the proper range. Ideally this would be measured with a calibrated pH meter, but brewing water calculators can be used to get close. When using wild lactic acid bacteria from grain, dropping the pH to 4.5 will inhibit the growth of unwanted microbes and reduce the risk of off-flavors and spoiling the wort before the lactobacillus can get to work. In addition, some lactic acid bacteria produce an enzyme that can break down the proteins that aid in foam retention. These enzymes are only active around a pH of 5, so pre-adjusting the pH to around 4.5 will help in producing a sour beer with good head retention.\nBut that’s enough science and background information.\nHere’s how to do it.\nThe pre-boil wort can be collected in the boil kettle the same way it is done on any other brew day, through lautering and sparging. A mash-out in the 170-180F temperature range should bring the grain bed up to pasteurization temperatures, but if a completely sanitary wort is desired it can be brought up to a short boil in the kettle, or held at 170F for 15 minutes or so to pasteurize the wort. Keeping the wort below 180F will also prevent any DMS from forming. Don’t add any hops yet – hops only serve to inhibit the growth of lactobacillus, which is counter-productive in quick souring. If some hop presence is desired, they can be added during the boil later.\nThe wort should be cooled to the optimal growth temperature for the type of lactobacillus used, which is 80-100F for l. plantarum, or 110-120F for other lactobacillus cultures. Especially for wild cultures, keeping the temperatures at the higher end of the range will favor the growth of the lactobacillus and inhibit the growth of spoilage bacteria. Third, the wort should be adjusted to pH 4.5 and the lactic acid bacteria culture pitched.\nAt this point:\nOxygen is the enemy. If a wild lactobacillus culture is used, oxygen exposure can lead to butyric or isovaleric acid. Some pure cultures, such as l. plantarum, can produce acetic acid if exposed to too much oxygen, so it’s best to take steps to minimize it. If left in the kettle, the headspace can be purged with carbon dioxide and sealed with tape and plastic wrap, and a layer of plastic wrap placed directly on the surface of the wort to keep out oxygen. The wort can also be transferred to a carboy with an airlock. Wrapping the vessel with a thick blanket or sleeping bag can help maintain temperature during the souring period, although the mass of the wort may be high enough that it will hold temperatures long enough to sour. Depending on the lactobacillus species, this may be anywhere from 12-24 hours for l. brevis and l. plantarum, or a few to several days for l. buchneri or l. delbruecki. Knowing whether your variety of lactobacillus is heterofermentative will also help avoid a mess from a blown-out lid or airlock.\nOnce the desired sourness is reached, the souring is stopped by heating to pasteurization temperatures to kill the bacteria, and can be boiled and hopped as any other brew. If a lactic acid bacteria culture containing some yeast was used, the presence of ethanol in the wort after souring may affect whether you boil the wort after souring or just pasteurize it, as the ethanol will certainly evaporate during the boil. If a significant gravity drop is measured, pasteurizing at 165-170F for 15 minutes will minimize the amount of ethanol lost, as ethanol boils at 173.1F. For styles such as Berliner Weisse and Gose that do not have a significant hop presence, pasteurizing the soured wort and cooling to yeast-pitching temperatures is a good approach, and boiling the wort is not necessary. For styles that do require some hop presence, such as porter or saison, the wort can be boiled and hopped as normal.\nSince the low pH and oxygen level creates a somewhat hazardous environment to yeast, pitching the yeast at double the normal rate is a good idea and will help ensure a good fermentation of the sour beer. Some yeast varieties are more tolerant than others of acidic environments. For liquid yeasts, acid tolerant strains include saison and kolsch yeasts, and for dry yeast US-05 works well. This is certainly not a complete list – many varieties can perform well in an acidic environment, but if in doubt, these examples should work fine.\nAnd that’s it. Kettle soured beers can usually be bottled or kegged after two or three weeks, but as always, let the hydrometer be the real guide. Had the souring bacteria not been killed by boiling the wort, the fermentation time could easily stretch out to several months as wild yeast and bacteria can slowly ferment longer-chain sugars over time leading to over-carbonation and possibly fracturing bottles. The real trade-off with kettle souring is complexity and flavor development for quick turn-around. For a style of beer that is meant to be consumed young, though, this is a fairly easy technique in your toolbox to add a different dimension in your homebrewed beers.']	['<urn:uuid:1030954f-139a-464a-8049-56872a9a2b31>', '<urn:uuid:55e1dcdc-2107-4b0f-ba6d-643e9dc346ae>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T23:00:04.658849	29	90	3154
63	what are two types of warranties	There are two types of warranties: express and implied. Express warranties are those that are defined specifically in the contract, while implied warranties are based in statutory and/or common law. Implied warranties include warranty of merchantability (goods/services must conform to ordinary buyer's standards) and warranty of fitness for a particular purpose (if seller knows intended purpose, product is implied to be fit for that purpose).	['Construction contracts can contain terms that impact your company’s bottom line. Review all contracts carefully before signing to save your company time and money. The following guide highlights some common contract terms and their potential impact. Understanding these will help protect your company’s assets. Always seek the help of licensed counsel to analyze the commercial risks associated with your construction contracts.\nScope of the Agreement – Examine the definition of services to be provided to ensure the language is clear enough for an unrelated third party to understand the scope. The contract should include a time frame for completion of services. The rights and obligations of both parties should be clearly outlined. Any mechanism for changing the scope of the contract, as well as any of the terms, if allowed, should also be outlined within the contract.\nTerms of Payment – Terms of payment should be clearly listed within the contract so that the expectations of both parties are clear. The contract should specify the agreed payment schedule for goods received.\nWarranties – There are two types of warranties: express and implied. Both types are assurances regarding particular issues, such as performance. Express warranties are those that are defined specifically in the contract. Implied warranties are based in statutory and/or common law, depending upon your jurisdiction. They are two-fold: a warranty of merchantability, which requires that goods/services must reasonably conform to an ordinary buyer’s standards, and a warranty of fitness for a particular purpose, which states that if a seller knows the intended purpose for the product or service, the act of selling the product to that customer implies that it is fit for that purpose. Be aware of warranty disclaimers and understand how the disclaimer limits your statutory rights. If it disclaims all warranties, express and implied, then you will likely be limited to the remedies in the contract for issues related to things like performance. You should also examine any disclaimer in the context of the contract. While it may require you to disclaim your statutory rights, other contract language may give you adequate rights and remedies regarding the points about which you are most concerned.\nDamages, Limits of Liability and Indemnification – These three items are often in close proximity to one another in a contract, as they are interrelated. Damages may be defined as certain types of losses that could create liability under the contract. A limit on liability would restrict the amount of damages that a party would be required to pay if found liable for such damages. Sometimes this may also include a limit for indemnification. Indemnification provisions allocate risk and cost between the parties. It is important to examine whether the party assuming the risk is the party with the most control over that risk. For instance, when a company’s employees are required to work at a customer’s location, the company is often asked to release the customer from all liability relating to the employees’ presence at the customer’s location. In some cases, indemnification is limited to negligence or to a specific dollar amount, under a heading of “limits of liability.”\nInsurance – Some contracts will contain minimum bodily injury and property damage liability coverage amounts that the party must possess and also may require that the customer is added as an additional insured on those coverages. Prior to consenting to any contract, it is prudent to examine insurance coverage against the amount of liability exposure in a particular contract.\nTerms and Conditions – It is also vital to examine any terms and conditions contained in the contract:\n- Governing Law & Jurisdiction – Look at the governing law provision to make sure that you are comfortable with the implications of the state law chosen by the drafter. This can impact the interpretation of the contract from warranties to indemnification. Additionally, when specific statutes or regulations are referenced in the body of a contract, it is as though that statute or regulation is wholly contained within the contract itself. It is vital to read and understand that language prior to giving your consent. This happens regularly in government contracting situations.\n- Dispute Resolution – This is another clause with which you must be comfortable with the laws of the state or forum chosen by the drafter. The rules chosen to govern dispute resolution can impact the outcome. Additionally, you should consider whether dispute resolution is right for your situation.\n- Intellectual Property – When you are disclosing and/or licensing your company’s intellectual property, be it trademarks, copyrights or patents, it is important to include a clause that recognizes the owner of such intellectual property and affirmatively states that the agreement does not transfer any rights.\n- Standard of Care – A standard of care clause may appear in certain types of contracts. The standard of care that is provided by the law should provide the minimum standard of care for the provision of services under the contract.\n- Term/Termination – The contract should provide both parties with the right to terminate the contract. The situations in which termination is allowed will vary from contract to contract. Some contracts will allow the right to terminate in cases of dissatisfaction; others will allow it with a specific notice, for no cause. It is important that you contemplate in what cases you would want the right to terminate the contract. There should also be language defining the term of the contract. Does it have a finite term? Does it automatically renew each period?\n- Right to Cure – Related to termination, some contracts will contain a right to cure clause. This would give the defaulting party notice of a breach and a finite period of time in which to remedy such a breach.\nStandard Form Contracts – Unlike other industries, construction lacks a consistent set of laws like the Uniform Commercial Code or a federal statutory scheme. Contracts produced by professional and trade associations for architects (American Institute of Architects), engineers (Engineers Joint Contract Documents Committee) and commercial contractors (Associated General Contractors of America) can serve as important references and benchmarks when drafting a new contract. They are a good source of industry best practices, and using them can greatly reduce drafting and review time, meaning lower overall transaction costs for your company. For all of their advantages, there are several things that you should be cautious about when using standard form contracts. Note the following cautions about standard forms before using them:\n- Standard forms, which are written broadly to encompass many different contexts, require transaction-specific and jurisdiction-specific modifications. For example, certain states require that indemnities be written in a certain way.\n- Changes made to one part of the document, such as definitions of words or terms, may affect other parts that make reference to it.\n- Custom-drafted and industry-drafted forms are often incompatible. Even industry-drafted forms from different publishers can be incompatible.\n- Standard forms always contain the bias of the drafter. Use this bias; know when to use various standard forms published by different industry organizations.\nReviewing general terms and features of construction contracts will help you grasp the consequences of their terms and conditions for your business. In any case, to ensure its completeness and accuracy, it is necessary to submit each contract you must sign to legal review.']	['<urn:uuid:3314e167-4de0-4f4b-a9e1-7e1c57b8d557>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T23:00:04.658849	6	65	1218
64	alzheimers disease symptoms treatment approaches	Alzheimer's disease symptoms include memory loss, difficulty with daily tasks, unfamiliarity with known faces, and problems with speech and writing. While there is no cure, treatment approaches include FDA-approved drugs like donepezil, rivastigmine, galantamine, and memantine that can help maintain cognitive abilities for months to years. Additional medications can help manage behavioral symptoms like anxiety, depression, and sleeplessness.	['Age cannot be a consideration for forgetfulness; we all succumb to it at some point or other in our lives. As school-going kids and teens, we lose track of the right answers during an exam out of sheer nervousness. As we grow older, we might misplace things, or even leave things behind. And it is a known fact that memory tends to lose its earlier sharpness, as we step into our fifties and sixties. That is when people tend to comment, “Oh, you are growing old.”\nThese are quite common occurrences, however, and not really a cause for worry. The trouble begins when these lapses in memory begin to increase in frequency and other complications make their appearance. Medical experts refer to this as the onset of Alzheimer’s disease. Now, there is a specific protein known as beta-amyloid protein, which circulates in human blood, as well as the cerebrospinal fluid. Suddenly one day, this protein decides to make its home in the brain, the area which is responsible for our thoughts, language and memory. As a result, the nerve cells here begin to die.\nThe process is slow, for the manifestations of Alzheimer’s disease are visible only after the age of sixty and beyond. Recent events are just wiped off from memory. Familiar faces seem unfamiliar, and even names are forgotten. After some time, even family members are treated as strangers. Routine daily tasks such as combing the hair or brushing the teeth are skipped because the person cannot remember. The severity of the disease can be understood when there is no lucidity in speech, and reading and writing become difficult tasks to perform. The individual may wander away from home and fail to return, or even forget the way. When anxiety and aggression become dominant features of the person’s personality, it is time to seriously think about a personal caregiver or admission into a skilled care facility. Thus, planning, reasoning, sense of judgment, language and perception—are all affected.\nAs with other diseases, Alzheimer’s disease can be hereditary. If a family member has already had it, it is possible to be passed on. Fortunately, this is still a rare occurrence. For those who are already suffering from chronic illnesses such as heart disease, high blood pressure or hypertension, diabetes or obesity, Alzheimer’s disease is an associated risk factor. Those afflicted with Down’s syndrome have no escape; the illness is going to hit them by age forty. A surprising new fact that has come to light is that someone with less than eight years of education could also succumb to this disease.\nConsidering that there is still much to be discovered about this disease, doctors cannot promise a surefire treatment to cure it. After all, it is associated with aging. But, there are specific drugs that can delay the process and offer a better prognosis. Most important of all, one can take heart from the fact that every aged person need not necessarily fall prey to Alzheimer’s disease; it chooses its targets.', 'How is Alzheimer’s Disease Treated?\nAlzheimer’s disease is a complex disease, and no single “magic bullet” is likely to prevent or cure it. That’s why current treatments focus on several different issues, including helping people maintain mental function, managing behavioral symptoms, and slowing Alzheimer’s disease.\nAlzheimer’s disease research has developed to a point where scientists can look beyond treating symptoms to think about delaying or preventing Alzheimer’s disease by addressing the underlying disease process. Scientists are looking at many possible interventions, including treatments for heart disease and Type 2 diabetes, immunization therapy, cognitive training, changes in diet, and physical activity.\nWhat drugs are currently available to treat Alzheimer’s disease?\nNo treatment has been proven to stop Alzheimer’s disease. The U.S. Food and Drug Administration has approved four drugs to treat Alzheimer’s disease. For people with mild or moderate Alzheimer’s disease, donepezil (Aricept®), rivastigmine (Exelon®), or galantamine (Razadyne®) may help maintain cognitive abilities and help control certain behavioral symptoms for a few months to a few years. Donepezil can be used for severe Alzheimer’s disease as well. Another drug, memantine (Namenda®), is used to treat moderate to severe Alzheimer’s disease. However, these drugs don’t stop or reverse Alzheimer’s disease and appear to help patients only for months to a few years.\nThese drugs work by regulating neurotransmitters, the chemicals that transmit messages between neurons. They may help maintain thinking, memory, and speaking skills and may help with certain behavioral problems.\nOther medicines may ease the behavioral symptoms of Alzheimer’s disease: sleeplessness, agitation, wandering, anxiety, anger, and depression. Treating these symptoms often makes patients more comfortable and makes their care easier for caregivers.\nNo published study directly compares the four approved Alzheimer’s disease drugs. Because they work in a similar way, it is not expected that switching from one of these drugs to another will produce significantly different results. However, an Alzheimer’s disease patient may respond better to one drug than another.\nWhat potential new treatments are being researched?\nNIA, part of the National Institutes of Health, is the federal agency for Alzheimer’s disease research. NIA-supported scientists are testing a number of drugs and other interventions to see if they prevent Alzheimer’s disease, slow the disease, or help reduce symptoms.\nScientists are very interested in the toxic effects of beta-amyloid–a part of amyloid precursor protein found in deposits (plaques) in the brains of people with Alzheimer’s disease. Studies have moved forward to the point that researchers are carrying out preliminary tests in humans aimed at removing beta-amyloid, halting its formation, or breaking down early formation before it can become harmful. For example, in a clinical trial sponsored by NIA, scientists are testing whether “passive” immunization with an FDA-approved drug called IGIV can successfully treat people with Alzheimer’s.\nThe aging process\nSome age-related changes may worsen Alzheimer’s disease damage in the brain. Researchers think that inflammation may play a role in Alzheimer’s disease. Studies have suggested that common nonsteroidal anti-inflammatory drugs (NSAIDs) might help slow the progression of Alzheimer’s disease, but clinical trials so far have not shown a benefit from these drugs. Researchers are continuing to look at how other NSAIDs might affect the development or progression of Alzheimer’s disease.\nScientists are also looking at free radicals, which are oxygen or nitrogen molecules that combine easily with other molecules. The production of free radicals can damage nerve cells. The discovery that beta-amyloid generates free radicals in some Alzheimer’s disease plaques is a potentially significant finding in the quest to understand Alzheimer’s disease.\nHeart disease and diabetes\nResearch has begun to tease out relationships between Alzheimer’s disease and vascular diseases, which affect the body’s blood vessels. Some scientists have found that some chronic conditions that affect the vascular system, such as heart disease and diabetes, have been tied to declines in cognitive function or increased Alzheimer’s disease risk. Several clinical trials are studying whether treatments for these diseases can improve memory and thinking skills in people with Alzheimer’s disease or mild cognitive impairment.\nA number of studies suggest that factors such as a healthy diet, exercise, and social engagement may be related to the risk of cognitive decline and Alzheimer’s disease. For example, emerging evidence suggests that physical activity might be good for our brains as well as our hearts and waistlines. Some studies in older people have shown that higher levels of exercise are associated with a reduced risk of Alzheimer’s disease. Clinical trials are underway to study the relationship of exercise to healthy brain aging and the development of Alzheimer’s disease.\nScientists have also studied whether diet may help preserve cognitive function or reduce Alzheimer’s disease risk. Some studies have found that the “Mediterranean diet” is associated with a reduced risk of Alzheimer’s disease. To confirm the results, scientists are conducting clinical trials to examine the relationship between specific dietary components and cognitive function and Alzheimer’s disease.\nStudies are looking into many other possible treatments, including hormones and cognitive training, to see if they might improve thinking skills in people with Alzheimer’s disease or even prevent Alzheimer’s disease in people who are at risk.\nWhat are clinical trials?\nPeople who want to help scientists test possible treatments may be able to take part in clinical trials, which are research studies that test the safety, side effects, or effectiveness of a medication or other intervention in humans. Study volunteers help scientists learn about the brain in healthy aging as well as what happens in Alzheimer’s disease. Results of Alzheimer’s disease clinical trials are used to improve prevention and treatment approaches.']	['<urn:uuid:0d954c7f-a1ce-4d96-a721-ab2a2aaab685>', '<urn:uuid:0f294026-6a17-44dd-8369-19460ba33a23>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T23:00:04.658849	5	58	1419
65	compare historical impact lasting effects zheng he african expeditions vasco da gama india route	Vasco da Gama's route had more significant lasting effects than Zheng He's expeditions. Gama's discovery created far-reaching changes in commercial, agricultural, political, religious, and social aspects of many eastern countries, established Portuguese maritime supremacy, and led to centuries of European trade monopoly. In contrast, Zheng He's expeditions to Africa, despite their grand scale, left no durable trade ties, and even in places like Kenya's Lamu archipelago, no lasting Chinese customs or language remained, with only legends persisting.	"['More than half a millennium later, Zheng has become a potent symbol for modern China. In 2005, the country marked the 600th anniversary of the seven voyages from 1405 to 1433 undertaken by Zheng\'s vast ""treasure fleets"" with nationwide celebrations; the opening ceremony of the 2008 Olympic Games in Beijing dramatized his explorations from Southeast Asia to the Middle East and the shores of Africa. On Feb. 26, China\'s Ministry of Commerce announced it was funding a three-year project with the assistance of the Kenyan government to search for Ming-era vessels that had supposedly foundered off the East African coast. ""Historical records indicate Chinese merchant ships sank in the seas around Kenya,"" Zhang Wei, a curator for a state museum, told China\'s official Xinhua news agency. ""We hope to find wrecks of the fleet of the legendary Zheng He.""\nThere is more than historical curiosity behind these new efforts. For centuries after his expeditions, Zheng a Muslim eunuch slipped out of public awareness, obscured by the rise and fall of new dynasties. Talk of his exploits was revived briefly at the beginning of the 20th century as the fledgling Chinese republic sought to build a navy in the shadow of imperial Japan. But experts say his place as a patriotic national hero has been truly cemented only in the past two decades, parallel with China\'s geopolitical rise and the growth of its significant economic presence in many African nations and countries around the Indian Ocean.\nThe legacy of Zheng\'s voyages involving hundreds of ships, some exponentially larger than the three captained by Christopher Columbus decades later, in 1492 is being invoked by the Chinese as historical proof of the difference between China\'s and the West\'s roles in the world. Though the unprecedented display of maritime power was meant to extend the Ming dynasty\'s reach over a network of tributary states, Zheng rarely resorted to the type of violent, coercive measures taken for centuries by European colonizers, especially in Africa. ""Zheng\'s a nominal symbol of China\'s peaceful engagement with the world,"" says Geoffrey Wade, a historian at the Institute of Southeast Asian Studies in Singapore who has translated Ming records pertaining to the voyages. ""With him, it\'s like the Chinese have an ambassador of friendship a sign that they aren\'t going to hurt anybody.""\nIn recent years, though, Beijing has come under criticism for an approach to Africa that is perhaps more bloodless than it is cuddly. China\'s support of autocratic regimes, from Zimbabwe to Sudan where Beijing effectively built up an oil industry from scratch has exposed the Asian giant to accusations of turning a blind eye to human-rights abuses as it goes about securing natural resources and political influence. China has pumped billions of dollars into infrastructure projects throughout the continent, tying up key contracts in resource-rich states like Angola and the war-torn Democratic Republic of Congo.\nYet as total annual trade between Africa and China has surpassed $100 billion, Beijing has won its fair share of admirers too, not least among them many Africans whose quality of life has been improved by an influx of cheap Chinese household goods. China has also established a network of ""Confucius Institutes"" in various African cities to disseminate Chinese culture, while more and more African exchange students are attending Chinese universities. A flotilla of Chinese warships is part of an international operation attempting to curb piracy off the shores of Somalia. ""This discussion of Zheng He is being carried out in China at a higher and more expensive level not just to boost the glory of his personal story,"" says Barry Sautman, a specialist on China-Africa relations at Hong Kong University of Science and Technology, ""but as a particular cog in China\'s projection of itself into Africa.""\nAlthough the aura of Zheng\'s expeditions may somehow bolster China\'s budding soft power, it\'s unclear what lasting impact the visiting fleets had on medieval Africa. No durable trade ties were left in place. And while stories linger in Kenya\'s Lamu archipelago of a light-skinned community descended from shipwrecked Chinese sailors, the population there retains no trace of Chinese customs or language. ""Not much endured beyond the legend,"" says Sautman. Indeed, scholars like Wade suggest the voyages themselves were something of an ""aberration"" in the wider context of Chinese foreign policy in that era, which for centuries was far more focused on staving off the threat of invasion along its fragile land borders.\nMoreover, though Beijing plays up the voyages as a triumphant Chinese adventure, the journeys had a distinctly Muslim character. Zheng practiced Islam, as did Ma Huan, the main chronicler aboard the ships. It\'s likely they were guided to their many ports of call, such as Malacca, India\'s Malabar coast and Malindi in Kenya, by Muslim pilots of Arab, Indian or African extraction. ""They were essentially following maritime routes that had been in use by people in the Indian Ocean for ages,"" says Wade. Many academics argue that the popular Arab-Persian tale of the Seven Voyages of Sinbad, littered also with snippets of Indian folklore, was derived from the real travels of Zheng He making the mariner as much a pan-Asian protagonist as a Chinese one.\nNo matter the many layers of myth surrounding Zheng He, the Chinese are confident they\'ll uncover a Ming-era wreck near the Lamu archipelago, where bits of Ming ceramic ware have surfaced in the past, and that it will be their legacy that gets burnished when they find it. A team of Chinese archaeologists is expected to commence work in July. It won\'t be alone last year, following a visit to Kenya by Chinese President Hu Jintao, a Chinese state petroleum company won concessions to explore more than 100,000 sq km of Kenyan waters for oil. That will be theirs too. Africa, after all, holds more for China these days than just exotic animals.', ""Vasco da Gama is known as the hero of the age of discoveries, the European historical era from the 15th to 18th centuries. He discovered a new sea route from Europe to the far east around the Cape of Good Hope on the southern tip of Africa. Gama started from Lisbon in 1497 and reached Panthalayani Kollam, a port on the Malabar coast near Calicut, after sailing for ten and a half months.\nAccording to Adam Smith, “the two greatest incidents in world history” are the discovery of America by Columbus and the finding of the new sea route to India by Gama. Historians describe Gama’s arrival as an entente cordial that allowed the east and the west to meet without an intermediary.\nGama was born in 1460 as the third son of Estêvão da Gama, a minor provincial nobleman, who was commander of the fortress of Sines, a coastal town in Alentejo province in southwestern Portugal. The Portuguese kings at the time were in search of newer options to build up the royal treasury in order to reduce their dependence on the feudal nobility. Spice trade with the east was one of their prime targets. Several expeditions were mounted to open up a new route to the east, and Gama, with his proven seafaring capability, was among the chosen ones to lead one of those.\nHe set sail from Lisbon on July 8, 1497, and braving several hardships, managed to reach the Malabar coast on May 20, 1498. With his voyage, Gama showed the western world the immense possibilities of trade with the east. The occupation of Constantinople by the Ottoman Turks in 1453 had shut the European trade routes to India. Pepper and other spices were denied to the Europeans by the Arabs. Gama’s new route opened up a wide range of possibilities. It helped the Portuguese avoid the Mediterranean, the Persian Gulf and the Arabian peninsula and ensure monopoly over the lucrative spice trade. The next three centuries, however, were noted for the struggle among the Europeans for the monopoly of spice trade with India, especially Malabar, which was known as the “land of pepper”\nGama's discovery heralded a new age of European imperialism in Asia. Starting with spices like pepper and cinnamon, the Portuguese traders gradually diversified into other products, cementing a commercial monopoly. It helped the Portuguese rulers revive their economy, which until then relied heavily on trade with coastal Africa. The new sea route was a beginning. The spice growers got good prices, cultivation was increased and hinterland navigation and trade improved.\nBy cleverly mixing politics and war, Gama kept the Portuguese flag flying in India, and, in the process, ensured his country's supremacy in maritime trade. He showed the importance of establishing secure outposts on the eastern coast of Africa along the trade routes to the east. Gama’s voyage made an indelible mark not just on Malabar. It created far-reaching changes in the commercial, agricultural, political, religious and social arena of a number of countries in the east.\nApart from laying the foundation for Portuguese imperialism, Gama's voyage also marked the beginning of global multiculturalism. Homages have been paid worldwide to celebrate his accomplishments. The Portuguese national epic, Os Lusíadas, was written in his honour by Luís Vaz de Camões. The port city of Vasco da Gama in Goa is named after him. Three football clubs in Brazil, including Club de Regatas Vasco da Gama, and Vasco Sports Club in Goa are named after him. A church in Kochi, Kerala, and a private residence on the island of Saint Helena bear Gama's name. The suburb of Vasco in Cape Town, South Africa is named after him. A few places in Lisbon's Parque das Nações, such as the Vasco da Gama Bridge, Vasco da Gama Tower and the Centro Comercial Vasco da Gama shopping centre, too, are named in his honour.\n“Portugal’s navigators, showing Europe the way across the oceans, brought back more than spices, gold, precious stones and silk,” said Mario Soares, former president of Portugal. “They interacted with ancient cultures and gave us the vision of the one world we have today.” Whether some people like it or not, Gama enjoys a prominent position in history as the man who led the expedition that connected the western world to the east.\nCharles Dias is a former MP, vice president of the Kerala History Association, and director of the Indo-Portuguese Cultural Centre\nFaxian was the first Chinese pilgrim scholar to leave an account, A Record of Buddhistic Kingdoms, of his travels to India and Sri Lanka. He set out from China in 399 CE in search of the authentic Buddhist canon and travelled extensively in the “holy land” of Buddhism. He visited the important places in the Buddha’s life, including Kapilavastu, Sarnath, Bodh Gaya and Kushinagar. He was so impressed by what he saw and said India was very productive and the people were happy beyond compare. His journey inspired many Chinese scholars to travel to India in search of scholarly texts and the ultimate truth. He also spent two years in Sri Lanka and collected Buddhist texts to take back home. While returning from Sri Lanka, Faxian’s boat ran into rough weather and he spent about 200 days stranded at sea. Faxian’s accounts helped start relations between India and China and contain exclusive information on Indian Buddhism. He returned to China with many Sanskrit texts and translated them to strengthen the foundation of Buddhism in the country.""]"	['<urn:uuid:cef85e23-d388-49bd-b0d0-88727d49710d>', '<urn:uuid:4d13e1d3-2df7-4fce-b167-fde1a13f5b97>']	factoid	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-12T23:00:04.658849	14	77	1890
66	treaty jewish romans military cooperation terms	The Romans and Jews established a treaty of alliance inscribed on bronze tablets. According to the terms, if war was made on Rome, the Jewish nation would fight alongside them wholeheartedly. The Jews would not provide grain, weapons, money, or ships to Rome's enemies. Similarly, if the Jewish nation was attacked, the Romans would fight with them willingly. Both parties could add or remove terms if they mutually agreed.	"[""- Prayer and Worship\n- Beliefs and Teachings\n- Issues and Action\n- Catholic Giving\n- About USCCB\nEulogy of the Romans. 1* Judas had heard of the reputation of the Romans. They were valiant fighters and acted amiably to all who took their side. They established a friendly alliance with all who applied to them. 2He was also told of their battles and the brave deeds that they performed against the Gauls,* conquering them and forcing them to pay tribute; 3and what they did in Spain to get possession of the silver and gold mines there. 4By planning and persistence they subjugated the whole region, although it was very remote from their own. They also subjugated the kings who had come against them from the far corners of the earth until they crushed them and inflicted on them severe defeat. The rest paid tribute to them every year. 5Philip* and Perseus, king of the Macedonians, and the others who opposed them in battle they overwhelmed and subjugated. 6Antiochus* the Great, king of Asia, who fought against them with a hundred and twenty elephants and with cavalry and chariots and a very great army, was defeated by them. 7They took him alive and obliged him and the kings who succeeded him to pay a heavy tribute, to give hostages and to cede 8Lycia, Mysia, and Lydia* from among their best provinces. The Romans took these from him and gave them to King Eumenes. 9* When the Greeks planned to come and destroy them, 10the Romans discovered it, and sent against the Greeks a single general who made war on them. Many were wounded and fell, and the Romans took their wives and children captive. They plundered them, took possession of their land, tore down their strongholds and reduced them to slavery even to this day. 11All the other kingdoms and islands that had ever opposed them they destroyed and enslaved; with their friends, however, and those who relied on them, they maintained friendship. 12They subjugated kings both near and far, and all who heard of their fame were afraid of them. 13Those whom they wish to help and to make kings, they make kings; and those whom they wish, they depose; and they were greatly exalted. 14Yet with all this, none of them put on a diadem or wore purple as a display of grandeur. 15But they made for themselves a senate chamber, and every day three hundred and twenty men took counsel, deliberating on all that concerned the people and their well-being. 16They entrust their government to one man* every year, to rule over their entire land, and they all obey that one, and there is no envy or jealousy among them.\nTreaty with the Romans. 17So Judas chose Eupolemus, son of John, son of Accos, and Jason, son of Eleazar, and sent them to Rome to establish friendship and alliance with them.a 18He did this to lift the yoke from Israel, for it was obvious that the kingdom of the Greeks was subjecting them to slavery. 19After making a very long journey to Rome, the envoys entered the senate chamber and spoke as follows: 20“Judas, called Maccabeus, and his brothers, with the Jewish people, have sent us to you to establish alliance and peace with you, and to be enrolled among your allies and friends.” 21The proposal pleased the Romans, 22and this is a copy of the reply they inscribed on bronze tablets and sent to Jerusalem,* to remain there with the Jews as a record of peace and alliance:b\n23“May it be well with the Romans and the Jewish nation at sea and on land forever; may sword and enemy be far from them. 24But if war is first made on Rome, or any of its allies in any of their dominions, 25the Jewish nation will fight along with them wholeheartedly, as the occasion shall demand; 26and to those who wage war they shall not give or provide grain, weapons, money, or ships, as seems best to Rome. They shall fulfill their obligations without receiving any recompense. 27In the same way, if war is made first on the Jewish nation, the Romans will fight along with them willingly, as the occasion shall demand, 28and to those who attack them there shall not be given grain, weapons, money, or ships, as seems best to Rome. They shall fulfill their obligations without deception. 29On these terms the Romans have made an agreement with the Jewish people. 30But if both parties hereafter agree to add or take away anything, they shall do as they choose, and whatever they shall add or take away shall be valid.\n31“Moreover, concerning the wrongs that King Demetrius is doing to them, we have written to him thus: ‘Why have you made your yoke heavy upon our friends and allies the Jews? 32If they petition against you again, we will enforce justice and make war on you by sea and land.’”\n* [8:1] This chapter contains the account of the embassy which Judas sent to Rome, probably before the death of Nicanor, to conclude a treaty of alliance between Rome and the Jewish nation. Without precise chronology, the pertinent data are gathered into a unified theme.\nThe image of the Roman Republic greatly impressed the smaller Eastern peoples seeking support against their overlords (vv. 1–16), because of Roman success in war (vv. 2–11) and effective aid to their allies (vv. 12–13). Numerous interventions by Rome in the politics of the Near East bear witness to its power and prestige in the second century B.C. See 1:10; 7:2; 12:3; 15:15–24; 2 Mc 11:34. With the increased Roman control of Palestine after 63 B.C., the Republic and later the Empire became heartily detested. The eulogy of Rome in this chapter is one of the reasons why 1 Maccabees was not preserved by the Palestinian Jews of the century that followed.\n* [8:2] Gauls: probably the Celts of northern Italy and southern France, subdued by the Romans in 222 B.C., and again in 200–191 B.C.; but also those in Asia Minor (the Galatians), whom the Romans defeated in 189 B.C.\n* [8:5] Philip: Philip V of Macedonia, defeated by a Graeco-Roman alliance at Cynoscephalae in 197 B.C. Perseus, his son, was defeated at Pydna in 168 B.C., and died a prisoner. With this, the kingdom of Macedonia came to an end.\n* [8:6] Antiochus: Antiochus III, greatest of the Seleucid kings. He was defeated at Magnesia in 190 B.C. By the Treaty of Apamea in 189 B.C., he was obliged to pay Rome a crushing indemnity of 15,000 talents. The weakening of Antiochene power and the growing military and economic influence of Rome may have led Antiochus IV to adopt the policy of political, religious, and cultural unification of Syria and Palestine.\n* [8:8] Lycia, Mysia, and Lydia: regions in western Asia Minor. “Lycia” and “Mysia” are restored here by conjectural emendation; the Greek text has “India, Media,” most likely through scribal error. Eumenes: Eumenes II (197–158 B.C.), king of Pergamum, an ally of Rome who benefited greatly from Antiochus’ losses.\n* [8:16] They entrust their government to one man: actually the Roman Republic had two consuls chosen yearly as joint heads of the government.\n* [8:22] The reply…on bronze tablets and sent to Jerusalem: the decree of the Senate would be inscribed on bronze and kept in the Roman Capitol, with only a copy in letter form sent to Jerusalem.\nBy accepting this message, you will be leaving the website of the\nUnited States Conference of Catholic Bishops. This link is provided\nsolely for the user's convenience. By providing this link, the United\nStates Conference of Catholic Bishops assumes no responsibility for,\nnor does it necessarily endorse, the website, its content, or""]"	['<urn:uuid:efef13b2-93b2-4faf-8338-b8ba260149d5>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T23:00:04.658849	6	69	1295
67	I need to start taking ambrisentan but I'm worried about side effects - what are the most common problems that people experience with this medication?	The most common side effects of ambrisentan tablets include swelling of hands, legs, ankles and feet (peripheral edema), stuffy nose (nasal congestion), inflamed nasal passages (sinusitis), and hot flashes or getting red in the face (flushing).	['Important Safety Information and Indication\nAmbrisentan tablets are indicated for the treatment of pulmonary arterial hypertension (PAH) (WHO Group 1):\n• To improve exercise ability and delay clinical worsening.\nStudies establishing effectiveness included predominantly patients with WHO Functional Class II–III symptoms and etiologies of idiopathic or heritable PAH (60%) or PAH associated with connective tissue diseases (34%).\nWhat is the most important information I should know about ambrisentan tablets?\nFemales who are able to get pregnant must use two acceptable forms of birth control during treatment with ambrisentan tablets, and for one month after stopping ambrisentan tablets because the medicine may still be in the body.\n- If you have had a tubal sterilization or have an IUD (intrauterine device) or progesterone implant, these methods can be used alone and no other form of birth control is needed.\n- Talk with your doctor or gynecologist (a doctor who specializes in female reproduction) to find out about options for acceptable forms of birth control that you may use to prevent pregnancy during treatment with ambrisentan tablets./li>\n- If you decide that you want to change the form of birth control that you use, talk with your doctor or gynecologist to be sure that you choose another acceptable form of birth control.\nSee the chart below for Acceptable Birth Control Options during treatment with ambrisentan tablets.\nAcceptable Birth Control Options\n- Do not have unprotected sex. Talk to your doctor or pharmacist right away if you have unprotected sex or if you think your birth control has failed. Your doctor may tell you to use emergency birth control.\n- Tell your doctor right away if you miss a menstrual period or think you may be pregnant for any reason.\nIf you are the parent or caregiver of a female child who started taking ambrisentan tablets before reaching puberty, you should check your child regularly to see if she is developing signs of puberty. Tell your doctor right away if you notice that she is developing breast buds or pubic hair. Your doctor should decide if your child has reached puberty. Your child may reach puberty before having her first menstrual period.\nFemales can only receive ambrisentan tablets through a restricted program called the Ambrisentan Risk Evaluation and Mitigation Strategy (REMS) program. If you are a female who can get pregnant, you must talk to your doctor, understand the benefits and risks of ambrisentan tablets, and agree to all of the instructions in the Ambrisentan REMS program.\nMales can receive ambrisentan tablets without taking part in the Ambrisentan REMS program.\nWhat are Ambrisentan Tablets?\n- Ambrisentan tablets are a prescription medicine used to treat pulmonary arterial hypertension (PAH), which is high blood pressure in the arteries of your lungs.\n- Ambrisentan tablets can improve your ability to exercise and it can help slow down the worsening of your physical condition and symptoms.\n- It is not known if ambrisentan tablets are safe and effective in children.\nWho should not take ambrisentan tablets?\nDo not take ambrisentan tablets if:\n- you are pregnant, plan to become pregnant, or become pregnant during treatment with ambrisentan tablets. Ambrisentan tablets can cause serious birth defects. (See “What is the most important information I should know about ambrisentan tablets?”)\nSerious birth defects from ambrisentan tablets happen early in pregnancy.\n- you have a condition called Idiopathic Pulmonary Fibrosis (IPF)\nWhat should I tell my doctor before taking ambrisentan tablets?\nBefore you take ambrisentan tablets, tell your doctor if you:\n- have been told that you have a low red blood cell level (anemia)\n- have liver problems\n- have any other medical conditions\nTell your doctor about all the medicines you take, including prescription and over-the-counter medicines, vitamins, and herbal supplements. Ambrisentan tablets and other medicines may affect each other, causing side effects. Do not start any new medicines until you check with your doctor.\nEspecially tell your doctor if you take the medicine cyclosporine (Gengraf, Neoral, Sandimmune). Your doctor may need to change your dose of ambrisentan tablets.\nHow should I take Ambrisentan Tablets?\n- Ambrisentan tablets will be mailed to you by a certified pharmacy. Your doctor will give you complete details.\n- Take ambrisentan tablets exactly as your doctor tells you to take it. Do not stop taking ambrisentan tablets unless your doctor tells you to stop.\n- You can take ambrisentan tablets with or without food.\n- Do not split, crush or chew ambrisentan tablets.\n- It will be easier to remember to take ambrisentan tablets if you take it at the same time each day.\n- If you take more than your regular dose of ambrisentan tablets, call your doctor right away.\n- If you miss a dose, take it as soon as you remember that day. Take your next dose at the regular time. Do not take two doses at the same time to make up for a missed dose.\nWhat should I avoid while taking Ambrisentan Tablets?\n- Do not get pregnant while taking ambrisentan tablets. (See the serious birth defects section of the Medication Guide above called “What is the most important information I should know about ambrisentan tablets?”) If you miss a menstrual period, or think you might be pregnant, call your doctor right away./li>\n- It is not known if ambrisentan passes into your breast milk. You should not breastfeed if you are taking ambrisentan tablets. Talk to your doctor about the best way to feed your baby if you take ambrisentan tablets.\nWhat are the possible side effects of ambrisentan tablets?\nAmbrisentan tablets can cause serious side effects including:\n- See “What is the most important information I should know about ambrisentan tablets?”\n- Swelling all over the body (fluid retention) can happen within weeks after starting ambrisentan tablets. Tell your doctor right away if you have any unusual weight gain, tiredness, or trouble breathing while taking ambrisentan tablets. These may be symptoms of a serious health problem. You may need to be treated with medicine or need to go to the hospital.\n- Decreased sperm count. Decreased sperm counts have happened in some men taking a medicine that is like ambrisentan tablets. A decreased sperm count may affect the ability to father a child. Tell your doctor if being able to have children is important to you.\n- Low red blood cell levels (anemia) can happen during the first weeks after starting ambrisentan tablets. If this happens, you may need a blood transfusion. Your doctor will do blood tests to check your red blood cells before starting ambrisentan tablets. Your doctor may also do these tests during treatment with ambrisentan tablets.\nThe most common side effects of ambrisentan tablets include:\n- swelling of hands, legs, ankles and feet (peripheral edema)\n- stuffy nose (nasal congestion)\n- inflamed nasal passages (sinusitis)\n- hot flashes or getting red in the face (flushing)\nSome medicines that are like ambrisentan tablets can cause liver problems. Tell your doctor if you get any of these symptoms of a liver problem while taking ambrisentan tablets:\nTell your doctor if you have any side effect that bothers you or that does not go away. These are not all of the possible side effects of ambrisentan tablets. For more information, ask your doctor or pharmacist.\nCall your doctor for medical advice about side effects. You may report side effects to FDA at 1-800-FDA-1088.\nHow should I store ambrisentan tablets?\nStore ambrisentan tablets at room temperature between 68°F to 77°F (20°C to 25°C), in the package it comes in.\nKeep ambrisentan tablets and all medicines out of the reach of children.\nGeneral information about the safe and effective use of ambrisentan tablets\nMedicines are sometimes prescribed for purposes other than those listed in a Medication Guide. Do not use ambrisentan tablets for a condition for which it was not prescribed. Do not give ambrisentan tablets to other people, even if they have the same symptoms that you have. It may harm them.\nThis Medication Guide summarizes the most important information about ambrisentan tablets. If you would like more information, ask your doctor. You can ask your doctor or pharmacist for information about ambrisentan tablets that is written for health professionals.\nWhat are the ingredients in ambrisentan tablets?\nActive ingredient: ambrisentan\nInactive Ingredients: croscarmellose sodium, lactose monohydrate, lecithin, magnesium stearate, microcrystalline cellulose, partially hydrolyzed polyvinyl alcohol, polyethylene glycol, povidone, talc and titanium dioxide. Additionally, 5 mg tablet contains: FD&C red#40 aluminum lake.']	['<urn:uuid:9e8cfd27-d7eb-46f8-ab5c-b38f4d335c4b>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T23:00:04.658849	25	36	1406
68	what temperature good for elephant skin and candy cane corals	Both corals thrive in similar temperature ranges. Elephant Skin Coral requires 74° - 83° F (23° - 28° C), while Candy Cane Coral needs 75° - 82° F (24° - 28° C).	"['Elephant Skin Coral\nCorduroy Coral, Castle Coral, Groove CoralPachyseris rugosaPhoto © Animal-World: Courtesy David Brough\nThe Elephant Skin Coral is a curious looking stony coral, with an attractive ruffled surface!\nThe Elephant Skin Coral Pachyseris rugosa gets its name from the ""wrinkled"" look of its surface that is similar to an elephant\'s skin. The coral has an uneven skeletal surface that looks like it is almost ruffled with deep grooves and valleys that twist and turn. They have a very thin tissue that covers the skeleton, which can be bluish-gray to brown. Under bright lighting they can be a fluorescent greenish brown. It is also known as Corduroy Coral, Castle Coral, and Groove Coral.\nIn the wild, the Elephant Skin Coral lives in large colonies that form mounds over 26 feet (8 m) across in shallow areas with turbid water. They grow in bifacial plates that are generally upright, contorted, and irregular. These corals are unique, in that they have no discernible polyps. They use the nutrients in the water as well as the zooxanthellae in their bodies to nourish themselves, and can also use a mucous net to capture small particles.\nThe Pachyseris genus are for advanced aquarists due to their stringent requirements. They need a lot of nutrients, yet at the same time need pristine water conditions. They appreciate high light and a strong water movement. They are slow growing and If not kept in pristine conditions, will quickly deteriorate. They are susceptible to white-band disease and other afflictions, and don\'t tolerate the typical treatments used for other stony corals. Captive reared specimens may be slightly easier to care for.\nThe Elephant Skin Coral has been reproduced in captivity, though in a limited capacity. Propagation has been slow. There has been limited success because this coral heals slowly and stresses easily.\nDistribution / Background Pachyseris Coral Information: The Elephant Skin Coral Pachyseris rugosa was described by Lamarck in 1801. Some common names they are known for are Elephant Skin Coral, Corduroy Coral, Castle Coral, and Groove Coral. They have reproduced in captivity, and limited captive propagation has been practiced. Currently the Pachyseris genus and the Pavona genus are the only 2 genera regularly offered to aquarists from this family, Agariciidae.\nWhere Pachyseris Corals Are Found: The Pachyseris rugosa are found from the Red Sea to the Marshal Islands, Samoa, and Micronesia, as well as the Great Barrier Reef to the Houtman Abrolhos Islands of Australia.\nPachyseris Coral Habitat: P. rugosa are found in deeper waters, as well as shallow areas with turbid water. They inhabit reef slopes, surge channels, and shallow lagoons. They live in large colonies that form mounds over 26 feet (8 m) across in the shallower areas, as well as smaller colonies throughout their range.They are found at depths down to 131 feet (40 m). These corals use the nutrients in the water as well as the zooxanthellae in their bodies to nourish themselves, and can also use a mucous net to capture small particles.\nDescription What do Pachyseris Corals look like: The Elephant Skin Coral Pachyseris rugosa growth form is in bifacial plates that are generally upright, contorted, and irregular. It has an uneven skeletal surface that looks like it is almost ruffled with deep grooves and valleys that twist and turn. They have a very thin tissue that covers the skeleton, which can be bluish-gray to brown. In bright lighting, they can be a fluorescent greenish brown. It is unknown how long they live.\nDifficulty of Care Pachyseris Coral Care: The P. rugosa is a very difficult coral to care for. This is a coral for advanced aquarists. They need a lot of nutrients, yet at the same time need pristine water conditions. They need high light and a strong water movement. They are slow growing and If not kept in pristine conditions, will quickly deteriorate. Captive reared may be slightly easier to care for.\nFoods / Feeding Pachyseris Coral Feeding: In the wild, small polyp stony (SPS) corals have developed several feeding strategies. Through a symbiotic relationship with a marine algae, known as zooxanthellae, they receive the majority of their nutrients. They also capture planktonic organisms, food particles from the water column, and can absorb dissolved organic matter.\nIn captivity, the Pachyseris corals primarily use their zooxanthellae for nutrition, and can also use mucosal capture techniques. Direct feeding will not benefit this coral, but proper water flow will help it to feed.\nAquarium Care Pristine tank conditions are typically needed to keep all SPS corals. Keep the nitrate levels low, and maintaining calcium and alkalinity levels. Typically you can do water changes of 20% to 30% a month, 15% every 2 weeks, or 5% a week for SPS corals. The 5% a week also seems to really make a big difference in the corals health.\nThe following water supplements are suggested for Pachyseris species:\n- Calcium: 385 to 425 ppm. If a small poly stony (SPS) coral does not have enough calcium, it will not grow.\n- Alkalinity: 3.2 TO 4.8 MEQ/L (8 to 12 dKh)\n- Phosphates: 0, zero. Phosphates are the worst of all and all corals hate them.\n- Magnesium: 1350-1500. Magnesium makes calcium available, so if your calcium is low, check your magnesium levels before adding any more calcium.\n- Strontium: Strontium (10 for most SPS Corals), and trace Elements are also suggested.\nA well-feed live rock/reef environment is what is needed for your Elephant Skin Coral, along with some fish for organic matter production. These corals are slow-growing and can succumb to disease, a mature tank is recommended.\n|Quick Reference Chart|\nWhen positioning your Pachyseris, keep in mind that lighting rather than food is more important to them. Strong water movement is also needed for a healthy Pachyseris. Make sure that no other corals can come in contact with your specimen. Pachyseris spp. are generally peaceful but will loose any chemical warfare with other species.\n- Minimum Tank Size / Length: 50 to 100 gallons (190 - 380 L) or larger\n- Marine Lighting: High, but not direct metal halide\n- Temperature: 74° - 83° F (23° - 28° C)\n- Salinity / Specific Gravity: 1.023 - 1.025\n- Water Movement: Strong\n- Water Region: Middle of the aquarium\nCompatibility and Social Behaviors The Pachyseris genus are generally peaceful since they do not have tentacles that extend. It does well in a typical small polyp stony (SPS) coral tank, but will suffer in crowded tanks. It should be kept well away from other corals. They are fine with fish that are considered reef safe.\nBreeding and Reproduction The small polyp stony (SPS) corals are male and female and can reproduce both sexually and asexually. In the wild they reproduce sexually by releasing eggs and sperm at the same time, resulting in a fertilized egg which then forms into a free-swimming planula larva. Eventually the planula larvae settles onto the substrate, becoming plankters. This then forms a tiny polyp which begins to excrete calcium carbonate and develops into a coral. Planula larvae are extremely vulnerable to predation, and very few survive. The Pachyseris genus reproduce asexually as well. In the wild Pachyseris spread from breakage due to storms and fragmentation. The P. rugosa has been known to use asexual fragmentation, yet this takes many weeks.\nPropagation for the P. rugosa has been slow and with limited success, since this coral heals slowly and stresses easily. It can easily be cut off of a mother colony that is healthy and has had a chance to grow itself out for many months. Keeping the water in the tank very clean and pristine will help this coral heal if fragging is in its future. Bone cutting implements or scoring can be used to get fragments. Allow the frags to heal before moving them to a new tank. Give the frags ample water flow.\nPotential Problems The Pachyseris spp. tend to be the first to bleach out in the ocean, indicating their sensitivity to water conditions. They tend to get white-band disease and treating the water is not suggested, yet putting the antibiotic directly on the coral is the best choice. If not kept in pristine conditions, they will quickly deteriorate.\nThe Elephant Skin Corals do grow very slow, and do not have polyps that can warn you of illness. The only way to tell if it is healthy is if there is no tissue degeneration. If your P. rugosa has any kind of tissue recession, you may be able to get ahead of it by cutting off some of the healthy tissue just beyond the deteriorating tissue This will ensure if any deterioration is starting on visibly healthy tissue, it will not spread. If you have problems with cyanobacteria or algae, keeping strong water movement will prevent them from attaching to your Pachyseris.\nAvailability Pachyseris Corals for Sale: The Elephant Skin Coral P. rugosa is moderately hard to find at pet shops, but can be found online at times. Online they can run about $49.00 USD or more depending on size and/or color.\n- Animal-World References: Marine and Reef\n- Eric Borneman, Aquarium Corals : Selection, Husbandry, and Natural History , TFH Publications, 2001\n- Anthony Calfo, Book of Coral Propagation, Volume 1 Edition 2: Reef Gardening for Aquarists, Reading Trees; 2nd edition, 2007\n- Bob Goemans, Elephant Skin/Corduroy/Castle Coral, Pachyseris rugosa, Animal Library, Saltwatercorner.com', 'Caulastrea furcata, often known as candy cane, trumpet, or bullseye coral, is a bright and colorful LPS (large polyp stony) coral. Characterized by striped polyps inspiring its common name, it can be found in vivid yellow, green, red, or blueish brown. Each branching polyps contains a neon green mouth. Some common varieties available to purchase are Kryptonite, Orange, Green, and Alien Eye Candy Cane Coral.\nCandy Canes originate from the Indo-Pacific Ocean from Fiji to Australia and the surrounding waters. In the wild, they can usually be found in shallow waters, tide pools and even lagoons. Though they prefer moderate water flow and lighting, they can flourish in a variety of conditions making them perfect for a beginner hobbyist.\nIn this article, we will talk you through everything you need to know to care for your Candy Canes and keep your saltwater aquarium looking vibrant and healthy.\nCandy Cane Coral Care\nCandy Cane Coral is extremely easy to keep as it is a hardy coral, meaning it’s resistant to variations in lighting, flow, and other minor changes in its environment. This makes it forgiving to mistakes commonly made by novices and a great choice whether you are experienced or just starting out.\nAre Candy Cane Coral Aggressive?\nCandy Cane Coral is considered not very aggressive since it has relatively short (2 inch) sweeping tentacles, unlike many other LPS corals. However, it could still sting corals which are close by, so it is important it has plenty of room to grow. As it is one of the fastest growing corals you should aim to give it around 6 inches of space at a minimum.\nCandy Cane Coral Placement\nThe best placement for Candy Cane Coral is in an area of moderate lighting and flow, although it can also thrive in low flow or light intensity. For this reason, it is ideally placed at the bottom of your tank and no higher than the middle. You can place it directly on the sand bed as sand is its preferred substrate.\nIf the flow is too high, it can damage the fleshiness of the polyps; too low and the coral will need more help with feeding, although this isn’t an issue if you plan to feed it regularly yourself. To ensure it takes well to your desired placement it’s recommended to start with a darker and lower flow part of your tank and gradually move it over the course of a few weeks.\nCandy Cane Coral Lighting Requirement\nCandy Cane Coral does not have a high lighting requirement, responding best to low-to-moderate intensity. Anything from 30 to 150 PAR is acceptable, though between 50-70 is recommended. Avoid more than 150 though as light intensity that’s too high could bleach or irritate the coral.\nPlacing your Candy Cane Coral towards the edges of your tank can help to soften the intensity it’s exposed to. Alternatively, a shaded area can be a good spot to provide the right conditions too.\nCandy Cane Coral Temperature\nThe temperature preferred by Candy Cane Coral is between 75°C-82°C/24°C-28°C. Higher temperatures cause thermal stress, resulting in much of the zooxanthellae algae being expelled and depriving the coral of the nutrients they provide.\nCandy Cane Coral pH\nA pH range of 8.1-8.4 is ideal for Candy Cane Coral. The calcium levels should also be kept between 350-420ppm. As it is an LPS coral, these parameters are essential to prevent erosion of the coral’s hard skeleton and to keep it strong and healthy.\nCandy Cane Coral Growth Rate\nCandy Cane Coral has a fast growth rate, especially compared to other corals. This is particularly true when it is fed on a regular basis. The exact growth speed will depend on the available nutrients, light and space, including nearby competition and rocks which could limit the area it spreads.\nCandy Canes grow by dividing a polyp into two identical polyps. This is a big reason why their growth is so rapid if their environment allows it, as their polyps can quickly multiply, and the process can occur with multiple polyps simultaneously.\nCandy Cane Coral Growth Height\nThe growth height of Candy Cane Coral can depend on a number of factors from the amount of food to the light intensity it receives, just like its growth rate. In the average aquarium you can probably expect it to grow up to several inches tall. However, due to their variability it’s difficult to predict exactly how high Candy Canes will grow.\nWhat Do Candy Cane Coral Feed On?\nIn general, Candy Cane Coral is happy to feed on a variety. Bitesize meaty food is most suitable, such as prawns, mysis shrimp and small pieces of krill. It will also accept LPS pellets and other coral foods, or even pellets and flakes intended for fish.\nWhen feeding your Candy Canes, you’ll get the best results if you use some form of feeding apparatus, or even a regular turkey baster. Try to place the food directly into the mouths to make it easiest for the sweeping tentacles to reach. This also helps to prevent your fish from stealing it – they are sometimes known to harass the coral by biting it to access the food inside if given the opportunity.\nIt’s useful to remember that Candy Canes are nocturnal, so the tentacles will naturally come out at night, or when the lights are off. While they extend during the day if they detect food nearby, strategic timing can speed up the process. You should aim to spot feed them 2-3 times a week.\nIn some cases, you may not need to directly feed your Candy Cane Coral at all. Providing that the flow isn’t too low, and you feed the rest of your tank heavily enough, it may get enough nutrients on its own. It will also benefit from its symbiotic relationship with the zooxanthellae algae living in its surface tissues photosynthesizing and giving another source of nutrients. However, spot feeding Candy Canes can help them to grow faster and appear brighter in color, so it’s up to you which approach will achieve your desired effect.\nHow to Split Candy Cane Coral\nIn order to split Candy Cane Coral, look for an outer branch for easy access and ensure you cut as far away from the polyps as you can to avoid damaging them. Glue the piece to a frag plug or rock to allow it to heal.\nBone cutters or a band saw work well as a cutting tool – the latter has the advantage of creating a flat base which is helpful when gluing. After a couple of weeks of healing the frags should be ready to be traded or even sold.\nCandy Cane Coral Dying\nIf your Candy Cane Coral is dying you will notice a change in appearance – in particular, the polyps not opening or puffing up is a tell-tale sign that something is wrong. In some cases, the flesh may recede, causing the skeleton to show. You may also see changes in color.\nDon’t panic if you notice these symptoms however, as with careful monitoring you should be able to get your Candy Canes back to being happy and healthy. The most common cause of these changes is a chemical composition in the water that is harmful to Caulastrea. To diagnose the problem, start by measuring the levels of calcium, magnesium, nitrates, and water hardness. The ideal ranges for these variables are as follows:\n- Calcium: 350-420ppm\n- Magnesium: ~1350ppm\n- Nitrates: <10ppm\n- Water hardness: 8°-12° dH\nIf your water falls within these recommendations, there could be an issue with the temperature or pH. As previously mentioned, Candy Canes require a temperature of between 75°C-82°C/24°C-28°C and alkalinity ranging from 8.1-8.4 pH. Check the light intensity too as anything higher than 150 PAR can cause the polyps to retreat. If the water flow is too high, it can also damage the coral.\nIt’s also possible that the coral is simply adjusting to recent changes in conditions. If you have moved it to a different spot in the tank, replaced your lighting or made other alterations, it can affect the appearance. If this is the case, your Caulastrea should recover quickly on its own.\nSometimes the cause is other livestock in your tank. Try observing your Candy Canes to spot fish or other coral bullying them, especially around feeding time. If this is the case you may be underfeeding your fish, or there may not be sufficient space between your corals. Keep in mind though that at times this behavior from fish is unavoidable and you might need to manage the problem instead of attempting to prevent it entirely.\nFinally, if none of the above methods prove successful, you may need to resort to fragging a healthy segment and starting over.\nThat concludes our guide to Candy Cane Coral! If you found it helpful, be sure to check out our other articles to help make your aquarium the best it can be.']"	['<urn:uuid:0198492d-d614-4fd2-8d13-5ee7b2225dc8>', '<urn:uuid:8f5fe40f-154b-4e39-840e-1e66b49280c3>']	factoid	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-12T23:00:04.658849	10	32	3063
69	What is the recommended operating temperature range for Italian motorcycles like Ducati, and how does it compare to Japanese bikes?	Italian motorcycles like Ducatis typically operate at a temperature range of around 90 to 105 degrees Celsius. This is notably higher than Japanese manufacturers' bikes, which usually operate at lower temperatures between 70 to 80 degrees Celsius.	['TechTOC is a new weekly technical column moderated by “Foreman” Oh Kah Beng (KB).\nKB is now the Consultant for the TOC Automotive College’s motorcycle racing venture.\nSend in your technical questions, as the winner receives a complimentary session at KB’s Most Fun Gym MX-school!\nMalaysian GP legend and now motocross instructor, “Foreman” Oh Kah Beng is no stranger to most of us. He was the very first Malaysian to have ridden in international GP events on 500cc 2-stroke machines in the 80’s, before moving over to the automotive industry where he was the head of Lotus Cars Malaysia and Driving Instructor for the Lotus community’s Performance Driving Program.\nGrowing up at his father’s motorcycle shop, he had learned about wrenching on a motorcycle from an early age. In fact, all his brothers are well-known technicians, too, including Sunny Oh and Randy Oh.\nLately, he is the operating his own motocross and off-roading as the Principal Coach, to teach riders the finer points of motorcycle control at his facility, Most Fun Gym (MFG). His students start from as young as four-years-old.\nAs TOC Automotive College makes a foray into two-wheeled education and racing, in addition to auto racing, it was only right that KB (as he likes to be called) plays the role of consultant to the racing project.\nPlease click on this link to visit TOC Automotive College to know more about their new TOC Superbike Technician Course. https://www.facebook.com/TOC.edu/\nHere, we kick off the first installment of TechTOC, where readers such as you can send in technical questions regarding your motorcycle and KB will answer them. The best question earns a complementary riding session at Most Fun Gym! So get typing!\nI own a Harley-Davidson and a Ducati 1299 Panigale. What would be the ideal operating temperature for these bikes? I also own a scooter for my daily runabout, should the same temperatures apply to it as well?\n– Muhammad “Don Ducati” Anis, Kuala Lumpur\nMotorcycle engines need to operate at a certain temperature range for the optimal thermodynamic efficiency. At this optimal temperature, the fuel-air mixture is in a state which is easier to burn efficiently. This efficiency translates to better power production, fuel efficiency and lower exhaust emission.\nAs an example, you may have noticed that familiar exhaust smell from a cold engine as it heats up. That is because the engine has not reached it working temperature.\nNow, over to your questions.\nThe operating temperature range of a motorcycle depends on what type of motorcycle, which brand, whether it is Japanese or “continental” and if it is liquid- or air/oil-cooled, so please note that this is a general statement.\nAn air-cooled Harley-Davidson engine’s operating temperature correlates to its oil temperature. From what I understand, that is around 90o to 120o C on a 32o C day. It is okay as long as it stays below 150o C. The only way to tell is if you install an oil temperature gauge, sold by Harley as an accessory. (It’s a dispstick-like device which fits in the place of the stock engine oil filler cap.)\nAs for the Ducati 1299 Panigale, the operating range for Ducatis, including my Monster is around 90o to 105o C. Ducatis, Cagivas and most Italian bikes have traditionally run temperatures up in this range. Our TOC BMW S 1000 RR racebikes could hit close to 100o C in the heat (pun intended) of competition.\nWhat scooter do you own? May I safely assume that it is from a Japanese manufacturer? The Japanese manufacturers adhere to lower engine operating temperatures, usually between 70o to 80o C, conversely.']	['<urn:uuid:09d01c0f-0e3f-4334-9fca-dd57a16f3837>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T23:00:04.658849	20	37	602
70	As a cardiologist, I often monitor patients with high blood pressure in their lungs - could you explain the basic mechanism of how pulmonary hypertension affects the cardiovascular system?	Pulmonary Hypertension damages the pulmonary arteries, making them narrow and thick. This compromises normal blood flow, forcing the heart to work harder to pump blood. This stress makes the heart weak and enlarged, which can lead to right heart failure and death.	['Pulmonary Hypertension (PH) is a rare but severe condition that causes high blood pressure in the lungs by damaging the pulmonary arteries. The vessels transport blood from the heart to the lungs, but because of the disease they become narrow and thick. When the normal blood flow becomes compromised, the heart is forced to work under stress to pump blood, which makes the heart weak and enlarged. There are numerous risks associated with this condition, including right heart failure and death.\nWhen the cause of a specific case of pulmonary hypertension is unknown, the condition is known as idiopathic. When the disease is caused by a primary medical condition, it is called associated pulmonary hypertension. Common primary diseases include lung and heart diseases but inflammatory bowel disease ulcerative colitis (UC) which affects the digestive track, can also be a cause.\nDevelopment Of Pulmonary Hypertension Due To Ulcerative Colitis\nInflammatory bowel disease (IBD), ulcerative colitis included, causes irritation in the digestive tract which leads to diarrhea often with blood or pus, abdominal pain and cramping, rectal pain or bleeding, difficulties in normal defecation, weight loss, fatigue and fever. In addition, the disease can bring about a variety of respiratory complications, as explored in the study “Pulmonary complications of inflammatory bowel disease,” authored by Drs. Steven E. Weinberger and Mark A. Peppercorn.\nAccording to the report: “Case series vary in terms of the proportions of patients with ulcerative colitis and Crohn’s disease who have associated lung disease… . The pathogenesis of pulmonary parenchymal disease and serositis associated with IBD is unknown. However, the more common airway inflammatory changes are thought to represent the same type of inflammatory changes that occur in the bowel.”\nThe correlation between pulmonary hypertension and ulcerative colitis is not fully disclosed but case studies indicate many instances of IBD- associated lung involvement.\nDiagnosis and Treatment Of Pulmonary Hypertension And Ulcerative Colitis\nSince patients who suffer from ulcerative colitis are at risk of lung diseases and pulmonary hypertension, patients and physician should be aware of symptoms like shortness of breath (dyspnea), fatigue, dizziness or fainting spells (syncope), chest pressure or pain, swelling (edema) in the ankles, legs and abdomen (ascites), bluish color in lips and skin (cyanosis), or irregular heartbeat.\nThere is currently no cure for pulmonary hypertension, but there are treatments that can help patients ease symptoms and extend their lifespan. Blood vessel dilators (vasodilators), endothelin receptor antagonists, sildenafil and tadalafil, high-dose calcium channel blockers, anticoagulants, diuretics, and supplemental oxygen are the most used therapeutic options for patients with pulmonary hypertension. However, a tailored treatment taking into consideration the coexistence of the two conditions, the patient’s characteristics, and other medical information should be discussed with a specialized physician.\nNote: Pulmonary Hypertension News is strictly a news and information website about the disease. It does not provide medical advice, diagnosis or treatment. This content is not intended to be a substitute for professional medical advice, diagnosis, or treatment. Always seek the advice of your physician or other qualified health provider with any questions you may have regarding a medical condition. Never disregard professional medical advice or delay in seeking it because of something you have read on this website.']	['<urn:uuid:c70fc772-d1ba-41a9-8fc2-8003315336ef>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T23:00:04.658849	29	42	530
71	Where were the warships for Caesar's fleet constructed?	Twelve warships were built at Arelate (modern Arles).	"[""|Full Index||Subjects||Concepts||Country||Documents||Pictures & Maps|\nThe naval battles of Massilia (49 BC) were two victories won by Caesar's naval commander Decimus Brutus during the siege of the same town.\nMassilia (modern Marseille) had chosen to side with Pompey and the Senate (Great Roman Civil War), accepting Lucius Domitius Ahenobarbus as governor of Transalpine Gaul and commander of the city during the siege. This was a major problem for Caesar, who had decided to move against Pompey's supporters in Spain. He laid siege to Massilia with three legions, before leaving Caius Trebonius in command and heading to Spain. Twelve warships were built at Arelate (or Arelas, modern Arles) and placed under the command of Decimus Brutus.\nThe Massiliotes also possessed a fleet, reported by Caesar as consisting of eleven decked ships of war, six un-decked ships of war and a number of smaller ships. These ships were manned by local sailors, and large numbers of archers and warriors from the nearby Albiei tribe.\nThe Massiliote fleet came out to attack Brutus's smaller squadron, which was based at an island opposite the city. The two fleets fought in different ways. The Massiliotes relied on the speed and agility of their ships, hoping to surround Caesar's fleet and carry out massed attacks on individual ships.\nBrutus relied on the skill of his legionaries and hoped to board the enemy ships. These tactics paid off. Caesar's men were happy to be attacked by two enemy ships at once, grappling with them both and sending boarding parties onto both. The Massiliotes were forced to retreat back into harbour, having lost nine ships, some sunk and some captured. Caesar doesn't say what type of ships these nine were.\nThe second battle took place after Cneius Pompey sent a fleet of sixteen ships under Lucius Nasidius to aid the defenders. Once he reached the vicinity of Massilia he sent messengers into the city to ask the Massiliotes to risk another battle. They restored their fleet to its original size by repairing older ships found in the port, then sailed to Taurois, where they met up with Nasidius. The Massiliotes made up the right division of the combined fleet, Nasidius the left division.\nBrutus also had a larger fleet for this second battle, having added six of the captured ships to his original twelve. Confident after his earlier victory he led his fleet towards Taurois.\nThis time the Massiliotes appear to have held their own for some time, even coming close to trapping Brutus between two of their three-decked galleys. Brutus managed to escape from this trap, and the two enemy ships were damaged in a collision caused by their eagerness to get into the fight. Some of Brutus's nearby ships spotted their discomfort and both were sunk.\nThe turning point appears to have come when Nasidius fled from the scene of the battle, taking one of the Massiliote ships with him. Of the Massiliote ships one managed to return to the city, five were sunk, four captured and the rest escaped to Spain.\nThese two naval victories allowed Trebonius to continue with the siege, which only ended in victory after Caesar's return from Spain.\n||Save this on Delicious|\nHelp - F.A.Q. - Contact Us - Search - Recent - About Us - Subscribe in a reader\n|Subscribe to History of War|\n|Browse Archives at groups.google.co.uk|""]"	['<urn:uuid:460bc3ba-cc57-4fa2-8b4a-ec6b7ba921ed>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T23:00:04.658849	8	8	557
72	When did British forces cross the Rhine?	British forces crossed the Rhine on March 24, 1945.	['In this moving docu-drama, some 70 years after British forces crossed the Rhine (March 24, 1945) Bremen historian Diethelm Knauf and the History and Culture Centre in Bremen’s harbour district Walle re-visit peoples‘ reactions when they came face to face with the „enemy“. Achim Saur scoured state archives and Knauf researched British and US archives, even travelling to the UK to video-record interviews with members of the Scottish 51st Highland Division – one of three which captured the city.\nIt was a time of change, insecurity and fear of the future. One woman wrote in her diary: „What the future will bring nobody knows, rumors are rampant, you cannot grasp what is happening, you only feel a deep pain in your breast.“\nThe Scottish 52nd Division advance on Bremen\nThe key question guiding the historians’ research was, what actually was the first impression people had of the „enemy“, when they first encountered them. Knauf explains: „We were not interested in repeating the stories of chewing gum, nylons and reconciliation. We believe that these stories were reconstructed later to serve political purposes of the late 1940s. In April/May 1945 there must have been animosity, hatred, fear“.\nThe result of this historical research is a unique snapshot of the times: British soldiers entering the city; searching for hidden weapons or German soldiers and the effect on private, family lives. Knauf’s impressions from interviewing British veterans was that British units had „lived through Hell“ in the fighting between Gennep and Goch, suffering continual artillery and mortar bombardment, sometimes with traumatised units needing to be pulled out of combat. Having surrounded Bremen, however, he says that the actual capture went relatively easily. Peter White, who as a lieutenant in the 52nd Lowland Division also lived through harrowing episodes (and kept a secret diary) recalls his memoir: „With the Jocks“ – and the battle for Uphusen.\nHow British soldiers perceived the Germans depended on their experiences. It made a difference if your family had been a victim of German air-raids on British cities, whether you witnessed the cruelties of the Waffen SS, whether you took part in fierce fighting and realised that the Germans were intent on killing you.\nInterviewed on video, Bill Robertson, 51st Highland Division, says as an individual, he was keen to join the army to become part of „the movement towards freedom, liberation and democracy“. Later, after his experiences in France, Belgium and Holland, „sometimes it became more personal“ for example when a comrade had been killed or wounded and „altruistic feelings faded into the background“.\nAmerican Tony Vaccaro, later to become a famous photographer, hated the Germans because they had forced him to kill. To aim and fire became a traumatic experience for him, he said in an interview with Diethelm Knauf in 2001.\nTo document this terror with his camera was like a cure. But later in the 1940s when he travelled through Germany with his camera he, nevertheless, felt pity for the people living in the devastated cities. One shot of Bremen reveals his concern and sympathy. Focusing on the footsteps in the snow and the smoke coming out of the chimney of a stove he records the deplorable conditions many Bremener’s lived under and tried to survive the cold winter. His image preserves the memory of their misery. This was not the attitude of an arrogant conqueror.\nSuch emphasis was certainly not the rule and encountering the Waffen SS ’s brutal warfare changed a soldier.\nJohn MacKenzie, 51st Highland Division, says in the video, after the capture of Bremen what triggered his greatest shock was the liberation of the Bergen Belsen concentration camp. They were not prepared for this „hell.“ Yet, referring to it, he would always differentiate between the Germans and the Nazis. The strange thing was, however, all Germans he encountered claimed „Ich nix Nazi“.\nLabels like „chewing gum and nylons“ in the reconciliation period after the war, including the Marshall Plan, are not what British and American veterans remember – rather it was a lack of national hostility. The „Guide for British Soldiers“ (1944) set out appropriate attitudes to be shown to Germans after victory; democracy, self-confidence, strictness and ‚fair and square dealing‘ with the defeated nation.\nHow did it look from the Bremeners viewpoint? The State Archives holds a collection of diaries – unfiltered, personal perceptions of the British and later American soldiers and the occupation policy. In addition, there are texts written after a call from Mayor Wedemeier 50 years after the war ended for the „Weser Kurier“ newspaper. This collage tries to reproduce the mood of these writings.\n„You cannot grasp what is happening… “ Bremeners recollections >>> translation\nFear of the enemy\nBremeners had spent the final days of the war almost continuously in underground shelters. Until April 1945, air raids were followed by British artillery bombarding German positions and the surrounding neighbourhood. Not everyone ventured out during lulls in the firing though later people longed to do so because of the unbearable heat in their shelters – one diarist recording temperatures of more than 40 degrees and lack of oxygen.\nMany Bremeners only saw the „enemy“ when they left the shelters. British soldiers ordered people onto the streets to search for hidden soldiers and weapons. The impact of Nazi propaganda on the people can be seen in a testimony written in 1995 for the „Weser Kurier“, recording how, after soldiers had checked the shelter they gave out chocolate and chewing gum to children, but once inside the shelter again, the sweets were handed over to adults „because it was believed they were poisoned“.\n„Because of his school English“ the late Senator Günther Czichon was delegated to open the shelter door to the soldiers. A black soldier pushed a machine gun against his stomach and asked if German soldiers were inside. „I said no and invited him in. My American friend had probably already heard about young German ‚werewolves’” says the account, adding “that he went off cursing, and I was left with the feeling he had been only slightly less frightened than I was“.\nAmong all the testimonies we find only one where relief at the end of the war turns into a real celebration. Probably the writer comes from one of the forced labourers‘ quarters, where the terror of the regime was felt most directly. „We saw the invasion of the troops as liberation and danced, laughed and cried with the whole neighbourhood from the bunker. The war was over for us. Nothing counted more than liberation from terror and war. Whatever might happen now could only be better“.\nForced laborers welcomed the British troops, © Imperial War Museum, London\nAfter the fear – everyday encounters under occupation\nAn American training film for occupation soldiers, also used in the British Army, warned: „You will see beautiful countryside. Make no mistake. You are in enemy territory. Be constantly vigilant and suspicious“.\nThe directive warned against fraternisation. German people were not friends. The film „Your Job in Germany“ explained to the GI’s – in a rather crude way – the gritty story of German militarism and a ubiquitous Nazi underground. In the „British Zone Review“ a lively discussion developed as to whether the inhabitants of the devastated cities might not be approached with a „modest measure of compassion“.\nSo, in Bremen’s memories there were always stories that the „enemy“ would behave quite differently than expected. There is surprised talk of „friendly faces“ among occupation soldiers who saved a youthful Wehrmacht deserter before his mandatory detention, and in confiscated houses there is co-operation; British help when moving: sharing kitchens. And when new orders are received to move on, one Bremen family leaves a captured deer and gives advice on how to kill a pig in the barn – despite the ban. In this way, the caricature of the enemy dissolves and the witness recalls, „From this moment on they were people like us“.\nOn the other hand, combat troops searching for weapons and hidden soldiers in the Bremen houses were not always considerate and it could not be expected of them that they would deal with commandeered houses carefully. Though army regulations were very strict, they could not prevent personal enrichment, specially popular were watches, cameras and other valuables. Bremeners remember „rude fellows“ and one trader from the Steintor area, writes in her diary: „I have to say, fortunately, Gerhard and Rudolf once told me that German soldiers have done such things, too.“\nEven three days after the capture of Bremen, one senior Nazi, director of the psychiatric hospital in Ellen, still hoped for a reversal in the war. To him British soldiers displayed an outrageous attitude. They’d consider themselves democratic, „but you can be sure that such an attitude would not endure sorious strains“, he observed.\nAfter 12 years of National Socialism, after the Nazis had permanently hammered home the idea of German racial superiority, after the war euphoria of the first years and all the rallying calls of the final ones, Germany’s unconditional surrender on May 8 came not only as a great disappointment but as an overall individual and collective identity crisis. One post office clerk notes: „There is peace in Europe. /…/ The future is dark, we are without a fatherland, there is no law and no protection. Six years have been wasted, you better not think of it“. The old belief in the Führer and the Reich had collapsed, and so had the „Volksgemeinschaft“, leaving nothing but nothingness and emptiness.\nIncoming refugees at the Hastedter Heerstraße, Photo: State Archives Bremen\nEnglish Version by Brian McGuire, North Berwick, Scotland, and Diethelm Knauf']	['<urn:uuid:822bba97-c4ee-4c8d-9f56-319f75e52f92>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T23:00:04.658849	7	9	1605
73	What's the duration between orbits for the two stars in Kepler-47?	The binary stars in Kepler-47 orbit one another every 7.45 days.	['By Duncan Geere, Wired UK\nAstronomers have published a study revealing that NASA’s exoplanet-hunting Kepler telescope has spotted two planets orbiting two suns for the first time.\n[partner id=”wireduk” align=”right”]The find proves that circumbinary planetary systems can not only form in, but continue to withstand, the intense pressures exerted by a binary star system — until now, astronomers had only been able to identify binary star systems with one planet in orbit, a find that was confirmed in 2011 when Kepler-16b was spotted.\n“The presence of a full-fledged circumbinary planetary system orbiting Kepler-47 is an amazing discovery,” commented Greg Laughlin, professor of astrophysics and planetary science at the University of California, in a press release. “These planets are very difficult to form using the currently accepted paradigm, and I believe that theorists, myself included, will be going back to the drawing board to try to improve our understanding of how planets are assembled in dusty circumbinary disks.”\nCircumbinary planets only came to light in the last two decades, with several discoveries being flagged up ahead of Kepler’s 2011 confirmation. Until then, Luke Skywalker’s home planet Tatooine in the Star Wars films was the most famous example of a circumbinary planet. Just one planet orbiting a binary system is an impressive feat, considering the dynamics of the gravitational pull being exercised by two stars orbiting one another. Astronomers have theorised that more often than not, the complex gravitational system would result in young planets being forced out or cause planetary collisions.\nAccording to the study, published in the journal Science, the two planets — Kepler-47b and Kepler-47c — have most likely thrived because of their relatively small size, their mass being affected by the binary system they grew up in.\n“In contrast to a single planet orbiting a single star, the planet in a circumbinary system must transit a ‘moving target.’ As a consequence, time intervals between the transits and their durations can vary substantially, sometimes short, other times long,” explained Jerome Orosz, a professor of astronomy and lead author of the paper, in the press release. “The intervals were the telltale sign these planets are in circumbinary orbits.”\nNearly 5,000 light-years from Earth, located in the Cygnus constellation, inner planet Kepler-47b orbits its stars in just under 50 days and is presumed to be a hostile environment with a searing hot surface. It has a radius three times that of Earth’s, which makes it the smallest ever circumbinary planet identified. Outer planet Kepler-47c has a radius 4.6 times that of Earth’s and orbits the binary stars every 303 days.\nLike Earth, that distance puts it in the habitable zone where liquid water could potentially exist, though its likely Kepler-47c is an inhospitable gaseous planet surrounded by thick clouds. “While the outer planet is probably a gas giant planet and thus not suitable for life, large moons, if present, would be interesting worlds to investigate as they could potentially harbour life,” commented Orosz’ colleague and co-author William Welsh in another press release.\nThe binary stars, which orbit one another every 7.45 days, also differ vastly in size, with one around the same size as our Sun but 84 percent as bright and the other a third of the size of our Sun and 1 percent as bright.\nDespite the planets both being considered inhospitable, the existence of a full circumbinary planetary system proves that there are even more possibilities for the existence of life in the universe, and that Kepler truly has its work cut out for it — Nasa’s phones are presumably ringing off the hook with Star Wars fans hoping the next find will be of a hospitable circumbinary planet with a scenic view of its two suns.\nSource: Wired.co.ukGo Back to Top. Skip To: Start of Article.']	['<urn:uuid:b45f40fd-b78d-4445-988c-30b1494c5057>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T23:00:04.658849	11	11	629
74	How do the power requirements and effectiveness of lighting systems vary between reef tanks and freshwater aquariums, and what metrics should we focus on to ensure proper light delivery?	For reef tanks and freshwater aquariums, lighting requirements vary significantly. Reef tanks, especially those containing anemones and corals, require particularly bright lighting that can be achieved through multiple fluorescent lamps or metal halide systems. For basic freshwater or fish-only marine tanks, a standard fluorescent strip light is usually sufficient. However, when evaluating lighting effectiveness, it's important to note that lumens (brightness as perceived by human eyes) is not the critical metric. Instead, PAR (Photosynthetically Active Radiation) is the key measurement, as it indicates how much usable light actually reaches the plants and corals. The spectrum of light is equally important, with both plants and corals requiring specific wavelengths for photosynthesis - specifically, they need light containing at least one Chlorophyll-A peak (around 470nm and 620nm) and one Chlorophyll-B peak (around 450nm and 660nm) to thrive.	"['Basic Saltwater Aquarium Equipment\nChoosing the right filter and lighting depends on what you will keep in your saltwater aquarium.\nThis is not going to be one of those ""Choose a sturdy support for the tank, water is heavy..."" basic equipment articles. I shall take the liberty of assuming that you have some knowledge of aquariums, although you may never have kept one before.\nTo successfully maintain marine organisms in the home for any reasonable length of time, you will need an aquarium of some kind. This should be a rectangular tank of any size larger than, say, 10 gallons (38 liters), although the bigger the better. It will need a sturdy support near electricity, and a source of water should be convenient.\nThe tank should be made entirely of glass or acrylic. You will need a cover for the tank, and a heater of sufficient wattage to keep the aquarium at a constant 75 degrees Fahrenheit (24 degrees Celsius). About 3 watts per gallon is usually satisfactory.\nYou will need an assortment of accessories for maintenance chores: a couple of 5-gallon (20-liter) buckets, a larger plastic container of about 30 gallons (115 liters), some flexible hose, nets...in short, the same sort of stuff you need for a freshwater tank. You will also need both a filtration system and a lighting system, the selection of which you should give considerable thought to.\nWhen you think about equipment for a new marine aquarium, think about nutrients. That\'s right, nutrients — of which there are not very many in the waters around coral reefs. The novice saltwater enthusiast should always remember that the life forms destined to inhabit his or her marine aquarium come from the waters around coral reefs. For reasons explained below, most of what you will be doing when caring for your marine aquarium will be involved with removing nutrients from the water in the tank.\nSo let\'s think about nutrients for a moment. Nutrients are, of course, essential for the survival of most living organisms. However, one of the special features of the coral reef environment is the paucity of nutrients in the water. Reef organisms have developed marvelously efficient ways of capturing and, in many cases, recycling most of the available nutrients. The result is that the bulk of the nitrogen, phosphorus and organic carbon present in the reef habitat is found in the biomass, not dissolved in the water.\nIn the aquarium, which is a closed system and an ecologically incomplete environment even under the best of circumstances, excess nutrients begin to accumulate from the moment living organisms are added. This accumulation results in a decline in the water quality of the system. If nothing is done to reverse this process, water conditions will soon deteriorate to a point outside the range of tolerance of the life forms. They, in turn, will fare poorly.\nFiltration is therefore necessary to prevent, or at least retard, this gradual worsening of water conditions. Filtration, properly chosen and combined with water changes and the judicious application of tank additives, enables the aquarist to maintain aquarium water in good condition almost indefinitely.\nSunlight plays an important role in the nutrient recycling process that occurs naturally on coral reefs. This fact has important implications in the selection of aquarium lighting.\nChoosing suitable filtration and lighting is only one aspect of setting up a successful saltwater aquarium, but it is a very important aspect, for two reasons. First, a significant portion of the total cost of the aquarium will be spent for the filtration and lighting systems. Second, there is a trade-off between the initial investment in equipment and the amount of effort you will expend later in keeping the tank in the appropriate condition. Such effort may not only involve maintenance chores but, alas, may also be associated with medicating sick fish, replacing filter media and light bulbs, coping with prolific algae growth and repairing worn or faulty equipment. The time to be choosy and critical is now, when you are first setting up the tank, not six months from now when that bargain-priced pump fails while you are away for the weekend.\nThere is such a bewildering array of options available for filtering a marine tank. Which one is best? The correct answer, of course, is that no one filtration system is suitable for all applications, and that the choice for your particular aquarium should be determined by what you intend to keep in the tank. There are probably as many filtration theories as there are aquarists, but for simplicity\'s sake, I will discuss only three approaches to marine tank filtration. I shall refer to these as ""high-tech,"" ""natural"" and ""traditional"" setups.\nThoroughly dedicated marine hobbyists, by the way, can actually forgo filtration altogether, provided they are willing to carry out frequent, large water changes. The ultimate purpose of any filtration system is to extend the useful life of the water in the tank. And no filtration system, no matter how sophisticated, can eliminate the need for partial water changes. All filtration methods are applied with the intent of preventing changes in the chemistry of the aquarium water that would render it unsuitable (i.e., stress-producing) for the inhabitants of the tank.\nHigh-tech filtration systems seek to automate, insofar as possible, maintenance of the appropriate water quality. The goal is stability of the water chemistry with minimal labor for the aquarist. The redox controller (to be discussed later in this series) receives my vote for the centerpiece of such a system, although only a couple of years ago any tank fitted with a wet-dry filter was high-tech. If you are comfortable with or like to use computers, fax machines and so on, you may be a candidate for a high-tech aquarium system. To do it right, you will need a roomy budget.\nNatural filtration systems rely primarily on the good judgement of the aquarist, a protein skimmer and an ample quantity of live rock. This is my personal choice, but not one I can, in good conscience, recommend to the novice. Developing good judgement about marine aquarium husbandry requires some hands-on experience. For the beginner, it is wiser to employ some equipment that will provide a margin for errors in judgement.\nTraditional systems are, in my view, defined by the use of the undergravel filter. This somewhat less-than-satisfactory device has the decided advantage of being both cheap and almost foolproof. Its primary disadvantage is that proper cleaning is difficult to accomplish.\nIn the last few years, wet/dry filtration systems have received much attention, largely with regard to their application in maintaining so-called reef tanks. Any marine aquarium, however, whether reef or fish-only, will fare better and be easier to maintain if a wet-dry filter is installed instead of using an undergravel filter. Because traditionalists will be horrified at this allegation, and the novice aquarist may be horrified at the cost of a wet/dry filter, let me explain why I believe there is a decided advantage to this method of filtration over the more commonplace, and certainly cheaper, undergravel filter.\nAt minimum, we require a filter to process toxic ammonia into less toxic nitrate via the process known as biological filtration. If you do not clearly understand what is meant by biological filtration as it applies to aquarium maintenance, stop right here and do some reading on the subject. Complete descriptions of this process, which is achieved by beneficial bacteria, are found in virtually any decent book on aquariums, whether freshwater or saltwater. Excellent articles that deal with biological filtration in detail regularly appear in this and other magazines.\nIn a nutshell, your saltwater aquarium filtration system must be able to detoxify the ammonia (a form of nitrogen — a nutrient) that will be produced by the inhabitants of the tank, or they will die rather promptly. Both undergravel filters and wet/dry filters easily accomplish biological filtration.\nWet/dry filters, however, excel in two ways. They trap detritus in such a manner that it can be removed with much less effort than is required for an undergravel filter, and they are far easier to maintain, with little physical disruption of the tank.\nA wet/dry filter is essentially a watertight box (sump) with a smaller box (biological chamber) sitting on top. Water flows from the aquarium by gravity, trickles through the biological chamber, collects in the sump and is pumped back into the aquarium. The biological chamber is filled with pieces of plastic that become colonized by the beneficial bacteria that detoxify ammonia. The sump often houses additional filter media, or other equipment, and thus serves as a convenient place to hide this extra hardware underneath the tank, out of sight.\nThe trickling of water through the biological chamber permits maximum contact with the air, allowing essential oxygen to be dissolved in the water and harmful carbon dioxide to escape. Detritus, which consists mostly of dead bacteria, is trapped in a simple sheet of polyester fiber pad placed at the top of the biological chamber through which water from the tank must flow. Good filters have a nifty drawer in this position in order to facilitate periodic cleaning of this pad. Additional detritus accumulates in the sump, beneath the biological chamber, from which it may be siphoned with minimal disturbance to the tank itself. Maintenance chores are therefore more easily accomplished and, consequently, more regularly performed.\nWith an undergravel filter, in contrast, all of this stuff — filter media (substrate), powerheads, airstones, wires, hoses and so on — is in the tank. This not only spoils the appearance of the tank, but makes proper cleaning almost impossible without severe imposition upon the peace of mind of the tank\'s inhabitants.\nI have mentioned detritus several times. Why is detritus such a problem? It acts as a storage depot for nutrients, and, as I mentioned above, most of what you want a filter to do is eliminate excess nutrients from the aquarium. Phosphorus compounds, of which detritus is a rich storehouse, will promote thick, obscuring growths of algae if allowed to accumulate. Organic carbon compounds, dissolved in the water but also found abundantly in detritus, will provide food for undesirable and possibly disease-producing bacteria. The water in the tank will be analogous in purity to the air over the Los Angeles freeways and your fish will not thrive.\nIf you cannot justify the purchase of a wet/dry filter and must settle for an undergravel system, bear in mind the limitations of the latter. Resolve to spend more time maintaining the tank and resist the temptation to stock a large number of fish. I will present some important facts about choosing a wet/dry filter system in the next installment of this article.\nLighting plays a key role in the marine aquarium, but is a critical consideration only if you plan to keep invertebrates, such as anemones or corals. An aquarium that will not be home to such organisms should nevertheless be brightly illuminated. Reef fish are accustomed to high light levels in their natural habitat, and bright light will promote the growth of beneficial green algae. As you will come to understand, algae can be both a bane and a blessing in a marine aquarium.\nIf you plan on keeping anemones, which require bright light, you should consider fixtures that permit you to place several fluorescent lamps over the tank, or perhaps a metal halide system. I will discuss both types of lighting in a subsequent installment of this series.\nMost beginners opt for a tank containing fish and perhaps a few invertebrates, such as starfish or shrimps, that have no special light requirements. For these organisms, the standard fluorescent strip light that comes with most aquarium tanks is satisfactory. Such fixtures will accommodate a single fluorescent tube. Select a hood for the tank that will accommodate a tube of the maximum length that will fit across the top of the tank. The longer the tube, the brighter it will be.\nThe particular type of fluorescent tube is important, since all tubes are not created equal in terms of their light output or the aesthetic appeal of the color of the illumination. Some of the best are the Ultralume 5000K and Advantage X lamps manufactured by the Philips corporation. Some of the worst are cool white (the lamp usually supplied in hardware store fluorescent fixtures) and the various brands of lamps typified by the Sylvania Gro-Lux, which are designed for growing and displaying terrestrial plants. Gro-Lux type lamps are marketed widely under a variety of names for aquarium lighting. In my view, they are less than satisfactory.', ""This page contains general information about LEDS to help demystify the jargon.\n|BEAM||aka Optic angle, important for depth penetration|\n|LUMENS||A measure of light as seen by the human eye, NOT an important spec in lighting systems because a bright light with high lumens does not necessarily mean a bright light for plants and corals.|\n|PAR||aka Photosynthetically Active Radiation, a more important spec in lighting systems than lumens, a measure of light that your plants and corals receive|\n|SPECTRUM||aka light wavelength, an important spec because plants and corals require specific wavelengths of light to survive.|\n|KELVIN||a measure of the overall colour of a lighting system as perceived by the human eye. Contrary to popular belief, KELVIN is NOT an important specs for plant and coral growth, it is a measure of what looks good to you.|\n|CRI||a measure of how natural the light looks compared to sunlight, commonly specified as a number for 1 to 100, 100 being exactly like daylight. NOT an important spec for plant and coral growth but important for visual aesthetics.|\nBEAM ANGLE (aka OPTIC ANGLE)\nMakeMyLed have four beam angles (aka optic angle), 120°, 90°, 60° and 45°, to deliver the right amount of light for the width and depth of your aquarium.\nThe way beam angles work is simple, the wider the beam angle the more surface area coverage you get and the less depth penetration you get, the narrower the beam angle the less surface area coverage you get and the more depth penetration you get so it's simply a matter of choosing a beam angle that suits the height of your tank.\nUse the chart below to work out what beam angle suits your tank. For height, measure the distance from the top of your substrate (soil/gravel) to the top of your tank and add 7cm for the distance from the top of your tank to the light fixture. If you plant to suspend the fixture above the tank or to the underside of a hood make sure you include that distance in your calculation.\nPAR (aka Photosynthetically Active Radiation)\nYou've probably heard of LUMENS , PAR (Photosynthetically Active Radiation) and PPFD (Photosynthetic Photon Flux Density) as a measure of brightness, put simply LUMENS is brightness as perceived by the human eye and PPFD is brightness as perceived by your tank inhabitants and varies according to the depth of your tank and the condition of your water.\nA light that looks bright to you has high LUMENS but doesn't necessarily make it a good light for your aquarium. PPFD (aka PAR) is the measure of the amount of light that falls on objects in your tank e.g. plants and corals. PPFD is measured by a PAR/PPFD Meter (AKA Quantum Meter) and is a measure of the number of light photons that reach a target over a square metre every second, commonly measured in units called μmol/m2/s (micromoles per square meter per second)\nPAR readings vary with depth and are also affected by the clarity of aquarium water, the deeper you go the lower the PAR reading will be so you can see that PAR readings at specific depths are very useful measurements of how much light your plants and corals actually get at those depths and is scientic way to evaluate light performance.\nLED fixture manufacturers and resellers who don't publish PAR readings for their lights either don't know what the readings are or they have such poor PAR performance that they don't want to disclose this. As a general rule if you don't see PAR readings for the light you are cnsidering buying then steer clear or you may end up wasting your $ on something that won't do the job.\nMakeMyLed publish PAR readings for all of their light fixtures as measured by LAB grade quantum meters in real conditions so can make an informed choice and know exactly what you'll be getting.\nSPECTRUM (aka light wavelength)\nWe've seen that PPFD/PAR is a measure of how much light falls on Plants or Corals in your aquarium, Spectrum is the measure of how useful that light is for Plants and Corals.\nSpectrum is measured as a wavelength in nanometers (nm) and certain wavelengths are more beneficial for plants and corals and other wavelengths are less benefcial. So it's not only PAR that is important,Spectrum is equally if not more important.\nBoth plants and corals use light to go through a process called photosynthesis in which light energy is converted to chemical energy which in turn is used to fuel the organism's activities. Plants photosynthesise light to grow and zooxanthellae algae in Coral polyps do the same.\nThe diagram below shows the photosynthesis peaks (aka Chlorophyll peaks) that are important for both plants and zooxanthellae algae in Corals.\nYou can see from the diagram that the Chlorophyll-B has two wavelength peaks, one at just under 450nm and the other at 660nm, Chlorophyll-A also has two wavelength peaks, one at around 470nm and the other at 620nm.\nAs a general rule, Plants and Corals need light containing at least one Chlorophyll-A peak and one Chlorophyll-B peak to thrive so LED light fixtures must meet this requirement to be of any use.\nAgain the general rule is that if LED manufacturers don't quote the light wavelength then this should ring alarm bells and you should steer clear from buying these.\nMakeMyLed quote both PAR and Spectral wavelengths for all of their LED systems.\nKELVIN TEMPERATURE (aka K Rating)\nThis is a very misunderstood measurement of light. KELVIN TEMPERATURE (aka KELVIN RATING or KELVIN COLOUR) refers to the colour of light as perceived by the human eye. Low KELVIN readings have more red in the overall colour and high KELVIN readings have more blue in the overall colour. KELVIN is not an important measurement for for plants and corals but it is important for aesthetic reasons because you're the one that will be looking at the aquarium.\n6500K to 1000K is a pleasant colour to look at for fresh water tanks, 12000K and higher is a pleasant colour to look at for marine tanks.\nCRI (aka Colour Rendering Index)\nThis is a percentage from 1% to 100% representing how accurately the light will reproduce natural colours in your aquarium, the higher value being the better. This is also not important for the health and wellbeing of plants and corals and high numbers are purely sought for aesthetic reasons.\nAll prices are in USD""]"	['<urn:uuid:a38ecf75-da2f-46c9-a07b-638cfea20658>', '<urn:uuid:d371e34a-1f66-47c9-88a0-42ede88c0f5c>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T23:00:04.658849	29	135	3176
75	What are the career opportunities available for Chinese language graduates in international settings, and what unique advantages does the logographic writing system offer for global communication?	Chinese language graduates can pursue diverse international careers including research officer, foreign affairs and trade officer, interpreter/translator, tourism guide, and roles in global business and international relations. The Chinese logographic writing system offers unique advantages for global communication as people speaking different languages can understand the written meaning without knowing the spoken language - similar to how everyone understands what '1' means regardless of what they call it. Additionally, communication through writing (筆談) was historically the norm for international trade and diplomacy in East Asia.	"[""- Chinese Major (BA)\n- Admission criteria\n- Guaranteed ATAR 70\nA full-time study load usually consists of 200 credits (approximately eight units) per year, with 100 credits (approximately four units) in each semester.\nChinese is the world’s most spoken first language. Classes cater for beginners through to native speakers.\nLast semester I had an exchange scholarship to Fudan University in Shanghai. The culture, the people and the general environment was a dream – it solidified my desire to work in Asia after my studies.\nHumanities courses teach transferable skills for international careers.\nThis course can be taken as part of a double degree.\nYou’ll have access to a range of international study opportunities.\nSpoken by 1.3 billion people, Chinese is the world’s most spoken first language. China also has one of the world’s most vibrant economies, and is a major trade partner of Australia.\nWith Asia’s continuing social and economic growth, an understanding of Chinese language and culture is increasingly important in global engagement.\nThis major will provide you with the knowledge and skills required for a diverse international career.\nYou will gain a high level of literacy in Chinese – listening, speaking, reading and writing – and a deep understanding of Chinese culture and society.\nThis major is suitable for all students: previous study of Chinese is not required; advanced classes are provided for students who have studied Chinese previously and specific units are offered for native speakers of the language.\nChinese is offered as part of the Bachelor of Arts. You can enhance your studies with a secondary major or choose from a range of elective units to support your career goals.\nPlease refer to the handbook for additional course overview information.\nHow to apply\nThis major sits within the Bachelor of Arts degree. To apply for this major, you will apply for the Bachelor of Arts. In step 3 of the application process, you will be asked to confirm the major you wish to study.\nApplying as an undergraduate student is easy. For information tailored to your situation, visit our easy to follow instructions to get your application started.\nReady to apply?\n- The Chinese language has a long history with rich cultural heritage.\n- A working knowledge of the Chinese language will open up many career opportunities in Australia and abroad.\n- We combine enjoyable classroom learning with online Chinese language resources to help you build the knowledge, skills and understanding crucial for effective intercultural communications.\nView our student profile table to get an indication of the likely peer cohort for new students at Curtin University. This table provides data on students that commenced undergraduate study and passed the census date in the most relevant recent intake period for which data are available. It includes those admitted through all offer rounds and international students studying in Australia.\n- Research officer\n- Foreign affairs and trade officer\n- Interpreter / translator\n- Tourism information officer / tour guide\n- Welfare worker\n- Teacher – English as a Second Language\n- Global business\n- International relations\n- Public service\nWhat you'll learn\n- communicate in Chinese, and apply knowledge of applied linguistics and intercultural communication in a professional context\n- critically analyse issues in the context of cultural and linguistic differences\n- locate, evaluate and synthesise information from various sources using more than one language\n- function in Chinese with an ability to communicate in an appropriate register in a range of situations and for a range of purposes\n- use technology effectively to collect information and communicate in more than one language\n- take responsibility for own learning and professional development in an international context\n- evaluate and analyse issues arising in the region with in-depth knowledge of Chinese language and culture\n- interact with people from a range of cultures and backgrounds with an understanding of cultural differences\n- work independently and in teams with an intercultural understanding\nCurtin requires all applicants to demonstrate proficiency in English. Specific English requirements for this course are outlined in the IELTS table below.\n|IELTS Academic (International English Language Testing System)|\n|Overall band score||6.5|\nPlease note: The cut-off scores below are for 2018 only.\n|GCE/STPM/HKALE (A-levels - best of three)||5|\n|Ontario Gr 12 (best of 6)||60|\n|ATAR (including WACE/SACE/HCE/VCE)||70|\nPlease select a country above.\nOther requirements and notes for this course\nApplicants are required to choose one of the Bachelor of Arts majors when applying. If you with to study a double major, you elect the second major once you have been accepted into the course.\nIn addition to the course-specific admission criteria listed above, please read our general admission criteria. Our general admission criteria apply to all courses at Curtin University.\nAt Curtin, we understand that everyone’s study journey has been different.\nYou may have already studied some of the units (subjects) listed in your Curtin course at another educational institution, or you may have work experience that matches the degree requirements.\nA successful CRL application exempts you from having to complete certain units within your course and means you could finish your degree in a shorter amount of time.\nCRL is also known as recognition of prior learning, advanced standing and credit transfer.\nUse the CRL search to find out how much CRL you qualify for, or contact us at:\nTel: 1300 222 888\nFees & charges\nPlease view the Bachelor of Arts degree for indicative fees.\nHow to apply\nPlease review information on how to apply for the campus of your choiceApply now\nHelp is at hand\n- Can I apply for special consideration?\n- Can I remain in Australia until my student visa expires if I have completed my course?\n- What are the alternative pathways into Curtin?\n- What are my study mode options?\n- What are my postgraduate study options?\nThe offering information on this website applies only to future students. Current students should refer to faculty handbooks for current or past course information. View courses information disclaimer.\n- Curtin course code: MJRU-CHNSE\n- Last updated on: November 25, 2019"", 'A logogram, or logograph, is a single grapheme which represents a word or a morpheme (a meaningful unit of language). This stands in contrast to other writing systems, such as syllabaries, abugidas, abjads, and alphabets, where each symbol (letter) primarily represents a sound or a combination of sounds.\nLogographs are commonly known also as ""ideograms"". Strictly speaking, however, ideograms represent ideas directly rather than words and morphemes, and none of the logographic systems described here is truly ideographic.\nLogographs are composed of visual elements arranged in a variety of ways, rather than using the segmental phoneme principle of construction used in alphabetic languages. As a result, it is relatively easier to remember or guess the sound of alphabetic written words, although it is relatively easier to remember or guess the meaning of ideographs. Another feature of logographs is that a single logograph may be used by a plurality of languages to represent words with similar meanings. While disparate languages may also use the same or similar alphabets, abjads, abugidas, syllabaries and the like, the degree to which they may share identical representations for words with disparate pronunciations is much more limited.\nLogographic systems are the earliest true writing systems; many of the first civilizations in the Near East, India, China, and Central America used some form of logographic writing. Examples of languages that have logographic systems include:\n- partly Consonant-based\n- partly Syllable-based\n- Chinese-based systems\nThere are no purely logographic language systems in existance today. A common myth is that Chinese is a logographic language. Though many characters have associated meanings, nearly all Chinese words involve combinations of characters. Only a small minority of words in Chinese involve single characters. Additionally, characters are made up of sub-character radicals that can also cue pronunciation and meaning. Only the most basic monosyllabic words in Chinese could be considered logographic.\nLogographs are used in modern shorthand systems in order to represent common words. In addition, the numerals and mathematical symbols used in modern writing systems are also logograms — 1 stands for one, 2 for two, + for plus, = for equals and so on. In English, the ampersand & is used for and and et (such as &c for et cetera), % for percent, $ for dollar, # for number, € for euro, £ for pound, etc.\nIdeographic and phonetic dimensionsEdit\nAll full logographic systems include a phonetic dimension (such as the ""a"" in the logogram @ at). In some cases, such as cuneiform as it was used for Akkadian, the vast majority of glyphs are used for their sound values rather than logographically. Many logographic systems also have an ideographic component, called ""determinatives"" in the case of Egyptian and ""radicals"" in the case of Chinese. Typical Egyptian usage is to augment a logogram, which may potentially represent several words with different pronunciations, with a determinative to narrow down the meaning, and a phonetic component to specify the pronunciation. In the case of Chinese, the vast majority of characters are a fixed combination of a radical that indicates its semantic category, plus a phonetic to give an idea of the pronunciation, although this has become somewhat opaque over the last three millennia. The Mayan system used logograms with phonetic complements like the Egyptian, while lacking ideographic components.\n- Main article: Chinese character classification\nChinese scholars have traditionally classified Chinese characters into six types by etymology.\nThe first two types are ""single-body"", meaning that the character was created independently of other Chinese characters. Although the perception of most Westerners is that most characters were derived in single-body fashion, pictograms and ideograms actually take up but a small proportion of Chinese logograms. More productive for the Chinese script were the two ""compound"" methods, i.e. the character was created from assembling different characters. Despite being called ""compounds"", these logograms are still single characters, and are written to take up the same amount of space as any other logogram. The final two types are methods in the usage of characters rather than the formation of characters themselves.\n- The first type, and the type most often associated with Chinese writing, are pictograms, which are pictorial representations of the morpheme represented, e.g. 山 for ""mountain"".\n- The second type are ideograms that attempt to graphicalize abstract concepts, such as 上 ""up"" and 下 ""down"". Also considered ideograms are pictograms with an ideographic indicator; for instance, 刀 is a pictogram meaning ""knife"", while 刃 is an ideogram meaning ""blade"".\n- Radical-radical compounds in which each element (radical) of the character hints at the meaning.\n- Radical-phonetic compounds, in which one component (the radical) indicates the general meaning of the character, and the other (the phonetic) hints at the pronunciation. An example is 樑 (Chinese: liáng), where the phonetic 梁 liáng indicates the pronunciation of the character and the radical 木 (""wood"") its meaning of ""supporting beam"". Characters of this type constitute the majority of Chinese logograms.\n- Changed-annotation characters are characters which were originally the same character but have bifurcated through orthographic and often semantic drift. For instance, 樂‘music’ is also read 樂‘pleasure’ .\n- Improvisational characters (lit. ""improvised-borrowed-words"") and come into use when a native spoken word has no corresponding character, and hence another character with the same or a similar sound (and often a close meaning) is ""borrowed""; occasionally, the new meaning can supplant the old meaning. 自 used to be a pictographic word meaning ""nose"", but was borrowed to mean ""self"". It is now used almost exclusively to mean ""self"", while the ""nose"" meaning survives only in set-phrases and more archaic compounds. Because of their derivational process, the entire set of Japanese kana can be considered to be of this character, hence the name kana (仮名; 仮 is a simplified form of 假).\nThe most productive method of Chinese writing, the radical-phonetic, was made possible because the phonetic system of Chinese allowed for generous homonymy, and because in consideration of phonetic similarity tone was generally ignored, as were the medial and final consonants of the characters in consideration, at least according to theory following from reconstructed Old Chinese pronunciation. Note that due to the long period of language evolution, such component ""hints"" within characters as provided by the radical-phonetic compounds are sometimes useless and may be misleading in modern usage. This is particularly true in non-Chinese languages, such as Japanese, that have also attached native readings to Chinese characters.\nChinese characters used in Japanese and KoreanEdit\nWithin the context of the Chinese language, Chinese characters by and large represent words and morphemes rather than pure ideas; however, the adoption of Chinese characters by the Japanese and Korean languages (where they are known as kanji and hanja, respectively) have resulted in some complications to this picture.\nMany Chinese words, composed of Chinese morphemes, were borrowed into Japanese and Korean together with their character representations; in this case, the morphemes and characters were borrowed together. In other cases, however, characters were borrowed to represent native Japanese and Korean morphemes, on the basis of meaning alone. As a result, a single character can end up representing multiple morphemes of similar meaning but different origins across several languages.\nAdvantages and disadvantagesEdit\n- Compared to alphabetical systems, logographies have the disadvantage of requiring the memorization of many more glyphs, and their respective pronunciations (which can be numerous in Japanese, Korean, and Chinese). However, it require far less effor in comparison to memorising proper spelling in English.\n- The pronunciation of a written word is not obvious unless you know all the logographs (but it can be guessed at). In Japanese this is particularily difficult as it has several possible pronunciations for almost every logogram.\n- Conversely, the spelling of a word is not obvious from the pronunciation like it is in many alphabetical systems such as Italian and Finnish. (English is not a very good example on this point.) That is, unless you also know the meaning of the word and can guess which logographs it consists of. However, recent development in word processing technology made it easier to pick the correct logogram.\n- Logographs cannot be inflected like words in alphabetic systems can. Languages which has imported Chinese logograms, such as Japanese and Korean (which both inflect extensively) cannot accurately describe their languages with logograms alone, and a separate alphabetic or syllabaric system is needed anyway.\n- The biggest advantage is that one does not necessarily need to know the spoken language of the writer to understand them — everyone understands what 1 means, whether they call it one, eins, uno or ichi. Likewise, people speaking different Chinese dialects may not understand each other in speaking, but can to a limited extent in writing, even if they don\'t write in standard Chinese. Moreover, in ancient orient (including Vietnam, Manchuria, Korea Japan, etc), communcation by writing (筆談) was the norm of international trade and diplomacy. Deaf people also find logogram system much easier to learn as the words are not related to sound, and it reduce the amount of words one must memorise to tremendous degree.\n- The meaning of words can be known directly. This singnificantly reduce the amount of effor required to advance from basic literacy to functional and academic literacy, despite the initial difficulty in becoming literate. Everyone who knows what the characters mean, can know what a new word means without explanation. This advantage become more pronounced as one advance in academia. In English, for example, more abstract words are constructed artifically from Greeks or Latin words. These words are often uninteligible to most people outside of the speciality. For example, the word ""logogram"" is a combination of greek words logo (""word"" or ""speech"") and gram (“something written” or “drawing”). In Chinese, it is written as 表語文字 (Word expressing letter) and anyone who are literate at basic level can correctly guess the meaning. Once one learn basic 2000-3000 letter/words in logogram, one immediately become functionally literate. And it take small effort to become academically literateat at highly advanced level. On the other hand, in Western language, for example, there is no lowering of learning curve for new terms and new vocaburary as they progress academically unless one learn Greek or Latin. The use of logogram reduce the amount of words one must memorise as most can be read and written almost instinctively. This is cited as the primary reason of close correspondence between literacy rate and functional literacy rate in Japan and China.\n- A logogram-based system uses fewer characters to express something compared to an alphabetic system. Compare the following title in English, Chinese(traditional/simplified) and Japanese, respectively:\n- ""Return of the King""\nUsually, the more complicated the idea being expressed, the more apparent this trend becomes; for example, the military term APFSDS and the translation in Chinese and Japanese:\n- ""armour-piercing fin-stabilized discarding sabot""\nAnd the weapon:\n- ""smoothbore gun""\nAnd also terms like:\n- ""Soviet-Sino Conflict""\nNote however, that the number of spoken syllables in either langue is similar, and that the number of strokes needed to write the English version is significantly lower (21 versus 38 and 33 in the first example, and 53 versus 100 and 101 in the second example) which means that the logographic version can take significantly longer to write. This is less of a problem when typing on a computer.\nOn the other hand, for examples like the following, there\'s little advantage:\n- ""Union of Soviet Socialist Republics""\nThis is particularly true of cases where English can express an idea in a word, such as:\n- ""Secretary"" (of organization)\nMoreover, alphabets have a slight advantage in utilising acronyms, such as ""Light Amplification by Stimulated Emission of Radiation"" into ""LASER"". This is also possible to a lesser degree in logogram based languages. For example the United Nations:\n- ""国連"" (from 国際連合)/""联合国""\nOr the North Atlantic Treaty Organisation:\n- ""北約""/""北约"" (From Chinese 北大西洋公約組織. The Japanese name is 北大西洋条約機構)\nOne advantage of logograms in cases like the first example is that, while one who has not heard of the United Nations would have no clue as to what UN is, with logograms a moderately educated individual could easily decipher that this 国連 is something to do with ""国 -> country"" and ""連 -> union"", thus making the meaning more or less apparent. The second one, ""北 -> north"" and ""約 -> promise/treaty"" would however be confusing.\nShorter sentence lengths are beneficial to major communication media, such as newspapers (particularly headlines), and users of mobile phone web browsers and similar devices which display information on small screens. These devices typically have few buttons, but systems for breaking up Chinese characters into their constituent parts, as well as phonetic systems based on Bopomofo or Pinyin have been used to enter a single Chinese character with multiple keypresses.\nAlso due to the number of glyphs, in programming and computing in general, more memory is needed to store a character of that type than a Latin-based character, although a word in Chinese is represented by one or two glyphs (two to four bytes in Unicode), compared to an average of five characters plus a space (six bytes in ASCII) in English (more in languages like Spanish and German). Unicode is increasingly being used even for English (as in the Java programming language) and depending on the encoding, it use one or more bytes per character.\nBecause character recognition is not difficult (comparable to short English words of similar size, such as \'cat\', \'dog\' or \'cake\') once the system is learned, and sentences are relatively short, a logogram-based system allows for faster reading times overall.\n- A Typographic Outcry: a curious perspective\n- DeFrancis, John (1984). The Chinese Language: Fact and Fantasy, University of Hawaii Press. ISBN 0-8248-1068-6.\n- Hannas, William C. (1997). Asia\'s Orthographic Dilemma, University of Hawaii Press. ISBN 0-8248-1892-X.\n- Hoffman, Joel M. (2004). In the Beginning: A Short History of the Hebrew Language, NYU Press. ISBN 0-8147-3690-4. - Chapter 3.de:Schriftzeichen#Logogramm\n|This page uses Creative Commons Licensed content from Wikipedia (view authors).|']"	['<urn:uuid:0f4abb18-e1fa-46c2-891d-db8e03a1de56>', '<urn:uuid:c144a965-7f8f-4af8-9aa5-5f4767c61188>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T23:00:04.658849	26	85	3337
76	I'm researching health impacts of industrial processes - could you explain the main health risks associated with exposure to fracking chemicals?	Exposure to fracking chemicals can cause multiple serious health issues. People exposed to these chemicals may experience skin, eye and sensory problems, respiratory and gastrointestinal issues, cardiovascular problems, and kidney issues. Some of these chemicals are known to cause incurable diseases, including cancer. The toxic chemicals, which make up approximately 0.5-2% of fracturing fluids, can pollute underground water, affecting drinking water sources. The contamination can persist for many years after the fracturing is completed, and affects not only human health but also poses risks to animals and plants.	['geologic formations sometimes contains large pools of oil and/or gas but may\nhave a poor flow rate due to clogging or low permeability of the formation\nduring drilling and occurs in laces that have tight sands, shale’s or places\nthat have methane formations. To extract the mineral deposits in such places,\nhydraulic fracturing/cracking or fracking process is used since it has the\ncapability of stimulating wells drilled into the mineral formation locations\nthus enabling the extraction of oil or gas economically. Hydraulic fracturing\nis therefore a well stimulation technique that enables a rock to be fractured\nby hydraulically pressured liquid that comprises of water sand and other\nchemicals into a wellbore to create cracks that allow gas and other natural gases\nto flow freely and easily towards the collection point. The science behind the\nfracking process occurs after wells have been drilled and steel pipe inserted\nin the well bore. The steel case piping is perforates within the targeted\noil/gas zones such that when the fracturing fluid is injected into the well, it\nflows through the perforations into the targeted zones where pressure is\ncreated that causes the formation to crack/fracture, after which injection is\nceased allowing the fracturing fluid to flow back to the surface. The fractures\nmay also be created by injecting gases like propane or nitrogen causing\nacidizing and fracturing where acid is pumped into the formation to dissolve\nrock materials and to clean the pores to enable gas or oil to flow better into\nEconomic analysis of fracking\ncosts associated with fracking include the large amount of water required to\ncrack a single well and the toxic chemicals that have to be pumped into the\nrock formations to crack the surface. Fracking also produces greenhouse gases\nwhich affects the environment negatively. There has been reported tectonic\nimpact of the disposal wells that lead to minor earthquakes in the nearby\nsettlements. This is disastrous as it may lead to loss of lives and destruction\nof buildings. Water contamination cannot be avoided as a result of fracking.\nthe advent of fracking, gas prices have lowered comparatively because of the\ncompetitive nature of the natural gas. One of the benefits associated with\nnatural gas is that it produces less carbon dioxide and less sulfur dioxide\nwhich leads to multiple health problems. Green house emission have also been\nreported to have decreased as a result of the fracturing. Before the advent of\nthe shale gas, importation of the gas would have been inevitable, thus\nincreasing the cost of the product to the consumer. Natural gas can be burned to produce electricity\nthat is cheaper and cleaner than coal (Wicander & Monroe, 2015). Upon the extraction\nof the hydraulic fracturing, a lot of employment opportunities are availed to\nthe many unemployed Americans. This in effects uplifts the living standards of\nthe population and reduces the burden of government support to the unemployed.\nAt the same time, fracking has helped accelerate the growth of the energy\nsector thus increasing the annual GDP by over $300 billion annually (Gilbert, 2011).\nSteady production of oil can be deduced from the undertaking thus reducing the\nimport capacity costs.\nhydraulic fracturing is one of the key methods that are used to extract\nunconventional gases and other natural gas resources, it poses various risks,\nboth to the human being and the environment/habitats. Some of these issues that\nimpact the environment includes the water usage, chemical usage, pollution,\neffect on the air among other effects.\nto a 2010 survey that was carried out by the US Environmental Protection\nAgency, it was estimated that between 70 and 140 billion gallons of water were\nused to fracture 35000 wells every year. This is equivalent to the annual water\nconsumption of 40 – 80 cities whose population is 50,000. The extraction of\nsuch amount of water has ecological impact in that it affects the aquatic life\nas well as the aspect of dewatering the water aquifers. At the same time, the water transportation modes\n(trucks) affects the habitat by creating a localized air quality and road\nmaintenance issues (Spellman, 2013). It should be noted that these gas and oil\nwells use large quantities of sand and proppants that leads top excess water\nconsumption and air pollution. Health problems may also arise that relates to\nthe extraction of the unconventional minerals and shale gas, a lot of toxic\nchemicals are used that amount to approximately 0.5-2% of the volume of the\nfracturing fluids. The more the water used, the more the toxic gas implying\nthat the toxic fluid are a health hazard\nto human, animals and plants. Some of these chemicals cause incurable diseases\nincluding cancer (Spellman, 2013). These chemicals pollutes the water\nunderground, thus affecting the aquatic life as well as the air and drinking\nwater. Any human being that get exposed to the fracking chemicals are likely to\nhave health effects including skin, eye and sensory issues, respiratory and\ngastrointestinal problems, cardiovascular problems, kidney issues among many\nother health concerns (Howarth, Ingraffea & Engelder, 2011).\nalso exists the contamination of soil and surface water during waste disposal\nand chemical injection and preparation. The ground water which may have coal\nbends containing enough water for drinking may become contaminated as a result.\nOf concern is the chemical component that is left in the underground after fracturing is done. This\nmay lead to water contamination for many years to come. The air quality is also\ncompromised as a result of chemical toxicity near the wells due to flow backs and\nHurdles to regulating fracking\nstate regulations have enabled the extraction of the unconventional gas to\noperate safely and in an environmental friendly manner. Stiff measures\nincluding fines and other punitive measures have been put in place to prevent\ncompanies undertaking fracturing from destroying the environment. Ground water\nprotection agencies have been formed to address possible pollution to the\nwaters and each state has been mandated to disclose chemicals used in fracturing.\nWaste management commissions have also been formed to ensure safe disposal of\nthe waste products resulting from the fracking activities.\nEffect of the energy markets since the advent of\nadvent of fracking has increased the competition in the energy sector in the United\nStates. In particular, the prices of\nenergy/gas has decreased considerably and its increase has led US into being self-sufficient\nthus limiting their imports. The US market has now become an exporter of gas\nand coals that it no longer need. At the same time, as a result, the regulatory\nframework has changed to accommodate fracturing.\nThe future role of natural gas and shale oil in the\nstatistics depicts a positive trend in the production of shale gas. In the\nfuture, it is expected that the fractured gas will play a major role in the\nenergy sector in the United States. It is expected that about 42% of the total\nUS gas will emanate from the unconventional gas production. At the same time,\nthe demand for the natural gas will be expected to increase and many parties will\nvolunteer to work towards the exploration of the natural gas that will help\nsupply the nation for many years to come. Finally, I is expected that the low\nnatural gas and oil prices will help improve the energy sector by increasing\nthe firms fueled by the cheap natural gas. The continued production of the\nnatural oil will enable US to be self-sufficient in oil production and will\nhelp the country export surplus to other countries\nPolicy recommendation and course of action on\n300,000 barrels of natural gas are produced each day through the process of\nhydraulic fracking. This is a good economic undertaking, but it succeeds at the\nexpenses of environmental, safety as well as health hazard. This being the\ncase, it is important for all the concerned authorities to address the issue\nand come up with strategies that could exploit these minerals without\ndestroying the future. With the rising ozone degradation, the environment has to\nbe protected under whatever cost. The extracting companies ought to liaise with\nthe relevant bodies as they undertake their fracturing processes.\nStrategically, regulations should be put in place so that the companies\nundertaking the fracturing report and disclose promptly the type of chemical’s\nbeing used and the effect on the environment. The use of nontoxic chemicals\nought to be encouraged including nontoxic deliveries. At the same time, the\ngovernment and the federal bodies should ensure that the disposal of the waste\nwater that contains chemicals is done appropriately. In this, toxic waste\ntreatment facilities should be created that concentrate all waste products in a\ngiven place for efficient disposal. Strategies to recycle waste water to be\nused for re-drilling should also be put in place. All in all, the government should aim at\nensuring transparency in hydraulic fracturing and should put measures in place\nthat help identify the processes and effects of the hydraulic fracturing\nJ. R. (2011). Assessing the Risks and Benefits of Hydraulic Fracturing. Mo.\nEnvtl. L. & Pol’y Rev., 18, 170-208.\nR. W., Ingraffea, A., & Engelder, T. (2011). Natural gas: Should fracking\nstop?. Nature, 477(7364), 271-275.\nF. R. (2013). Environmental impacts of hydraulic fracturing. Boca Raton, FL: CRC\nR., & Monroe, J. (2015). Historical geology. Cengage Learning.\nThe post Research Paper Help on Hydraulic fracturing appeared first on Best Essay Services.\nOur writing company helps you enjoy campus life. We have committed and experienced tutors and academic writers who have a keen eye in writing papers related to Business, Management, Marketing, History, English, Media studies, Literature, nursing, Finance, Medicine, Archaeology, Accounting, Statistics, Technology, Arts, Religion, Economics, Law, Psychology, Biology, Philosophy, Sociology, Political science, Mathematics, Engineering, Ecology etc.']	['<urn:uuid:1f83221c-8671-49c1-977d-4659c320df50>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T23:00:04.658849	21	88	1606
77	Which cereal company used Wizard of Oz characters?	The Egg-O-See Cereal Company used characters from the Wonderful Wizard of Oz in their coloring book handout in 1906.	['Browse the children’s coloring book section in any book or toy store today and you are bound to notice that nearly all are associated with a recognizable character or franchise: Barbie™, Marvel Comics, Walt Disney™, and others. Their familiarity is engaging to children who relish the opportunity to interact with their favorite characters. This is not a modern phenomenon, children having long been drawn to characters they recognize.\nThe popularity of Palmer Cox’s Brownies and the success of related merchandise follows this pattern beginning in the 1880s, culminating in the publication of the primer in 1923. Cox not only marketed his illustrated books, he ran entire line of toys related to the Brownies.\nAs early as the 1890s, newspapers ran serial comic strips featuring specific characters who soon captured children’s imaginations. The idea behind comic strips was to increase newspaper sales to adults who would then share the paper. Children whose parents could not (or would not) buy the newspapers were still familiar with the comic strips, acquiring papers from other adults or friends willing to share. During the Great Depression, publishers reprinted comic strips into cheap coloring books, in an attempt to increase revenue.\nEarly advertisers used this principle to leverage children as a new market. The popular comic strip Buster Brown (first printed in 1902) was one of the earliest examples of synergy among advertising, comics, and character fame. Catalogs for consumer products soon featured new characters whose adventures entertained children in the same way as comic strips. Some marketed directly to children; others encouraged them to promote certain products to their parents.\nThe Dutch Boy’s Hobby, for example, establishes its hero on the cover. The subsequent pages build the story of the Little Dutch Boy and his Lead Horse fighting off General Trouble. Like other early coloring books, each page paired a colored and an uncolored picture with an accompanying\nIn this example, the National Lead Company, makers of house paints, meant for this to be a self-contained activity book. (No one understood the hazards of lead paint in those days.) It contains a page at the end with red, yellow, and blue pigment squares so that children only needed a wet brush to color. Finally, the book contains a perforated, tear away pamphlet for children to pass on to their parents. The entire booklet was an exercise in advertising and disposability: made of cheap materials, held together with staples, and intended to be colored and cut up.\nOther advertisers appropriated popular characters without permission, as in the case of these versions of the Tin Man and the Scarecrow from L. Frank Baum’s Wonderful Wizard of Oz. Wherever the characters came from, though, every single one of these give-a-ways marketed the chosen product on each page.\nThese familiar faces were found in a coloring book handout issued by the Egg-O-See Cereal Company in 1906. The caption below the picture reads “I wish we were really and truly alive so we could eat some of that good Egg-O-See and E-C-Corn.” Hood’s Sarsaparilla offered a painting book that includes a page with a version of Edward Lear’s popular poem “The Owl and Pussy Cat” (albeit with words that fit their message).\nThis form of advertising was especially popular in the 1920s, coinciding with coloring books into cheap, easily replaceable toys. These books are bound with staples and printed on cheap paper without board covers. By the end of the decade, children’s coloring books had achieved the form they take today. They were meant to be taken up, enjoyed, and abandoned.\nOak Hall Catalog – Click here to see another example of a catalog with coloring pages.']	['<urn:uuid:fc7a9de3-1d57-4f70-a59b-14bc840e95f7>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T23:00:04.658849	8	19	607
78	As an athlete who frequently experiences foot pain, I'd like to understand what stress fractures are and what activities put me at highest risk. What are the key symptoms of stress fractures, and which sports are most commonly associated with this condition?	Stress fractures are microscopic breaks in bones caused by repetitive overuse. The key symptoms include sharp pain while walking, mild swelling in the affected area, and discomfort when pressure is applied. The pain typically develops gradually, increases with weight-bearing activity, and diminishes with rest. Sports that commonly lead to stress fractures include tennis, track and field, gymnastics, basketball, and dance, particularly due to the repetitive pounding of the foot on the ground. These activities involve athletes whose feet repeatedly strike the ground with force.	"['What Is a Podiatrist?\nA podiatrist is a doctor of podiatric medicine (DPM), also known as a podiatric physician or surgeon. Podiatrists diagnose and treat conditions of the foot, ankle, and related structures of the leg.\nWhat Are the Qualifications of a Podiatrist?\nPodiatrists are the most qualified doctors to care for your feet. They complete four years of training in a podiatric medical school and three years of hospital residency training. This training is similar to that of other doctors. Podiatrists can specialize in many fields, including surgery, sports medicine, wound care, pediatrics (children), and diabetic care.\nAre Podiatrists Board Certified?\nPodiatrists can earn board certification with advanced training, clinical experience, and by ultimately taking an exam. The American Board of Foot and Ankle Surgery and the American Board of Podiatric Medicine are the certifying boards for the field.\nCommon Foot Problems:\nAchilles Tendonitis – The Achilles tendon is a resilient part of the foot, but it can become damaged due to overuse. It is common among athletes who do a lot of jumping, such as basketball or volleyball players, and runners. Symptoms: pain and stiffness in the morning; pain along the back of the heel, which worsens with activity; severe pain after exercising; swelling that gets progressively worse.\nAthlete’s Foot (Tinea Pedis)– This is a contagious fungal infection that can be difficult to eradicate completely. It is known as ""athlete’s foot” because the infection, a form of ringworm, spreads easily when feet are wet for a prolonged period of time, particularly after excessive sweating. Symptoms: itchy red skin; burning, stinging; oozing blisters.\nBunions – When your big toe points toward the second toe, it is known as a ""bunion.” Because of the unusual setting of the toes, bumps often form on the edges, causing pain or discomfort. Bunions can be hereditary or can be caused by other factors, such as wearing narrow-toed, high-heeled shoes. Bunions are more common in women. Symptoms: bony bumps; red, callused skin along the inside edge of the big toe; pain of the affected joints which can be aggravated by increased pressure.\nDiabetes – Diabetics are at increased risk for foot and ankle complications caused mostly by neuropathy. Neuropathy, or numbness in the body’s extremities, make it difficult for diabetics to know when they have suffered from a small wound, such as a blister. Because diabetes adversely affects the body’s ability to heal itself and prevention infection, even the smallest of wounds can have serious consequences.\nGout – Although gout can affect any joint of the body, it is most common in the joint at the base of the big toe. Gout is caused by an increased level of uric acid in the blood, which crystallizes and deposits into joints, tendons and other tissues. The increase of uric acid which causes gout is commonly caused by an improper diet that is rich in alcohol, meat and seafood. Although diet is the most common cause of gout, the condition can also be the result of genetics or metabolic diseases. An increase of ulric acid is also the culprit behind the development of kidney stones. Symptoms: swollen, tender, painful joint; fatigue; fever.\nPlantar Fasciitis – The arch of the foot is formed by a tight tissue. At times this tissue can become irritated and inflamed, causing plantar fasciitis. This condition is common in middle-aged people; young people who are on their feet often, such as soldiers or athletes; and those with high arches or flat feet. Symptoms: heel pain with prolonged walking or standing; swelling.\nSprains – Sprains are caused when the ligaments are either stretched too far or tear. They are common primarily among athletes or those who live an active lifestyle, although sprains can also occur as the result of a sudden injury. Symptoms: immediate pain following the injury; difficulty putting weight on the foot or ankle; swelling.\nStress Fractures – Stress fractures are small cracks in the bone that develop from overuse. Most stress fractures occur in the weight-bearing bones of the foot and lower leg and are particularly common among tennis players, track and field athletes, gymnasts, dancers and basketball players – athletes whose feet are repeatedly striking the ground with force. Symptoms: pain that develops gradually, increases with weight-bearing activity, and diminishes with rest; pain that is progressively severe; swelling at the top of the foot or outside of the ankle; tenderness.', 'What is a Stress Fracture of the Leg and Foot?\nA fracture (or broken bone) is a disruption, or break, in a bone. Some fractures, called stress fractures, are microscopic, and because they are not displaced (when the bone has shifted its position), they cannot initially be seen on regular x-rays. After time and healing starts to occur, the bone repair process shows up. Stress fractures occur as a result of repetitive overuse. In the foot, stress fractures usually are in the metatarsal bones in the ball of the foot, especially the second. Common sites for stress fractures in the leg include the tibia and fibula. Though 50 percent of stress fractures occur in the lower leg, stress fractures can happen to any bone in the body. Medical studies have shown that female athletes seem to experience more stress fractures than men.\nWhat is Metatarsal Stress Fracture?\nThere are five metatarsal bones, each of which travels out to a toe in the foot. The metatarsals connect with the tarsal bones in the hindfoot (rear of the foot) and the phalanges (toes) in the front of the foot. There are several kinds of metatarsal fracture, including stress fractures as well as more acute, and possibly multiple fractures of any of the five metatarsals.\nStress fractures (hairline breaks) are common in the metatarsals. They are usually caused by repetitive stress, such as with a new military recruit who has a sudden increase in his/her need to march. Other metatarsal fractures typically result from a direct blow to the foot, resulting from a trauma such as a fall from a height.\nCauses of Stress Fractures of the Leg and Foot?\nCauses of stress fracture include:\n•Training errors (in athletes) such as too rapid an increase in a runner’s mileage\n•Impact of an unfamiliar surface (a runner who has switched from a treadmill to a concrete path)\n•Stress caused by another foot problem, such as bunions\n•Wearing poorly cushioned shoes while walking on hard surfaces\n•Abnormal foot structure, such as flat feet\nIn sports like tennis, track and field, gymnastics and basketball, the repetitive pounding of the foot on the ground can cause stress fractures of the foot and leg.\nSymptoms of Stress Fractures of the Leg and Foot?\nGeneral symptoms of a stress fracture include a sharp pain while walking and mild swelling, in the affected area. It is uncomfortable when pressure is applied to the small area surrounding the site. Acute fractures may result in inability to put weight on the foot, swelling, and more severe pain. Even acute fractures usually have little displacement when they occur in a metatarsal bone.\nWhen you visit your physician with pain in the lower leg or knee, you will be asked questions about your injury and medical history. The doctor will physically examine the injured area, determining the cause of your discomfort. The doctor will want to know the following:\n•What hurts? This helps identify the affected area.\n•What happened? This helps determine the mechanism of injury.\n•What was new after the injury? With stress fracture, there is likely to be localized pain and tenderness.\n•Have you injured this part of your body before? This helps the doctor identify a chronic weakness or the likelihood of further problems.\n•What did it feel like at the time of the injury? With stress fractures, patients are usually unable to identify a precise moment of injury, but they see a pattern of recurring pain in a particular area.\n•Where did the injury occur, and under what circumstances? Stress fractures are closely associated with particular sports and activities, such as basketball, aerobics, and running.\n•Are you susceptible to stress fractures due to a known condition such as osteoporosis, anorexia, or obesity?\nIn analyzing the questions above, your doctor may be able to diagnose your injury strictly on the basis of a physical exam. The doctor will focus on the injured area, comparing it to the uninjured leg or knee. He or she will evaluate visible signs of injury such as tender skin, swelling, and differences in joint mobility or appearance.\nThe doctor usually presses and squeezes — palpates — the injured area. While this may produce pain, it helps locate the injury and any structural abnormalities. The doctor looks for:\n•Sounds of grating or grinding\n•Restrictions in the range of motion of the injured area\n•Inability of the injured part to bear weight\nX-rays usually miss small skeletal traumas such as stress fractures. Sometimes the doctor may advise a bone scan. Other tests providing three-dimensional images of skeletal trauma include computed tomography (CT) scans, and magnetic resonance imaging (MRI) scans.\nTreatment of Stress Fractures of the Leg and Foot?\nStress fractures are usually treated conservatively, without surgery. Rest of the injured area is essential, and usually requires suspending the activity that caused the injury for six to eight weeks. The treatment regime known as R.I.C.E., involving rest, ice, compression, and elevation, is an effective method for healing stress fractures. Sometimes a cast is required, especially if it may prevent a more serious fracture. Additional treatments include:\n•Stiff-soled or wooden shoes\n•Avoidance of high-impact activities\n•Keep weight off the foot\n•Use of ice\n•Use of a bandage\n•Use of a cast\n•Nonsteroidal anti-inflammatory drugs (NSAIDs)\nA highly competitive athlete may find it difficult to comply with the period of immobilization necessary to bring about complete healing of a stress fracture. Sometimes an orthopedic surgeon will recommend surgical treatment in these cases.\nRehabilitation After Stress Fractures?\nOnce the stress fracture heals, sports activities should be renewed gradually, with warm up and cool down exercises used before and after strenuous workouts. It is important to eat calcium-rich foods as part of a healthy diet. Replace any old or ill-fitting athletic shoes. Slowly increase any new sports activity. For example, do not immediately start running five miles a day; instead gradually build up your mileage on a weekly basis. Running can also be done on alternate days.\nEach fracture has its own distinguishing characteristics and treatment options, so physicians need to be attuned to both detection and management of these injuries. Medical studies show that women, especially female athletes, experience more stress fractures than do men. Many orthopedists explain this by referring to a condition known as “the female athlete triad.” Components of this triad are:\n•eating disorders such as bulimia and anorexia\n•amenorrhea—(infrequent menstrual cycles) due to rigorous exercise\n•osteoporosis (brittle bones), especially in post-menopausal women\nWhen a woman’s bone mass decreases, the likelihood of her getting stress fractures goes up.\nPeople with some form of metabolic bone disease, diabetes, rheumatoid arthritis, or cancer, as well as those who smoke or consume large amounts of alcohol, are also more likely to develop stress fractures. Preventing these injuries often requires proper management of other medical conditions, as well as changes in lifestyle.\nAthletes involved in tennis, track and field, gymnastics, basketball, and dance are all very susceptible to stress fractures. Participants in these activities should be aware of the risk, counterbalancing it as much as possible with rest between workouts, sufficient warm-up before and after exercise, and the use of proper shoes.\nThe absence of pain does not always indicate the complete repair of a stress fracture, and re-injury is common if patients return to sporting activities too soon. If the symptoms of stress fracture are recognized early and treated appropriately, allowing for complete healing, most people can eventually return to sports at their normal playing level.\nThe information provided herein is not intended to be a substitute for professional medical advice. You should not use this information to diagnose or treat a health problem or disease without consulting a licensed physician.\n©2000 DynoMed.com, LLC, Indianapolis, IN']"	['<urn:uuid:4fc9e654-b5a0-4dc1-b34d-b0619e6e5554>', '<urn:uuid:6d9fcbbc-db39-40b4-8d17-da44c74c604b>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T23:00:04.658849	42	84	2015
79	I'm researching historical record-breaking events. What's fascinating to me is how the Cumbre Vieja volcanic eruption's duration compares to the way Scripture was historically transmitted - are there any interesting parallels in how both set historical precedents?	Both indeed set historical precedents in different ways. The Cumbre Vieja eruption became the longest known eruption on La Palma, lasting 85 days (surpassing the previous record of 84 days set by Tajuya volcano in 1585), with reliable records only existing after Spanish colonization in the 15th century. Similarly, Scripture's transmission has a historical precedent documented in the Talmud, where Rabbi Yeremiah described how Samuel gave David a scroll that established three defining characteristics: it was 'in writing' (distinguishing it from Oral Torah), came 'from the hand of the Lord' (indicating inspiration), and was meant 'to be expounded' - characteristics that became historical precedents for identifying authentic Scripture.	"['The Bible provides minimal help for anyone trying to write a description of it for inclusion in a Statement of Faith. As a result, such descriptions typically claim more than the Bible discloses about itself.\nWhen formulating a declaration about Scripture, I recommend adhering to the following guidelines:\n1) Echo the language which Scripture uses to speak about itself.\n2) Reflect an appreciation of how ancient Jews viewed the Bible—the fountainhead of their literary heritage.\n3) Demonstrate an awareness of and appreciation for the achievements of text-critical scholarship, since they constitute a foundation on which all modern English translation rests.\nA key New Testament passage for discussing the nature of Scripture is 2 Tim. 3:16-17:\nAll Scripture inspired by God is also profitable for teaching, for reproof, for correction, and for training in righteousness, so that one who belongs to God may be competent, equipped for every good work.\nA reader well versed in the New Testament will recognize that the above translation reflects the content of a footnote appearing in some English editions. Preferring the note’s alternate wording for the purpose of this essay, I have rendered the passage’s opening phrase as “All Scripture inspired by God” as opposed to “All Scripture is inspired by God.”\nBehind the word “Scripture” stands the Greek word γραφή, which is related to the verb γράφειν (to write). Scribes transmitted Scripture by transcribing it. Being written, it was intended to be read (aloud). One can, therefore, characterize Scripture as having a scribal-literary quality. These two verses also establish a close bond between inspiration and profitability for teaching, reproof, correction, and training. When speaking about inspiration, I make a habit of speaking about Scripture’s profitability for instruction or training in the next breath. These concepts are two sides of the same coin and should not be separated one from the other.\nAt the end of the passage, the desired objective is stated: to prepare a person for an effective life of doing. Although not explicitly mentioned in the immediate context, teachers do have a role to play in the program. Scripture’s usefulness for teaching and training depends not only on the inspiration of the text, but also on that of the teacher. Just as a score of music is ultimately only as good as the conductor who leads, so it is the case with the Bible and those who teach and preach it. A popular rabbinic story about Ben Azzai makes a similar point by tapping the imagery of a different metaphor. On one occasion, while he sat and taught, fire glowed around him. Ben Azzai’s spontaneous combustion harks back to the giving of God’s fiery Torah. In other words, his auditors were witnessing a sublime event, which was less dramatic, but similar in essence to the one which the original recipients of the Torah at Mt. Sinai had experienced.\n1 Chron. 28:19 is a short verse about a text containing building instructions for the temple and its furnishings. It literally says: “All [the specifications of this plan] are in writing and they [come] from the hand of the Lord. I am responsible to explain [them].” At first glance, this verse seems to have little relevance for a discussion centering on 2 Tim. 3:16-17; however, in light of a Talmudic passage, both verses actually address similar issues.\nRabbi Yeremiah once taught the following in the name of another:\n[Consider] the scroll which Samuel entrusted to David. It was given in order to be expounded. What is the proof? All of this in writing—This [refers] to its scriptural-literary character. From the hand of the Lord—This [refers] to the Holy Spirit. I am responsible to explain—From this [we learn] that it was given to be expounded.\nThese remarks belong to a discussion about canonicity. For his part, Rabbi Yeremiah reminded his colleagues that the prophet Samuel gave David a scroll which possessed three defining characteristics of Scripture:\n1) The scroll was “in writing,” thereby distinguishing it from Oral Torah.\n2) The scroll came “from the hand of the Lord,” meaning that it was inspired like Oral Torah.\n3) The scroll was given in order “to be expounded,” meaning that it could serve as the objective of exegesis, thereby distinguishing it from Oral Torah.\nThe same elements are present in 2 Tim. 3:16-17. The Greek word γραφή (graphae) conveys the idea that Scripture is written. The Greek word θεόπνευστος (theopneustos, i.e., God breathed) parallels the idea of coming “from the hand of the Lord” (i.e., God delivered). The former is regularly called divine inspiration, whereas the latter could be described as divine manipulation. Interestingly, Rabbi Yeremiah attributed this manual act to the Holy Spirit. In Greek, the association of θεόπνευστος with the Holy Spirit (i.e., πνεῦμα ἅγιον) is easy to make, because of the shared etymology. The clause “for teaching, for reproof, for correction, and for training in righteousness” looks like an expanded, but equivalent way of saying “for expounding.”\nWe now have the benefit of consulting not only an early Christian epistle, but also a conceptual parallel from Talmudic literature before formulating a description of Scripture for inclusion in a Statement of Faith. Ideally, our declarations should echo the content of this old Jewish concept to which the New Testament author subscribed and which the editors of the Jerusalem Talmud included in their compilation. Being inspired (i.e., emanating from the Holy Spirit) and profitable for teaching, reproof, correction, and training are intrinsic attributes of Scripture. These two attributes allow the Bible to play an indispensable and salubrious role in the life of Jewish and Christian communities of faith. When an inspired (and learned) teacher expounds the biblical text, it becomes like a living spring whose cathartic and curative waters nourish, refresh, and stimulate the community, and no matter how often revisited, they remain plentiful and efficacious.\n-  An earlier version of this essay was entitled A Jewish Comment about Scripture. ↩\n-  Compare the NAB and NASB. ↩\n-  Another article that addresses this point is my “Toward an Inerrant View of Scripture.” ↩\n-  Lev Rabbah 16:4. ↩\n-  J. Meg. 70a (ch 1:1) (Krotoschin ed.). ↩\n-  I have benefited from Jose Faur, Golden Doves with Silver Dots: Semiotics and Textuality in Rabbinic Tradition (Bloomington, IN:Indiana University Press, 1986). ↩', '2021 Cumbre Vieja volcanic eruption\nThe 2021 Cumbre Vieja volcanic eruption was a volcanic eruption of Cumbre Vieja on the island of La Palma, one of the most active volcanoes in the Canary Islands, Spain. It is the first volcanic eruption on the island since the eruption of Teneguía in 1971. At 85 days, it is the longest known eruption of a volcano on La Palma.\n|2021 Cumbre Vieja volcanic eruption|\n|Start date||19 September 2021|\n|End date||13 December 2021|\n|Location||La Palma, Spain|\nMap of the lava flow on 23 November 2021\nAn earthquake swarm started on 11 September, which led to the volcanic eruption on 19 September.\nAround 300 people from the local area were evacuated shortly after the eruption.\nIt is the most damaging volcanic eruption on La Palma since records began.\nInitially, when the eruption started, it had a Volcanic Explosivity Index (VEI) of 0, when it was a purely lava eruption. But with the ashfall that began a week later, the Volcanic Explosivity Index of the eruption rose to 2.\nOn 28 September, at around 23:02 UTC, the lava flow reached the sea at the Perdido Beach.\nOn 20 November, the Volcanic Explosivity Index (VEI) was upgraded from VEI 2 to VEI 3, when the ashfall reached 10 million m³.\nOn 12 December, the volcanic eruption broke the local record, when it reached 85 days of continuous activity. The eruption is the longest known eruption of a volcano on La Palma. Previously, the eruption of the Tajuya volcano in 1585 was the longest at 84 days. No reliable data is available about earlier volcanic eruptions, before Spanish colonization of La Palma in the 15th century.\nOn 13 December, the volcanic eruption stopped. Noting weak seismicity and zero tremors, scientists said that a resumption of the eruption is highly unlikely, giving locals hope that it is over.\nOn 25 December 2021, after 12 days with no activity from the volcano, the eruption was officially declared to have ended.\n- ""Entra en erupción el volcán en La Palma"". El País. 19 September 2021. Retrieved 2021-09-19.\n- Parra, Aritz; Hatton, Barry (2021-12-15). ""After 3 tense months, Spanish volcano eruption may be over"". Associated Press. Retrieved 2021-12-20.\n- EFE (2021-11-20). ""El volcán de La Palma sube el índice de explosividad por la emisión de piroclastos"" [The La Palma volcano increases the explosivity index due to the emission of pyroclasts] (in Spanish). EFE. Retrieved 2021-11-23.\n- ""Overall Orange alert Volcanic eruption for La Palma"". Global Disaster Alert and Coordination System (GDACS). 2021-09-27. Retrieved 2021-09-27.\n- Carracedo, Juan Carlos; Troll, Valentin R. (1 January 2021), ""North-East Atlantic Islands: The Macaronesian Archipelagos"", in Alderton, David; Elias, Scott A. (eds.), Encyclopedia of Geology (Second Edition), Oxford: Academic Press, pp. 674–699, ISBN 978-0-08-102909-1, retrieved 10 October 2021\n- ""Spanish island volcano eruption hits local record of 85 days"". Associated Press. 2021-12-12.\n- ""Lockdown lifted for 33,000 confined on La Palma due to bad air quality from volcano"". El País. 2021-12-13.\n- ""Spanish Canary Island volcano erupts after weeks of earthquakes"". the Guardian. 19 September 2021.\n- ""Lava shoots up from volcano on La Palma in Spain\'s Canary Islands"". Reuters. 19 September 2021. Retrieved 19 September 2021.\n- ""La Palma island volcano erupts spewing lava, ash and pyroclastic debris, local evacuations begin"". The Canary - News, Views & Sunshine. 19 September 2021. Archived from the original on 21 September 2021. Retrieved 19 September 2021.\n- 20minutos (4 October 2021). ""Últimas noticias de las erupciones volcánicas en La Palma"". www.20minutos.es – Últimas Noticias (in Spanish). Archived from the original on 4 October 2021. Retrieved 4 October 2021.\n- ""The new volcano is the most damaging among the historical eruptions on La Palma"". Canarian Weekly. 20 October 2021.\n- ""La lava del volcán de La Palma alcanza el mar"". elDiario.es (in Spanish). 2021-09-29. Retrieved 2021-09-29.\n- Jones, Sam (2021-12-15). ""\'Tremor is zero\': La Palma volcano may be calming down"". The Guardian.\n- Trujillo, Marco (2021-12-16). ""Inside La Palma\'s volcano: lull in activity allows look into crater"". Reuters.\n- Vega, Guillermo (2021-12-15). ""La Palma volcano comes to stop: \'It is not emitting lava, nor sulfur dioxide, nor registering seismic activity\'"". El País.\n- ""Spain\'s La Palma volcano eruption declared over after three months"". BBC News. 2021-12-25.']"	['<urn:uuid:36a4fb66-f66a-40fd-9f59-feac60a7ab91>', '<urn:uuid:f25aa0a4-9888-46b9-8ba8-3f3845009027>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T23:00:04.658849	37	108	1752
80	As a gear manufacturing specialist, I'm wondering about the relationship between drill selection, high-speed gear operations, and tool performance - what are the key factors to consider when optimizing these elements?	"The optimization requires considering multiple factors. For high-speed gears, which can operate at up to 24,000 rpm and handle power ratings up to 70,000kW, tool selection is critical. When selecting drills, key considerations include the workpiece material (carbide performs better in abrasive materials while HSS works well in carbon steel), hole size (carbide drills above 1/2\"" are expensive, and indexable-insert drills are limited below 5/8\""), and machine tool capabilities (high-performance drills require rigid machine tools with minimal spindle runout). The toolholder interface is also crucial, as low-quality toolholders can allow drill runout, reducing geometric accuracy and tool life. For high-speed gear applications, these factors become even more critical as they affect power losses due to mesh friction, gear windage, and pumping effects between teeth."	"['speed increasers - Search Results\nArticles About speed increasers\nArticles are sorted by RELEVANCE. Sort by Date.\nWhen a gear set is to be designed for a new application, the minimum size gears with the required capacity are desired. These gears must be capable of meeting the power, speed, ratio, life, and reliability requirements.\nSeveral articles have appeared in this publication in recent years dealing with the principles and ways in which the inspection of gears can be carried out, but these have dealt chiefly with spur, helical and bevel gearing, whereas worm gearing, while sharing certain common features, also requires an emphasis in certain areas that cause it to stand apart. For example, while worm gears transmit motion between nonparallel shafts, as do bevel and hypoid gears, they usually incorporate much higher ratios and are used in applications for which bevel would not be considered, including drives for rotary and indexing tables in machine tools, where close tolerance of positioning and backlash elimination are critical, and in situations where accuracy of pitch and profile are necessary for uniform transmission at speed, such as elevators, turbine governor drives and speed increasers, where worm gears can operate at up to 24,000 rpm.\nThe power of high speed gears for use in the petrochemical industry and power stations is always increasing. Today gears with ratings of up to 70,000kW are already in service. For such gears, the failure mode of scoring can become the limiting constraint. The validity of an analytical method to predict scoring resistance is, therefore, becoming increasingly important.\nAre trains still a growth industry prospect for manufacturers?\nNew material technology allows for more efficient and flexible hobbing.\nThe newer profile-shifted (long and short addendum) gears are often used as small size reduction gears for automobiles or motorcycles. The authors have investigated the damage to each cutting edge when small size mass-produced gears with shifted profiles are used at high speeds.\nFor a high-speed gearbox, an important part of power losses is due to the mesh. A global estimation is not possible and an analytical approach is necessary with evaluations of three different origins of power losses: friction in mesh contact, gear windage and pumping effect between teeth.\nThe main theme of this article is high-capacity, high-speed load gears in a power transmission range between 35 MW and 100 MW for generators and turbo-compressors driven by gas or steam turbines.\nTo meet the future goals of higher productivity and lower production costs, the cutting speeds and feeds in modern gear hobbing applications have to increase further. In several cases, coated carbide tools have replaced the commonly used high speed steel (HSS) tools.\nHobs, broaches, shaper cutters, shaver cutters, milling cutters, and bevel cutters used in the manufacture of gears are commonly made of high speed steel. These specialized gear cutting tools often require properties, such as toughness or manufacturability, that are difficult to achieve with carbide, despite the developments in carbide cutting tools for end mills, milling cutters, and tool inserts.\nDepo provides all-in-one machining capabilities for the gear industry.\nAbove all, a gear is not just a mechanical transmission, but is developed to a system fulfilling multiple demands, such as clutch integration, selectable output speeds, and controls of highest electronic standards. This paper shows the basics for high-speed gear design and a selection of numerous applications in detailed design and operational needs.', 'October 2013 / Volume 65 / Issue 10|\nConsiderations when selecting a drill\nBy Christopher Tate, Savannah Machinery Works\nThe variety of drills on the market seems limitless, and each salesman says he has an offering that is better than the rest. The substrates, point geometries and coatings vary greatly—along with the price. I consider several factors when purchasing a drill and try to find the balance between price and performance that gives the best cost advantage while maintaining part quality and productivity.\nThe first consideration is the number of holes to be produced. A common misconception is that carbide is always the best substrate choice. An expensive, high-performance carbide drill may provide the highest penetration rate and shortest cycle time. However, if the number of holes being drilled is low, there may be a cost advantage to applying a less-expensive alternative, such as a HSS drill, because the tool cost savings can trump the increased cycle time.\nCourtesy of All images courtesy C. Tate\nAlso consider hole size. Carbide drills above ½ "" (12mm) in diameter are expensive, and indexable-insert drills have limited selection in sizes below 5/8 "" (16mm). Therefore, a HSS drill might be better for making holes from ½ "" to 1 "". On the other hand, an indexable drill may be more practical than a HSS or carbide drill for producing holes larger than 1 "".\nBecause high-performance carbide drills can hold tight size and geometric tolerances, I have opted for them when they eliminate a secondary operation, such as reaming. In addition, an indexable drill can be applied like a boring bar on a lathe. After drilling the hole, the indexable drill is stepped off center so the periphery insert enlarges the hole to the desired diameter, possibly eliminating the need for a boring tool.\nBesides reducing the number of tools by combining operations, reducing setup is a common goal when machining. If a drill can be used on a variety of parts, selecting an expensive, high-performance drill to extend tool life and minimize tool changes generally cuts costs. It may be possible to produce various hole sizes by drilling one size and circular interpolating with an endmill to enlarge holes that are larger than the drill diameter all in one setup.\nWorkpiece material is another consideration. A basic HSS drill performs well in carbon steel but tool life is poor when drilling abrasive materials like cast iron or die cast aluminum, justifying a costlier alternative like carbide. And P/M HSS drills effectively cut austenitic (300 series) stainless steels and similar materials. In these materials, P/M drills often have penetration rates similar to carbide and cost significantly less.\nMatching the drill to the machine tool is yet another consideration. High-performance drills require rigid machine tools that have spindles with minimal runout. Vibration from loose ways and excessive spindle runout can damage the fragile edges of carbide drills, causing premature failure. Large-diameter drills, especially large indexable drills, may require more torque than the spindles of some light-duty machines can supply. This scenario may require drilling smaller-than-specified holes and enlarging them via circular interpolation or boring.\nToolholders play a critical role in drill performance. The toolholder is the interface between the drill and the spindle, so it is important to balance this relationship. Low-quality toolholders can allow drills to run out, which reduces geometric accuracy and tool life. If the holder’s grip is insufficient, the drill can slip. This causes incorrect hole depth, putting other tools at risk of failure when they enter holes that are too shallow. Insufficient grip can also allow chatter, damaging the cutting edges on carbide drills and ultimately causing catastrophic failure. The best machine tool is only as good as the toolholder in its spindle.\nFinally, consider the machining environment and a shop’s practices. If the machinists, toolmakers and programmers are accustomed to using a particular type of drill, it may not be practical to introduce alternatives. For example, I tried to introduce high-performance carbide drills at a machine shop where I used to work. HSS jobber drills had been the standard. The operators applying the HSS tools were accustomed to running them at speeds many times slower than the speeds at which carbide drills can run. I was unable to change the culture, so there was no benefit to using costlier, high-performance carbide drills.\nJobs, work materials and production requirements vary greatly and there is no one definitive method for selecting the proper drill. The correct drill is the one that enables a high-quality part to be profitably shipped on time. CTE\nAbout the Author: Christopher Tate is manufacturing engineering lead for machining at Mitsubishi Power Systems, Savannah (Ga.) Machinery Works, a global builder of gas and steam turbines. He has 19 years of experience in the metalworking industry and holds a Master of Science and Bachelor of Science from Mississippi State University. E-mail: firstname.lastname@example.org.\nCUTTING TOOL ENGINEERING Magazine is protected under U.S. and international copyright laws. Before reproducing anything from this Web site, call the Copyright Clearance Center Inc. at (978) 750-8400.|']"	['<urn:uuid:99394bcf-2545-469c-b7a3-10a5a5a525f3>', '<urn:uuid:6069aebc-f3fc-4f27-b26c-867f7fe91a11>']	open-ended	with-premise	verbose-and-natural	distant-from-document	three-doc	expert	2025-05-12T23:00:04.658849	31	124	1406
81	I'm considering switching to a competitive energy supplier but I'm worried that if there's a power outage, no one will come to help. Who would handle emergencies in that case?	If you choose a competitive energy supplier, your local utility will continue to be responsible for the reliable delivery of electricity and natural gas, and will continue to respond to all service interruptions and emergencies.	"[""You may have heard a lot of things about energy choice, competition and suppliers. You may also be confused about what to believe. Let us help you sort the facts from the fiction.\nIf I choose to purchase energy supply from a competitive energy supplier, my local utility won't respond to my energy emergencies, like power outages or natural gas leaks.\nFACT: If you choose a competitive energy supplier, it is important to remember that the local utility will continue to be responsible for the reliable delivery of electricity and natural gas to the your home or business, and that the local utility will continue to respond to all service interruptions and emergencies.\nI’ve always liked my utility, and if I switch to a competitive energy supplier I will hurt them by taking business away.\nFACT: Utilities are regulated by state government (typically through public utility commissions), and their rates are controlled. Typically, utilities’ profits come from the delivery of your energy (known as “transmission and distribution”), not from the supply of energy. Switching to a competitive supplier will not harm your utility.\nI can switch to ANY competitive energy supplier and save a lot of money!\nFACT: Many suppliers do offer rates that are lower than the standard utility price. But energy choice is not solely about lower prices – in an open market, retail electricity suppliers can offer customers different products and services to meet individual energy needs, such as fixed or variable prices, renewable energy options, time-of-use rates, energy efficiency audits, and other value-added benefits. For some of these products, the rates may be higher, but what’s important is the value you get for your money.\nI have to choose a competitive energy supplier or I won’t get electricity or natural gas delivered to my home or business.\nFACT: If you choose to not switch to a competitive supplier, the utility will continue to deliver AND supply your energy. Remember, whatever you decide, you are making a choice – so it is worth your time to investigate all your options before making a decision.\nIf I switch to a competitive energy supplier, my bill will keep going up every month!\nFACT: Energy is a commodity – the price can change daily, even hourly, based on many different factors. This is true whether your supply comes from a competitive firm or a utility. The price that the utility or suppliers pass on to you, however, is more stable, and typically changes on a monthly basis. This means the price can go DOWN as well as up! If you would prefer more certainty, many suppliers offer partially or fully fixed price plans, in which your price is guaranteed. This is just another option available to consumers in an open market.\nIf I switch to a competitive energy supplier, I will have to pay two bills a month, instead of just one – it will be a major headache.\nFACT: In many states that have energy choice, customers can choose “consolidated billing,” in which their supply charges and delivery charges appear on a single bill. Depending on the state, this bill may come from either the supplier or the utility, but either way, you get one piece of paper and have to make just one payment. If you prefer two separate bills, in many states this is an option available to you as well.""]"	['<urn:uuid:b92237e9-98c7-495b-8fdc-33d25431c8a2>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T23:00:04.658849	30	35	564
82	How does the Woonerf design enhance safety and what's its history?	The Woonerf design enhances safety by creating a curbless environment that seamlessly merges streets and sidewalks, promoting eye contact and human interaction instead of relying on curbs, signs and rules. It gives equal priority to all modes of transportation. The concept originated in the Netherlands in the 1970s and 1980s, initially in dead end residential areas, where vulnerable road users legally have absolute right of way over vehicles, speeds are limited to 20 km/h, and the curvy road design makes speeding difficult.	['A Louisville, Colorado development incorporates Bomanite decorative\nconcrete products to seamlessly merge streets and sidewalks.\nBY SEAN O’KEEFE\nIN COMMERCIAL DEVELOPMENT, THE THIRST FOR SOMETHING NEW IS NEARLY CONSTANT. Developers, designers and builders all continually search for a competitive edge in an ultra-competitive industry to distinguish their product from a sea of sameness. In this quest for new and unique, certain successful developments become mile markers along the highway of innovation; noteworthy reference points defining a technical or aesthetic advance that inspires future design. Such is the expectation for a small mixed-use, multifamily community in Louisville, Colorado where development vision, design response and construction craftsmanship all combine to deliver a distinct, pedestrian-centric, multimodal community experience.\n“Placemaking really impacts the people that occupy a space,” says Paul Shoukas, Vice President and landscape architect with PCS Group, of the innovative streetscape at the Delo Apartments designed to enhance community connections. Located just east of Louisville’s downtown, the development objective for Delo was to foster easy pedestrian access to the city’s recently revitalized commercial district. Particularly challenging, the link between the town center and Delo is bisected by a railroad, a potentially huge impediment to pedestrians. The solution: a living street.\n“We borrowed from a Dutch design strategy called a Woonerf to create a curbless environment,” continues Shoukas of the site’s unusual, flat interface between the street and sidewalk. The design means to give equal priority to all modes of transportation including vehicular, bicycle, pedestrian and wheeled. By seamlessly merging street and sidewalk into a single, more or less flat surface, the idea behind the Woonerf is that eye contact and continual human interaction are a more effective means of creating a safe, pedestrian-friendly environment than curbs, signs and rules.\n“Rather than separating uses with barriers, the design integrates several really beautiful architectural concrete techniques, colors and patterns with small, raised truncated domes to delineate street and sidewalk,” says Shoukas. The combination of color and texture changes are accomplished through the use of Bomanite decorative concrete products, which were closely pre-planned and installed by local Bomanite licensed contractor Colorado Hardscapes. “The team at Colorado Hardscapes was extremely informative in the material selections we made. Their showroom and the number of different product samples we were able to see, touch and mix and match was incredible.”\nUltimately, the roadway at Delo incorporates Bomanite’s Sandscape® Texture, which combines locally sourced aggregates, black beauty sand and seed colored glass to create a granulated surface speckled with shiny jewels. Chosen for both durability and aesthetics, Sandscape’s textured effect is similar to what happens when concrete is sandblasted, except more uniformly controlled and the glass jewels don’t get frosted by the blasted sand in the process. At Delo, the only formal separation between road and sidewalk occurs through the use of steel plates covered in raised metal domes. Commonly used as an ADA measure where sidewalks slope to streets, here the raised domes will cause a warning rumble effect if a vehicle drives over them.\nDefining the work on paper and putting it into the ground are often two different challenges in the building industry. Incorporating architectural concrete into the site’s drivable surfaces meant completely rethinking the way the property was built. Often in greenfield construction, paved surfaces are placed first to provide immediate access for the heavy equipment and manpower required by vertical construction. Once the buildings are established, sidewalks and curbs are added, and finally, roads and parking lots are resurfaced with a topping layer when construction is nearing conclusion. When using decorative concrete as a drivable road, hard surfaces must be placed after heavy-equipment construction is complete to protect the beauty and integrity of the finished product. Adding to the complexity of the build, Delo’s public spaces also significantly incorporate concrete stem walls to account for elevation changes across the site and provide ample opportunity for stopping, sitting and chatting among neighbors.\n“We were working on Overture pre-COVID, so this was in development for more than three years,” continues Bender of an R&D process that stretched back to 2019. “We wanted to design a sensor that was very sensitive yet also affordable for homeowners. Achieving this came down to a combination of the constituents inside the product and the algorithm that links all these systems. Fundamentally, it’s our 90 years of experience in home ventilation that makes this possible.”\nBroan-NuTone’s Overture System enters the market either through home builders or the mechanical trades builders are accustomed to working with. The MEP often installs the ERV, bathroom fans, range hoods, and switches, and then hands Overture’s components off to a low-voltage technology inte¬grator that installs the sensors, before turning system control over to the homeowner and explaining how everything works.\n“For the industry, the important thing to understand here is that this is just another technology trade activity,” says Bender of product integration. “Installing these sensors is essentially the same as putting in a light switch. Just do it before the sheetrock goes up.”\nThe man responsible for overseeing Colorado Hardscapes’ work on the property was construction superintendent Rick Boer. Over more than 33 years at Colorado Hardscapes, Boer has seen his fair share of construction complexity. Between 2007 — 2009, he worked with Texas Bomanite, a licensed Bomanite Contractor in Dallas, on the installation of exposed aggregate concrete at the newly constructed Dallas Cowboys’ football stadium. Boer believes that the comradery and willingness to share technical knowledge and even employees from one contractor to another is an important part of Bomanite’s uncommon bond.\n“The most unusual thing on the Delo Apartments was the length of time we were on site,” notes Boer. Typically, decorative concrete is one of the last external elements built, but in the case of Delo, Boer and his team were among the first trades activated and the last to leave. Through an estimated 80 pours and 2,000 yards of concrete, Colorado Hardscapes laid more than 25,000 square feet of Sandscape for the road and another 35,000 square feet of Colored Sandscape sidewalks. Concrete pours continued for the entire time the apartment and retail/office buildings were under construction and all the way up to the end of site cleanup. “The concrete was here before the buildings were, so we really had to put an awful lot of trust in the surveyors and the markings they gave us to get things right.”\nThough the volume of horizontal installations wasn’t overwhelming compared to Colorado Hardscape’s previous experience, they were challenged to produce more than 500 linear feet of stem walls, many of which are curved, and all of which called for decorative finishes to match the overall design. Bomanite’s Sandscape Refined technique incorporated a Cappuccino color hardener with flecks of mirror and blue glass to add a sparkle to seat walls, planter walls and concrete columns.\n“Integrating a decorative concrete finish on a flat surface and getting it to stand up vertically are two different things,” says Boer with a slightly secretive grin. Typically, in vertical concrete construction builders set up formwork, pour the concrete, allow it to set for 24 hours, and strip the formwork off the next day. In order to add the decorative glass and aggregate mix to the vertical surface, at Delo the walls were only allowed to set to a certain point, and the formwork had to be stripped the same day. Immediately, the patented Bomanite chemical compounds and mix are added into the still-wet concrete by specially trained craftsmen. Working quickly and deliberately, skilled hands bond finish and substrate in an evenly blended mix that is more a slurry than a coating.\n“The Bomanite products, technical knowledge and commitment to innovation have led the decorative concrete industry since the start,” says Boer of the product support and resources available through the corporate offices and network of contractors across the country. He adds that he feels blessed to be a part of such a dynamic project that will surely be a reference point in both decorative concrete and pedestrian-friendly streetscapes for many years to come.', 'Vulnerable road users (VRU) are grossly overrepresented in road safety statistics. The World Health Organisation (WHO) estimates that close to 50% of all traffic deaths are VRUs. This means roughly 600,000 worldwide casualties per year. SWOV, the Netherlands’ Institute for Road Safety Research, defines VRUs as a group of road users that can be defined as ‘vulnerable’ in a several of ways. For instance by the amount of protection in traffic (e.g. pedestrians, cyclists and motorised two wheelers (motorcycles, mopeds and scooters) or by the amount of task capability (e.g. the handicapped, the mobility and orientation constrained, the young and the elderly).\nVRUs lack a protective shell (vehicle chassis) and adequate restraints (seat belts, airbags). These prevent the body from the full impact of a crash. Moreover, they are light in weight compared to heavier motorised road users, which means that VRUs are thrusted, often even launched, in case of high impact collisions. Besides the direct impact on their body, this also results in additional injury risk from the subsequent impact. This creates an inequality whereby the VRU is virtually always the party wit the highest number of casualties compared to the better protected and/or higher speed counterpart.\nFor non-motorised VRU the speed difference compared to motorised traffic is an additional risk factor. That is why motorcyclists, despite their helmets, run a particularly high risk: relatively high mass, together with high speed generates a lot of kinetic energy. This combined with the absence of a shell is bound to lead to high casualty and injury levels.\nDue to the better internal vehicle safety features, demographic characteristics and the increased popularity of (electric) cycling in many urban areas the number of VRUs is likely to increase as a percentage of road safety casualties. What can be done to reduce the number of VRU victims in road safety statistics?\nLower speeds save Vulnerable Road Users\nThe survival rate of a VRU (e.g. a pedestrian) decreases dramatically with an increasing collision speed (ref Figure 1). Almost all people survive a collision with a car at 30 km/h. For this reason this should be the preferred speed limit in residential areas. As 30 km/h it becomes very easy to brake and yield to VRU.\nAdditionally, lower speeds need strict enforcement by automated enforcement cameras or manually by the police.\nInfrastructure adaptation benefits VRUs\nSeparating traffic on different types of infrastructure hugely benefits road safety. This means pavements and cycle tracks besides the regular road. In this way the speed differences of the various types of traffic are spread over different infrastructure. This reduces collision and crash risk. Even on pavements and cycle tracks crashes and injuries occur. However, the severity is greatly reduced due to lower speeds, lower speed differences and separation of traffic. Here many crashes or injuries are one-sided, i.e. not involving another vehicle, due to design, bumping against poles or lack of maintenance (lose tiles). Such injuries are not registered as traffic crashes but ‘fall injuries on public roads’. As such they are not included in road safety statistics.\nHumps, bumps and plateaus\nSpeed humps, bumps or plateaus in combination with zebra or cycle track crossings of in streets prone to speeding are also considered to be effective in improving VRU safety.\nLiving street concept\nThe ‘(woon)erf’ or ‘living street’ concept was introduced on a large scale in the Netherlands in the 1970s and 1980s initially only in dead end residential areas. VRUs legally have the absolute right of way over vehicles in these areas. Speeds are mostly limited to 20 km/h and the curvy road design is such that speeding is made very difficult. Motorised vehicles are thus only a guest in areas where people live and children play. The living street concept is applied in many other countries.\nFigure 3 – ‘Woonerf’ or ‘living street’ and signage from the Netherlands and Sweden, where VRUs have priority, vehicle speeds are curtailed and yielding rights adjusted.\nVehicle related technologies\nVehicle manufacturers have recently introduced a range of VRU friendly technologies mosly aimed at pedestrians. When ITS-related, such features are called V2P (Vehicle to Pedestrian) technologies. Applications include side-underrun facilities, blind spot warning devices, pedestrian friendly front-end design pedestrian air bags, infrared pedestrian monitoring and detection (with autonomous emergency braking (AEB) system), pedestrian in crossing detection system (based on V2P communication). EuroNCAP reports that the fitting of effective pedestrian detection systems on passenger cars could prevent one in five fatal pedestrian collisions.\nDue to the fact that vulnerable road users represent almost half of the road safety fatalities, the impact of effective measures improving their safety are very important. Such measures come from three areas\n- Lower speeds (e.g. 30 km/h zones and enforcement)\n- Infrastructure adaptation (e.g. humps and plateau crossings, living streets)\n- Vehicle adaptation by introducing new in-vehicle or V2P technologies (infrared pedestrian detection or AEB).\n- SWOV Fact sheet – Vulnerable Road Users – https://www.swov.nl/sites/default/files/…/fs_vulnerable_road_users_archived.pdf\n- Euro NCAP – http://www.euroncap.com\n- Living Streets – https://en.wikipedia.org/wiki/Living_street\n- European Commission – Mobility and Transport – Road Safety']	['<urn:uuid:5a540311-0c04-4c01-b561-ebcc0583cb45>', '<urn:uuid:b5147f16-be64-4b0f-b8e1-0498cc2469ac>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T23:00:04.658849	11	82	2173
83	risk management transparency versus iso 27001 compliance benefits	Risk management promotes transparency by ensuring all parties are on the same page regarding project risks, allowing for informed planning and operational awareness among stakeholders. In comparison, ISO 27001 compliance benefits focus on providing verifiable evidence to customers and stakeholders about information security practices, offering global recognition as a reputable supplier, and demonstrating adherence to statutory and regulatory requirements including GDPR, ultimately helping organizations win more business through proven credentials.	['Risk Management: Your Foundation for Better Project Management\nDecember 16, 2020\nRisk assessments are often an afterthought. A company is required to be audited, and the auditor finds gaps or holes in its processes. That company then scrambles to make corrections. The auditor, as an objective third-party, may see things that the company misses or have seen more cohesive operations elsewhere – but his or her recommendations often come after the damage is done. Risk management resolves this issue by making risk assessments proactively and continuously.\nConsequences of Poor Risk Management\nWhen companies practice poor risk management, they aren’t just risking failing an audit. They are putting their time, costs, and quality at risk. Many end up feeling like they spend their days “fighting fires” and “managing” surprises, then find themselves unable to explain the ensuing delays to their shareholders, sponsors, or other stakeholders. The worst part is that because they do nothing to fix the root cause of these issues, the situation never gets any better, and the company loses money.\nBenefits of Risk Management\nProper risk management changes that prognosis. When your company puts into place a process for identifying and responding to project risks, it reaches new levels of transparency that allow all of the parties involved to be on the same page. Your company can use that operational awareness to inform planning, protecting time and quality. Furthermore, you create a situation in which you can improve your future study management, learning best practices.\nUnderstanding the Risk Management Process\nTo manage risk effectively, you need to have a process in place. It starts with formalizing and operationalizing the risk management process. This helps to establish your company’s priorities and identifying the risks associated with reaching those parameters. Next, you also want to conduct regular risk assessments. Ask yourself what could go wrong and try to determine the likelihood that there will be a negative outcome – and do so often. The most effective risk assessments happen frequently.\nOnce you’ve identified the risks you face, controls need to be put into place. Specifically, management and project decisions need to be made with the express purpose of mitigating the risks your team identifies. Finally, your company needs to reassess the situation, incorporating new information and developments.\nDeveloping a Risk Management Plan (RMP)\nOnce you understand the risk management process, you can build a risk management plan or RMP. This document details the study conduct as well as the potential controls that could be put into place. At Allucent, our project managers (PMs) write the RMP. It is reviewed and revised at each stage of the study. This including start-up, enrollment, treatment, follow-up, database lock, and closeout, as well as for any changes to the protocol. In addition, the PMs review study risks and the actions taken to mitigate those issues monthly at a minimum. High-risk studies may receive more frequent reviews. They share risk assessments with the associated Sponsors as well the team as a whole, so everyone understands the problems facing the study as well as the efforts taken to manage the risks.\nPutting It Together\nTo manage risk properly, it takes time and consistent effort. Risk assessments are not something to be completed on an ad-hoc basis. Moreover, overseeing risk well requires a plan. Many people will contribute to your company’s ability to mitigate risk within a study. Having an RMP in place outlines everyone’s role as well as the dangers of which they should be aware. Furthermore, as individuals take action to influence risks, these efforts are recorded and monitored. Ultimately, everyone involved can see what is happening and have the chance to work together transparently towards making the study a success.', 'What is ISO 27001?\nThe ISO 27001:2013 (also known as BS EN 27001:2017) standard provides a framework for an Information Security Management Systems (ISMS) that enables the continued confidentiality, integrity and availability of information as well as legal compliance. ISO 27001 certification is essential for protecting your most vital assets.\nISO 27001 implementation is an ideal response to customer and legal requirements such as the GDPR and potential security threats including:\n- Cyber crime\n- Personal data breaches\n- Vandalism / terrorism\n- Fire / damage\n- Viral attack\nThe ISO 27001 standard is also structured to be compatible with other management systems standards, such as ISO 9001 and it is technology and vendor neutral, which means it is completely independent of any IT platform. As such, all members of the company should be educated on what the standard means and how it applies throughout the organization.\nHelps you with\n- Asset protection\n- Security policy\n- Cyber security strategy\n- IT governance\n- Incident management\n- Threat mitigation\n- Downtime reduction\n- Loss prevention\n- Data breaches\n- Compliance checklist\n- Management system\nBenefits of ISO 27001 Certification\nGive customers confidence that their personal data/information is protected and confidentiality upheld at all times.\nAvoid downtime with management of risk, legal compliance and vigilance of future security issues and concerns.\nUnderstand how statutory and regulatory requirements impact your organization and its customers, whilst reducing risk of facing prosecution and fines.\nImproved risk management\nEnsure customer records, financial information and intellectual property are protected from loss, theft and damage through a systematic framework.\nProven business credentials\nIndependent verification against a globally recognized industry standard speaks volumes.\nAbility to win more business\nProcurement specifications often require certification as a condition to supply, so certification opens doors.\nGlobal recognition as a reputable supplier\nCertification is recognized internationally and accepted throughout industry supply chains, setting industry benchmarks for sourcing suppliers.\nIs ISO 27001 right for me?\nIt’s right for you and your organization if you need the evidence or assurance that your most important asset is protected from misuse, corruption, or loss.\nWe have certified organizations to ISO 27001 in a diverse range of sectors including Royal Mail Group, Smart Water Technology, Barcode Warehouse and the Northern Ireland Council for Curriculum, Examinations and Assessment.\nWhat is an ISMS?\nAn ISMS is a systematic approach to managing sensitive company information so that it remains secure. It includes people, processes and IT systems by applying a risk management process.\nIt can help any size organization within any industry keep business information assets secure.\nGDPR and ISO 27001\nThe General Data Protection Regulation (GDPR) has a much more extensive scope than the previous Data Protection Act (DPA) and has been introduced to stay in touch with the modern digital landscape. The Regulation affords more data rights to individuals and requires organizations to develop defined policies, procedures and to adopt relevant technical and organizational controls to protect personal data.\nThe GDPR applies to two types of users, of which we will undoubtedly all fall; Controllers and Processors. Briefly put; the controller determines how and why the personal data is used or processed and the processor acts on the controllers behalf, much like many organizations relying on the services of an IT service provider. Processors have more legal obligations placed on them in the case of a breach however a controller will be responsible for ensuring the contracts with the processor comply with the GDPR.\nThis is not a complete overview of the regulation and should not be used as such. Find out the key points and how they map to ISO 27001 here.\nSteps to Certification\nComplete a Quote Request Form so that we can understand your company and requirements. You can do this by completing either the online quick quote or the online formal quote request form. We will use this information to accurately define your scope of assessment and provide you with a proposal for certification.\nOnce you’ve agreed your proposal, we will contact you to book your assessment with an NQA Assessor. This assessment consists of two mandatory visits that form the Initial Certification Audit. Please note that you must be able to demonstrate that your management system has been fully operational for a minimum of three months and has been subject to a management review and full cycle of internal audits.\nFollowing a successful two stage audit, a certification decision is made and if positive, then certification to the required standard is issued by NQA. You will receive both a hard and soft copy of the certificate. Certification is valid for three years and is maintained through a programme of annual surveillance audits and a three yearly recertification audit.\nISO 27001 Toolkit\nNeed a Consultant?\nStart Your Journey\nISO 27001 Implementation Guide\nDownload Certification Logos']	['<urn:uuid:7cb755f0-aacd-4ef5-a64a-a17e8031e77d>', '<urn:uuid:68de8129-55a0-40be-9a63-d4b6f6df67cd>']	open-ended	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-12T23:00:04.658849	8	70	1417
84	Why did plants struggle to grow at Royal Wharf Eastern Terrace?	Plant life was poor in this area because the section was too low relative to the upper shoreline, causing the planted common reed to experience prolonged periods underwater and significant scouring. Additionally, fish and other aquatic life were feeding on the plants, making it difficult for them to grow and thrive.	['Royal Wharf, Eastern Terrace\nGrid Reference TQ4099979727\n1.1 Site Conditions Summary\n|Salinity (during low fluvial flows)||~7.5‰ (Wade, Hawes and Mulder, 2017)|\n|Date of construction||2014|\n|Percentage of area accreted above design level||60%|\n|Degree of exposure to waves:||High|\n|Degree of exposure to currents:||Medium (straight section of estuary)|\n|Max wave height|\n|Slope angle||Near horizontal normal and parallel to flow.|\n|Average Whole Structure lifespan||20-50 Years|\nTable 1: Site conditions summary.\n1.2 Site Characteristics\n- Formerly known as Minocco wharf, this previously industrial site was redeveloped into high density housing and apartments in the 2010s and rebranded as Royal Wharf.\n- Open estuary, currents are likely to be significant although the watercourse is fairly straight at this point running west-east.\n- The fetch of around 600m for the prevailing south-south-westerly could generate some minor wind waves.\n- This site is situated in a higher wash area for vessel and is likely to see waves of 1.2m. However, the river in this area is very wide with extensive foreshores in front of the enhancements which should help to dissipate the energy of the waves and likely not to see additional reflected wash.\n- The area is used by commuter vessels and most inland commercial operations including tugs and tows.\n1.3. What the developers did\n- The original fronting sheet steel pile wall cut down to foreshore level.\n- Sheet steel piles were used to form the rear flood defence wall just over 1.5m further inland. The rear flood defence wall was capped with concrete.\n- A foundation to the terrace was constructed of mass concrete, 1m in depth. This did not include additional drainage.\n- Timber was used to both clad the concrete foundation at the front and form a retaining wall by extending this wall upwards by 0.6m above the concrete found. This was the same format used as Deptford Creek Saxon Wharf and Deptford Creek Thanet Wharf.\n- The terrace was lined with geotextile and filled\n- Pre-planting of phragmites was conducted.\n2. Aerial photo and as-built drawings: plan view and cross-sectional view\nArtist’s impression combining ecology and engineering using a photograph not available for this site.\nDuring larger tides (spring) the terrace has water on it for over two hours and 22 minutes while smaller (neap) tides the terrace has water on it for 32 minutes.\nRoyal Wharf Western Embayment is immediately west of this site. It has had some limited biodiversity sampling conducted but not fish. Greenwich peninsula terraces north east, at approximately 2km further up estuary, is the nearest comparable site where all biodiversity sampling was conducted. It will be slightly less saline although is open estuary and will therefore be subject to the same fluid dynamics.\nFish weren’t able to be measured at this site as the site is too low (relative to the upper shoreline) and not very wide. See Greenwich peninsula terraces north east for the nearest comparable location.\nPlant life was dismal in this area, likely due to the section being too low (relative to the upper shoreline), as a result the planted common reed (Phragmites australis) were experiencing prolonged periods underwater and significant scouring. It is believed fish and other aquatic life are feeding on the plants, making it difficult for them to grow and thrive.\nInvertebrates were not sampled at this site. See Royal Wharf Western Embayment which is immediately west of this site.\nJust 30% of the accretable area has accreted with sediment above the design level (Figure 7).\nContributing reasons for this include:\n- High vessel wake and minor wind waves causing reflection of the rear wall (see earlier Figure 3).\n- Not recessed back from those current which exist.\n- Water logging: impermable concrete foundation, fill too shallow.\n- Lower elevation (0.3m lower) than the adjacent Royal Wharf Western Terrace and the inability for sediment to accrete upward with the high wave energy.\n4. Social, Litter, Safety and Navigation\nThe site was not publicly accessible at the time of the surveys and therefore no social surveys were completed as part of this review. It is recommended that these be completed once construction is complete.\nAlmost no litter was observed at Royal Wharf Eastern Terrace (see earlier Figure 3).\nContributing reasons for this:\n- Little to no trapping effect from vegetation or groynes combined with high wave energy, high wave reflection and some current energy.\n- Likely high degree of maintenance as construction is still underway and homes are still for sale.\n4.3 Safety and Navigation\nThe site is a considerable distance from the navigational channel across a shallow foreshore, and therefore protected by the lack of water depth in front of it. This means that the site is not a significant concern or navigational risk. Marking and lighting of the site is helpful and consideration of any change of use in the area.\n|Structural Element||Assessed Condition||Expected lifespan||Recommendations|\n|Sheet steel pile wall and concrete capping||Has a design life of 110 years.||> 50 years||No comments.|\n|Mass concrete gravity retaining wall.||Has a design life of 110 years.||> 50 years||No comments.|\n|Existing sheet steel piles cut down to foreshore level*||Questionable as to whether this is integral to the design||20 to 50 years||Detailed inspection (including pile thickness measurement)\nand comparison of existing foreshore level and prediction of future foreshore levels is recommended.\n|Recycled timber cladding/retaining wall||Inspector commented that this has no structural integrity but this forms the retaining wall for the sediment.||Not assessed.||No comments.|\n*where it was not possible to assess the element, assumptions have been made that it was previously designed for inter-tidal environment with a 50-year design life but has since deteriorated.\nTable 2:Table showing individual engineering elements of Royal Wharf Eastern Terrace and their expected lifespan.\nPotential engineering improvements/ refinements\n- Cutting down pre-existing structures to provide toe details presents a risk due to uncertainties/ lack of available information on original date of installation, design details and design assumptions (with respect to issues such as loading and assumed minimum foreshore levels). It is therefore difficult to predict residual life of these vital structural elements.\n- Avoid vertical structures, in particular interfaces/ corners between vertical structures, which result in wave reflection and localised erosion.\n- Avoid narrow intertidal terraces immediately adjacent to vertical reflective structures which are likely to result in localized scour and prevent establishment of intertidal habitat.\n- Design intertidal terraces to consider potential intertidal habitat zones, wave exposure (including vessel generated waves) and future climate change (sea level rise and increased fluvial flow).\n- Provide greater depth of gravel/ alluvium and drainage on terraces.\n- Install rock rolls/ gabions: i) immediately inshore of steel sheet toe piles; ii) riverward of rear steel sheet piles and iii) at interfaces/ corners between vertical structures to reduce scour due to wave reflection off vertical structures.\n- Install pre-grown intertidal habitat planting (which is tolerant to wave action) with suitable substrate.\n- If necessary, replace softwood with recycled hardwood throughout to increase residual life.\n- Establish a suitable monitoring and maintenance regime for the intertidal habitat/ structures/foreshore (monitor foreshore/ nearshore levels to identify any future potential risk of lowering which could result in undermining of the river wall at the toe).\n6.1 Within the first 4 Years (2014 to 2018)\n- There is limited biodiversity data to prove how well this site is performing, but the lack of vegetation and scour suggests that it is not performing well.\n- Improvements would be:\n- A deeper terrace (horizontal distance) combined with rock armour and/or biodegradable geotextile on the fill surface to succeed in the high energy open estuary environment. This guidance has been added to the geomorphology\n- Deeper fill and therefore drainage. The impermeable relatively shallow mass concrete foundation was also found to be a problem for drainage at Deptford Creek Saxon Wharf and to a small degree at Deptford Creek Thanet Wharf, Unlike Deptford Creek Thanet Wharf, the south facing nature of this site did not outweigh the poor drainage. This guidance has been added to guidelines specific to vegetated intertidal terraces.\n- Terraces in the high energy open estuary must be constructed to the correct level as there is no opportunity for sediment accretion to form a higher level. This guidance has been added to guidelines specific to vegetated intertidal terraces.\n- The site performed well for litter.\n- The site performs well for safety and navigation.\n- A monitoring regime is required.']	['<urn:uuid:35049ac3-0c30-4b67-9e73-347e67c60aee>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T23:00:04.658849	11	51	1389
85	What exactly happens in the body when someone gets lupus?	In lupus, which is a chronic autoimmune disease, the body's immune system mistakenly attacks its own healthy cells and tissues instead of fighting off foreign substances like bacteria and viruses. This immune system malfunction can cause damage to multiple parts of the body, including the joints, skin, blood, kidneys, heart, lungs, and brain. When lupus affects the kidneys specifically (lupus nephritis), it attacks the nephrons that are responsible for filtering blood and removing waste products.	"['How Do I Know I Have Lupus Nephritis?\nSystemic lupus erythematosus refers to lupus and it is a health condition, where the immune systems start attacking varying areas in your body. Especially, patients deal with a severe complication i.e. lupus nephritis and it takes place whenever SLE forces the body’s immune system to attack kidneys, particularly nephrons responsible to filter blood and eliminates waste products.\nHow Do I Know I Have Lupus Nephritis?\nYou may identify the problem of lupus nephritis or kidney disease based on following common symptoms.\nKidney disease often remains silent with very few or almost no symptom. Furthermore, symptoms related to lupus nephritis may vary largely among different people.\nHowever, nephrologists have identified a few common symptoms, which include the following-\nFoamy Urine: Foamy urine is one of the first signs related to the problem of lupus nephritis. The problem indicates the presence of protein in the urine, because of which doctors often recommend for diagnosis procedure.\nBlood in Urine: Blood in urine is another major symptom associated with the problem of lupus nephritis. In this situation, a few drops of blood pass through the urine because kidneys and a few other parts of one’s urinary tract force blood cells to cause leakage in the urine.\nDark Urine: Another symptom to indicate lupus nephritis is dark urine i.e. urine in brown, maroon or dark yellow.\nPain and Swelling: In addition, a few individuals with lupus nephritis may even experience mild to severe form of pain in their middle back or flank regions. Alternatively, others feel pain in the abdominal areas. Other than pain, you may deal with swelling in your ankles, feet and legs, while slightly in your face and hands. The problem tends to worse with progression or sitting resulting in edema problem.\nHigh Blood Pressure: Kidney problems, including the common issue of lupus nephritis may result in high blood pressure or hypertension problems.\nFrequent Urination: Some of the lupus nephritis patients experience increase in urge to urinate than before or frequent urination.\nVomiting: As your kidneys fail to remove toxins from your body, it leads to toxins buildup inside and thereby, leads to abnormal function of your liver and the overall digestion process.\nRetention of Water and/or Weight Gain: Because of malfunctioning of kidneys, your body bloats to affect your hands and legs highly as compared to other body parts. In addition, as kidneys fail to remove additional water present in your blood, you experience weight gain and other related health problems.\nDiagnosis of Lupus Nephritis\nOnce you identify any of the mentioned symptoms, you have to consult with nephrologists to undergo with either of the diagnoses, as mentioned here.\nBlood Tests and Urine Tests: Blood tests allow your doctor to check the elevated levels of urea, creatinine and other waste products filtered by your kidneys. On the other side, urine tests identify the level/percentage of white blood and red blood corpuscles and protein in the urine to judge the kidneys’ functions.\n24-Hour Urine Collection: This is a type of urine collection test and it measures ability of the kidney to filter waste products in a selective manner and determines the level of protein in urine during the period of 24hours.\nIothalamate Clearance Test: To judge the filtration functions of kidneys, doctors recommend for radioactive iothalamate clearance test. Accordingly, doctors inject radioactive iothalamate (a type of contrast dye) in the blood to test how the kidneys excrete it from your urine. Nephrologists consider the clearance test as the most accurate diagnose to judge the ability of kidney filtration.\nKidney or Renal Biopsy: Kidney or renal biopsy is also an accurate and invasive way to diagnose lupus nephritis or any other related kidney problem. Accordingly, doctors insert a big needle from your abdomen into the kidney to collect a sample of your kidney tissue and analyze signs related to kidney damage.\nTherefore, with awareness related to major kidney problem symptoms and proper diagnoses, you will expect to identify lupus nephritis and kidney problem early to undergo with the essential treatment.', ""What is lupus it is a disease where the body’s defense system attacks healthy cells and tissues, causing damage to many parts of the body. What are autoimmune diseases these diseases occur when your immune cells attack your body by mistake these diseases can affect almost any part of the body. What is lupus lupus is an autoimmune diseasethis means that your immune system attacks healthy cells and tissues by mistake this can damage many parts of the body, including the joints. Lupus is a chronic inflammatory autoimmune disease there are three common types of lupus. Lupus is a chronic autoimmune disease that can damage any part of the body chronic means that the signs and symptoms tend to last longer than six weeks.\nSystemic lupus erythematosus (sle), also known simply as lupus, is an autoimmune disease in which the body's immune system mistakenly attacks healthy tissue in many parts of the body. Autoimmune diseases are where your immune system attacks healthy cells in your body by mistake learn about the symptoms, tests, and treatments. Researchers think that our microbiome might play a role in the autoimmune disease lupus. Lupus is an autoimmune disease in a normal, healthy immune system the body recognises and destroys foreign objects like bacteria and viruses with autoimmune diseases, however, the immune. Find out which autoimmune disorders are most common, such as lupus, psoriasis, and multiple sclerosis, and get information on autoimmune disease. Lupus is a type of inflammatory disease that results from the immune system attacking its own tissues it can come on very strong with symptoms being easy to spot, or it can develop slowly.\nAutoimmune illnesses occur when your body's own immune system, which is supposed to fight off anything foreign (not self), starts attacking your own b. An autoimmune disease is defined as there are 40 other diseases thought to be autoimmune in nature autoimmune disease is one of cushing's disease, lupus. Lupus is one of many disorders of the immune system known as autoimmune diseasesin autoimmune diseases, the immune system turns against parts of the body it is designed to protect. Lupus is an autoimmune disease, which means the body attacks its own cells and organs—including the skin, joints, heart, lungs, blood, kidneys, and brain—because it can’t differentiate.\nLupus is among the ranks of rather serious illnesses that aside from the name, not too many people know about – which is surprising when you know that 15 million americans suffer from it. Lupus is an autoimmune disease that causes swelling and a variety of other symptoms, ranging in severity lupus effects everyone differently, and many of the common lupus symptoms may be. Lupus lupus is a chronic disease that causes the immune system to attack the body’s own tissues and organs, resulting in inflammation of the skin, joints, blood, and kidneys.\nEmail aarda autoimmune disease lupus lyme disease is dedicated to the eradication of autoimmune diseases and the alleviation of suffering and the. What is lupus nephritis lupus nephritis is a type of kidney disease caused by systemic lupus erythematosus (sle or lupus) lupus is an autoimmune disease—a disorder in which the body’s.\nCbd from hempworx can help manage several autoimmune diseases read reviews from people with lupus that have taken our herbal drops. Autoimmune disease definition, a disease resulting from a disordered immune reaction in which antibodies are produced against one's own tissues, as systemic lupus erythematosus or rheumatoid. I wanted to know if anyone here with lupus is rh negative i have talked to a few people who have lupus and they all have been rh negative i am rh negative myself and have had some symptoms. A doctor who treats arthritis and other rheumatic diseases, such as scleroderma and lupus some autoimmune diseases if you have an autoimmune disease. Lupus as the prototypical autoimmune disease, lupus occurs when the immune system malfunctions the immune system is designed to protect the body from foreign invaders, such as bacteria and.\nA chronic and complex autoimmune disease, lupus can affect the joints, skin, brain, lungs, kidneys, and blood vessels, causing widespread inflammation and tissue damage in the affected. See a picture of and learn about acute systemic lupus, a type of autoimmune disease, in the emedicinehealth image collection gallery. See a picture of and learn about systemic lupus erythematosus, a type of autoimmune disease, in the emedicinehealth image collection gallery. Overview of autoimmune diseases, including a list of examples of autoimuune disorders and links to useful web resources. Lupus erythematosus is a collection of autoimmune diseases in which the human immune system becomes hyperactive and attacks healthy tissues symptoms of these diseases can affect many. By learning the symptoms of lupus, a rare autoimmune disorder, girls may be able to lead their doctors to a correct diagnosis at an earlier age.\nAll Rights Saved.""]"	['<urn:uuid:eb26ce5b-7306-4f78-89d1-657335c0ae3a>', '<urn:uuid:7e9ce7c7-5238-4207-9aca-91ab9dac0524>']	open-ended	direct	concise-and-natural	distant-from-document	three-doc	novice	2025-05-12T23:00:04.658849	10	75	1479
86	rubicon crossing consequences ancient rome	When Caesar crossed the Rubicon river with his troops in 49 B.C., it was effectively a declaration of war against Rome. This action caused chaos in Rome, as Caesar was backed by a loyal army forged through 10 years of military campaigns. In response, Pompey and the aristocracy fled to Greece to gather a more powerful army. The subsequent battle of Pharsalus in 48 B.C. resulted in Pompey's defeat, who then fled to Egypt where he was beheaded by the king.	['Facts about Julius Caesar: between History and Myth\nBy the time of Julius Caesar in 50 B.C. the Roman Republic was living a time of deep crisis. These were years marked by revolts and political clashes. Murders and violence were the order of the day, and the republican government seemed to have been closer to anarchy than ever before.\n|Credits: image by @efrye|\nCaesar’s personal ambitions\nJulius Caesar belonged to one of the most noble families in Rome, the gens Iulia. However, they had lost most of their fortune and were burdened by debts. Caesar’s dream was to give back to his family the lost honor and the power that once belonged to the gens Iulia.\nIn 65 B.C, Caesar was no longer a boy. He was 32 years old and had already shown his value as a soldier. During these years he was sent to Rome’s provinces in Spain to stop an insurrection. On this occasion, Caesar proved his strength, his value and impressive strategic skills. The military campaign in Spain also had political implications. For Julius Caesar, building his own reputation among the ranks of the army in an empire found on military supremacy was just the first step to acquire a political influence.\nHis final goal was indeed to become consul, the most prestigious office in Rome. This way, not only he could have achieved great powers, but also the governorate of one province. Caesar’s object of desire became the Gaul, one of the richest provinces of Rome. He knew that he could have made a lot of money out of a victorious campaign in Gaul, and he was eager to escape from his creditors in Rome.\nThe Run for Office\nWilling to do everything it took to reach his goals, Caesar started showing another interesting trait of his personality. Not only he was a great soldier, but he was a leader too, able to reach, involve and persuade the audience. His populist campaign inspired many. Among them there was Brutus, a young man who quickly become Caesar’s favorite.\nHowever, by achieving his goal to conquer the favor of the masses, Caesar soon lost the support of the Roman aristocracy and the of senators, who saw his innovative ideas as a threat to the status quo.\nCaesar knew his weaknesses. He also knew that, notwithstanding the electoral results, the Senate would have still had the power to refuse him the governorship of Gaul. So, how do you win an electoral campaign and the governorship of the richest province of the time without the favor of the nobility and of the senate?\nFor Julius Caesar the answer was pretty obvious! He needed allies and not just any ally. He turned to two of the most important men in the Roman Republic, Crassus and Pompey Magnus. The first one, Caesar’s longtime friend, was also the richest men in Rome, while Pompey was one of the most popular general at the time.\nThanks to his power of persuasion, Caesar convinced both of them to create an alliance, the first Triumvirate, and to seal the contract he gave his daughter Julia in marriage to Pompey.\nCaesar won the consulate and became governor of two Gaul provinces for 5 years. All the three of them had become very powerful. Once he had reached his goals, Caesar left Rome and headed North looking for gold and fame.\nMilitary success in Gaul\nDuring his time in Gaul, Caesar and his army fought against barbarians threatening Rome. In 58 B.C the Romans faced the Helvetians. The enemies were annihilated. By that time, Caesar’s plans have changed. Now he was aiming at expanding the Roman empire and conquering new lands.\nMeantime in Rome, Caesar’s expansionist ambitions severely concerned the senators, who tried to convinced Pompey to break the alliance. Although the antagonism between the two of them had grown strongly over the past years, Pompey was more concerned with his personal life. He had married Giulia and was very in love with her. Together they traveled a lot across Italy, and Pompeo started neglecting politics to spend time with his beloved wife.\nCaesar’s military successes continued in Gaul. He defeated the barbarians led by Ariovistus and brought the Roman troops up to Britain. He was the first Roman to ever enter those lands. However, the dispatches he sent to Rome only increased the senators rivalry. Also Brutus himself started questioning the real intentions of his mentor and doubting the legitimacy of Caesar’s growing power. By that time Caesar could rely on a trusty army and he was collecting great wealth.\nThe time had come for the senators to stop Caesar, who in their minds had become the major threat to the status quo and the aristocracy. A series of fortuitous coincidence allowed the Senate to succeed. Crassus, longstanding supporter of Caesar, died during a military campaign. At the same time, Julia, Pompey’s wife, died in childbirth, along with their baby-son. Pompeo was devastated and probably felt like he had nothing to lose. He broke the alliance with Caesar and became the champion of the Roman aristocracy and conservatorism.\nIn the meantime Caesar’s military successes continued. The Romans had conquered the North of Europe. His last desire was to go back to Rome and see his successes recognized. In 49 B.C. the senators asked Caesar to go back to the capital as a private citizen, which meant he had to give up his army. Caesar knew that by doing so he would have faced a sure death. He was ready to do anything it took, also to invade Rome, if necessary.\nLeading his troops, he moved south from Gaul and reached the Rubicone river, which basically marked the border between the provinces and Rome itself. Caesar knew that crossing the border with his troops meant a declaration of war, but he had no choice and did it anyway.\nChaos spread all over Rome. Pompey and the aristocracy knew that Caesar was backed by loyal soldiers, a unified army forged by 10 years of military campaigns. To take time, Pompeo went to Rodi, Greece, to put together a new, more powerful army.\nThe battle of Pharsalus in 48 B.C. marked the defeat of Pompey’s troops. He fled to Egypt seeking refuge, but he was stopped and betrayed by the king, who beheaded him and sent his head to Caesar.\nThe Ides of March\nIn 47 B.C. Caesar had reached an undisputed and absolute supremacy over Rome. He started the reconstruction of the city and funded military campaigns to conquer new colonies. In the middle of the Republican age, Caesar was the first one to foresee the destiny of Rome as a great, real Empire. However, the dictatorship was causing unrest in Rome. Brutus was persuaded by the Senators to stop him and became the champion of the conspiracy that caused Caesar’s assassination.\nOn 44 B.C, the Ides of March, 40 plotter led by Brutus killed Caesar with 23 knife wounds in the halls of the Theater of Pompeo. His body was then carried to the area known today as the Roman Forum and there it was cremated.']	['<urn:uuid:f2f530af-3dc4-43e7-a37d-2658b1924cce>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T23:00:04.658849	5	81	1189
87	What is known about the size and significance of the art collection discovered in Munich, and how many pieces are suspected of being looted?	The collection discovered in Munich in 2012 contained works by renowned artists including Pablo Picasso, Henri de Toulouse-Lautrec, Oskar Kokoschka, Canaletto, Pierre-August Renoir, Franz Marc and Gustav Courbet. Experts described the collection's value as 'so high it cannot be estimated.' According to current information, at most 3% of the 1,280 confiscated works (approximately 38 paintings) could be involved in looting claims.	"['- A stunning art collection confiscated from Cornelius Gurlitt has been returned to him\n- He has agreed to return works to their owners if it\'s proven they were stolen by the Nazis\n- Gurlitt says he inherited the collection from his father and was unaware any were suspect\n- The huge art trove was found in his Munich apartment by German tax authorities in 2012\nPart of an art collection confiscated because it may contain pieces looted by the Nazis is being returned to its owner, prosecutors in the German town of Augsburg said Wednesday.\nProsecutors said they had returned the seized collection to Cornelius Gurlitt, who inherited the almost priceless artworks from his father, in light of new evidence.\nTheir statement came two days after news of a deal between Gurlitt, Germany\'s cultural authorities and the Bavarian Justice Ministry under which he agreed to allow research into the origins of suspected cases of looted or so-called ""degenerate"" art.\nUnder the deal, works owned by Gurlitt which are not under suspicion can be returned to him. Those suspected of being stolen will be held securely while a task force investigates their provenance -- and will be returned to their original Jewish owners or their descendants if a claim is proven.\nWorks which have not been investigated by the end of a year will be returned to Gurlitt, it says.\nIt\'s not clear how many pieces may be the subject of claims from those who believe they were looted decades ago.\nBut a statement on Gurlitt\'s website says that ""according to current information at most 3% of the 1,280 confiscated works"" could be involved. That would work out to about 38 paintings.\nGurlitt\'s spokesman, Stephan Holzinger, said Monday that there was a second claimant for a Henri Matisse painting found in his hoard, ""Femme assise"" or ""Seated Woman.""\nGurlitt\'s lawyer has said the claim must be properly examined before the painting can be returned. However, this doesn\'t change Gurlitt\'s intention to return pictures to their rightful owners if a claim is proven, Holzinger said.\nAmong the staggering haul found in Gurlitt\'s Munich, Bavaria, apartment in early 2012 were paintings by Pablo Picasso, Henri de Toulouse-Lautrec, Oskar Kokoschka, Canaletto, Pierre-August Renoir, Franz Marc and Gustav Courbet.\nGurlitt, 81, says he never suspected that the collection he inherited from his father might include stolen artworks.\nThousands of pieces of art condemned as ""degenerate"" by the Nazis were confiscated from galleries and private collectors in the 1930s and 1940s. Other works were stolen from Jewish families or sold for a fraction of their true value as the owners tried to flee the country.\nGurlitt\'s father, Hildebrand Gurlitt, who was part Jewish, survived the war as one of only four ""degenerate art"" dealers permitted by Adolf Hitler.\nProsecutors cite new evidence, objections\nIt was last May when experts revealed that the artworks -- many long feared lost or destroyed, and some which had never been recorded -- had been discovered in Gurlitt\'s apartment in the Schwabing area of Munich.\nThe vast collection, which experts have said has ""a value so high it cannot be estimated,"" was recovered by German tax authorities in connection with an inquiry into tax evasion in February and March 2012.\nIn its statement, Wednesday, the prosecutor\'s office defended its decision to seize the cache, now known as the Schwabing art trove, on the grounds it might contain stolen art.\n""At the time of the confiscation of the entire collection, the Augsburg prosecution was entirely convinced of the rightfulness of the measure,"" it said.\nBut, it said, the new evidence and the representations of Gurlitt\'s defense lawyers have led the office ""to assess the legal situation anew.""\nThe investigation is not yet concluded, however, the prosecution said.\nThe defense has applied to view more files and has yet to make its position public, it added.\nGurlitt told German magazine Der Spiegel last November that he longed for the paintings he inherited to be returned to him -- and that he regarded them as his only friends.\nThe reclusive retiree also defended his father\'s wartime dealings, saying ""maybe he was offered something privately, but he would not have taken it. It would not have been good for him.""\nHis father, by all accounts a savvy dealer, removed the huge collection from his house in Dresden during the devastating bombing in the final days of World War II, and moved it to what later became West Germany.\nA statement on Gurlitt\'s website states that he ""was at all times convinced that he had inherited a collection from his father that predominantly consisted of so-called degenerate art from former German Reich property in public collections and museums,"" and that he was unaware it ""includes a few works that today can be qualified as looted art.""\nNow, it says, he is ""is prepared to review and arrive at fair solutions together with the claimants for those works that are suspected of being looted art in such instances where qualified, documented, and justified claims for their return are asserted by heirs of Jewish of persecution and where morally compelling grounds exist.""']"	['<urn:uuid:9d2bbedc-1e20-483d-982b-d59bdaede262>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T23:00:04.658849	24	61	855
88	Why did Penn dining halls stop Black History Month meals?	Bon Appétit, the dining hall management, decided to avoid serving special Black History Month meals this year because they wanted to prevent the embarrassment faced by other institutions that had been criticized for serving stereotypical 'soul food' meals featuring watermelon, Kool-Aid, and fried chicken. However, recognizing disappointment from staff and students, the university planned to have a dinner highlighting famous black chefs later that week.	['About 75 University of Pennsylvania students gathered in the blustery cold Monday for what they called a “solidarity” gathering to support workers at Penn Hillel’s Falk Dining Commons, who said they were told they could not make any special meals in honor of Black History Month this year.\nTroy Harris, a longtime cook at the dining hall, said Bon Appétit, the dining hall management, had recently informed him and other workers that the dining hall didn’t want to suffer the embarrassment of institutions that had come under fire for serving stereotypical “soul food” meals featuring watermelon, Kool-Aid, and fried chicken.\n- Superintendent William Hite: Philly schools need diverse curriculum and diverse teachers | Opinion\n- In what’s so far been a dumpster fire of a Black History Month, Philadelphia classroom doors are a much needed bright spot | Helen Ubiñas\n- Philly renames parts of Market and Sixth Streets in honor of founding fathers, black history\n“Last year, we just acknowledged the work of a black chef in Washington, D.C., who is known for his kosher recipes,” Harris told the crowd gathered outside “the Compass” on Locust Walk near 37th Street.\nPenn spokesperson Barbara A. Lea-Kruger said that for years, chefs created menus to celebrate cultural events, but this year, “we took a different approach in light of what we saw happening in other universities where some students found the menus that were being served to be insensitive and stereotypical.”\n“The consensus was to continue to serve food to meet the dietary needs of the students, for example, fish on Fridays during Lent or special Passover meals,” she said in an email. “But until we met with the student groups on campus to get their input, we should pause our celebrations of cultural dates (such as Black History Month or Chinese New Year).”\nRecognizing the disappointment of some staff and students, the university plans to have a dinner this week highlighting famous black chefs, she said.\nElijah Wingate, a dining-hall worker, told the gathering the discontent has been simmering since Jan. 21 — Martin Luther King Day, a federal holiday.\n“We had to work that day even though classes were canceled,” Wingate said. But there was no acknowledgment that it was a holiday for King. “We see them celebrate all these other holidays; it seemed like they should have celebrated for MLK Day, too,” he said.\nBefore workers arrived, students held signs that read “Black Lives Matter,” “Celebrate Black History,” and “Students Support Workers.”\nErik Vargas, 20, a physics major from Texas, said that beyond the Black History Month issue, students were concerned that the staffers “work very hard, for long hours and poverty wages.”\nBut the dining-hall staffers who took turns at the bullhorn emphasized that they loved their jobs at Falk Dining Hall. They also talked about feeling a strong bond with students they serve every day.\n“Love has no color,” Harris told the students. He described being treated warmly by the students and staffers at Penn Hillel.\nOne student came up and hugged Wingate. “We spend more time with you guys than we do with our families,” she said.\nBoth students and workers were careful to describe the event as a “gathering” and not a protest or march.\nLast Tuesday, a group of student organizations led by Penn Student Power posted an announcement about the event on Facebook and titled it, “What Happened to Black History Month?”\n“It’s Black History Month, but being at Penn you wouldn’t know. … This is our response as students. We’re here to rise and resist the erasure of black history,” the announcement began, urging student organizations across campus to support by “standing in solidarity” with dining-hall workers.\nSome students said there was a general sense that the university as a whole had not done much to observe Black History Month, except for a week of activities by UMOJA, an organization representing black students.\nAfter the Facebook post went up, students pointed out that Bon Appétit told the Daily Pennsylvanian that it plans to have a special Black History Month celebration by Thursday, the last day of February.\nIn addition to Penn Student Power, other student groups joining the action included Penn for Immigrant Rights; Fossil Free Penn; Penn Lambda; SOUL, an African American group; Penn Hillel; Penn First; Penn Socialists; MEChA, a Mexican American group; and Radical South Asian Collective.\nJanine Liu, one of the organizers, said the university and Bon Appétit tried to have Monday’s gathering canceled. Then she showed an email about “Fabulous February Events.”’\nThe email noted that dining halls across the campus would acknowledge events this month, including Israeli Shabbat on Feb. 8, Quaker Kitchen on Feb. 13, and Tasty Tuesday on Feb. 12.']	['<urn:uuid:03a33f81-d6cf-4e78-8f5c-e016f37e77d9>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T23:00:04.658849	10	65	787
89	shade cover solutions patio outdoor wooden structure	A pergola is a versatile outdoor structure that can provide shade and function for your patio. The roof boards can be spaced as needed to control sun exposure, and fabric accessories that are outdoor-safe can be added for additional shielding from sunlight. The structure can be integrated with patios and decks to designate specific areas like seating zones, and can incorporate built-in benches for additional functionality.	"['Pretty, Inspiring Pergola Ideas\nFrom shade to structure, pergolas are a great way to add beauty and function to your yard. These pergola ideas and yard solutions offer helpful tips and tricks for you to find your own pergola designs, pergola plans, and inspiration and bring character to your outdoor room.\nEverything In This Slideshow\n1 of 22\nWorking with the Landscape\nOftentimes there\'s a natural spot for a pergola, but there are landscape elements that are essential, too. Luckily, the flexibility of pergola designs allow tweaks to best fit needs and wishes. Here, a tree supplies shade, so the pergola\'s roof was built around it. A solid back wall -- with a cute oval window accent -- offers privacy, too.\n2 of 22\nPergolas as Transitions\nSome pergola designs are more decorative than they are functional. This seaside home -- which mixes a little bit of Cape Cod and a little bit of Shaker styles -- offers a pretty lesson in how to use a pergola to accent a home. Textural materials on the exterior repeat in the pergola, and the structure offers a welcoming, softening transition between home and deck areas.\nFast & Simple Outdoor Tips\nTry these done-in-minutes outdoor decorating tips for a stylish space beneath your pergola.\n4 of 22\nIntegrate with a Patio or Deck\nA pergola can stand on its own, but it can also work well with another outdoor structure such as a patio or deck. Its placement can help delineate traffic or use zones, such as a seating area. This pergola design designates a lounge area and incorporates a built-in bench.\n5 of 22\nPergolas are a natural fit in expansive landscapes, particularly when integrated well with other outdoor living amenities. Here, the massive beams on this pergola fit in with the bulky structure of the stucco fireplace. A casual bench serves as both a defining edge for the pergola and extra seating spot, too.\n6 of 22\nOpen and Shut Sides\nAlthough many pergolas have four open sides, there are options to increase seclusion on one or multiple sides. This pergola uses three trellises on one side, detailing which maintains the pergola’s connection and openness to the landscape.\nEasy Patio Landscaping Ideas\nAdd greenery around your patio and pergola design with these gorgeous landscaping ideas.\n8 of 22\nPergolas are a good way to ease transition from house to other hardscape amenities -- here, both pool and outdoor kitchen (in the background) received a shading structure. Space the pergola\'s roof boards as needed to shield the sun (this one relies on closeness to offer more shade).\n9 of 22\nPergolas as Accents\nSpace constraints may limit how much of a pergola you can include in an exterior space. But even a smaller pergola design such as this can help to accent a home\'s patio, providing an interesting backdrop to an otherwise boring exterior facade. Here, a trellis offers a pretty and secluded backdrop for the seating space.\nGlam-up Outdoor Rooms\nGive your outdoor spaces and pergola a bit of luxe style with these tips.\n11 of 22\nFabric can be a useful accessory in pergola designs, especially to shield the space from sunlight or unwanted views. To provide flexibility in the enclosures, consider fabric -- here, with pull backs -- that\'s outdoor safe.\n12 of 22\nPergola + Roof\nWhile most pergolas are designed to be at least semi open to the sky, they can be integrated into the roof structure of the house. Here, the shingles on the pergola offers shade to the narrower porch structure, while colors pick up on detailing in the pretty home’s windows.\n13 of 22\nPergola Meets Arbor\nA little bit pergola, a little bit trellis, a little bit arbor, this charming structure blends in the best of those three hardscape items to create a lovely entrance to this cottage garden\'s side space. Narrow seating nooks offer restful spots, while the pergola-arbor\'s overhead spacing offers room enough for a flowering vine.\n14 of 22\nPergola as a Focal Point\nA pergola is adaptable enough that it can be used in a range of structures -- for example, this charming and cheerful seating structure. Used in this way, a pergola adds a focal point to a garden. A few containers help to soften and disguise the structure\'s base.\n15 of 22\nContrast in Colors\nDifferent hues can be a good way to use a pergola for visual impact. Here, the vertical columns establish a bright and light boundary for a seating area, while the stained wood roof offers a more naturalistic complement to the landscape.\n16 of 22\nArchitecture at Play\nSpace and budget are two possible constraints with a pergola, but in general, the ""rules"" of pergola design are flexible. Here, multiple heights of the pergola\'s ceiling add visual interest, while columns lend formality that\'s in tune with the detailing on the rails of the patio space.\n17 of 22\nA Contemporary Pergola\nThe outlines of this contemporary pergola idea offer structure and boundary between the paved exterior space and the rest of the landscape. Using a pergola in this way offers a simple way to define large landscapes in particular. Here, additional elements such as the fire pit accent the clean detailing of the pergola design.\n18 of 22\nVarying Roof Structures\nThere’s no set width for the ceiling elements in pergola designs. In fact, landscaping may help determine final detailing. Here, full-grown shade trees offer shelter from the sun, so the pergola roof beams are space further apart.\n19 of 22\nA Deck-Ready Pergola\nA pergola can offer shade to a large gathering space, even if it\'s not next to a house. Here, gently arched beams topped with 2x2s form the roof over this freestanding 8x8-foot deck. Lattice offers a partial enclosure and increases the sense of intimacy.\n20 of 22\nPergolas as a Passage\nAlthough pergolas are often stopping points in a landscape, they can also be used as transitions. Here, a tall structure offers a waypoint between front and backyard. Vines that ramble up the pergola repeat in the trellis accents against the home’s exterior wall.\n21 of 22']"	['<urn:uuid:fb95b42b-3fd7-4868-840b-846fd5f7a1ee>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T23:00:04.658849	7	66	1028
90	Did Emperor Henry II and the Council of Ephesus agree on modifying creeds?	No, they took opposing approaches. Henry II successfully petitioned in 1014 to add the creed to the Roman Mass, while the Council of Ephesus in Canon VII had explicitly forbidden modifications to the creed, though this specifically referred to the Nicene rather than the Constantinopolitan Creed.	['In the late ninth century, the city of Rome entered into a period of crisis, which has with some justification been called a “dark age” (saeculum obscurum) of the papacy, lasting well into the 11th century. Spiritual and cultural leadership was found north of the Alps, and this held for the liturgy too, which flourished in episcopal cities and Benedictine monasteries on both sides of the Rhine, such as Tours, Corbie, Metz, Mainz, Lorsch, Fulda, Reichenau, and St. Gall. New impulses came with the monastic reform movements, above all from Cluny, and with the German emperors of the Ottonian dynasty. Like their Carolingian predecessors, the Ottonians took a vivid interest in ecclesiastical matters, and showed themselves patrons of the liturgy in their realm, which also led to a flourishing of sacred architecture and art.\nThe Ordo Missae\nThe most momentous step in early medieval liturgical development was the organization of the recurring parts of the Eucharistic celebration into what is known to this day as the Ordo Missae (Order of Mass). The earliest vestiges of such an order are already found in many Gregorian-type sacramentaries, which begin with a separate section, “How the Roman Mass is to be celebrated” (Qualiter missa romana caelebratur). This instruction corresponds with the description of the papal stational liturgy in Ordo Romanus I (see installment VII of this series) and may go back to the late seventh century.\nA subsequent step was taken with the collections of private prayers to be said by the celebrant at different moments of the rite. The earliest known example of such a collection is attested in the Sacramentary of Amiens (Paris, Bibliothèque Nationale, lat. 9432), dating from the second half of the ninth century. Most of these texts, some of which consist of a single psalm verse, accompany and elucidate the spiritual meaning of particular ritual actions for the celebrant priest. They thus serve to sustain his personal piety and help the devout offering of the sacrifice of the Mass.\nThe Ordo Missae as a distinct liturgical genre flourished between the ninth and 11th centuries. Contents and form varied considerably: for prayers, the opening word or phrase or the complete text is given; more or less detailed ritual instructions are sometimes provided, and occasionally musical notation is added. The limits of this article do not allow for an adequate discussion of the widely received classification of the Ordo Missae of Bonifaas Luykx. Some ordines could be excessive in their use of private priestly prayers, above all the ordo produced for Sigebert, bishop of Minden in northern Germany from 1022-1036. The text has achieved some notoriety since it was published in 1557 by the Lutheran theologian and historian Matthias Flacius Illyricus (1520-1575), and it became known as Missa Illyrica. Other ordines, however, are more measured in tone and less likely to overlay the traditional sequence of the rite of Mass. This testifies to an effort of pruning that resonates with views of the chronicler and Gregorian reformer Bernold of Constance (c. 1050-1100), who objects to the length and the private character of such prayers.\nModern liturgical scholarship has tended to interpret the creation of the Ordo Missae as a departure from the “classical form” of the Roman Rite that was determined by the cultural needs of the Franco-Germanic people. Joanne Pierce and John F. Romano offer a balanced version of this critique: “The Roman Mass before this point was known for its soberness, simplicity, and straightforwardness. These [ordines Missae] filled out the framework of the Roman Rite with new prayers, psalms, and gestures, elaborating the ‘soft spots’ of the liturgy that had not previously received full elaboration, especially actions that occur without words. They imbued the Roman Eucharistic liturgy with new embellishment, drama, and allegorical symbolism.”\nThis assessment is not without problems; first of all, the characterization of the Roman Mass as sober, simple, and straightforward. The ritual shape of the Mass, for which Ordo Romanus I is our key witness, certainly featured lavish and dramatic elements, especially in its processional parts, which were indebted to imperial ceremonial. Secondly, the elaboration of the “soft spots” can be understood as offering genuine development and even enrichment. For instance, the pontiff’s moment of silent prayer before he approached the altar occasioned the recitation of Psalm 42, with the evocative antiphon “Introibo ad altare Dei…” (“I will go to the altar of God…”; v. 4).\nThirdly, and perhaps most importantly, the Ordo Missae, rather than offering drama and embellishment, above all provided a coherent (and memorable) schema that facilitated the success of the “private Mass” with its much-reduced ceremonial. The shift towards the personal devotion of the offering priest gives some credence to the oft-repeated charge that the early medieval period saw a “clericalization” of the Mass and a detachment of the laity from its liturgical enactment.\nThe Creed at Mass\nThe last Ottonian ruler, the devout Henry II (king 1002, emperor 1014, died 1024), who took great interest in ecclesiastical matters, is known to liturgical historians above all for his initiative to insert the creed into the Roman Mass. The creed in question was that of the first two ecumenical councils of Nicaea (325) and Constantinople (381), which had been used as a baptismal profession of faith in the Christian East since the fourth century. Peter the Fuller, anti-Chalcedonian patriarch of Antioch (r. 471-488), is credited with the introduction of the creed into the Eucharistic liturgy, to be recited after the kiss of peace. In the Latin West, the creed became part of the Mass first in Visigothic Spain, after the conversion of King Reccared and his nobles to Catholic Christianity. At the third synod of Toledo in 589, it was decreed that the Niceno-Constantinopolitan Creed should be said with the filioque clause, affirming the procession of the Holy Spirit from the Father and the Son (filioque), at every Mass, in preparation for Holy Communion, preceding the Lord’s Prayer. The Stowe Missal (c. 792-803), an important source for Irish liturgical use, places the creed after the gospel. Towards the end of the eighth century, Charlemagne had the singing of the creed (including the filioque) inserted after the gospel at the celebration of Mass in his Palatine chapel at Aachen. This decision was part of the Carolingian struggle against Adoptionist Christology in Spain. Pope Leo III (r. 795-816) approved of the use of the creed at Mass, though without the filioque, and he did not adopt the practice in Rome itself. The new custom spread slowly throughout the Carolingian realms and was commonly accepted in Franco-German churches by the 10th century.\nStaying in the city of Rome in 1014 for his coronation as emperor, Henry was surprised to find that, unlike in Germany, the creed did not form part of the rite of Mass and petitioned Pope Benedict VIII (r. 1012-1024) to add it. Subsequently, the creed was adopted in Rome on Sundays and major feasts in the liturgical year.\nThe shift from an oral to a written culture in the high medieval period gave the codified liturgical text a renewed importance. The pruned form of the Ordo Missae became normative for the celebration of the Eucharist and was incorporated into the 13th-century missal of the Roman curia. The next installment will show how the papacy, strengthened by the 11th-century reform movement, took charge again of the development of the Roman Rite.\nFor previous instalments of Father Lang’s Short History of the Roman Rite of Mass series, see:\n- Part I: Introduction: The Last Supper—The First Eucharist\n- Part II: Questions in the Quest for the Origins of the Eucharist\n- Part III: The Third Century between Peaceful Growth and Persecution\n- Part IV: Early Eucharistic Prayers: Oral Improvisation and Sacred Language\n- Part V: After the Peace of the Church: Liturgy in a Christian Empire\n- Part VI: The Formative Period of Latin Liturgy\n- Part VII: Papal Stational Liturgy\n- Part VIII: The Codification of Liturgical Books\n- Part IX: The Frankish Adoption and Adaptation of the Roman Rite\nImage Source: AB/Cluny Abbey at Wikipedia\nSee Alain-Pierre Yao, Les “apologies” de l’Ordo Missae de la Liturgie Romaine: Sources – Histoire – Théologie, Ecclesia orans. Studi e ricerche 3 (Naples: Editrice Domenicana Italiana, 2019), 355-358. ↑\nBecause of the red ink commonly used for such instructions, they have become known as “rubrics” (from “ruber,” the Latin word for red). ↑\nBonifaas Luyxk, “Der Ursprung der gleichbleibenden Teile der heiligen Messe (Ordinarium Missae)” in Liturgie und Mönchtum 29 (1961), 72-119. ↑\nBernold of Constance, Micrologus de ecclesiasticis observationibus, 18: PL 151,989BC. ↑\nJoanne M. Pierce and John F. Romano, “The Ordo Missae of the Roman Rite: Historical Background”, in A Commentary on the Order of Mass of the Roman Missal, ed. Edward Foley et al. (Collegeville, MN: Liturgical Press, 2011), 3-34, at 21. ↑\nOrdo Romanus I, 50. ↑\nThis psalm verse was already employed by Ambrose of Milan in his mystagogical catecheses for the newly baptised to evoke the approach to the altar of the Eucharist: De sacramentis IV,2,7, and De mysteriis 8,43. ↑\nThe Latin translation of the creed used in Mozarabic sources is different from the version later introduced in the Roman Rite. Interestingly, the loanword “homusion” is used, where the Roman version translates “consubstantialis”; see Marius Férotin, Le Liber Mozarabicus Sacramentorum et les manuscrits mozarabes, Monumenta Ecclesiae Liturgica 6 (Paris: Firmin-Didot, 1912), 773. ↑\nSee the excellent documentation of Andreas Amiet, “Die liturgische Gesetzgebung der deutschen Reichskirche in der Zeit der sächsischen Kaiser 922–1023”, in Zeitschrift für schweizerische Kirchengeschichte 70 (1976), 1–106 and 209–307, at 222-228. ↑', 'In Eastern Orthodox apologetics, Canon VII of the Ecumenical Council of Ephesus I is often invoked as an argument against the Filioque. The argument is that the addition of “and the Son” after the words “proceeds from the Father” in the Constantinopolitan Creed is tantamount to adding to the Nicene Creed–the two Creeds really being one of the same. In the words of one popular apologist, Robert Arakaki:\nFor Roman Catholics the Nicene Creed is under the Pope, not over the Pope. When the Pope inserted the Filioque into the Nicene Creed a major realignment of ecclesial authority took place. The Pope without the assent of the other historic patriarchates: Constantinople, Alexandria, Antioch, and Jerusalem, and without convening an ecumenical council of bishops, unilaterally altered the Nicene Creed. This was done even though the Third Ecumenical Council (Ephesus 431, Canon VII) forbade the creation of a new creed. In essence, the Bishop of Rome was claiming a magisterium (teaching authority) equal to or superior to the Ecumenical Councils.\nIn short, the accusation is that the Filioque is an addition to the Nicene (i.e. Nicene with Constantinopolitan additions) Creed, because it adds words that were not there before.\nBut, does this allegation hold water? After all, Canon VII explicitly condemns adding or subtracting anything to the Creed–wouldn’t the addition of three words be obviously worthy of condemnation?\nIn this article, I argue that the mere adding of words is not condemned Canon VII, due to the teachings on the subject by the fathers in the Council of Chalcedon. We can address this by answering a few questions:\nIs the Creed that cannot be added to or subtracted from, according to Canon VII of Ephesus I, the Constantinopolitan Creed?\nIn a word: NO. The minutes of the Council of Ephesus (specifically the Session of July, 22 431) were read in the Council of Chalcedon. In those minutes, the Creed that they condemned there being additions to and subtractions from was quoted ad verbatim (First Session of Chalcedon’s Proceedings, Paragraph 914). The words were specifically for the Nicene, not the Constantinopolitan, Creed.\nWas there a contemporary understanding that the Constantinopolitan Creed had replaced the Nicene Creed?\nYes and no.\nEvidence for the affirmative can be seen after the letter from archimandrite Eutyches of Constantinople (a monophysite heretic) was read during the First Session. Eutyches quotes only Nicea (not Constantinople I) and then states:\nThis is the creed in which I was born and immediately dedicated to God and accepted by his mercy. With this creed I received the seal of baptism and have lived till today, praying also to die in it. This is the creed that was also confirmed by the aforesaid holy and ecumenical council held here earlier at which our father Bishop Cyril of blessed and sacred memory presided, and at which he issued a decree that whoever added to it in thought or teaching is subject to the penalties then laid down (Ibid., Paragraph 157).\nIn short, Eutyches was making the claim that his Monophysitism was endorsed by Saint Cyril of Alexandria and the Nicene (not Constantinoplitian) Creed. A few Bishops objected to Eutyches’ interpretation of Ephesus and Cyril’s letter.\nDiogenes, bishop of Cyzicus, had a telling response which reveals that he viewed the Nicene Creed to have had been modified by the holy fathers of Constantinople I:\nHe [Eutyches] adduced the council of the holy fathers at Nicaea deceptively, since additions were made to it by the holy fathers on account of the evil opinions of Apollinarius, Valentinus, Macedonius and those like them, and there were added to the creed of the holy fathers the words “He came down and was enfleshed from the Holy Spirit and Mary the Virgin.” This Eutyches omitted [in the letter], as an Apollinarian…The holy fathers who came after clarified the words “was enfleshed” of the holy fathers at Nicaea by adding “from the Holy Spirit and Mary the Virgin” (Ibid., Paragraph 160).\nIn this response, Cyzicus explicitly said that the Nicene Creed has an addition to it, and then quotes part of the Constantinopolitian Creed.\nTellingly, the Monophysites immediately cried afoul of the words of Cyzicus, asserting that Canon 7 of the Council of Ephesus disallowed for the Constantinopolitan additions–\nThe most devout Egyptian bishops and those with them exclaimed: ‘No one admits any addition or subtraction to [the Nicene Creed]. Confirm the work of Nicaea; the orthodox emperor has commanded this’ (Ibid, Paragraph 161).\nCan the preceding be used as justification for saying that the orthodox/catholic fathers of Chalcedon were always speaking of the Nicene and Constantinopolitan Creeds as one of the same? The answer is no. There are at least a couple dozen examples of this, but we will only list a few.\nPapal Legate Paschasinus clearly differentiated between the Creed of Nicea and that of Constantinople:\nIt is clear and cannot be disputed that the faith of the most blessed pope of the apostolic see Archbishop Leo is one and in accord with the creed of the 318 fathers who met at Nicaea, that it upholds both the creed of the 150 who convened at Constantinople and also the decrees of Ephesus under Cyril of holy memory when Nestorius was deposed on account of his errors (Ibid., Paragraphs 2-4).\nTheodosius of Canatha likewise speaks of the Creeds as mutually exclusive:\nThe creed of the 318 is unshakeable. If anyone attempts to shake the unshakeable, he will himself be shaken, while not shaking the unshakeable. This creed we follow and believe, and also the definition defined by the 150 holy fathers who met at Constantinople (Ibid., Paragraph 27).\nEuphratas of Eleutherna concurs:\nWe uphold the creed of the 318 holy fathers as being our salvation and pray to depart from life with it; and that of the 150 is in no way in disharmony with the aforesaid creed (Ibid., Paragraph 98).\nFlorentius of Hadrianopolis said:\nBefore the interpretations of our most God-beloved and blessed father Cyril and the most blessed Archbishop Leo, we adhered to the definition of the holy fathers at Nicaea; so we believed and believe. In addition we assent to the creed of the 150, which clearly states that our Lord Jesus Christ was enfleshed from the Holy Spirit and Mary the Virgin. In this creed of the holy fathers we believe as they have interpreted it, and we doubt nothing.’ (Ibid., Paragraph 133).\nClearly, the majority of Chalcedon’s fathers believed that the Nicene and Constantinopolitan Creeds were mutually exclusive, but that the “definition” of Constantinople was just as correct as Nicea.\nIf Canon VII of Ephesus I did not refer to the Constantinopolitan Creed, does that mean additions can be made to it?\nIn a word: NO. In the Fifth Session of Chalcedon “by a unanimous decree…renewed the unerring faith of the fathers, proclaimed to all the creed of the 318, and endorsed as akin the fathers who received this compendium of piety, that is, the 150 who subsequently assembled at great Constantinople and set their seal on the same faith” (Paragraph 31). Both Creeds were then read ad verbatim (Paragraphs 32-33).\nFinally, the warning not to add or subtract from the Creed was made applicable to both Creeds:\nNow that these matters have been formulated by us with all possible care and precision, the holy and ecumenical council has decreed that no one is allowed to produce or compose or construct another creed or to think or teach otherwise (Paragraph 34).\nWait a second–didn’t Constantinople I in the words of Diogenes, bishop of Cyzicus, “make additions” to the Nicene Creed?\nNO. True, popular Orthodox apologetics equates adding words, whether orthodox in their content or not, to a creed as equivalent to defying the universal decree of the Bishops in Paragraphs 32-34 of the Fifth Session of Chalcedon. However, the fathers of Chalcedon did not view it this way. Such a narrow view was in fact erroneously taken by the monophysite heretics (First Session of Chalcedon, Paragraph 161)!\nInstead, it is more appropriate to interpret the “no additions” rule as forbidding a different faith than that of Nicea, not changes in verbiage per se–otherwise Canon VII would have forbidden the use of the Constantinopolitan Creed (which it in fact never did.)\nFurthermore, we have Chalcedonian fathers affirming the understanding that it was the preservation of the creed’s faith and content that was important. One example we have recorded is from Anatolius of Constantinople, who said, “The creed of our 318 holy fathers at Nicaea and of the 150 who subsequently assembled at Constantinople…confirmed the same faith” (Fourth Session of Chalcedon’s Proceedings, Paragraph 9).\nHence, while the fathers viewed the Creeds as mutually exclusive, the Constantinopolitan Creed did not violate Canon VII of Ephesus I specifically because it was re-iterating what the Nicene Creed said in that it “confirmed the same faith.” In doing so, it confirmed the same faith in such a way to prevent Apollonarian misinterpretations of the plain words of Nicea.\nTherefore, additions to a Creed are viewed as legitimate provided that the faith therein elaborated upon is the same. The words of Papal Legate Paschasinus also allowed for the same logic to be applied to Saint Leo I’s Tome, so we can already see the seeds of dispute over authority that would serve as the basis of the Filioque controversy.\nConclusion. The preceding study leaves the question in the title of this article unanswered: Is the Filioque a legitimate addition to the Creed or not? To this we respond, this is something that cannot be definitively answered unless we have worked out what is the basis of understanding Christian doctrine as a whole.\nThe Filioque was never the historical consensus of the Church nor was it elaborated upon by a consensus of the fathers at these early councils. So, without such consensus reached in the Church, the Orthodox must conclude that it is debatable at best (for there are ways to correctly understand Filioque in Orthodoxy) and heterodox at worst (the Orthodox reject that the Son has some role in the eternal causation of the Spirit.)\nMeanwhile, the Roman Catholics ultimately do not view consensus as the decisive factor. While the Council of Trent affirms that an explicitly expressed consensus in doctrine is dogmatically binding, the agreement of the whole Church on a doctrine is not necessary to prove the small “o” orthodoxy of a doctrine in all cases. Rather, a minority viewpoint (or one never explicitly elucidated in church history) can be the correct one provided that this has been discerned as such by the Roman Pontiff.\nUnless we answer which one of the preceding views (Orthodox or Roman Catholic) is right, we cannot definitively answer whether the Filioque was an “addition” to the Creed in the sense that Chalcedon would have condemned.\nCertainly Catholics must admit that the Filioque is an addition to the verbiage. Orthodox must admit that the mere addition of words is not a violation of Chalcedon.\nRather, the real question is whether the Filioque is an addition to the faith of the Creeds.\nRome knows that the reason Orthodox object to the Filioque is because the latter do not believe the Spirit does not eternally originate from both the Father and the Son. The Filioque potentially allows for this allegedly heterodox idea.\nIn fact, it probably is the position of Rome that the Spirit originates eternally from the Father and the Son. The Roman Catholics and Orthodox disputed this issue at length in the Council of Florence. Saint Mark of Ephesus, an attendee of the Council, reported that when the said council asserted that the Spirit proceeded eternally from the Father and the Son from “a single principle,” that:\n[W]e [Orthodox], together with St. Maximos and the Romans of that time, as well as the Western Fathers, “do not make the Son the Cause of the Spirit”; while they, in their Conciliar Decree [in Florence], proclaim the Son “in Greek, ‘Cause,’ and in Latin, ‘Principle’” of the Spirit.\nThe Council of Florence was published in Greek and Latin. By using the words principle/cause in Latin/Greek respectively, the council ignored the writings of saints such as Maximus the Confessor and John of Damascus (“For the Father alone is cause,” An Exposition of the Orthodox Faith, Chap 12) who specifically rejected this idea.*\n*I am trusting Mark on Ephesus on this, as I cannot read the published Greek or Latin. I presume he would have not made such an easily refutable claim falsely. A Catholic parsing of the term “spiration” appears to confirm this reading.\nFor a moment, let’s consider the before referenced teaching from Saint Maximus the Confessor in his Letter to Marinus which argued that the Filioque was not an addition to the faith. He said this because he alleged that the Catholics of Rome of his time denied that the Filioque taught that the Spirit eternally originated in both the Father and the Son.\nIt would seem that if Rome explicitly and dogmatically took the view as elaborated upon by Saint Maximus, the Filioque dispute would be over. However, instead of endorsing this view, Rome for all appearances has rejected it over the ages.\nFirst, the plain meaning of Rome’s dogmatic constructions (which state that the Spirit was “spirated,” i.e. “breathed out” by the Father and Son as a single “principle”/”cause”) would mitigate against Maximus’ view. Second, Maximus’ Letter of Marinus was alleged by French Catholics of being a forgery in the Council of Florence and was not accepted by Catholics in their elucidation of the Filioque doctrine. It appears that Rome’s view of the Filioque is clearly different than that of Saint Maximus’ otherwise they would have taken the opportunity, which they have had 1,150 years* to do, to clear up the confusion. This means that Saint Maximus the Confessor, confidante of Pope Martin I, was either misunderstanding what Rome taught about the Filoque at his time or that Rome has changed their understanding of the doctrine.\n*If we begin count from the Photian Schism.\nIn the end, even if the interpretation Rome has for the Filioque appears to add to the faith of Nicea, Ephesus, and Chalcedon (the Councils, and their fathers, nowhere endorsed that the Spirit eternally originates from the Son in any way) one would be compelled to presume if the Roman view of authority is correct that the fathers of these councils, in an unsaid way, agreed with the later Roman elucidation.\nThis is because the Roman epistemology does not need to show that the faith of the fathers is consistent with their present day understanding of the Filioque. This is because a consensus of opinion since the time of the Apostles, or even a mere whisper of their view here or there, is even really necessary. Rather, as long as the fathers said nothing to explicitly to contradict the Filioque’s verbiage, what was left unsaid in Chalcedon must implicitly agree with the councils of Second Lyons and Florence simply because they do not explicitly disagree. Hence, the unsaid faith of the past is the same as the explicitly stated faith of later councils.\nIt would appear to the honest and impartial observer that Rome’s epistemology is a negative case where as long as people in the past did not explicitly reject a modern construction of a doctrine, the modern elucidation of the said doctrine may be read back into the past. If there is not a contradiction between past and present, then the doctrine is worthy of approval.\nMeanwhile, Orthodoxy demands a positive case where the faith of the fathers elucidated by the Church in the present must be something that is explicitly taught in the past. Therefore, if the fathers did not explicitly take the view that the Spirit eternally originates in the Father and the Son, the presumption is that this was never the faith of the Church.\nIn short, the epistemological difference may be summed up as follows: Roman Catholicism ultimately reads backwards and nowhere demands an ancient consensus for all doctrines. Orthodoxy reads forwards, requiring ancient consensus to justify modern understandings.\nIt is no surprise that the former view leads to more “changes” and “development,” because there is no governing factor over the Roman church other than how doctrines are defined with Papal approval in the present. Hence, the past is constantly re-evaluated in light of present-day understandings. Orthodoxy, no stranger to change, but with a different epistemological basis, changes much more slowly–as it uses the past to evaluate the present, not vice versa.\nSo, ultimately Rome may define the Filioque in such a way that explicitly states the Holy Spirit does not originate from the Son in any way. How such an explicit definition would not contradict the construction from Lyons II that the “Spirit proceeds eternally from the Father and the Son, not as from two principles, but as from one principle” will be left to the theologians to explain.\nBut, the preceding will not definitively settle the dispute. Three hundred years later, Rome can further elucidate that while the Spirit does not eternally originate from the Son, He eternally “emanates” from the Father and Son equally. It would appear the potential for future elucidations of doctrine ad infinitum epistemologically makes it impossible to ever settle any doctrine. In the Roman system, any doctrine can be perpetually put back into dispute–always waiting for the newest Roman elucidation.\nUntil the issue of epistemology is settled, the Filioque dispute cannot be.']	['<urn:uuid:e59b34e6-870b-476c-9056-c52c0e7c8e17>', '<urn:uuid:871683d3-4db4-41f7-b73a-194d99fc7664>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T23:00:04.658849	13	46	4513
91	What are the main implications for patient care when there are no advance directives in place, and how does this affect decision-making in intensive care settings?	Without advance directives, there can be significant misunderstandings and conflicts between family members trying to determine patient wishes. In ICU settings, the default approach becomes maximizing chances of survival, even when treatment might be futile. Research shows substantial differences in care between patients with and without advance directives - those without directives may receive more aggressive interventions, such as increased feeding tube placements in skilled nursing facilities for dementia patients. Additionally, healthcare providers must start with the assumption of extending life rather than having clear guidance about the patient's treatment preferences, which can lead to the continuation of potentially futile treatments.	"['Department of Intensive Care, Erasme Hospital, Université libre de Bruxelles, Brussels, Belgium\nReceived date: Febvery 26, 2014; Accepted date: July 05, 2014; Published date: July 07, 2014\nCitation: Vincent JL (2014) When ICU Treatment Becomes Futile. J Clinic Res Bioeth 5:186. doi: 10.4172/2155-9627.1000186\nCopyright: 2014 Vincent JL. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.\nVisit for more related articles at Journal of Clinical Research & Bioethics\nMany patients admitted to the intensive care unit (ICU) have no hope of meaningful survival and receive “futile” therapy. When a patient is no longer capable of deciding (so that autonomy is no longer applicable), giving futile treatment is against the three other key ethical principles: beneficence, non-maleficence, and distributive justice. Continuing futile treatments impacts on the patient and his/her family, but also on other patients and society as a whole. In such situations, the goal of intensive care should no longer be to maximize a patient’s chances of a good quality of life on discharge but rather to provide them with a dignified and comfortable death. Once a decision has been made that intervention is futile, all on-going treatment should thus be withdrawn except for comfort measures. Good and continuing communication with the family and all members of the healthcare team is an essential aspect of insuring the best possible dying process. In this article, we will explore this complex area and try to provide answers to some of the difficult issues involved, including how futility can be recognized, who should determine that further treatment is futile, and what should be done once it has been decided that further treatment is futile in a particular patient.\nICU; Futile; Withhold; Withdraw; End-of-life\nThe word ‘futility’ is derived from the Latin word futilis, literally meaning “pouring out easily” or “leaky” and hence considered of no use or worthless. In terms of critical care, futility is generally considered as the condition when a patient receives treatment that will have no beneficial effect in terms of quality of life or duration; there may be a physiological effect but this will not correspond to benefit for the patient. Importantly, it is the treatment that is futile and not the patient, and futility should not be considered synonymous with failure. When a patient is no longer capable of deciding (so that autonomy is no longer applicable), giving futile treatment is against the three other key ethical principles: beneficence, because it offers no benefit to the patient; against non-maleficence, because it may harm the patient; and against distributive justice, because the additional time and cost expended on providing treatment to a patient who will not survive may be better used on patients who would benefit.\nBut how can such a state be recognized, who should determine futility, and what should be done once it has been decided that further treatment is futile in a particular patient? We will review and explore the available literature in this field to try and provide some answers to the difficult issues involved in this complex ethical issue.\nDefinition and Recognition of Futility\nFutile intensive care has been defined as “use of considerable resources without a reasonable hope that the patient would recover to a state of relative independence or be interactive with his or her environment” . A more recent definition is “interventions that prolong life without achieving an effect that the patient can appreciate as a benefit” . However, although these definitions may seem fairly straightforward at first glance, there are several areas of potential confusion/conflict that should be noted, not least in the meaning of the word “benefit”. Benefit can be interpreted in terms of simply extending duration of life, but what about quality of life? Is providing organ support to sustain physiological functions really a benefit when the patient is clinically unresponsive?\nIn an attempt to quantify “futility”, it has been suggested that treatment be considered futile when it is known to have a less than 1% chance of success , but this necessitates a good knowledge of all outcome data for the intervention in question for all groups of patients, data which are anyway not always available in the intensive care population. Moreover, in an interview-based study, although the proportion of surrogates who would want life-sustaining treatments continued decreased as the presented prognosis of their loved one decreased, 32% of surrogates still said they would have wanted treatment to be continued despite being told that the patient had a less than 1% chance of surviving, and 18% when the physician thought there was no chance of survival . Others have suggested that scoring systems, such as the APACHE II or SAPS scores, that provide an indication of prognosis, could help determine which patients are not likely to benefit from therapy [5,6]; however, these scores were not designed for this purpose. For example, a young, previously healthy polytrauma victim may have a very high severity score, but surely warrants every effort to save his/her life.\nClearly the definition of futility is a complex and sensitive issue and raises many questions for which there are no easy answers. The Council on Ethical and Judicial Affairs of the American Medical Association reached the conclusion that a “fully objective and concrete definition of futility is unattainable” . Without a clear definition, the next problem is who should then have the responsibility of making this rather subjective judgement call for individual patients. Some would argue that futility is purely a medical “diagnosis” and others that it is rather a moral judgement . As a medical decision, clearly the physician in charge must make the final judgement. If futility is considered a moral judgement, then the family or surrogate (or patient) may have a more important role to play in deciding what to call “futile”. Whatever one’s personal viewpoint, futility effectively refers to the inability to respond to treatment and thus is essentially a medical condition – some have suggested the term “medically inappropriate treatment” rather than “futile treatment” , highlighting the clinical nature of these decisions. However, all such decisions have a subjective component as different individuals will often have different opinions as to what constitutes “futility” in a particular patient [7,10]. Religious belief, cultural background, personal values, local legislation can influence one’s interpretation of futility. There is, therefore, a risk that, if made unilaterally, the decision-maker’s values (whether the physician or a surrogate) may be imposed on the patient [11,12]. Hence, decisions should be made after due consideration of all the clinical data and full consultation with all members of the healthcare team, and the family whenever possible.\nFamily involvement in end-of-life decisions varies considerably across the globe , traditionally being greater in North America than elsewhere and in northern Europe than in South America, Asia and southern Europe [14-17]. Family members should not be expected to make such decisions, especially those who do not have the medical knowledge to fully understand the complex medical issues involved. The psychological and moral burden of this process on grieving family members can increase the risks of post-traumatic stress disorder, anxiety and depression . Nevertheless, in general, the trend is moving towards increased family involvement in these decisions worldwide [19-21]. However, although families should be involved in discussions and kept closely informed, the final decision should remain the responsibility of the physician-in-charge. The reasons why the decision is a medical one and why stopping or not starting treatment is the most reasonable option must be clearly explained to the family members. Indeed, healthcare providers must never feel obliged or pressured to provide futile treatment when there is medical consensus that it will be ineffective [22,23], although studies report that physicians are often influenced by family wishes .\nThe Extent and Impact of “Futility” in Intensive Care\nThe continuation of futile treatment in an ICU patient impacts not only on that particular patient, potentially prolonging suffering and the natural dying process, but also on family members and on other patients who could have benefited from the intensive care facilities being used by the patient receiving futile therapy. This latter concept raises the ethical principal of distributive justice and in some parts of the world (including in Europe), where the number of ICU beds is quite limited, this can be a major practical issue [25,26]. Yet, a considerable proportion of ICU patients are considered by physicians and other healthcare workers to be receiving “futile” care. In a survey of European ICU physicians, 73% of the respondents said that their ICU frequently admitted patients with no hope of survival . In a survey of medical and nursing unit directors from Canadian ICUs, 87% of physicians and 95% of nurses said that futile care had been given in their ICU over the past year . In a survey of 1953 ICU nurses and physicians from 82 adult ICUs in 9 European countries and Israel, 27% perceived that care was inappropriate in at least 1 patient and in 57% of these cases it was considered inappropriate because it was excessive . In a recent survey of critical care specialists in 5 ICUs from one medical centre, 11% of patients were perceived by the intensivists as receiving futile treatment and 8.6% as receiving probably futile treatment . These authors estimated that the cost of futile treatment in critical care was $2.6 million over the 3-month study period. Clearly, futile interventions account for a considerable proportion of ICU interventions and are associated with high costs. So how should this problem be approached?\nWithholding/Withdrawing in the ICU\nThe majority of deaths in the ICU are now preceded by a decision to limit treatment in some way, although the actual percentage may vary among units and across national and international borders [15,29-32]. There are various forms of withholding/withdrawing ranging from the do-not-resuscitate order in which resuscitation efforts are withheld in the case of cardiorespiratory arrest, through do-not-escalate orders in which no new therapies are started but current interventions maintained, to the withdrawal of all on-going treatment (Table 1). The appropriateness of each of these decisions will depend on the individual situation. As noted earlier, decisions to withhold or withdraw futile treatments are influenced by multiple factors, including physician age and sex, religious beliefs and cultural background , peer pressure, and fear of litigation [15,29,34]. Importantly, it is generally accepted that there is no ethical or moral distinction between withholding and withdrawing life-sustaining treatments [19,22,35]. However, this is not accepted everywhere: in Israel for example, a law authorizes the use of timers on ventilators so that the respirator can stop by itself after a programmed amount of time, without a physician\'s intervention to withdraw the therapy [36,37].\n|-Do-not-resuscitate (DNR): do everything, except in case of cardiac arrest|\n|-Do-not-escalate for some interventions e.g., endotracheal intubation, renal replacement therapy, vasopressor therapy…|\n|-Do-not-escalate for anything (DNE)|\n|-Withdrawal of life-support|\n|-Increased doses of sedatives|\nTable 1: The various forms of withholding/withdrawing.\nIn any case, withdrawing is seen by many as more difficult than the more “passive” withholding . In some situations, withdrawing may be seen as preferable , allowing the patient the benefit of the doubt and a trial of therapy or “time-limited trial”, sometimes referred to as ""the ICU test"" . Importantly, if it is decided that a trial of therapy should be conducted, time limits and targets for the trial must be set in advance and, if treatment goals are not met by the allocated time, treatment should be withdrawn according to the plan . Good communication with the family is particularly important in this situation to insure that the purpose of the trial is clearly understood.\nEssentially, when it has been decided that treatment is futile, all therapeutic interventions already on-going should be withdrawn and no new interventions should be started. The decision, resulting from a consensus among the caregivers, should be clearly documented in the patient’s notes and a plan of action determined and agreed on by the healthcare team. The family should be approached regarding the possibility of organ donation if relevant. Although much has been written about the ethical issues surrounding end-of-life care in the ICU, relatively little has been published regarding the practicalities of the process of withholding/withdrawing [20,35,40]. Yet, with as many as 80% of ICU deaths now associated with an end-of-life decision [41-44], information regarding how best to implement the decision to withhold/withdraw is important to insure a “good” dying process for the patient. Throughout the terminal process, it must be remembered that it is treatment that is being withdrawn or withheld and not “care” [19,20]. Good communication among medical staff and with patients (where possible) and family members is essential  and all discussions, decisions, plans, and outcomes should be carefully documented [30,35].\nMany recommend the discontinuation of all forms of monitoring to provide as peaceful an environment as possible for the dying process . However, others prefer to continue minimal monitoring so that the patient’s course can be followed. In either case, there is no need to prolong the withdrawal process, as long as time has been taken to explain to family members what they can expect. Most treatments should be removed immediately, for example antibiotics, blood products, intravenous fluids and nutrition . The only real reason for removing a therapy more slowly is if rapid withdrawal could cause distress or discomfort to the patient, for example, when withdrawing ventilator support, or be upsetting for the relatives. In the mechanically ventilated patient, withdrawal of ventilator support can be performed by transition from full support to T-piece or extubation over a period of 10-20 minutes [20,30], but others prefer to maintain mechanical ventilation with a progressive decrease in the respiratory settings. Adequate sedation and analgesia should be given to insure the patient remains comfortable at all times, using sufficient doses even if this speeds death [20,35]. In some units, including in Belgium, medication, such as analgesics/sedatives, is commonly used even if there is no apparent discomfort and the dying process may be shortened, because this can improve the quality of dying . This practice is often even encouraged by family members at the bedside, who can feel that once a decision has been made the end-of-life process should not be prolonged. The complex ethical debate surrounding this approach is beyond the scope of this review. Although many patients will die soon after therapy has been withdrawn, others may remain relatively stable for a prolonged period of time. These are difficult situations for all involved, and it is important that the patient and family do not feel neglected in any way and are kept fully informed. The attending physician and nursing staff remain responsible for the well-being of their patient and the withdrawal process should be considered and conducted with the same care and concentration as any other medical procedure. Regular attendance at the bedside, updating and reassuring the relatives, answering questions, are all essential until the goal is achieved satisfactorily.\nContinuing futile therapy is against three key ethical principles: against beneficence, because it offers no benefit to the patient; against non-maleficence, because it may harm the patient; and against distributive justice, because the additional time and cost expended could be better used on patients who would benefit. It is, therefore, important that futility of the treatment is acknowledged so that a decision to withhold/withdraw on-going support can be taken. Patients and families should be involved in these discussions, but as with other medical interventions on the ICU, withholding/withdrawing therapy is ultimately a medical judgment. Once the decision has been made, every effort should be taken to insure a peaceful and dignified death.', 'Although end-of-life planning is often associated with older adults, the information applies to all individuals over 18 years of age. In many cases, especially with young adults, patients are not aware of the function or importance of advanced directives and have not given any thought to the concept.\nAn advance directive is an affirmation made by a person, who is competent to make their own decisions, describing how they wish to be medically treated or not treated in the event they lose the capacity to communicate those wishes. This can include withholding or withdrawing life sustaining treatment, decisions regarding resuscitation, and can give authority to others to make these decisions on a patient’s behalf when they are not able. (Salins, N., Gursahani, R., Mathur, R., Iyer, S., Macaden, S., Simha, N., Mani, R.K., & Rajagopal, M.R., 2018)\nThe Patient Self-Determination Act (PSDA) is a federal law that obligates nearly all health care facilities to furnish patients with a written statement of their rights and to document the advance directives of patients when provided. The PSDA also mandates these health care facilities to educate their employees regarding advance directives. Under the PSDA it is unlawful to necessitate advance directives or deny patients who have them. (Campbell, J., 2020)\nThe PSDA has had a substantial impact of the end-of-life decision making process. Because of the presumption that extending life, with rare exception, is in a person’s best interest, the law is built with this as a default. When a person has an advanced directive in place that refuses treatment that may ultimately shorten their lives, health care providers now have an understanding of the treatment that may or may not be provided, instead of starting with the assumption of extending life. These decisions do not have to be defended or explained. (Shepherd, L., 2014)\nState laws also impacts advance directives and end-of-life decisions. The law varies state by state. Some states may legally remove a patient from life support in the event it is decided to benefit the patient, but that discretion is limited. The general rule is that a person should be terminally ill or in an unconscious state that is believed to be permanent. However, some states allow the withdrawal of life support in patient’s with end-stage diseases or conditions and advanced dementia, and a few states do not restrict the jurisdiction to withdraw life-support to any specific medical condition. (Shepherd, L., 2014)\nState laws differ significantly on an individual’s right to die and pain management, as well. When we talk about a person’s right to die, it is referring to a person’s right to end their life when and in a way they see fit. There are several methods and terms that are used when discussing this, including ‘aid in dying’, ‘physician-assisted suicide’, and ‘VSED’ (voluntarily stopping eating and drinking).\nPain management, in the setting of end-of-life decisions, include high-dose medications, medical marijuana, and palliative/total sedation. Palliative/total sedation, also known as terminal sedation, is used to alleviate extreme pain that is not helped by other medications, by keeping a dying person in a sedated state until death. (Glossary of terms, 2019) And, I’m sure we have all heard about the most recent debate regarding medical marijuana. While medical marijuana is now legal in almost three-quarters of the country, not all medical facilities will prescribe or even recommend this as a pain management option. (Hanson, K., nd) So, while the state laws regarding these issues can impact your end-of-life decisions, it is important to research your own state laws.\nEnd of life decisions are part of advanced care planning. Advanced care planning and advanced directives are related, but not the same thing. Advanced directives are legal documents expressing your wishes regarding medical treatment, but advanced care planning is more than this. A significant step in advanced care planning is discussions with family members and any other person who would be involved in decisions regarding your health in the case of a critical illness or injury. The person(s) that you choose as your ‘Durable’ Power of Attorney (POA), will need to understand your wishes and be willing to take responsibility to ensure proper action is taken. (Advance Care Planning, 2018). The Power of Attorney is one of the documents considered to be an advanced directive, this gives legal authority to a person(s) to make decisions on your behalf when you are no longer able to do so. Another part of an advanced directive is a living will, which expresses specifically which medical treatment you want or do not want in the event of critical illness or injury.\nEven though the PSDA makes it unlawful for healthcare facilities to require advance directives or deny patients who have them, there are implications of missing directives for dying patients. It can cause misunderstanding and conflict between family members who are attempting to determine what the patient would want. (Advance Care Planning, 2018) There is also research that shows a substantial difference in the medical care that is given to, or not given to patients with advanced directives versus patients without advanced directives. In patients without advanced directives, skilled nursing facilities see an increase of feeding tube placements and more difficult transfers to hospitals in patients with dementia. In the Veterans Affairs (VA) system, senior adults who have an advanced directive are less likely to die in a hospital setting versus community living centers. (Kossman, D. A., 2014)\nCertain treatments and courses of action are impacted by a person’s religious and cultural beliefs. This is where medical ethics, religion/cultural beliefs, and regulations in healthcare intersect. Although death is an event that happens to everyone, regardless of background, one thing that every culture has in common is the hesitancy to discuss the end of one’s life. (Moss, K. O., Deutsch, N. L., Hollen, P. J., Rovnyak, V. G., Williams, I. C., & Rose, K. M., 2018) This is why it is so important to consider advanced planning early. One might ask if it is ethically responsible to remove a person from life support or assist them in dying. But beliefs in end-of-life issues are profoundly rooted in religion and cultural background and vary widely within socio-cultural contexts, and even generations. (Schweda, M., Schicktanz, S., Raz, A., & Silvers, A., 2017) This is why the laws vary from state-to-state, and are changing constantly. There are no right answers to these complex questions regarding end-of-life decisions. We can only do what is right for ourselves and our families.']"	['<urn:uuid:e4ece234-0722-41b5-8c48-45466bcafe2f>', '<urn:uuid:8ed529d8-b8b0-490e-88ab-1c60a608dea9>']	factoid	direct	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-12T23:00:04.658849	26	101	3696
92	yale bulldogs patriots both have blue color	Yes, both use blue in their colors. The New England Patriots use dark 'royal' blue as part of their color scheme since 1971, while Yale University's sports team Yale Bulldogs uses Yale Blue, which was introduced in the late 1800s.	['New England Patriots logo png\nNew England Patriots are a professional rugby franchise from the East Division of NFL, which was established in 1959 in Massachusetts. Today the franchise is owned by Robert Kraft and the team’s head coach is Bill Belichick.\nThe visual identity of The New England Patriots has always been a graphical reflec-tion of their name and their value of national heritage and legacy. Since 1971, the team’s official color palette has been composed of red, blue and white, the colors of the American flag. Only the very first logo of the franchise was different, but it only stayed for one year and transformed into a new one.\nThe initial logo for the Boston Patriots depicted a blue and white tri-corner-hat. It was a simple and laconic image, which perfectly reflected the mood and idea behind the naming of the franchise.\n1961 — 1964\nThe redesign of 1961 brought the new symbol to the Patriots — it was a Minuteman wearing blue, white, and red closing, in the same Tri-corner hat, but in red now. The football on this logo version was colored red.\n1965 — 1970\nIn 1965 the contours of the logo have been modified and the football gained a new color — brown and blue. The minute man’s hands and face also gained a natural pink skin tone.\n1971 — 1992\nThe team kept the logo from 1965 after their name change to New England Patriots and it stayed with the franchise for more than 20 years.\n1993 — 1999\nThe completely new image was designed by Ken Loh in 1993. The symbol depicting a profile of a man in a tri-corner hat, with its left part weaving as the flag, has become iconic. The hat is executed in blue and red with a white star in the middle.\n2000 — Today\nThe redesign of 2000 only changed the color palette, both blue and red gained new darker and more “royal” shades. In addition to the main logo the secondary mono-gram was designed — two overlapping letters, “N” and “E” in blue are accompanied by the same red weaving stripes as on the hat from the main emblem. The white star is placed on the top part of the letter “N”, adding recognizability to the monogram.\nNew England Patriots alternateve logo\nPat Patriot, the minuteman from the original team’s logos, is the symbol and mascot of the franchise. The man symbolizes strength and power along with the patriotic mood and fundamental approach. The color of his clothing is a celebration of national spirit and love to the motherland.\nThe image, designed by Ken Loh, was nicknamed “Flying Elvis” and on the club’s emblem, it is usually placed on a solid blue or red background and is sometimes accompanied by the script “Patriots” wordmark in white.\nThe two main features of a simple great Patriots’ helmet are a bright red grill and an enlarged Flying Elvis emblem placed on the sides. It looks stylish and bright, staying professional and authentic at the same time.\nThe color palette of the main Patriots’ uniform repeats the helmet design — blue jersey with white numbers in red outline feature delicate gray details, and the gray pants are complemented by the wide blue and thin red side stripes.\nAs for the road uniforms, the team wears white jerseys with blue and red details and royal-blue pants with red and white side stripes.', 'Yale University is a private university owned by the Ivy League. It is one of the oldest in the United States since it was opened in 1701 by the Connecticut Colony. At first, the educational institution received the status of Collegiate School. In 1718 it was renamed Yale College. Now it is the largest educational institution, consisting of 14 departments, including 12 vocational schools, one high school of arts, and a bachelor’s degree. She also has her own sports club called Yale Bulldogs. The university is located in New Haven, Connecticut. Many prominent personalities have emerged from its walls, including five presidents of the United States.\nMeaning and History\nThis institution of higher education originated from the An Act for Liberty to Erect a Collegiate School, approved by the General Court Colony of Connecticut. This happened in October 1701, intending to train the clergy and secular leadership of the state. The founders included many people, including Samuel Andrew, Samuel Mather, Israel Chauncy, Thomas Buckingham, and many others. James Pierpont headed the group.\nThe institute was initially based in the home of the first president of Yale University, Abraham Pierson. He lived in the city of Killingworth (now – Clinton). Then the educational institution was moved to Saybrook and later to Wethersfield. In New Haven, Connecticut State, this educational institution appeared in 1716.\nTo adequately settle in a new place, the priest Cotton Mather, at the insistence of the leadership (the rector of the university Samuel Andrew or the governor of the colony Gurdon Saltonstall), turned to the successful Boston businessman Elihu Yale with a request to help in the construction of the new building. He, in turn, provided them with nine bales of goods, which were subsequently sold for 560 pounds sterling, which made it possible to build the main building. At the same time, Cotton Mather proposed to name the university in honor of the sponsor – Yale College.\nThe Yale University crest comprises three key components that are of immense importance to the institution. The open book is not only a symbol of enlightenment. Some researchers see in her an image associated with theology and church clergy who founded the university. They assume it is a Bible open to a page with an important message to posterity. The inscription is in Hebrew.\nThe background of the white and yellow book is a dark blue shield of the correct shape: an even top, a narrowed bottom. Along the edge, it is outlined with a black stripe that runs along the entire contour. Below is a tape with bifurcated ends and a dictum in Latin “Lux et Veritas.” This is the motto of the university. On both sides of the book, there are strings that were used to seal paper editions.\nThe modern emblem of Yale University contains part of its name – the first word. It is made with classic printed characters – straight lines, with miniature serifs. But “A” is different from the usual version. Its central part is not completely round but slightly cut diagonally. “E” has an overly large bottom and a disproportionately small top. The inscription is made in the form of a combination of one uppercase and three lowercase letters.\nYale Bulldogs Logo\nThe university has its own sports club, whose representatives compete in several interuniversity programs. They are part of NCAA Division I, Ivy League, ECAC Hockey, NEISA. The most famous are the football and ice hockey teams, which are considered among the oldest in the college system. The club plays at the Yale Bowl Arena and has an interesting history of logos.\n1901 – 1972\nThe history of the logos of the sports department, owned by Yale University, began with the “Y” sign. It is very symbolic for the university since it denotes several concepts at once: the location of the higher educational institution, as well as its name, which has been transferred to university teams. It was decided to make the initial letter an independent emblem to show their unity. It is large, blue, complemented by rectangular serifs.\n1930 – 1942\nSports emblems have nothing to do with university symbols. They contain completely different images. The debut version features the head of a bulldog wearing a football player’s leather helmet. The drawing is executed in sinuous blue lines and is set against a white background. The dog’s gaze seems to be riveted to some object, which it closely follows, slightly tilting its head. The design is line-like, so the bulldog appears to emerge from negative space.\n1960 – 1972\nThe club chose a realistic drawing of a dog as its logo. The bulldog is depicted standing on four legs and with an arrogantly raised muzzle. His gaze is formidable, fangs eerily protrude from his lower lip, his forehead is wrinkled, and his ears are thrust forward. The animal is wearing a black collar with large spikes.\n1972 – 1995\nDuring this period, the anthropomorphic dog logo was used. She stands tall, crossing her legs and leaning her elbows on a large “Y.” The eyes of the bulldog are large and black, and the gaze is intent. The animal is wearing a sweater with the name of the university. The big letter on which the dog is leaning has two edging stripes – black and white.\n1995 – 1997\nAs a result of the redesign, the logo received a completely different interpretation – a serious, stylish, aggressive one. The developers turned the dog’s head in profile and placed it in the center of the circle on a light blue spot. It is followed by a larger circle – a dark shade. Next comes a wide strip with the name of the sports department: at the top is the word “Yale,” and at the bottom – “Bulldogs.” Both inscriptions are typed in a thin grotesque. The letters are white, with minimal intercharacter space. Along the edge of the emblem is a dark blue ring that replaces the frame. But the most interesting element of the visual identity is the 15 stripes. They overlap all layers and add dynamics to the logo.\n1997 – 2019\nThe current version is abstract: it contains only the capital “Y” and the bulldog’s head, located sideways. He looks to the right and looks very menacing. Moreover, the designers returned to the stroke-like style of drawing, so the dog again seems to emerge from the negative space. Its background is the first letter of the name Yale University. The author of this version is graphic artist Jeffrey Russo.\n2019 – today\nThe current version of the logo is a reincarnation of the debut emblem. This is its improved version since a double border has been added to the existing letter: white (inner line) and dark blue (outer frame).\nFont and Colors of the Emblem\nThe identity of the oldest university in the United States contains iconic elements that reflect the history of its origin. But sports symbols are more modern and have nothing to do with legendary images. It is dominated by the bulldog, which appeared thanks to the sports department Yale Bulldogs. The last logo was nicknamed Y-Dog.\nThe original typeface is called Yale. It was designed by Matthew Carter, an art professor, using the oldest typeface, De Aetna. Its author is Francesco Griffo, who created the typeface back in 1495. In 2014, the university completely overhauled its typography when Carter & Cone introduced an updated set of Yale fonts in the OpenType standard. This format has replaced the outdated PostScript Type 1. Now the family of university typefaces consists of almost 30 variants.\nIn addition, the school has its color – Yale Blue. It is available in two shades: spot and triad. Introduced in the late 1800s, it is now found in all identity signs. Typically, a logo combines blue with white. Some emblems and coats of arms also use black and yellow.']	['<urn:uuid:d848b9dc-f3ab-4fbc-acf2-339469f34714>', '<urn:uuid:be8bccd9-e614-4532-943f-533ba6286c6c>']	factoid	direct	short-search-query	similar-to-document	comparison	novice	2025-05-12T23:00:04.658849	7	40	1889
93	how do furiten rules in mahjong compare to fouling rules in open face chinese poker	In Japanese mahjong, furiten is a state where you cannot win by ron (taking a discarded tile) but can still win by tsumo (drawing a tile yourself). This occurs mainly when you've previously discarded one of your winning tiles. In Open Face Chinese Poker, fouling occurs when hands aren't arranged in ascending strength from front to back (e.g. if middle hand is stronger than back hand). Fouling in OFC results in an automatic loss worth 6 points to opponents, while furiten in mahjong is less punitive as it only restricts one winning method while allowing others.	['How is Japanese mahjong different? Japanese Mahjong (Japanese: 麻雀, 麻将 or マージャン; mājan), also known as riichi mahjong, is a variation of mahjong. While the basic rules to the game are retained, the variation features\nHow is Japanese mahjong different?\nJapanese Mahjong (Japanese: 麻雀, 麻将 or マージャン; mājan), also known as riichi mahjong, is a variation of mahjong. While the basic rules to the game are retained, the variation features a unique set of rules such as riichi and the use of dora.\nIs Japanese mahjong different than Chinese?\nWhile the same basic ideas apply (Build a hand with 4 sets and a pair, more difficult hand patterns score more), there are special additions to Japanese mahjong that make it more viable to play defensively and allow for a greater depth. In Chinese mahjong, the best defense is a strong fast hand to beat out the others.\nHow do I get better at Japanese mahjong?\n101 Tips on How to NOT Lose in Riichi Mahjong\n- Don’t deal into yakuman.\n- Tenpai doesn’t matter if you deal in.\n- Be sure you can read all the man/character tiles properly.\n- Make sure that yakuhai is actually your own/round wind.\n- Make sure you aren’t in furiten.\nWhat is the Japanese game mahjong?\nRiichi mahjong is a Japanese variant of the ancient Chinese game of mahjong. It is a tabletop game that is played by four players, with each player having a hand which they must try and complete to win points from the other players. It shares similarities with Rummikub, and card games such as gin rummy and poker.\nWhat does Furiten mean in mahjong?\n“Furiten” is a state where you are unable to win a hand by ron. However, while in furiten, you can still win by tsumo. Your hand can be furiten due to a few different reasons. The main one is if you previously discarded one of your winning tiles. This puts your hand in permanent furiten.\nWhat does Furiten mean in Mahjong?\nHow do you score mahjong?\nScoring in Mahjong, a game for four players that originated in China, involves the players obtaining points for their hand of tiles, then paying each other based on the differences in their score and who obtained mahjong (won the hand). The points are given a monetary value agreed by the players.\nHow many players can play mahjong’s?\nAlthough Mahjong is usually played with four players, no major rule needs to be altered from the four player game in order to work for only two players.\nCan you play mahjong with 2 people?\nMahjong, the classic Chinese tile game, dates back to the Tai Ping Rebellion of 1851-1864 and was popularized in the West in 1920 with the publication of the book “Rules for Mah-Jongg.” Although normally played with four players, Mahjong can be played by two people with only a slight variation to the rules.\nHow many players in mahjong?\nMahjong, a Chinese game of cunning, strategy and chance that is played around the word, typically requires four players. A variation of mahjong, however, allows three players to compete in the game. The three-player mahjong game requires reducing the number of tiles and dice in play.', 'Open Face Chinese Poker (OFC)\nA variation of Chinese Poker called Open Face Chinese Poker (OFC) has been spreading wildly across the world. In OFC the basic gameplay is as follows:\n- Each player is dealt five cards initially.\n- These cards are set in either the front, middle or back hands in any way the player sees fit.\n- Play continues in rounds with each player being dealt one card at a time and placing that card in any available hand.\n- Play rotates clockwise with the ‘under the gun‘ player always revealing his cards first. This process is repeated until all players have been dealt 13 total cards.\n- The hands and royalties are scored and each player settles with the other players. Rules for fouling a hand are the same as traditional Chinese Poker.\n- A 52-card deck is used (2 to Ace, excluding Jokers). The card suits are equivalent.\n- The maximum 3 number of players at a table\n- 17 pocket cards.\n13 of 17 cards must be laid out into 3 boxes.\nThe remaining 4 cards must be discarded.\n- No community cards.\n- No betting rounds (no blinds, ante, etc.)\nEvery player at a table takes one seat with 3 boxes to lay 13 cards out into (5 cards into the middle and back (the lowest) boxes, 3 cards into the front box (the highest). At the end of a hand, the back box hand must be stronger than or equal to the middle box hand, and the middle box hand must be stronger than or equal to the front box hand.\n- The game is played for points. The goal of the game is to achieve more points than your opponents by winning more hands and/or by collecting royalties on premium hands.\nPoint score is the cost of one point (for example, 1 cent for 1 point). The calculation is made at the end of a match.\n- Cap – the maximum limit of loss in one match. Thus, over a match, a player can lose the maximum amount of money equaling to that specified in Cap.\nHand Structure in Open Face Chinese\nIn both traditional Chinese Poker and Open Face, players will look at their cards and begin setting up three different poker hands,\n- A three card hand, known as the Front\n- A five card hand, known as the Middle\n- Another five card hand known as the Back\nFor a hand to be valid, the back hand must be the strongest of the three hands in accordance with traditional poker hand rankings. The middle hand must be of a lower rank than the back hand, but higher than the front hand. The front hand must be the lowest strength. A valid hand would look like:\n- Front – 2 2 7 (Pair of Twos)\n- Middle – A A 4 4 J (Two pairs: Aces & Fours)\n- Back – 8 9 T J Q (Straight)\nIf the order of strength does not go front to back, weakest to strongest, it is called a fouled hand.\n- Front – 2 2 7 (Pair of Twos)\n- Middle – K K 3 3 9 (Two Pairs: Kings & Threes)\n- Back – Q Q 8 8 J (Two Pair: Queens & Eights)\nOnce all three hands have been set, players will compare front, middle and back hands. Each hand is worth one point. If one player has won two of the three hands they will score 1 point (winning two points and losing one for a net of +1 points). More points can be earned if the player wins all three hands, known as scooping. Players can also earn points by making high ranking hands known as royalties.\nA player scoops the hand by winning all three showdowns with their front, middle, and back holdings. The player is awarded one point for each showdown won, and three bonus points for scooping for a total of six points.\nWhen a player has fouled their hand, it counts as an automatic scoop and is worth six points. If both players have fouled their hand, the game is considered a tie.\nFront, middle and back rows may earn points for making certain hands as long as the hand does not foul. The type of royalties and how much they are worth varies from game to game. Below is one common scoring chart:\nMulti Player Scoring\nIn a game with more than two players, scores are settled by position. The player under the gun will settle with each player in a clockwise order. The player to the left of the under the gun player will then settle their debts. This continues until are players have paid.\nIf a player fouls in a multi-player game, that player will pay 6 units to each player plus any royalties on hands that have not fouled.\nFantasyland is a type of ‘bonus round’ in Chinese Poker. A player may enter Fantasyland when they have made a pair of Queens or better in the front hand without fouling.\nEntering Fantasyland means that on the next hand, they receive all 13 cards at once. They make all three hands at once, a huge advantage as they’ll be able to see if they hit any straights or flushes in advance, and lay cards perfectly without any risk of fouling.\nOther players play as normal, with just one card dealt at a time. The button does not move during Fantasyland – this is considered a continuation on the previous hand.\nA player may stay in Fantasyland (and receive the same advantage next hand) if they make any of the following hands:\n- Three of a kind in the front\n- Full House or better in the middle\n- Four of a Kind or better in the back']	['<urn:uuid:7317a16b-c4dc-4161-b2c9-da04baf86b39>', '<urn:uuid:fdd22102-04b0-4361-844b-f6bb124d9c45>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-12T23:00:04.658849	15	96	1513
94	How did the printing press impact Luther's reforms and Vatican II's approach to scripture?	The printing press enabled Luther to widely circulate his 95 Theses throughout Germany and translate the Bible into German for mass distribution. In contrast, Vatican II through Dei Verbum encouraged all faithful to read Scripture with enthusiasm, emphasizing that prayer should accompany reading God's Word, while maintaining that Scripture must be interpreted within Church tradition.	"['A. What is it?\n>> “Dei Verbum” is the Vatican II Dogmatic Constitution on Divine Revelation, promulgated by Pope Paul VI on 18 November 1965\n>> The phrase “Dei Verbum” is Latin for “Word of God”\n>> It is one of the smallest Vatican II Documents (26 paragraphs or roughly 3,000\nwords in Latin)\nB. What does it speak of?\n>> “Dei Verbum” addresses the Catholic Church’s beliefs in regards to Sacred Scripture.\n>> “Dei Verbum” is laid out into 6 Chapters:\n1. Chapter 1: Divine Revelation Itself\n>> Speaking on the Nature of Revelation, this chapter demonstrates God’s desire to communicate with human beings, revealing the mystery of the Divine Will.\n>> It offers a summary of the Salvation History\n>> It also emphasizes the Truth of this Revelation and the fact that it is accomplished in such a way that human beings can comprehend it.\n2. Chapter 2: Transmission of Divine Revelation\n>> The Truth of Revelation, is rooted in Christ’s very person and in his own proclamation of the Gospel; having commissioned the Apostles to carry it forward…, the truth of the\nGospel also lies in the Apostolic Tradition.\n>> Both Scripture and Tradition must be accepted and honored with equal devotion and\n>> Tradition and Scripture make up a single Sacred deposit of the Word of God\n3. Chapter 3: Sacred Scripture: Its Divine Inspiration and Interpretation\n>> It affirms the importance of both- the Old Testament and New Testament\n>> It adopts the threefold-process of the Formation of the Gospels with the three levels: (i) the time of the Historical Jesus (ii) The oral preaching of the earliest apostles (iii) The time of the Evangelists\n4. Chapter 4: The Old Testament\n>> The plan of salvation was spoken through the authors of the Old Testament.\n>> Its purpose was to prepare for the coming of the Christ and to show to all, how God interacts and deals with mankind in justice and mercy.\n>> God wisely arranged for the New Testament to be hidden in the Old, and the Old to be made manifest in the New. While Christ made the new covenant with His blood, the Old Testament sheds light on and explains this mystery.\n5. Chapter 5: The New Testament\n>> The New Testament stands as a Perpetual and Divine Witness to the Reality of Salvation.\n>> The Gospel Authors wrote about things handed on by word of mouth or in writing, sometimes a synthesis, sometimes as a proclamation, but always the honest truth about Jesus.\n6. Chapter 6: Sacred Scripture in the Life of the Church\n>> The Church has always venerated the scripture together with the Tradition as the supreme Rule of Faith.\n>> The Church encourages the study of the Church Fathers as well as those exegetes who so well illuminate the teaching within the scriptures.\n>> Individuals should read with enthusiasm, following the mind of the Church.\n>> All clergy must read the scriptures with diligence. The same is encouraged for the laity and Religious. All faithful should not forget that prayer should always be the companion to reading God’s Word.\nC. Pointers for Reflections\n1. “Dei Verbum” is considered as one of the important achievements of the Vatican Council II since its implications is for the treatment of Sacred Scripture itself.\n>> It accords rightful significance to the Bible as the special locus of Divine Communication or Divine Revelation.\n2. It presents three key principles of Catholic biblical interpretation:\n(i) Pay attention to the content and unity of all the Sacred Scriptures.\n(ii) Read and interpret the Bible within the living tradition of the Church.\n(iii) Keep in mind the coherence of all the truths of revelation\n3. The understanding from “Dei Verbum” is enshrined in the Catechism of the catholic Church (CCC), affirming reading Scripture for its four classical sense – the literal sense, and then the spiritual sense divided into three: the allegorical, tropological, and anagogical senses.\n>> The allegorical sense (Typology) concerns how the Old and New Testaments relate, the tropological sense is the moral sense, and the anagogical sense concerns the soul’s progress to heaven.\nD. What virtues/points can we pick up from the “Dei Verbum” for this Season of Lent?\n1. Making it a Daily Habit to Read the Bible\n2. Studying the Bible and going deeper into understanding the meaning of Scripture in our daily life\nE. Tips to practice these virtues\n1. Set apart a time, daily, to read God’s Word.\n>> Just as our meals become a daily “must”, so should the Bible be part of our daily “sustenance for strength”\n>> “Ignorance of Scriptures is ignorance of Christ” says St Jerome\n>> Christ is the primary and ultimate revelation of God. So the more we read and reflect on Scripture, the more we can know Him and love Him\n2. Learn, practise and revive the Catholic Tradition of the “Lectio Divina”(= a Latin term, means “divine reading”)\n(i) The first stage is LECTIO (reading): Read any passage of the Word of God, slowly and reflectively so that it sinks into us\n(ii) The second stage is MEDITATIO (reflection): Think about the text we have chosen and ruminate upon it so that we take from it what God wants to give us\n(iii) The third stage is ORATIO (response): Leave thinking aside and simply let the heart to speak to God.\n(iv) The final stage is CONTEMPLATIO (rest): Let go of our own ideas, plans and meditations and also holy words and thoughts. Simply rest in the Word of God and listen, to God, who speaks within us with a still small voice.\n>> As we listen, we are gradually transformed from within and this will have a profound effect on the way we actually live.\nMay this Lent and the familiarity with “Dei Verbum” – the Vatican II Dogmatic Constitution on Divine Revelation – help us to grow in our acclamation: “Eureka – I have found the Lord”\n(The Full Text of “Dei Verbum” can be found at:\nGod Bless! Live Jesus!', 'History of the Church\nRe-Cap of Part I:Forces that Weakened the Catholic Church’s Stronghold in Europe\nCrusades– “Holy Wars” not so holyGreat Schism– Bitterness between East/WestInquisition– Only True Believers ClubThe Plague– Death Comes to EveryoneGreat Western Schism– Will the real Pope please stand up?\nAnd some other factors. . .\nRise in Nationalism–Don’ttell us what to do!Selling of Indulgences–Payyour way to Heaven?\nPopular PreachersReformerswithinthe Church – JeanGerson, Cardinal Nicholas ofCusaHumanists – Thomas More, John Fisher, John Colet, ErasmusProtesting Men who went public with their grievances. . .\nProtestant Reformation:Many voices want Reform!\nMartin Luther - Germany\nFrom priest (Augustinian Monk) to ProtestorSin really bothered himFelt like a “dung hill”Only grace could redeemIndulgence selling REALLY bothered him!Faith or Good Works?Justification by Faith Alone\nTetzel – Indulgences for Sale]\nDominican Friar sent to Germany to raise money to re-build St. Peter’s""As soon as the coin in the coffer rings, the soul from purgatory [also attested as \'into heaven\'] springs.“Luther hated this! Wrote a long letter to the Bishop\nSampling of Luther’s Main Gripes : Became the “Ninety-five Theses”\nFAITH ALONE SAVES, Not good worksForgiveness comes only from God, not from the church(as ingrantingindulgences)Church “happens” wherever and whenever the Gospel is proclaimed purelyNo Need for Pope/Hierarchy – Authority comes from Scripture, notClergy; “sola scriptura”No Need for sacraments except Baptism and Eucharist\nLuther –WittenburgCastle –Posted The 95 Theses and He was Done!\nExcommunication – 1521 – Pope Leo Xat Diet of Worms, Germany\nGuttenberg’s Revolutionary Invention: The Printing Press\nUsed the printing pressto circulate his 95 Theses – everyone in Germany had themTranslatedthe Bible into German,printed it, putinto the hands of anyone who wanted itWrote a catechism, composed hymns, created liturgies and spirited theological writingsMany priests/nuns left the monasteries and convents to follow himIdeas embraced quickly throughout Germany, Switzerland, Scandinavia\nArgued with Luther : Are we dung hills or are we divine?Humanists like Erasmus were adamant that FREE WILL is the greatest of human giftsStayed faithful to the church and worked to reform from withinCondemned Luther for throwing out Sacred TraditionNot even the reformers could agree!\nMeanwhile. . .HuldrychZwingli – Switzerland – Luther Didn’t Go Far Enough\nZwingli’s Main Gripes –Luther Plus More\nFasting during Lent – Hated It!Statues and Images – Hated Them!Religious Music and Art – Hated them!Sacraments, mass, Eucharist – Hated Them!Hierarchy –ReallyHated it!We do NOT have free will; rather, salvation is pre-determined\nMeanwhile. . .John Calvin – France/Switzerland. . .continued the split\nCalvin’s Main Gripes & Ideas ~ Luther plus a little Zwingli and More\n“Sola Scriptura” - Scripture is the ONE and ONLY authority (NOT the Pope)Unity of Church realized through representative assemblies (individualscanNOTbe trusted!)Though there are titles, there is NO hierarchyPeople have the right to choose their own leadersChurch is SUPREME over theState\nMore Calvinistic Ideas. . .\nWrote “Institutes of Christian Religion”- the first written down theology of the new thoughtNEW DOCTRINE: PREDESTINATION – LOVED IT!Thriftiness, cleanliness, prosperity – Loved them! Signs of God’s presence and blessingCapitalism, individualism, democracy – Loved it!Influenced Puritanism, Presbyterianism, Reformed Churches and MANY churches in America\nAnd then there wasKing Henry VIII - England\nHenry’s Main Gripes and Actions:\nWanted an annulment from his wife, Catherine of Aragon – Pope hated it!Excommunicatedfor his marriage to Anne BoleynAct of Supremacy – 1534 – Parliament declared him head of the ChurchHenry appointed Cranmer Archbishop of Canterbury and so. . . . heRuled Henry’s marriage to Catherine invalid and their child, Mary, not an heir to the throneSeparation from Church -NOT DOCTRINAL THIS TIME! But Church and State are ONE AGAIN\nMeanwhile, Henry was busy trying to find the right woman who would bear him a son. . . Not much success!\nCatherine ofAragan– married 24 years; annulledAnne Boleyn – married 3 years - executedJane Seymour – married 1 year - died in childbirthAnne of Cleaves – married 6mos- AnnulledCatherine Howard – married 1 year - executedCatherinePaar– married 4 years,untlHenry’s death\nFurther Splintering: Radical Ecclesiology of the Left Wing\nEvangelicalism: Restore the First Century Church and make it separate from the world!Spiritualists– Spirit alone constitutes; no need for Scripture, structures, voluntary communityAnabaptists– re-baptism into early church; no organization, no infant baptism, highly moralistic, pacifistic, nonviolent; Quakers, Amish, Mennonites, etc.Unitarians-Some denied Trinity\nCounter Reformation: Council of Trent1545-63 (A long Process)\nCalled by Pope Paul II – Reactionary?Re-fined the Catholic position on sacraments, clergy, Scripture, doctrine of faith and grace, salvationEstablished a seminary systemReformed the Liturgy into a “Grand Spectacle;” No Laity involvementRevised the Latin “Vulgate” version of the BibleReinforced hierarchyAt the Heart: Society of Jesus (Jesuits)Emphasized Devotions and Practices (ex: Veneration of the saints, Marian devotions, Eucharistic Adoration)Reforms Shaped the Church for 400 years!\nIgnatius of Loyola – Society of Jesus(the Jesuits) began the Missionary Movement All Over the World\nInfluence of The Enlightenment: A Superfluous Church?\nIN EUROPE 1789Rene Descartes,Immanuel Kant, Diderot,Voltaire. RousseauFRENCH REVOLUTION’s 3-Point Agenda: Liberty, Equality, and FraternityFreed Church from feudalismDestroyed monasteries, churches, art\nDEISTS INAMERICA: 1770’sBen Franklin, Thomas Jefferson, Thomas Paine, other Founding FathersAMERICAN REVOLUTIONIndividualism, Freedom of thought, action, pursuit of happinessSeparation of Church and StateAnti-Catholicism in the colonies\nInfluence of Science and Philosophy on Religion\nComplexityGalileo and Copernicus declared heretics!Science “debunks” what was being taught as TruthNo more superstition!Rise in Secularization – people identified more with society than church\nPolarizationThe Missionary Movement – religion or culture?Individualism and Democratic ideas counter to Church hierarchyEducation and Modernism Push Back\nAmerican Catholicism: The Carroll Brothers: John and Charles\nCharles a patriot –signed the Dec. of Ind.John the first Bishop of BaltimoreDesired to make the American church more democratic – denied!Helped establish the church’s legitimacy\nThe Modern Church of 19thCentury\n19thCentury InfluencesDemocratic Ideas of Freedom of Speech, Press, due process, etc.Papal States strippedQuestion: What is the role of the church in society?\nFirst Vatican Council 1869 or: It’s All About the Pope!Papal Primacy(apostolic succession,vicarship, ministry)Papal Infallibility –immunity from error on matters of faith and morals; “ex-cathedra”\nDown with Modernism!20thCentury Developments\nInfluence of modern thinking about nature of TruthSome Catholics wanted reform! Modern church needs modern waysIdeas were condemned by Pope Pius X (1907)Upheld the foundations of the faith in all matters – DOGMA DOES NOT CHANGE!(butour understanding of it does)\nPre-Vatican II ( 1940’s – 60’s)\nA Great Divide – Protestants and Catholics did NOT dialogue; much division, anti-Catholicism in AmericaCatholics still in the “fortress mentality,” defending the faith like soldiers of ChristEra of “Pay, Pray, Obey!” Formula CatholicsLaity NOT involved. . .only ClergyIs the Church irrelevant? Left Behind? Superstitious?New Theology Emerges ecumenically: Dietrich Bonhoeffer, KarlRahner, EdwardSchillebeecks,HerideLubac, Hans Kung, etc.And then, most importantly:\nSt.PopeJohn XXIII – 1958-1963\nSecond Vatican Council: 1962-63\nCalled to “renew the Church” – “A New Pentecost”Diplomats, Press, Non-Catholics, ecumenical observers and Women were all invited to be thereChurch re-defined from the inside out (twin documents: LumenGentiumandGaudiemetSpesChurch called “people of God” as sacramentand servantTelevised; First council to have electricity!\nVatican II Summary Chart\nPre-Vatican II EcclesiologyChurch as institutionChurch as hierarchyMission: word and sacramentChurch as absolute monarchyTriumphalismChurch = Kingdom\nVatican II EcclesiologyChurch as mystery/sacramentChurch as People of GodMission: word, sacrament, servantChurch as a communionChurch as ecumenical communityChurch as eschatological community\nThe Church of the Future:Where will we be in 2020 and beyond?\nPondering the Future!\nGrowing number of people with no religious affiliationMore people who are “spiritual” but not “religious”Declining Participation in ChurchesIncreasing Diversity and PluralismInfluence of IndividualismChanging Structures of Family LifeDeclining Social Influence of ChurchIncreasing Impact of Internet and Social Media\nChurch of the Millennium\nGreat Values Shift (See handout)What are your values?When you die, what do you want to leave as a legacy? What will be said of the time you had here on earth?\n“I will be with you until the end of time. . .”']"	['<urn:uuid:d2844229-3d17-4735-bafc-d636b6c4f4a8>', '<urn:uuid:af6b3de2-45ad-4def-afcc-7b1b5d0b9303>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T23:00:04.658849	14	55	2279
95	what improved features m2a1 50 caliber machine gun compared to m2	The M2A1 .50-caliber machine gun has three major improvements over the legacy M2: 1) a quick-change barrel that pops into the receiver and locks in place quickly, similar to the M240B machine gun, 2) fixed headspace and timing, which eliminates the need for manual adjustment and reduces operator risk, and 3) a flash hider that reduces the weapon's signature by 95 percent at night.	['M2A1 Machine Gun Improves Marines’ Lethality, Survivability\nMARINE CORPS BASE QUANTICO, Va. — Marine Corps Systems Command (MCSC) is equipping Marines with the updated M2A1 .50-caliber machine gun, increasing Marines’ survivability and lethality on the battlefield.\nThe M2A1 serves as an upgrade to the legacy M2 machine gun currently used by Marines across the Corps. The updated M2A1 is easier and safer to use due to three major changes: a quick-change barrel, fixed headspace and timing, and a flash hider that reduces the weapon’s signature by 95 percent at night.\n“M2s are the most reliable heavy machine gun,” said Maj. Harry Thompson, team lead for General Purpose Weapons at MCSC. “The improved M2A1 makes Marines more lethal because they’re able to get rounds down range quicker. Marines will have better mobility because of the fixed headspace and timing — it’s much quicker to move the gun from position to position and put it back into action. Because they’re less exposed, Marines will have better survivability too.”\nThe current M2 requires Marines to manually set headspace and timing before firing, after assembly, and after required barrel changes when the barrel becomes extremely hot from high volumes of fire. “Headspace” is the distance between the face of the bolt and the base of the cartridge case when it is fully seated in the chamber. “Timing” is the adjustment of the gun so that firing takes place when the recoiling parts are in the correct position for firing.\nCorrect adjustments of both are necessary for the gun to work properly and continue to work; improper adjustments can lead to malfunctions that could potentially injure the user or damage the weapon. Fixed headspace and timing reduces operator risk and eliminates the need for Marines to master and execute this time-consuming procedure.\nThe fixed headspace and timing on the M2A1 enables the machine gun’s second major improvement — the quick-change barrel, Thompson said.\n“Marines no longer have to manually screw the barrel in to set the headspace,” he said. “Instead, the barrel pops into the receiver and locks in place quickly, similar to the M240B machine gun.”\nHaving a quick-change barrel improves Marines’ readiness by reducing the amount of time they are exposed to enemy fire and shortens the amount of time the weapon is out of operation. By spending less time manually adjusting headspace and timing settings, and having the ability to change the M2A1’s barrel quickly, Marines can respond faster and more efficiently to enemy fire.\nThe third major improvement is the flash hider at the end of the barrel, Thompson said. By significantly reducing muzzle flash, Marines can better mask their position in firefights. The flash hider also limits white-out conditions caused by the brilliance of the weapon firing, enabling Marines to use night vision devices more effectively.\n“All in all, MCSC will be fielding around 3,600 M2A1s to Marines,” said Kelly Sullivan, a program analyst for General Purpose Weapons at MCSC. “We completed Phase I fielding in March, which covered infantry units, the majority of infantry-like units and infantry Reserves. We’re currently starting Phase II, which should run through the end of May. The fielding process will be will be finished during [fiscal] ’18.”\nThe M2A1 is one of the many capabilities acquired and managed by MCSC’s Infantry Weapons Systems.']	['<urn:uuid:e0d1e7fb-9797-4538-bd4a-9b090043b17f>']	open-ended	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T23:00:04.658849	11	64	549
96	dehydrator vs hanging method herbs drying differences drawbacks	Both dehydrator and hanging methods have distinct characteristics for drying herbs. A dehydrator provides quicker, easier, and more consistent drying compared to other methods, with drying times between 6-24 hours. The hanging method works well specifically for herbs with lower moisture content like oregano, thyme, rosemary, marjoram, savory, dill, and sage - these can be hung in bundles from rafters allowing air circulation. However, hanging has drawbacks - it can lead to mold problems in humid areas, and herbs must be kept away from sunlight. Additionally, herbs with high moisture content like mints, tarragon, and basil should not be hung-dried as they are prone to mold formation and require a dehydrator or oven technique instead.	['A dehydrator provides its users with a quicker, easier and more consistent drying compared to drying herbs in the sun or through other methods. When using a dehydrator, there are certain things you need to do so that the herbs dry evenly and do not lose any of their functional properties. It is worth noting that drying herbs proves to be a useful and reliable way to preserve the herbs for later use in craft and cooking. It also helps in retaining the herbs’ flavors and as such; they remain nutritional beneficial throughout the time they will be preserved. Same as the article posted by Paul Woods, this article provides a guide on drying herbs in a dehydrator. Below are some of the steps to follow:\nWash the herbs\nHerbs may contain a ton of dirt from the garden, and as such, they should be washed to remove the dirt. When washing the herbs, ensure that you rinse them gently using clean water to remove the dirt without damaging the herbs. After washing them, spread the herbs on towels in the shade to remove the surface water. Avoid sun drying the herbs in the sun to remove the surface water, as this may be detrimental to the nutrients and moisture content in the herbs. A mild warm breeze is enough to remove the surface water when the herbs are spread out for about an hour.\nPrepare the herbs\nAfter all the surface water has been removed from the herbs, the next thing you should do is prepare the herbs for drying. Remove the stems (if you are drying leafy herbs). This helps in increasing the surface area of the herbs thus allowing for quick drying. Spread out the leaves on drying screens to provide sufficient space for air circulation. Do not place too many leaves on the tray at once, as this is likely to lead to uneven drying whereby some parts of the herbs dry more than the others.\nDehydrate until crumby\nOnce you have prepared the herbs for dying, the next thing you should do is place the herbs in the dehydrator to dry. You should then set the ideal temperature of the dehydrator depending on the herbs you are drying. Some herbs require high temperatures to dry while others require low temperatures. As such, it is necessary to understand the temperature requirements of the herbs you are drying. Avoid using too high or too low temperatures, as this is likely to interfere with the nutritional composition of the herbs. Generally, herbs take between 6-24 hours to dry well. The best way to know that the herbs are fully dried is by checking if they are crumby.\nStore the herbs in a glass jar\nAfter drying the herbs in the dehydrator, you should store them in a glass jar. Ensure that you use airtight storage to prevent moisture from affecting the moisture. Keep the leaves whole makes it possible to have a long-lasting from the herbs than if they were crumbled. As such, only crumble the herbs when they are ready to use.\nDrying herbs in a dehydrator provide you with control, flexibility and guarantee of constant supply of the herbs throughout even when the herbs are the off-season. This process is reliably fast and not as complicated as it is presumed to be.…', 'In many parts of the country, the beginning of August is the time to harvest and dry herbs. Many leafy herbs have budded and are ready to flower… the perfect time to harvest for drying. Herbs at this stage — just ahead of flowering — have the most flavorful, aromatic oils. Some herbs — basil, rosemary, lemon balm, parsley and rosemary — can be harvested multiple times over the course of the summer. It’s best to harvest in the morning after the dew has dried. Inspect your pickings carefully for dead or diseased leaves or signs of mold. Most herbalists recommend rinsing herbs and gently shaking them dry. We’ve always felt that rinsing removes valuable oils and try to keep it at a minimum, especially after a previous day’s rain.\nHerbs with a lower moisture content — oregano, thyme, rosemary, marjoram, savory, dill, sage — do well with hanging and air drying. They may be simply inverted, the stems bound together by string, and hung from a rafter or any overhang that allows circulation around the entire bunch. Keep your herbs from exposure to sunlight. Check frequently for signs of mold or mildew. Back in the rainy Pacific Northwest, we used to hang bunches of herbs in our barn and with luck and a week’s worth of dry weather, would have success. (We eventually bought a food dehydrator to avoid our mold problems.) Some herbalists, in an effort to keep their herbs from dust and other contaminants, suspend their herbs in a brown paper bag poked through with plenty of ventilation holes.\nHerbs with a high moisture content — mints, tarragon, basil — are best dried using a food dehydrator or the oven technique to prevent mold from forming. Oven drying requires spreading herbs in a single layer evenly across a cookie sheet and setting the temperature at the lowest setting, often 150 degrees or less. If this temperature is hard to maintain — and it is with most ovens — try turning your oven on for only five minutes every hour. Turn and rearrange your herbs at this time. In three hours or so, leaves should be crispy and ready to store. In her book The Whole Herb, Barbara Pleasant suggests using the microwave to dry small batches of herbs. Arrange them on a plate and microwave for 15 to 30 seconds before letting them cool completely and repeating the process. If after several cycles, you find that the leaves are ready but the stems are still moist, she suggests you finish them off in a warm oven.\nDried herbs are best stored in jars with tight fitting lids. Remember that when using dried herbs in your favorite recipe, they are three to four times stronger than fresh herbs… depending on how well you dried them. There’s nothing like opening a jar of dried herb — say thyme, rosemary or oregano — in the dead of winter and having the scent remind you of the glorious August day you picked it and set it to drying. Detailed instructions on drying herbs can be found here (PDF).\nAs in all gardening discussions, there are disagreements as to the best methods of drying herbs. Jim Long, author of the pamphlet Growing & Using the Top 10 Most Popular Herbs says that herbs should never be dried in the microwave. Too much of their oil is lost in the process. He’s also against hanging herbs up to dry “in little bundles in the kitchen. While they look pretty, it’s not a good way to dry them if you plan on using them for cooking afterward.” Best, he suggests, is the food dehydrator, or in a warm dark place, like an attic. Help us here, dear readers and fellow herb growers: what’s your favorite method for preserving herbs?']	['<urn:uuid:4064b42f-b8b1-4ff8-b903-73b6583142b6>', '<urn:uuid:01745903-e9f6-4156-b56e-da2499baf742>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T23:00:04.658849	8	115	1191
97	national cultural festivals food sharing customs worldwide ancient modern	Various cultures worldwide have traditions of sharing food during festivals. In the United States, families gather for Thanksgiving to share turkey, dressing, mashed potatoes and pumpkin pie. Similarly, Canada celebrates Thanksgiving with turkey and pumpkin pie on the second Monday in October. In the United Kingdom, they hold Harvest Festivals in September where they display food from their fields in churches and distribute it to the needy. On Amrum island, during the traditional Biakefest celebration on February 21, people gather around bonfires and share kale. The German Erntedankfest also involves sharing harvest foods and donating leftovers to those in need.	"['Frisian. Pure. Culture on Amrum\nFrom the Stone Age tombs of pre- and early history, through the burial mounds of the Bronze Age, the Iron Age house, the history of seafaring and whaling, right up to modern times – there is plenty to discover here. Visit our museums and history-steeped sites, and learn some interesting facts about the lives of the islanders. It’ll be worth it.\nSt. Clemens Church with a baptismal font from the 13th century, Gothic apostolic row, and other artistic treasures. The “speaking” gravestones in the cemetery reveal the lives lived by Amrum natives when they were whalers, seafarers, and millers.\nExperience the beautiful traditional Frisian costume worn by Amrum islanders during entertainment evenings showcasing local history at the midsummer festival and village fairs.\nIn earlier times, islanders used to bid farewell to sailors during the Biakefest. Every year on February 21, the Biakehaufen is ignited on the islands. People warm themselves by the fire and then savor some kale. Truly delicious.\nVisit the Iron Age plot at the Vogelkoje with a Stone Age tomb and an Iron Age house.\nThings really heat up on Amrum in the winter! On New Year’s Eve, disguised characters – so-called Hulken – move from house to house. At midnight, everyone congregates in Norddorf in the “Hüttmannwiese” or in front of St. Clemens Church.\nArt markets and exhibitions bring you closer to authentic Frisian craftwork.\nAmrum’s lighthouse is the highest accessible tower on the North Sea coast.\nAmrum on Wikipedia\nPre- and early history\nMegalithic tombs such as the dolmen of Nebel mark the earliest traces of human settlement. They originate from the Early Stone Age. There are numerous burial mounds from the Bronze and Iron Age, for example the “Esenhugh” in Steenodde. The remnants of an Iron Age village can be found in the dune area west of the Vogelkoje. It remains disputed as to whether the Ambrones, who together with the Cimbri and the Teutons fought the Roman Republic around the year 100 BC, originally came from the area of the island that was still connected to the mainland back then.\nRelics from Viking times such as dwellings and fireplaces have been discovered at several sites. The “Borag” hill situated to the east of Norddorf is home to what is believed to be a fortified tower castle from this period (German: “Burg” or English: “fortress”). The“Krümwal”, an earthwork stretching approximately 1.5 km between Nebel and Steenodde, is also believed to originate from this time.\nMedieval and modern times up to around 1890\nIn the early Middle Ages, the island was colonized by the Frisians who arrived from the mouth of the Rhine. During the Middle Ages, Amrum belonged to the so-called Uthlande, or outer lands, which only gradually fell under the dominion of the Danish King or the Duke of Schleswig. The Danish Census Book of King Waldemar from 1231 lists two county divisions of Föhr, namely Føør voestæ (western division) and Føør østær (eastern division), with Amrum belonging to the western division. Amrum is listed in the census book as ambrum, hus, ha, co. with the presence of houses, hares, and rabbits also mentioned in the text. After conflicts between the Danish kings and the counts of Schauenburg and Holstein over the rule of Schleswig, the western division and Listland became enclaves of the Danish Kingdom and – unlike neighboring regions – did not belong to the Duchy of Schleswig. This state of affairs endured until 1864, with no change between 1460 and 1484 with the pledging of the county division to Bishop Nicholas IV of Schleswig, and 1661 to 1677 or 1683 with the sale of the division to Count Hans von Schack.\nAmrum was represented by several councilors in the county division. They were replaced by “Gangfersmänner” in 1697 whose duties included collecting taxes among other things. At the same time, the county division was turned into a Birk, which would be led by a Birkvogt who lived on the island of Föhr. The state power was usually represented by just a few people or by nobody at all in the county division, meaning that the inhabitants remained largely independent save having to pay taxes. They enjoyed additional privileges; for example they were not required to serve in the military “indefinitely” from 1735.\nIn addition to salt works, agriculture, and fishing, seafaring was one of the island’s main sources of income. Sailors from Amrum, including many captains, were active in whaling and merchant shipping between the 17th and 19th centuries in particular.\nHark Olufs, a sailor from Amrum who had been enslaved by Algerians in 1724, advanced to the rank of a General until he was allowed to return to his native island in 1736. In the 18th and 19th centuries, the recovery of stranded ships became an important source of income for the island. The number of shipwrecks on Amrum’s western coast only started to decrease significantly with the construction of lighthouses from 1875 and the application of modern navigational technologies. Tourism began to flourish in the late 19th century and this would change the island’s economic structure forever more.\nAfter the war in 1864, Amrum, just like the whole of Schleswig, was jointly ruled by Austria and Prussia. Amrum then fell to Prussia and in 1867 became part of the Prussian province of Schleswig-Holstein. The island initially formed a municipality within the district of Tønder.\nEspecially after 1864, more than a quarter of Amrum’s population emigrated, with the vast majority heading for the USA. Links between Amrum and the USA remain strong to this very day.\nFrom the start of bathing to today\nOn September 1, 1885, the architect, Ludolf Schulze of Waldhausen bei Hannover submitted a request to the island’s community representatives in the hope of securing permission to start building work on a seaside resort in Wittdün on the southern tip. Even though his request was rejected, the seed of the idea for a coastal resort had been planted. Amrum local Volkert Quedens and Heligoland native Paul Jansen Köhn seized the initiative and started building the island’s first hotels in 1889. Heinrich Andresen came to the island in 1891. He founded a joint-stock company, bought the hotels and permit from Quedens and Köhn, and built a large spa hotel on the southern tip of Wittdün and the “Kaiserhof” which were opened in 1892. Unlike in many other seaside resorts, Amrum also welcomed large ships to dock. From 1893, a railway service was operated on Amrum.\nIn Norddorf, it was Pastor Friedrich von Bodelschwingh who assumed the role of building contractor. In 1890, he was granted approval to build a facility that would later consist of several sea hospices. These were run by the deaconess house Serepta. While Bodelschwingh had a Christian vision for his sea resort, secular endeavors to enter the tourism and recreational market were also afoot in Norddorf, where the hotelier Heinrich Hüttmann, among others, was a prominent player.\nOn October 13, 1912, the municipality of Wittdün was formed from the southern part of the island; on July 25, 1925, the municipality of Norddorf was formed from the northern part. On February 23, 1926, the remaining communities were merged and renamed Nebel.\nIn the 1920 referendum on nationality, there was a clear majority in favor of Germany, whereas a large portion of the district of Tønder voted for Denmark.\nFrom the 1950s, there was an upsurge in construction activity, especially in the western part of Nebel and Süddorf. The dikes in Norddorf and Steenodde broke with the storm surge of 1962, resulting in the flooding of Amrum’s two marshy areas. Today, Amrum residents live solely on tourism. Initially, this meant guest houses with simple rooms or hotels in typical resort architecture. However, since the 1970s, holiday homes have become especially popular. The “sea hospices” have long been a thing of the past and are run nowadays by the AOK-Nordseeklinik for mother-child recuperation. Property on the island is gradually being sold off due to the high prices that sellers can demand, just like on Sylt.\nLanguage and culture\nNowadays, High German is the main language spoken on Amrum. Around one third of the population can still speak the local island dialect of the North Frisian language called Öömrang. These 800 or so Amrum natives are generally multilingual. The North Frisian dialects vary quite considerably. Öömrang is very similar to the variety of Frisian spoken on Föhr but is quite difficult for speakers of the Sylt dialect to understand even though both belong to the same dialectal branch. Many Amrum natives also speak Low German because this was the language of the coastal seafarers. Danish is spoken by just a small number of Amrum residents.\nAmrum’s traditional costume bears the colors black and white, and is richly decorated with valuable silver jewelry. It is worn by girls and women, especially at confirmations and during tourist events. See also: Traditional costume of the islands Föhr, Amrum, and the Halligen\nThere are many peculiar customs on Amrum. Biakin is celebrated on February 21 (Öömrang: Piadersinj, German similar to: Petersabend). Huge bonfires are lit in a ritual to bid farewell to winter. People blacken each other’s faces with soot. This custom dates back to the old feast day of Petri Stuhlfeier (Öömrang: Piadersdai) which was originally marked on February 22, and is also celebrated in other North Frisian communities.\nThe Hulken takes place on New Year’s Eve and involves groups of mainly young, imaginatively dressed Amrum natives running from house to house as people try to guess their identity.', 'HOW PEOPLE GIVE THANKS AROUND THE WORLD\nIn the United States, Thanksgiving is the time when families gather together and express gratitude for all they have — often while enjoying the traditional feast of turkey, dressing, mashed potatoes and pumpkin pie.\nYou might consider Thanksgiving to be a uniquely American holiday, but national days of thanks are actually a global, multicultural phenomenon. While the event goes by different names in different countries, it almost always incorporates a harvest theme. Here\'s a brief rundown of some traditions around the world for you to enjoy as you eagerly await this year\'s turkey day. Happy Thanksgiving!\nCanada, our neighbor to the north, celebrated its version of Thanksgiving more than 40 years before the first Pilgrims arrived in Massachusetts. Explorer Martin Frobisher began the tradition in 1578 to give thanks for his safe arrival in Newfoundland. For a few hundred years, the date of Canada\'s Thanksgiving kept changing from late October to early November. In 1957, Canada\'s parliament designated the second Monday in October to be a day of thanksgiving to God and to the bountiful harvest. Other than the date, Canada\'s Thanksgiving traditions are very similar to America\'s, complete with large family dinners featuring turkey and pumpkin pie.\nIn South Korea, the celebration is called Chuseok, a three-day festival that begins on the 15th day of August according to the lunar calendar. Translated to the solar calendar in use today, the date of Chuseok varies every year, but always falls in September or October. During the festival, Koreans visit their ancestral properties and make offerings of freshly harvested food. The traditional dish for Chuseok is called Songpyeon, a rice cake filled with sesame seeds and beans or chestnut paste. During the festival, Koreans participate in singing, wrestling and archery competitions. They also give gifts to relatives, friends and business acquaintances to show their appreciation and respect.\nAlthough not declared a national holiday, the United Kingdom\'s thanksgiving is a Harvest Festival usually celebrated in September, on the Sunday of or near the harvest moon. On this day, they give thanks for a successful harvest and celebrate it by praying, singing and preparing baskets of fruit to take to churches. They bring food from their fields to display at a Harvest Festival service and then distribute it to the needy.\nHarvest is also the focus of thanksgiving celebrations in many African nations. These celebrations usually take place in August, marking the end of the rainy season. Because yams are often the first crop to be harvested, the Festival of Yams is one of the most popular events, but there are other celebrations, as well.\nFor Germans, the day is called Erntedankfest, which means ""harvest thanksgiving festival,"" and it\'s held in September or October. It\'s a religious observance, mostly celebrated in rural areas, in which participants attend a sermon in the morning, then have a thanksgiving procession to present the harvest crown to Erntekonigin, the harvest queen. The feast consists of fresh foods from the harvest and chicken that has been fattened up for the occasion. Participants donate any leftover food to the needy and further celebrate the holiday with music and dancing. Evening services, torch and lantern parades, and fireworks are also held in some towns.\nHowever you celebrate Thanksgiving, remember that by taking time to step back a bit and feel gratitude for what you have, you are part of a global tradition!']"	['<urn:uuid:76cb6ae7-ced7-4c16-ae33-35ace50ca540>', '<urn:uuid:21f215ce-62fa-4b73-9c8b-72f828abb972>']	open-ended	direct	long-search-query	distant-from-document	three-doc	novice	2025-05-12T23:00:04.658849	9	100	2163
98	building my home workbench need advice wood countertop or concrete countertop durability cost maintenance	Both wood and concrete countertops have different durability and maintenance requirements. Wood countertops can get scars and stains, and are susceptible to bacterial contamination, but can be renewed by sanding and resealing. Concrete countertops are durable and won't dry-rot, but need an epoxy sealer and routine maintenance to prevent stains and scratches. In terms of cost, concrete countertops are relatively inexpensive, while wood countertops require a skilled carpenter for installation.	"['Cabinetry is an important aspect of kitchen planning. So if you are planning a complete kitchen overhaul or just a modest facelift, any slight changes in the counter, sink and faucet can make a difference in your kitchen. However, the greatest difference can come about with changes in your countertop, the workbench of the kitchen.\nThe choices of countertop material are vast. You also don’t have to stick to one type of countertop material for your kitchen. Here are some options for countertops, and a bit more information to help you make an educated choice when you are remodeling your kitchen.\n1. Wood Counters\nWood countertops, often made of hardwood such as maple or oak, come in a wide range of colours and finishes. One popular type of wood countertop is face-laminated wood countertop or butcher block. Other types are oiled wood planks and lacquered wood. Wood counters can give your kitchen a beautiful warm inviting look that is functional.\nHowever, scars and stains are always inevitable, and wood is susceptible to bacterial contamination. Fortunately, wooden surfaces can be cleaned quite easily, and worn surfaces can be renewed by sanding and resealing.\nWood countertops can also be customized. You should note that wooden countertops must be installed and maintained properly. Therefore, you will need a skilled carpenter.\n2. Concrete Counters\nConcret countertops offers versatility in any type of kitchen style (modern or traditional); the different finishes add to its timelessness. You can craze, grind, polish, stamp, and stain it. You can even embed object in it. You can create the desired feel from your finish. So, it does not have to be rough, gray or monotonous; instead, it can be smooth and polished, and it can look coloured sandy, or rocky.\nA great advantage of concrete is that it is durable, it would not dry-rot. Even though concrete is tough, it is not resistant to stains or scratches, especially if the concrete is unsealed. Concrete surfaces is discoloured or made slightly rough by acidic liquids, such as wine, lemon juice, and vinegar. Therefore, an epoxy sealer and routine maintenance can help keep your concrete countertop in pristine condition. Also, you will need to keep hot pots away from the concrete surface. For this reason, if you think you are not a fan of the wabi-sabi look, and you firmly believe a countertop needs to be kept immaculate and new-looking you may not like concrete countertops.\nConcret countertops have a rustic and simplistic appearance. It should be noted that concrete is heavy, and it is a good idea to survey the conditions of the floor and cabinetry that will support it.\n3. Ceramic Tile Counters\nA ceramic tile countertop can give your kitchen an elegant or rustic look. It is one of the most prefered countertop style, because its is durable, inexpensive, attractive, and easy to install. Ceramic tiles come in a variety of colours, textures, and sizes, which lends itself to all sorts of attractive accents. Some are hand-painted and can add to a distinctly personal appeal in your kitchen.\nCeramic tile surfaces are heat resistant, but it can be uneven and likely to be damaged or damage dishes or cookware. The tiles can easily chip or crack and the grout lines can stain if not sealed regularly. You can minimize stains in the grout if you use grout that contains a latex additive and apply a grout sealer after the grout cures and reapply once a year thereafter.\nYou should note that porcelain tiles are the least of all tiles likely to stain; it is also more expensive than the other types of tiles. Other type of tile countertops are made of marble and stone materials (such as slate and limestone).\n4. Laminate Counters\nAnother common countertop surface is laminate; it is often referred to as high-pressure decorative laminate or plastic laminate. Plastic laminate is quite durable, has a smooth surface, and it is easy to clean. It is inexpensive, and comes in a variety of colours, patterns, and finishes (gloss, stain, and textured). It complements any home decor.\nHowever, laminates can be easily scratched or chipped and are impossible to repair. Therefore, you should avoid laminates that are dark coloured, too textured, or deeply embossed with patterns. It also has a low resistance to heat. Plastic laminate is easy to install and inexpensive to maintain, but will wear out with heavy use.\n5. Solid Surface Counters\nYou will find solid surface countertops an attractive choice, because they can be custom-made to your specification, are quite durable and easy to clean, and scratches and burn marks can be sanded out and polished. Solid surface countertops are available in a limited selection of colour and patterns for your decor. They are also expensive to buy at first, but inexpensive to maintain.\n6. Stainless Steel Counters\nIf you are going for the contemporary kitchen look, you would like the stainless steel countertops. A stainless steel countertop is a good choice because is heat and stain resistant, durable, easy to clean, and can be custom build to your specifications. However stainless steel is expensive, and it can be scratched easily. There are other metal type countertops, such as galvanized sheet and copper sheets. Stainless steel is most suitable where food preparation is concerned.\n7. Granite Counters\nGranite counters are an expensive choice for the homeowner. It is beautiful and comes in a wide variety of colours. It is durable, functional, and easy to clean. It could last a lifetime. However, it can crack if improperly installed.\nThere are a lot more options in the market for kitchen countertops, those presented here are the most popular selections. Then, there are countertops which are made from recycled materials, such as reclaimed wood or recycled lumber, recycled glass (beer bottles, stemware, traffic lights, etc.), and paper. Recycled-material countertops are a fashionable choice for the environmentally conscious homeowner. However, their manufacturing require extensive labour and are therefore expensive. This option is quite new, so methods on repairing any damage done to the material may not be fully known yet. Another downside to recycled material countertops is that there may be undesirable components in the products.\nAll pictures courtesy www.photobucket.com', 'Kitchen + Dining\nThese countertops are durable, heat resistant, and relatively inexpensive.\nBLACK+DECKER B+D Contributor 193 Projects\nConcrete countertops have many pluses and few minuses. They are durable, heat resistant, and relatively inexpensive. But most of all, they are highly attractive and a great fit with contemporary styles.\nOnce used exclusively for outdoor building projects, concrete has expanded its range to become a premier material for indoor construction as well. Still utilized mostly to cast countertops and vanity tops, concrete is continually finding new applications inside the home, including fireplace hearths, floors, and even furnishings. Along with remarkable strength and extreme durability, concrete has charm and appeal unlike any other building material.\nA concrete countertop may be cast in place or formed offsite and installed like a natural stone countertop. For a number of reasons, casting offsite makes more sense for most of us. In addition to keeping the mess and dust out of your living spaces, working in a garage or even outdoors lets you cast the countertops with the finished surface face-down in the form. This way, if you do a careful job building the form, you can keep the grinding and polishing to a bare minimum. In some cases, you may even be able to simply remove the countertop from the form, flip it over, and install it essentially as is.\nThorough planning and careful form construction are the keys to a successful concrete countertop project. One of the first issues to tackle is weight: concrete weighs about 140 pounds per cubic foot (roughly 25 pounds per square foot for a 2"" thick countertop). Most floors should be able to support a heavy countertop, but be sure to inspect floor joists and framing, especially in older homes, to determine if any reinforcement is needed. If you are unsure, consult a building professional, your local building inspector’s office or a structural engineer.\nThe weight of the concrete is also a factor when it comes to cabinetry. Typical base cabinets should be reinforced at the back and across the top with 1⁄2"" or ¾"" plywood. Reinforcing cabinetry may increase the overall dimensions, which can cause problems with modular units or in areas with limited space.\nAfter you design your project and determine the actual dimensions, you’ll need to estimate the amount of concrete you’ll need. Concrete is measured by volume in cubic feet; multiply the length by the wide and then by the thickness of the finished countertop for volume in cubic inches, then divide the sum by 1728 for cubic feet. For example, a countertop that will be 48-in.-long × 24-in.-wide × 31⁄2-in.-thick will require 21⁄3-cu.-ft. of mixed concrete (48 × 24 × 3.5 / 1728 = 21⁄3). The best way to achieve consistent results when mixing concrete is to use premixed materials. One 60-lb. bag of premixed high-strength concrete equals 1⁄2-cu.-ft. of mixed concrete. A number of online concrete outlets also offer a virtual rainbow of dry-mix color pigments that are formulated with a water reducer admixture. Water reducers limit the amounts of water the concrete mix uses to help produce a stronger mix and a smoother finished produce.\nAs you mix the concrete materials, blend all dry ingredients thoroughly in a motorized mixer for five minutes prior to adding liquid ingredients. Do not mix the concrete until the form is completely built, with any sink or faucet knockouts and the reinforcement in place. For best results, mix the concrete in a single batch with a power mixer. Because the mixing container on power mixtures should never be more than half full (one-third full on some models), you’ll need a relatively large mixer for all but the smallest countertops. For the island countertop projects shown here, a tow-behind nine cubic foot mixer was rented (yet another good reason for casting your countertop offsite). Once your casting is done, let the concrete cure for at least a week before you strip off the forms.\nThe basic supplies needed to build your countertop form and cast the countertop include: (A) Melamine-coated particleboard for constructing the form; (B) poultry netting or welded wire for reinforcement; (C) concrete sealer (product shown is adequate, but for better protection look for a sealer that has both penetrating and film-forming properties); (D) high/ early bagged concrete mix rated for 5,000 p.s.i.; (E) coloring agent (liquid or powder); (F) grinding pads (shown are 5"" diamond pads ranging from 50 grit to 1,500 grit for grinding and polishing); (G) paste wax; (H) buffing bonnet for polisher; (I) fiber reinforcement (nylon); (J) acrylic fortifier, latex bonding agent (or water reducing admixture if you can locate it); (K) black or colored silicone caulk; (L) faucet set if installing sink; (M) sink (self rimming shown); (N) No. 3 rebar (3⁄8”).']"	['<urn:uuid:bfebb806-5887-4888-af12-b09e6b8f9a0a>', '<urn:uuid:9bf75af0-4f82-4866-83c2-c6e4995dcba8>']	factoid	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-12T23:00:04.658849	14	70	1824
99	brooklyn dodgers 1940s 1950s achievements total number pennants world series wins before moving la	During the 1940s and 1950s, before moving to L.A. in 1958, the Brooklyn Dodgers won eight pennants and one World Series championship.	['With the exception of that team from the Bronx and, perhaps, the Red Sox, no baseball franchise in history is as storied as the Dodgers — especially the incarnation that played at old Ebbets Field in Brooklyn until the club’s abrupt (and, for countless Brooklynites, unforgivable) move to L.A. in 1958.\nThose Brooklyn teams from the 1940s and ’50s — filled with players bearing names like Reese, Campanella, Snider, Robinson, Newcombe, Hodges, Stanky, Reiser, Furillo, Branca and so many more — hold a special place not only in the memories of millions of fans of a certain age, but in the annals of the game itself. Dem Bums, after all, won eight pennants and one World Series during those two decades, and probably would have won a few more championships if they didn’t have to keep facing (and losing to) the powerhouse Yankee teams of the era.\nHere, however, LIFE.com offers a series of photos — many of which never ran in LIFE — not of well-known, Hall of Fame-worthy Dodgers but, for the most part, long-forgotten young hopefuls at spring training in 1948, the very first year the team trained at the “Dodgertown” complex in Vero Beach, Florida. (The Los Angeles Dodgers left Dodgertown in 2008 to hold their spring training in Arizona — ending 60 years of tradition.)\nThe photos here, by LIFE’s George Silk, are marvelous for a number of reasons, not least because, quite simply, they’re just damn good pictures. There is energy in the images, and a genuine playfulness as well as terrific personalities — captured by an acknowledged master of sports photography. But the real pleasure to be derived from these shots is the glimpse they afford us of baseball in the post-WWII era in America. To be sure, the players and coaches pictured here are all very, very white. Jackie Robinson had only broken the color barrier the previous year, and in 1948 there were only three (that’s not a typo) black players in the National and American leagues.\nBut the rather jarring racial uniformity aside, the photos in the gallery serve as a welcome reminder of the game’s inimitable appeal for fans who might have forgotten why they first fell in love with baseball in the first place: the sound of bat against ball, and ball against glove; the mingled smells of wood, dirt, grass and leather; warm sunshine, and the cool of the dugout; the thrill of competition and the beauty of teamwork; the pure geometry of the diamond — all of these pleasures are, to varying degrees, illustrated or suggested in Silk’s spirited pictures.\nThe cover story in which a few of these photos first appeared, meanwhile, in the April 5, 1948, issue of LIFE, made it plain that while professional baseball is certainly a game, it’s also undeniably a business — and it’s often nearly impossible to tease the two apart, and to tell where one aspect of the national pastime stops and the other starts:\nLast month 550 fresh-faced young Americans, supercharged with ambition to play baseball for the Brooklyn Dodgers, began pouring into Vero Beach, on the east coast of Florida, to participate in one of the most extensive talent roundups ever undertaken. The Dodgers paid the players’ fares to Vero Beach from every corner of the U.S., then fed and housed them, for eight weeks in an abandoned wartime naval station which they renamed the baseball city of Dodgertown. There Branch Rickey, parsimonious panjandrum of the Brooklyn National League club, personally superintended the operation of a baseball stock farm devised to improve the breed of the Brooklyn Bums, win at least five National league pennants in the next 10 years and enrich the company’s coffers by several hundred thousand dollars.\nBranch Rickey himself did not succeed as a major-league field manager (with the St. Louis Cardinals from 1919 to 1925), but that was because he had too many scientific theories about how baseball should be played and too few good players to make the theories work. Dodgertown proved to be the ideal place to test all of Rickey’s ideas. At the outset he laid down the law to his 35 instructors on how he wanted Dodgertown run — i.e., with metronomic precision. Everybody had to bounce out of bed at 6:45 a.m. After breakfast there was a classroom session on the intricacies of “inside baseball,” followed by mass calisthenics. Rickey wandered all over the camp, shaking hands briskly with the kid pitchers, not just to be friendly but to test their grip as well.\nSome of [the prospects] will be playing for Brooklyn three years from now if by that time Rickey, a past master at turning baseball talent into gold, has not sold them to other major-league teams at a handsome profit. When Dodgertown breaks up, Rickey and his staff will know, or think they know, how far every player can go in the Brooklyn baseball chain and how fast he can get there. Almost all of them will get a chance to make the grade in 1948 with one of the 25 minor-league farm clubs operated by the Dodgers. But a disappointed few will be taken aside and quietly advised to forget the whole thing and get a job in a filling station.\n— Ben Cosgrove is the Editor of LIFE.com\nLiz Ronk, who edited this gallery, is the Photo Editor for LIFE.com. Follow her on Twitter at @LizabethRonk.']	['<urn:uuid:be4189ff-99ad-4fac-b171-6ca53342ea19>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T23:00:04.658849	14	22	905
100	signal processing engineer compare resolution bandwidth effects between persistence spectrum and regular spectrogram analysis methods	While regular spectrograms show frequency content changing over time with resolution bandwidth (RBW) inversely affecting spectrum-time (e.g., 100 kHz RBW needs 22.3ms vs 10 kHz RBW needs 223ms), persistence spectrums uniquely display the percentage of time frequencies persist in a signal. Persistence spectrums can reveal hidden signals through histograms in power-frequency space, showing brighter colors for frequencies that occur more often, while regular spectrograms simply show power levels over time without indicating persistence.	"['This example shows how to perform and interpret basic time-frequency signal analysis. In practical applications, many signals are nonstationary. This means that their frequency-domain representation (their spectrum) changes over time. The example discusses the advantages of using time-frequency techniques over frequency-domain or time-domain representations of a signal. It answers basic questions, such as: When is a particular frequency component present in my signal? How do I increase time or frequency resolution? How can I sharpen the spectrum of a component or extract a particular mode? How do I measure power in a time-frequency representation? How do I visualize the time-frequency information of my signal? How do I find intermittent interference within the frequency content of the signal of interest?\nYou can divide almost any time-varying signal into time intervals short enough that the signal is essentially stationary in each section. Time-frequency analysis is most commonly performed by segmenting a signal into those short periods and estimating the spectrum over sliding windows. The\npspectrum function used with the\n\'spectrogram\' option computes an FFT-based spectral estimate over each sliding window and lets you visualize how the frequency content of the signal changes over time.\nConsider the signaling system of a digital phone dial. The signals produced by such a system are known as dual-tone multi-frequency (DTMF) signals. The sound generated by each dialed number consists of the sum of two sinusoids or tones with frequencies taken from two mutually exclusive groups. Each pair of tones contains one frequency of the low group (697 Hz, 770 Hz, 852 Hz, or 941 Hz) and one frequency of the high group (1209 Hz, 1336 Hz, or 1477Hz) and represents a unique symbol. The following are the frequencies allocated to the buttons of a telephone pad:\nGenerate a DTMF signal and listen to it.\n[tones, Fs] = helperDTMFToneGenerator(); p = audioplayer(tones,Fs,16); play(p)\nListening to the signal, you can tell that a three-digit number was dialed. However, you cannot tell which number it was. Next, visualize the signal in time and in frequency domain over the 650 to 1500 Hz band. Set the\n\'Leakage\' parameter of the\npspectrum function to 1 to use a rectangular window and improve frequency resolution.\nN = numel(tones); t = (0:N-1)/Fs; subplot(2,1,1) plot(1e3*t,tones) xlabel(\'Time (ms)\') ylabel(\'Amplitude\') title(\'DTMF Signal\') subplot(2,1,2) pspectrum(tones,Fs,\'Leakage\',1,\'FrequencyLimits\',[650, 1500])\nThe time-domain plot of the signal confirms the presence of three bursts of energy, corresponding to three pushed buttons. To measure the length of the burst, you can take the pulse width of the RMS envelope.\nenv = envelope(tones,80,\'rms\'); pulsewidth(env,Fs)\nans = 3×1 0.1041 0.1042 0.1047\ntitle(\'Pulse Width of RMS Envelope\')\nHere you can see three pulses, each one approximately 100 milliseconds long. However, you cannot tell which numbers were dialed. A frequency-domain plot helps you figure this out because it shows the frequencies present in the signal.\nLocate the frequency peaks by estimating the mean frequency in four different frequency bands.\nf = [meanfreq(tones,Fs,[700 800]), ... meanfreq(tones,Fs,[800 900]), ... meanfreq(tones,Fs,[900 1000]), ... meanfreq(tones,Fs,[1300 1400])]; round(f)\nans = 1×4 770 852 941 1336\nBy matching the estimated frequencies to the diagram of the telephone pad, you can say that the dialed buttons were \'5\', \'8\', and \'0\'. However, the frequency-domain plot does not provide any type of time information that would allow you to figure out the order in which they were dialed. The combination could be \'580\',\'508\',\'805\',\'850\', \'085\', or \'058\'. To solve this puzzle, use the pspectrum function to compute the spectrogram and observe how the frequency content of the signal varies with time.\nCompute the spectrogram over the 650 to 1500 Hz band and remove content below the 10 dB power level to visualize only the main frequency components. To see the tone durations and their locations in time use 0% overlap.\npspectrum(tones,Fs,\'spectrogram\',\'Leakage\',1,\'OverlapPercent\',0, ... \'MinThreshold\',-10,\'FrequencyLimits\',[650, 1500]);\nThe colors of the spectrogram encode frequency power levels. Yellow colors indicate frequency content with higher power; blue colors indicate frequency content with very low power. A strong yellow horizontal line indicates the existence of a tone at a particular frequency. The plot clearly shows the presence of a 1336 Hz tone in all three dialed digits, telling you that they are all on the second column of the keypad. From the plot you can see that the lowest frequency, 770 Hz, was dialed first. The highest frequency, 941 Hz, was next. The middle frequency, 852 Hz, came last. Hence, the dialed number was 508.\npspectrum function divides a signal into segments. Longer segments provide better frequency resolution; shorter segments provide better time resolution. The segment lengths can be controlled using the\n\'TimeResolution\' parameters. When no frequency resolution or time resolution values are specified,\npspectrum attempts to find a good balance between time and frequency resolutions based on the input signal length.\nConsider the following signal, sampled at 4 kHz, that consists of the trill portion of a Pacific blue whale song:\nload whaleTrill p = audioplayer(whaleTrill,Fs,16); play(p)\nThe trill signal consists of a train of tonal pulses. Look at the time signal and the spectrogram obtained by\npspectrum when no resolution is specified and when time resolution is set to 10 milliseconds. Set the\n\'Leakage\' parameter to 1 to use rectangular windows. Since we want to localize the time position of the pulses, set overlap percent to 0. Finally, use a\n\'MinThreshold\' of 60 dB to remove background noise from the spectrogram view.\nt = (0:length(whaleTrill)-1)/Fs; figure ax1 = subplot(3,1,1); plot(t,whaleTrill) ax2 = subplot(3,1,2); pspectrum(whaleTrill,Fs,\'spectrogram\',\'OverlapPercent\',0, ... \'Leakage\',1,\'MinThreshold\',-60) colorbar(ax2,\'off\') ax3 = subplot(3,1,3); pspectrum(whaleTrill,Fs,\'spectrogram\',\'OverlapPercent\',0, ... \'Leakage\',1,\'MinThreshold\',-60,\'TimeResolution\', 10e-3) colorbar(ax3,\'off\') linkaxes([ax1,ax2,ax3],\'x\')\nThe 47 milliseconds time resolution chosen by\npspectrum is not small enough to localize all the trill pulses in the spectrogram. On the other hand, a time resolution of 10 milliseconds is enough to localize each trill pulse in time. This becomes even clearer if we zoom into a few pulses:\nNow load a signal that consists of an echolocation pulse emitted by a big brown bat (Eptesicus fuscus). The signal is measured with a sampling interval of 7 microseconds. Analyze the spectrogram of the signal.\nload batsignal Fs = 1/DT; figure pspectrum(batsignal,Fs,\'spectrogram\')\nThe spectrogram with default parameter values shows four coarse time-frequency ridges. Reduce the frequency resolution value to 3 kHz to get more details on the frequency variation of each ridge.\nObserve that now the frequency ridges are better localized in frequency. However, since frequency and time resolution are inversely proportional, the time resolution of the spectrogram is considerably smaller. Set an overlap of 99% to smooth out the time windows. Use a\n\'MinThreshold\' of 60 dB to remove unwanted background content.\npspectrum(batsignal,Fs,\'spectrogram\',\'FrequencyResolution\',3e3, ... \'OverlapPercent\',99,\'MinTHreshold\',-60)\nThe new settings yield a spectrogram that clearly shows the four frequency ridges of the echolocation signal.\nEven though we have been able to identify four frequency ridges, we can still see that each ridge is spread over several adjacent frequency bins. This is due to the leakage of the windowing method used in both time and frequency.\npspectrum function is capable of estimating the center of energy for each spectral estimate in both time and frequency. If you reassign the energy of each estimate to the bin closest to the new time and frequency centers, you can correct for some of the leakage of the window. You can do this by using the\n\'Reassign\' parameter. Setting this parameter to\ntrue computes the reassigned spectrogram of the signal.\npspectrum(batsignal,Fs,\'spectrogram\',\'FrequencyResolution\',3e3, ... \'OverlapPercent\',99,\'MinTHreshold\',-60,\'Reassign\',true)\nNow the frequency ridges are much sharper and better localized in time. You can also localize the signal energy using the function\nfsst, which is discussed in the next section.\nConsider the following recording, consisting of a chirp signal whose frequency decreases over time and a final splat sound.\nload splat p = audioplayer(y,Fs,16); play(p) pspectrum(y,Fs,\'spectrogram\')\nLet us reconstruct a portion of the ""splat"" sound by extracting a ridge in the time-frequency plane. We use\nfsst to sharpen the spectrum of a noisy version the splat signal,\ntfridge to identify the ridge of the chirp sound, and\nifsst to reconstruct the chirp. The process denoises the reconstructed signal.\nAdd Gaussian noise to the chirp portion of the ""splat"" sound. The added noise simulates an audio recording taken with an inexpensive microphone. Examine the time-frequency spectral content.\nrng(\'default\') t = (0:length(y)-1)/Fs; yNoise = y + 0.1*randn(size(y)); yChirp = yNoise(t<0.35); pspectrum(yChirp,Fs,\'spectrogram\',\'MinThreshold\',-70)\nSharpen the spectrum using the Fourier synchrosqueezed transform,\nfsst localizes energy in the time-frequency plane by reassigning energy in frequency for a fixed time. Compute and plot the synchrosqueezed transform of the noisy chirp.\nThe chirp appears as a localized ridge in the time-frequency plane. Identify the ridge using\ntfridge. Plot the ridge along with the transform.\n[sst,f] = fsst(yChirp,Fs); [fridge, iridge] = tfridge(sst,f,10); helperPlotRidge(yChirp,Fs,fridge);\nNext, reconstruct the chirp signal using the ridge index vector\niridge. Include one bin on each side of the ridge. Plot the spectrogram of the reconstructed signal.\nyrec = ifsst(sst,kaiser(256,10),iridge,\'NumFrequencyBins\',1); pspectrum(yrec,Fs,\'spectrogram\',\'MinThreshold\',-70)\nReconstructing the ridge has removed noise from the signal. Play the noisy and denoised signals consecutively to hear the difference.\np = audioplayer([yChirp;zeros(size(yChirp));yrec],Fs,16); play(p);\nConsider a complex linear frequency modulated (LFM) pulse, which is a common radar waveform. Compute the spectrogram of the signal using a time resolution of 1.27 microseconds and 90% overlap.\nFs = 1e8; bw = 60e6; t = 0:1/Fs:10e-6; IComp = chirp(t,-bw/2,t(end), bw/2,\'linear\',90)+0.15*randn(size(t)); QComp = chirp(t,-bw/2,t(end), bw/2,\'linear\',0) +0.15*randn(size(t)); IQData = IComp + 1i*QComp; segmentLength = 128; pspectrum(IQData,Fs,\'spectrogram\',\'TimeResolution\',1.27e-6,\'OverlapPercent\',90)\nThe parameters used to compute the spectrogram give a clear time-frequency representation of the LFM signal.\npspectrum computes a power spectrogram, this means that the color values correspond to true power levels in dB. The color bar shows that the power level of the signal is around 4 dB.\nIn certain applications, it may be preferable to visualize the spectrogram of a signal on a logarithmic frequency scale. You can achieve this by changing the\nYScale property of the y-axis. For example, consider a logarithmic chirp sampled at 1 kHz. The frequency of the chirp increases from 10 Hz to 400 Hz in 10 seconds.\nFs = 1e3; t = 0:1/Fs:10; fo = 10; f1 = 400; y = chirp(t,fo,10,f1,\'logarithmic\'); pspectrum(y,Fs,\'spectrogram\',\'FrequencyResolution\',1, ... \'OverlapPercent\',90,\'Leakage\',0.85,\'FrequencyLimits\',[1 Fs/2])\nThe spectrogram of the chirp becomes a straight line when the frequency scale is logarithmic.\nax = gca; ax.YScale = \'log\';\nview command, you can visualize the spectrogram of a signal as a three-dimensional waterfall plot. You can also change the display colors with the\nFs = 10e3; t = 0:1/Fs:2; x1 = vco(sawtooth(2*pi*t,0.5),[0.1 0.4]*Fs,Fs); pspectrum(x1,Fs,\'spectrogram\',\'Leakage\',0.8)\nview(-45,65) colormap bone\nThe persistence spectrum of a signal is a time-frequency view that shows the percentage of the time that a given frequency is present in a signal. The persistence spectrum is a histogram in power-frequency space. The longer a particular frequency persists in a signal as the signal evolves, the higher its time percentage and thus the brighter or ""hotter"" its color in the display. Use the persistence spectrum to identify signals hidden in other signals.\nConsider an interference narrowband signal embedded within a broadband signal. Generate a chirp sampled at 1 kHz for 500 seconds. The frequency of the chirp increases from 180 Hz to 220 Hz during the measurement.\nfs = 1000; t = (0:1/fs:500)\'; x = chirp(t,180,t(end),220) + 0.15*randn(size(t));\nThe signal also contains a 210 Hz interference, with an amplitude of 0.05, that is present only for 1/6 of the total signal duration.\nidx = floor(length(x)/6); x(1:idx) = x(1:idx) + 0.05*cos(2*pi*t(1:idx)*210);\nCompute the power spectrum of the signal over the 100 to 290 Hz interval. The weak sinusoid is obscured by the chirp.\nCompute the persistence spectrum of the signal. Now both signal components are clearly visible.\nfigure colormap parula pspectrum(x,fs,\'persistence\',\'FrequencyLimits\',[100 290],\'TimeResolution\',1)\nIn this example, you learned how to perform time-frequency analysis using the pspectrum function and how to interpret spectrogram data and power levels. You learned how to change time and frequency resolution to improve your understanding of signal and how to sharpen spectra and extract time-frequency ridges using\ntfridge. You learned how to configure the spectrogram plot to get a logarithmic frequency scale and three-dimensional visualization. Finally, you learned how to find interference signals by computing a persistence spectrum.\nThe following helper functions are used in this example.', 'What is a Spectrogram\nA spectrogram is a graphic that shows the viewer how the frequency domain content (spectral content) of a signal or signals is changing or progressing over time. It consists of a collection of spectrum traces that are collected/processed over time and stacked next to each other, much like slices of bread in a loaf. While each individual spectrum trace would be on the “butter side” of each slice, the viewer’s perspective for the spectrogram is looking down at the top of the loaf. The top “crust” of each slice is color coded from end to end to represent the amplitude/magnitude variations across the frequency span of the spectrum trace. With the slices stacked together, and the color-coded amplitude across the span, it is easy to observe the static, dynamic and transient nature of the spectral activity over time (from the first slice through to the last).\nThe spectrogram is drawn as a two-dimensional plot where the horizontal axis represents the frequency span of the spectral content, and the vertical axis represents time. The plot is filled with rows of colored pixels, with each row representing a spectrum “slice”, and the individual pixel color representing the magnitude of the trace at each point across the frequency span of the trace. The vertical time-domain axis can be configured such that the most recent spectral trace (slice) occurs at the top of the display and the “older” slices cascade below it. This is often called a waterfall display since new spectral traces appear at the top and then scroll down towards the bottom as time marches on. When the time axis is reversed, with the newest spectral traces appear at the bottom, this is called a reverse-waterfall configuration.\nBasic Spectrum Processing\nMost all spectrograms are composed of spectrum traces that are based on raw data collected over time and computed using some form of discrete Fourier Transform – often a Fast Fourier Transform (FFT) or Chirp-Z Transform (CZT). Regardless of the transform used, the time-duration of acquired data used for each FFT/CZT is called the “spectrum-time”. The spectrum-time is inversely proportional to the frequency resolution, or Resolution Bandwidth (RBW) of the spectrum traces. Narrower RBW settings lead to longer spectrum times. For example, a spectrum trace with a 100 kHz RBW may require a spectrum-time (acquisition length) of 22.3 mS, while an RBW of 10 kHz would require a spectrum time of 223 mS. It is helpful to understand the concept of spectrum-time when it comes to looking at how spectrograms are built.\nTypes of Spectrograms\nThere are two main types of spectrograms – the discontinuous type, and the continuous type. These two types refer to how the data is acquired, processed and displayed in the spectrogram.\nThe first type of spectrogram is the discontinuous type. The spectrum “slices” in a discontinuous spectrogram are added one at a time, with each “slice” coming from a new acquisition of data. Essentially, a “spectrum-time’s” worth of data is acquired, an FFT is performed to create the spectrum trace, and it is added to the spectrogram. This process is repeated to eventually add more and more slices to the spectrogram. The discontinuous type of spectrogram may best be described as a “strip-chart recording” of the spectral history. There will be time-gaps between each of the spectrum slices in the spectrogram. The duration of the time gap will be a function of many things, including trigger interval, acquisition length, processing time, RBW setting, etc. A discontinuous spectrogram is best suited to observing relatively long-term trends and changes in spectral activity.\nThe second type of spectrogram is the continuous type. This type of spectrogram is built from a single, time-seamless acquisition record rather than from multiple consecutive acquisitions. The continuous spectrogram is only available when the acquisition length exceeds the spectrum length (or spectrum time). Once this condition is met, the spectrogram slices are generated by moving the FFT spectrum-time through the seamless acquisition record, computing spectrums at regular intervals along the way to fill in the slices in the spectrogram. In most cases, this means that consecutive FFT results may be computed from time-domain data that was included in the adjacent FFT results. This is known as “overlap”. As the amount of overlap is increased, the number of individual spectrum slices within a given time period is also increased. In some implementations, the amount of overlap used is computed automatically to fill the available spectrogram area. Other implementations give the user the ability to adjust the overlap in order to achieve a desired spectrum slice-to-slice time interval. How often or how quickly the spectral content changes, as well as the overall acquisition length, will dictate what level of overlap might be most useful. More overlap gives you a shorter time interval between each spectrum slice and increases the number of slices in a given time period. The downside with high levels of overlap is “smearing” in the vertical (time domain axis) of the spectrogram, because the same time-domain samples are used in adjacent spectrum slices.\nSpectrograms are available in a number of different Tektronix hardware and software platforms. Each platform has its own particular characteristics for the spectrograms.\nThe MDO, or Mixed Domain Oscilloscopes, include the 3 Series MDO as well as the older MDO3000 and MDO4000 series instruments. In all of these instruments, the spectrogram offered is the discontinuous type (built one slice per acquisition) using the reverse-waterfall configuration (newest spectrum slices at the bottom). Given the discontinuous spectrogram type, there is no ability to correlate spectral activity in the body of the spectrogram with other time-domain events in the acquisition. This spectrogram essentially serves a long-term spectral activity monitoring function.\nThe 4, 5 and 6 Series MSO instruments include spectrograms as part of the Spectrum View SV-RFVT option (the RF vs. Time option). This spectrogram is the continuous type, using the waterfall configuration (newest slices at the top). The scope acquisition must be longer than the spectrum-time in order for the spectrogram to function. Spectrum View will automatically adjust the FFT overlap so that the spectrogram fills the allocated display area. If the spectrogram display area is adjusted, the overlap will adjust accordingly to fill the new area. Similarly, if the time-domain area is zoomed-in, the spectrogram will automatically adjust the overlap to show the activity in the zoomed area. This implementation ensures that the time-seamless spectrogram is always time correlated to all of the time-domain waveforms and displays, which makes it easy to correlate specific spectral events to other time-domain activity across all channels. Cursor functionality is enhanced, providing the ability to view the spectral “slice” at each of the two cursor locations. Together with the normal spectrum time position, spectral activity from three distinct locations in time can be simultaneously displayed and compared. This functionality is available on every channel.\nThe Tektronix Realtime Spectrum Analyzers (RSAs), as well as the SignalVu-PC application which can work with the USB-RSAs as well as the MDO4000B/C and 5/6 Series MSOs, all feature a spectrogram display as a standard feature. This spectrogram can function as both discontinuous and continuous type. When the acquisition/analysis length is shorter than 2x the spectrum-time, or the span is greater than the realtime bandwidth, the spectrogram operates in discontinuous mode. When the acquisition/analysis length exceeds twice the spectrum-time, then the continuous mode is added. As more acquisitions are made, the continuous spectrograms from the previous acquisitions are stacked up in the spectrogram, much like the discontinuous type. In other words, these products give you a history record of the continuous spectrograms. The FFT overlap for the continuous spectrogram is under the user’s control in this platform. When large amounts of overlap are used, resulting in the number of spectrum slices exceeding the available display area, a scroll bar is added to allow the user to navigate to any desired area in the spectrogram.\nTo download a PDF copy of the information in this blog click the button below.']"	['<urn:uuid:d6d769dd-1ac7-4ee1-a62b-c3c03ef323e5>', '<urn:uuid:f3cce867-7b08-41bb-90b7-ced16f763b2c>']	factoid	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-12T23:00:04.658849	15	73	3344
