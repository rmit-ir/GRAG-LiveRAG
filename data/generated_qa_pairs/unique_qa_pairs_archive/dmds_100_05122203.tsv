qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	stop motion animation history and modern 3d printing techniques compare traditional handmade craft with digital technology advances	Stop-motion animation evolved from early works like The Humpty Dumpty Circus (1898) and progressed through Ray Harryhausen's Dynamation techniques in films like Jason and the Argonauts. The traditional handmade craft involves frame-by-frame animation to create the illusion of motion. Modern productions now combine this traditional artform with advanced technology - particularly 3D printing. This technological integration is exemplified in current practices where 3D printing allows for precise creation of detailed models and props. The process requires different modeling techniques than traditional animation, as all details must be physically modeled into the mesh rather than added through textures. This combination of traditional craft and modern technology can be seen in both animation studios like Laika and in other fields, such as Printable Scenery's creation of detailed miniature props for tabletop games.	"[""The handmade art of frame-by-frame, stop-motion animation is a tedious, time-consuming craft that produces an astonishing illusion of motion. It clearly displays the most basic way we absorb images, from the initial explorations of the technique in 1898's The Humpty Dumpty Circus and the brilliant dino-work of Willis O'Brien's The Lost World from 1925, advancing past the monumental achievements of Ray Harryhausen's Dynamation in films like The Beast From 20,000 Fathoms, Jason and the Argonauts and The Golden Voyage of Sinbad and far into the modern age of holiday TV classics by Rankin/Bass like Rudolph, the Red-Nosed Reindeer and Santa Claus Is Comin' to Town.\nLaika's Kubo and the Two Strings unspools in theaters today with its samurai odyssey and amazing armatured puppets, and only time will tell if it can unseat our crowned champion. Kubo represents the next stage in stop-motion animation in the 21st century and showcases some truly magical advances in 3D printing technology and fluid, computer-aided design.\nWith Kubo's triumphant arrival, now is the perfect time to reflect back on 14 all-time stop-motion favorites and see how they stack up against each other in a competitive Olympic-level event. See if you agree with my positioning and shout out any that might have been missed in this elemental cinematic dance of time and space.\n14) MAD MONSTER PARTY - 1967\nProduced at the height of the Rankin/Bass stop-motion TV spectacles, this feature film was a magnificent monster mash of all the classic horror creatures including Frankenstein's Monster, The Werewolf, Dracula, The Invisible Man, The Hunchback and Dr. Jekyll. What's not to like about a swingin' shreik-fest featuring the voices of Boris Karloff and Phyllis Diller as the Host With The Least! This is a forgotten treasure for anyone who was weaned on the Animagic miracles of Rankin/Bass.\n13) THE PIRATES! BAND OF MISFITS - 2012\nA smart-alecky buccanneer adventure from the producers of Chicken Run and Wallace and Gromit that pokes serious fun at the rousing pirate mythology with our hapless captain hero (Hugh Grant) racing across the seven seas trying to best his smarmy, cutlass-equipped competitors Black Bellamy and Cutlass Liz. Adapted from the first book in The Pirates! series, The Pirates! In an Adventure with Scientists, by Gideon Defoe. The movie is a silly sea saga with some knockout jokes suitabe for the whole family and was Grant's first animated role.\n12) CORALINE - 2009\nBased on the book by fanatsy and horror icon Neil Gaiman, Coraline was the first stop-motion feature to be conceived and photographed in stereoscopic 3D. Written and directed by Henry Selick (The Nightmare Before Christmas, James and the Giant Peach), it was nominated for an Academy Award for Best Animated Feature film and helped catapult little Portland-based Laika into the limelight.\n11) PARANORMAN - 2012\nAnother entry from Laika made our celebrated list, and although this stirring supernatural tale suffers from some poor pacing and editing issues, it has the pure heart the Portland studio was trying to develop and refine, but the whole production falls a bit short in the end. ParaNorman was the first stop-motion film to utilize a 3D Color Printer to create replacement faces for its roster of puppets. Over 40,000 individual face parts were printed for the production. An important entry in the advancement of the craft, done with loving care.\n10) JAMES AND THE GIANT PEACH - 1996\nAnother Roald Dahl classic brought to life by the creative team behind The Nightmare Before Christmas. Beautifully rendered in flawless animated sequences, the film drags a bit toward the end but children always seem to be captivated by the antics of the big bugs and the adventurous British boy, James, in his humongous piece of fruit. There are a small measure of live-action scenes but the majority was all filmed using stop-motion techniques. Nominated for a 1997 Academy Award for Randy Newman's Original Musical Score.\n9) FRANKENWEENIE - 2012\nA full-length feature adaptation of Tim Burton's Frankenweenie short from 1984 while under contact at Disney, telling the touching tale of a boy trying to resurrect his expired dog, Sparky. Contains all of Burton's odd-ball humor, imagination and life lessons learned after other beloved pets go on a monstrous rampage following resuscitation. Gloriously filmed in stark black and white, Frankenweenie was nominated for Best Animated Feature at the 85th Academy Awards and was the first stop-motion film to be released in IMAX 3D.\n8) THE BOXTROLLS - 2014\nA strange and surreal entry from Laika cracks the Top 10, thanks to its irreverant tone and audacity to depict a weird community of cardboard-clad trolls happily raising a kidnapped kid named Eggs beneath the Victorian village of Cheesebridge. .Based on the 550-page children's book, Here Be Monsters!, by Alan Snow, the 3D animated fantasy was nominated for Best Animated Feature at that year's Academy Awards. If you missed this one from Travis Knight's Laika, give it a look to see the advancements the company made since ParaNorman and its wild absurdist sense of humor and tell us if Kubo and the Two Strings tops it.\n7) CORPSE BRIDE - 2005\nI always felt Corpse Bride was a little underappreciated, living in the shadow of its bigger Burton brother, The Nightmare Before Christmas. Tim Burton and Mike Johnson shared directorial duties on this 19th century Russian fairy tale centered around Victor, a young nobleman dragged into the land of the undead right before his wedding. Johnny Depp provides the voice for Victor and Danny Elfman returns with a soothingly sophisticated musical score.\n6) CHICKEN RUN - 2000\nExactly what do the hens and roosters do when the coop is locked tight for the night? This crowd-pleasing Claymation offering by the creators of the Wallace and Gromit shorts answers that very question with a rowdy barnyard soap opera adventure. It's a playful adaptation of The Great Escape paired with a monstrous pot pie-making machne and all the humor and originality fans loved from Aardman Animation. A real gamble for Dreamworks that paid off in a lot more than just a basket of eggs. Of all the films on this list, Chicken Run holds the honor of having the biggest box office tally at $225 million worldwide.\n5) GUMBY: THE MOVIE - 1995\nThe little green glob of clay and his pony pal Pokey are front and center in this deliriously cool Claymation feature. It's hard resist the near-nightmarish quality of this children's release which centers around Gumby searching for his dog Lowbelly, who was kidnapped by the dastardly Blockheads who desire the pearl drops he weeps. Directed by 74-year-old Gumby creator Art Clokey, who also did the vocals for the golden dino, Prickle, Pokey the mini red equine and Gumby's dad, Gumbo. Watch it, dammit!\n4) THE ADVENTURES OF MARK TWAIN - 1985\nBilling itself as the first feature film produced in Claymation, this trippy stop-motion movie takes you from the Mississippi River to the Milky Way and into the heart of Haley's comet on a grand adventure with the irascible Mark Twain. Directed by legendary animator Will Vinton, the man responsible for The California Raisins ad campaign. Will Vinton Studios folded in 2002 after Nike investors took control of the company and renamed it Laika Studios. The animation is wildly inventive and the production has an celebratory, old-fashioned quality missing from today's jaded times.\n3) WALLACE AND GROMIT: CURSE OF THE WERE-RABBIT - 2005\nWallace and Gromit's shorts by Nick Park were already well-established and acclaimed when this feature-length madcap adventure surprised audiences in 2005 as a follow-up to Aardman Animation's highly-successful Chicken Run. This was the kooky Claymation duo's first feature film and it scored at the box office, raking in nearly $200 million worldwide, telling a Halloween-ish tale of a mutant rabbit gobbling up gardens at Tottington Hall. Won the Academy Award for Best Animated Feature of 2005. Cheese, anyone?\n2) FANTASTIC MR. FOX - 2009\nQuirky director Wes Anderson (Rushmore, The Life Aquatic With Steve Zissou) dove into stop-motion to adapt Roald Dahl's (Charlie and The Chocolate Factory) hilarious tale of the resourceful Mr. Fox and his plans to steal tasty grub from a trio of nasty farmers, Boggis, Bunce and Bean. This book was a great favorite of mine as a kid staying home from school reading a stack from my bedroom bookshelf. A killer vocal cast including George Clooney, Meryl Streep, Bill Murray, Jason Schwartzman, Owen Wilson and Willem Dafoe provides some uproarious laughs.\n1) THE NIGHTMARE BEFORE CHRISTMAS - 1993\nI tried so hard to find one stop-motion feature that could take down Tim Burton and Henry Selick's Halloween-time masterpiece from 1993. Nobody will unseat this gold medal champion! It's stood the test of time and represents a striking achievement in the art, a perfect synthesis of scary holiday fun with astounding detail and a casket-ful of bone-tapping tunes by Oingo Boingo's Danny Elfman. At nearly 25 years old, Nightmare still has the ability to captivate and charm, and is considered essential viewing for millions of families at yuletide, just as popular today as it was more than two decades ago."", 'There is something about tabletop war games that has the ability to create a kind of excitement not found in other more modern types of games. Video games provide a realistic feel, while allowing gamers to take part in the action as if they are actually “in the game.” However, the strategy, and the study of human behavior exuded in the play, is something that video games can’t come close to when compared to the more traditional tabletop games.\nEven though tabletop war games may be seen as a traditional type hobby, thousands upon thousands of hobbyists still play these types of games. However, one company, called Printable Scenery, is combining the modern-day technology of 3D printing with that of these traditional games.\n“Due to our interest in table top war-games, we decided to experiment with creating miniature 28mm fantasy scenery,” Mathew Barker of Printable Scenery tells 3DPrint.com. “For the last year we have been experimenting with different designs. Having worked in the television industry for many years I’m used to getting involved in projects where no-one is 100% sure what’s happening or how things will turn out, so the transition to experimental 3d printing has been frightfully seamless.”\nWhile Barker says that the process for modeling for animation and the process for modeling 3D prints are quite different and challenging to adjust to, the main challenge for him is keeping everything to scale.\nThe tabletop war game designs created by Printable Scenery are quite intricate, capturing many details often not seen in traditional game props. The company has designed quite a bit of this game scenery and has made the designs available for purchase and download via their website. They include things such as a war tower, stone walls, castle walls, bridges, fences, and even extremely detailed cottages.\n“All the details have to be modeled into the mesh as we cannot cheat anything with textures or normal maps,” Barker tells us. “This way the model quickly becomes very hi-poly and can be a nightmare to change once the details have been added. Creating brick patterns that don’t tile or repeat can take a while to model – as opposed to in animations, where applying a tile-able texture is much easier. This all becomes more painful when changes are requested, making locking down concept designs an absolute must.”\nAs you can see in the images, Printable Scenery’s designs are not something you would find on a free model repository site. These are carefully created with hours upon hours of work. When it comes to 3D printing, the prints are done on a MakerBot Replicator 3D printer, and then of course post processed and painted.\nPersonally I am not into tabletop war games, but if I were, I know I would certainly be purchasing some of these designs. What do you think about Printable Scenery’s creations? Discuss in the 3D printed war game scenery forum thread on 3DPB.com.\nYou May Also Like\nJCRMRG’s 3D Health Hackathon Aims for Sustainable 3D Printed PPE\nAs we’ve mentioned many, many times over the last few months, the 3D printing community has really stepped up in a big way to help others as our world got...\nObjectify and 3DPrint.com Partner to Launch Advanced Additive Manufacturing Webinar Series\nUnder the Objectify AddMics (derived: Additive Academics) initiative—from India’s largest additive manufacturing bureau—Objectify Technologies joins hand with one of the most followed 3D printing media houses in the world, 3DPrint.com,...\n3D Printing Webinar and Virtual Event Roundup, July 7, 2020\nWe’ve got plenty of 3D printing webinars and virtual events to tell you about for this coming week, starting with nScrypt’s webinar today. 3Ding and Formlabs will each hold a...\nInterview: Redefine Meat CEO’s Insight into New Alternative Meat & 3D-Printed Food\nAmid lifestyle changes toward wellness and health, as well as an inclination of industries to adopt disruptive technologies, the 3D printed plant-based meat industry could go from niche to mainstream...\nView our broad assortment of in house and third party products.']"	['<urn:uuid:bf51ece4-219a-4055-b98e-8c20a0e4e128>', '<urn:uuid:5755f603-6748-412e-afd0-f6936177e2ee>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T22:03:18.033296	17	130	2183
2	developing primer for uv vacuum coating what properties needed prevent coating quality issues during metal application	The primer must have excellent leveling performance to guarantee high gloss and mirror effect of the metal coating. Crucially, the primer curing process should not produce volatile small molecules as these would destroy the quality and performance of the metal coating. Therefore, macromolecular quantization in the primer formula is an important consideration. The primer must also provide good adhesion to both the plastic substrate and vacuum coating metal layer, and demonstrate sufficient system flexibility to prevent coating cracking or jumping off.	['UV metal coating\nUV vacuum electroplating coating is mainly composed of UV curing resin, monomer, photoinitiator, pigment, leveling agent, adhesion promoter, etc. According to the division of application surface, it is mainly composed of UV vacuum electroplating primer and UV vacuum electroplating topcoat. In order to match the modern new electroplating technology, some of them also have vacuum electroplating coating.\nThe effect of primer UV vacuum coating coating primer is coated on the surface of the plastic substrate coating, its role is to close the plastic substrate, to prevent the volatile substances in the substrate during vacuum plating affect the quality of the coating.Plastic surface is rough, through the primer coating can obtain smooth smooth mirror effect, is conducive to obtain a thicker and uniform vacuum metal coating, show higher metallic luster and reflection effect.For some substrates with lower polarity, better adhesion properties can be obtained by vacuum electroplating of metal coating through primer transition.The primer also ACTS as a thermal buffer for some plastic substrates with poor heat resistance.\nPrimer coating primer UV vacuum plating of the above, requires a primer must be on the plastic base material and vacuum coating metal layer, good adhesion, is both good and weak polarity plastic coating substrate adhesion is good, and has good adhesion with polarity metal, the plastic coating is more demanding than simple, also indirectly proves that most of the UV vacuum plating resin are same or higher than applicable to simple UV plastic coatings.\nAt the same time, in order to guarantee the high gloss and mirror effect of the metal coating, the primer must have excellent leveling performance.In addition, the primer curing should not have volatile small molecules, otherwise it will destroy the quality and performance of the metal coating, so the primer formula of macromolecular quantization is an important consideration.For some of the heat resistant base material, UV primer with its thermal performance is also very important, such as heat resistance of BMC requirements up to 180 °.In addition, in order to prevent coating cracking or jumping off, the system flexibility of primer also appears particularly important.\nPrimer of formula design according to the performance requirements of the UV vacuum coating primer, primer formula on choosing oligomer to fully consider the flexibility, adhesion, flowing property and molecular weight, so the need to give attention to two or more things, generally choose two or three faculties of polyurethane acrylic resin as the main body, such as the market popular 6158 and UA202, also considering the cost and performance factors, tend to select certain or polyester epoxy resin using collocation, monomer with TP commonly as the priority, the initiator for the longer wavelengths is first selection, the other the right amount of phosphate and adhesion promoter can be appropriate to add.\nThe function of finish and requirements UV vacuum electroplating finish is coated on the vacuum coating metal coating, it plays a protective role in the coating and a certain decorative role.So the surface paint requires good adhesion to the metal coating film, to have enough performance to resist the external friction, scratch, scratch and other external forces, and provide a certain resistance and resistance.In addition, most of the topcoat or coating is the need to add color modification, the entire coating system of wettability requirements of the color filler is also very high.\nFinish the formula design of UV vacuum plating finish and primer is different, in order to assure the mechanical properties of coatings, tend to choose high some functionality resin to compatibility, and give full consideration to add color, will allow to add high functionality to improve wear resistance of resin, in addition, for pigments added convenience in order to guarantee system, polishing performance good resin is preferred, Such as the import of Changxing chemical 6071 and domestic Qianchen material UA378, Europe and the United States have a good material selection..In terms of additives, HEMA auxiliary properties can be selected in small amounts on the premise of ensuring the performance.']	['<urn:uuid:932bf1b7-90e2-4aec-a178-e34afb2109b5>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T22:03:18.033296	16	81	664
3	I want eco-friendly fruit packaging. What's new in that?	A new innovative packaging solution has been introduced that is plastic-free, tree-free, and renewable. It's made from bagasse, which is a byproduct of sugarcane production, requiring no new materials or cultivation areas. This packaging is compostable, recyclable, and printed with vegetable-based inks. Under the right conditions, it can decompose at home in as little as four weeks. The packaging is currently being used for one-pound units of green kiwifruit under the KeeWee brand.	['Committed to sustainability in the industry, Oppy BB #:116424 has introduced an innovative first: a plastic-free, tree-free, renewable packaging solution.\nAs part of its long-standing commitment to environmental stewardship and innovation-led approach to sustainable development, the leading fresh produce grower, marketer and distributor has partnered with CanePak to create bagasse-based packaging.\nCompostable, recyclable, and printed with vegetable-based inks, the pack debuts with one-pound units of green kiwifruit bearing Oppy’s popular KeeWee brand.\nSince bagasse fibers are a byproduct of sugarcane production, the new pack utilizes an existing agricultural waste stream so it requires no new materials, no additional cultivation areas and has no impact on existing forest areas.\nIt also leverages the growing consumer emphasis on environmental sustainability in packaging and is home compostable in as little as four weeks under the right conditions.\n“Bagasse requires less chemical processing than tree-based pulp which means its environmental impact is reduced even further,” Oppy’s Director of Marketing Services Cathie MacDonald said. “Oppy is dedicated to innovative packaging solutions that place the environment at the forefront of our work instead of as an afterthought, which is aligned with our expect the world from us promise.”\nThe KeeWee brand, which features a playful and bright kiwi character, was introduced in 2016 to meet retailers’ demands for a product that energizes the consumer and fuels repeat sales. The groundbreaking pack is therefore a natural fit for the fun, youth-focused KeeWee character which aims to resonate with consumers and retailers alike.\n“CanePak Paperboard is proud to support Oppy’s commitment to environmental stewardship and innovation by advancing the use of 100% bagasse fiber packaging,” CanePak Paperboards Co-Founder Minto Roy said. “Tree-Free packaging that aligns with the growing consumer demand for eco-friendly packaging that is recyclable, minimizes landfill waste, greenhouse gas emissions and deforestation.”\nOppy’s other sustainability initiatives include partnering with the How2Recycle label program to inspire families to recycle produce packaging, in addition to introducing a Top Seal machine in its Vancouver, BC warehouse for repacking bulk items, reducing necessary plastic by 30% in comparison to traditional packs. Oppy also supports the work of the BC Marine Trails Network Association by donating to their plastic clean-up initiatives.\n“Innovation is at the heart of Oppy’s continued success over the years and this is just one of a series of developments that underscore our deep commitment to sustainable business practices across our value chain,” Oppy’s Senior Manager of Insights & Innovation Garland Perkins said. “At Oppy, innovation is more than a product or a package, it is a framework that guides all of our work on a fundamental level and we expect to launch even more developments on this front in the coming years.”\nGrowing, marketing and distributing fresh produce from around the globe for more than 160 years, Vancouver, BC-based Oppy discovers and delivers the best of the world’s harvest. With over 50 million boxes of fresh fruits and vegetables grown on every continent moving through its supply chain annually, Oppy offers popular favorites from avocados and berries to apples and oranges year-round, alongside innovative seasonal specialties. Over the years, Oppy has introduced North Americans to a number of items across its diverse produce range, including Granny Smith, JAZZ and Envy apples, as well as green and gold kiwifruit. Go to oppy.com to learn more.']	['<urn:uuid:5a88ca47-be1b-4eb2-ad85-2f3ee522ad47>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T22:03:18.033296	9	73	547
4	key differences cell size prokaryotes eukaryotes diameter comparison	Prokaryotes cells are smaller, measuring about 0.1-5 μm in diameter, while eukaryotic cells are larger, measuring around 10-100 μm.	['Prokaryotes room unicellular organisms that lack membrane-bound structures, the many noteworthy of i beg your pardon is the nucleus. Prokaryotes cells often tend to be small, an easy cells, measuring around 0.1-5 μm in diameter.\nWhile prokaryotic cells perform not have membrane-bound structures, they do have distinct cellular regions. In prokaryotic cells, DNA bundles together in a an ar called the nucleoid.\nYou are watching: What are two distinguishing characteristics of prokaryotes\nProkaryotic cell features\nHere is a failure of what you might find in a prokaryotes bacterial cell.Nucleoid: A main region the the cabinet that consists of its DNA. Ribosome: Ribosomes space responsible because that protein synthesis. Cell wall: The cell wall surface provides structure and also protection indigenous the external environment. Most bacteria have a strict cell wall surface made indigenous carbohydrates and also proteins referred to as peptidoglycans. Cell membrane: Every prokaryote has a cabinet membrane, also known as the plasma membrane, the separates the cell from the external environment. Capsule: part bacteria have actually a class of carbohydrates that surrounds the cell wall surface called the capsule. The capsule help the bacterium attach to surfaces. Fimbriae: Fimbriae room thin, hair-like structures that help with to move attachment. Pili: Pili space rod-shaped structures connected in lot of roles, consisting of attachment and also DNA transfer. Flagella: Flagella space thin, tail-like frameworks that help in movement.\nExamples the prokaryotes\nBacteria and also archaea are the two species of prokaryotes.\nDo prokaryotes have actually mitochondria?\nNo, prokaryotes carry out not have mitochondria. Mitochondria room only uncovered in eukaryotic bio cells. This is additionally true of other membrane-bound structures choose the nucleus and the Golgi device (more on this later).One theory for eukaryotic advancement hypothesizes the mitochondria were first prokaryotic cell that lived inside various other cells. Over time, development led to these separate organisms functioning as a single organism in the form of a eukaryote.\nEukaryotes are organisms who cells have a nucleus and also other organelles fastened by a plasma membrane. Organelles are inner structures responsible for a selection of functions, together as power production and protein synthesis.\nThe key structures current in a eukaryotic bio cell.\nEukaryotic cells are large (around 10-100 μm) and complex. While many eukaryotes are multicellular organisms, there space some single-cell eukaryotes.\nEukaryotic cabinet features\nWithin a eukaryotic cell, every membrane-bound framework carries out certain cellular functions. Below is an overview of countless of the primary components of eukaryotic cells.Nucleus: The nucleus stores the hereditary information in dyed form. Nucleolus: found inside of the nucleus, the nucleolus is the part of eukaryotic bio cells where ribosomal RNA is produced.Plasma membrane: The plasma membrane is a phospholipid double layer that surrounding the whole cell and encompasses the oribel within.Cytoskeleton or cabinet wall: The cytoskeleton or cell wall provides structure, permits for cabinet movement, and plays a function in cabinet division. Ribosomes: Ribosomes are responsible for protein synthesis. Mitochondria: Mitochondria, likewise known together the powerhouses of the cell, room responsible for power production.Cytoplasm: The cytoplasm is the an ar of the cell in between the atom envelope and also plasma membrane. Cytosol: Cytosol is a gel-like substance within the cell that consists of the organelles. Endoplasmic reticulum: The absorbent reticulum is one organelle devoted to protein maturation and also transportation. Vesicles and vacuoles: Vesicles and also vacuoles are membrane-bound sacs connected in transportation and storage.\nOther common organelles uncovered in many, however not all, eukaryotes incorporate the Golgi apparatus, chloroplasts and also lysosomes.\nExamples the eukaryotes\nAnimals, plants, fungi, algae and also protozoans room all eukaryotes.\nComparing prokaryotes and also eukaryotes\nAll life top top Earth is composed of either eukaryotic cells or prokaryotes cells. Prokaryotes were the very first form of life. Scientists think that eukaryotes developed from prokaryotes around 2.7 billion years ago.The primary distinction between these two types of biology is that eukaryotic bio cells have actually a membrane-bound nucleus and also prokaryotic cells perform not. The nucleus is wherein eukaryotes save their genetic information. In prokaryotes, DNA is bundled together in the new closed region, yet it is not stored in ~ a membrane-bound nucleus.The nucleus is only one of plenty of membrane-bound oribel in eukaryotes. Prokaryotes, ~ above the other hand, have actually no membrane-bound organelles. Another important difference is the DNA structure. Eukaryotic bio DNA is composed of multiple molecule of double-stranded linear DNA, while the of prokaryotes is double-stranded and circular.\nKey similarities in between prokaryotes and also eukaryotes\nA comparison mirroring the shared and unique functions of prokaryotes and also eukaryotes\nAll cells, whether prokaryotic or eukaryotic, re-publishing these four features:\n2. Plasma membrane\nTranscription and also translation in prokaryotes vs eukaryotes\nIn prokaryotes cells, transcription and also translation space coupled, definition translation starts during mRNA synthesis.In eukaryotic bio cells, transcription and also translation are not coupled. Warrior occurs in the nucleus, producing mRNA. The mRNA then exits the nucleus, and also translation wake up in the cell cytoplasm.\nSee more: Do 4 Strokes Have Power Bands, What Is A Dirt Bike Power Band\nWhat space the an essential differences in between prokaryotes and eukaryotes?Prokaryotes and also eukaryotes vary in several important ways - these differences include structure variation - even if it is a nucleus is current or absent, and also whether the cell has membrane-bound organelles, and also molecular variation, including whether the DNA is in a one or linear form. The distinctions are summarized in the table below.\n|Cell structure||Unicellular||Mostly multicellular; some unicellular|\n|Cell size||Smaller (0.1-5 μm)||Larger (10-100 μm)|\n|Examples||Bacteria, archaea||Animals, plants, fungi, protists|']	['<urn:uuid:8eff4dd3-ad9b-41d4-8c45-7ccd956b0bde>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T22:03:18.033296	8	19	920
5	how trees stop desert growth health impact	Reforestation helps prevent and reverse desertification, particularly in areas with more than 300mm annual rainfall. The United Nations endorses reforestation as an effective method for repairing degraded lands. This is crucial as desertification affects large areas - on the Southern edge of the Sahara alone, an area the size of Somalia became desert over 50 years. Additionally, forests provide essential health benefits through ecosystem services like air filtration, helping address issues like household air pollution, which caused 2.9 million deaths in developing countries in 2015.	"['How many school children can be endangered by the schools themselves? The answer was over 600,000 in metropolitan Lima alone.\nIn the region, fraught with frequent seismic activity, nearly two-thirds of schools were highly vulnerable to damage by earthquakes. Working with the Peruvian Ministry of Education (MINEDU), the World Bank and the Global Facility for Disaster Reduction and Recovery (GFDRR) conducted a risk assessment that ultimately helped make an estimated 2.5 million children safer and paved the way for a $3.1 billion national risk-reduction strategy.\nWhether it is building safer schools or deploying early warning systems, disaster risk management is an integral part of caring for our most vulnerable, combating poverty, and protecting development gains.\nOver the last 30 years, the world has lost an estimated $3.8 trillion to natural disasters. , and to undo decades of development progress overnight.\nOver the past decade, commitments and support for Forest Landscape Restoration have grown significantly. As part of the Bonn Challenge, for instance, some 40 countries, sub-national jurisdictions, and non-governmental entities have now pledged to restore forest landscapes across 148 million hectares. Although the environmental benefits in terms of ecosystem services, soil restoration, water, biodiversity and climate resilience are evident, the tremendous economic arguments and the value proposition for poor people living in, or nearby, the forests, are not always at the forefront of the efforts to restore landscapes.\nIn fact, some 1.3 billion people around the world depend on forests for their livelihood—that is 20% of the global population. This includes income from the sale of trees and tree-related products. It also includes the value of fruit, fodder, medicines, and other direct or indirect products that they consume. However, the restoration of forest landscape at a global scale needs a new vision for an integrated forest economy which appreciates and understands forests along their entire value chain. Thus it is crucial to see forest landscape restoration efforts as much more than just protecting forests, but as a force for economic growth and poverty reduction.\nSocial safety nets – predictable cash grants to poor households often in exchange for children going to school or going for regular health check-ups – have become one of the most effective poverty reduction strategies, helping the poor and vulnerable cope with crises and shocks. Each year, safety net programs in developing countries lift an estimated 69 million people living in absolute poverty and uplifting some 97 million people from the bottom 20 percent – a substantial contribution in the global fight against poverty.\nGlobally 2.9 million people died from household air pollution in 2015, caused by cooking over foul, smoky fires from solid fuels such as wood, charcoal, coal, animal dung, and agricultural crop residues. Well over 99% of these deaths were in developing countries, making household air pollution one of their leading health risk factors.\nMany women across the world spend their days and evenings cooking with these fuels. They know the fumes are sickening, which is why some cook in a separate outhouse or send the children to play while they cook. Sadly, these small actions cannot fully protect the young. As for the women themselves, they suffer incredible morbidity and mortality from household air pollution.\nBy the early 2000s, Peru faced serious environmental problems. Air pollution in urban areas was so severe that it caused thousands of premature deaths every year. In fact, air quality in Lima was worse than in other large Latin American cities, such as Mexico City or Sao Paulo. Other environmental challenges that damaged people’s health included air pollution inside homes caused by the use of wood for cooking; insufficient access to clean water, sanitation, and hygiene; and exposure to lead, a highly toxic chemical. Together, these environmental problems caused 12 million cases of illnesses annually, dramatically affecting young children, the elderly, and poor people who couldn’t afford medical care. The World Bank estimated that these negative impacts had an economic cost equivalent to 2.8% of Peru’s Gross Domestic Product (GDP) in 2003.\nOne of the main reasons the Peruvian government wasn’t able to respond promptly to these serious environmental problems was the country didn’t have governmental organizations with a clear responsibility for environmental protection. Another important reason was the absence of a system of reliable environmental information to support the government’s decision-making process. For example, there was little awareness about the seriousness of air pollution, largely because most cities didn’t have a functional air quality monitoring network. Even in the few cities that did, the information was not widely disseminated. In the absence of such information, it was difficult to identify which environmental problems were most severe, and to develop actions and assign resources to solve them. In addition, lack of information limited the opportunities for the public—including the poor families and other vulnerable groups that suffered the most from pollution —to discuss their environmental concerns and agree on solutions with government officials.\nIt is widely acknowledged that reducing emissions from deforestation could bring about one-third of the greenhouse gas emission reductions we need by 2030 to stay on a 2-degrees trajectory. But protecting and managing forests wisely does not only make sense from a climate perspective. It is also smart for the economy. Forests are key economic resources in tropical countries. Protecting them would increase resilience to climate change, reduce poverty and help preserve invaluable biodiversity.\nHere are just a few facts to illustrate why forests are so important. First, forests provide us with ecosystem services like pollination of food crops, water and air filtration, and protection against floods and erosion. Forests are also home for about 1.3 billion people worldwide who depend on forest resources for their livelihood. Locally, forests contribute to the rainfall needed to sustain food production over time. When forests are destroyed, humanity is robbed of these benefits.\nThe New Climate Economy report shows us that economic growth and cutting carbon emissions can be mutually reinforcing. We need more innovation and we need more investments in a low carbon direction. This requires some fundamental choices of public policy, and the transformation will not be easy. However, it is possible and indeed the only path to sustained growth and development. If land uses are productive and energy systems are efficient, they will both drive strong economic growth and reduce carbon intensity.\nAlready, the world\'s large tropical forest countries are taking action.\nÉ amplamente reconhecido que a redução de emissões provenientes do desmatamento poderia corresponder a um terço da diminuição das emissões de gases de efeito estufa necessária até 2030 para o planeta não aquecer mais de 2ºC. No entanto, proteger e gerenciar as florestas de forma prudente não somente faz sentido de uma perspectiva do clima. É também algo inteligente para a economia. As florestas são recursos econômicos de suma importância nos países tropicais. Protegê-las aumentará a resiliência às mudanças do clima, reduzirá a pobreza e ajudará a preservar a biodiversidade.\nSeguem apenas alguns fatos para ilustrar por que as florestas são tão importantes. Primeiro, as florestas nos prestam serviços de ecossistema, tais como polinização de safras de alimentos, água e filtração do ar, bem como proteção contra inundações e erosão. As florestas também abrigam cerca de 1,3 bilhão de pessoas no mundo inteiro que dependem dos recursos florestais para subsistência. Em nível local, as florestas contribuem para a pluviosidade necessária para manter a produção de alimentos no correr do tempo. Quando as florestas são destruídas, esses benefícios são roubados da humanidade.\nO novo Relatório sobre a Economia Climática nos mostra que o crescimento econômico e a redução das emissões de carbono podem se reforçar mutuamente. Precisamos de mais inovação e mais investimentos para ter uma economia de baixo carbono. Isso requer certas escolhas fundamentais de política pública e a transformação não será fácil. No entanto, é possível e na realidade trata-se do único caminho para um crescimento e desenvolvimento sustentados. Se o uso da terra for produtivo e os sistemas energéticos forem eficientes, ambos impulsionarão um desenvolvimento econômico sólido e reduzirão a intensidade das emissões carbono.\nEm âmbito mundial, os países com as grandes florestas tropicais já estão agindo.\nWhen the water is poor, people get sick: they have diarrhea; their growth is stunted; they die. When the air is poor, people get sick: they cough; they cannot leave their beds; they die. However, they do not look sick when there is lead in their blood. You cannot look at a child who has an unhealthy blood lead level (BLL) and say, ""This is not right. Something must be done,"" because in most cases, there is nothing to see.\nLead (Pb) exposure—which is making headlines in the U.S. because of recent events in Flint, Michigan-- is a major source of critical environmental health risks. But the problem is subtle: Affected children do not perform as well in school. They are late to read. They are slow to learn how to do tasks. Perhaps a few more children are born with cognitive deficits. Perhaps these children have less impulse control. Perhaps they exhibit more violence.\nThese symptoms are not always understood as an environmental or a public health problem – or indeed a development problem. Instead, people will say it is an issue of morals or of education. They will discipline the children, and then they will take themselves to task and ask how and why they are failing to raise these children correctly. Furthermore, they will have no idea that the problem is in the children’s blood.\nYoung children are particularly vulnerable to lead exposure. Studies have documented that exposure leads to neuropsychological impacts in children--including impaired intelligence, measured as intelligence IQ losses--at blood lead levels even lower than 5 micrograms of lead per deciliter of blood (µg/dL). So, clearly, the effect occurs at even very low BLLs.\nNamed by Peruvian fishermen because of its tendency to appear around Christmastime, El Niño is the planet’s most large-scale and recurring mode of climate variability. Every 2-7 years, a slackening of trade winds that push sun-warmed water across the Pacific contributes to a rise in water temperature across large parts of the ocean. As the heat rises, a global pattern of weather changes ensues, triggering heat waves in many tropical regions and extreme drought or rainfall in others.\nThe fact that we are undergoing a major El Niño event should cause major concern and requires mobilization now. Already, eight provinces in the Philippines are in a state of emergency due to drought; rice farmers in Vietnam and Thailand have left fields unplanted due to weak rains; and 42,000 people have been displaced by floods in Somalia.\nAnd this is before the event reaches its peak. Meteorologists see a 95% chance of the El Niño lasting into 2016, with its most extreme effects arriving between now and March. Coastal regions of Latin America are braced for major floods; India is dealing with a 14% deficit in the recent monsoon rains; and poor rainfalls could add to insecurity in several of Africa’s fragile states. Indeed, Berkeley Professor Soloman Hsiang has used historical data to demonstrate that the likelihood of new conflict outbreaks in tropical regions doubles from 3% to 6% in an El Niño year.\nBut despite its thousand-year history, the devastation associated with El Niño is not inevitable. Progress made by many other countries since the last major event, in 1997-98, shows that we can get a grip on its effect – and others caused by climate trends.\n- weather risks\n- Disaster Repsonse\n- disaster relief\n- disaster recovery\n- disaster prevention\n- disaster preparedness\n- Disaster management\n- Sustainable Communities\n- Public Sector and Governance\n- Agriculture and Rural Development\n- Climate Change\n- The World Region', 'Erosion and Flooding\nDeforestation contributes to erosion by exposing soils to wind and rain.\nWhen the ground surface is stripped of vegetation, the upper soils are vulnerable to both wind and water erosion. Soil is washed into rivers when it rains, and then out to sea. This destroys the ability for the land to regenerate because it has lost its topsoil. It also destroys marine environments. In several parts of the world, entire sections of countries have been rendered unproductive because of soil loss.\nOne of the world’s most serious erosion problems is in China. From the Yellow River, over 1.6 billion tons of sediment flows into the ocean each year. The sediment comes mainly from water erosion in the Loess Plateau in the northwest of the country.\nErosion from deforestation is a problem in many other countries. In the United States there have been significantly increased incidents of flooding over the past fifty years. Forests have a natural ability to absorb water when it rains, and to release that water slowly into rivers. Following deforestation in a rainfall catchment area, the water moves more quickly from the land to the rivers, causing erosion and stripping the topsoil. Because the rivers fill more quickly they are much more prone to flashfloods. Floods that break the banks of the rivers then exacerbate the problem by changing the path of the river and causing additional severe erosion where the water now flows.\nWhen we observe floods on television and see dirty brown water surging through a city or out to sea, it is more likely than not that severe erosion has occurred and that the flood was the result of deforestation.\nBy reforesting bare land, Wild Again restores natural water catchments and decreases the risks of floods, mudslides, and other forms of erosion.\nDesertification means means land degradation in arid, semi-arid and dry sub-humid areas resulting from various factors, including deforestation.\nReforestation also helps to prevent desertification and can reverse desertification trends.\nIt is a common misconception that droughts cause desertification. In most cases the causes are social and economic, resulting from human intervention in the form of deforestation and over-exploitation of the land. Typically this has involved overgrazing and felling of trees and brushwood for fuel. Increased population and livestock pressure on marginal lands has accelerated desertification. Droughts are common in arid and semiarid lands. Well-managed lands can recover from drought when the rains return. Continued land abuse during droughts, however, increases land degradation.\nOn the Southern edge of the Sahara, an area the size of Somalia has become desert over the past 50 years. The same fate threatens more than one third of Africa. Virtually all of the inhabited areas of Africa are prone to soil and environmental degradation in one form or another. Most regions of the continent suffer from several forms of environmental degradation, leading to desertification and with its detrimental impact on food and agricultural productivity and production. The process is often not obvious because it is usually gradual and unnoticeable.\nThe United Nations believes that reforestation is an effective method for repairing degraded lands and reversing the trend of desertification. In areas with more than 300 mm of rainfall per year for example, dry reforestation can be effective when the plantation site is wisely chosen and stream water collection techniques are applied. Below 300 mm per year, extra watering is required according to the particular features of the intervention area.']"	['<urn:uuid:7ae62fbb-587e-49ed-85be-7bdd082fa923>', '<urn:uuid:841e21e8-d563-4fcf-80c2-e43d93d48a6b>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T22:03:18.033296	7	85	2518
6	How do researchers collect data and protect participants' privacy?	Researchers collect data through in-depth interviews, observations, journals, art, and poetry, asking participants about their experiences and the contexts that influenced them. To protect privacy, researchers implement safety measures from the study's start to publication, collecting only minimal personal data necessary for proper population sampling. They must obtain informed consent, train staff in confidentiality procedures, and clearly outline how the information will be used.	"['Phenomenological Research is a kind of qualitative research.\nIt’s a qualitative research.\n- It describes the meaning for several individuals of their lived experiences of a concept or phenomenon.\n- Phenomenologists focus on describing what all participants have in common as they experience a phenomenon.\n- The purpose of phenomenology is to reduce individual experiences with a phenomenon to a description of a universal essence (grasp of a very nature of the thing) (van Manen, 1990).\n- This human experience may be phenomena such as insomnia, being left out, anger, grief, etc.\n- The researcher then collects data from people who have experienced the phenomena, and develops a composite description of the essence of the experience for all of the individuals. This description consists of ‘what’ they experienced and ‘how’ they experienced it (Moustakas, 1994).\n- Researchers first turn to a phenomenon that seriously interests them. They reflect on essential themes, what constitutes the nature of the lived experience. They write a description of the phenomenon, maintaining a strong relation to the topic of inquiry and balancing the parts of the writing to the whole. Then the researcher tries to interpret the lived experience or phenomenon.\n- Moustakas’s (1994) transcendental or psychological phenomenology is focused less on the interpretations of the researcher and more on a description of the experiences of the participants. Transcendental means: everything is perceived freshly as if for the first time.\n- The type of problem should be one in which it is important to understand several individuals’ common or shared experiences of a phenomenon.\n- These lived experiences and phenomena are conscious and directed toward an object.\n- In a psychological or transcendental phenomenology, the researcher must bracket out his own experiences.\n- Data are collected from the individuals who have experienced the phenomenon.\n- Data collection is usually done through in-depth interview or multiple interview with the participants.\n- Observations, journals, art, poetry may also be used to collect data.\n- The participants are asked two broad and general questions: (1) what have you experienced in terms of the phenomenon? And (2) What contexts and situations have typically influenced or affected your experiences of the phenomenon?\n- Building on the data from the first and second research questions, data analysts go through the data and highlight ‘significant statement’ sentences or quotes that provide an understanding of how the participants experienced the phenomenon. Moustakas (1994) calls this stage “horizonalization’. Next the researcher develops clusters of meaning from these significant statements into themes.\n- The significant statements or themes are then used to write a description (not interpretation) of what the participants experienced (textural descriptions). They are also used to write a description of the context or setting that influenced how the participants experienced the phenomenon (imaginative variation or structural description).\n- From the structural or textural descriptions, the researcher then writes a composite description that presents the ‘essence’ of the phenomenon called ‘the essential or invariant structure or essence). This passage focuses on the common experiences of the participants. It is a descriptive passage, a long paragraph or two and the reader should come away from the phenomenology with the feeling.', ""Science depends on research participants to volunteer information regarding individual beliefs and actions on a host of topics. A participant or subject is more likely to provide honest responses when their identity is not going to be exposed.\nConsiderations for confidentiality should be given to the following:\nNature of the response.\nPossible punishment for the response.\nPossible punishment for participation.\nPerceived punishment for the participation.\nFear of embarrassment\nParticipants answer differently if they think that privacy may be compromised\nThe research may focus on topics that are taboo in society and it would be difficult to illicit honest responses to some of the questions posed when a participant did not feel secure in knowing that their identity is protected.\nThere also maybe some personal liability involved depending on the topic of the study. If the study involved cheating on testing at the university they attended and a guarantee of anonymity could not be provided, the likelihood of honest answers being given would be very low, for fear of retaliation.\nBias in employment although illegal is very real, if confidentiality is not upheld and if the wrong information is passed around regarding a participant it could very well effect the ability of the participant to maintain employment and be employed in the future.\nIn addition there are other negative responses that a participant may face if the research is on an unpopular topic, or a controversial issue. If the group involved in the research all works for the same organization and the study is to determine the employer's unethical practices, participants if not kept confidential could suffer great consequences for their responses.\nIn some cultures a subject's participation alone could result in public punishment, being excommunicated from the community or even death.\nThere may only be a perception of punishment for participation if confidentiality is not guaranteed. The perception alone will keep potential subjects from participating in the research, and will also prevent honest answers.\nResearch is never meant to punish anyone, it's only goal should be greater knowledge and understanding working toward a positive goal.\nPrivacy matters should be addressed from the inception of the research to the publication of the results. There should be safety nets put in place to guarantee confidentiality. The only amount of personal data that should be collected for the research is the minimal amount needed to insure a proper sampling of the population. Personal identifiable information should not be collected nor maintained unless absolutely necessary. Research staff should be properly trained in procedure to maintain confidentiality.\nInformed consent is required in all studies and research using human participants. The consent to participate should clearly outline the purpose of the study and what the information gathered will be used for.\nWhen is Confidentiality not an Issue\nConfidentiality is not an issue when observing large groups, where individual responses or actions are not considered or when participants' identifiable information is not involved. Even in cases where there are large groups being assessed it is up to the researcher to use good judgment in making decisions regarding what information should be shared.\nA Legal Breach\nThere is a sticky area when it comes to confidentiality. If a participant in a research event or a study falls into any of the below categories than the researcher is legally compelled to report the participant.\nRisk of bodily harm - if there is a potential that a violent act will occur against someone else or the participant at his/her own hand.\nCases of child abuse - sexual misconduct with a child, or there is imminent danger to a child that has been disclosed.\nTerrorism - there is a terrorist act or plan in place that the participant has disclosed.\nThere is a eminent threat to public health.\nThere is some ethical concern by being required to report someone in any of the categories above. Let us look at these few examples to review the ethical quandary.\nA study is being conducted looking at the effectiveness of a drug on depression. The participants are asked to record feelings of violence, suicide, anger and despair, both before they began taking the medication and after three weeks of being on the medication.\nEach category is assigned a numerical value to display a range of feelings from very likely to least likely. The study is dependent on honest answers, the participants have been promised confidentiality.\nIf 30 of the 100 participants report that they feel more suicidal since taking the medication, is the researcher required to report all thirty of the participants?\nAIDS patients have been asked to participate in a study to gauge sexual habits since being diagnosed. 60% of the participants report that they have adjusted their sexual habits to include safe sex practices or abstinence, but 40% report that they still sometimes do not practice safe sex.\nShould they be reported?\nThey are a definite threat to public health, but the study depends on honest answers, once that confidentiality has been breached it certainly will sour future participants from being honest. How can we ever gauge the effectiveness of programs and education within certain communities if participants have to fear potential criminal charges?\nShould Science Be Required to Act As a Step in the Penal Process?\nA Real Life Story\nA woman in Florida is expecting baby number 3. She appears at a public health department for prenatal care and agrees to participate in a study for drug use and pregnancy.\nShe confesses that she has an addiction to cocaine (a known teratogen). She also answers yes to one of the questions asking if she would accept rehabilitative services if they were available. The information is passed on to the local authorities, her existing children are turned over to the state, and her in uteri child is taken upon birth and placed in foster care while the mother is forced into treatment so she can get her children back.\nThe case goes to court; the mother sues the state of Florida and its public health system for breach of confidentiality. She loses, based on the required reporting of suspected child abuse. The case has gone to the Supreme Court.\nThere are a few ethical issues that are evident in this brief breakdown of this story.\nMom disclosed willingly the information, she evidently had not acted in a manner to bring attention to herself, she disclosed the information believing it would be kept confidential.\nThe case was brought to court and the argument was that the mother was abusing her unborn child by snorting cocaine while pregnant hence the defense in reporting based on the abuse that was purportedly to her unborn child. Can we mandate good health practices by a pregnant women or risk being charged with child abuse and neglect?\nShould willing disclosure be punished especially when it is a part of research?\nThere are many instances of ethical concerns when it comes to confidentiality and privacy matters in research. As a result of allowing a participants information to be revealed there can be insurmountable damages, it should only be done when absolutely necessary.""]"	['<urn:uuid:4f53c048-3cf7-495b-939b-1e48889ce485>', '<urn:uuid:bf7258af-61a9-4416-8c17-f5e6de333fd8>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T22:03:18.033296	9	64	1710
7	what laws and environmental effects regulate fracturing wastewater disposal radioactive materials	The law prohibits treating, discharging, disposing, or storing wastewater and other byproducts from hydraulic fracturing in the state due to radioactive materials found in high concentrations in the wastewater. These materials pose risks because wastewater treatment plants are not designed to treat radioactivity, which can form deposits in equipment and contaminate biosolids. Additionally, studies show that fracking brings radioactive materials like Radium-226 to the surface, which has a 1,600-year half-life and can contaminate nearby lands for thousands of years.	"['Senator ROBERT M. GORDON\nDistrict 38 (Bergen and Passaic)\nSenator CHRISTOPHER ""KIP"" BATEMAN\nDistrict 16 (Hunterdon, Mercer, Middlesex and Somerset)\nAssemblyman REED GUSCIORA\nDistrict 15 (Hunterdon and Mercer)\nAssemblywoman VALERIE VAINIERI HUTTLE\nDistrict 37 (Bergen)\nAssemblywoman MARLENE CARIDE\nDistrict 36 (Bergen and Passaic)\nAssemblyman CARMELO G. GARCIA\nDistrict 33 (Hudson)\nAssemblywoman BONNIE WATSON COLEMAN\nDistrict 15 (Hunterdon and Mercer)\nSenators Beck, Cunningham, Greenstein, Lesniak, Pou, Rice, Singer, B.Smith, Turner, P.Barnes, III, Weinberg, Holzapfel, Ruiz, Sacco, Scutari, Vitale, Codey, Assemblyman Diegnan, Assemblywoman Jasey, Assemblymen Benson, Johnson, Caputo, McKeon, O\'Donnell, Assemblywoman Sumter, Assemblymen Singleton, Chivukula, Wimberly, Mainor, Assemblywoman Stender, Assemblymen Giblin, Cryan, Assemblywomen Oliver, Spencer, Assemblymen Eustace, Wilson, Conaway, Assemblywoman Mosquera, Assemblyman Lagana, Assemblywomen Jimenez and Riley\nProhibits treatment, discharge, disposal, application to roadway, or storage of wastewater, wastewater solids, sludge, drill cuttings or other byproducts from natural gas exploration or production using hydraulic fracturing.\nCURRENT VERSION OF TEXT\nAs reported by the Senate Environment and Energy Committee on March 17, 2014, with amendments.\nAn Act concerning wastewater, wastewater solids, sludge, drill cuttings and other byproducts from certain drilling techniques and supplementing P.L.1977, c.74 (C.58:10A-1 et seq.).\nBe It Enacted by the Senate and General Assembly of the State of New Jersey:\n1. The Legislature finds and declares that the practice of the drilling technique of hydraulic fracturing for natural gas exploration and production has been found to use a variety of contaminating chemicals and materials; that the drilling technique uses vast quantities of water mixed with chemicals and solids pumped into shale formations at high pressure to fracture the shale formations; that millions of gallons of contaminated water flow back out of the well; and that the companies engaging in the use of this drilling technique have been less than forthcoming in revealing the “cocktail” of chemicals and their concentrations and volume.\nThe Legislature further finds and declares that the treatment of wastewater, wastewater solids, sludge, drill cuttings or other byproducts from the hydraulic fracturing process poses financial, operational, health, and environmental risks to the citizens of the State; that the high concentrations of solids present in hydraulic fracturing wastewater and other wastes may include calcium, magnesium, phosphates, nitrates, sulphates, chloride, barium, cadmium, strontium, dissolved organics such as benzene and toluene, and copper, which would interfere with the processes of wastewater treatment plants by inhibiting the anaerobic digestion processes and disrupting the biological digestion processes; that the heavy metals present may precipitate during the treatment process and contaminate biosolids from the plant, which would require expensive decontamination of drying beds or prevent the usual methods for reuse or disposal of those biosolids; and that no federal or State standards have been adopted governing the treatment and disposal of hydraulic fracturing wastes.\nThe Legislature further finds and declares that in addition to the chemicals and solids used in the well drilling process, the United States Environmental Protection Agency has noted that radioactive materials have been found in ""fairly high concentrations"" in hydraulic fracturing wastewater and hydraulic fracturing wastes; that wastewater treatment plants are not designed to treat for radioactivity; that not only does wastewater contaminated with radionuclides pose a risk to public drinking water supplies if not properly treated, but those radioactive materials may also form deposits over time in equipment and pipes which would cause a health risk to plant workers; and that radioactive materials, as well as heavy metals and other toxic materials, present in hydraulic fracturing wastewater pose a risk of contamination of the treatment plant\'s biosolids.\nThe Legislature further finds and declares that in addition to the dangers and uncertainties for wastewater treatment plants, other wastes from the hydraulic fracturing process such as drilling mud, drill cuttings, sludge and concentrated byproducts pose a danger to the environment and the health and safety of the citizens of the State; that the chemical content of wastes from hydraulic fracturing varies based on location of the well and the chemicals injected; that the regulatory requirements for ultimate treatment and disposal of such waste are not clear with regard to whether it may be disposed of at a landfill or must be treated at a wastewater treatment facility; that such waste has been exempted from many federal hazardous waste laws even though it contains hazardous materials; 1that the high salt content of the waste poses a risk that it may be applied as a road treatment to prevent icing and that such use would therefore pose a danger of a release of hazardous materials into the environment;1 that the waste has been sent to landfills and hazardous waste processing or recycling facilities even though some of the waste contains concentrated radioactive liquids, hydrocarbons and toxic pollutants; and that landfills and treatment facilities in the State are not designed to accept or treat radioactive materials.\nThe Legislature therefore determines, in light of the State\'s small size, population density, and heavy reliance on surface waters for drinking water purposes, that it is prudent and in the best interest of the health, safety and welfare of the people of the State of New Jersey to prohibit the treatment, discharge, disposal, or storage of wastewater, wastewater solids, sludge, drill cuttings or other byproducts resulting from hydraulic fracturing.\n2. As used in this act, “hydraulic fracturing” means the drilling technique of expanding existing fractures or creating new fractures in rock by injecting water, often with chemicals, sand, or other substances, and often under pressure, into or underneath the surface of the rock for purposes including, but not necessarily limited to, well drilling and natural gas exploration and production. The term “hydraulic fracturing” shall include “fracking,” “hydrofracking,” “hydrofracturing,” and other colloquial terms for this drilling technique.\n3. No wastewater, wastewater solids, sludge, drill cuttings or other byproducts resulting from hydraulic fracturing for the purpose of natural gas exploration or production in any state may be treated, discharged, disposed of, 1applied to a roadway or otherwise released into the environment,1 or stored in the State.\n4. This act shall take effect immediately.', 'Hydraulic fracturing of the Marcellus shale gas presents the opportunity for not only an energy source, but also for serious, negative environmental impacts. From air pollution to land degradation, these impacts must be looked at and considered to better understand what fracking is. Environmental impacts are important to the fracking situation at hand because when any unnatural extraction of a natural material occurs, it will inherently impact the environment. When the process is as intense as hydraulic fracturing, the potential of a serious threat increases, and the integrity of the environment must be looked after and protected.\nAdditionally, the environmental impacts of fracking are important to the controversy of fracking because if industries wish to explore the benefits of fracking and use this process for several decades, all negative aspects must be considered as well. The environmental impacts fracking could have would potentially be catastrophic, and understanding of these must exist to make informed decisions. While many argue that the extraction of natural gas will give the United States energy independence and economic benefits, the uncertainty of these impacts and potential loss of environmental services are far too important to ignore.\nThe controversy of fracking, including the environmental issues associated with it, is part of a larger energy issue that affects all communities and people. Understanding how fracking will impact the environment, and therefore all aspects of life in these natural areas, will allow people to form opinions on fracking. These opinions can be part of a greater decision on which direction our energy use and industries should head – in a sustainable or unsustainable direction.\nThere are a great many environmental concerns raised by the process of hydraulic fracturing including the creation or air, noise, and visual pollution, the potential for chemical spills, radioactivity associated with fracking processes, impacts on and disturbance of natural lands, and impacts on biodiversity. Studies of these impacts are relatively limited, due to the newness of this type of intense hydraulic fracturing, and there is some level of uncertainty in these issues. The impacts that have been investigated, however, do show a strong correlation with the presence of fracking in the area.\nAir pollution created by fracking activities is the greatest threat to the environment. Air pollution leads to decreased air quality for all life in the area and can even contribute to climate change. The most threatening component of air pollution that fracking creates is methane, a major component of greenhouse gas pollution (Marcellus Shale Advisory Commission, 2011). Methane is released into the atmosphere as a fugitive emission, or an emission of a gas from pressurized equipment due to leaks or irregular releases of gases. This can occur through leaks in processing equipment and pneumatic devices. A pneumatic device is one that is operated by air or by the pressure or exhaustion of air.\nMethane presents a serious problem to the environment because as methane is vented into the atmosphere, global warming is amplified, which contributes to an already concerning problem. While some may argue that burning natural gas for energy is a cleaner solution to burning other materials, the amount of methane gas leaking during production negates the cleaner-burning advantages and can be 20% more harmful to the environment than coal (Lovejoy, 2012). In a 100-year period, methane is 21 times more harmful to the atmosphere than carbon dioxide. Recent studies have shown a strong correlation between natural gas industry- produced methane and atmospheric methane levels; as shale gas production has increased tenfold since 2005, the atmospheric methane levels have also been rising since 2006 (Lavelle, 2012).\nTo address this problem, the Environmental Protection Agency has established a new regulation that states all gas companies must begin capturing air emissions, including methane, by 2015 (Eilperin and Mufson, 2012). In the meantime, companies will be required to flare their emissions. While the EPA believes this is a wasteful action, as methane could have other useful applications, it will eliminate 90% of volatile organic compounds and break down the methane into water and carbon dioxide.\nOther air pollutants created by fracking that are of concern include nitrogen oxides; volatile organic compounds (VOCs); benzene, toluene, ethyl benzene, and xylenes (BTEX); carbon monoxide; sulfur dioxide; particulate matter (PM); ground level ozone, created by the combination of VOCs and nitrous oxides; and hydrogen sulfide (League of Women Voters, 2009). Many of these compounds are created during transportation and production of shale gas, but can occur during any phase of production, and can be highly toxic and harmful. Overall, air emissions contribute to 2.4% of the overall environmental impact of fracking (Elcock, 2007).\nNoise pollution, while not the greatest pollution threat, does create problems in the environment. The noise created by the all phases, from construction to deconstruction, can last between 800 and 2,500 days (Broomfield, 2012). This can disturb the natural habitats of many animals by deterring them from being in these areas. Many animals may use these sites as their everyday habitats, areas to migrate through, and even areas to find mates in.\nVisual impacts poses many of the same threats and are especially damaging during the four-week construction period (Broomfield, 2012). The visual impact can be in the form of light pollution or the visible presence of fracking machinery. Light pollution occurs in these areas because fracking activities persist though all hours of the night. Since there is no relief from the strong lights at the well pads, animals that are nocturnal are faced with disruptions in their daily life activities. Even animals that are not nocturnal can have disturbed sleeping patterns from these intense lights. Additionally, the presence of these large machines can affect migratory patterns and scare away organisms from their niche. A niche is the area that a specific organism does especially well in and is specialized to live in.\nAll of these impacts were documented in a study on the environmental impacts of hydraulic fracturing in Europe (Broomfield, 2012). Air pollution presented high cumulative risks during four of the six different stages of fracking and overall was presented as a high risk (Table 1). Noise and visual impacts varied across different stages of fracking, and cumulatively noise impacts were a high risk and visual impacts were a moderate risk (Table 1).\nChemical spills are of great concern to the public and the environment due to the great, unknown risks associated with these events. When chemical spills occur, the chemicals can leach into the soils and persist there for an extended period of time. This can lead to the death of plants and even agricultural crops in the area. Once the chemicals are incorporated into the topsoil, it is extremely difficult to remove them. In Bradford County, Pa., on July 4, 2012, 4,700 gallons of hydrochloric acid spilled at a well pad operated by Chief Oil and Gas. (Detrow, 2012). The Department of Environmental Protection conducted an investigation and blamed valve failure for the accident. While 4,000 gallons were successfully captured in a containment pond, 700 gallons still moved past the well pad site and made their way into the soil as well as a nearby creek. In this creek, a small fish kill occurred shortly after the contaminants reached the waterway, making it obvious the potential harm these chemicals could cause in the environment if released.\nWhile radioactive materials occur naturally within soils in the ground, the type and amount of radiation that fracking of the Marcellus shale is contributing is of great concern. Fracking brings to the surface radioactive materials that could contaminate nearby lands for thousands of years (Grassroots Environmental Education, 2012). Radium-226, a radioactive material, is the main radioactive element found in the Marcellus shale, and it has a half-life of 1,600 years. This means that in 1,600 years, only half of the material that exists will have decayed and no longer be harmful. When these materials enter the soil, they can cause the death of plants in the area as well as agricultural lands and crops.\nThe disturbance of natural lands is another environmental impact highly correlated to fracking activities. This disturbance can come in the form of forest fragmentation, the creation of roads and increase in traffic, and even edge effect.\nForest fragmentation occurs when forested areas are split by the presence of machines and vehicles, changing contiguous forests into smaller, more isolated areas. Fracking often creates forest fragmentation, which can lead to the loss of soil integrity, the death of native species of plants and animals, the creation of the edge effect, and disturbance to all organisms in the area. Some estimates have said that Pennsylvania could lose between 38,000 and 90,000 acres of forest cover by 2030 from fracking (Marcellus Shale Advisory Commission, 2011), which would severely fragment the forests of these areas. On average, companies clear between three and four acres of land per well pad.\nThere are both direct and indirect impacts that affect the land around a well pad. Direct impacts include the direct clearing of land for the pad site, as well as land cleared for infrastructures such as the roads (Johnson, 2011). This can account for approximately 8.8 acres of land lost (Table 2). Indirect impacts include land lost adjacent to the site, which can extend about 300 feet, and this is due to forest fragmentation, the creation of new forest edges, and changes in habitat conditions. These total to about 21.2 acres lost, giving a total of about 30 acres of land being lost to one site (Johnson, N., 2011) (Table 2).\nState parks face a particularly unique challenge with fracking. In Pennsylvania, the state does not own rights to the minerals that underlie nearly 80% of all parks (Lavelle, 2010). The private owners can sell the rights to the minerals, which means that fracking companies can easily buy these rights and exploit the lands. Many fear what would happen in such a situation, and there is reason to have these fears. In 2008, the U.S. Forest Service conducted a study in the Fernow Experimental Forest in the Monongahela Nation Forest in West Virginia after fracking fluids from a nearby well pad were sprayed on trees, killing them (Kusnetz, 2011). This application of fracking fluids was intentional and a method of disposal called land-application. Immediately after application, there was an apparent impact, as massive foliage losses occurred and trees began to shed their barks (Figure 1). Initially, the project killed approximately 1,000 trees and caused damage to the ground cover as well. A year later, in 2009, the number of trees affected had increased, and dead foliage was still prevalent (Figure 2).\nMuch of the land disturbance is caused by the presence of trucks and traffic that normally does not exist in these areas. Since the companies cut into the forests, fragmenting the area, the roads are not always well constructed or planned. Some sites will see trucks moving between 7,000 and 11,000 times for a single ten-well pad and on average 250 truck trips are made daily to an individual site (Broomfield, 2012), all of which create significant road damage. In addition to this, these massive trucks and heavy equipment can lead to soil compaction. Soil compaction can occur as topsoil compaction, which is caused by tire pressure and can severely reduce plant productivity in the short term, and subsoil compaction, which is caused by axel loads and reduces productivity for decades (League of Women Voters, 2009). Cumulatively, the impact is decreased soil percolation and increased runoff of water, leading ultimately to less growth of vegetation and more soil erosion.\nLoss of biodiversity is a great threat to Pennsylvania, because out of the 329 species in the Marcellus drilling regions, 132 species, or 40%, are globally rare and critically endangered in Pennsylvania. These species include the green salamander and the snow trillium. In addition to these rare species, Pennsylvania may see a decrease in populations of the Northern Flying Squirrel, the Northern Goshawk, the Scarlet Tanager, and the Black-Throated Blue Warbler. Scarlet Tanager populations could decrease by 23% and the Black-Throated Blue Warbler could see decreases between 10% and 40% (Johnson, 2011).\nDecreases in these populations would be seen mainly due to the increase in edge effect. The edge effect occurs when an area is fragmented, creating more edges in an area. Some edges can increase as much as 1,500 feet, leading to a significant change in the landscape. This can push out species native to these areas and make way for new, invasive species. Many predators thrive along edges and forage for resources and other animals, creating an uninhabitable area for many species (Blankenship, 2011). Additionally, construction can create ditches, making it harder for amphibians to move around, and can create changes in canopy cover, increasing the amount of light in a forest and changing growth rates of plants (League of Women Voters, 2009).\nThe impacts of hydraulic fracturing on the environment raise concern as to whether this is a process that should be continued in the future. With the economic gains and the claims by some that natural gas is a cleaner alternative to other fuels, it is easy to support fracking activities. However, when the whole picture is considered, fracking tells a much different story, one that could destroy our air, our lands, and the life that exists there.\nAs citizens, I believe it is most important to focus on the most threatening aspect of environmental impacts: air pollution. The best way to address this problem is to first talk to local agencies and industries and find out who is polluting, who is not within regulations, and what is being done about it. If the proper actions are not being taken to control pollution by fracking industries, then actions should be taken and citizens can make recommendations for these actions. Some recommendations could include: using lower toxicity of fracking chemicals to minimize the quantities of the chemicals needed; burn the emitted gases to capture any other air pollutants that may be escaping during production; use low bleed pneumatic devices to reduce methane emissions up to 90%; and use electric motors instead of combustion engines. Citizen involvement and action is the most efficient way to begin to protect our environment and reduce the harmful effects of fracking.']"	['<urn:uuid:7ddf5aa7-6b26-4441-96c4-2e2da5e3b19f>', '<urn:uuid:fb777320-40c2-4d3c-a68a-5ef26515e33f>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T22:03:18.033296	11	79	3365
8	How do soccer goalies check field conditions?	During warm-up, goalkeepers should test how the ball rolls across the ground and bounces, particularly between inside and outside the 6-yard box. They should also practice with match balls to understand how different brands feel and move in the air, and adjust their play style according to conditions like wet surfaces, turf, grass, mud, wind, sun or ice.	['As a goalkeeper, you need to prepare both your mind and body before a game. You have to be confident that you are in the best possible shape to play.\nThe goalkeeper’s warm-up should be different from that of an outfield player and should ideally consist of four elements.\nThe first and most important thing is the goalkeeper is in a confident and in a relaxed state. This can be obtained through positive reinforcement and putting him or her in situations where they will achieve success.\nThe body has to be ready to perform the movements required of a high-level goalkeeper (footwork, movements, contact with the ground and contact with other players).\nIt is important to go through a variety of technical aspects of goalkeeping that will be used in a game.\nGame related situations\nThis has to do with either a scouting report on how your team or the other team play, the type of surface you are playing on, dealing with conditions and the balls that you are playing with.\nThe Psychological & Game related situations can be incorporated into the Physical and Technical. Below I will address each.\nPhysical – Preparing the body for optimum performance\nA significant amount of research has shown that performing a proper dynamic warm-up before an activity is a much better route to optimizing performance than simply static stretching. A proper dynamic warm-up has many physiological benefits:\n- Increases core body temperature\n- Increase blood flow to the muscles\n- Stimulates the nervous system and activates muscle groups\n- Improves joint mobility and flexibility\n- Increases coordination and balance\n- Reinforces proper movement patterns\n- Decreases risk of injury (due to all of the above mentioned)\nThere are four parts to this specific category.\n- Pre-activation. First, we want to get your stabilizer muscles firing. This includes activating the smaller core muscles surrounding your torso. This is because we all have muscle imbalances and by going straight into exercises, you automatically work your stronger muscles thus creating more of an imbalance.\nExamples of preactivation suitable for goalkeeper warm up.\n- Balance drills\n2. Mobility / Flexibility exercises. These are exercises geared towards improving range of motion and raising the heartbeat\nExamples of mobility /flexibility exercises suitable for goalkeeper warm up.\n- High kicks\n- Open / Close the gate\n- Arm circles\nSee this video of Courtois doing mobility and flexibility exercises as he enters the field.\n3. Moving stretches. This can be done with a ball and coupled with handling.\nExamples of moving stretches include\n- Backward reaches\n4. Build up & Load acceptance Now it is time to ramp it up if your planned activity involves power, speed, or agility drills to prepare for the game. This final step is preparing your nervous and muscular systems for higher speed, more complex athletic movements. This can certainly be incorporated into the Technical part or your warm up.\nSee this video of Courtois doing these type of exercises as the final part of his physical warm up.\nThe Psychological aspects needed will be reinforced through completing clean saves, or finishing on good saves, having the GK coach or other GK reinforce good technique. Service would have been hard, but not impossible and sufficient rest in between technical exercises.\nThe goalkeeper should also be conscious that much of the aim of shooting practice is to give strikers confidence. The goalkeeper should maybe go in for a few shots to get a feel for the speed in which shots may come in and improve reactions, but should certainly not get down if shots go in. The backup goalkeeper should field the remainder of these shots.\nGame related situations\nDuring the technical warm-up, you would have dealt with some of the natural conditions. Wet surfaces and skidding balls, turf, grass, mud, wind, sun or ice.\nIt is important that you request one or two match balls to practice with ( If the team you are playing against has them) to get a feel. In my experience, different brands of ball feel bigger or move differently in the air.\nDuring the warm-up, you should have got an understanding of how the area and the goalmouth play before you kick off. Make sure between you and the server you really understand how the ball rolls when hit across the ground, how does it bounce? This can often vary drastically between in and outside of the 6-yard box so really do test it out and adjust your play style accordingly if needed.\nIf a scouting report shows that the opposition prefers to do something, in particular, some additional time may be spent on that in the warm-up. We must realize, however, that the game warm-up should not be confused for a practice session.']	['<urn:uuid:17c7876f-e8eb-4dde-9906-418ead7fc4fb>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T22:03:18.033296	7	58	795
9	iterative development collaborative difference between agile concurrent methodology	Agile and concurrent engineering methodologies both embrace iterative development but differ in their collaborative approaches. Agile development emphasizes small teams of 5-7 people delivering incremental functionality through continuous collaboration with customers, focusing on useful customer outcomes. Concurrent engineering, on the other hand, uses larger cross-functional teams including manufacturing, hardware and software design, and marketing working simultaneously, with individual engineers having more say in the overall design process. Both methods replace traditional sequential development, but agile focuses on software delivery while concurrent engineering emphasizes integrated product development across all lifecycle aspects.	"['Agile growth is a sort of software development approach which includes the collaboration of end-users and cross-functional groups to ship enterprise options and merchandise in a versatile and nimble means. The usage of an agile growth methodology offers organizations with early supply and steady enchancment to assist fast and versatile adjustments to the necessities. If you wish to perceive what’s the agile methodology, then a great place to begin is with the Agile Manifesto and the related Twelve Rules. These have been first printed in 2001 and proceed to outline the method. They embody a full set of rules and values for agile growth and supply the perfect agile methodology definition obtainable at the moment.\nResponding to alter\nIn essence, such a growth method offers the potential to create and reply to alter. It’s a means of coping with, and in the end succeeding in, an unsure and turbulent surroundings. That kind of surroundings has all the time existed however is much more prevalent in at the moment’s enterprise world. Agile growth approaches present a mind-set and never inflexible constructions. This has prevented agile growth from changing into inflexible and frozen in time. Adopters and the broader neighborhood have all the time supported, inspired, and embraced the event of recent concepts and ideas.\nAgile Improvement: Totally different Methodologies\nSome folks suppose that there’s just one agile growth method, the scrum methodology. Whereas this is among the varieties of agile methodology and was the primary widespread incarnation in software program growth and, for some, remains to be the perfect agile growth methodology, at the moment, there are a lot of extra varieties obtainable. Frequent themes that exist in the entire agile methodologies for growth embody:\n- Small quantities of performance are delivered incrementally in frequent iterations.\n- Small growth groups, usually 5-7 folks.\n- Continuous collaboration between groups and the purchasers.\n- Give attention to helpful buyer outcomes.\nAgile growth encourages the creation of self-sufficient, self-determining groups, empowered to develop options, methods of working, and backbone to points themselves. Every workforce member ought to have a number of ability units, with the power to work confidently as a person or with different workforce members, relying on the necessities on the time.\nEmbracing the ideas\nThe most effective methods to be taught agile methodology ideas is to grow to be a part of an agile growth workforce. The educational will even require an open thoughts that’s receptive to being guided by concepts and ideas, as an alternative of inflexible frameworks and processes. That is greatest nurtured by being a part of a workforce that has already embraced the mandatory adjustments to angle, habits, and tradition. Agile methodology coaching will also be helpful however generally is a wasted funding except the ideas are quickly utilized to real-life conditions.\nReworking how developments are examined\nOne instance of how agile growth has continued to evolve is within the utility of the ideas to how software program is examined. Most waterfall methodologies outlined the exams within the design stage, making certain that each requirement had an related take a look at. The testing was then carried out by a separate workforce, because it was believed that this unbiased view would remove any bias in assessing take a look at outcomes.\nAgile growth, nevertheless, has a totally contrasting view. An agile testing methodology acknowledges that testing is just not a separate part however is an integral a part of software program growth. In truth, it ought to be performed by the identical individuals who did the coding. All members of the workforce ‘personal’ the code and are collaboratively chargeable for its success, offering a “whole-team” method to making sure high quality within the delivered merchandise. Testing and coding are performed incrementally and interactively, build up every function till it offers sufficient worth to launch to manufacturing.\nA very good testing methodology will embody a number of varieties of exams, each purposeful and non-functional. The scope of those exams ought to embody:\n- Code testing\n- Module testing\n- System testing\n- Integration testing\n- Finish-to-end transaction testing\n- Failure mode testing\n- Safety testing\n- Scalability testing\n- Load testing\n- Usability testing\n- Regression testing\nTesting is seen as a key side of supply and one thing that must not ever be bypassed simply to hit timescales. Exams ought to be complete and canopy each testing new performance and making certain that beforehand current performance continues to work as anticipated. A very good agile growth methodology will even use automated exams, with the variety of ‘use circumstances’ being examined rising over time, capturing and re-playing how the product is utilized by prospects. Instruments can be found that may present appropriate take a look at harnesses. The surroundings used for testing have to be as live-like as potential.\nOne helpful idea is ‘canary’ testing. That is the place a change is deployed to a small a part of the infrastructure and may be very carefully monitored after launch. If any points are discovered then, it may be shortly remoted from the remainder of the customers. The phrase comes from using canaries in cages in mines to confirm the existence of toxic gases.\nAgile Improvement: Conclusion\nSince its inception, agile has remodeled how software program growth is finished. Right now there are only a few software program growth groups that haven’t taken up some if not the entire ideas. Agile growth has grow to be the norm, relatively than the exception that it was within the early days. Because of new fascinated about the right way to apply the values and rules, software program is now developed and launched in timescales measured in minutes as an alternative of months. Organizations can react shortly to adjustments, meet new calls for, and reply to competitors. Software program growth is now seen as an enabler to enterprise agility, not the hindrance that it was beforehand. Agile growth has come of age however remains to be as thrilling and vibrant because it was when it was first envisaged. In case you are one of many few that hasn’t already embraced it, then now could be the time to make the change.\nConclusion: So above is the Agile Development article. Hopefully with this article you can help you in life, always follow and read our good articles on the website: Megusta.info', 'This article needs additional citations for verification. (December 2007) (Learn how and when to remove this template message)\nConcurrent engineering (CE) is a work methodology emphasizing the parallelisation of tasks (i.e. performing tasks concurrently), which is sometimes called simultaneous engineering or integrated product development (IPD) using an integrated product team approach. It refers to an approach used in product development in which functions of design engineering, manufacturing engineering, and other functions are integrated to reduce the time required to bring a new product to market.\nA 2008 publication described concurrent engineering as a new design management system that has matured in recent years to become a well-defined systems approach to optimizing design and engineering cycles. Concurrent engineering has been implemented in a number of companies, organizations, and universities, most notably in the aerospace industry. Beginning in the early 1990s, CE was also adapted for use in the information and content automation field, providing a basis for organization and management of projects outside the physical product development sector for which it was originally designed. Organizations such as the European Space Agency\'s Concurrent Design Facility make use of concurrent design to perform feasibility studies for future missions.\nThe basic premise for concurrent engineering revolves around two concepts. The first is the idea that all elements of a product\'s life-cycle—from functionality, production, assembly, testing, maintenance, environmental impact, and finally disposal and recycling—should be taken into careful consideration in the early design phases.\nThe second concept is that design activities should all be occurring at the same time, i.e., concurrently. The idea is that the concurrent nature of these activities significantly increases productivity and product quality. This way, errors and redesigns can be discovered early in the design process when the project is still flexible. By locating and fixing these issues early, the design team can avoid what often become costly errors as the project moves to more complicated computational models and eventually into the actual manufacturing of hardware.\nAs mentioned above, part of the design process is to ensure that the product\'s entire life cycle is taken into consideration. This includes establishing user requirements, propagating early conceptual designs, running computational models, creating physical prototypes, and eventually manufacturing the product. Included in this process is taking into full account funding, workforce capability, and time requirements. A 2006 study claimed that a correct implementation of the concurrent design process can save a significant amount of money, and that organizations have been moving to concurrent design for this reason. It is also highly compatible with systems thinking and green engineering.\nConcurrent engineering replaces the more traditional sequential design flow, or ""Waterfall Model"". In Concurrent Engineering an iterative or integrated development method is used instead. The Waterfall method moves in a linear fashion, starting with user requirements and sequentially moving forward to design and implementation, until you have a finished product. In this design system, a design team would not quickly look backward or forward from the step it is on to fix or anticipate problems. In the case that something does go wrong, the design usually must be scrapped or heavily altered. The concurrent or iterative design process encourages prompt changes of tack, so that all aspects of the life cycle of the product are taken into account, allowing for a more evolutionary approach to design. The difference between the two design processes can be seen graphically in Figure 1.\nA significant part of the concurrent design method is that the individual engineer is given much more say in the overall design process due to the collaborative nature of concurrent engineering. Giving the designer ownership is claimed to improve the productivity of the employee and quality of the product, based on the assumption that people who are given a sense of gratification and ownership over their work tend to work harder and design a more robust product, as opposed to an employee that is assigned a task with little say in the general process.\nChallenges associated with concurrent design\nConcurrent design comes with a series of challenges, such as implementation of early design reviews, dependency on efficient communication between engineers and teams, software compatibility, and opening up the design process. This design process usually requires that computer models (computer aided design, finite element analysis) are exchanged efficiently, something that can be difficult in practice. If such issues are not addressed properly, concurrent design may not work effectively. It is important to note that although the nature of some project activities project imposes a degree of linearity—completion of software code, prototype development and testing, for example—organizing and managing project teams to facilitate concurrent design can still yield significant benefits that come from the improved sharing of information.\nService providers exist that specialize in this field, not only training people how to perform concurrent design effectively, but also providing the tools to enhance the communication between the team members.\nCross-functional teams include people from different area of the workplace that are all involved in a particular process, including manufacturing, hardware and software design, marketing, and so forth.\nConcurrent product realization\nDoing several things at once, such as designing various subsystems simultaneously, is critical to reducing design time and is at the heart of concurrent engineering.\nIncremental information sharing\nIncremental information sharing helps minimize the chance that concurrent product realization will lead to surprises. ""Incremental"" meaning that as soon as new information becomes available, it is shared and integrated into the design. Cross-functional teams are important to the effective sharing of information in a timely fashion.\nIntegrated project management\nIntegrated project management ensures that someone is responsible for the entire project, and that responsibility is not abdicated once one aspect of the work is done.\nSeveral definitions of concurrent engineering are in use.\n|“||Concurrent Engineering (CE) is a systematic approach to integrated product development that emphasizes the response to customer expectations. It embodies team values of co-operation, trust and sharing in such a manner that decision making is by consensus, involving all perspectives in parallel, from the beginning of the product life cycle.||”|\nThe second one is by Winner, et al., 1988:\n|“||Concurrent Engineering is a systematic approach to the integrated, concurrent design of products and their related processes, including, manufacturing and support. This approach is intended to cause the developers from the very outset to consider all elements of the product life cycle, from conception to disposal, including quality, cost, schedule, and user requirements.||”|\nCurrently, several companies, agencies and universities use CE. Among them can be mentioned:\n- European Space Agency Concurrent Design Facility\n- NASA Team X - Jet Propulsion Laboratory\n- NASA Integrated Design Center (IDC), Mission Design Lab (MDL), and Instrument Design Lab (IDL) - Goddard Space Flight Center\n- CNES – French Space Agency\n- ASI – Italian Space Agency\n- EADS Astrium – Satellite Design Office\n- Thales Alenia Space\n- The Aerospace Corporation Concept Design Center\n- STV Incorporated\n- German Aerospace Center Deutsches Zentrum für Luft- und Raumfahrt\n- EPFL Space Center\n- Concurrent design and manufacturing\n- ESA\'s Concurrent Design Facility\n- Open Concurrent Design Server\n- Product Life Cycle\n- Product lifecycle management\n- ""The Principles of Integrated Product Development"". NPD Solutions. DRM Associates. 2016. Retrieved 7 May 2017.\n- Ma, Y., Chen, G. & Thimm, G.; ""Paradigm Shift: Unified and Associative Feature-based Concurrent Engineering and Collaborative Engineering"", Journal of Intelligent Manufacturing, DOI 10.1007/s10845-008-0128-y\n- Kusiak, Andrew; Concurrent Engineering: Automation, Tools and Techniques\n- Quan, W. & Jianmin, H., A Study on Collaborative Mechanism for Product Design in Distributed Concurrent Engineering IEEE 2006. DOI: 10.1109/CAIDCD.2006.329445\n- Kusiak, Andrew, Concurrent Engineering: Automation, Tools and Techniques\n- ""The standard waterfall model for systems development"", NASA Webpage, November 14, 2008\n- Kock, N. and Nosek, J., ""Expanding the Boundaries of E-Collaboration"", IEEE Transactions on Professional Communication, Vol 48 No 1, March 2005.\n- Ma, Y., Chen, G., Thimm, G., ""Paradigm Shift: Unified and Associative Feature-based Concurrent Engineering and Collaborative Engineering"", Journal of Intelligent Manufacturing, DOI 10.1007/s10845-008-0128-y\n- Royce, Winston, ""Managing the Development of Large Software Systems"", Proceedings of IEEE WESCON 26 (August 1970): 1-9.\n- Kusiak, Andrew, ""Concurrent Engineering: Automation, Tools and Techniques""\n- Rosenblatt, A. and Watson, G. (1991). ""Concurrent Engineering"", IEEE Spectrum, July, pp 22-37.\n- Winner, Robert I., Pennell, James P., Bertrand, Harold E., and Slusarczuk, Marko M. G. (1991). ""The Role of Concurrent Engineering in Weapons System Acquisition"", Institute for Defense Analyses Report R-338, December 1988, p v.']"	['<urn:uuid:db8ba33b-bb7c-4bbd-a7d6-9da439243273>', '<urn:uuid:748e3629-ef89-491b-ad8c-499dc6b552a8>']	open-ended	direct	long-search-query	distant-from-document	comparison	expert	2025-05-12T22:03:18.033296	8	90	2458
10	Doing art studies, wondering what determines a color's visual properties?	Color has three distinct properties: hue (spectral color name), value (lightness or darkness), and saturation (brightness or dullness).	['Color has three distinct properties:\nHue – spectral color name\nValue – lightness or darkness\nSaturation – brightness or dullness\nAlthough you see color in our brain, it is the eye that has the receptors that tell your brain what you are looking at. There are two sets of receptors in the retina in the back of the eye: rods and cones.\nThere are about 125 million rods (named for their shape). They are very sensitive to light but are mostly color blind. We use them in dim light and so the saying: “all cats are gray in the dark.”\nThe color detectors in the eye are the cones. There are about 7 million of these in three forms concentrated in the center of vision. Individual cones can only sense one of three narrowly defined frequencies of light: red, green and blue. The response from these three “primary” colors is sorted in our brain to give us the perception of color. One or more of these color receptors malfunctions in a color blind person.\nColor is a property of light. Our eyes see only a small part of the electromagnetic spectrum. Visible light is made up of the wavelengths of light between infrared and ultraviolet radiation (between 400 and 700 nanometers). These frequencies, taken together, make up white (sun) light.\nWhite light can be divided into it’s component parts by passing it through a prism. The light is separated by wavelength and a spectrum is formed. Sir Isaac Newton was the first to discover this phenomenon in the seventeenth century and he named the colors of the spectrum.\nIf the ends of the spectrum are bent around and joined a color circle (color wheel) is formed with purple at the meeting place.\nColor has three distinct properties: hue, value and saturation. To understand color you must understand how these three properties relate to each other.\nThe traditional color name of a specific wavelength of light is a hue. Another description is: spectral color. All of the colors of the spectrum are hues. There are only limited hue names: red, orange, yellow, green, blue and violet. Magenta and cyan are also hues.\nValue is concerned with the light and dark properties of color. All colors exhibit these properties. The hues have a natural value where they look the purest. Some colors, like yellow, are naturally light. Some, like violet, are darker.\nAll hues can be made in all values. Adding white paint will make any pigment lighter. Adding black paint will make most pigments darker, but will cause yellow paint to shift in hue to green.\nValue can exist without hue (see achromatic). Black, white and gray are values without color.\nSaturation is concerned with the intensity, or the brightness and dullness of color. A saturated color is high in intensity — it is bright. A color that is dull is unsaturated or low in intensity. Another term for saturation is chroma. A color without any brightness (no hue) is achromatic (black, white and/or gray.\nThere are two theories that explain how colors work and interact. The light, or additive theory deals with radiated and filtered light. The pigment, or subtractive theory deals with how white light is absorbed and reflected off of colored surfaces. This course will investigate the pigment theory.\nLight theory starts with black — the absence of light. When all of the frequencies of visible light are radiated together the result is white (sun) light. The color interaction is diagramed using a color wheel with red, green and blue as primary colors. Primary here means starting colors. These are the three colors that the cones in the eye sense. This is an RGB color system (Red, Green and Blue).\nThe primary colors mix to make secondary colors: red and green make yellow, red and blue make magenta and green and blue make cyan. All three together add up tomake white light. That is why the theory is called additive.\nYou can see an example of light theory in action almost every day on a computer monitor or a colored television. The same three primary colors are used and mixed by the eye to produce the range of colors you see on the screen.\nPigments behave almost the opposite of light. With pigments a black surface absorbs most of the light, making it look black. A white surface reflects most of the (white) light making it look white. A colored pigment, green for instance, absorbs most of the frequencies of light that are not green, reflecting only the green light frequency. Because all colors other than the pigment colors are absorbed, this is also called the subtractive color theory.\nIf most of the green light (and only the green light) is reflected the green will be bright. If only a little is reflected along with some of the other colors the green will be dull. A light color results from lots of white light and only a little color reflected. A dark color is the result of very little light and color reflected.\nThe primary colors in the pigment theory have varied throughout the centuries but now cyan, magenta and yellow are increasingly being used. These are the primary colors of ink, along with black, that are used in the printing industry. This is a CMYK color system (Cyan, Magenta, Yellow and (K)black). These are the secondary colors of the light theory.']	['<urn:uuid:00770ee0-d111-4865-ae9a-e792bf3d8ef6>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T22:03:18.033296	10	18	904
11	What mathematical principles apply to levers, and what environmental benefits do green roofs offer?	Levers follow mathematical principles where two magnitudes, whether commensurable or incommensurable, balance at distances reciprocally proportional to their magnitudes - this is demonstrated through Archimedes' propositions using unit weights and proportional distances. Green roofs provide environmental benefits including better insulation, reduced power consumption, stormwater retention, improved air quality, and create a biodiverse environment with aesthetic diversity, particularly valuable in urban areas where vegetation is scarce.	"[""Archimedes' Law of the Lever\nArchimedes' Law of the Lever, i.e. the statement about balancing a beam with different weights distributed along its length, is a classical example of a problem drawn from and applied in, the physical world but which is most illuminated when treated in abstract mathematical terms. The great Archimedes (c. 287-212 BC) was the first to give such treatment in one of his surviving works. Following in footsteps of Euclid, Archimedes sets up a few axioms which are simple abstractions of the everyday experience and from which he proceeds to derive step-by-step the less obvious properties. He is concerned with the situation where the beam is supported at a point (known as a fulcrum) from which the distances to the weights are measured. The center of gravity of several weights placed on the beam is exactly the fulcrum for which the beam is horizontal, i.e. is in equilibrium.\nTo give you the flavor of the work, [Archimedes, p. 502] starts thus:\nI postulate the following:\n- Equal weights at equal distances are in equilibrium, and equal weights at unequal distances are not in equilibrium but incline towards the weight which is at the greater distance.\n- If, when weights at certain distances are in equilibrium, something is added to one of the weights, they are not in equilibrium but incline towards that weight to which the addition was made.\n- Similarly, if anything is taken away from one of the weights, they are not in equilibrium but incline towards the weight from which nothing was taken.\n- When equal and similar plane figures coincide if applied to one another, there centers of gravity similarly coincide.\nWeights that balance at equal distances are equal.\nFor, if they are not equal, take away from the greater the difference between the two. The remainder then will not balance [Post. 3]; which is absurd [Post. 1]. Therefore the weights cannot be unequal.\nUnequal weights at equal distances will not balance but will incline towards the greater weight.\nFor take away from the greater the difference between the two. The equal remainders will therefore balance [Post. 1] Hence, if we add the difference again, the weights will not balance but incline towards the greater [Post. 2].\nUnequal weights will balance at unequal distances, the greater weight being at the lesser distance.\nLet A, B be two unequal weights (of which A is the greater) balancing about C at distances AB, BC respectively.\nThen AC be less than CB. For, if not, take away from A the weight\nHence, AC < CB. Conversely, if the weights balance, and\nWhat we are concerned with here are the Law of the Lever, as expressed in Propositions 6 and 7:\nPropositions 6, 7\nTwo magnitudes, whether commensurable [Prop. 6] or incommensurable [Prop. 7], balance at distances reciprocally proportional to the magnitudes.\nThe applet below illustrates the proof of Proposition 6, i.e. the case where two weights can be measured in terms of a common unit measure. The applet follows the modern rendition of the proof from [Stein and Hirshfeld].\nGiven two unequal weights - 3 and 4 at the beginning, but these can be changed by clicking at the two numbers at the bottom of the applet.\nReplace each with an equivalent number of unit weights and place them so as to preserve the center of gravity of the two given weights. For 3 this means placing one unit at the same point and two other units on both sides and at equal distances from there.\nFor 4 this means placing two units on each side.\nSubject to this requirements, the unit weights could be placed arbitrarily. For the sake of the proof, their distribution over the beam needs to be uniform.\nTo achieve the uniformity, the segment between the initial weights (of 3 and 4) needs to be divided into 7 equal pieces and a few pieces of the same length need to be added on both sides of the beam. The result is seven unit weights with three sharing the center of gravity of the weight 3 and the other four sharing the center of gravity with the weight 4. The seven unit weights have the center of gravity at the midpoint between the extremes, i.e., at the location of the middle unit weight. Placing a fulcrum at this point will balance the system of 7 weights and also the system of the two initial weights of 3 and 4 units. This point divides the beam in the ratio of 4:3.\n|What if applet does not run?|\nNote that at times it is possible to find several common units for the given two weights. For example, for weights 6 and 8 we could choose either 1 or 2 as a common unit. In the first case, the fulcrum will be located at the point that divides the beam in ratio 8:6; in the second 4:3, which is one and the same point. The situation repeats itself every time the given integer weights are not mutually prime, i.e. have a non-trivial (greater than 1) common factor. The applet allows you to pick the largest possible - the greatest common divisor of the two integer weights.\nThe Law of Lever is easily extended to the case of rational weights as two fractions are always commensurable. Given two fractions p/q and r/s,\nTo prove his Proposition 7, i.e., the case where the two given weights are incommensurable, Archimedes makes an implicit assumption that each is arbitrary close to a magnitude commensurable with the other. As we now know this is of course true. This follows from the density of the rational numbers among all reals (and relevant here is an application of the Pigeonhole principle).\nAssume two incommensurable weights U an V located at points A and B are not balanced at point C satisfying\n- S. Stein, Archimedes: What Did He Do Besides Cry Eureka?, MAA, 1999\n- A. Hirshfeld, Eureka Man: The Life and Legacy of Archimedes, Walker & Company, 2009\n- Archimedes, On the Equilibrium of Planes or The Centers of Gravity of Planes I, Great Books of the Western World, v 11, Britannica, 1952, pp. 502-509\nCopyright © 1996-2018 Alexander Bogomolny"", ""Sustainability Series: Green Roofs\nArticle information and share options\nWith increased urbanization, green roofs are becoming a major trend. They offer many advantages to building owners and occupiers, the general public, and the environment: better insulation, reduced power consumption, stormwater retention, improved air quality, and a biodiverse environment offering aesthetic diversity. However, green roofs may introduce property considerations and concerns that owners and occupiers should be aware of when reviewing their property and business risks.\nGreen roofs comprise specific vegetation designed and installed on a series of components, including moisture retention material, a drainage system, a root barrier, and a protective layer for both the insulation and underlying roof surface.\nProperty Risk Considerations\nLet's take a closer look at a few features that will likely be of interest to commercial property insurers when evaluating the building's overall risk profile.\nDownload our guide\n1. Potential for water leakage damage\nThe irrigation pipework may be susceptible to freezing. Leaks in waterproofing membranes due to root growth or temperature fluctuations may expose the underlying roof structure to damage or corrosion. Drainage systems may become blocked by soil and vegetation. These risks are particularly relevant in the construction phase.\nLoss Prevention Tips: Penetrations in the roof should be minimized. It is important to include leak detection systems below the waterproofing membrane. Size gutters to accommodate both rainfall and irrigation runoff and maintain them regularly, with specific inspection points included. Consider the impact of roof gradient on the density of growth media and its propensity to shift or slide during heavy rain events.\n2. Fire load\nGreen roofs and their supporting components (e.g., vegetation and waterproofing membrane) typically add combustible loading to a roof and increase the potential for ignition from exposing fires or other ignition sources. In some cases, the design of a green roof may encompass space for recreational activities, introducing ignition sources such as lighting, electrical installations, barbecues/grills, and smoking.\nLoss Prevention Tips: Limit the overall fire risk by maintaining adequate moisture content of vegetation, regular removal of dead vegetation, and careful design and placement of noncombustible fire breaks. Include manual firefighting equipment such as fire hydrants, fire hose reels, and portable fire extinguishers, along with access provisions for the fire brigade. Potential ignition from electrical equipment, smoking, or other sources should be identified and carefully managed.\n3. Susceptibility to collapse\nGreen roofs introduce live loads associated with landscaping, precipitation, induced saturation due to irrigation, and periodic replacement of new soil and growth of vegetation.\nLoss Prevention Tips: The load carrying ability of concrete roofs versus all other types (e.g., long span steel) should be considered over the roof lifecycle. Some roofs may be susceptible to deformation over time, affecting the operational efficiency of certain components that originally functioned properly. Future changes may affect the structural integrity and should undergo formal review. Concrete roofs are generally more resilient than other roofs to changes that increase live loading.\n4. Damage from natural hazards\nGreen roofs are susceptible to natural hazard perils such as seismic forces and wind uplift pressures. Vegetation or potentially the entire roof system may be damaged, requiring replacement.\nLoss Prevention Tip: The system, including any moisture/root barriers, should be properly secured to structural elements (growth media should not be relied upon). Green roofs should not generally be installed in areas with elevated wind exposures such as coastal areas subject to hurricanes.\nGreen Roofs provide many environmental benefits, especially in urban locations where vegetation may be scarce. However, if a green roof is to be installed, ensure that the considerations outlined above are incorporated in the design, installation, and future maintenance requirements to mitigate the potential for loss.""]"	['<urn:uuid:3ac74d82-7409-4a52-9b6d-6fd140c14bec>', '<urn:uuid:adefe97e-66fa-495a-b3a2-8e24a422fce1>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T22:03:18.033296	14	65	1646
12	I'm interested in getting a Silkie chicken as a pet. What makes them special in terms of their appearance, and how do they behave around people and other chickens?	Silkie chickens have a truly unique appearance with fine, fluffy feathers that feel like silk or satin because they lack barbicels (hook-like projections). They have several distinctive physical features including black skin and bones, turquoise blue earlobes, dark wattles, and 5 toes instead of the normal 4. They come in various colors including white, black, blue, buff, grey, and partridge. As for their behavior, they are extremely calm, friendly, and docile birds that are especially good with children. They enjoy human interaction and are easy to handle, though their submissive nature means they do better when kept with other gentle breeds rather than more dominant chickens. They're well-suited to both confined spaces and urban settings, making them excellent pets for families.	['Silkie chicken is one of the most popular and beloved ornamental chicken breeds. And it is currently one of the most entertaining birds to watch. The breed is also known as Silky. It was named as Silkie chicken because of it’s a typically fluffy plumage, which is said to feel like silk. It has hair-like plumage, which is considered as actual mammalian fur. It is a very old Asiatic chicken breed. Although the Silkie chickens have been around for several hundred years, but it’s origin is uncertain. The breed is thought be originated in China, India or Japan. The exact location from where they were developed or found is not known.\nThey arrived in Europe around 200 years ago, and reached the United Kingdom around the mid 1880s. Silkie chicken was first admitted into the American Poultry Association’s Standard of Perfection in 1874. Today the breed is the most popular and ubiquitous ornamental chicken breed. And they are kept for both ornamental and exhibition purpose. There is a bantam version of this breed. Review characteristics, behavior, temperament and full breed profile of Silkie chicken below.\nSilkie chicken is a very unique looking and beautiful bird. They are a lightweight chicken breed with broad, stout looking body which is covered in fine fluffy feathers. Their plumage looks like fur or down instead of feathers. Because, their plumage lacks barbicels. Barbicels are the hooked projections normally extending from and interlocking the secondary branches of chicken’s feathers. Because of their such plumage, they are unable to fly. As the breed is small in size, so they are mistakenly called a bantam breed in some countries. But it is a standard breed of large fowl class. And the bantam Silkie chicken is actually a separate variety. The Silkie chicken appears in two distinct varieties. Which are Bearded and Not-Bearded. Bearded variety has an extra muff of feathers under the beak area which covers the earlobes. Silkie chickens are also separated according to color. White, Black, Partridge, Buff, Grey and Blue are the colors of Silkie chicken recognized for competitive showing. Other colors also exist, which include; Red, Splash, Cuckoo and Lavender.\nAccording to the American Poultry Association’s Standard of Perfection, all the Silkie chickens have small walnut comb which is a deep mulberry approaching black. They have turquoise blue earlobes and dark wattles. They have black skin and bones and 5 toes instead of the normal 4. Other chicken breeds which exhibit this rare 5 toe trait include; Faverolles, Dorking and Sultan. Their legs color is dark-blue with abundant feathering. Their flesh color is grayish-black. Standard Silkie cocks weight about 1.8 kg and hens about 1.36 kg. And the bantam chicken’s weight vary depending on different standards. American Standard of Perfection calls for cocks that are 1 kg, and hens that are 0.907 kg. The British Poultry Standard and Australian Poultry Standard call for much smaller bantam Silkies. According to the British Standard, the cocks weight 0.6 kg and hens 0.5 kg. And Australian Poultry Standard weight for bantam Silkies is 0.68 kg for cocks and 0.57 kg for hens. Photo from Wikipedia.\nSilkie chicken is very calm, trusting and friendly. They are unable to fly and can be kept easily with low fencing. They usually do little or no damage to the garden if given them to access freely. Silkie hens are not good layers. They lay a few cream or tinted colored eggs. But they are great mothers, and go broody frequently. They are even supremely happy to hatch the eggs of other birds. So they are frequently used as foster mothers for other birds.\nSome poultry fanciers consider Silkie hens as the ideal organic incubator. Silkie hens cheerfully brood and raise their unorthodox offspring too, if given the chance. The hens stop laying eggs during the summer months. The Silkie chicken should be kept in dry conditions, because their feathers are not waterproof. The breed is extremely suitable as pets, especially suitable for children. They are very cold hardy and also do well in confinement. The breed was valued as a medicinal food item because of its black skin and bones. The average lifespan of Silkie chicken is around 9 years. Review full breed profile of Silkie chicken below.\n|Silkie Chicken | Breed Profile\n||Bears Confinement Well, Calm, Easy to Handle, Friendly, Quiet, Docile\n||Cream or Tinted\n||Black, White, Grey, Blue, Splash, Buff, Partridge are the standard colors with many off standard & derivative colors in existence today.\n|Country of Origin\n||Thought be originated in China, India or Japan.', 'In case you’re looking for an ornamental chicken breed that won’t annoy the neighbors but still has plenty of friendly personalities, the Chinese Silkie chicken is worth a look.\nSilkies stand out from the crowd in a very obvious way. Some have compared the feel of their beautiful plumage to that of satin or silk.\nHistory of the Chinese Silkie Chickens\nAlthough the true beginnings of Chinese Silkie chickens are unknown, the first documented sightings were in China; consequently, the origin of “Chinese Silkie” was given to this breed.\nIt has also been suggested that the breed came from places like India and Java in Asia. The first known written record of the species, from around the 13th century, describes it as a “furry” chicken encountered by the Spanish explorer Marco Polo in his travels through Asia.\nWriter and naturalist at the Italy’s University of Bologna, Ulisse Aldrovandi published a treatise on chickens in 1598 that is still studied and admired today. There, he mentioned chickens that produce wool and others that have “hair like a black cat.”\nThe exact beginnings of the special combination of characteristics that characterize this breed are obscure, but they are most strongly linked to ancient China.\nWhen looking at the timeline, this first appeared before the year 1200. As a matter of fact, the Silkie is also known as the Chinese silkie chicken.\nOther possibilities include Southeast Asian locations such as Java and India. An abundance of data about the Silkies can be found in ancient Chinese texts.\nThe Silkie chicken has been revered in Eastern cultures for centuries due to the belief that it has superior medicinal properties to those of any other breed.\nThis belief is supported by research showing that Silkies have higher levels of carnosine production compared to other poultry. Over the centuries, the Silkie chicken has become one of China’s most coveted breeds.\nTraditional practices, such as giving Silkies to new mothers as a form of postpartum recovery, date back to a time when such thinking was less grounded in science.\nMany Asian countries regard this particular breed of chicken as a culinary delight, and so many recipes center around it.\nThe earliest written mention of Silkies in writing dates back to the 13th century when Italian explorer Marco Polo described seeing “furry chickens” in Asia.\nA second well-written mention can be found in a treatise on chickens from 1598, which is still widely read today.\nIt was written by Ulisse Aldorvandi of the University of Bologna in Italy, and it detailed the existence of chickens with wool on their bodies or clothing made of hair similar to that of a black cat.\nThe first examples of the Silkie chicken probably made their way to the West via maritime trade and the Silk Road.\nAccreditation with the North American Standard of Perfection came in 1874. The prevalence of Chinese Silkie Chickens coincided with a rise in myths about the breed.\nChickens purportedly clad in mammalian fur were on display at carnivals. In fact, early Silkie sales pitches claimed the breed was created by combining chickens and rabbits.\nPhysical characteristics of the Chinese Silkie Chickens\nThe Chinese Silkie Chicken is sometimes classified as a bantam breed, though this varies by country and many breed standards list them officially as large fowl.\nThe bantam Chinese Silkie chicken is typically considered to be a different variety altogether. Whereas the original version of the breed originated in Europe as a standard-sized variety, virtually all North American strains are bantam-sized.\nThe average male Silkie weighs only four pounds (1.8 kg) and the average female weighs three pounds (1.4 kg), demonstrating that even the largest Silkies are still relatively small chickens (1.36 kg).\nMales weigh in at 36 ounces (1 kg) to meet the American Standard of Perfection, while females must weigh in at 32 ounces (900 grams) (910 grams).\nAs a result of their small size, Silkies are classified as bantams. Once unique among chicken breeds, Chinese silkie chicken feathering has been developed in several breeds, most notably the Chabo, where it is now standardized in Britain and the Netherlands.\nSeveral analogies to silk and fur have been made. As a whole, the effect is fluffy and light. Their feathers are more down because the barbicels don’t work. Because of this trait, Silkies can’t take to the air.\nBearded and shaved Silkies are the two main types seen. Bearded Silkies are distinguished by the presence of a thick plume of feathers that extends from the base of the beak to the lobes of their ears.\nAs with the other types, the colors are used to categorize them. Silkies can be shown competitively in a variety of colors, including black, blue, buff, grey, partridge, and white.\nThere are other colors available besides traditional pinks, such as cuckoo, lavender, red, and splash. All perfect Silkies have dark wattles, small combs in the shape of a walnut, and turquoise in their earlobes.\nAlong with these distinguishing characteristics, Silkies also have five toes on each foot. The Dorking, Faverolles, and Sultan are just some of the other breeds that share this unique quality.\nTheir Chinese name, Wu Gu Ji, translates to “black-boned chicken” because their skin, bones, and grayish-black meat are all the same color.\nFibromelanosis is a rare mutation that appears to have originated in Asia and causes chickens to have a form of melanin that extends beyond the skin into the connective tissue.\nThe Chinese Silkie chickens and other breeds with Asian ancestry carry the mutation. Regardless of feather color, the breed is not as productive as other, more common chicken breeds used for meat.\nTemperament and Personality of Chinese Silkie Chickens\nChinese Silkie Chicken is a great choice for families who want to introduce their children to chicken keeping and for those who keep chickens as a hobby.\nBecause of their laid-back docility, friendliness, and submissive nature, they are widely admired.\nThe males are just as pleasant as the females. They have a gentle demeanor and enjoy interacting with people. Even young children can easily pick them up and cuddle with them.\nWhen kept in a flock with more dominant chicken breeds, the Silkies’ submissive nature can cause tension.\nTherefore, the best Chinese Silkie chicken companions are other docile breeds. One excellent illustration is the Polish hen. Famous as companion animals, brooding birds, and decorative birds, silkies have earned a place in the spotlight.\nUntil the eggs hatch, the hens will proudly sit atop their brood.\nThere are fewer Silkies than there used to be, so many people will use the eggs from the fewer Silkies to feed their chickens.\nEven though chickens aren’t typically thought of as potential pets, Silkies help to close the gap between backyard fowl and household companions.\nThey thrive in both confined and wild environments. As a species, they lack both flying proficiency and fearlessness. The Silkie is well-suited to urban settings, families, and farms that require broody hens.\nEgg Production of Chinese Silkie Chickens\nWhen it comes to sitting on eggs, Silkies are among the best chickens. Unfortunately, their own egg-laying abilities aren’t exactly top-notch.\nIn a normal year, a Silkie hen might only produce 100 to 120 eggs. During the warmer summer months, they might even stop laying eggs altogether.\nThough smaller than eggs from larger chickens, these breeds still produce nutritious and small chickens. It has been said that Silkie eggs are actually blue, but this is not the case. Small eggs are a uniform light brown in color.\nWhile Silkies may not set any records for annual egg production, they are remarkably reliable birds. About three eggs are laid per week on average by each hen.\nDue to the comfort provided by their feathers, these hens are more likely to continue laying eggs all through the winter.\nIf hatching eggs is a priority, Silkies are best housed with more prolific egg layers. This trait that makes Silkies such good foster mothers also makes them less likely as biological mothers because it disrupts their egg-laying.\nHealth Issues and Care of Chinese Silkie Chickens\nWhen compared to other chicken breeds, Silkies are not particularly prone to common health problems. The fluffy feathers make them more susceptible to parasites like mites and lice, however. Silkies must be checked frequently for these parasites.\nIt is important to treat a Silkie that has been infected with parasites as soon as possible. A veterinarian or the pet’s owner and caretaker are both good sources for this information.\nThe potential for an infestation to spread throughout the flock emphasizes the urgency of treatment. If parasites are not treated, it can lead to severe health issues in chickens.\nSilkies need to take regular dust baths to protect themselves from parasites like fleas, mites, and lice. Make sure there are plenty of dust bathing facilities available for the flocks.\nSilkies, like all chickens, need regular worming treatment. It is also possible for internal pests to cause disaster in a flock if they go unnoticed and untreated.\nMarek’s disease is a bacterial health that can be fatal to Chinese Silkie Chickens. A mutated herpes virus is responsible for this illness. It’s important to note that the virus’s pathogenicity depends on the strain.\nEven though many breeders have worked to increase Silkie’s natural immunity, vaccination is another option for protecting the breed from health.\nIf you’re looking for a fluffy, sociable, and out-of-the-ordinary pet or chicken, a Chinese Silkie chicken is a great choice. They are friendly and affectionate birds that enjoy interacting with humans but don’t get into trouble in the yard or coop.\nUnique among chicken breeds, this small chicken has a rich history and a striking appearance.\nChinese Silkie chickens are highly sought after as pets, rather than for chickens, so there is a good market for those who intend to raise them. White Silkies in particular are rising in popularity.\nThey are one of the most profitable chickens to raise because of their high demand and high market price.']	['<urn:uuid:d440e846-84a3-4a33-b8b8-7d4cde80bd69>', '<urn:uuid:270148cb-61c7-4096-b7f5-5eab9c78f25b>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T22:03:18.033296	29	121	2424
13	What role do modern medical imaging technologies play in diagnosis compared to traditional methods, and how has the approach to patient care evolved from historical monastic healthcare traditions?	Modern medical imaging technologies represent a significant evolution from traditional methods, offering comprehensive diagnostic capabilities through tools like CT scans, MRI, nuclear medicine, and digital X-rays that provide detailed views of organs, bones, and tissues without delays associated with traditional film methods. These advanced imaging techniques allow real-time viewing of scans and remote access by consultants. This technical evolution builds upon the historical foundation of monastic healthcare, which began with figures like St. Basil who established the first dedicated spaces for sick people, and St. Benedict who emphasized caring for those who came to monastery gates seeking help. The modern approach combines technological advancement with the traditional values of patient care, seen in how modern facilities like Beacon Hospital maintain both cutting-edge equipment and a focus on patient comfort and support during procedures.	['THE SOCIAL DIMENSION OF FAITH\nSr Maria Consuelo Tavares, OSB\nA Glance at Monastic Origins\nWe, the Benedictine Missionary Sisters of Tutzing, attempt to live from day to day the essential elements of monastic life, recovering the missionary dimension in the Benedictine tradition, in accordance with a thought which inspired our founder, Fr Andreas Amrhein. Looking over the monastic tradition we find already in St Antony a concern for helping his brothers: from time to time he would emerge from his hermitage to confirm his brothers in faith and to offer them what he had left over from his daily sustenance. This was just the help with Christians gave to Pachomius when he was a prisoner, a help which opened his heart to the Christian faith and made him decide to set out on the path of monastic life, without however neglecting the needs of his brothers.\nSt Basil, putting into practice his ideal of fraternal community, was concerned with granting hospitality to his brothers and helping them in their spiritual and material needs. It is claimed that he was the first to set aside a place specifically for sick people - the origin of the hospitals we have today. It was the experience of urban monasticism that brought him to see the necessity of sharing as a pastor the material needs of his flock. Basing himself on the eastern tradition, St Benedict attempted a synthesis of eastern and western monastic life. In the solitude of Subiaco, he made his own the experience of savouring the scriptures and the teachings of the Church Fathers. This brought him to accept the mandate to announce the gospel to those who did not know it. Thus he evangelized shepherds, cured the sick, and comforted everyone who came to him thirsting for a word of life to cure their afflictions. St Benedict recommends special care of those who come to the monastery gate: ‘As soon as someone knocks or a poor man calls out, the gate-keeper is to answer “Deo Gratias” or “Benedic”, and with all humility and the fear of God he is to reply swiftly and with the fervour of love (RB 66).\nWith this welcoming attitude Benedict teaches us how important it is to be aware of the needy people who arrive continually at our monasteries and to share with them what we have and what we are, bearing in mind all the implications of Benedict’s time, and those of our contemporary world. Thanks to the tradition of going on pilgrimage, of setting forth, this is how a great part of Europe was evangelized by the Benedictines. Our own founder, Fr Andreas tried to actualize in the nineteenth century this early fervour of our tradition. Thus in 1884 that he founded a masculine branch of the missionary Benedictines, and in 1885 the feminine branch of the Benedictine missionary sisters, whose 125 anniversary of their foundation is being celebrated this year.\nPutting our experience into Practice\nPutting our Benedictine missionary charism into practice has been a priority for our sisters from the very beginning of our mission, the concern to live the monastic charism of personal prayer, communal life and liturgy, as a support for missionary work, which in turn has always been a fundamental element for us. It is in the community that our sisters have come to live out their charism of evangelization and social work. In all the communities, scattered throughout Europe, Asia, Africa and America, the social dimension of faith has always been accentuated, that is to say, evangelization on the basis of concrete reality, based on an integral vision of the human person.\nHere in Brazil, particularly in the north-east, characterized as it is by marked social inequality, climatic difficulties and shortage of governmental funding, it became an evangelical necessity to unite evangelization with social work. For this reason social centres were created in order to work for the advancement of the human dignity of children, adolescents and adults.\nIn Caruaru we have the S. José do Monte social centre, where children and young people in difficult situations are welcomed, orientated, and offered professional courses which can guarantee them a source of income and enable them to get off the streets. Equally in Casa Caida in Olinda there is a social centre where children attend a crèche, receive food and begin their formal education. Mothers in need take part in professional courses, making use of recyclable material to generate income for their families. In its turn the third age has its own space given to it, in the sense of receiving geriatric orientation, which enables them to gain opportunities for a better quality of life.\nIn the south of Bahia, together with the work of evangelization in 36 rural communities, we work on a programme of combating drought in the semi-arid north-east. The specific task of the sisters consists in mobilizing communities so that, organized together, they can construct systems to capture rainwater, and so minimize the effects of the long dry summers. It is a work done in common, accompanied and stimulated, to improve living standards. The government and international organizations, such as Taufkirchen, finance the programme, while those for whom it is destined engage in the manual work, thus promoting a culture of solidarity.\nIn Olinda in the Campo du V8 Favela also we encounter a great challenge of evangelizing and improving human living standards, the challenge of proclaiming the gospel where the conditions of life are minimal. In this we rely upon a great ally, Dom Helder Camara.\nIn its beginnings, the favela was a great rubbish pit, with disastrous consequences for the people living there. One day Dom Helder, heeding the cries of the poor, pronounced firmly, ‘From today onwards, every rubbish cart dispatched here will be my responsibility.’ From then on his voice was heard, and the rubbish was diverted to another location. The struggle continued. In collaboration with the evangelical Christians we are trying to mobilize people for the communal construction of houses. Right at the beginning forty-five houses were communally built by the women of the community, who gathered together in groups of fifteen and built one house after another. The residents’ association then called on the public authorities to grant the community a better quality of life.\nThe work of evangelization and human improvement in this favela has become an important element in the formation of our sisters in the course of their formation. They go daily to the community in order to give classes in the community school, which was set up there also at Dom Helder’s suggestion. Despite the many challenges caused by drug trafficking, alcoholism and unemployment, we already perceive an improvement of conditions of life of the 3,000 people who live in this community.\nAs a result of the programme for urbanizing the favelas there is already basic sanitation, a cleansing of the canal which cuts through the favela, together with the construction of houses in stone, so that people can move their dwellings from the edges of the canal.\nIn the last two years an organization for children has been set up. It attempts to organize people and identify heads of families who need help for pregnant women and children under six years old. The heads of families who offer themselves voluntarily for work do this with the support of faith, with the aim of saving lives through basic work on health, nutrition, education and citizenship, normally holding meetings to discuss the problems experienced in the community and to work out possible solutions. Our sisters in formation are engaged in this work, and they do it at the same time as evangelizing groups of women, children and adolescents. Our missionary charism is richly expressed through social action which leads us to respond to the cry of the poorest. This makes it possible for us to respond to Jesus’ teaching, ‘Whatever you do to the least of my brothers and sisters, you do to me’ (Matthew 25.40).\nThis social dimension of faith brings us closer to the roots from which the force of our action springs, in an attempt to live in today’s world the motto which inspired our founder, Fr Andreas Amrhein, when he said, ‘Charity should be the characteristic virtue of the messengers of faith’.', 'Beacon Hospital is home to the most technologically advanced Radiology equipment in Ireland. Our comprehensive Radiology department are here to offer an extension range of diagnostic treatments, to combat all ailments. Our diverse specialists are here to listen to all your needs and evaluate your condition from every angle, to give you the greatest piece of mind possible.\nBeacon Hospital is home to Ireland’s most technologically advanced diagnostic radiology equipment. Our department is the European show-site for General Electric which manufactures some of the world’s top medical equipment.\nOur team of 15 expert Consultant Radiologists work together with our highly qualified and experienced Radiographers in ensuring that patients receive the best care and top quality imaging and reporting.\nOur Radiology Department is a fully digital unit which allows your radiographer to see the scans in real-time as they are taken to ensure all essential areas have been included. Digital scans allow your primary Consultant access to the images remotely, ensuring that all medical professionals involved in your care are kept fully informed of your reports and results without delays associated with more traditional film methods of imaging.\nBeacon Hospital’s Radiology Group is led by our highly qualified Radiologists with dedicated PET/CT and MRI fellowship training from world leading medical centres.\nRadiology Department Imaging\nCT or CAT Scan\nA computed tomography (CT or CAT scan) allows doctors to see inside your body. It uses a combination of X-rays along with a computer to create pictures of your organs, bones, and other tissues. It shows greater detail than that shown on a regular X-ray.\nA CT scan can be performed on any part of your body. It doesn’t take very long, is completely painless, and claustrophobia is not an issue.\nSometimes, depending on the area being targeted, your doctor may request a contrast CT. This is a CT scan with the addition of a dye, to help the doctor see the area being examined in greater detail. The dye used may be injected or can be in the form of a liquid you may be asked to drink, prior to your scan.\nDEXA scans are sometimes called bone densitometry scans, QDR scans or BMD measurement.\nA DEXA scan is used to measure bone mineral density (BMD) of the spine and hips which helps to assess the risk of bone fractures. This measurement is often referred to as bone mineral density (BMD).\nDEXA scans use a very low radiation dose, which is lower than less than one day’s exposure to natural environmental radiation. DEXA scans are most commonly used for diagnosing osteoporosis (weakened bones that may fracture easily) and assessing the risk of osteoporosis developing. They can also be used to detect other bone disorders and conditions, such as osteopenia, and to measure the relative amounts of body fat and muscle.\nA DEXA scan is a quick and painless way of measuring BMD. It is more sensitive than a normal X-ray when identifying low bone density.\nYou may be offered a DEXA scan if you are considered at high risk of having or developing osteoporosis. The scans are also used to monitor the effectiveness of treatment for conditions such as osteoporosis.\nFluoroscopy is a study of moving body structures-similar to an X-ray “movie.” A continuous X-ray beam is passed through the body part being examined. The beam is transmitted to a TV-like monitor so that the body part and its motion can be seen in detail.\nFluoroscopy enables physicians to look at many body systems, including the skeletal, digestive, urinary, respiratory, and reproductive systems.\nFluoroscopy may be performed to evaluate specific areas of the body, including the bones, muscles, and joints, as well as solid organs, such as the heart, lung, or kidneys.\nA Hepatobiliary Iminodiacetic Acid scan or HIDA as it is more commonly referred to as, is an imaging procedure used to diagnose problems of the liver, gallbladder and bile ducts.\nDuring a HIDA scan a radioactive tracer substance is injected into a vein in your arm. This tracer travels through your bloodstream to your liver, where bile-producing cells absorb it. The tracer then travels with the bile into your gallbladder, through your bile ducts and into your small intestine.\nA special nuclear medicine scanner called a gamma camera is used to track the flow of the tracer from your liver into your gallbladder and small intestine. The camera records images of the path and feeds these to your radiographer.\nThe scan can take 60 to 90 minutes and so patients are advised to bring along some headphones with their favourite music, podcast or audiobook to help pass the time.\nA mammogram is an X-ray of the breast. Mammograms are the best way to find breast cancer early when it is easier to treat and before it is big enough to feel or cause symptoms. At this time, a mammogram is the best way to find breast cancer for most women. While women often report a mammogram not being the most pleasant of experiences, it is extremely quick and is a hugely important tool in the examination of the breast. Any discomfort felt during the scan is short lived.\nPlease note that Ultrasound may be used to examine breast tissue in younger patients. A mammogram is often unsuitable for imaging on the more dense breast tissue associated with younger women.\nMagnetic resonance imaging, or MRI as it is more commonly known as, is a diagnostic test that produces very clear images of the human body without the use of X-rays.\nMRI uses a large magnet and radio waves to look at organs and structures inside your body. MRI scans can be used to diagnose a variety of conditions, from torn ligaments to tumours. MRI’s are particularly useful for examining the brain and spinal cord.\nBeacon Hospital is home to two new, state-of-the-art MRI machines. These modern machines offer superior comfort to more traditional machines. The bore (hole in the machine), is larger in these modern machines which eases anxiety some persons suffer in small spaces.\nWhen patients are having their scan, they will be provided with headphones to listen to their favourite music. Our radiologists are able to talk to patients throughout their scan providing reassurance and support.\nNuclear medicine is a specialised area of radiology that uses very small amounts of radioactive materials, or radio-pharmaceuticals, to examine organ function and structure. A small amount of a radioactive material is given to the patient via injection, inhalation or in tablet form prior to the scan.\nBecause X-rays pass through soft tissue, such as intestines, muscles, and blood vessels, these tissues are difficult to visualise on a standard X-ray, unless a contrast agent is used. This allows the tissue to be seen more clearly. Nuclear imaging enables visualization of organ and tissue structure as well as function. The extent to which a radio-pharmaceutical is absorbed, or “taken up,” by a particular organ or tissue may indicate the level of function of the organ or tissue being studied. Thus, diagnostic X-rays are used primarily to study anatomy. Nuclear imaging is used to study organ and tissue function.\nThe radiation given is absorbed by the body tissue and is detected by a radiation detector, or gamma camera. By measuring the behaviour of the radionuclide in the body during a nuclear scan, we can assess and diagnose various conditions, such as tumours, infections, hematomas, organ enlargement, or cysts. A nuclear scan may also be used to assess organ function and blood circulation.\nPositron Emission Tomography (PET) uses small amounts of radioactive materials called radiotracers or radiopharmaceuticals, a special camera and a computer to evaluate organ and tissue functions.\nA PET scan requires the patient to be injected with a small amount of a radioactive material called Fluorodeoxyglucose (FDG). The movement of the FDG around the body is then monitored during the scan, highlighting areas of interest. These areas are known as ‘hot spots’.\nBy identifying changes at the cellular level, PET may detect the early onset of disease before other imaging tests can.\nUltrasound, also known as sonography, uses high frequency sound waves to capture live images of the inside of your body. Your Consultant may refer you for an ultrasound to get a visual of your organs, vessels or tissues. Ultrasound does not use radiation which is why it is so commonly used for pregnancy imaging.\nX-rays are a type of radiation called electromagnetic waves. X-ray images create black and white pictures of the inside of your body. This is because different tissues absorb different amounts of radiation. Calcium in bones absorbs x-rays the most, so bones look white. Fat and other soft tissues absorb less and look grey. Air absorbs the least, so lungs look black.\nThe most common use of x-rays is checking for fractures (broken bones), but x-rays are also used in other ways. For example, chest x-rays can be used to see pneumonia. Mammograms use x-rays to look for breast cancer.\nWhen you have an x-ray, you may be asked to wear a lead apron to protect certain parts of your body. The amount of radiation you get from an x-ray is very small. For example, a chest x-ray gives out a radiation dose similar to the amount of radiation you’re naturally exposed to from the environment, over just 10 days.\nResults & Next Steps\nFollowing your diagnostic imaging, the images taken will be thoroughly examined by one of our expert Consultant Radiologists. A report will be compiled and sent to your referring Consultant or GP who will relay the results to you.\nIf nothing further is required, you will remain discharged into the care of your GP. If further investigation or treatment is required, this will be explained to you and you may be referred on to a specialist Consultant in the relevant area.']	['<urn:uuid:f976c786-182d-4987-a59e-36eac4e35551>', '<urn:uuid:5e5a125c-5d3e-4941-9bca-0b0aeda9d6ad>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T22:03:18.033296	28	133	3017
14	bb king guitar style legacy influence	B.B. King's guitar style combined Mississippi blues and jazz with gospel influences, mastering single-string guitar and string bending techniques. While he directly influenced rock guitarists like Eric Clapton and Jeff Beck, his legacy also lives on through supporting musicians like Eric Demmer and Charlie Dennis who continue carrying the blues flame after playing in his band.	"['1 Rusty Dusty Blues\n2 Telephone Blues\n3 Blind Love\n4 Blue Shadows\n5 Cant Get Enough\n6 Let The Good Times\nThe KING of the Blues\n""The Blues? It\'s the mother of American music. That\'s what it is--the Source.""----B.B.\nSince the late 1960s, when rock and pop audiences discovered him and his refined, majestic brand of the blues,\nguitarist and singer B.B. King has been the music\'s most successful concert artist and its most consistently recognized ambassador.\nHe has been bestowed with more awards and honorary degrees than any other bluesman and has made the cause of preserving the\nblues his lifetime work. Almost singlehandedly he brought the blues out from the fringe of the American music spectrum and\ninto its mainstream. Thanks to King, blues is now performed in the most prestigious venues and in front of audiences whose\nintroduction to the blues often stems back to the first time they heard a B.B. King record.\nKing has also had a\nprofound effect on the inner workings of rock & roll. Few, if any, bluesmen have exerted more influence on rock guitarists\nthan King. Greats such as Eric Clapton, Jeff Beck, and Jimmy Page, along with Johnny Winter, Billy Gibbons, and Stevie Ray\nVaughan were all touched by King to some degree. As for blues guitarists, virtually every major stylist from the postwar period\nhas, in some capacity, been influenced by the King style. A member of the Blues Hall of Fame and the Rock & Roll Hall\nof Fame, B.B. King has continued to be a vital performer and prolific recording artist despite advancing age and health problems.\nHis graciousness and articulation, especially when discussing the meaning and significance of the blues, have done much to\nbuild respect for the music and its culture.\nKing\'s guitar style is essentially a consolidation of deep Mississippi\nblues and jazz, coupled with strains of gospel, rock, and pop. A trained ear can detect traces of Blind Lemon Jefferson, T-Bone\nWalker, and Lonnie Johnson in King\'s guitar solos, as well as those of jazz guitar legends Charlie Christian, Eddie Lang,\nand Django Reinhardt. King is the acknowledged master of the single-string guitar style and the technique called string bending,\nwhich is employed to embellish the emotional intensity of a guitar passage.\nKing is not a flashy or busy guitarist,\nyet his solos sting rather than soothe the senses. He often uses vibrato to accent notes and phrases, and he gives his guitar\npassages plenty of room to breathe within the context of a song\'s arrangement. At his best, King pushes his guitar solos to\nbecome an extension of his voice, so that the result is a practically seamless blues presentation. If there\'s been any criticism\nof King and his guitar style, it usually centers on his work being too slick and too neat. However, there is no denying the\ndynamics and tension that run through his best work. What King has done more than anything else is elevate the blues guitar\nsolo to a high art. He has taken the blues guitar out of smokey clubs and funky roadhouses and relocated it to a more sophisticated\nsetting, namely the concert stage.\nBorn in Mississippi, King\'s earliest interest in music came from the church,\nwhich is where he learned to sing gospel music. After being taught a few chords on the guitar by the minister of his church,\nKing\'s interest started to extend beyond just singing. He began listening to guitar-playing bluesmen more intently and was\nmoved by the jazz guitar work of Charlie Christian. As a young man King was a Mississippi Delta farmhand and tractor driver,\nworking the fields during the week and playing music on weekends.\nAfter World War II, King went to Memphis and stayed\nwith his cousin, bluesman Bukka White, before returning to the Delta in late 1946. He did farm work for one more year before\nleaving it for good. In 1947, King moved to Memphis. Me had heard harmonica player Sonny Boy Williamson (Rice Miller) perform\non West Memphis radio station KWEM; King went to see Williamson and requested work. Williamson had him perform on his program,\nwhich led to other work for King on Memphis station WDIA, hawking an alcohol-based tonic called Pepticon and playing and singing\nblues songs for ten minutes every day.\nIn 1949, King became a full-time disc jockey on WDIA. Calling himself the\nBeale Street Blues Boy, later shortened to B.B., King got a blues and jazz education by listening to the records he spun on\nthe air. He also gained some local fame as an on-the-air personality, which he translated into performing dates in Beale Street\nblues clubs. King worked with numerous musicians, including Robert Lockwood, Jr., who helped him broaden his blues view by\nshowing him uncommon chords and jazz licks, and the Beale Streeters, an informal group of Memphis musicians ( Rosco Gordon,\nBobby ""Blue"" Bland, Earl Forest, Johnny Ace, etc.) that were lumped together for broadcasting and advertising purposes.\nKing\'s recording career began in 1949 when he cut four sides for the Nashville-based Bullet label. None of the songs\nmade much of an impression on black record buyers. But in 1951 he recorded at Sam Phillips\'s Sun studio for Modern RPM and\nthen, later in the year, at the Memphis YMCA. One of the songs from the latter session, ""Three O\'Clock Blues"" (116\nk, 10 sec.) launched King to blues stardom. The record lodged itself in the number 1 slot on the R&B charts and stayed\nthere for seventeen weeks. King\'s startling success enabled him to go on tour and play as far north as the Apollo Theater\nin New York City. There were three more number 1 hits: ""You Know I Love You"" in 1952, ""Please Love Me""\nin 1953, and ""You Upset Me Baby"" in 1954, all on the RPM label.\nDuring the early and mid- 1950s, King\nrecorded prolifically, as he was to do throughout most of his career. Many of his best recordings were not original songs\nbut interpretations of songs penned by other blues composers such as Lowell Fulson, who wrote ""Three O\'Clock Blues,""\nas well as Memphis Slim, who wrote ""Everyday I Have the Blues,"" which King turned into a hit in 1955. From Tampa\nRed, King got ""Sweet Little Angel,"" one of his signature pieces. Kingwas able to breathe new life into these songs\nand others with his increasingly sculptured guitar work and his powerful vocals.\nAnother factor in King\'s success\nwas the sound of his band and the arrangements they used. King had been greatly influenced by the big band blues sound of\nCount Basie and Duke Ellington and wanted it for his own band, which usually consisted of between eleven and fourteen members.\nUnder the astute direction of West Coast arranger Maxwell Davis, who possessed a keen understanding of how to meld horns into\na blues framework and give the resulting sound a sharp sense of swing, King\'s band had at its disposal some of the best big\nband blues arrangements ever created. So well crafted were they that King continued to use many of them right into the 1980s.\nThroughout the 1950s King seemed to finish one tour only to begin another. In 1956, he reputedly did 342 shows. When\nnot performing, he was recording. However, as the \'60s dawned, King\'s popularity began to wane. Black interest in the blues\nbegan to shrink, thanks to the advent of soul and the more urban sounds of R&B, and whites were more fascinated with country\nbluesmen than a full-fledged blues band of the kind that King led. In 1962 King switched to the ABC/Paramount label with the\nhope of cultivating a new sound and attracting a new audience. It didn\'t work; though King\'s guitar work had never sounded\nstronger, his blues framework seemed, to some blues fans, stale. Nonetheless, in 1962, King recorded Live at the Regal, an\nalbum many blues critics contend is the greatest blues recording ever made. King\'s performance was classic; his guitar gushed\nwith emotional fervor and his vocal delivery was impeccable. Yet the album\'s critical success did little to push King\'s career\nIn the late \'60s, B.B. King finally found a new and appreciative audience: rock fans. He began playing\nrock venues like the Fillmore (East and West) and rock festivals and opened concerts for the Rolling Stones. Although not\na gritty blues guitarist, the kind that most rock fans favored, King was regarded as a blues guitar master by the rock crowd.\nKing solidified his standing within the realm of rock and pop with the success of his version of the Roy Hawkins tune ""The\nThrill Is Gone"" in 1970. The record made it all the way to number 15 on the pop charts (and number 3 on the R&B charts)\nand reignited interest in King in black music circles.\nAfter ""The Thrill Is Gone,"" King became an elder\nstatesman of the blues. He carried the music through the 1970s on the whole, bad times for the blues with routinely inspiring\nlive performances. He continued to make albums, but King\'s reluctance or inability to expand his sound or even probe new ideas\nmade them only mildly interesting, except to serious blues guitar listeners and longtime King fans. During the decade King\ntoured Europe regularly and played Las Vegas. He appeared on network television and survived the disco craze at the end of\nthe l970s. King continued to record and perform through the 1980s, adding Atlantic City to his list of performance locales.\nWith the passing of Muddy Waters in 1983, King was looked upon more and more as dean of the blues.\nKing struck a\nresponsive note with a new generation of rock fans when he forged a friendship with Irish supergroup U2 and appeared on its\nacclaimed album Rattle and Hum in 1988. His guitar and vocal performance on the song ""When Love Comes To Town"" (116\nk, 10 sec.) proved that King could still belt out the blues in grand fashion. Although King now suffers from diabetes, his\nconcert schedule remains packed solid and he still manages to make new records. His 1991 album, There Is Always One More Time,\non MCA Records, was recorded with L.A. session musicians and contained a conscious, though uneven, attempt by King to work\nhis blues into contemporary pop.\nKing was inducted into the Blues Foundation\'s Hall of Fame in 1980 and the Rock\n& Roll Hall of Fame in 1987. A CD box set compilation of some of King\'s best work, called King of the Blues, was released', ""BB King. Clarence “Gatemouth” Brown. Elmore James. These are some famous names in American blues music. You might have heard them before.\nBut none of those beloved artists made their music on their own. There were supported by many other talented performers. Recently, some of these lesser known musicians decided to record and release their own music.\nThese artists have played for years in bands. Millions of people have heard their work. VOA has been following the musicians for the past year as they prepared their own CD.\nThey call themselves, “Original Legends of the Blues.” They chose the name not in praise of themselves but to honor the legendary musicians with whom they played.\nEric Demmer performed around the world for more than 12 years with Clarence “Gatemouth” Brown.\n“I was with him up until about four days before he died and it was real hard to see that happen. I felt like it would have been nice to have him around a few more years.”\nJames “Boogaloo” Bolden has been playing trumpet with BB King’s band for more than 35 years. He says people around the world have heard the blues, music created by slaves and former slaves in the southern United States.\n“Now, other people, they listen to it and they say, ‘ Well, okay, I wasn’t a slave, but, man, I had a hard time paying this bill last month and I am about to lose this job.’”\nCharlie Dennis has played guitar with the BB King band since 2002.\n“We started realizing that BB’s getting older, so we started to working on, trying to work on on other projects, to keep stuff going on.”\nJimmi Mayes sings and plays the drums. He has played with Elmore James and Jimi Hendrix. He joined the Original Legends of the Blues for the recording of the CD.\n“We are not backingup anybody. We are playing for us.”\nHe has many stories to tell, but he enjoys the stories of others as well.\n“I've been listening to Boogaloo and Charlie talking about some of the things with BB, and Eric be talking about Gatemouth Brown. There’s a lot of history in this group.”\nThe CD is called “Still Carrying the Flame.” It was released recently. Radio stations around the world are playing songs from the album. The band hopes to perform the songs at live music shows soon.\nI’m Caty Weaver.\nGreg Flakus reported this story from Houston, Texas. Christopher Jones-Cruise wrote it for VOA Learning English. Caty Weaver was the editor.\nWords in This Story\ntalented – adj. having a special ability to do something well\noriginal – adj. happening or existing first or at the beginning\nlegend – n. a famous or important person who is known for doing something extremely well\nback up – phrasal verb, to support\nflame – n. the hot, glowing gas that can be seen when a fire is burning\nBlues music was born in the United States. What kind of music was born in your country? We want to hear from you. Write your thoughts in the comments section.""]"	['<urn:uuid:f892bd1b-e15c-4230-8ec4-84e2963f6193>', '<urn:uuid:145db675-b92b-49ff-895c-0cde940e1d5c>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T22:03:18.033296	6	56	2283
15	city planner inquiring community engagement methods ecosystem preservation benefits	Community engagement in ecosystem preservation operates on two levels. First, through citizen advocacy and municipal partnerships, where citizens can perform sustainability assessments to evaluate natural resource management and work collaboratively with agencies to improve environmental sustainability programs. The assessment process serves as a catalyst for sustainable community development through combined advocacy from within and outside agencies. Second, cities are finding that protecting and restoring natural infrastructure delivers multiple benefits, including increased resilience to climate risks and improved urban livability. This requires co-producing innovative approaches with practitioners and stakeholders, while considering policies and institutions that affect all community members, particularly vulnerable populations.	"[""Citizen Sustainability Assessment\nA Demand Management Tool for Assessing Citizen Stewardship of the Community’s Natural Resources\nA starting point for demand management is to assess how well a community currently benefits from citizen resource conservation and the potential for improvement. In this regard, U.S. EPA's Office of Urban and Economic Development provided Empowerment Institute funding to develop a tool which enables municipal decision-makers and citizens to evaluate how sustainably they are using the community's natural resources. This Citizen Sustainability Assessment is the product. It will help a municipality assess:\n- The effectiveness of the various municipal policies and programs that enable citizens to conserve the community's natural resources and protect it's environment, identifying the incentives and disincentives built into the system.\n- The potential of a Sustainable Lifestyle Campaign to support the municipality's commitment to citizen resource conservation.\nThe Citizen Sustainability Assessment is divided into sections on solid waste, water, energy and transportation. In each section the user is guided by a series of questions. If the municipality is doing a self-assessment, it will need to appoint a lead agency or staff person to manage the process. This agency or person will then identify representatives of each of the appropriate agencies to do the self-assessment.\nIf the assessment is performed by a citizen advocate or group, they will need to research the data. This will be a remarkable education about the natural resource management of the community. In the beginning of each section there is a list of likely agency sources for the needed information. If the information is not in a readily usable form, persevere! In most cases the data are available, but may need to be interpreted. If after due effort, the data are not available, this knowledge, when remedied, will help the agency improve it's ability to do performance evaluation. For this feedback to be well received by the agency, it needs to be presented in the spirit of collaboration. If lack of time to institute changes is a concern for the agency, suggest that you or your citizen group are willing to provide volunteer help, if useful.\nIn some cases it will be seen that a new policy needs to be adopted or the program improved. This is where this citizen assessment tool can serve as a catalyst for sustainable community development. To create change will generally require combined advocacy from within and outside the agency. A collective effort between the city and its citizens to improve the community’s environmental sustainability is one of the major outcomes of this assessment tool. Since citizens often need municipal services and programs to enable their adoption of sustainable lifestyle choices, and municipal agencies wish citizens to participate in its programs, a mutually beneficial partnership can be developed. And, as citizens take greater responsibility for natural resource sustainability the entire community benefits.\nThere is much at stake for the future of your community—preserving its natural resources will enable current residents and future generations to enjoy those same resources. Your actions can be of lasting value to your community. Success to you and your community on your path of sustainability!"", 'Sustainable, Livable CitiesSupporting resilient, sustainable, and equitable cities through nature-based solutions\nTHE CHALLENGEThe Challenge: Around the world, city leaders are turning towards nature-based solutions to build cities that are more livable and resilient to climate and health risks. Globally, we need more practice and research on urban ecosystem services to highlight when and how urban nature can contribute to safer, more equitable cities. OUR SOLUTIONOur Solution: Our work aims to demonstrate the power and value of nature to manage urban challenges. We review existing knowledge, develop tools to quantify urban ecosystem services, and co-produce information highlighting how natural infrastructure benefits people in complex urban systems. OUTCOMESOutcomes: Acting locally, regionally, and globally, we are co-developing innovative approaches and policies that explore nature-based urban solutions and promote sustainable, livable cities. We are implementing pilot projects in the U.S. and in China and working with partners to learn about opportunities around the world (in particular in the Global South).\nBy 2050, there will be 9 billion people on Earth, and a staggering 75 percent of them will live in cities. Ongoing urbanization and rising global prosperity will combine to increase the size and density of the world’s cities, forcing municipal leaders to make hard choices in the funding and management of both built and natural infrastructure.\nClimate change and associated risks, together with health threats and economic insecurity, press the urban community to find innovative solutions to build livable and resilient cities. Heat waves are more frequent and intense. Sea levels are rising, and changes in precipitation patterns may increase the risk of coastal and riparian flooding or potentially overwhelm many urban stormwater systems. A changing climate may also alter the spread of mosquito-borne infectious diseases in urban areas, and drier climates may put other cities at risk of catastrophic wildfire.\nHumans have already made tremendous investments in the buildings, transportation, water and energy systems that sustain urban communities. However, growing cities need to plan for investing in and maintaining infrastructure at an unprecedented rate, while also meeting the mounting challenge of climate adaptation. NatCap works to fill the gaps in knowledge and contribute tools that leverage the full potential of nature in building resilient cities.\nSan Francisco Resiliency Planning\nTo increase the resilience of San Francisco Bay Area coastlines and communities to sea-level rise, we include natural infrastructure (native landscape, river systems, etc.) in climate adaptation planning. Working with the Bay Area Regional Council (BARC), Bay Area Conservation and Development Commission (BCDC), and San Mateo County, we implemented in-depth scoping with decision-makers to balance research and stakeholder needs, building on other efforts to improve coastal community resilience in the Bay Area. We have found that additional research on the co-benefits of nature-based solutions and specific urban ecosystem services, including but not limited to coastal protection, nature-based recreation, and stormwater management, have the potential to guide ongoing design and policy decisions.\n“Cities around the world need to know how to quantify the benefits that nature provides, so that they can choose to incorporate nature appropriately into their city, leading to a resilient, healthy, more verdant urban world”.\nGlobal Cities Lead, The Nature Conservancy\nDemonstrating the power—and value—of nature can help cities manage the challenges they face.\nNature offers its own infrastructure and can help cities mitigate these consequences, delivering vital services that are cost-effective and resilient to disruption.\nOur work within the Livable Cities program focuses on three areas:\nWe assess the distributional consequences of green infrastructure management (i.e. considering policies, institutions and diverse values) that affect vulnerable populations and marginalized communities.\nWe co-produce innovative approaches, policies and financial mechanisms with practitioners across local, regional, and global scales that explore nature-based urban solutions and promote sustainable, livable cities.\nA growing number of influential urban networks and partners—TNC’s Global Cities program, the Rockefeller Foundation’s 100 Resilient Cities or 100RC, ICLEI, C40, and others—have embraced the protection and restoration benefits of natural infrastructure as a means to promote more sustainable and livable cities. We are establishing partnerships to co-produce innovative approaches, policies and financial mechanisms with practitioners across local, regional, and global scales that explore nature-based urban solutions and promote sustainable, livable cities. Our pilot projects are in the U.S. and in China and we are working with partners to learn from other geographies (in particular in the Global South).\nNatural Capital of Golf Courses\nThe US loves golf. There are approximately 16,000 golf courses in the USA, spread across a variety of social and ecological contexts; urban to rural, forest to swamp, grassland to desert. Economically, golf courses contribute billions of dollars to the economy and create jobs for local communities. Environmentally, golf courses require inputs of water, nutrients and pesticides and can be taxing. While these immediate, direct costs and benefits are fairly well-understood, the indirect contribution of golf courses to landscape and the public value is not as well-understood: What are the ecosystem service values of golf courses to their surrounding community? How do the biophysical drivers of ecosystem services affect the golf experience? We launched a collaborative project to explore environmental and public values associated with golf courses in urban areas and explore these important questions further.\nUsing social media to understand drivers of urban park visitation in the Twin Cities, MN\nWe developed and tested an approach for using geotagged social media data from the websites Flickr and Twitter to look at park visitation in the Twin Cities, Minnesota, USA. Looking at what characteristics (i.e. water features, trail networks) drive patterns of visitation to a city’s green space can help inform local and regional park planners. We found that Twin Cities parks with nearby water features, more amenities, greater accessibility from trails, and located within more densely populated neighborhoods are associated with higher rates of visitation. This analysis demonstrated that using targeted information about user behavior and preferences available from social media data can rapidly assess park use at a lower cost than traditional surveys. It is a flexible tool and has the potential to inform public green space management in urban areas wherever social media data is widely available.\nAn Update on NatCap’s Livable Cities Program By Marie Donahue | October 2017 Photo credit: https://pixabay.com/ Supporting the development of livable cities has been a core component of NatCap’s strategic engagement and research in recent years, and for good reason....\nExploring the Natural Capital Value of Golf Courses By Marie Donahue | October 2017 Photo credit: https://pixabay.com/ If you randomly ask someone on a city street to describe the game of golf, odds are good that even someone unfamiliar with the nuances of the game...\nNotes from informal urban settlements in Fiji and Indonesia By Anne Guerry | October 2017 An informal settlement before (left) and after (right) proposed RISE project interventions that include toilets, communal septic tanks, and wetlands for flood reduction and...\nPlease contact Sustainable, Livable Cities Outcome Lead Perrine Hamel (email@example.com) for more information.\nA draft Working Vision for the Sustainable, Livable Cities Program (2017) is available for download.']"	['<urn:uuid:f92b7dd8-32b7-4364-b370-3fd55c86c344>', '<urn:uuid:9773d127-b7a8-48e7-a75d-99b0027cce31>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T22:03:18.033296	9	101	1681
16	describe major obstacles encountered during implementation human genome project financing	The Human Genome Project faced initial challenges with cost efficiency and clinical value alignment. In 1984, the initial discussions centered around the Department of Energy and estimated a cost of one dollar per base. However, there was disappointment about the inability to radically reduce prices and achieve clinically valuable outcomes. This led some early proponents to become conscientious objectors to their own project and instead focus on developing new sequencing technology throughout the project's duration.	['First published April 1, 2019 - More info\nJCI Editor at Large Ushma S. Neill interviews the geneticist, innovator, and entrepreneur George Church, of Harvard and MIT. Dr. Church developed the methods for the first genome sequencing, and his subsequent work has brought down the price of sequencing over 10-million-fold. His was one of the two research groups that first got CRISPR/Cas9 to work for precise gene editing in human cells by a homologous recombination, and he has been behind countless other scientific innovations and disruptions, specifically in the world of precision genome sequencing.\nThe geneticist, innovator, and entrepreneur, George Church of Harvard and MIT, developed the methods for the first genome sequencing and his subsequent work has brought down the price of sequencing over 10-million-fold. His was one of the two research groups that first got CRISPR-Cas9 to work for precise gene editing in human cells by a homologous recombination and he has been behind countless other scientific innovations and disruptions, specifically in the world of precision genome sequencing. To see Church (Figure 1) talk about the ethical implications of reversing aging, and how his first experiment presaged the movie Little Shop of Horrors, see the full interview on the JCI website at http://jci.org/videos/cgms\nGeorge Church. Image credit: Wyss Institute at Harvard University.\nJCI: What were you like as a kid?\nChurch: I had three fathers; the first one was in the Air Force and had a variety of jobs thereafter but was only with me for the first few months. I remained in Florida getting an early terrible education. That was a defining feature, but the result was that I got a lot of extracurricular experience. I loved nature. I spent a lot of time on the mudflats and swamps and with living creatures. My third father was a physician; I was heavily influenced by his medical bag that he carried around on house calls, which even then was quite an anachronism.\nI didn’t know any scientist or any engineers growing up. I didn’t have a science teacher until seventh grade and even then, she was part-time because the school didn’t want to make a commitment to such a radical topic. I left Florida to go to Massachusetts when I was 13 for boarding school at Phillips Academy. My third father had been there for two years. He didn’t particularly like it, but it was his gut feeling that I would. It was four years of heaven. It totally challenged me and stimulated me to study all kinds of things like art, photography, and athletics, but mostly science and math.\nOne of the first things I sought out in ninth grade was a computer, as Phillips had a connection with the Dartmouth computer time share, which nobody used. I found that it was in a basement with no chairs, no furniture, and I just started working on it.\nJCI: What led you to want to do undergraduate degrees in chemistry and zoology?\nChurch: I never wanted to specialize. As it turned out I was fairly strong in math and physics, but zoology and chemistry were what I ended up doing. In my spare time I did research in crystallography, which really did combine all of the fields in an obligatory way.\nI had been looking for a part-time job and finally one caught my eye. I walked in for the interview and here’s this assistant professor, Sung-Hou Kim, who is quite small in comparison to the model that he was building. He’s about half the size of this model for transfer RNA, which was done back then with wrenches and half-silvered mirrors and electron density maps. After figuring out that it required Fourier transforms and computers and deep knowledge of chemistry, and had implications for medicine, I thought it was the whole package. I was in love. I would have flunked out of undergrad but I managed to finish early; instead I flunked out of graduate school at Duke.\nJCI: How did you manage to then go to Harvard Graduate School for your PhD?\nChurch: It sounds mysterious at first how you can recover from a complete flunk. But I had published five papers while I was flunking out, and I had been accepted to Harvard Graduate School before, so they, perhaps, felt like I couldn’t have gone that much downhill overnight. They took a chance on me. They never said so, and they never acted like there was anything out of the ordinary. I buckled down, tried to become more mature overnight. I figured I’d better not flunk out twice.\nJCI: How did you end up in the Wally Gilbert lab doing research on genetics and genomics?\nChurch: As it turned out, Wally Gilbert, Mark Ptashne, Steve Harrison, Don Wiley, and others were all interested in crystallography. I was one of the few incoming students who had extreme experience in crystallography. But I sort of felt, “been there, done that.” I felt that every other field of biology, and chemistry to some extent, didn’t have what crystallography had — solid biophysics foundations, computers, or automation. I set out to take the lessons I learned from crystallography and apply them to adjacent fields: DNA, RNA, proteins, and eventually synthetic biology.\nMy PhD thesis was a hodgepodge of two things, one having to do with yeast and one having to do with mammalian immunoglobulin genes. I tried to stitch it together as “functions of introns.” Introns had just been discovered and everybody thought it was junk DNA. The part on immunoglobulins was not just that, it was also on a new sequencing method, what we call “genomic sequencing.”\nWally and many people in his lab were not just interested in sequencing but in applying sequencing to methylation and protein-DNA interactions. I did that, but on a genomic scale; it wasn’t just the scale, it was completely rethinking the way that we collected data. It was more of an emphasis on re-probing and imaging than on electrophoresis, which I think directly led me to what would become fluorescent reprobing-based imaging, which was the first of the “next-gen sequencing.”\nJCI: Did you think that your path was to be academic or entrepreneurial?\nChurch: I was not very thoughtful about my career at any point. Probably still not, but definitely not then. I was doing entrepreneurial things, before I knew of Biogen, which was Wally’s first company. I had written software in ‘78 to do automatic DNA sequencing. I actually did it in a crystallography lab using a new instrument that they were intimidated to set up. I said, “I’ll use my rotation to set it up for you, if I get to use it for something non-crystallographic for a few weeks.” I wrote the software and then I took it to Bio-Rad; the point is that I was a mixture of academic and entrepreneur. I wasn’t really concerned about what I was going to do long term.\nJCI: What made you decide to do a postdoc with Gail Martin, an embryonic stem cell pioneer?\nChurch: I had already been working with Gail, and had committed to doing a postdoc in her lab before I went to Biogen. Part of what I’d done in my thesis was studying how B cells develop different progenitor stages. I thought the ultimate progenitor stage would be embryonic. But I didn’t know how to get embryos. Gail provided the world’s first embryonic stem cells (called PSA-1).\nI started thinking about homologous recombination, but didn’t get to it [during my postdoc], and I thought it would be interesting research to do with Gail. Another motivation was that my girlfriend had applied to a lab at Stanford. She eventually became my wife and now we’ve been together for 40 years and have two grandchildren.\nJCI: How did you end up back at Harvard for your first assistant professorship?\nChurch: The same woman, Ting Wu, decided about four months in that she didn’t like her project at Stanford, and so she started her own institute, got her own grants, and was acting like an assistant professor back on the East Coast. I cleaned up my loose ends as fast as I could and cut my postdoc short. I was very lucky that Harvard would take me with such a pathetic postdoctoral career. I basically had done nothing post-Biogen that was publishable, and my work was all over the map, from embryonic stem cells to crystallography to a half-baked Genome Project. I have to say Harvard continually picked up the pieces of my failed career and did it again when I came up for tenure. At least three times they’ve saved my bacon when I was not doing such a good job of saving myself.\nJCI: You were one of the few people instrumental in getting the Human Genome Project started, but it’s been noted that you didn’t think enough was accomplished with the initial $3B investment.\nChurch: Oh yes. I was in the first discussions in 1984, which were Department of Energy–centric. We thought we could do it for a dollar per base, but I was immediately disappointed because I wanted to bring the price down radically and to do something that would be clinically valuable. I thought the goals were misaligned, so I became a conscientious objector of my own project and spent the whole time during the genome project developing new sequencing technology.\nJCI: Do you think every human should have their genome sequenced?\nChurch: I don’t know if they should, but I think they should have the opportunity. What we need is to educate people about the value and now we also have privacy tools that can make it so that there’s essentially no risk, and wherein you retain control of your information.\nJCI: Has one of the main stumbling blocks been the privacy issues? You have famously put all of your medical records and your genome in the public domain. Has anyone tried to steal your identity?\nChurch: There haven’t been any negative consequences of it — but that’s not an argument that it is safe. I didn’t do it to show that, “Hey, join me with the killer whales, the water’s safe!” We need both the highly private version, and a highly public version and we only need a few volunteers for the highly public version, because there’s a lot of things you can do with a small number of people.\nJCI: Synthetic biology is another main thrust in your lab. Some of the many applications in your lab have been gene drives in mosquitoes to eradicate malaria, to using CRISPR to eliminate 62 different porcine endogenous retrovirus genes at once toward the goal of using pig organs for human transplantation. And then, one that has captured the public’s attention: using CRISPR to transplant woolly mammoth genes into African elephants. Some of this sounds like science fiction.\nChurch: Nearly everything we touch seems to go swiftly from science fiction to normal. Millions of families each year use noninvasive prenatal testing, which is a consequence of next-gen sequencing. With synthetic biology, there’s a growing number of green chemistry applications, from flavors, fragrances, and complex polymers that are made biologically rather than with petroleum or high temperatures; this list is just going to expand. I have speculated that essentially everything that we can currently manufacture today without biology, we will be able to manufacture with biology and with potential advantages; biology is intrinsically atomically precise and it’s scalable to cover the whole planet essentially for free. I think that we’re in a truly revolutionary phase and that we need to be very thoughtful.\nJCI: Including using CRISPR on humans?\nChurch: We are already using CRISPR and other gene therapies on humans. Not only ongoing human clinical trials, there are now three approved gene therapies — none of them are CRISPR — but CRISPR is approved for clinical trials. I think what you’re alluding to is this: Is there a barrier between doing it in adults, children, fetuses, embryos without germline, embryos with germline, and so forth? These are under discussion, and now even the most off-the-table things have been tested. We’ve got reports of babies with an enhanced germline. The experiment has been done, but there are many more that could be done. It’s a matter of having that discussion and seeing whether the benefits outweigh the risks.\nJCI: What do you think the next 10 years holds for your research?\nChurch: I am addicted to technology development. We’re getting more into biomanufacturing things containing inorganic materials, like computers. I’m interested in space genetics and sending objects or entities to Alpha Centauri that can radio back...the biological engineering of such objects. I’m interested in biomanufacturing the moral equivalent of electronics, which might be actual, fully biological brains that can compete with artificial intelligence. I’m also still doing some of the same things I was doing as a teenager, like crystallography and transfer RNA — still two of my favorite things.\nJCI: If you could not be a scientist, what do you think you would have taken up?\nChurch: The cheat answer is engineering. In fact, it took me years to realize I was actually more of an engineer than I was a scientist. But artist would be the next choice. I think that that’s also a cheat because “practical arts” was a term for engineering at one point. My first semester in college, I was a cinematography major, and when I first started looking for a lab, one of the things I liked about the crystallography lab is that they actually had a scanner where you could scan in photographs. I had been looking for such a scanner for years because I thought, “Wouldn’t it be cool if you could scan a photograph and manipulate it on a computer?” And just about everybody asked, “Why would you want to do that?”\nCopyright: © 2019 American Society for Clinical Investigation\nReference information: J Clin Invest. 2019;129(4):1403–1404. https://doi.org/10.1172/JCI128550.']	['<urn:uuid:9fa15aa7-5456-462d-94c1-a1ae8388d332>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T22:03:18.033296	10	75	2330
17	power system flight duration specs	The aircraft can achieve up to 55 minutes of flight time with TB60 smart batteries that support hot-swapping functionality. For comparison, the M200 can fly up to 35 minutes with its dual-battery power system that includes automatic battery heating for sub-zero temperatures. The M600 Pro uses 6 intelligent LiPo 6S batteries, offering options of TB46S, TB47S (4500 mAh), or TB48S (5700 mAh) batteries. The battery system includes a UAV Health Management System that monitors status and provides maintenance guidance.	"['Which DJI drone for LiDAR mapping?\nDA Jiang Innovation, more known under DJI name is the number 1 drones manufacturer in the world. DJI drones are drones you must consider for professional applications, using photogrammetry or LiDAR.\nThe DJI Matrice 600 is definitely the most widespread UAV on the professional market. This 9kg UAV is an hexacopter multirotor, equipped with 6 intelligent LiPo 6S 4500 mAh batteries. These batteries can be either TB46S, TB47S or better, TB48S batteries.\nOne of the reason why DJI M600 Pro is often the right solution for professional LiDAR surveys is because of its payload capacity. This drone can lift up to 6kg: DJI payloads such as Ronin (3-axis stabilization gimbal that can hold a wide variety of cameras for filmmaking professional), Zenmuse cameras or specific payloads provided mounting is compatible (thermal cameras, LiDAR systems….). It can carry cameras such as Sony A-6000, Sony A7R and LiDAR systems at the same time (LiDAR + dual camera setup).\nDJI M600 pro is equipped with a dust-resistant propulsion system and self-cooling motors that ensure very good reliability for long duration data acquisitions.\nDJI M600 has 2 D-RTK GNSS antennas receiving the GPS L1/L2 and GLONASS F1/F2 constellations, which are resistant to strong magnetic interference and allow centimeter-level positioning.\nYou can fly DJI M600 UAV with any YellowScan UAV LiDAR mapping systems (Mapper, Surveyor and Vx series).\nOur recommended configuration for DJI Matrice 600 Pro:\n- By default DJI M600 Pro is shipped with 1 set of TB47S 4500 mAh batteries (6 batteries). We recommend to have at least 1 extra set of batteries, and more precisely a set of TB48S 5700mAh batteries (with better capacity than TB47S).\n- Transportation Case\n- Galaxy GRS GBS multirotor UAV parachute with :\n- 868MHz Opale radiocommand\n- Galaxy GRS 107dB Buzzer\n- Stop-engine module Galaxy GRS\n- Step Down 5v Pololu 2.5A Regulator\n- 6x male-male servo cable 30cm\n- UgCS flight planner with Android tablet and Windows computer\nDJI M200 / DJI M210\nDJI M200 is the first-ever enterprise drone launched by DJI in 2017 Q1. It’s been designed to replace DJI M600 for industrial application not needing big-sized UAV like M600.\nMore compact than M600, DJI M200 is to be used for aerial inspections of roads, railroads, power turbines, bridges as well as construction site mapping.\nThat’s why there is a broader selection of camera for mapping and inspection (2 cameras possible on the M210 variation).\nDJI M200 UAV can fly for up to 35 minutes with his new dual-battery power system that automatically heats batteries when flying in sub-zero temperatures. You have the choice to fly with TB50 batteries or larger TB55 batteries.\nThis UAV is OK for photogrammetry and inspection. We do have a few customers using it with light LiDAR (below 2kg LiDAR such as YellowScan Surveyor) but they know their UAV limits: short landing gear (meaning LiDAR closer to the ground than on M600) and small autonomy with LiDAR payload. We do not recommend using DJI M200/M210 for LiDAR surveys with YellowScan Surveyor and Vx series but DJI M600.\nDJI M300 RTK\nNewly launched this year (2020 Q2), DJI M300 is presented as a game changer. This new UAV from DJI is replacing the DJI M200/DJI M210 series with better performance for industrial applications. It can fly up to 55 minutes with its TB60 smart batteries (allowing hot swaps) and has a max payload of 2.7 kg.\nIt can support up to 3 payloads simultaneously. Payload integration with this new DJI drone is made easy as you can use either DJI Skyport v2 or DJI X-Port to attach any sensor.\nDJI M300 offers 6 Directional Sensing & Positioning and is sold with the Zenmuse H20 wide-angle camera.\nDJI Matrice 300 is a good option for photogrammetry and LiDAR mapping. You need to have very light and compact LiDAR system (under 1.5kg) to fully benefit from its features, like the new DJI’s Livox-based UAV LiDAR introduced on the market. This drone looks good, reliable and robust for LiDAR mapping.\nDid you know? There are different scan patterns of LiDAR systems depending on the scanning method used. You may know how a LiDAR...\nWEBINAR Presentation of the YellowScan Mapper and Vx in Japanese.Our Japanese users you will have the opportunity to see our Vx series and our new...\nDid you know? It is not recommended to use LiDAR in rainy or foggy weather. In contact with water, light deviates and so does the laser of a...', ""- 15 km Max Transmission1\n- 55-min Max Flight Time2\n- 6 Directional Sensing & Positioning\n- Primary Flight Display\n- IP45 Rating\n- -20°C to 50°C Operating Temperature\n- Hot-swappable Battery\n- UAV Health Management System\nThe all-new OcuSync Enterprise enables transmission up to 15 km away and supports triple-channel3 1080p video. Real-time auto-switching between 2.4 GHz and 5.8 GHz4 enables more reliable flight near high-interference environments, while AES-256 encryption offers secure data transmission.\nThe refined airframe and propulsion system design gives you a more efficient and stable flight, even in harsh conditions.\nMax Flight Time\nMax Descend Speed5\nConfigure your M300 RTK to fit your mission needs. Mount up to 3 payloads simultaneously, with a maximum payload capacity of 2.7 kg.\nRecord mission actions such as aircraft movement, gimbal orientation, photo shooting, and zoom level to create sample mission files for future automated inspections.\nAutomate routine inspections and capture consistent results every time. Onboard AI recognizes the subject of interest and identifies it in subsequent automated missions to ensure consistent framing.\nCreate up to 65,535 waypoints and set multiple actions for one or more payloads, including 3rd party ones, at each waypoint. Flightpath planning is also optimized to maximize flexibility and efficiency for your missions.\nMark an object in camera or map view with a quick tap, and advanced sensor fusion algorithms will immediately calculate its coordinates, which are projected to all camera views as an AR icon. The location of the subject is automatically shared with another remote controller, or to online platforms such as DJI FlightHub9.\nIdentify and follow moving subjects like people, vehicles, and boats with the Smart Track function, where auto-zoom is applied for steady tracking and viewing. The subject’s dynamic location is continuously acquired and shared to another remote controller or to DJI FlightHub9.\nThe M300 RTK adopts a new Primary Flight Display (PFD) that integrates flight, navigation, and obstacle information to empower the pilot with exceptional situational awareness.\nEither operator can now obtain control of the aircraft or payload with a single tap. This creates new possibilities for mission strategies as well as higher flexibility during operations.\nTo enhance in-flight safety and aircraft stability, dual-vision and ToF sensors appear on all six sides of the aircraft, offering a maximum detection range of up to 40 m, with options to customize the aircraft’s sensing behavior via the DJI Pilot App. Even in complex operating environments, this 6 Directional Sensing and Positioning system helps keep the aircraft and the mission safe.\nThe new integrated Health Management System displays the current status of all systems, notification logs, and a preliminary troubleshooting guide. Also in the system are the aircraft’s flight logs, duration, and mileage throughout its entire lifecycle, and tips on aircraft care and maintenance.\nThe M300 RTK’s built-in advanced redundancy systems help keep your critical missions going even in unexpected scenarios.\n-20°C to 50°C\nAirSense ADS-B Receiver\nThe Zenmuse H20N integrates starlight sensors into its zoom and wide-angle cameras\nEstimate your M300 RTK’s flight time based on the payload configuration.\nIntegrate a variety of 3rd party payloads like gas detectors, loudspeakers, multispectral sensors, and more. Payload SDK supports DJI SkyPort, DJI SkyPort V2, and DJI X-Port. These greatly reduce the payload development lifecycle and maximize the potential of your payloads in more diverse scenarios.\nWith a large network of 3rd party mobile applications, you can unlock the capabilities of your drone platform to meet specialized mission needs. Utilizing Mobile SDK, the M300 RTK supports highly customizable mobile app development.\nWith Pilot 2's built-in MQTT based protocols in DJI Cloud API, you can directly connect the Matrice 300 RTK to Third-Party cloud platforms without having to develop an App. Access the drone's hardware, video live-stream, and photo data.\nQuickly assess a situation and plan accordingly while improving officer and bystander safety.\nConduct inspections of pipelines, well sites, and more while keeping workers away from risky areas.\nDigitize surveying workflows and improve efficiency in data collection and analysis.\nMatrice 300 RTK is available for order through official DJI Dealers. Contact us below for the team to reach out.\n1.Unobstructed, free of interference, when FCC compliant. Maximum flight range specification is a proxy for radio link strength and resilience. Always fly your drone within visual line of sight unless otherwise permitted.\n2. Actual flight time may vary because of the environment and payload configurations.\n3. Each RC supports two streams. Triple-stream channeling is only supported with dual RC.\n4. Due to local policies, some countries do not support 5.8 GHz transmission.\n5. Achieved in Forward Flight using S Mode.\n6. The service ceiling of 7000 m is achievable with high altitude propellers.\n7. This feature is only supported when the aircraft is paired with the Zenmuse H20 Series payloads.\n8. This feature is only supported when the aircraft is paired with the Zenmuse H20 Series payloads.\n9. Support for location sharing via DJI FlightHub is coming soon.\n10. D-RTK 2 Mobile Station for Matrice 200 Series V2 and Phantom 4 RTK can be upgraded to support Matrice 300 RTK.\n11. The CSM Radar will be available soon.""]"	['<urn:uuid:92e190f7-e8c6-4c65-9b51-909ed2f1a3c0>', '<urn:uuid:469779b3-9781-4cd5-be93-41108b4c32f7>']	open-ended	direct	short-search-query	distant-from-document	three-doc	expert	2025-05-12T22:03:18.033296	5	79	1600
18	what steps did vertical bridge take to achieve carbon neutrality tower company	Vertical Bridge took several steps to achieve carbon neutrality. First, they upgraded old generators and HVAC units to be more fuel efficient about four years ago. They installed LED lighting to reduce energy consumption for safety lights. They then engaged Capital Natural Partners to help chart their course to carbon neutrality. They conducted detailed analysis of their business operations and emissions sources using their financial records and operational data. Finally, they supported four carbon offset projects and began working with contractors and suppliers to further reduce emissions.	['Thank you to our speakers:\n– Bernard Borghei, Co-Founder and EVP, Operations, Vertical Bridge\n– Manjit Dhillon, Interim CFO and Head of Investor Relations and Corporate Finance, Helios Towers\n– Will Stocker, Head of Performance Engineering, Helios Towers\n– Wole Abu, CEO, Pan African Towers\n– Tuoyo Ebigbeyi, CTO, American Tower (Africa)\nGreening the Network is the new series by TowerXchange focusing on energy efficiency, renewable investment and network resilience. In this summary of the July’s live session of Greening the Network, we hear from Bernard Borghei, Co-Founder and EVP, Operations for Vertical Bridge, Manjit Dhillon, Interim CFO and Will Stocker, Head of Performance Engineering at Helios Towers, Wole Abu, CEO of Pan African Towers and Tuoyo Ebigbeyi, CTO of American Tower in Africa.\nWe began by discussing the scale and source of our emissions. American Tower (Africa) has around 20,000 towerco sites in seven countries. 99% of American Tower’s GHG emissions pertain to powering tenant equipment. In large part in Africa that power comes from its 15-16,000 diesel generators. American Tower has been investing in new technologies to reduce its carbon footprint, in EMEA over the last few years American Tower has spent US$100mn on storage and solar solutions which has reduced its diesel consumption by 100mn litres – that’s the equivalent of taking 13,000 cars off the road.\nHelios Towers own and operate just under 7,000 sites in five different markets. Similar to American Tower, grid availability is not nearly good enough to rely on in their markets. Whereas in the west we are used to the grid being the primary power, for Helios grid power is a back-up and diesel generators take the strain. 80% their of sites are on grid, and typically African grid power is clean because of its reliance of hydropower, but Helios Towers are lucky to get half a day of grid power to their sites. Of their 7,000 sites, 4-500 now have solar installed, and a total of 7-800 sites are now hybridised to include battery storage which allows the cycling of different energy sources.\nPan African Towers is an independent towerco based in Nigeria with roughly 1,000 tenanted towers. Its emissions are primarily Scope 1 emissions from the burning of diesel in their generators. Pan African Tower’s network is effectively completely off grid because the grid in Nigeria is very poor. Towers have dual generators installed running 24/7/365. A solar hybrid site can be a third cheaper in terms of total cost of ownership than a site powered by diesel. Pan African Towers is also raising equity to expand, and ESG considerations have come to the fore with their investors. For Pan African Towers therefore, the story of sustainability is also the story of economics.\nIn the future rollout will be completely based on renewables, and old sites are being retrofitted by a new ESCO partner. So far 10% of sites have been converted into renewable sites (with around 3-4 hours a day of diesel generator runtime overnight). Progress would have been quicker this year, but with the disruption to supply chains due to COVID-19, Pan African will be happy to reach 25% solarised by the end of the year.\nVertical Bridge are the largest private REIT in the United States with a portfolio of 270,000 assets of varying sizes. Vertical Bridge are the only towerco who are able to boast of carbon neutrality, and it is to this achievement our panel turned first.\nFigure 1: How to talk about emissions\nGoing carbon zero\nBernard Borghei described the move to carbon zero as a natural evolution for Vertical Bridge, but that doesn’t mean it was easy or did not require a lot of preparation. Vertical Bridge governs itself according to the three Ps: Profit, People and Planet. They do not view these three as mutually exclusive and with the profitability and people already covered through their normal operations and philanthropy, Vertical Bridge felt the time was right to commit to long-term carbon neutrality. Of course, Vertical Bridge is not a power-as-a-service towerco as our other towerco panellists are, but its process offers lessons for the whole industry.\nAround four years ago Vertical Bridge began upgrading its old generators and HVAC units to be more fuel efficient. Vertical Bridge owns 1,600 broadcast towers and those sites come with huge generators and HVACs. The move to energy efficiency also incorporated installation of LED lighting to reduce energy consumption for its safety lights. Importantly, all these investments were good for the environment and for the pocket book. By Q3 last year Vertical Bridge had already heavily shrunk its emissions and needed a plan to go further. That is when they decided it was time to aim for carbon neutrality, and when they engaged Capital Natural Partners to help them chart a course.\nFor over a month Vertical Bridge discussed the operations of their business with their consultants, so they could fully understand the business and capture exactly where the emissions footprint was being generated. Once an outline of the business was created then Borghei was tasked with finding detailed information on the firm’s activities.\nWithout effective book keeping, this would have been much harder. But because Vertical Bridge follows the best accounting practices, the data required was already logged in its financial system under various GL codes. By using the financial system to pull information it was relatively straightforward to identify the types of emissions associated with various business activities.\nBut financial records are not enough. On top of the nature of the towerco’s activities, it is also necessary to work out how those activities are carried out. For example, the distance driven by your staff and the cars they drive; the flights taken and whether they flew coach, business or private jet. Site operations information is also incredibly important with generator runtime a critical statistic. Once collated, a first round of analysis is possible before revisions and queries need to be resolved.\nTo eliminate any residual emissions, Vertical Bridge are supporting four carbon offset projects, but they are also working with their contractors and suppliers to further reduce emissions in the future. Once you go carbon neutral you make a pledge to stay that way for the long run. Vertical Bridge is at the start of its carbon neutral journey, and it hopes other towercos will join it.\nInvestor pressure to go green\nSince going public, Helios Towers has found more investors focusing on these issues, but they have always had socially conscious equity investors supporting them like the IFC and Albright Capital Management, and development funds like the EAIF and DEG providing debt. Manjit Dhillon, Interim CFO and Head of Investor Relations at Helios Towers, discussed the changing attitudes of investors towards the environmental impact of towercos, and the wider social impact of towercos.\nAt the time of going public Helios Towers worked on an ESG paper to highlight its ESG profile and set out how it could improve its work further, a process which is continuing today. Decarbonisation is one element, but investors are interested in seeing businesses become more sustainable overall. Barclays’ research has identified three key areas for telecoms to improve its social impact. The first is enabling faster connectivity, the second is creating a more diversity workforce, internally and externally and the third is environmental issues.\nSo in addition to Helios Towers’ work greening the network, it is also focusing on other areas. Since 2015 they have been making sure the regional teams are operating with a very local presence. 96% of Helios Towers’ workforce in Africa is local; the CEOs for the Congo is Congolese, for Ghana is Ghanaian and for Tanzania is Tanzanian.\nBecoming greener is possible in Africa, but becoming carbon neutral is a long way away because of the paucity of grid connections in much of the continent – although Wole Abu of Pan African Towers’ expressed an ambition to move to carbon neutrality in Nigeria and leave diesel power behind entirely. In Africa Scope 2 emissions are difficult to manage as unlike developed markets there is usually one source of grid availability, and power purchase agreements with renewable suppliers are not possible. This requires an understanding from investors that towercos can and will green their networks, but there are hard limits on what is possible.\nDhillon also highlighted the sector’s Scope 4 emissions. If Scope 1, 2 and 3 emissions refer to direct and indirect emissions produced by a business, Scope 4 emissions are the emissions which are avoided thanks to that business’s work. Towercos help the world avoid a lot of emissions through infrastructure sharing, co-location and investments in green generation.\nFigure 2: Who has joined Greening the Network\nEfficiency and emissions reductions\nSite availability remains the key metric by which towercos are measured. If you are providing power-as-a-service then your number one priority is to keep that site live and equipment connected. We asked our panellists if there was a trade-off between meeting availability targets and investing in a green network.\nPartly by definition the co-location of telecom equipment combines efficiency and emissions reduction, even if you are merely using one diesel generator to power the base stations of two tenants, these are the Scope 4 emissions mentioned earlier. You can maximise the efficiency with which you use steel in the air and genset runtime if your towers host more than one tenant. This means that compared to operator-owned networks, towercos are already combining green networks with efficiency.\nIn Tanzania Helios Towers acquired two nationwide networks. Now through synergies in those networks, it has been able to consolidate 106 towers. This reduction in tower numbers was also a reduction in generator run time, diesel consumption, power used and miles driven.\nAmerican Tower is committed to greening the network. They are now 59% of the way toward fulfilling their global goal of 20MW of installed renewable capacity by the end of 2022. That means they are nearly two thirds of the way to adding solar assist to 5,000 sites.\nBut there are limits to how much solar can be installed due to space constraints. Towercos are acquisitive and legacy sites are often not sized to include a solar array. Future sites may be acquired with the correct space for solar, but unless changes to design make solar arrays much smaller, or easily mountable on the tower, there will be a limit to rollout. For example, In India some towercos are approaching 10% of sites solarised, but are struggling to increase this ratio due to space constraints.\nBefore solarising a site there are a couple of considerations. Commercial viability comes first and thanks to huge technological advancements the cost per kilowatt hours from solar is now competitive with alternatives all over Africa. There has been less advances in wind on a distributed scale, and fuel cells lack the distribution and maintenance networks of diesel or solar.\nA second factor is viability at site. Even where a solar or alternative energy site might be viable you may lack the space at urban sites, or the supporting infrastructure for rural sites. These need to be factors considered to make sure the technology works It is an ongoing process, and our towerco panellists agreed that the ambition and aggression is there to see this through where possible.\nPan African Towers like to say they enjoy a last mover advantage. They see the mistakes and missteps of their forebears before they were established in 2018, and they apply the same strategy to their technology adoption. Since towercos began deploying solar and battery technology in Nigeria the technology has improved, and Pan African is ready to take advantage. They recognise the difference between efficiency and greening.\nThe cost per kilowatt hour is at least 20-30% cheaper than diesel in terms of both overall capex replacement cost and opex. All Pan African’s new sites will be fully designed as a hybrid site. This will reduce their Scope 1 emissions significantly. It also reduces their Scope 2 and Scope 3 emissions by reducing site visits.\nOver the next 24-36 months they will also be renovating their existing sites, moving sites from reliance on diesel to solar first designs with an ESCO partner. They are using solar because it is hard to pioneer new technologies in Nigeria due to the fact that the country lacks the ecosystem to support new technology. State-of-the-art equipment is fantastic until something goes wrong and there’s not the technical skills locally to repair your equipment. This leads to a high Mean Time To Repair (MTTR) which forces players to go back to utilising diesel generators, which is the precise outcome they are seeking to avoid. Reliability and resilience are going hand in hand with cost reduction.\nAmerican Tower has tried to standardise the power systems used for its operations, to optimise supply chain and spares management. Reducing equipment variety reduces difficulties with maintenance and upgrading, whether it comes to keeping an inventory of spare parts, or managing firmware updates. However, American Tower reviews new technologies frequently.\nReviews begin on the technical side to make sure suppliers’ processes match American Tower’s own processes. Once you pass those technical gates you move to the CTO’s team. They will look at how your units work in a lab, but they will also want to see how it works in the field. If your technology works well in Burkina Faso then you might be on to something. Once you pass through all these gates American Tower will want you to standardise your solution for its opcos and provide a scale discount.\nHelios Towers has a set of twelve key deliveries partners, which they have reduced from an original 60, as well as a broader list of other technology suppliers, but their introductory process is very similar to that of American Tower. Helios Towers’ current innovation hub focus is the site performance analysis team. At the moment the communication and analysis of data from sites is quite manual.\nTowercos are looking at new technology all the time. American Tower is looking at fuel cells, wind and other technologies at present. Storage tech is also very important, with various lithium-ion batteries under review, as well as other chemistries like Vanadium Redox because it is so resilient.\nWind as a power source has been under review for some time. There are questions of space and regulation of additional towers for turbines at telecoms sites. There’s the question of whether to attach wind turbines to existing sites and protect their structural integrity and shareability.\nAmerican Tower are also exploring bleeding edge technology by supporting the MIT and Princeton. American Tower research projects are looking at intercellular structure of solar panels. The towerco is also partnering with remote monitoring solution research institutions to help examine how storage systems operate.\nVertical Bridge likened their move to carbon neutrality and growing awareness of environmentalism to the changes that took place in the last decade in workplace safety – a necessary step to further evolve towards a mature industry. The momentum behind hybridisation, solarisation and energy efficiency is unstoppable. Towercos are looking for partnerships with their suppliers as they want to be ready to move with technology as it develops. 5G, small cells, mini-macros, densification and more are going to reshape networks and towercos need to understand these technologies as they are developed to ensure they are deployed in an efficient and green way.']	['<urn:uuid:040b7771-e203-4e9e-87ff-aa4bbdd71d49>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T22:03:18.033296	12	87	2566
19	Hey, I'm curious about the history of computer systems. When was Unix created and who were the people behind it?	Unix was originally developed in 1969 by a group of AT&T employees at Bell Labs, including Ken Thompson, Dennis Ritchie, Douglas McIlroy, and Joe Ossanna. Today, there are various Unix variants available in the market, including Solaris Unix, AIX, HP Unix, and BSD. Linux is also a freely available flavor of Unix.	"['Unix - Getting Started\nWhat is Unix ?\nThe UNIX operating system is a set of programs that act as a link between the computer and the user.\nThe computer programs that allocate the system resources and coordinate all the details of the computer\'s internals is called the operating system or kernel.\nUsers communicate with the kernel through a program known as the shell. The shell is a command line interpreter; it translates commands entered by the user and converts them into a language that is understood by the kernel.\nUnix was originally developed in 1969 by a group of AT&T employees at Bell Labs, including Ken Thompson, Dennis Ritchie, Douglas McIlroy, and Joe Ossanna.\nThere are various Unix variants available in the market. Solaris Unix, AIX, HP Unix and BSD are few examples. Linux is also a flavor of Unix which is freely available.\nSeveral people can use a UNIX computer at the same time; hence UNIX is called a multiuser system.\nA user can also run multiple programs at the same time; hence UNIX is called multitasking.\nHere is a basic block diagram of a UNIX system −\nThe main concept that unites all versions of UNIX is the following four basics −\nKernel: The kernel is the heart of the operating system. It interacts with hardware and most of the tasks like memory management, task scheduling and file management.\nShell: The shell is the utility that processes your requests. When you type in a command at your terminal, the shell interprets the command and calls the program that you want. The shell uses standard syntax for all commands. C Shell, Bourne Shell and Korn Shell are most famous shells which are available with most of the Unix variants.\nCommands and Utilities: There are various command and utilities which you would use in your day to day activities. cp, mv, cat and grep etc. are few examples of commands and utilities. There are over 250 standard commands plus numerous others provided through 3rd party software. All the commands come along with various optional options.\nFiles and Directories: All data in UNIX is organized into files. All files are organized into directories. These directories are organized into a tree-like structure called the filesystem.\nIf you have a computer which has UNIX operating system installed on it, then you simply need to turn on its power to make it live.\nAs soon as you turn on the power, system starts booting up and finally it prompts you to log into the system, which is an activity to log into the system and use it for your day to day activities.\nWhen you first connect to a UNIX system, you usually see a prompt such as the following −\nTo log in\nHave your userid (user identification) and password ready. Contact your system administrator if you don\'t have these yet.\nType your userid at the login prompt, then press ENTER. Your userid is case-sensitive, so be sure you type it exactly as your system administrator instructed.\nType your password at the password prompt, then press ENTER. Your password is also case-sensitive.\nIf you provided correct userid and password then you would be allowed to enter into the system. Read the information and messages that come up on the screen something as below.\nlogin : amrood amrood\'s password: Last login: Sun Jun 14 09:32:32 2009 from 126.96.36.199 $\nYou would be provided with a command prompt ( sometime called $ prompt ) where you would type your all the commands. For example to check calendar you need to type cal command as follows −\n$ cal June 2009 Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 $\nAll Unix systems require passwords to help ensure that your files and data remain your own and that the system itself is secure from hackers and crackers. Here are the steps to change your password −\nTo start, type passwd at command prompt as shown below.\nEnter your old password the one you\'re currently using.\nType in your new password. Always keep your password complex enough so that no body can guess it. But make sure, you remember it.\nYou would need to verify the password by typing it again.\n$ passwd Changing password for amrood (current) Unix password:****** New UNIX password:******* Retype new UNIX password:******* passwd: all authentication tokens updated successfully $\nNote − I have put stars (*) just to show you the location where you would need to enter the current and new passwords otherwise at your system, it would not show you any character when you would type.\nListing Directories and Files\nAll data in UNIX is organized into files. All files are organized into directories. These directories are organized into a tree-like structure called the filesystem.\nYou can use ls command to list out all the files or directories available in a directory. Following is the example of using ls command with -l option.\n$ ls -l total 19621 drwxrwxr-x 2 amrood amrood 4096 Dec 25 09:59 uml -rw-rw-r-- 1 amrood amrood 5341 Dec 25 08:38 uml.jpg drwxr-xr-x 2 amrood amrood 4096 Feb 15 2006 univ drwxr-xr-x 2 root root 4096 Dec 9 2007 urlspedia -rw-r--r-- 1 root root 276480 Dec 9 2007 urlspedia.tar drwxr-xr-x 8 root root 4096 Nov 25 2007 usr -rwxr-xr-x 1 root root 3192 Nov 25 2007 webthumb.php -rw-rw-r-- 1 amrood amrood 20480 Nov 25 2007 webthumb.tar -rw-rw-r-- 1 amrood amrood 5654 Aug 9 2007 yourfile.mid -rw-rw-r-- 1 amrood amrood 166255 Aug 9 2007 yourfile.swf $\nHere enteries starting with d..... represent directories. For example uml, univ and urlspedia are directories and rest of the enteries are files.\nWho Are You?\nWhile you\'re logged in to the system, you might be willing to know : Who am I?\nThe easiest way to find out ""who you are"" is to enter the whoami command −\n$ whoami amrood $\nTry it on your system. This command lists the account name associated with the current login. You can try who am i command as well to get information about yourself.\nWho is Logged In?\nSometime you might be interested to know who is logged in to the computer at the same time.\nThere are three commands are available to get you this information, based on how much you\'d like to learn about the other users: users, who, and w.\n$ users amrood bablu qadir $ who amrood ttyp0 Oct 8 14:10 (limbo) bablu ttyp2 Oct 4 09:08 (calliope) qadir ttyp4 Oct 8 12:09 (dent) $\nTry w command on your system to check the output. This would list down few more information associated with the users logged in the system.\nWhen you finish your session, you need to log out of the system to ensure that nobody else accesses your files while masquerading as you.\nTo log out\nJust type logout command at command prompt, and the system will clean up everything and break the connection\nThe most consistent way to shut down a Unix system properly via the command line is to use one of the following commands −\n|halt||Brings the system down immediately.|\n|init 0||Powers off the system using predefined scripts to synchronize and clean up the system prior to shutdown|\n|init 6||Reboots the system by shutting it down completely and then bringing it completely back up|\n|poweroff||Shuts down the system by powering off.|\n|reboot||Reboots the system.|\n|shutdown||Shuts down the system.|\nYou typically need to be the superuser or root (the most privileged account on a Unix system) to shut down the system, but on some standalone or personally owned Unix boxes, an administrative user and sometimes regular users can do so.']"	['<urn:uuid:2a715375-bd86-4105-8823-59e517c40141>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T22:03:18.033296	20	52	1305
20	Where should the fuse be placed on a car amp?	The Inline fuse should be fitted as close to the battery as possible and the rating (amperage) should be the same or higher than the total on board fusing of your Amplifier. This fuse is to protect the power cable from grounding and potentially starting a fire.	"[""To wire 2 x 4 Ohm speakers in Parallel to present a 2 Ohm load to the MS-AM702 Amplifier per channel. Simply follow the below diagram showing the Positive terminals from each speaker connected to each other and the same with the Negative terminals. Then connect the Positive and Negitive wires from the closest speaker to the Stereo.\nThe simple answer to this question is higher output power (more Watts) and reduced current draw (play your music for longer periods). To give you a comparison between Class AB and D class, the normal output of a standard Head Unit ( class AB) is say 40-50 watts Peak per channel. The Fusion 600 Series Head Units (MS-CD/IP/AV600 D Class) have a rating of 70 Watts per channel and due to their efficiency consume less power per Watt of output. In essence you can go louder for longer. The other major benefit is that the D Class amp in the 500 and 600 series is that it's 2 Ohm stable (stereo).What this means is that you can conect 2 x 4 Ohm speakers per channel in parallel wiring configuration. This produces more power but is divided between the 2 speakers. Eg. at 4 Ohms per channel ouput is 26 watts RMS and at 2 Ohms it is 43 watts RMS, so divided by two is 21.5 watts RMS per speaker.\nThe theoretical efficiency of class AB is about 75% as well with a real efficiency between 50%-70%; the theoretical efficiency of class D is about 100% with a real efficiency of more than 85%.The Fusion D Class Marine Zone amp MS-AM702 is a perfect example of a compact high output solution for the boating enthusiast that loves their music. loud\nSome Mono block (single channel) amplifiers have 4 speaker output terminals as shown below. These are internally linked Positive together and Negative together, this is to make connection to multi coil Subwoofers easier when you configure in parallel. You can connect each coil directly to the terminals (one per side) rather than at the subwoofer.\nThe Inline fuse should be fitted as close to the battery as possible and the rating (amperage) the same or higher than the total on board fusing of your Amplifier. This fuse is to protect the power cable from grounding and potentially starting a fire.\nMy speakers are 300 watts “MAX” and 90 watts “Rated Power” what size amp\ndo I need to drive them to achieve the best performance.\n. When you are choosing an amplifier to drive speakers or a Subwoofer you should\nuse the “RMS” per channel ratings(Amplifier) and the “Rated Power”\n(Speaker/Subwoofer) as the reference for your choice. Always define which\nratings you are comparing and try and get them as close as you can.\n“Max/Peak” Power Rating\nThe “MAX/PEAK” power rating of a speaker is the amount of power\nthat the speaker can handle in short bursts or peaks without causing\npermanent damage to the voice coil.If this rating is exceeded constantly\nyou will risk burning the voice coil.\n“Rated Power” Power Rating\nThe “Rated Power” rating of the speaker is generally considered to be the\nnominal constant power that the speaker will handle continuously\nwithout causing damage.\nTherefore if your speakers are rated at 90 watts (Rated Power) you should drive\nthem with an amplifier that is capable of supplying 90 watts RMS per\nChannel or as close to that as possible. That is not to say that an amp\ncapable of a higher output cannot be used if the amplifier settings are\nmanaged responsibly realising the potential to overdrive your speakers\nexists. This rule also applies to using an amplifier that is under driving\nyour speakers. Constantly driving a distorted signal from an overdriven\nSource to your speakers can cause damage to the voice coils\nBridge mode is commonly used to get more output power from amplifiers by combining two single channels output into one output. This can only be done safely if you know the minimum load (Ohms) permissible when bridged. This is more common when driving Subwoofers that have Dual voice coils or when you have multiple Subwoofers. First check the installation manual or look at the speaker terminal block on the amplifier as they normally have a diagram showing the bridged connections.\nThis is an example of a 4 channel amp speaker connection terminal block. As shown bridging is achieved by connecting the positive terminal from the left channel and the negative terminal of the right channel to the corresponding terminals on the subwoofer coil or coils depending on configuration. There are a number of different configurations depending on the load applied to the amplifier. See Subwoofer coil configuration. It all depends on whether you have a 4 Ohm, 2 Ohm or 1 Ohm stable amplifier (mono). An example of the different power ratings for a four channel amplifier are as follows.\n100 Watts RMS x 4 @ 4 Ohms\n200 Watts RMS x 2 Bridged @ 2 Ohms\nThese are examples only to give you an idea how this works. This also applies for 2 channel amplifiers and depends on whether they class AB or D class amps as the 1 Ohm rating applies to D class amps in general.\nIf the protection LED is illuminated this indicates the Amplifier is in protection mode due to a fault condition. There are numerous causes for this and it may mean the Amplifier will need to be inspected and diagnosed by a Technician. It may also be a speaker or subwoofer causing the fault condition.\nTo eliminate this as a possible cause disconnect the speakers or subwoofer from the output of the amplifier . If the Amplifier still goes into protection mode with nothing connected to the out terminals this indicates a fault with the amplifier.\nWhen running power cable for Amplifiers it is wise to calculate what the voltage drop will be especially if you are using long runs . The below table will help you to calculate the correct size cable you require.\nThe gain or level pot is not a volume knob, it is used to match the relative output level (voltage) of your source unit to the input circuit of your amplifier. The gain control determines how far you have to increase the volume on your source unit for the amplifier to reach full power. If you have your gain set too low your amplifier will not be able to reach full power and this could allow the source unit to clip which in turn will result in a distorted signal being delivered to your speakers. This is especially relevant with low voltage sources (lower than 2.5volts typically OEM units). With higher voltage sources (2.5volts or higher) if the gain is set too high the amplifier will be able to reach full power at a lower volume control setting from the source unit. This will allow the amplifier to be driven into clipping. The gain settings should generally be set lower, meaning a higher setting on the volume control of the source, to minimize amplifier clipping.\nGuide to Setting Gain Control\nIf you do not have the correct equipment (oscilloscope) this may help\nyou to set your gain.\n1. Once you have installed the system power it down.\n2. adjust the amplifier gain to the minimum.\n3. power up the system and adjust the volume control of your source unit\nto about 3/4 of the way up.\n4. slowly increase the gain control until it reaches the maximum level you\nare comfortable with or until it starts to clip, and then reduce the level\nuntil you are happy that you will not damage your speakers or Subwoofer.\nBefore any wiring and installation is performed, FUSION recommends you first plan the complete installation. Look at wiring routing, amplifier location and fitment. Please re-check the installation at completion\nAppropriate mounting is very important for prolonged life expectancy of any amplifier. Select a location that allows enough space so sufficient airflow is maintainable and a location that provides protection from moisture. Keep in mind that an amplifier should never be mounted upside down. Upside down mounting will compromise heat dissipation through the heatsink and could engage the thermal protection circuit.\nExcessive heat will shorten your amplifiers life. To maximise heat dissipation, be sure to leave at least 2.5 inches of clearance around the amplifier. If space is of the essence and the amplifier must be mounted in an enclosed or restricted area, a small 3 inch fan should be used in correspondence with a duct so the heat can flow past the heat sink.\nWARNING: Do not mount any amplifier on a subwoofer enclosure as extended exposure to vibration may cause malfunction of the amplifier\n1: Ensure the vehicle 12 volt lead is removed from the battery before any equipment is connected\n2: Investigate the vehicles gas tanks, brake lines and electrical wiring locations before you begin installation\n3: Attach the product securely to the vehicle to prevent damage in the event of an accident\n4: Ensure all wiring is protected to avoid damage or pinching of the cables\nMake sure before any connection is made to the amplifier or source unit, ensure that you turn the audio system off. Failure to do so could result in either the stock system or your new FUSION product being damaged. FUSION will not warranty damaged amplifiers due to incorrect installation.\nWhen wiring FUSION amplifiers, ensure that the wires are away from sharp objects and that rubber grommets and insulated bungs are used when wiring through door jams and any other steel panels.\nNote: Ensure the audio system is off during the installation of FUSION product. Once the installation is complete FUSION recommends that you turn the volume of the source unit up slowly so not to damage the speakers. Please recheck the complete installation prior to turning the audio system on.\nAlways ensure that the ground connection point is a clean bare metal connection and it is a good idea to apply silicone over the bolt and connection to prevent rust forming. If the amplifier is powered on without an Earth connection or a poor connection this will cause damage to the amplifier and / or head unit as it will try to find an earth via the least path of resistance which could be through the RCA connectors back to the Head Unit . This will burn the Earth track on the Head Unit or it may go through the chassis of the Amp to the floor of the car. This is traceable by a technician to verify the cause of the damage.\nMy Amplifier isn’t working.\n- Check to see which, if any, LED indicator is on.\n- If neither indicator is on first check your inline fuse in the power cable near the battery. (If blown replace with the same rated fuse ).\n- Ensure the Earth/Ground and power connections on the terminal block of the Amplifier are clean and tight.\n- Ensure your remote wire has a good connection at the amplifier and Head Unit/Source.\n- Check amplifier fuses, if blown replace with the same rated fuse but if they fail again you may have a problem in your amplifier circuit.\nDo not replace with a higher rated fuse as this could result in major damage.\n- If the problem persists please consult your authorized dealer at your place of purchase.\nProtection LED On\n- If the red protection LED is on it may be that the amp has gone into Thermal protection (overheating).\n- Let the amp cool down for 15-30 minutes and try again.\n- If the amp still goes into protection it could be the result of a speaker/sub problem, disconnect the speaker/sub from the amp and turn the amp on.\nIf the power indicator comes on and stays on that would indicate that you have a problem with your speaker/sub.\n- If the protection LED comes on you may have an intenal problem with your amp.\n- There could be a fault that needs professional diagnosing and servicing.\n- If the problem persists please consult the authorized dealer at your place of purchase.\nPower LED On No Sound\n- This can indicate that the amp is not receiving a signal from the head unit/source, check your RCA cables and settings.\n- Check the wiring connections on the output of your amp and the subwoofer.\n- You may have a problem with the output circuit of your amplifier.\n- If the problem persists please consult the authorized dealer at your place of purchase.""]"	['<urn:uuid:093870e9-9421-44a8-a8b5-2f45e72cc8d4>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T22:03:18.033296	10	47	2101
21	What different roles did Union regiments play in City Point, Virginia during the fall of 1864, specifically in terms of construction and security duties?	While at City Point, the 205th Regiment was engaged in picketing from the left of the army line to the James River and building forts and earth-works for the defense of City Point. Meanwhile, the 206th Regiment was assigned to the engineer corps and worked on constructing Fort Brady north of Dutch Gap, completing the work so well that they received a complimentary order from the department commander.	"['Companies A, C, and I of this regiment, were recruited in Blair county,J B, E, and H in Berks, F, and K in Mifflin, D in Huntingdon, and G in Blair, Dauphin, and Franklin. They rendezvoused at Camp Curtin, where, on the 2d of September, 1864, the following field officers were selected:Colonel Mathews had served in the Forty-sixth, and in the One Hundred and Twenty-eighth, Lieutenant Colonel Walter in the One Hundred and Fourth, Major Morrow in the Eighty-fourth regiment, and a large proportion of the remaining officers and men were skilled in military duty. On the 5th, the regiment left Harrisburg, and proceeding to Washington, crossed the Potomac, and went into camp at Fort Corcoran. At the end of a week, it moved to Camp Distribution, and taking in charge thirteen hundred recruits and drafted men, proceeded with them by transports to City Point. Moving out fonr miles, it reported to General Benham, and under his direction, was engaged in picketing from the left of the army line, to the James, and in building forts and earth-works for the defense of City Point, nearly the entire regiment being called to duty daily. On the 9th of October, it was ordered to the Army of the James, and at the end of twenty days, during which it was employed on picket duty, it returned and proceeded to join the Army of the Potomac. With five other new Pennsylvania regiments, it formed a provisional brigade, commanded by General Hartranft, and was attached to the Ninth Corps. Early in December, this brigade moved to the relief of the Second and Fifth Corps, which were threatened with an attack by the enemy, while out upon a demonstration on the left. On the 15th of December, the six regiments composing this brigade, were organized into a division, which became the Third of the Ninth Corps, composed of two brigades, the Two Hundred and Fifth, Two Hundred and Seventh, and Two Hundred and Eleventh, forming the Second Brigade, to the command of which Colonel Mathews was assigned. General Hairtranft commanded the division, and General Parke the corps. With the exception of occasional marches to the left, in support of aggressive movements, the regiment remained in camp, near Fort Prescott, on the Army Line Railroad during the winter, engaged in drill and fatigue duty, the division being held in reserve, just in rear of the other two divisions of the corps. Before daylight, on the morning of the 25th of Ma1arch, 1865, the regiment was summoned to arms, and ordered to stand in readiness to move upon the first signal, the enemy having broken through the line on Wilcox\'s front, and captured Fort Steadman. General Hartranft was quickly upon the ground where further disaster threatened, and;gathering in the regiments of his division, attacked and checked the victorious onset of the enemy. The Two Hundred and Fifth was ordered to move down a ravine which ran in rear of the captured line, and when opposite Fort Haskell, was halted under cover, and held in support of the rest of the line, which was hotly engaged. For nearly an hour, impatient to move upon the defiant foe, it was forced to stand in waiting. Finally, when all his plans for a combined assault had been perfected, General Hartranft gave the signal to charge, and with a united front, and with the greatest determination and daring, the lines moved on, sweeping every thing before them, and re-gaining all that was lost. The Two Hundred and Fifth, moved at once to the support of the charging column, and held a large number of prisoners, small arms, and one battle-flag. The loss, fortunately, was but slight, being but ten wounded. Of the part taken by the Two Hundred and Fifth, in the final assault upon the works before Petersburg, on the morning of the 2d of April, an idea will be best gained by the following extract from Captain Holmes\' official report:\n- Joseph A. Mathews, Colonel;\n- William F. Walter, Lieutenant Colonel;\n- B. Mortimer Morrow, Major."" On the night of the 1st instant,"" he says, "" at eleven o\'clock, the regiment was ordered to form on the color line in front of the camp, Major. Morrow in command. At one o\'clock A. M., of the 2d, the regiment was moved towards, and on the Plank Road in rear of Fort Sedgwick, halted sometime, and then advanced by the right of the fort in the covered way, and formed in line of battle, with the Two Hundred and Seventh in front, directly in rear of our picket line. The order was given to charge the enemy\'s works at daylight, which was gallantly accomplished. The regiment captured Battery 30, with a number of prisoners; also one battle-flag fell into our hands, being captured by private John Lilly, of company F, who acted very gallantly throughout the engagement. This flag was forwarded to General HLrtranft\'s headquarters, with a statement of its capture. Our colors were planted on the works, and remained there until the regiment was relieved. At this time, Major Morrow received a severe wound in the foot, and was taken off the field.. I then assumed command, and remained with the regiment in the works, repulsing several charges made by the enemy during the day, and at night, assisted in placing the abatis in front of our works, under a severe enfilading fire from the enemy, remaining upon the line until two o\'clock on the following morning, when I was ordered to move with the regiment to the rear of our picket line.""In the advance upon the hostile works, and in driving out the enemy and holding the line when captured, the regiment was exposed to a fearful fire of infantry and artillery, from the effect of which it suffered heavy losses. Two officers, Lieutenants Henry A. Lower, and David B. Roberts, and twenty-two enlisted men were killed, six officers and ninety-one enlisted men were wounded, one officer, Samuel L. Hughes, mortally, and five men were missing, an aggregate loss of one hundred and twenty-six. Major Morrow lost a leg. At daylight, the regiment was ordered to advance towards Petersburg; but everywhere the evidences of a general evacuation were observable, and on arriving within the city, it was found that the enemy had fired it in several places. By the aid of the fire companies, the flames were subdued, and the bridges crossing the Appomattox were saved. At noon the regiment returned to its former camp, and striking tents. started with the division to follow up the advantage. The progress to Burkesville Junction was slow, the command being charged with the repair of the the South Side Railroad as it went, and with keeping open this line of communication with the main body of the army. At Burkesville, the regiment remained until after the surrender of the rebel armies in the east, and hostile operations were at an end. It then proceeded via City Point to Alexandria, and encamped at Seminary Hill, where it remained until the 2d of June, when it was mustered out of service. Source: Bates, Samuel P. History of the Pennsylvania Volunteers, 1861-65, Harrisburg, 1868-1871.\nDyer, Frederick H. A Compendium of the War of the Rebellion Compiled and Arranged from Official Records of the Federal and Confederate Armies, Reports of he Adjutant Generals of the Several States, the Army Registers, and Other Reliable Documents and Sources. Des Moines, Iowa: The Dyer Publishing Company, 1908\nOrganization:Organized at Harrisburg September 2, 1864.\nMoved to Washington, D.C., September 5, thence to City Point, Va., in charge of 1,300 Recruits.\nAttached to Provisional Brigade, Defences of Bermuda Hundred, Va., Army of the James, to October, 1864.\nHartranft\'s Provisional Brigade, 9th Army Corps, Army Potomac, to December, 1864.\n2nd Brigade, 3rd Division, 9th Army Corps, to June, 1865.\nService:Siege operations against Petersburg and Richmond, Va., September, 1864, to April, 1865.\nDuty at City Point, Va., constructing fortifications, till October 9, 1864.\nPicket with Army of the James till October 29.\nJoin Army Potomac October 29.\nMovement in support of Weldon Railroad Expedition December 7-11.\nDabney\'s Mills, Hatcher\'s Run, February 5-7, 1865.\nFort Stedman March 25.\nAppomattox Campaign March 28-April 9.\nAssault on and fall of Petersburg April 2.\nPursuit of Lee to Burkesville.\nMoved to City Point, thence to Alexandria April 21-28, and duty there till June.\nGrand Review May 23. Mustered out June 2, 1865.\nLosses:Regiment lost during service:\n3 Officers and 37 Enlisted men killed and mortally wounded and\n17 Enlisted men by disease\n© Alice J. Gayley, all rights reserved\nWeb Space provided by', '|You are here: Cambria > Military > 206th Regt|\n|206th Regt History|\nThis regiment was principally recruited in the counties of Indiana, Westmoreland, and Jefferson, and was organized at Camp Reynolds, near Pittsburg, on the 8th of September, 1864, with the following field officers: Hugh J. Brady, Colonel; John T. Fulton, Lieutenant Colonel; Joseph B. Ferguson, Major. The field, staff, and line officers, were for the most part veterans, and the majority of the men had served in other organizations. On the day following its organization, it was ordered to Washington, but while en route, an order was received directing it to proceed from Baltimore by transport, to City Point, where, upon its arrival, it was temporarily assigned to a provisional brigade in the Army of the James, occupying the Bermuda front, attached to the Eighteenth Corps. On the 4th of October, the regiment was ordered to duty with the engineer corps, and crossing to the north bank of the James, was put to work upon a fort a mile north of Dutch Gap. It was here under fire of the enemy\'s guns, and had one man, and several wounded. So promptly and well was this work executed, that a complimentary order was issued by the commander of the Department, commending the energy and skill displayed, and in recognition of the service, directing the work to be called Fort Brady.\nOn the 26th of October, the regiment was ordered to report to General Terry, in command of the Tenth Corps, and was assigned to the Third Brigade of the First Division, where it was associated with the Eleventh Maine, Twenty-fourth Massachusetts, Tenth Connecticut, and One Hundredth New York. It was soon after placed in comfortable winter-quarters, on the line of works north of Fort Harrison, where every facility was afforded for drill and discipline that could be desired. By an order of the War Department, of the 3d of December, the Tenth and Eighteenth corps were merged, the Twenty-fourth and Twenty-fifth corps being formed from them, the Twenty-fourth comprising all the white troops, and General Ord was placed in command of the Army of the James.\nWhen the general movement of the army commenced on the 27th of March, 1865, the regiment was detached from the brigade, and ordered to remain in camp, reporting to General Devens, commanding the Third Division. This disposition was exceedingly distasteful to the command, and in response to a remonstrance made against it, the following communication from division headquarters was received: ""I am directed by General Foster, to state that he regrets exceedingly, that your command should have been ordered to remain. The order came from Department Headquarters, and the General did all in his power to have it revoked, but could not."" The convalescents of the First Division were ordered to report to Colonel Brady, who was directed to organize and hold them in readiness to move. On the 3d of April, the troops in front of Richmond were ordered to Advance. It was soon discovered that the enemy had evacuated his works, and fired the city, so that the troops marched in without opposition. On the 22d, the regiment was relieved from General Devens\' command, and ordered to report to General F. T. Dent, Military Governor, who assigned it to provost duty in the city. A month later, it returned to its brigade, Colonel Brady, as the ranking officer, assuming command. The regiment was soon after sent to Lynchburg, where it reported to General Gregg, and was, by him, assigned to provost duty in that town. It remained here for two weeks, and was then ordered to return to Richmond and re-join the division. On the 26th of June, no further service being required of it, it was mustered out of service, and ordered to Pittsburg, where on the 2d of July, it was finally disbanded. In his farewell order, General Dandy, in command of the brigade, said: ""Under your gallant commander, Colonel Hugh J. Brady, you were the first to enter Richmond, and to display in the capital of traitors, the Stars and Stripes of your country. Carry home with you, and bequeath it to your children, the red heart, the badge of the First Division. It is the symbol of deeds that will live when this present, and many succeeding generations, have passed away.""\n|Source: Bates, Samuel P., History of Pennsylvania Volunteers, 1861-5 (Harrisburg: B. Singerly, State Printer. 1871)|']"	['<urn:uuid:08a5f868-2885-40af-8eb3-ab6d852c055c>', '<urn:uuid:cdd642ca-acbf-446b-bea8-c34983424a6e>']	factoid	direct	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-12T22:03:18.033296	24	68	2174
22	best cookware materials convection microwave cooking	The best cookware for convection microwave cooking is made of metal, such as stainless steel or aluminum. These materials conduct heat evenly and quickly, resulting in thoroughly and evenly cooked meals. Heavy-duty aluminum or stainless steel pans are particularly good options as they heat evenly and resist warping due to high temperatures. They are also easy to clean and long lasting. Glass bakeware can also be used if it's labeled as microwave safe, offering attractive presentation and even cooking.	['Convection microwaves are a great choice for busy cooks who want to speed up meal preparation. The best cookware for use in a convection microwave is made of metal, such as stainless steel or aluminum. Metal cookware conducts heat evenly and quickly, resulting in meals that are cooked thoroughly and evenly.\nNon-metallic materials such as glass or ceramic should be avoided because they can interfere with the cooking process by not allowing the food to brown properly. Additionally, plastic containers can melt in high temperatures which could damage your oven. Make sure you select cookware that fits well inside your oven; too small pieces can cause food to overcook while too large pieces will leave parts of the meal undercooked.\nFinally, avoid any kind of cookware that has exposed wires since these could cause sparks within your oven’s interior cavity and lead to disaster!\nCooking with a convection microwave oven is becoming increasingly popular among busy families, who appreciate the convenience of being able to cook meals quickly and efficiently. But if you’re new to this type of cooking, you may be wondering what kind of cookware works best in a convection microwave oven.\nThe good news is that there are several types of cookware that work well in these kinds of ovens.\nHere are some great options for those looking for the best cookware for their convection microwave: 1) Heavy-duty aluminum or stainless steel pans – These heavy-duty metal pans heat evenly and resist warping due to high temperatures. Plus, they’re easy to clean and long lasting.\n2) Glass bakeware – Not only does glass bakeware offer an attractive presentation when serving food, but it also cooks evenly and doesn’t require any special care like nonstick coatings do. If using glass bakeware in your convection microwave, make sure it’s labeled as “microwave safe” before placing it inside the appliance.\nWhat Cookware Can Be Used in a Microwave Convection Oven?\nCooking with a microwave convection oven can be intimidating at first, but the results are well worth it. A microwave convection oven is a great way to save time and energy in the kitchen, as you can use it to prepare meals quickly and efficiently. With that said, not all cookware is suitable for a microwave convection oven.\nIn this blog post, we’ll discuss what types of cookware are best suited for use in one of these versatile appliances. The most important thing to remember when using any type of cookware in your microwave convection oven is that it should be made from materials that won’t warp or melt under heat exposure. This means sticking to metal pans such as cast iron skillets and stainless steel baking dishes.\nCeramic ware also works well in microwaves, although some people prefer glass dishes due their superior heat retention capabilities and because they don’t impart any flavor onto the food being cooked inside them. It’s important to note that plastic containers should never be used in a microwave convection oven; they will melt under high temperatures!\nDo You Need Special Pans for Convection Microwave?\nIf you’ve been considering purchasing a convection microwave for your kitchen, one of the questions you may have is whether or not special pans are necessary. The answer is yes, in some cases. Convection microwaves use an additional heating element and fan to circulate hot air around food as it cooks.\nThis helps ensure even cooking, but can cause problems if you try to cook with traditional metal bakeware that wasn’t designed for this process. When using a convection microwave oven, the best results will be achieved when using cookware specifically designed for this type of appliance. These types of pans typically feature materials such as glass or ceramic which allow heat to penetrate more evenly than metal pans do due to their higher thermal mass and low reflectivity properties.\nAdditionally, these pans are often shaped differently from traditional bakeware with deeper wells that help contain liquids during cooking so they don’t splatter all over the walls of your oven cavity like they would with regular baking sheets or cake tins. In addition to choosing the right material and shape for your cookware, there are other considerations when selecting appropriate items for a convection microwave oven:\nCan You Put Metal Pan in Convection Microwave?\nYes, you can put metal pans in a convection microwave. However, there are some safety considerations to keep in mind before doing so.\nConvection microwaves work by circulating heated air around the food as it cooks, which results in faster and more even cooking.\nAs such, they can be used to cook almost any type of food that would traditionally be cooked on the stovetop or in an oven. Metal is perfectly safe to use in a convection microwave—in fact most manufacturers recommend using metal pans with their appliances for the best results. That said, not all types of metal are suitable for use with a convection microwave; aluminum foil should never be used as it may cause sparks and create hazardous conditions inside the oven cavity.\nAdditionally, large pieces of uncoated stainless steel should also not be used since these will reflect heat away from your food instead of trapping it like other materials do when cooking. When using metal pans with your convection microwave (or any kind of pan for that matter), make sure you place them away from the walls and sides of the oven cavity to avoid overheating or sparking due to contact between them and other surfaces inside your appliance’s interior design .\nBest Cookware For Convection Microwave Ovens of 2023 [Updated]\nCan I Use Metal Pans in My Microwave Convection Oven\nIf you’re in the market for a new oven, you may be wondering if metal pans can be used in a microwave convection oven. The answer is…it depends!\nWhen shopping for microwaves with convection technology, it’s important to read the product descriptions carefully.\nSome manufacturers clearly state that their models are safe for use with metal cookware whereas others do not mention this at all. Before investing in any appliance or cookware, take some time to understand what types of materials and utensils are recommended by its manufacturer. Generally speaking, using metal cooking vessels in your microwave convection oven should be avoided unless specifically stated as compatible by the manufacturer.\nMetal reflects heat from microwaves which causes arcing – an electrical discharge that occurs when electricity jumps between two conductors such as metals inside your oven cavity – and potentially damaging sparks or fires may occur due to overheating components within the device itself.. It also increases wear on internal parts like door seals, fans and heating elements which could lead to premature failure of your appliance over time if done frequently enough.\nBaking Utensils for Convection Microwave Oven\nIf you love to bake but don’t have the time or space for a traditional oven, then a convection microwave oven may be the perfect solution. Convection microwaves are becoming increasingly popular because they offer all of the convenience and speed of traditional microwaving with added features like browning, crisping, and even baking. To make sure that you get the most out of your new appliance, however, it’s important to know what type of utensils will work best for convection cooking in your microwave.\nWhen selecting baking utensils for use in your convection microwave oven, opt for materials like glass or ceramic as opposed to metal pans or aluminum foil which can cause arcing (a spark caused by electricity). Additionally, since heating times differ from those used with traditional methods such as gas or electric ovens; make sure that whatever pan you choose is heat resistant so it won’t warp when exposed to high temperatures. Some good options include stoneware dishes made specifically for microwaves and special heat-resistant glassware like Pyrex.\nWhen using these types of containers there are some additional tips to keep in mind:\nConvection Microwave Oven Accessories\nWhen it comes to cooking, convection microwave ovens offer the best of both worlds – a combination of traditional microwaving and convection baking. But what makes them so unique? Convection microwave ovens use hot air circulated with fans to cook food more quickly and evenly than traditional microwaves.\nAnd with just the right accessories, you can take your meals from ordinary to extraordinary in no time! The most important accessory for any convection microwave oven is an appropriate rack or tray. Racks vary in size, shape and material depending on the model of your appliance.\nThe correct rack should be placed at least one inch above the bottom of your oven so that air can circulate properly for even heat distribution. Be sure to check whether your convection microwave comes with a suitable rack before making a separate purchase! Another popular accessory is an extender ring which increases the capacity of your oven by raising its interior walls higher than usual, providing enough space for larger dishes like roasts or casseroles.\nAn extender ring also helps create an ideal environment inside while keeping temperature consistent during cooking cycles.\nMicrowave And Oven Safe Cookware\nWhen cooking, it’s important to have the right cookware that won’t damage your appliances. If you want to use both a microwave and oven for your recipes, then you need cookware that is safe to use in both. This type of cookware is called “microwave and oven safe cookware.”\nNot all types of cookware can be used in the microwave and oven; some materials are too reactive or not heat-resistant enough for this purpose. Microwave and oven safe cookware is typically made from metal or ceramic materials like stainless steel, aluminum, copper, glass or enameled cast iron/steel. These materials can handle high temperatures without reacting with food particles or leaching chemicals into the food being cooked.\nAdditionally, these materials transfer heat quickly so foods will get cooked evenly with minimal risk of burning them on the bottom of the pan due to direct contact with an element inside an oven setting. Glass dishes such as Pyrex are popular options when it comes to cooking in microwaves because they barely absorb any energy during cooking which means no additional cleaning is required afterwards – simply wipe down after use!\nConvection Microwave Tray\nIf you’re looking for a way to make your microwave cooking easier, then a convection microwave tray is an excellent option. This simple piece of kitchen equipment can help you cook food faster and more evenly than ever before.\nA convection microwave tray works by utilizing hot air circulation to heat food quickly and efficiently.\nIt creates an even temperature throughout the entire dish and helps prevent uneven cooking or burning in certain areas. The result is perfectly cooked meals every time! The main advantage of using a convection oven over other types of microwaves is that it cooks food much faster than traditional methods.\nConvection trays allow you to cook larger portions with less energy consumption, making them great for busy households or those trying to save money on their electricity bills. Plus, these trays are designed to fit into almost any type of standard-sized microwaves so they’re easy to use regardless of what kind of appliance you have at home. When purchasing one, there are several things that should be taken into consideration such as size, shape and materials used in construction—all which will affect how well the tray performs in terms of speed and efficiency when heating up your meals.\nMicrowave Convection Oven Rack\nIf you’re looking for a way to cook your food faster and more efficiently, then the microwave convection oven rack may be just what you need. With a combination of microwaves and conventional heating elements, this type of oven can help you get dinner on the table in no time. But before you invest in one, it’s important to understand how they work and why they might be an ideal addition to your kitchen.\nA microwave convection oven is essentially two appliances combined into one unit: A traditional microwave oven plus a convection heating element that circulates hot air around the food being cooked. The result is faster cooking times and better results – not only because there are two sources of heat but also because the circulating hot air helps evenly distribute the temperature throughout whatever food is inside. Not all models have both features, so if this is something that interests you make sure to double check when shopping for an oven like this.\nThe other thing to consider about these types of units is their racks or shelves, which come in different sizes depending on model and brand (so again do your research!).\nDo You Need Special Cookware for Convection Oven\nConvection ovens are becoming more popular in home kitchens due to their energy efficiency and ability to cook food faster than traditional ovens. As a result, many people may be wondering whether they need special cookware for use in these ovens. The answer is yes—specialized cookware can help you get the best results from your convection oven.\nWhen cooking with a convection oven, it’s important to understand how this type of appliance works differently than regular ovens. Convection heats food by circulating hot air around the chamber using fans, which means that heat reaches all areas of the food evenly and quickly. This also means that foods cooked in a convection oven tend to dry out faster since there is no moisture retention like traditional heating elements allow for.\nTo ensure even cooking and prevent sticking or burning on pans when using a convection oven, special cookware should be used instead of standard bakeware such as aluminum or glass dishes.\nMicrowave Safe Baking Pans\nBaking pans are an essential part of any kitchen. They come in all shapes and sizes, from round to square, from cake tins to muffin tins. But what about baking pans that can be used in a microwave?\nThis is where microwave safe baking pans come into play! Microwave safe baking pans are specially designed for use in the microwave oven. They often have special coatings or materials that allow them to withstand the high temperatures of microwaves without melting or becoming damaged.\nThe best part is they still provide you with great results when cooking food – just like regular bakeware! For those who don’t want to invest in traditional bakeware but still want delicious baked goods, using a microwave-safe pan is the perfect solution. Not only are these types of pans easier on your wallet, but their ease of use makes them ideal for busy bakers who don’t always have time to preheat their ovens and wait around while things cook up slowly over many minutes (or hours).\nIf you’re looking for the best cookware to use in your convection microwave oven, you’ve come to the right place. Convection microwaves are a great way to get a fast cooked meal without all of the hassle that comes with using a conventional oven. But when it comes to choosing cookware and bake ware, there are some important considerations.\nWe’ll go over what types of materials work best in a convection microwave – like glass and ceramic – as well as how to choose the right size pieces for even cooking results!']	['<urn:uuid:8d2d1f19-9c3a-4ced-a449-7b99a6d454bb>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T22:03:18.033296	6	79	2542
23	What ports did the USS Lynde McCormick visit during its cruise?	The USS Lynde McCormick's ports of call during this cruise included Hawaii, Sasebo, Olongapo, and Hong Kong.	['Change of Command – March 18, 1968\nThe USS Lynde McCormick’s ports of call on this cruise include Hawaii, Sasebo, Olongapo, and Hong Kong.\nThe USS Lynde McCormick DDG-8 saw extensive action on the gunline and in patrolling the Vietnam coastline. Thousands of rounds of 5-inch ammunition were expended at enemy emplacements and supply lines, in aid of U.S. troops and other Allied troops ashore.\nThe USS Lynde McCormick happened to be in the Sea of Japan when the USS Pueblo AGER-2 came under attack from the North Koreans. The USS Lynde McCormick steamed at full speed to assist the USS Pueblo. Unfortunately we arrived on the scene a few hours too late to be of any assistance. The USS Pueblo had been taken into the harbor of Wonson, North Korea. The USS Pueblo was captured by the North Koreans on January 23, 1968. The USS Pueblo was the first U.S. Navy ship to be hi-jacked on the high seas by a foreign military force in over 150 years. To date, the capture has resulted in no reprisals against the North Koreans. This guarantees the USS Pueblo’s place in history as a watershed event in our national conscience. The USS Pueblo was a U. S. Navy vessel sent on an intelligence mission off the coast of North Korea. On January 23, 1968, the USS Pueblo was attacked by North Korean naval vessels and MiG jets. One man was killed and several were wounded. The Eighty-two surviving crew members were captured and held prisoner for 11 months.\nThe USS Pueblo was captured because the Russian’s wanted the crypto equipment. It turns out they had gotten the crypto codes from John Walker. John Walker worked as a Soviet spy while serving as communications specialist for the U.S. Navy. It is estimated that he helped the Soviet Union gain more than one million messages and compromised U.S. code security. He had already retired as a Navy officer when arrested in 1985. Other members of his ring included his sailor son Michael, his brother Arthur James Walker, who also served in the Navy, and his friend Jerry Alfred Whitworth, who trained in the Navy’s satellite communications.\nThe USS Lynde McCormick returns to San Diego, California, 6 April 1968 and operated off the west coast for the remainder of the year.\nFrom Deck Logs\nJanuary 1968: Berthed at Sasebo\n9 January: Depart Sasebo for Gulf of Tonkin\n13 January: Yankee Station\n19 January: Depart Yankee Station for HK, BCC\n21 January: Arrive HK, BCC\n28 January: Depart HK, BCC for Subic Bay, PI\n29 January: Arrive Subic Bay, PI\n31 January: Month closes berthed at Subic Bay, PI\n1 February: Month commences in transit to Gulf of Tonkin\n2 February: Proceeding to Danang Harbor\n2 February: Briefly standing in Danang Harbor before resuming NGFS\n14 February: Depart NGFS in vicinity of Hue for Subic Bay, PI\n16 February: Arrive Subic Bay, PI\n20 February: Depart Subic Bay, PI for Sasebo, Japan\n23 February: Arrive Sasebo, Japan\n29 February: Month closes moored Sasebo, Japan\n1 March: Month commences berthed at Sasebo, Japan\n1-22 March: Steaming in Sea of Japan\n22 March: Sasebo, Japan\n24 March: Depart Sasebo, Japan for San Diego, CA\n31 March: Month closes mid Pacific en route to San Diego, CA.\nI remember being anchored in Subic Bay on December 31 1968, when we were alerted of something big happening in Viet Nam. The ship canceled all leave, and away we left, full speed and with great haste. My duty station was in Combat Information Center. (CIC) We were informed en route that 50,000 North Vietnamese crossed the DMZ (which was the beginning of the TET Offensive). As we entered the Gulf of Tonkin, we were assigned to relieve the USS SAINT PAUL CA 73 (A Baltimore Class Cruiser with 9 – 8 inch guns and 12 – 5 inch guns ) that was supporting the ground troops near and in the Ancient City of Hue. The fighting was intense as the ground forces were fighting back the onslaught of attacks by both the Viet Cong and the North Vietnamese Army (NVA). Because of the tremendous accuracy of our (2) 5 inch 54s, the ground forces were directing our gun fire support to targets as close as 50 yards from their positions. Keep in mind that we were conducting these firing missions while we were out at least 12,000-13,000 yards. What was significant of this action was that it was supporting the army troops around the famous “Citadel”. If you remember correctly, we fired around the clock except when we were taking on ammo during the night (600 rounds & powder for mount 51 and the same for mount 52). We stayed on Station until we were relieved by (I believe) the USS SAINT PAUL CA 73. The McCormick had a great reputation with the ground forces, including requests from the Republic of South Korea (ROKs) to support their efforts. Not many ships got those missions. All in all, some 44 + years later, I still cherish the (2) tours that I spent aboard the Mighty Mac. The crew was the best of the best.\nWhile I was on board, we went through a couple of typhoons and took some of those 55 degree rolls. That was very scary. Once, I went up to the signal bridge to see the storm. We were not allowed outside on the main deck or 02 level during bad weather. I crawled out to the front handrail to get a hold of it – (as it was too rough to stand up and walk and the roll of the ship was quite exaggerated up there and in those days we didn’t think of using or even have safety devices like a safety belt with a line attached or using ear plugs when firing the guns. We shot thousands of rounds in Viet Nam)– anyway, I managed to get to the handrail and proceeded to pull myself up to peek over the handrail just as the ship came off a wave and dived into the next one. The green water came up over the bow (which was called a hurricane bow) hit the gun mount and covered that and then hit the base of the bridge and splashed up over the signal bridge. I got soaked and was given the fright of my life. I crawled back to the signal bridge shack and made my way down through the ship to my berthing space to change. I must have given the signalmen a good laugh. Needless to say, I never went outside during a storm after that. I was dumb, but not stupid. I have lots of stories to share. I look forward to meeting all of you at the reunions.']	['<urn:uuid:f462c551-c221-4c4e-b67a-d74a4796a6ae>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T22:03:18.033296	11	17	1133
24	need help with my indoor photos in my dark studio how to position single soft box	For indoor studio portraits with a single soft box, position it about 6 feet high and angle it downward to illuminate the subject. Place it off to the side of the camera. This creates natural-looking light since it's relatively large and diffused, with shadows falling naturally to the ground. This positioning can create a dramatic look with the subject's face half lit and half in shadow.	['If you want to shoot indoor studio portraits but have never done it before, one of the major challenges you’ll face is creating and controlling your lighting. When working with strobes, you need to understand how to adjust your power levels, and even then, you won’t know how the light is going to look until after you’ve taken the shot.\nContinuous lighting is easier in this respect, as it allows you to see the quality of light falling on your subject before you take the shot. But the problems with many continuous light systems is that you can’t easily make adjustments to power levels the way you can with strobes, and if you’re using Tungsten lamps (bulbs), you can’t shoot in mixed lighting situations where there is daylight without experiencing color imbalance.\nWith the Photoflex Constellation3 Kits, however, both of these limitations have been eliminated. The Constellation3 head accommodates three separate lamps that can be turned on and off separately to give you flexibility with power levels, and now you can use daylight-balanced fluorescent lamps in the heads to color balance with daylight.\n- The Wrong Way\n- Positioning a Single Soft Box\n- From Main to Rim\n- Bouncing the Rim Light\n- The Front Fill\n- Adjusting Power Levels\n- Keeping the Light Directional with Grids\n- Lay the Groundwork, then Get Creative\nThe Wrong Way\nWhen shooting studio portraits, lighting is everything. Sometimes, the best way to show how to do something right is by doing it wrong.\nIn this situation, I decided to show the “wrong way” by taking an auto-mode snapshot of my assistant, James, with just the built-in flash of a digital SLR activated.\nThis result shows the pitfalls of built-in flash lighting. Since the light from the flash is very small and travels in the same direction as the lens, it tends to flatten out the dimensions of your subject. It’s also very good at casting distracting shadows directly behind your subject.\nPositioning a Single Soft Box\nAfter my fun with the built-in flash, I promptly disabled it and set the exposure mode to Manual so that I could fine-tune my exposure levels and control my depth of field.\nNext, I set up a Photoflex Medium Constellation3 Kit, installed three CoolStar 150-watt daylight fluorescent lamps into the head, attached casters to the LiteStand to be able to move it easily, and positioned it to the right of James. I positioned the soft box about 6 feet high, and angled it downward to illuminate James.\nBecause the light is always on with these units, it’s easy to see how the light is affecting your subject. When I had the light where I wanted it, I took my first shot.\nAlready I saw a dramatic difference. Because the light was off to the side of the camera and because it was relatively large and diffused, it created a much more natural-looking light. The shadows now naturally fell to the ground, and James’ face — half lit, half in shadow — made for a very dramatic look.\nFrom Main to Rim\nAlthough this was an interesting look, I noticed that the soft box was also lighting up the steel door in the background a little more than I wanted it to. I also wanted to try using the soft box as a rim light instead of a main light for a more dynamic look.\nSo I rolled the Constellation3 Kit toward the back of the set and angled it back toward James. When the lighting looked good from the point of view of the camera, I took another shot.\nAt this point, the lighting had a very different look. James’ face was almost entirely in shadow, but the soft box helped to create an outline along the right side of his body and separated him more against the metal door background, which was now much lower in tone.\nBouncing the Rim Light\nNext, I decided to bounce some of this rim light into the left side of his body, so I set up a Photoflex 39×72” LitePanel frame, mounted it to a LiteStand with casters by way of a GripJaw and Heavy Duty Swivel, and attached a Silver panel fabric to it. Once it was set up, I rolled it into position.\nThanks to the continuous light of the Constellation3 Kit, I could see when the light was hitting James directly while angling the LitePanel. You can even see the effects of the LitePanel in the setup shots below.\nFrom this angle, you can see how the light from the Constellation3 Kit bounces off the LitePanel and into the left side of James.\nOnce the LitePanel was where I wanted it, I took another shot.\nWhile the light illuminating James’ left side was not as strong as it was on the right, it nevertheless helped to create separation from the background and did not compete with the overall direction of light.\nHad I placed another Constellation3 on the other side to match the intensity of light, the shot would have had a very different feel and you would have seen contradictory shadows on the floor.\nBelow is a side-by-side look at the Constellation3 result shots taken.\nThe Front Fill\nIn reviewing my most recent result, I realized I wanted to add some light into the front area of James. I wanted this light to act more like a fill light, rather than a main light, and have the amount of light be relatively minimal in order to maintain the overall dramatic feel of the shot.\nSo, I set up another Constellation3 Kit, this time with a large SilverDome soft box, and placed only two daylight lamps into the head. I positioned this box back about eight feet and turned on both lamps.\nAlthough the light seemed to brighten up the shot considerably, I decided to take a test shot and see how the exposure would be.\nAdjusting Power Levels\nAs you can see from the result, this second Constellation3 Kit added considerably more light. While the light levels were perfectly acceptable, it wasn’t the look I was after. Instead, I wanted to maintain a somewhat more moody look for this portrait.\nThis meant that I needed to tone down the light from the second Constellation3 Kit. But rather than repositioning the large Constellation3 Kit back away from the set, I simply turned off one of the lamps from the head and took another shot.\nIn reviewing the result, I saw that the light levels on James were better (although still somewhat bright), but that it was the background that was giving me some trouble. Specifically, I realized that the metal doors were now reflecting both soft boxes, which ended up flattening them out dimensionally, as well as brightening them more than I wanted.\nKeeping the Light Directional with Grids\nTo cut down the light from the large Constellation3 Kit, as well as to keep it from reflecting into the metal doors, I attached a set of Grids to the face of the soft box. While Grids still allow a soft box to maintain its soft quality of light, it keeps the light directional and prevents it from spilling off to the sides.\nWith the Grids in place, the lighting in person looked exactly how I wanted it for this shot. Without changing the exposure settings on the camera, I then took a series of shots from various angles. This one, taken only inches from the floor, ended up being one of my favorites.\nHere’s a side-by-side look at the results taken with the Large Constellation3 added to the mix.\nIt’s important to remember that none of these lighting options are “right” or “wrong”. Depending upon the look you’re after, you may choose one lighting style over another.\nThat being said, let’s now take a comparative look at the very first shot taken with the built-in flash next to my final result. I’ll let the differences speak for themselves.\nLay the Groundwork, then Get Creative\nOnce you’ve dialed in the lighting strategy for your shot, you can then focus on the other elements of the shot: perspective, composition, posing, etc… Here’s another favorite, in which the camera is tilted to break up the symmetry somewhat, and James appears to be caught in a candid moment.\nWhile the pose and camera angle here are interesting, it’s the dramatic lighting that brings this shot to the next level.\nWritten and photographed by Ben Clay.']	['<urn:uuid:b4ef6873-0cc3-4462-856f-1cd01286febe>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T22:03:18.033296	16	66	1415
25	Do carbon trading and ISO 14001 have similar business benefits?	Both systems offer distinct business advantages. Cap and Trade programs provide direct financial benefits through trading emission allowances in the market, allowing companies to profit from reducing emissions. ISO 14001:2015 certification offers broader business benefits including improved market position, reduced insurance costs, prevention of environmental fines, stronger customer confidence, and competitive advantages. Both approaches help organizations comply with environmental regulations and improve their market standing, though Cap and Trade focuses specifically on emissions while ISO 14001 covers broader environmental management.	['Greenhouse gases (GHG) are a group of gases that, when in the atmosphere, reflect sunlight back towards the planet, heating it up as a result. Carbon dioxide, water vapor, methane, nitrous oxide, and ozone are the primary gases that carry this property, varying drastically from gas to gas with respect to the magnitude of their contribution and its duration. Cumulatively, this process, known as the greenhouse effect, plays a major role in determining the surface temperature of Earth.\nAs a result of concerns raised over the magnitude and significance of the human role in atmospheric GHG levels, GHG reduction efforts have blossomed, ranging in scale from personal initiatives to international laws and agreements that operate on a truly global level. In the past, GHG levels were restrained to some degree by laws aimed at increasing air quality by reducing pollution. Today, initiatives take aim at the reduction of GHG emissions specifically. These initiatives, alongside standards published through a consensus process, laws and agreements enacted at all levels of authority, and the creation of organizations and processes designed to hold others accountable, together function to address concerns over atmospheric GHG levels.\nStandards, designed by open consensus-based processes, are voluntary guidelines and procedures that represent industry-wide views on the proper way to go about a specific activity, promoting fair competition, interoperability, reliability, and consistency. Regarding GHG, standards exist to guide basic assessment and measurement of GHG, quantification, monitoring, and reporting of GHG reductions or removals at a project or organizational level, as well as more specific standards, such as one detailing a specific test method for measurement of particulate emissions and heating efficiency of outdoor solid fuel-fired hydronic heating appliances.\nLaws and agreements that affect GHG emissions take many forms. Chief among these are enforced mandatory caps or carbon taxes. Additionally, others voluntarily promise to reduce their GHG emissions. In either case, those involved in GHG reduction make assertions about their efforts. In those cases where the amount of GHG emitted is tied to a financial or legal system of incentives and/or repercussions, assertions of GHG emission quantities must be validated/verified by an impartial third-party. GHG emitters must, for example, make assertions based upon GHG emission measurement methods that have been shown to be both accurate and precise, as well as comply with many other prerequisites for trustable and actionable GHG assertions.\nThe impartial third-parties that validate/verify GHG assertions are known as GHG Validation/Verification Bodies (V/VB). The American National Standards Institute (ANSI), the national standards organization of the United States of America and its representative internationally, offers accreditation services for V/VBs through the ANSI Accreditation Program for Greenhouse Gas Validation/Verification Bodies. The program essentially does for V/VBs what V/VBs do for those making GHG assertions; it looks for consistency and reliability in the operating procedures of V/VBs, inspiring confidence in the claims they make.\nCap and Trade Programs\nCap and Trade programs, one of the more popular approaches to GHG reduction, seek to reduce GHG emissions by utilizing the free market instead of working against it, consisting of two predictably named parts, an initial cap and subsequent trading. The cap is a mandatory reduction in the total amount of emissions. Trading then serves to incentivize the reduction of GHG emissions by offering a competitive financial motive.\nEssentially, everyone involved receives a certain number of allowances (either for free or by auction), translating to an absolute volume of GHG they can each legally emit. If an entity does not utilize all of their allowances, the remainder can be either kept for use at a later date or auctioned off. Conversely, an entity emitting more GHG than they have in allowances would purchase additional allowances at auction. In this manner, the auction price is dictated by supply and demand. As a whole, Cap and Trade programs are cost-effective with respect to their goal of reducing GHG emissions.\nThe effectiveness of Cap and Trade programs hinges directly upon the accuracy of GHG emissions and reductions data, coupled with rigorous enforcement for instances of fraud or noncompliance. This accuracy is necessary for both the accomplishment of the program’s environmental goals, as well as promoting trust among the companies involved and therefore the market as a whole. ANSI-accredited V/VBs fill this niche, serving as an impartial check upon GHG emitters and assuring that programs like Cap and Trade are achieving their desired goals.\nA tax on GHG emissions is another method of financially incentivizing GHG reduction efforts. Simple in its workings, a carbon tax appends a cost to every unit of emitted GHG. Lower emissions translate directly to less carbon taxes being paid. However, while carbon taxes do encourage reductions in GHG emissions, they do not put a mandatory limit on GHG emissions, allowing for some to simply pay the tax if doing so is more profitable than reducing emissions or, alternatively, passing the cost directly to the consumer or end-user. Comparing carbon taxes to Cap and Trade programs, both options have their relative advantages and disadvantages and care must be taken to select the appropriate GHG reduction strategy.\nHere, as in Cap and Trade programs, accurate GHG assertions are vitally important, creating the same need for V/VBs and the ANSI accreditation process behind them.\nValidation/Verification and Accreditation\nValidation/Verification, as explained earlier, is crucial to any GHG reduction program. However, while V/VBs look at GHG assertions, V/VBs must themselves be tested for competency, impartiality, and consistency. Filling this role, the ANSI Accreditation Program for Greenhouse Gas Validation/Verification Bodies accredits GHG V/VBs based upon their adherence to the requirements set out in ISO 14065:2013, including, for example, the requirement “that validation bodies and verification bodies establish and maintain a procedure to manage the competence of its personnel,” among others. Another relevant standard is ISO 14064-3:2006, offering “guidance for the validation and verification of greenhouse gas assertions.” Additionally, a relatively recently published standard, ISO 14066:2011, will be introduced into the process in the near future as well.\nIn this way, ANSI accreditation serves to assure other entities of a V/VB’s competency and adherence to international and widely agreed upon standards. The ANSI GHG V/VB accreditation procedure is itself thorough, lending additional credibility to those organizations that successfully complete the process, earn accreditation, and go on to retain it after ongoing ANSI surveillance and reassessment.\nThe Entire Process\nThe entire process, with GHG assertions validated/verified by V/VBs and ANSI accrediting those V/VBs, is an integral part of the foundation of any GHG reduction program. Essentially, the influence of ANSI-accredited V/VBs is always preferable. For some emission reduction programs, such as The Climate Registry, a collaboration spanning across Canada, The United States, and Mexico, validation/verification by an ANSI-accredited V/VB is not only preferable but a prerequisite for participation. As today’s world is increasingly concerned with the level of GHG in the atmosphere and the processes that affect it, this system of validation/verification and accreditation is a critically important fixture.\nFor further information, consult, alongside others, the following sources:\nWhat are Greenhouse Gases? From the U.S. Energy Information Administration (EIA), a statistical agency of the U.S. Department of Energy (DOE).\nGreenhouse Gas Emissions and Cap and Trade. Both are from the U.S. Environmental Protection Agency (EPA).\nThe Frequently Asked Questions section of the ANSI Accreditation Program for Greenhouse Gas Validation/Verification Bodies website.', 'ISO 14001:2015 Environmental Management System Certification\nThe current version of ISO 14001 was reviewed and published in 2015. Currently, more than 420,000 organizations around the world are ISO 14001-certified. This number is growing rapidly.\nISO 14001 is an international standard offered by the International Organisation for Standards (ISO). Through the ISO 14001 management system, companies can monitor the impact their products and services have on the environment.\nIt’s primarily for organizations that use natural resources. As a result of their processes that convert natural resources into useful products, the organizations tend to release a lot of wastes that negatively affect the environment. These negative effects need to be recognized and reduced, throughout their lifecycle. Recently, ISO 14001 was updated in 2015, which is why the latest certification in the ISO 14001 family that is available is called ISO 14001:2015.\nISO 14001:2015 - An Introduction\nISO 14001:2015 defines the conditions imposed by a standard Environmental Management System. It includes the guidelines for a company looking to enhance their environmental performance and sustainability policies. Its use is directed towards associations who are looking to manage their liabilities with a planned approach towards sustainability. ISO offers a 35-page implementation guide, curated and overseen by Technical Committee ISO/TC 207/SC 1.\nApplicability of ISO 14001:2015\nAccording to ISO, the Environmental Management System Certification can be applied to any organization. This helps organizations of different sizes, belonging to different industries, get the same advantages of ISO 14001:2015’s application. ISO 14001:2015 eliminates the ‘one size fits all’ approach.\nOne must remember that ISO 14001:2015 is not a scale with which environmental management parameters must be matched to. It’s more like an internationally-authorized guidebook that states how certain operations must be conducted with respect to environmental conservation. The basic fundamentals of ISO 14001:2015 are based on the Plan-Do-Check-Act (PDCA) iterative management method.\nObjectives of ISO 14001:2015\nThe objective of ISO 14001:2015 is to deliver a framework for the applying organization aimed towards the protection of the environment. It consists of policies leveled with socio-economic needs that help create a response plan for dynamic environmental conditions. The certification outlines specific requirements for sustainable development, which include:\n- Mitigation of negative effects caused by an organization on the environment, protecting the environment.\n- Mitigation of possible adverse impacts of the environment on the organization.\n- Enhancing immediate environmental performance.\n- Helping the organization fulfil compliance obligations\n- Demonstration of compliance with the changing regulatory requirements of the certification.\n- Achieving strategic business goals by embedding environmental issues into business management\n- Setting up guidelines for a product’s life cycle, including design, production, curation, distribution, and disposal. This ensures environmental effects are not shifted elsewhere within the product life cycle, unintentionally.\n- Strengthening market position of an organization while achieving operational and financial profits from the implementation of environmentally sound alternatives.\n- Boosting leadership involvement and employee engagement\n- Improvement of confidence in the company and company reputation\n- Clear communication of environmental information to relevant and interested entities.\nAdvantages of ISO 14001:2015 Environmental Management System\nThe advantages an organization taking up ISO 14001:2015 Environmental Management System certification may benefit from are:\n- Boost in customer confidence, recognition for the community, employees, and environmental authorities\n- Improvement of company perception through an internationally-recognized certificate\n- Advantage over competitors, both in business and sustainability\n- Reduced risks of environmental accidents drive down costs of insurance\n- Prevent incidents that may lead to fines and sanctions, catapulted by the lack of environmental protection measures\n- Prevent possible incidents that may lead to sanctions /fines due to lack of environmental protection policies\n- Better alignment to market requirements through sustainable approaches\nSustainable Development Goals\nThrough Intercert’s enriched ISO certifications, organizations will be able to contribute to Sustainable Development Goals (SDG) that the United Nations has prescribed in their ambitious 15-year plan that address crucial issues ailing the world. ISO 14001:2015\nEnvironmental management systems contribute to the following SDG codes:\n- 1: No Poverty\n- 2: Zero Hunger\n- 3: Good Health and Well-being\n- 4: Quality Education\n- 5: Gender Equality\n- 6: Clean Water and Sanitation\n- 7: Affordable and Clean Energy\n- 8: Decent Work and Economic Growth\n- 9: Industry, Innovation, and Infrastructure\n- 12: Responsible Consumption and Production\n- 13: Climate Action\n- 14: Life Below Water\n- 15: Life on Land\nWhy Intercert for ISO 14001:2015 Environmental Management System\nIntertcert serves transparent and impartial services so that your organization realizes every detail advised by ISO 14001:2015 accurately. Our certifications are highly sought-after due to our competitive and cost-effective services. With an experience of over 13 years, we’ve mastered the art of delivering excellence in the form of training and international certifications. We are an accredited management system body with certifications from IAF, IAAC, APAC, and Standard Council of Canada (SCC).']	['<urn:uuid:13cbb6f4-02a7-49be-93be-c3de1cc9a41b>', '<urn:uuid:a65457b0-cba5-480f-b63c-9b932d4a4f43>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T22:03:18.033296	10	80	2011
26	food bank healthy initiatives vs fda menthol regulations enforcement	The enforcement approaches for these health initiatives differ significantly. Food bank healthy initiatives use positive reinforcement methods, such as cooking demonstrations, nutrition education, and encouraging healthy choices through display arrangements, without any punitive measures. In contrast, there have been concerns about enforcement of the FDA menthol ban potentially leading to increased police interactions with Black Americans. However, advocates have emphasized that the menthol ban implementation would not criminalize tobacco use, and have signed statements addressing systemic racism in enforcement. Both initiatives prioritize education and health promotion over punitive measures.	"['Joint Statement from the Plaintiffs on FDA’s Response in Favor of Banning Menthol\nA Powerful Step Towards Saving Lives\nWASHINGTON, DC – April 28, 2022 –Today, we celebrate with all Americans that the Food and Drug Administration (FDA) took a powerful step towards banning menthol and saving lives, especially Black lives. While a menthol ban does not go into immediate effect, our hope for a better, brighter, and healthier tomorrow starts today.\nThe plaintiffs in the lawsuit against the FDA, Action on Smoking and Health (ASH), African American Tobacco Control Leadership Council (AATCLC), the American Medical Association (AMA), and the National Medical Association (NMA), represented by the legal team at Pollock Cohen LLP are proud to have spurred on this action, which was initiated by the organizations behind the Citizen Petition.\n“Today’s proposed ban on menthol cigarettes and flavored cigars will save Black lives. If the ban is finalized, it will prevent children from becoming the next generation of smokers and help adult smokers quit. For too long, menthol cigarettes have had an insidious grip on African American communities,” said Dr. Rachel Villanueva, President of the National Medical Association. “As Black physicians, we applaud the FDA for taking such an important measure to reverse the adverse health outcomes that disproportionately affect our communities and are directly related to the use of these tobacco products.”\n“Menthol cigarettes violate the right to health of Americans, and especially Black Americans,” said Kelsey Romeo-Stuppy, Managing Attorney at Action on Smoking and Health. “With this proposed rule, the FDA is taking an important step to protect its citizens and is also getting in line with many other countries around the world who have already taken this essential measure.”\nAfter more than a decade of inaction from the Federal government on menthol, the Biden Administration acted quickly and definitively to prioritize Black lives and to make progress towards ending the institutionalized racism African Americans face as a result of menthol cigarettes.\n“We are pleased by the FDA’s action today to propose a ban on menthol-flavored cigarettes and finally remove these harmful products from the market. For far too long, tobacco companies have used menthol-flavored products to prey on young people, particularly Black youth. While long over-due, this rule, if finalized, will be a big step toward preventing a new generation from becoming addicted to tobacco products and suffering unnecessary harm, including death,” said AMA President Gerald E. Harmon, M.D. “We will continue to support policies and initiatives that will keep tobacco products out of the hands of our nation’s youth.”\nWhile we are certain that tobacco companies will attempt to continue to insert themselves into the rulemaking process and attempt to muddy the facts around the harms of menthol, we are equally confident that the public health community and research community will stand strong, and we encourage those communities to submit comments to the rule.\nUnfortunately, that has already begun, with groups asking the FDA to reconsider because a menthol ban may increase police interactions with Black Americans. As advocates, we have been conscious of this concern from the beginning. We do not, and would not, support an action that criminalized tobacco. ASH, AATCLC, and several of the co- petitioners, signed a Joint statement Addressing Systemic Racism in the Enforcement of Commercial Tobacco Control that emphasizes this point.\nWith the FDA’s draft rule today, we are one step closer toward preventing the deaths of 39,000 African Americans every year from tobacco-related diseases. Let’s remain firm in our mission and realize that goal.\n“We are working towards a day where no Black people in America die from preventable deaths due to tobacco,” explained AATCLC Co-Chair Carol McGruder. “This critical move in the rule-making process by the FDA helps make that day a reality.”\nRegister here for our joint webinar on Tuesday, May 3, 2022 at noon Eastern. Hear the Plaintiffs Perspectives on the proposed FDA rule on menthol.\nPlaintiff direct media contacts are:\nFor inquiries specific to the Citizen Petition, please contact Joelle Lester from the Public Health Law Center.\nAbout The African American Tobacco Leadership Council (AATCLC)\nThe AATCLC was formed to educate the public about the effects of tobacco on the Black American and African Immigrant populations, the tobacco industry’s predatory marketing tactics and the need to regulate menthol and all flavored tobacco product. To more effectively reach and save Black lives, the AATCLC partners with community stakeholders and public serving agencies to inform and direct tobacco control policies, practices and priorities. https://www.savingblacklives.org.\nAbout Action on Smoking & Health (ASH)\nFounded in 1967, Action on Smoking and Health (ASH) is America’s oldest anti-tobacco organization, dedicated to a world with ZERO tobacco deaths. Because tobacco is the leading cause of preventable death worldwide, ASH supports bold solutions proportionate to the magnitude of the problem. https://ash.org.\nAbout the American Medical Association (AMA)\nThe American Medical Association is the physicians’ powerful ally in patient care. As the only medical association that convenes 190+ state and specialty medical societies and other critical stakeholders, the AMA represents physicians with a unified voice to all key players in health care. The AMA leverages its strength by removing the obstacles that interfere with patient care, leading the charge to prevent chronic disease and confront public health crises, and, driving the future of medicine to tackle the biggest challenges in health care. For more information, visit https://www.ama-assn.org.\nAbout the National Medical Association (NMA)\nThe NMA promotes the collective interests of physicians and patients of African descent. We carry out this mission by serving as the collective voice of physicians of African descent and a leading force for parity in medicine, elimination of health disparities and promotion of optimal health. https://www.nmanet.org/\nAbout Pollock Cohen LLP\nPollock Cohen LLP focuses on representing plaintiffs and protecting the public interest through impact litigation. The firm also handles a broad range of complex commercial litigation, maintains an active pro bono practice, and is a leader in whistleblower actions. https://www.pollockcohen.com.', ""Food pantry and food bank healthy food initiatives combine hunger relief efforts with nutrition information and healthy eating opportunities for individuals and families with low incomes. Such initiatives offer clients healthy foods such as fruits, vegetables, whole grains, low-fat dairy products, and lean proteins, and can implement client choice models. Initiatives can include fruit and vegetable gleaning programs, farm Plant-a-Row efforts, and garden donations. Healthy food initiatives can also modify the food environment via efforts such as on-site cooking demonstrations and recipe tastings, healthy meal kits, featured product placements, produce display stands, or point-of-decision prompts. Some food pantries and food banks establish partnerships with health and nutrition professionals to offer screening for food insecurity and medical conditions (e.g., diabetes), provide nutrition and health education, and health care support services as part of their healthy food initiatives (FA-Nutrition initiative, Bush-Kaufman 2019*).\nExpected Beneficial Outcomes (Rated)\n- Increased healthy food consumption\n- Increased food security\nOther Potential Beneficial Outcomes\n- Improved nutrition\n- Improved weight status\n- Reduced emissions\nEvidence of Effectiveness\nThere is some evidence that food pantries and food banks that use healthy food initiatives increase fruit and vegetable consumption, improve diet quality, and increase food security for clients more than traditional food pantries and banks (An 2019, Grabow 2020, Cochrane-Martin 2018). Available evidence associates such initiatives with a significant increase in fresh fruit and vegetable distribution through food pantries (Long 2019), and when implemented as intended, with improved quality of foods available and selected by pantry clients (Caspi 2019). However, additional evidence is needed to confirm effects especially over the long-term (An 2019).\nFood pantry initiatives that provide nutrition education and recipe demonstrations can increase the variety of fruits and vegetables and the amount of produce clients consume, and can improve nutrition knowledge, home cooking skills and habits, and food security status (An 2019, Flynn 2013*, Keller-Olaman 2005*). In a Rhode Island-based study, food pantry clients participating in a plant-based cooking and nutrition education program and preparing these meatless recipes at home 2-3 meals per week, improved weight status and reduced total food costs (Flynn 2013*). In a rural Missouri case study, healthy food initiatives at food pantries are associated with improved healthy food access and food choices, as well as increased gardening knowledge, support, and resources (Chapman 2017). Program evaluations of comprehensive healthy food initiatives that include a client choice model with a variety of featured healthy food options, connections to community resources and services, a respectful and welcoming environment, and on site opportunities for clients to build skills are associated with significant improvements in food security, self-sufficiency, and diet quality over time (Cochrane-Martin 2018).\nFood banks and pantries with healthy food initiatives that use client choice models for food selection and tailor messaging, recipes, and food tips for their clients appear to have greater effects on healthy eating decisions and vegetable use than generic messaging and food tips (Clarke 2011*). One Utah-based study suggests food pantry healthy food nudge programs may improve dietary choices, with the largest effects reported among Hispanic survey respondents (Coombs 2020*). A Connecticut-based study suggests ingredient bundles with recipe tastings may increase the likelihood that food pantry clients select healthier food options more than tastings alone (Stein 2019). Healthy food display interventions can help food pantry clients select healthier food items (An 2019) and may improve diets among people who are food insecure (Grabow 2020).\nHealthy food initiatives in food pantries that include gleaning, farm, or garden donations can reduce waste and increase the availability of local, seasonal produce for participants, which may reduce emissions from fossil fuels used to produce, process, and transport food (Ringling 2020, FAO-Food waste, Hic 2016, SSSA-McIvor 2017, CCAFS-Campbell 2012). Healthy food initiatives in food pantries may also reduce the energy intensity of an individual’s diet if more plant-based foods are consumed in place of animal products (Ringling 2020).\nSuccessful healthy food initiatives are most often found in food pantries or food banks that have written nutrition guidelines, client choice distribution models, and adequate refrigerator storage (Long 2020). Building a healthier food inventory, enhancing partner agency access, storage, and distribution capacity for fresh foods, developing nutrition education, and expanding community partnerships and settings for healthy food distribution are key components for healthy food initiatives at food banks (Wetherill 2019*). Establishing strong nutrition policies at food banks is a suggested strategy to improve the nutritional quality of food distributed; however, such changes have the potential to alter relationships with existing donors, possibly reducing the total amount of food available for distribution (Handforth 2013*). Interviews with food bank and food pantry personnel suggest that other challenges to adopting healthy food initiatives include the procurement, handling, and monitoring of large quantities of perishable foods (Campbell 2013*), as well as limited staffing and other resources (An 2019). Such interviews also suggest food pantry clients may benefit from nonjudgmental nutrition guidance; however, some clients may have other reasons not to choose healthy foods, for example, lack of cooking equipment or transportation challenges (Cooksey-Stowers 2018*).\nThe cost of healthy food initiatives varies; some are very low cost and relatively straightforward to implement, such as healthy food display interventions (Grabow 2020).\nImpact on Disparities\nFeeding America, a hunger-relief organization with over 200 member food banks and 60,000 food pantries and meal programs nationwide, is one organization working nationally to increase healthy foods distributed through food banks (FA-Who we are). As of 2020, 69% of foods distributed through Feeding America member food banks were considered healthy and align with USDA nutritional guidelines. Feeding America also operates mobile food pantries to bring healthy foods to families in underserved areas and the Produce Matchmaker program that connects growers directly with food banks to donate surplus, fresh produce (FA-Nutrition initiative). In response to the COVID-19 pandemic, Feeding America’s network of food banks has implemented many new drive-through and outdoor food distribution locations, delivered emergency food boxes, and worked with many schools and school districts to provide grab-and-go cold meals for children and families while schools are closed (FA-Who we are).\nFarm to Food Bank, Farm to Food Pantry, and Farmers Ending Hunger programs are in place in many states, including Connecticut (CFB-Farm donations), Georgia (GFBA-FTFB), Kentucky (KAFB-FTFB), Maryland (MFB-FTFB), Montana (CFC-MT FTFB), Oregon (FEH-Oregon), and Rhode Island (RI Food Bank-Community farms). Some states offer growers a tax credit for donations of excess produce to state-sponsored food banks, as in Arizona, California, Colorado, Oregon (NRDC-Gunders 2012), Iowa (IA DOR-FTFD tax credit), and Kentucky (KAFB-FTFB).\nMany food banks use farming, gardening, gleaning, and other healthy food programs to procure fresh produce and support healthy eating for their clients, as in Community Food Bank of Southern Arizona in Tucson (CFBSA-Programs), Chester County Food Bank in Pennsylvania (CCFB-Programs), Inter-Faith Food Shuttle in Raleigh, North Carolina (IFFS-Food bank), and Santa Barbara County Food Bank (SBCFB-Programs). Ample Harvest, a non-profit organization, helps home and community gardeners donate their excess produce to over 8,700 food pantries in all 50 states (Ample Harvest-Garden donations). The Missouri Department of Agriculture supports the Grow & Give program part of the 10,000 Garden Challenge, which encourages home and community gardens to donate a portion of their produce to local food banks and pantries (MDA-Grow and give 2011).\nMany food pantries have implemented more comprehensive healthy food initiatives. For example, the More Than Food initiative builds capacity for food banks and food pantries to offer client choice, connections with community services, supports to reach their goals, and a welcoming environment; this initiative was developed through evaluations of the Freshplace food pantry program in Hartford, CT (Foodshare-More than food, Foodshare-Freshplace). The Fresh Start program at Kelly Memorial Food Pantry in El Paso, Texas also provides healthy food assistance, health and wellbeing classes, and case management services to address the root causes of hunger and overcome barriers to personal and financial independence (Fresh Start). The Whole Body Approach to Wellness (WBA) program, developed through a collaboration between Northern Illinois Food Bank and Northern Illinois University, is a health promotion, nondiet program to help food pantry clients develop a healthy relationship with food and fitness, while respecting body, shape, and size diversity and focusing on a holistic approach to wellness and behavioral outcomes. Participants who are at risk for or already diagnosed with Type 2 diabetes also receive additional fresh produce as part of the program (NIFB-WBA).\nTo increase the amount of healthy foods available, statewide food policy councils can require food banks to spend a portion of funds on fresh fruits and vegetables and low-fat milk, for example, the New York State Council on Food Policy (CDC DNPAO-FPC).\nAmple Harvest-Garden donations - Ample Harvest. Find a local pantry if you have garden surplus you want to donate.\nChangeLab-Banking on health - ChangeLab Solutions. Banking on health: Improving healthy beverage & nutrition standards in food banks.\nCHD-Healthy food planning 2005 - Columbus Health Department (CHD). Improving access to healthy food: A community planning tool. 2005.\nSRTSNP-Safe routes to healthy foods - Safe Routes to School National Partnership (SRTSNP). Healthy communities: Safe routes to healthy foods.\nISU-Food and sustainability resources - Iowa State University (ISU), Sustainable Food Processing Alliance. Online resources for food and sustainability.\nCitations - Evidence\n* Journal subscription may be required for access.\nAn 2019 - An R, Wang J, Liu J, et al. A systematic review of food pantry-based interventions in the USA. Public Health Nutrition. 2019;22(9):1704-1716.\nGrabow 2020 - Grabow KN, Schumacher J, Banning J, Barnes JL. Highlighting healthy options in a food pantry setting: A pilot study. Family and Consumer Sciences Research Journal. 2020;48(3):263-275.\nCochrane-Martin 2018 - Martin A, Booth JN, Laird Y, et al. Physical activity, diet and other behavioural interventions for improving cognition and school achievement in children and adolescents with obesity or overweight. Cochrane Database of Systematic Reviews. 2018;(3):CD009728.\nLong 2019 - Long CR, Rowland B, McElfish PA. Intervention to improve access to fresh fruits and vegetables among Arkansas food pantry clients. Preventing Chronic Disease. 2019;16(1):1-7.\nCaspi 2019 - Caspi CE, Canterbury M, Carlson S, et al. A behavioural economics approach to improving healthy food selection among food pantry clients. Public Health Nutrition. 2019;22(12):2303-2313.\nFlynn 2013* - Flynn MM, Reinert S, Schiff AR. A six-week cooking program of plant-based recipes improves food security, body weight, and food purchases for food pantry clients. Journal of Hunger & Environmental Nutrition. 2013;8(1):73-84.\nKeller-Olaman 2005* - Keller-Olaman SJ, Edwards V, Elliott SJ. Evaluating a food bank recipe-tasting program. Canadian Journal of Diabetic Practice and Research. 2005;66(3):183-186.\nChapman 2017 - Chapman D. Healthy food access in Missouri food pantries through evidence-based intervention. Journal of Human Sciences and Extension. 2017;5(1):141-157.\nClarke 2011* - Clarke P, Evans SH, Hovy EH. Indigenous message tailoring increases consumption of fresh vegetables by clients of community pantries. Health Communication. 2011;26(6):571-582.\nCoombs 2020* - Coombs C, Savoie-Roskos MR, LeBlanc H, Gast J, Hendrickson J. Nudging urban food pantry users in Utah toward healthier choices. Health Promotion Practice. 2020:1-7.\nStein 2019 - Stein EC, Stowers KC, McCabe ML, White MA, Schwartz MB. Ingredient bundles and recipe tastings in food pantries: A pilot study to increase the selection of healthy foods. Public Health Nutrition. 2019;22(9):1717-1722.\nRingling 2020 - Ringling KM, Marquart LF. Intersection of diet, health, and environment: Land grant universities’ role in creating platforms for sustainable food systems. Frontiers in Sustainable Food Systems. 2020;4(70).\nFAO-Food waste - Food and Agriculture Organization of the United Nations (FAO). Food wastage footprint & climate change.\nHic 2016 - Hic C, Pradhan P, Rybski D, Kropp JP. Food surplus and its climate burdens. Environmental Science and Technology. 2016;50(8):4269-4277.\nSSSA-McIvor 2017 - McIvor K. Soils in the city: Community gardens. Soil Science Society of America (SSSA). 2017.\nCCAFS-Campbell 2012 - Campbell B. Is eating local good for the climate? Thinking beyond food miles. Research Program on Climate Change, Agriculture and Food Security (CCAFS), CGIAR Research Programs. 2012.\nLong 2020 - Long CR, Narcisse MR, Rowland B, et al. Written nutrition guidelines, client choice distribution, and adequate refrigerator storage are positively associated with increased offerings of Feeding America’s detailed Foods to Encourage (F2E) in a large sample of Arkansas food pantries. Journal of the Academy of Nutrition and Dietetics. 2020;120(5):792-803.e5.\nWetherill 2019* - Wetherill MS, White KC, Seligman HK. Nutrition-focused food banking in the United States: A qualitative study of healthy food distribution initiatives. Journal of the Academy of Nutrition and Dietetics. 2019;119(10):1653-1665.\nHandforth 2013* - Handforth B, Hennik M, Schwartz MB. A qualitative study of nutrition-based initiatives at selected food banks in the Feeding America network. Journal of the Academy of Nutrition and Dietetics. 2013;113(3):411-415.\nCampbell 2013* - Campbell EC, Ross M, Webb KL. Improving the nutritional quality of emergency food: A study of food bank organizational culture, capacity, and practices. Journal of Hunger & Environmental Nutrition. 2013;8(3):261-280.\nCooksey-Stowers 2018* - Cooksey-Stowers K, Read M, Wolff M, et al. Food pantry staff attitudes about using a nutrition rating system to guide client choice. Journal of Hunger & Environmental Nutrition. 2018;14(1-2).\nCitations - Implementation Examples\n* Journal subscription may be required for access.\nFA-Who we are - Feeding America (FA), Hunger and Health. Who we are and our COVID 19 response.\nFA-Nutrition initiative - Feeding America (FA). Healthy communities need healthy foods.\nCFB-Farm donations - Connecticut Food Bank (CFB). Farm donations.\nGFBA-FTFB - Georgia Food Bank Association (GFBA). Farm to food bank program (FTFB): Georgia farmers feeding Georgia families.\nKAFB-FTFB - Kentucky Association of Food Banks (KAFB). Farms to food banks (FTFB).\nMFB-FTFB - Maryland Food Bank (MFB). Farm to food bank (FTFB).\nCFC-MT FTFB - Community Food Co-op (CFC). Southwest Montana Farm to Food Bank program (MT FTFB): Support Farm to Food Bank.\nFEH-Oregon - Farmers Ending Hunger (FEH). How Farmers Ending Hunger works and the Oregon Food Bank partnership.\nRI Food Bank-Community farms - Rhode Island Community Food Bank. Community farms growing food for the food bank.\nNRDC-Gunders 2012 - Gunders D. Wasted: How America is losing up to 40 percent of its food from farm to fork to landfill. New York City: National Resources Defense Council; 2012.\nIA DOR-FTFD tax credit - Iowa Department of Revenue (IA DOR). Farm to food donation (FTFD) tax credit.\nCFBSA-Programs - Community Food Bank of Southern Arizona (CFBSA). Programs and services include home and community gardening, farmers' markets, gleaning, education, and advocacy.\nCCFB-Programs - Chester County Food Bank (CCFB). Food bank programs include gardening kits, gleaning, farming, nutrition education, and healthy cooking classes.\nIFFS-Food bank - Inter-Faith Food Shuttle (IFFS). A food bank pioneering innovative, transformative solutions to end hunger: We feed, we teach, we grow.\nSBCFB-Programs - Santa Barbara County Food Bank (SBCFB). Programs moving the community from hunger into health.\nAmple Harvest-Garden donations - Ample Harvest. Find a local pantry if you have garden surplus you want to donate.\nMDA-Grow and give 2011 - Missouri Department of Agriculture (MDA). 10,000 Garden Challenge joins forces with Missouri's food banks to 'Grow & Give.' News Release. 2011.\nFoodshare-More than food - Foodshare. More than food: Because it takes more than food to end hunger.\nFoodshare-Freshplace - Foodshare. More than food: History of the Freshplace program.\nFresh Start - Fresh Start. Fresh Start program at Kelly Memorial Food Pantry in El Paso, Texas.\nNIFB-WBA - Northern Illinois Food Bank (NIFB), Feeding America. Health & nutrition blog: Whole Body Approach to Wellness (WBA) program.\nCDC DNPAO-FPC - Centers for Disease Control and Prevention (CDC), Division of Nutrition Physical Activity and Obesity. DNPAO state program highlights: Food policy councils (FPC).\nRelated What Works for Health Strategies\nTo see citations and implementation resources for this strategy, visit:\nTo see all strategies:""]"	['<urn:uuid:0916ccbb-503b-40f4-a5fd-5864885a0dae>', '<urn:uuid:1f67ed53-eeff-413d-9f3d-09f7e0fb8f57>']	open-ended	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-12T22:03:18.033296	9	89	3572
27	Why do college students get poor sleep and what health problems can it cause?	College students often get poor sleep due to demanding academic schedules and a culture that sees sleep deprivation as necessary. As noted at Harvard, there aren't enough hours in the day for students to complete their work, so they take hours from the night. This sleep deprivation can lead to multiple health complications including high blood pressure, type 2 diabetes, cancer, obesity, depression, respiratory issues, anxiety, and disrupted hormone production. It also weakens the immune system by preventing the body from producing cytokines, which are infection-fighting substances.	['They packed Ticknor Lecture Room in Boylston Hall, some wearing pajamas, some snuggling beneath blankets. They drank warm milk and ate cookies. They listened to soothing music.\nBut students jolted to alertness when they heard the message delivered at University Health Services’ “Why Sleep? A Pajama Party Panel” Wednesday evening (Dec. 3): Harvard students are not getting nearly enough sleep, and it’s taking a toll on their health and their academic performance.\n“Your mothers were right,” said Robert Stickgold, assistant professor of psychiatry at Harvard Medical School. “If you don’t get sleep, you’ll get sick, you won’t learn as well.” While individuals vary slightly, most of us need eight hours of sleep per night for optimal health and peak performance.\nStickgold, who bravely addressed the crowd wearing pajamas, a nightcap, and slippers, joined sleep experts and Harvard Medical School (HMS) faculty members Charles Czeisler and John Winkelman for an eye-opening discussion of the latest sleep research and its bearing on the sleep – or lack thereof – of overworked Harvard students.\nCzeisler, professor of medicine and chief of the Division of Sleep Medicine at Brigham and Women’s Hospital, outlined the factors that affect a person’s sleep cycle and level of alertness during the day. Our body’s internal clocks, which he called “circadian pacemakers,” work in concert with a system that monitors how long we’ve been awake, he said, in a surprising way. While one system tells us the longer we’ve been awake the sleepier we’re getting, our circadian pacemakers send out the strongest drive for wakefulness a few hours before bed. That’s because, unlike other mammals that sleep in bouts of naps, humans maintain long periods of sleeping and waking. We need that burst toward the end of our sleep or wakefulness to keep us going for the next few hours.\nAs long as we go to bed and rise at the same time each day, these two systems hum along smoothly. “But when we upset the apple cart and … try to pull an all-nighter, suddenly the two systems, instead of opposing each other and maintaining a high level of alertness, now both systems are sending out a strong drive for sleep,” said Czeisler. “It’s creating a critical zone of vulnerability for accidents.” Falling asleep at the wheel is the leading cause of fatal-to-the-driver accidents, Czeisler said.\nTo sleep, perchance to learn …\nNot only might you run off the road without enough sleep, you’re probably not learning as well as you could, said Stickgold, who described the typical student sleep routine of skimping on sleep during the week and making it up on weekends or vacations as “sleep bulimia.” This binge-purge cycle is likely as detrimental to health as food bulimia is, he said.\nTo research sleep-dependent memory consolidation, Stickgold trained subjects in a laboratory on a visual task, then tested them on that task after a night of sleep. With six hours of sleep or less, he said, subjects showed no improvement on the task.\n“But it gets worse,” he cautioned. If subjects got less than six hours of sleep the first night after the training but then slept all they wanted for the following two nights, they still showed no improvement. “If you don’t get sleep that first night after training on these tests, you lose it. You lose all the benefit of the training,” he said. “The sleep you have after you study might end up being as important to what you learn as the sleep you get before you study.”\n‘But when we upset the apple cart and try to pull an all-nighter, suddenly the two systems, instead of opposing each other and maintaining a high level of alertness, now both systems are sending out a strong drive for sleep.’\n– Charles Czeisler, Professor of medicine and chief of the Division of Sleep Medicine at Brigham and Women’s Hospital\nWinkelman, who is assistant professor of psychiatry at HMS and associate director of the Sleep Disorders Program at Brigham and Women’s, brought a clinical perspective to the discussion of sleep. What happens, he asked, when you decide to go to sleep and you can’t?\nWinkelman introduced the principles of “sleep hygiene” – not clean sheets or before-bed showers, but habits that promote a good night’s sleep. Get out of bed if you can’t sleep, he said, so that you don’t associate your bed with insomnia’s frustrating tossing and turning. “The bed is for two things,” he said, “if you’re lucky.”\nHe urged avoiding alcohol and caffeine for the several hours before bed (acknowledging that, in a Cain-and-Abel twist, his brother is marketing director for Starbucks), avoiding stressful activities before bed, and maintaining a regular sleep schedule.\nWake up, little Susie (and Sammy)\nThe experts admitted that their sage advice and groundbreaking research fights an uphill battle against a culture that sees sleep deprivation as necessary – and macho. Our culture’s current attitudes on sleep deprivation are analogous to our permissive views on drinking and driving in the 1960s, said Czeisler.\nBill Gates likes to brag that his Microsoft programmers can work through two nights to meet a deadline, said Stickgold. “The result, of course, is Windows,” he said.\nTaking a more serious tack, Stickgold noted that Harvard fosters a culture of sleep deprivation, even adding late-night “Brain Breaks” to its standard meal plan to fuel students as they study into the night. “The University has to face up to the fact that they’re putting demands on students that exceed their capacity,” he said. “There aren’t enough hours in the day, so you take them out of the night.”\nThe panel touched a nerve with the students in the audience, most of whom lingered through the milk-and-cookies “cocktail hour” to pepper the researchers with questions. But will the expert opinions help Harvard’s sleep-deprived students change their habits?\n“It seems hard for me to change anything, but I would like to,” said Peter Wood ’07, who estimated he gets less than six hours of sleep most nights. A football player, Wood doesn’t crack the books until practice and dinner are over, at about 8:30 at night. He’s exhausted in most of his lectures, he said. Still, the panelists’ advice was not entirely lost on him.\n“The most important thing would be to try to establish a regular sleep schedule,” he said with resolve. “I find myself playing catch-up on the weekends. It’s probably doing me more harm than good.”\nKevin Koo ’07 also struggles to get enough sleep; he hits the pillow for five to six hours per night. “I probably could be performing better in my classes if I got more sleep,” he said, but more sleep is tough to come by. “Students here naturally strive for the best. There really aren’t enough hours in the day,” he said.\nFor all their wisdom and research, the experts sympathized with the students. Many students likely don’t even realize they’re not reaching optimal snoozing, they said.\n“Sleep deprivation is like chronic pain,” said Stickgold. “After a while, it feels normal and you just learn to live with it.”', 'Health and Fitness\nHow Does Sleep Affect Your Health\nSleep deprivation is increasingly becoming a common problem in modern society. Many people are lacking quality sleep at some point in their lives. Occasional sleep interruptions are nothing more than a nuisance; however, continuous lack of sleep can lead to several health complications and even lower your overall quality of life. Most people focus on exercise and having a healthy diet to improve their quality of life, but it’s time we prioritized getting enough sleep as well. In this text, we’ll highlight some key points about sleep deprivation to help you get the rest you need:\nThere are many reasons why you may not be getting a healthy amount of sleep. For starters, people have busy schedules and intentionally choose not to sleep for the recommended amount of time to do other things like work, study, or entertainment. This creates inconsistent sleep-wake patterns that can lead to the accumulation of sleep debt and affect your quality of sleep in the long run. There are a number of environmental factors that can make it difficult for you to fall and stay asleep, including too much noise and/or light, extreme temperatures, an uncomfortable bed, and electronics in the bedroom. Some mental and health conditions such as depression, anxiety, tonsils, colds, back pain, and asthma can also prevent deep sleep, which in turn affects your sleep quality. While there are several other factors that can hinder you from getting sufficient sleep, it’s important to rule out sleep disorders such as sleep apnea, insomnia, or restless legs syndrome.\nA night without enough sleep will leave you feeling tired, sleepy, and irritable. It also makes it difficult to concentrate, which can affect your decision-making and even increase your risk of accidents and injury at work, home, and on the road. Over time, lack of sleep can negatively affect several systems in the body and make you prone to serious health and mental conditions. For starters, sleep deprivation prevents the body from producing more cytokines (infection-fighting substances), thereby disrupting your immune system. It’s only during sleep that neurons in the brain help to process new information. Lack of sufficient sleep will hinder that, leaving your brain exhausted and unable to perform its duties. Continuous lack of sleep also increases your risk of health ailments, including high blood pressure, type 2 diabetes, cancer, obesity, depression, respiratory issues, anxiety, and even affect hormone production.\nTreatment and Prevention\nTreatment is only necessary if you can’t sleep due to physical or psychological difficulties. Lack of sleep can be resolved using over-the-counter medications. Unfortunately, many people end up forming a dependency on sleeping medications. Luckily, there are several non-medical measures you can take to improve the quality of your sleep. Start by making your bed as comfortable as possible with one of many top mattresses available for every kind of sleeper. Other ways to ensure a comfortable bedroom for better sleep is by keeping it dark, cool, and quiet, as well as removing any distractions from your bedroom, including electronics, work materials, and clutter.']	['<urn:uuid:ca7e5971-cb09-4455-8b66-2d6add972726>', '<urn:uuid:74272f83-bc93-4d61-9006-ce75b8c7b6ce>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T22:03:18.033296	14	87	1692
28	What are the key photon propulsion methods used in these systems?	Two key photon propulsion approaches are discussed: The DE-STAR system uses photon recycling with a reflector cavity that bounces photons back and forth like a ping-pong ball, achieving a force multiplication factor of 5. The Alpha Centauri mission proposal uses the stars' own radiation, with a graphene-based light sail being decelerated by incoming stellar radiation and using gravitational swing-bys around Alpha Centauri A and B for propulsion.	['DE-STAR, or Directed Energy System for Targeting of Asteroids and exploRation, is the brainchild of UC Santa Barbara physicist Philip Lubin and Gary B. Hughes. DE-STAR initial objective is to deflect asteroids.\nThe DE-STAR system could be leveraged for many other uses, such as stopping the rotation of a spinning asteroid and achieving relativistic propulsion.\nTests simulated space conditions. Using basalt — the composition of which is similar to known asteroids — they directed a laser onto the basalt target until it glowed white hot — a process called laser ablation, which erodes material from the sample. This changes the object’s mass and produces a “rocket engine” using the asteroid itself as the propellant. In space, this would be powerful enough to alter its course.\nThe team simulated a spinning asteroid using basalt to determine whether they could slow, stop and change its rotation direction. They used magnets to spin the basalt and then directed the laser in the opposite direction to slow the rotation.\n“Our video shows the basalt sample slowing down, stopping and changing direction and then spinning up again,” said Brashears. “That’s how much force we’re getting. It’s a nice way to show this process and to demonstrate that de-spinning an asteroid is actually possible as predicted in our papers.”\nLab measurements have shown that in terms of thrust, the conversion of laser energy to force through this method is about 100 micronewtons per watt, which works out to 10 kilowatts per newton\nAccording to Lubin, a professor of physics at UCSB, manipulating the speed of a spinning asteroid offers another important possibility in space: the ability to explore, capture and mine asteroids. This is something NASA aims to do with its Asteroid Redirect Mission. The mission — which remains theoretical — is intended to visit a large near-Earth asteroid, collect and return a boulder from its surface and possibly redirect the asteroid into a stable orbit around the moon.\nPictures of Wafer Scale Spacecraft with laser on reflector. Includes fiber optic cables for cloaking\nand wafer as the payload. The red depicts the laser light.\nIn addition, the students explored photon propulsion, which is key to the group’s latest project, DEEP-IN, or Directed Energy Propulsion for Interstellar exploratioN. The DEEP-IN concept relies on photon propulsion, whereby thrust from photons emitted from the laser array could be used to propel a spacecraft. This allows for the possibility of relativistic flight — speeds approaching the speed of light — for the small spacecraft required for future interstellar missions.\nThe team also tested a photon recycler, a device that reuses photons from the laser by shining them on a reflector cavity. “We have a second mirror at some distance away that bounces the photons back and forth like a ping-pong ball onto the spacecraft reflector.” Brashears said. “In effect, we’re recycling these photons to achieve a force multiplication that allows the vehicle to go even faster. So far, with a simple implementation, we have achieved an amplification factor of five. Much more is possible with refinement. This works as predicted, though implementing it into the full flight system will be complex.”\nLaser pushed roadmap\nDirected Energy Interstellar Propulsion of WaferSats Getting to 25% of lightspeed will be over 4000 times faster than Voyager 1.\nResearchers propose a roadmap to a program that will lead to sending relativistic probes to the nearest stars and will open up a vast array of possibilities of flight both within our solar system and far beyond. Spacecraft from gram level complete spacecraft on a wafer (“wafer sats”) that reach more than ¼ c and reach the nearest star in 15 years to spacecraft with masses more than 100,000 kg (100 tons) that can reach speeds of near 1000 km/s such systems can be propelled to speeds currently unimaginable with our existing propulsion technologies. To do so requires a fundamental change in our thinking of both propulsion and in many cases what a spacecraft is. In addition to larger spacecraft, some capable of transporting humans, we consider functional spacecraft on a wafer, including integrated optical communications, optical systems and sensors combined with directed energy propulsion. Since “at home” the costs can be amortized over a very large number of missions. The human factor of exploring the nearest stars and exo-planets would be a profound voyage for humanity, one whose non-scientific implications would be enormous. It is time to begin this inevitable journey beyond our home.\nThey assume a slightly futuristic sail with thickness of 1 µm for many cases and 10 µm (thick even for todays sails). Future advancements in sails thickness down to 0.1 µm and below can be envisioned but are NOT assumed. They will only make the conclusions even more optimistic. The density of all sails we consider is about the same, namely ρ ~1,400 kg/m3\nWafer Scale Spacecraft\nRecent work at UCSB on Si photonics now allows us to design and build a “spacecraft on a wafer”. The recent (UCSB) work in phased array lasers on a wafer for ground-based optical communications combined with the ability to combine optical arrays (CMOS imagers for example) and MEMS accelerometers and gyros as well as many other sensors and computational abilities allows for extremely complex and novel systems. Traditional spacecraft are still largely built so that the mass is dominated by the packaging and interconnects rather than the fundamental limits on sensors. Our approach is similar to comparing a laptop of today to a super computer with similar power of 20 years ago and even a laptop is dominated by the human interface (screen and keyboard) rather than the processor and memory. Combining nano photonics, MEMS and electronics with recent UCSB work on Si nano wire thermal converters allows us to design a wafer that also has an embedded RTG or beta converter power source (recent LMCO work on thin film beta converters as an example) that can power the system over the many decades required in space. Combined with small photon thrusters (embedded LEDs/lasers for nN thrust steering on the wafer gives a functional spacecraft. While not suitable for every spacecraft design by any means, this approach opens up radically new possibilities. In addition the power from the laser itself can add significant power to the spacecraft even at large distances.\nThe laser sail is both similar to and fundamentally different than a solar sail. For small sails, even with low powers the flux can easily exceed 100 MW/m2 or 10000 Suns. This requires a very different approach to the sail. For the small reflectors we propose using a pure dielectric reflection coating on ultra-thin glass or other material. Spherical (bubbles) sails are an option for testing. The loss in fiber optic quality glasses allows loss in the ppt(10-12)/μm (of thickness) which is even better than we need. This is an area we need to explore much more. The flux at the tip of high power single mode fiber optic exceeds 10 TW/m2, higher than we need. Rather than the typical 1/4 reduced wavelength anti reflective (AR) dielectric coating, we will need to design a 1/2 wave reflection coating for the sail.\nThermoelectrics show the greatest promise for energy for the wafercraft in the near future. Plutonium-238, the traditional fuel source for radioisotope thermoelectric generators (RTGs) produces 560 mW/g of heat in its pure form, and 390 mW/g of heat in its fuel pellet form (Plutonium Dioxide). Current RTG technology (NASA’s Multi-Mission RTG, or MMRTG for short) has an electrical conversion efficiency of 6-7%. Assuming 6.5% efficiency from thermal to electrical conversion we get about 25 mw (electrical)/g. In order to generate 5 mw we need about 0.2 g.\nStirling engines are much more efficient with about 30-50% efficiency for the temperatures we can get BUT there are no chip scale Stirling engines and no 20-100 year lifetime Stirling engines currently exist even without the extreme requirement of chip level system. MEMS equivalents may be possible but this is a research item yet to be explored. This leaves open the possibility of much more power (by a factor of 5-10) that may be achieved. This would greatly expand our data rates as well as sensor suite possibility for the small systems\nG-Forces on Small-Scale Spacecrafts\nPossibly the greatest benefit of our wafer scale design is the high speeds our spacecraft can reach. We have discussed the ability of our ship to be accelerated to about 0.25c in ten minutes. This is an acceleration of roughly 10,000 g’s, an acceleration that could put a formidable strain on our delicate wafer. However, this may not be as big of a problem as it may seem. Many present day weapons systems incorporate electronic components into their artillery shells to correct trajectory mid flight. During launch, these electronic components must be able to sustain accelerations of at least 10,000 g’s, sometimes ranging to even higher than 15,000 g’s depending on the system. Upon muzzle exit, the artillery shells are subject to substantial pressure changes, resulting in significant shocks and vibrations. Our spacecraft would not be subject to such volatile environment, as our acceleration takes place over a period of at least ten minutes, rather than a fraction of a second. Many methods have been successfully developed to house electronic components during launch, most involving some sort of shock absorbing material such as foam or gel. It is reasonable to think that a similar method could be used for our spacecraft.\nPhoton recycling for larger thrust and efficiency\nThe efficiency of the photon drive can be improved by reusing the photons reflected by the spacecraft reflector in an effective optical cavity mode to get multiple photon reflections. This is known as photon recycling. It is not a new concept but may be of some use for some of our applications.\nFree space phase control over large distances during the acceleration phase will be a critical enabling capability. This will require understanding the optics, phase noise and systematic effects of our combined onboard metrology and off-board phase servo feedback. Reflector stability during acceleration will also be on the critical path as will increasing the TRL of the amplifiers for space use. For convenience we break the roadmap into several steps.\nOne of the critical development items for space deployment is greatly lowering the mass of the radiators. While this sounds like a decidedly low tech item to work on, it turns out to be one of the critical mass drivers for space deployment. Current radiators have a mass to radiated power of 25 kg/kw, for radiated temperatures near 300K. This is an area where some new ideas are needed. With our current Yb fiber baseline laser amplifier mass to power of 5kg/kw (with a likely 5 year roadmap to 1 kg/kw) and current space photovoltaics of less than 7 kg/kw, the radiators are a serious issue for large-scale space deployment.\nMass and speed with 100 GW laser\n1 gram 24% of lightspeed 10 grams 14% of lightspeed 100 grams 7.8% of lightspeed 1 kg 4.3% of lightspeed 10kg 2.4% of lightspeed 100kg 1.4% of lightspeed 1000kg 0.77% of lightspeed 10 tons 0.43% of lightspeed 100 tons 0.24% of lightspeed\nSOURCE – UCSB\nBrian Wang is a Futurist Thought Leader and a popular Science blogger with 1 million readers per month. His blog Nextbigfuture.com is ranked #1 Science News Blog. It covers many disruptive technology and trends including Space, Robotics, Artificial Intelligence, Medicine, Anti-aging Biotechnology, and Nanotechnology.\nKnown for identifying cutting edge technologies, he is currently a Co-Founder of a startup and fundraiser for high potential early-stage companies. He is the Head of Research for Allocations for deep technology investments and an Angel Investor at Space Angels.\nA frequent speaker at corporations, he has been a TEDx speaker, a Singularity University speaker and guest at numerous interviews for radio and podcasts. He is open to public speaking and advising engagements.', 'Full braking at Alpha Centauri\nFebruary 01, 2017\nInterstellar journey: The aim of the Starshot project is to send a tiny spacecraft propelled by an enormous rectangular photon sail to the Alpha Centauri star system, where it would fly past the Earth-like planet Proxima Centauri b. The four red beams emitted from the corners of the sail depict laser pulses for communication with the Earth.[less]\nInterstellar journey: The aim of the Starshot project is to send a tiny spacecraft propelled by an enormous rectangular photon sail to the Alpha Centauri star system, where it would fly past the Earth-like planet Proxima Centauri b. The four red beams emitted from the corners of the sail depict laser pulses for communication with the Earth.\nIn the recent science fiction film Passengers, a huge spaceship flies at half the speed of light on a 120-year-long journey toward the distant planet Homestead II, where its 5000 passengers are to set up a new home. This dream is impossible to realize at the current state of technology. “With today’s technology, even a small probe would have to travel nearly 100,000 years to reach its destination,” René Heller says.\nNotwithstanding the technical challenges, Heller and his colleague Michael Hippke wondered, “How could you optimize the scientific yield of this type of a mission?” Such a fast probe would cover the distance from the Earth to the Moon in just six seconds. It would therefore hurtle past the stars and planets of the Alpha Centauri system in a flash.\nThe solution is for the probe’s sail to be redeployed upon arrival so that the spacecraft would be optimally decelerated by the incoming radiation from the stars in the Alpha Centauri system. René Heller, an astrophysicist working on preparations for the upcoming Exoplanet mission PLATO, found a congenial spirit in IT specialist Michael Hippke, who set up the computer simulations.\nThe two scientists based their calculations on a space probe weighing less than 100 grams in total, which is mounted to a 100,000-square-metre sail, equivalent to the area of 14 soccer fields. During the approach to Alpha Centauri, the braking force would increase. The stronger the braking force, the more effectively the spacecraft’s speed can be reduced upon arrival. Vice versa, the same physics could be used to accelerate the sail at departure from the solar system, using the sun as a photon cannon.\nThe tiny spacecraft would first need to approach the star Alpha Centauri A as close as around four million kilometres, corresponding to five stellar radii, at a maximum speed of 13,800 kilometres per second (4.6 per cent of the speed of light). At even higher speeds, the probe would simply overshoot the star.\nDuring its stellar encounter, the probe would not only be repelled by the stellar radiation, but it would also be attracted by the star’s gravitational field. This effect could be used to deflect it around the star. These swing-by-manoeuvres have been performed numerous times by space probes in our solar system. “In our nominal mission scenario, the probe would take a little less than 100 years – or about twice as long as the Voyager probes have now been travelling. And these machines from the 1970s are still operational,” says Michael Hippke.\nTheoretically, the autonomous, active light sail proposed by Heller and Hippke could settle into a bound orbit around Alpha Centauri A and possibly explore its planets. However, the two scientists are thinking even bigger. Alpha Centauri is a triple star system. The two binary stars A and B revolve around their common centre of mass in a relatively close orbit, while the third star, Proxima Centauri, is 0.22 light years away, more than 12,500 times the distance between the Sun and the Earth.\nThe sail could be configured so that the stellar pressure from star A brakes and deflects the probe toward Alpha Centauri B, where it would arrive after just a few days. The sail would then be slowed again and catapulted towards Proxima Centauri, where it would arrive after another 46 years − about 140 years after its launch from Earth.\nProxima Centauri caused a sensation in August 2016 when astronomers at the European Southern Observatory (ESO) discovered an exoplanet companion that is about as massive as the Earth and that orbits the star in its so-called habitable zone. This makes it theoretically possible for liquid water to exist on its surface – water being a key prerequisite for life on Earth.\n“This finding prompted us to think about the possibility of stopping a high-velocity interstellar lightsail at Proxima Centauri and its planet,” says René Heller. The Max Planck researcher and his colleague propose another change to the strategy for the Starshot project: instead of a huge energy-hungry laser, the Sun’s radiation could be used to accelerate a nanoprobe beyond the solar system. “It would have to approach the Sun to within about five solar radii to acquire the necessary momentum,” Heller says.\nThe two astronomers are now discussing their concept with the members of the Breakthrough Starshot Initiative, to whom they owe the inspiration for their study. “Our new mission concept could yield a high scientific return, but only the grandchildren of our grandchildren would receive it. Starshot, on the other hand, works on a timescale of decades and could be realized in one generation. So we might have identified a longterm, follow-up concept for Starshot,” Heller says.\nAlthough the new scenario is based on a mathematical study and computer simulations, the proposed hardware of the sail is already being developed in laboratories today: “The sail could be made of graphene, an extremely thin and light but mega-tough carbon film,” René Heller says. The film would have to be blanketed by a highly reflective cover to endure the harsh conditions of deep space and the heat near the destination star.\nThe optical and electronic systems would have to be tiny. But if you were to remove all the unnecessary components from a modern smartphone, “only a few grams of functional technology would remain.” Moreover, the lightweight spacecraft would have to navigate independently and transmit its data to Earth by laser. To do so, it would need energy, which it could harness from the stellar radiation.\nBreakthrough Starshot therefore poses daunting challenges that have so far only been solved theoretically. Nevertheless, “many great visions in the history of mankind had to struggle with seemingly insurmountable obstacles,” Heller says. “We could soon be entering an era in which humans can leave their own star system to explore exoplanets using fly-by missions.”']	['<urn:uuid:0d5e39de-547d-4c8a-acb4-697acf9d4100>', '<urn:uuid:9a6a97cb-1832-41de-a602-58365f4edba0>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T22:03:18.033296	11	67	3073
29	difference between minimally invasive shoulder surgery rotator cuff repair traditional therapy treatment methods	Both surgical and non-surgical methods are used to treat rotator cuff injuries. Minimally invasive shoulder surgery (arthroscopy) involves small incisions to insert a camera and surgical instruments, resulting in less risk, less post-operative pain, and shorter recovery times. Traditional therapy methods include physical therapy exercises to strengthen muscles and improve coordination, steroid injections to reduce inflammation, and NSAIDs medications for pain control.	['In the course of recent decades, the medicinal field has seen an ascent in negligibly intrusive medical procedure systems overall claims to fame. New advances in insignificantly obtrusive strategies mean various things for patients, including quicker recuperation times and fewer complexities. In the orthopaedic field, this has made an interpretation of into better approaches to supplant joints, repair sports wounds and treat complex musculoskeletal issues.\nShoulder wounds specifically are a common issue that may lead you to an orthopaedic doctor. Here’s an outline of what insignificantly intrusive shoulder medical procedure includes and what wounds and conditions it might be utilized to treat.\nKeeping in mind the end goal to see how insignificantly intrusive shoulder medical procedure works, it’s essential to have an unmistakable comprehension of what’s required with a negligibly obtrusive medical procedure. Though conventional medical procedure frequently requires a substantial entry point and critical injury to delicate tissue encompassing the treatment area, negligibly intrusive orthopaedic medical procedure endeavours to limit both the extent of careful cuts and in addition the injury to encompassing delicate tissue, while achieving the same careful objective. In orthopaedics, this can be the contrast between uncovering a complete joint through a vast, open entry point and utilizing little, directed cuts to find the problem area. This outcome in less hazard amid medical procedure, less post-agent torment and shorter recuperation times for patients.\nHow does minimally invasive shoulder surgery work?\nNegligibly intrusive shoulder surgery, likewise alluded to as shoulder arthroscopy, includes making at least two little entry points in the shoulder and embeddings a camera mounted on a thin, tube (called an arthroscope) keeping in mind the end goal to picture the inside of the joint. Little careful instruments are then embedded through alternate entry points, and the whole activity is communicated to a TV screen. The specialist utilizes this augmented picture to help direct the instruments and make whatever repairs are important. This sort of methodology can be utilized to calm difficult indications caused by harm to the rotator sleeve ligaments, labrum, articular ligament, and other delicate tissues encompassing the shoulder joint.\nAm I a possibility for insignificantly obtrusive shoulder surgery?\nFor some orthopaedic shoulder wounds and conditions, nonoperative administration and conventional medical procedure are as yet the favoured techniques for treatment. Be that as it may, when surgery is essential, you ought to counsel with an accomplished orthopaedic specialist who can clarify your choices and decide if a negligibly intrusive method would be best for your necessities.\nShoulder issues that might be treated with bear arthroscopy incorporate the accompanying.\nRotator Cuff Injury\nAn arthroscopic rotator cuff restore can be wanted when you have suffered from a rotator cuff tear, both as a result of a disturbing harm (acute tear) or due to wear and tear over the years (degenerative tear). it can also be encouraged for a rotator cuff harm that has no longer advanced after numerous months of nonsurgical remedy.\nBiceps Tendon Injury\nThe biceps ligament, in reality, begins in the shoulder joint. at the off threat that it is torn or harmed then it very well may be an important wellspring of torment or inconvenience. Negligibly intrusive shoulder clinical procedure can either repair a torn biceps ligament or virtually pass it out of the shoulder joint so it isn’t always any more a wellspring of anguish.\nOn the factor whilst bone goads create in the shoulder because of ligament degeneration and bone-on-bone joint harm, a method referred to as a bone goad expulsion might be cautioned. this technique may be performed arthroscopically to expel the difficult goads and help calm side outcomes. Without a remedy, bone goads may also set off rotator sleeve tears and tendinitis within the shoulder.\nA shoulder that rather or definitely disjoins because of shoulder insecurity or lousy harm can also require a shoulder precariousness repair. carried out arthroscopically, this kind of technique can be applied to reattach and fix unfastened tendons in the shoulder to assure valid assist and help count on destiny separations.\nAn arthroscopic labral restore or expulsion might be required on the off risk that you have skilled a labral tear. Contingent upon whether or not the shoulder is steady or insecure due to the damage, this restore might also include simply evacuating the torn fold/segment or repairing and reattaching the ligament in conditions where the ligament is altogether withdrawn or at the off hazard that it stretches out into the biceps ligament.\nAt the off chance that you are experiencing terrible shoulder harm or degenerative shoulder condition, the preliminary step to discovering comfort starts off evolved with a council with an orthopaedic authority. Within the event that mild treatment picks neglect to ease your manifestations and medical method is absolutely vital, it merits getting a few records about whether undergo arthroscopy is appropriate for you.', 'Rotator Cuff Conditions\nRotator cuff disorders are one of the most common causes of shoulder pain. Injuries to the rotator cuff tendons are frequently seen in workers with physically demanding jobs, athletes, and physically active people. Some common examples include painters, welders, carpenters, swimmers and tennis players. It can be due to a one time traumatic injury or from repetitive use of the shoulder. Degenerative rotator cuff tears associated are common, are a major cause of shoulder pain, and can be aggravated by work or sports. Rotator cuff tears generally do not heal without treatment, and results in pain and disability. Many daily activities, like washing your hair or getting dressed, may become painful and difficult.\nPatients with a rotator cuff injury often experience pain when lifting or lowering their arm, muscle weakness and atrophy. These symptoms may appear suddenly after the tendon is torn, or develop gradually after long-term wear. The pain associated with a rotator cuff injury is often described as a deep shoulder pain or ache, disturb sleep, limit activities of daily living and cause weakness. Some common rotator cuff symptoms include pain, weakness, restricted motion, catching, locking and a feeling of instability. The symptoms are usually worse in certain positions, such as reaching backward to fasten a seat belt or pick up a briefcase out of the back seat. Symptoms can also be worse when the arm is elevated overhead, especially if there is weight on the arm, such as when picking up a stack of plates out of a cupboard. Overhead activities like pitching, throwing, playing tennis or playing racquet-ball commonly worsen symptoms.\nA patient’s History and Physical examination is the best way to initially evaluate any shoulder condition. It is important to make sure that the pain is actually coming from the shoulder and not from the neck or elbow. After discussing your symptoms and medical history, Dr. Li will examine your shoulder. He will check to see whether it is tender in any area or whether there is a deformity. To measure the range of motion of your shoulder, he will have you move your arm in several different directions. Your shoulder strength will also be evaluated.\nYour doctor will check for other problems with your shoulder joint. He or she may also examine your neck to make sure that the pain is not coming from a “pinched nerve,” and to rule out other conditions, such as arthritis.\nX-rays: Although plain x-rays do not show the rotator cuff tendons, they are helpful to look for calcifications, arthritis or bone spurs that can cause rotator cuff tears.\nMSK Ultrasound: An ultrasound scan can be helpful for diagnosing rotator cuff problems. It allows the soft tissues of the shoulder to be seen, and can detect fluid and damage to tendons and muscles.\nMRI: This is the most common imaging method to diagnose rotator cuff tears. It can be used to look for tears or inflammation of tissues and to help determine the size and character of the tear so proper treatment can be recommended.\nDepending on several factors including patient activity, quality of the remaining rotator cuff tendon and size of the rotator cuff tear; the problem may respond well to therapy, injections and medication or it may require arthroscopic surgery.\nTherapy– The purpose of Physical Therapy or Occupation Therapy is to improve symptoms and restore function. The approach will depend on whether you have a short-term (acute) problem or a long-standing (chronic) condition. The goal of the therapy program is to “functionally heal” the shoulder and could include:\n- exercises to strengthen weakened muscles, improve their co-ordination and restore function\n- advice on improving shoulder, neck and spine posture\n- exercises to ease or prevent stiffness\n- exercises to increase the range of joint movement\n- applying adhesive tape to the skin to reduce the strain on the tissues and to help increase your awareness of the position of the shoulder and shoulder blade\n- manual treatments to the soft tissues and joints.\nSteroid Injections– A steroid injection given along with a local anesthetic can reduce pain very quickly. The injection reduce inflammation and allow you to move your shoulder more comfortably, though you shouldn’t use your shoulder for anything too strenuous the first two days after an injection. The pain relief should allow you to do your therapy exercises more easily. Repeated injections (more than two or three) aren’t usually recommended. If the problem keeps coming back, then other treatments should be considered.\nMedications– NSAIDs (Non-steroidal anti-inflammatory drugs) are frequently used at the onset of rotator cuff injuries to reduce inflammation and control pain. They are available by prescription as well as over-the-counter. Some common over-the-counter NSAIDs include Ibuprofen (Motrin) and Naproxen (Aleve).\nArthroscopic Rotator Cuff Surgery– can be performed to repair torn tendons following a rotator cuff injury. Because it is minimally invasive, Arthroscopic Rotator Cuff Repair results in a much quicker and less painful recovery. Arthroscopic Rotator Cuff Repair has been performed successfully in an outpatient setting for many years to help people of all ages regain maximum use of their shoulder and resume normal routines and physical activities. Arthroscopic repair is superior to the traditional open repair and is successful even with very large tears, so there is no reason to settle for an open repair. Rotator Cuff surgery is performed on an outpatient basis which means going home the same day. A shoulder sling/orthosis will be used to protect your repair when you are in a public place, and may be used to protect your shoulder during sleep.\nPost-Operative Physical/Occupational Therapy– is critical to a fast and strong recovery, so it is typically started just a few days after surgery. The typical length of post-operative therapy is 3 months but can vary depending on the size of the tear, condition of the rotator cuff, patient’s job requirements, and other factors. Initially the therapy will focus on restoring your passive range of motion, and after 4-6 weeks then active range of motion is started. After active range of motion is restored then the focus shifts to strengthening the rotator cuff muscles. It is very important to continue a home exercise program for 12-18 months because it takes that long to regain your strength.']	['<urn:uuid:f90197c6-ea1a-4cb4-ace1-bb58a20d59df>', '<urn:uuid:7b426d4c-0a19-489f-8844-6802482cf02d>']	factoid	direct	long-search-query	similar-to-document	comparison	expert	2025-05-12T22:03:18.033296	13	62	1845
30	As someone interested in Chinese culinary history, I'm curious about how Xi'an's location influenced its food culture and how it fits into the broader Chinese cuisine landscape?	Xi'an, being part of the central northwest area of China, is characterized by its preference for spicy food, which differs from the sweeter flavors found in southern China. The local specialties include dishes like Yangrou Paomo (pita bread soaked in lamb stew), cold noodles, Rou Jia Mo (Chinese hamburger), and Bouza (steamed buns). This regional variation aligns with the historical development of Chinese cuisine, where by Tang Dynasty (618-907 AD), distinct northern and southern characteristics had emerged, with northern regions generally preferring saltier flavors while southern regions favored sweeter tastes.	['Spring In China\nFive LBCC students went to China to teach English, explore the country, and receive a full intercultural experience. Cynthia de la Torre shares her experience.\nHow I became involved:\nLast summer a good friend of mine invited me to apply to an English as a second language teacher assistant position at Xijing University through a partnership program with Linn-Benton Community College for either the upcoming Fall term or Spring 2017 term. The role would consist of assisting LBCC’s Dr. Rob Lewis’ classroom and implementing our higher education skills into a teaching curriculum. At first I wasn’t overly eager at the thought of going away for three months to a whole different country in a far away continent, however, after my friend’s persistence, I decided to apply anyways. Fast forward to early August: Rob emailed me my acceptance to the upcoming Spring term trip and after discussions with my family I decided to accept the position. In the end my friend had a few complications which didn’t allow her to leave with an extended absence and although I had more anxiety than enthusiasm, before I knew it I was on a flight to Shanghai and then to Xi’an, China!\nThe teaching assistant experience:\nThe group of five of us were split into partners with one of us assisting the English professors by ourselves. As assistants we had some influence in the learning module, encouraged the students to practice their oral English and overall learned to adapt to new situations. Although I’m not an education nor an English major it wasn’t too difficult to lead group activities or facilitate the learning process.\nNevertheless, this experience has definitely reminded me how complex the English language really is. It’s a challenge to question the assumptions and unspoken rules that we generally don’t consider often if we are fluent in a particular language. Most of our students have been learning English since they were in primary school, however in a highly populated country that speaks Cantonese or Mandarin Chinese, the opportunities to practice their English are very limited, so the students often felt hesitant to engage in conversation.\nWe definitely related to these circumstances when the university later provided Mandarin Chinese lessons for us. We studied the pinyin phonetic sounds, characters, and pronunciation. Despite our classes, it was an every day trial to have a successful line of communication. Being in China, however, has definitely required that we practice our Chinese and forced us to get out of our comfort zone and find other, non-verbal ways to communicate. Once we are back in the United States, most of us, including myself, aspire to continue exercising our Chinese.\nDespite the language barriers, I have found that the ability to teach is unquestionably a gift to humankind. The amount of energy a teacher dedicates to their students is outstanding. China recognizes a teacher’s hard work ethic and upholds a deep respect for their service to society. It certainly made this experience much more rewarding, feeling appreciated in such a way. I’m unsure if I will pursue an education major in the future, but I definitely have a deeper understanding of the tasks teachers are faced with.\nLiving in Xi’an is surreal. It sits squarely in the middle of the Shaanxi province and once used to be the nation’s capitol and occupied the ruling houses of many dynasties, such as the Zhou and Tang dynasty. Before adventuring out here, I read on many guides that have described Xi’an as a “living history book,” and I definitely feel the 3,000 year ancestral presence whenever I’m out in the city.\nThe Ming Dynasty’s Bell Tower, for example, is literally geographically the center of the city. The tower was originally built to rule over the city’s countryside and oversee an potential incoming attacks. Being on top of that fortress like building allows you to see north, west, south, and east of the city and makes you wonder the immensity that surrounds this beautiful ancient city. I almost couldn’t believe that I really was in China.\nThe Great Muslim Mosque is a little opposite in its environment. The mosque is still active despite dating back 1,300 years when Islam was first introduced in the Tang Dynasty. It’s something truly remarkable to be standing in ground that has been seen and traveled through for centuries. The place of worship is stylized with a blend of traditional Chinese and Muslim architecture. From the outside it seems relatively small, but once you step into the complex of courtyards the place is quite large. All around me I heard so many foreign tongues as group after group of tourists bustled in and out of the mosque. Although there were quite a lot of people, the mosque remained still and serene.\nFinally, one of Xi’an’s most widely characterized and beloved historical places is the Qin Tomb Terracotta Warriors and Horses. The Museum is a bit closed off into a well preserved area away from the inner city and is divided into four different sections: vault one, vault two, vault three and an exhibition hall.\nAs we made our way through each vault, I kept thinking about Xi’an being a breathing living history book. I had seen pictures before the trip but experiencing it in real life was so much more magical. The individuals belonging to the infantry, charioteers, and cavalry were all constructed out of clay in the span of forty years to guard Emperor Qin Shi Huang tomb in the afterlife. Although the artwork carved out in each warrior is immaculate and truly special to look at, I found myself thinking about the 720,000 builders who tirelessly worked on this massive project. It was the people’s work, the people’s art. It reflected the people living at that time. It seems much more impressive when you realize individuals left almost a lifetime dedicated to the warriors.\nOne of our good friends, Sarah, who is also a Xijing University student, invited our group to her hometown, Ganquanbei for the Dragon Boat Festival holiday. Before arriving to her hometown we stayed in the neighboring city of Ya’nan for about two days. This city reminded me of Portland. It had the big city feelings, but it also contained that smallness. Surrounding the inner city were large massive mountain and peak ranges; almost like an oasis.\nBefore arriving Sarah talked to us about the lack of foreign travelers there and they might be taken back whenever they saw us. This happened to be true; we ended up being stopped quite a bit and asked where we were from. Which is interesting, because it seems that there is this perception that Americans look like European descendants. It was good to have a little bit of diversity within our group to challenge that assumption about America. Regardless, a few people even took pictures from a distant. It was kind of fascinating to feel very unknown and seen at the same time. Nevertheless, people’s curiosity was much more friendly than anything else.\nOn the first night we walked around the city and ended up climbing a summit to see a light show the city puts on every night. Because there isn’t much tourism in this area, the light show is a significant gift to the people of Yan’an. Because Yan’an was once the military headquarters of Mao Zedong’s chinese communist party the light show shows images of the labor movement and the struggle for the workers liberation.\nThe next day we traveled about four hours away to visit the Hukou waterfall, an extension of the Yellow River which often is referred to as the” cradle of civilization” because it connects to so many other provinces across China, irrigating and providing life to its people. Being in the presence of such an ancient and majestic body of water was truly a memorable experience. I had no words. I was in awe.\nI was surprised the most by the people. There is so much kindness! Everyone on campus is very welcoming and genuinely interested in getting to know us, as individuals. The sense of community Xijing University embodies is quite surprising. In the United States, the school system and how the student approaches their academic life is for the most part individualistic. It’s pleasantly surprising to me that in China, the relationship of what it means to be a “classmate,” is much more intimate. Students help each other succeed beyond study groups–students within the same major and same age group are so much more present in each other’s lives. If we are classmates, we are family.\nMusic play a huge role in many of the students lives. A Xijing student who has quickly become a close friend of ours, Crystal is studying Chinese International Education, but in her spare time she takes lessons in a Chinese traditional instrument, Pipa, which resembles a westernized guitar or cello.. Another friend, Amy plays the Guzheng, another Chinese traditional string instrument played in a piano like fashion. Everyone in Xijing’s music club is incredibly talented. While I was in the Music & Art department I was introduced to Lucas, a guitar teacher who had studied abroad in Australia for a couple of years. After exchanging conversation I later learned he had a cousin residing in Portland, Oregon! What a small world!\nWithin the music floor there are several large choir rooms that Crystal asked me to go to. In one, a small group of students were hanging out. One of them was singing passionately the lyrics on a huge television screen. When I entered the room–it went still for a split second and then they quickly approached me and asked if they could have a selfie. They each waited patiently to take a turn. Later we exchanged WeChat (a social facebook version app very popular in China) and they invited me to sing with them. Which I attempted, to say the least.\nNevertheless, being surrounded by music was one of the most incredible situations I found myself in. Here the students use music to alleviate stress and shake off moods, almost like therapy. Much later we had the opportunity to attend three student performances–two concerts and one dance show. Even though Xijing University is more of a STEM, business, and Education major centered school, the artistic talent among these students leaves me amazed.\nXi’an is part of the central northwest area of China, which means people generally like spicy food a lot more here than the south part of China, where sweeter food from the sea is more popular. Growing up in a Mexican background I was very happy to notice this the first few days, but I later realized that Northern Chinese spices are a lot different than Latinx spices. It was definitely a culture shock when we realized we loved Hot Pot but could no longer enjoy it after some digestion issues. Not only is the soup you cook the raw vegetables and meat in spicy water but the heat is through the roof. Hence, the name hot pot.\nAnother dish we really enjoyed is Yangrou Paomo, pita bread soaked in lamb stew. The flavors of garlic, spicy sauce, and sesame oil all make this dish mouthwatering.\nCold noodles is probably one of my favorites dishes to eat in the hot weather. It’s wonderful because it’s a local speciality here in Xi’an. They’re made from either rice or flour. They’re usually accompanied with green bean sprouts and sesame paste.\nRou Jia Mo is often referred as a “Chinese hamburger.” It’s usually pork stewed for long periods of time in spices and seasonings between two fried pita bread buns. It’s incredibly delicious and is widely acclaimed as delicacy among the Chinese people.\nBouza, is one of our dearest breakfast favorites. I will most certainly miss them when I come back to the United States. They’re basically soft dough steamed into buns filled with pork, chicken, beef, red bean, or lamb. Students here on campus eat them as a quick breakfast usually with a cup of dou jiang, hot soy bean milk.\nAlthough China is very far away I’ve had a few moments that remind me of home, like the weather and school life. I’ve also met many wonderful people and established relationships in this short period of time that I feel will be life long. I’m surprised that I haven’t felt as homesick and I definitely believe that it’s because the community you travel with and meet along the way build a home for you. I’m so grateful that LBCC and the Xijing University partnered up to create this experience. Not only am I so much more aware of the world’s immensity but also where I can see myself. I’m definitely coming back to China, and I’m definitely not done exploring this wide, diverse world.\nStory by Cynthia de la Torre', 'How many Chinese cuisines are there? The most influential and typical known by the public are the “8 Chinese Cuisines”. Actually Chinese cuisine has more than 8 genres. China is large and the climate, ingredients, history and dining customs vary from place to place, leading to the differences in cooking methods and dish flavors and forming the different cuisines.\nAs early as the Shang Dynasty (17th century BC - 1046 BC) and Zhou Dynasty (1046 - 256 BC), China’s dietary culture has taken shape. In the Tang Dynasty (618 - 907 AD) and Song Dynasty (960 - 1279 AD) , the northern and southern food already had their own characteristics: generally sweet in south and salty in north. By the early Qing Dynasty (1644 - 1911 AD), four most influential cuisines prevailed. What are the four main styles of Chinese cuisine at that time? They are cuisines of Shandong, Sichuan, Guangdong and Jiangsu respectively. By the end of the Qing Dynasty, the four new cuisines - Zhejiang cuisine, Fujian cuisine, Hunan cuisine, and Anhui cuisine appeared; together with former four cuisines, they were listed as “8 Chinese Cuisines”.\nToday, the hot and spicy Sichuan and Hunan cuisines are the most popular in China; the Cantonese cuisine with light taste and delicate cooking methods is not only sought after in China, but also popular around the world.\n1. Sichuan Cuisine / Szechuan Cuisine / Chuan Cuisine\nChinese: 川菜 chuān cài\nPopular in: China’s Sichuan Province, Chongqing\nFeatures: hot and spicy, mouth-numbing; using a lot of pepper, chili, garlic, and Pixian bean chili paste.\nOriginated in Southwest China, Sichuan cuisine is the most popular cuisine in China. Sichuan dishes are famous with its spicy, pungent and hot flavor and red color. Sichuan chefs prefer seasonings like various pepper and chili, garlic and broad bean paste which is the soul of Sichuan dishes. Hot Pot is the most popular Sichuan dish in China and is the first choice when dining together with friends.\nNotable Sichuan Dishes: Kung Pao Chicken, Ma Po Tofu, Dandan Noodles, Fish-Flavored Shredded Pork, Sliced Pork in Hot Chili Oil, Sliced Beef and Ox Organs in Chili Sauce\n2. Cantonese Cuisine / Guangdong Cuisine / Yue Cuisine\nChinese: 粤菜 yuè cài\nPopular in: China’s Guangdong Province, Hong Kong and Macau\nFeatures: mild, fresh and natural taste; widely use of materials and rich cooking methods\nCantonese cuisine is welcomed by people around the world and is the representative Chinese cuisine. The cooking methods of Cantonese dishes are always complex, flexible, exquisite, and creative. Most Cantonese dishes taste mild and fresh, preserving the natural taste of food materials. The food materials are extremely rich which include poultries, birds, sea food, land animals and various vegetables and fruits.\nNotable Cantonese Dishes: Ah Yat Abalone, White Cut Chicken, Cantonese Roasted Goose, Roasted Suckling Pig, Char Siu (Chinese BBQ Pork), Poached Lobster in Soup, Slow-cooked Soup, Beef Chow Fun (Beef Ho Fun)\n3 Major Styles in Cantonese Cuisine: Guangzhou Cuisine, Teochew Cuisine (Chaoshan Cuisine), Hakka Cuisine(Dongjiang Cuisine)\n3. Hunan Cuisine / Xiang Cuisine\nChinese: 湘菜 xiāng cài\nPopular in: China’s Hunan Province\nFeatures: spicy, hot, salty; preference for chili & fermented soya beans; dark dish color\nOriginated in Eastern Zhou Dynasty (770 - 256 BC), Hunan cuisine, also Xiang cuisine, is another cuisine popular throughout China with the spicy taste. Hunan dishes are even hotter and spicier than Sichuan cuisine since they have even more dried or fresh chili in dishes. Hunan people also invent sour and spicy taste and numb and spicy taste, and they like pickled and preserved vegetables and meats very much.\nNotable Hunan Dishes: Steamed Fish Head with Chopped Chili, Steamed Preserved Meat, Chairman Mao’s Red Braised Pork, Fried Pork with Chili, Tasty Lobster, Changsha Stinky Tofu, Hunan Rice Noodles, Spicy Salted Duck\n4. Shandong Cuisine / Lu Cuisine\nChinese: 鲁菜 lǔ cài\nPopular in: China’s Shandong Province and Southern Liaoning Province\nFeatures: umami, salty, sweet and sour flavors; using much soup, scallion, ginger and garlic; good at cooking sea food\nOriginated in the Spring and Autumn Period (770 - 476 BC), Lu cuisine is the only self-originated cuisine among 8 Chinese cuisines which has the longest history of 2,500 years, hence seen as the No. 1 of 8 great cuisines of China. Shandong dishes pay much attention to the quality and natural taste of food. Soup and scallion are key seasonings and sea food plays a crucial role in Shandong cuisine. Most Shandong dishes taste fresh, umami and moderately salty.\nNotable Shandong Dishes: Sweet and Sour Carp, Braised Sea Cucumber with Scallion, Braised Intestines in Brown Sauce, Braised Prawns in Oil, Steamed Stuffed Tofu, Dezhou Braised Chicken, Caramel Sweet Potato, Omelet Chicken Slices, Four Joy Meatballs…\n5. Jiangsu Cuisine / Su Cuisine\nChinese: 江苏菜, 苏菜, sū cài\nPopular in: China’s Jiangsu Province\nFeatures: umami, mild, slightly sweet, and natural; exquisite cuttings and precise control of heat; delicate dish appearance\nJiangsu cuisine, the representative of South China food, is famous for its delicate appearance and lightly sweet, umami, natural and mild taste. Jiangsu dishes preserve much original flavor of food materials. The main food materials are mainly freshwater fishes, river fish and sea food. The cutting is extremely exquisite and changeful and the control of heat is precise. The dishes are always served delicately.\nNotable Jiangsu Dishes: Nanjing Salted Duck, Sweet and Sour Mandarin Fish, Braised Meat Balls in Brown Sauce, Stewed Tortoise and Chicken, Beggar’s Chicken, Yangzhou Fried Rice, Boiled Shredded Dry Bean Curd, Pork Trotter Aspic Jiangsu Style, Mutton in Fish Maw\n4 Major Styles in Jiangsu Cuisine: Nanjing Style, Huaiyang Cuisine, Suxi Style, Xuhai Style\nRead more about:\n6. Zhejiang Cuisine / Zhe Cuisine\nChinese: 浙菜 zhè cài\nPopular in: China’s Zhejiang Province\nFeatures: light and fresh, crisp and tender; good at controlling fire heat; prefer yellow wine as special seasoning\nPopular in Zhejiang, Zhejiang cuisine is often lightly seasoned with appealing appearance. Zhejiang cuisine chefs prefer fresh food materials like sea food, freshwater fishes, and vegetables in season and they are extremely strict in the selection of raw materials. In addition, Zhejiang cuisine is well-known for its varied cooking methods and frying is the most frequently used.\nNotable Sichuan Dishes: West Lake Fish in Vinegar Gravy, Fried Shrimps with Longjing Tea, Dongpo Pork, Braised Bamboo Shoot, Deep-Fried Beancurd Rolls Stuffed with Minced Tenderloin, Sliced Lotus Root with Sweet Sauce, West Lake Beef Soup, Steamed Pork with Rice Flour in Lotus Leaves\n7. Fujian Cuisine / Min Cuisine\nChinese: 闽菜 mǐn cài\nPopular in: China’s Fujian Province\nFeatures: light, fresh, sweet and sour; special condiment red vinasse; woodland delicacies and sea food widely used for ingredients\nOriginated in Fuzhou in Fujian, Fujian cuisine dishes are light, fresh, mellow and non-greasy in general. The food materials used are pretty rich, containing both materials in land and seas. The common seasoning include red vinasse, sugar and vinegar. Like Jiangsu cuisine, Fujian cuisine attaches great importance to cutting techniques as well.\nNotable Sichuan Dishes: Buddha Jumps over the Wall, Braised Sea Clam with Chicken Soup, Sweet and Sour Litchis, Sliced Whelk in Red Vinasse, Fuzhou Fish Balls, Oyster Omelette, Drunken Ribs\n8. Anhui Cuisine / Hui Cuisine\nChinese: 徽菜 huī cài\nPopular in: China’s Anhui Province\nFeatures: salty and fresh, light, preserving the original flavor of ingredients; preference for hams; light cooking methods including braising, stewing and steaming\nPopular in Anhui Province, Hui dishes taste salty, fresh, and light and have the unique flavor of ingredients. Hui cuisine attaches importance to dietetic invigoration with natural ingredients. It is worth notice that Anhui cuisine dishes have the effect of nourishing body for multiple ingredients with varied nutrition are matched scientifically.\nNotable Sichuan Dishes: Stinky Mandarin Fish, Fried Hairy Tofu, Steamed Partridge, Stewed Bamboo Shoots of Wenzheng Mountain, Huangshan Stewed Pigeon, Li Hongzhang Chop Suey, Stewed Soft-Shelled Turtle with Ham, Braised Masked Palm Civet\nFurther Reading on Chinese Cuisine History\nIn Shang and Zhou Dynasties, China had developed its dietary culture. By Tang and Song Dynasties, the dining customs in the north and south developed respectively. People in North liked sweet food while in South liked salty food.\nDuring the Southern Song Dynasty (1127 - 1279 AD), the northerners moved to the south in large numbers. Gradually, the northern food culture had influenced the south and formed its own style in the south. By the end of the Ming Dynasty, the Chinese cuisine was divided into Beijing style, Jiangsu style and Cantonese style. Beijing-style dishes were salty, and the Jiangsu-style and Cantonese-style dishes were sweet.\nBy the early Qing Dynasty, there existed four most influential cuisines: Shandong cuisine, Sichuan cuisine, Cantonese cuisine and Jiangsu cuisine.\nLater, the dining habits of various parts of China developed considerably. Sichuan cuisine was further divided into Sichuan cuisine and Hunan cuisine. Cantonese cuisine was divided into Cantonese cuisine and Fujian cuisine. Jiangsu style cuisine was divided into Jiangsu cuisine, Zhejiang cuisine and Anhui cuisine. By late Qing Dynasty, Sichuan, Cantonese, Hunan, Shandong, Jiangsu, Zhejiang, Fujian, and Anhui cuisines gradually become the most influential and popular, finally forming “8 Chinese Cuisines”.']	['<urn:uuid:b38082e6-f4fd-49e1-a328-3dbcad79487c>', '<urn:uuid:1e80c3d3-f3e2-488d-8a30-e0fc0563a7e8>']	factoid	with-premise	verbose-and-natural	distant-from-document	three-doc	expert	2025-05-12T22:03:18.033296	27	90	3658
31	How does Venus appear from Earth, and what makes its rotation unique?	From Earth, Venus exhibits phases similar to the Moon and appears as the brightest object in the sky after the Sun and Moon, reaching magnitude -4.7. It shows its full range of crescent and gibbous phases, being brightest when appearing as a crescent. As for its rotation, Venus has a unique retrograde rotation period of 243 days, spinning from east to west (opposite to other planets), causing the Sun to rise in the west and set in the east. This backwards rotation might be due to core-mantle friction combined with atmospheric tides, though some theories suggest it could have been caused by impact with planetesimals or celestial bodies during its formation.	"['Phase in sinusoidal functions or in waves has two different, but closely related, meanings. One is the initial angle of a sinusoidal function at its origin and is sometimes called phase offset or phase difference. Another usage is the fraction of the wave cycle that has elapsed relative to the origin.\nThe phase of an oscillation or wave refers to a sinusoidal function such as the following:\nwhere , , and are constant parameters called the amplitude, frequency, and phase of the sinusoid. These functions are periodic with period , and they are identical except for a displacement of along the axis. The term phase can refer to several different things:\nIt can refer to a specified reference, such as , in which case we would say the phase of is , and the phase of is .\nIt can refer to , in which case we would say and have the same phase but are relative to their own specific references.\nRefers to the periods of time during which a planet\'s surface reflects different amounts of sunlight, revealing different portions of the planet\'s surface from the perspective of a given point in space.\nThe two inferior planets, Mercury and Venus, which have orbits that are smaller than the Earth\'s, exhibit the full range of phases as does the Moon, when seen through a telescope. Their phases are ""full"" when they are at superior conjunction, on the far side of the Sun as seen from the Earth. (It is possible to see them at these times, since their orbits are not exactly in the plane of Earth\'s orbit, so they usually appear to pass slightly above or below the Sun in the sky. Seeing them from the Earth\'s surface is difficult, because of sunlight scattered in Earth\'s atmosphere, but observers in space can see them easily if direct sunlight is blocked from reaching the observer\'s eyes.) The planets\' phases are ""new"" when they are at inferior conjunction, passing more or less between the Sun and the Earth. (Sometimes they appear to cross the solar disk, which is called a transit of the planet.) At intermediate points on their orbits, these planets exhibit the full range of crescent and gibbous phases.\nPhase/feɪz/ is a UK-based rock band formed in Larissa, Greece in 2003. They made a digital single (""Perdition"") for Microsoft’s Playlist Seven program in Feb 2010, they released one album In Consequence in which Duncan Patterson, better known from his work with the British rock band Anathema, took part. According to Reverbnation\'s charts, Phase is ranked 1st nationwide in Greece. Phase climbed on #12 of Billboard\'s fastest rising acts powered by Next Big Sound in the week of June 14th 2012.\nPhase was formed in 2003 and played intermittently until 2008 when they began recording their album In Consequence and performed at concerts. Soon after they had a functional line up they recorded Perdition which was the very first track Phase ever recorded to be later a part of Microsoft\'s Playlist 7 campaign.\nPhase has been the subject of several alternative rock publications and has generally been well received by its critics. They have had interviews in various magazines and radiostations and have received praise for their shows.\nA phaser is an electronic sound processor used to filter a signal by creating a series of peaks and troughs in the frequency spectrum. The position of the peaks and troughs of the waveform being affected is typically modulated so that they vary over time, creating a sweeping effect. For this purpose, phasers usually include a low-frequency oscillator.\nThe electronic phasing effect is created by splitting an audio signal into two paths. One path treats the signal with an all-pass filter, which preserves the amplitude of the original signal and alters the phase. The amount of change in phase depends on the frequency. When signals from the two paths are mixed, the frequencies that are out of phase will cancel each other out, creating the phaser\'s characteristic notches. Changing the mix ratio changes the depth of the notches; the deepest notches occur when the mix ratio is 50%.\nThe definition of phaser typically excludes such devices where the all-pass section is a delay line; such a device is called a flanger. Using a delay line creates an unlimited series of equally spaced notches and peaks. It is possible to cascade a delay line with another type of all-pass filter. This combines the unlimited number of notches from the flanger with the uneven spacing of the phaser.\nJade Warrior\'s second of the four Island albums was dedicated to ""the last whale"". It had no recurring theme and was marked by a slightly jazzier feel than its predecessor, carrying a listener ""through dawn-lit countryside full of birdsong, downriver to the ocean, and out among the great whales"". Describing the band\'s musical vision at the time as ""increasingly exotic"", AllMusic found the Island albums ""dreamlike, pushing a lighter jazz sound to the forefront"", featuring ""myriad percussive sounds but drum kits were rarely in evidence"". ""The band liked to create a soothing, ethereal feel, then shatter it with gongs and unexpectedly raucous electric guitar, usually from guest David Duhig, Tony\'s brother. The albums featured occasional celebrity guests such as Steve Winwood, but Jade Warrior had a style of its own"", critic Casey Elston wrote.', 'The Venus Transit 2004\n... Extended InfoSheet D3\nVenus\' orbit and visibility - day, night and seasons on Venus\nWith a mean distance of about 108 million kilometres or 0.72 AU, Venus\nis the second closest planet to the Sun and our nearest planetary\nneighbour. Its orbit about the Sun takes 225 days with its distance\nfrom the Sun being almost three-quarters of the Earth\'s.\nVenus and its position in the solar system\nThe Earth\'s sister planet revolves around our central star in an\nelliptical orbit that is the most circular of any planet. The\ndifference between its aphelion (the orbital point farthest from the\nSun) and perihelion (the orbital point where a body is closest to the\nSun) is only 1.5 million km, which gives its orbit an eccentricity -\nthat is the ratio of the distance between the foci of an ellipse to\nthe length of its major axis - of 0.007, the lowest of any planet. For\ncomparison, the eccentricity of the Earth\'s orbit is 0.0167. The\ninclination of the Venusian orbit to the plane of the Ecliptic (the\nEcliptic is the projection of the Earth\'s orbit onto the celestial\nsphere) is about 3° 24\'.\nThe inclination of the planetary orbits to the plane of the Ecliptic\nWith an orbital speed of about 35 km/s (the Earth moves at 30 km/s), the siderial and synodic periods of Venus are about 225 and 584 days respectively. The term ""siderial period"" means the interval of time one body needs to complete one revolution around another body with reference to the stars and the ""synodic period"" is the interval of time until Venus is again in the same position relative to the Sun, as seen from the Earth. The orbit of Venus which lies inside the Earth\'s orbit takes it closer to Earth than any other planet. So Venus can be very easily seen from the Earth. It is often called the Morning or Evening Star and it is often the brightest object in the sky, with the exception of the Sun and Moon. At its brightest it reaches magnitude -4.7, far brighter than any other planet.\nThe phases of Venus\nLike all other planets, including our own Moon, Venus shines by\nreflected sunlight. Moreover, as its orbit is inside the Earth\'s,\nVenus displays phases which are very similar to those of the\nMoon. Galileo Galilei was the first astronomer who discovered this\nphenomenon by using a telescope and it helped him to confirm that the\nplanets are bodies like the Earth. The entire set of Venusian phases\ncould not be explained with the Ptolemaic Earth-centred system. The\nmain difference is that, because of the changing distance between\nEarth and Venus, its apparent size changes considerably more than does\nPhases of Venus\nAnother difference between the phases of Venus and the Moon is that, seen from the Earth, it is not normally possible to observe Venus when it is ""full"" because it is then on the other side of the Sun (at superior conjunction, see below) and located very close to the Sun in the sky. Venus is brightest when it appears as a crescent. That is the case when Venus is in the evening sky and it appears to move from its maximum distance, called ""greatest elongation"", back towards the Sun. When it appears in the morning sky, it is brightest before greatest elongation.\nConfigurations of Venus\nVenus is the closest planet to the Earth at inferior conjunction, a distance of 42 million km. The term ""inferior conjunction"" refers to the configuration when an inner planet passes between the Sun and the Earth. Venus appears in front of the Sun in the sky and it can sometimes happen that Venus transits the Sun\'s disk. But it is a rare phenomenon, because it occurs only when Venus is at inferior conjunction close to one of its nodes (See: What is the significance of the transit of Venus?)\nAt superior conjunction, Venus is on the opposite side of the Sun and nearly 257 million km from the Earth. This great difference in distance from the Earth produces a large variation in its apparent size. At superior conjunction, Venus\' apparent diameter is about 10"" and at inferior conjunction about 64"".\nConfigurations of Venus\nThe maximum elongation of Venus is about 47 degrees. Venus is a remarkable object in the night sky at its brightest, 35 days before or after inferior conjunction, when one third of the visible surface is illuminated. Under favourable conditions it is even possible to see the crescent shape of Venus with binoculars.\nThe rotation of Venus\nThe rotation period was unknown for a long time. Observers in the past saw faint hints of structure in the clouds, which seemed to indicate that these clouds might circle the planet in about four days, moving in the opposite sense to Venus\' orbital revolution. Finally in 1962, radar waves penetrated the clouds and the measurements revealed that the rotation period is 243 days in a direction opposite to the other planets; this backwards motion is called retrograde motion, to distinguish it from prograde (forward) rotation.\nThe planet spins from east to west, rather than west to east, like the Earth; so on Venus, the Sun (though invisible through the clouds) rises in the west and sets in the east. This slow retrograde rotation results in a solar day on Venus lasting 117 terrestrial days. The slow rotation of the solid surface contrasts with the rapid rotation of the clouds. The clouds rotate in the same sense as the surface of Venus but more rapidly, once every four days (See: The clouds of Venus!). The axis of rotation is almost perpendicular to the orbital plane; the inclination being 177 degrees.\nThe fact that Venus is in retrograde rotation seems very strange to astronomers, since the planets orbit the Sun in one direction and almost all the planets and satellites spin in the same direction. So it is difficult to explain why Venus spins ""the wrong way"". One possibility depends on the idea that when Venus was in the process of forming, the planetesimals formed clumps of different sizes. Perhaps the second largest clump struck the largest clump at such an angle as to cause the resulting clump to rotate backwards. Another possibility is that sometime later in Venus\' history a comet, asteroid or large heavenly body crashed into it causing it to flip. But these ideas are not supported by any good physical evidence, and it is strange that Venus ends up rotating slowly backwards instead of being tipped over at a random angle like Uranus.\nSo it is generally believed that for terrestrial planets with dense atmospheres like Venus one reason for developing a retrograde rotation from a prograde rotation (in the same direction as the other planets) could be the core-mantle friction inside the planet - together with atmospheric tides, and the ""obliquity"" angle between the planet\'s equator and the plane of its orbit around the Sun.\nSeasons on Venus?\nOne of the important characteristics of the Earth is its seasonal cycle - Winter, Spring, Summer and Autumn. It causes changes of vegetation especially in the mid-latitudes of our planet. But every planet in the solar system has seasons, because seasons are caused by two factors: axial tilt and varying distance from the Sun due to orbital eccentricity. Since the Earth\'s orbit is nearly circular, it has little effect on the seasons. The main factor is our planet\'s axial tilt which causes almost all seasonal change. When the north pole is tilted toward the Sun, it is northern spring and summer. Six months later the north pole tilts away from the Sun and we experience northern autumn and winter.\nVenus and Jupiter both have quite small axial tilts of about three degrees compared to the Earth\'s 23.5 degree tilt. Consequently seasonal changes on those planets are correspondingly small. Spring on Venus is not very different from autumn. The plane\'s dense, acidic atmosphere produces a runaway greenhouse effect (See: The clouds of Venus!) that keeps the surface at 500 degrees Centigrade the year round - that\'s hot enough to melt lead!\nGo to the corresponding Brief InfoSheet\nBack to the List of Extended InfoSheets.\n|last modified: 2004/02/16|\n|© ESO Education & Public Relations Department\nKarl-Schwarzschild-Strasse 2, D-85748 Garching, Germany']"	['<urn:uuid:f0cb7aca-4d17-49ef-bbfe-af303434c315>', '<urn:uuid:f7b2b128-02ce-4824-9564-6026e1fd3034>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T22:03:18.033296	12	111	2276
32	How do cells organize their structure, and what energy processes occur within them?	Cells are organized into complex structures with specialized functions. In multicellular organisms, cells are arranged into tissues, organs, and organ systems, allowing for specialization. For example, columnar epithelial cells line the gut and are organized with other cell types into digestive tissue. Regarding energy processes, cells contain specialized organelles like mitochondria and chloroplasts that transform energy. Mitochondria generate ATP through cellular respiration by breaking down sugars and fats in the presence of oxygen, while chloroplasts in plant cells perform photosynthesis to produce ATP and sugars from sunlight. These processes are interrelated, with photosynthesis producing glucose and oxygen used by mitochondria for respiration, which in turn produces carbon dioxide and water used by chloroplasts.	"['[This past fall, I taught a course at Emerson College called “Plagues and Pandemics.” I’ll be periodically posting the contents of my lectures and my experiences as a first-time college instructor]\nOne of the biggest challenges in organizing this class was figuring out how to incorporate readings into the class material. Since I wanted to give the students a firm grounding in evolution as a way to understand infectious disease, one of the required textbooks I assigned was Carl Zimmer’s excellent introductory textbook The Tangled Bank.\nUnfortunately, I felt I only had a couple of days to give the introduction to evolution, and I ended up loading the kids up with way too much reading. If I teach this course again, I think it would work much better to spread the discussion of evolution out over several class periods, but weave it in to the discussion of infectious diseases right from the start. As it was, I came back to evolution over and over again, and I think the students had a fairly firm understanding of it (at least in the context of disease) by the end, but I could have started out better I think.\nThe other assigned texbook was Laurie Garrett’s 1995 book The Coming Plague, which is phenomenal, but which I also didn’t use effectively. I only ended up assigning 3 chapters out of the 20 or more, though at least two or three times that many were relevant to various parts of the course. This book is written more like journalism than a textbook, and the students enjoyed the chapters that I assigned. I’ll definitely use both of these books again, but I think I can use them better.\nLecture 3a (Readings: Zimmer – The Tangled Bank, chapters 4 and 6)\nIn the last lecture (part 1, part 2), I discussed the development of the two fundamental theories that underpin almost all of biology: evolution by natural selection and the gene theory of inheritance. These two advances of the 19th century were fused in the early 20th century into the modern synthesis (also sometimes called neo-Darwinism), and still govern how we think about biology today. With that as a backdrop, I’m going to step out of the history lesson a bit and talk about our current understanding of multicellular organisms, cell biology, and what we call the “central dogma.”\nCells and their organization\nIt might seem obvious, but we are not made up of a single thing. Our bodies are composed of trillions of distinct entities, each with its own boundary, its own metabolism, and it’s own blueprint for life. In 1665, Robert Hooke observed cork under a microscope, and named these entities “cells” for their resemblance to the living quarters of monks (what Hooke was actually observing was the outline made by the hard walls of these plant cells left over after the cell had died).\nMany living things live as individual cells, and multicellular organisms run the gamut of complexity from dictyostelium, (a fungus that spends part of its life as a single cell, but then forms a multicellular structure to reproduce), to sponges (simple animals with less than 10 cell types), to people.\nWhy have many cells, instead of just one? Scientists still aren’t sure exactly what selective pressures first drove unicellular organisms to team up with one another into a larger aggregate, but it’s clear in hindsight that multicellularity caries many benefits. For one, when many cells team up together, some of them can specialize. A single-celled yeast has to know how to move, find food, collect the food, digest the food, keep itself protected from predators and parasites, find mates (if it’s sexual) etc etc etc. By contrast, a hepatocyte (liver cell) in a mammal has only a few functions, and can leave the rest of those jobs to others. Because of this, that hepatocyte can be really good at its limited functions.\nMulticellular organisms thus have many different cell types, from less than a dozen in a sponge, to hundreds or thousands of different types in a human. These cells may have a lot of different functions, or be highly specialized, or have no other function than to give rise to other cell types. In any case, these cells must be organized into larger structures in order to carry out their function, and to contribute to the larger organism.\nFor instance, “columnar epithelial” cells line the inside of our gut, and function to provide a barrier, absorb nutrients, and secrete mucous to lubricate and protect from invading microbes. These cells are organized into a larger structure, or “tissue,” that is comprised of several different cell types that all work together. The epithelial tissue is in turn organized with many other tissues (like smooth muscle), to form the “organ” of the small intestine. The small intestine is organized into an “organ system” – the digestive tract – along with the stomach, large intestine etc.\nLikewise, the nervous system is composed of the brain, spinal chord and peripheral nerves, all of which can be further subdivided into different tissues and cells etc. In this way, many individual cell types can be organized at various levels to comprise a complex, multicellular organism like a human.\nBut how do individual cells do their thing?\nIntro to Cell Biology\nAll* cells share a few basic requirements, and a few basic features. First, they need something to separate the outside of the cell from the inside. This is called the “plasma membrane,” and is composed of a double layer of molecules that serve as a semi-permeable barrier. Imagine it like a chain-link fence surrounding a property – dogs and cats might be kept on one side or the other, but insects can pass back and forth without trouble. Actually, the membrane surrounding cells is much more selective, and prohibits movement based on more than just size, but the idea is the same.\nBut you wouldn’t surround a property in a solid chain-link fence. Sometimes you want to allow things to pass from one side to the other that wouldn’t fit through the fence. But you want to be able to control when, where, and how much goes in and out, so you build in gates and other portals. In the same way, cells have channels that can allow certain molecules to pass in one direction or the other in a controlled way, or pumps that actively push molecules to one side or the other.\nCells also need ways to perceive the outside environment. Usually, this is accomplished by things called “receptors,” which are basically sensors that span the membrane. These receptors can perceive the environment on the outside of a cell and communicate that information to the inside of the cell. We’ll leave receptors as a black box for now, and get back to how they work in a later lecture.\nThe final basic feature that I want to talk about is the genetic material. This is the blueprint that every cell carries and that provides all of the instructions for how a cell behaves. You’ve probably heard of genes and DNA, but to go forward in this class, we need to put those things in context. But to understand what genes do, you first need to understand what proteins do.\nProteins do the work of the cell.\nProteins do everything (just about)! From carrying out chemical reactions, to communication between cells, to basic structural support, different proteins are responsible for almost every function in a cell. Those channels and pumps I mentioned above are proteins. If you’re thinking of protein from a dietary perspective, the reason that meat tends to be high in protein is because it comes from muscle, and muscle contains high quantities of proteins that are able to contract and exert mechanical force. In the cell factory, every screw, gear and conveyer belt, and even the workers that operate the machinery are proteins.\nProteins can be quite large as molecules go, but at their core they are just long chains of smaller molecules called “amino acids.” There are 20 different types of amino acids, and the order in which they are linked together influences their shape. Imagine a chain where each link is coated in either fuzzy or hooked velcro, or a positive or negative magnet. you could lay that chain out in a line, but if you put it in a box and shook it, what you pulled out of the box would be some tangled mess, but it would have some defined shape – each part of the chain may interact with other parts, repelling each other like two positive magnets, or sticking together like velcro or opposite-charged magnets.\nIn a long chain of amino acids, there are a lot of different types of interactions (not only 4 as in the example above). The order of those amino acids, as with the order of magnets and velcro, determines the final shape that the chain folds into, and that final shape determines the function of the protein. So how to determine that order? That’s where genes come in.\nBut seeing as how this post is already super long, I’m going to put the explanation of how genes become proteins in a separate post… coming soon (I learned how to use Adobe Illustrator just to make the diagrams, it’s going to be awesome!).\n* Any time I use categorical statements like “All cells…” or “Any time I…” when talking about biology, there’s a good chance that there are a couple of examples that run contrary to the statement I’m about to make. For instance, even though all cells must have had genetic material at some point, red blood cells jettison their nucleus during development and carry out their function quite happy without any genomic DNA.', 'Presentation on theme: ""AP Biology 2007-2008 Tour of the Cell 2 AP Biology Cells gotta work to live! What jobs do cells have to do? Build proteins proteins control every.""— Presentation transcript:\nAP Biology Tour of the Cell 2\nAP Biology Cells gotta work to live! What jobs do cells have to do? Build proteins proteins control every cell function Utilize energy for daily life for growth Make new cells growth repair renewal Maintain Homeostasis Digestion/Waste Removal\nAP Biology Utilize Energy ATP\nAP Biology Cells need power! Utilizing energy take in food & digest it take in oxygen (O 2 ) make ATP remove waste Organelles involved lysosome mitochondria chloroplast vesicles, vacuoles, peroxisomes ATP\nAP Biology Lysosomes Function little “stomach” of the cell digests macromolecules “clean up crew” of the cell cleans up broken down organelles, recycling Structure membrane sac w/ hydrolytic (digestive) enzymes only in animal cells only in animal cells synthesized by rER, transferred to Golgi Where old organelles go to die!\nAP Biology Lysosomes white blood cells attack & destroy invaders = digest them in lysosomes 1974 Nobel prize: Christian de Duve Lysosomes discovery in 1960s 1960 | 1974\nAP Biology Cellular digestion Lysosomes fuse with food vacuoles polymers digested into monomers pass to cytosol to become nutrients of cell vacuole lyso– = breaking things apart –some = body\nAP Biology Lysosomal enzymes Lysosomal enzymes work best at pH 5 organelle creates custom pH how? proteins in lysosomal membrane pump H + ions from the cytosol into lysosome why? enzymes are very sensitive to pH why? enzymes are proteins — pH affects structure why evolve digestive enzymes which function at pH different from cytosol? digestive enzymes won’t function well if some leak into cytosol = don’t want to digest yourself! We will talk more about this in Unit 3 - Energy.\nAP Biology When things go bad… Diseases of lysosomes are often fatal digestive enzyme not working in lysosome picks up biomolecules, but can’t digest one lysosomes fill up with undigested material grow larger & larger until disrupts cell & organ function lysosomal storage diseases more than 40 known diseases example: Tay-Sachs disease build up undigested fat in brain cells\nAP Biology But sometimes cells need to die… Lysosomes can be used to kill cells when they are supposed to be destroyed some cells have to die for proper development in an organism apoptosis programmed cell death lysosomes break open & kill cell ex: tadpole tail gets re-absorbed when it turns into a frog ex: loss of webbing between your fingers during fetal development ex: if cell grows uncontrollably, apoptosis triggered to remove cell (if it doesn’t get removed, can result in a tumor)\nAP Biology Fetal development 15 weeks 6 weeks syndactyly\nAP Biology Utilizing Energy Cells must convert incoming energy to forms that they can use for work mitochondria: from glucose to ATP chloroplasts: from sunlight to ATP & carbohydrates ATP = active energy carbohydrates = stored energy + ATP\nAP Biology Mitochondria & Chloroplasts Important to see the similarities transform energy generate ATP double membranes = 2 membranes semi-autonomous organelles move, change shape, divide internal ribosomes, DNA & enzymes\nAP Biology Mitochondria Function generate ATP from cellular respiration breakdown of sugars, fats & other fuels in the presence of oxygen break down larger molecules into smaller to generate energy = catabolism generate energy in presence of O 2 = aerobic respiration\nAP Biology Mitochondria Structure double membrane smooth outer membrane highly folded inner membrane cristae Internal fluid-filled space between 2 membranes mitochondrial matrix DNA, ribosomes & enzymes Why 2 membranes? increase surface area for membrane- bound enzymes that synthesize ATP\nAP Biology Membrane-bound Enzymes glucose + oxygen carbon + water + energy dioxide C 6 H 12 O 6 6O 2 6CO 2 6H 2 OATP +++\nAP Biology Dividing Mitochondria Who else divides like that? What does this tell us about the evolution of eukaryotes?\nAP Biology Mitochondria Almost all eukaryotic cells have mitochondria there may be 1 very large mitochondrion or 100s to 1000s of individual mitochondria number of mitochondria is correlated with aerobic metabolic activity more activity = more energy needed = more mitochondria What cells would have a lot of mitochondria? active cells: muscle cells nerve cells\nAP Biology Mitochondria are in both… animal cells plant cells\nAP Biology Chloroplasts Chloroplasts are plant organelles class of plant structures = plastids amyloplasts store starch in roots & tubers chromoplasts store pigments for fruits & flowers chloroplasts store chlorophyll & function in photosynthesis in leaves, other green structures of plants & in eukaryotic algae\nAP Biology Chloroplasts Function photosynthesis generate ATP & synthesize sugars transform solar energy into chemical energy produce sugars from CO 2 & H 2 O Semi-autonomous moving, changing shape & dividing can reproduce by pinching in two Who else divides like that? bacteria!\nAP Biology Chloroplasts Structure double membrane stroma = internal fluid-filled space DNA, ribosomes & enzymes thylakoids = membranous sacs where ATP is made grana = stacks of thylakoids Why internal sac membranes? increase surface area for membrane-bound enzymes that synthesize ATP\nAP Biology Membrane-bound Enzymes + water + energy glucose + oxygen carbon dioxide 6CO 2 6H 2 O C 6 H 12 O 6 6O 2 light energy +++\nAP Biology Chloroplasts Why are chloroplasts green?\nAP Biology Mitochondria & chloroplasts are different from other organelles Not part of endomembrane system Grow & reproduce semi-autonomous organelles Proteins primarily from free ribosomes in cytosol & a few from their own ribosomes Own circular chromosome directs synthesis of proteins produced by own internal ribosomes ribosomes like bacterial ribosomes Who else has a circular chromosome not bound within a nucleus? bacteria\nAP Biology Endosymbiosis theory Mitochondria & chloroplasts were once free living bacteria engulfed by ancestral eukaryote Endosymbiont cell that lives within another cell (host) as a partnership evolutionary advantage for both one supplies energy the other supplies raw materials & protection Lynn Margulis U of M, Amherst 1981 | ??\nAP Biology Endosymbiosis theory Evolution of eukaryotes\nAP Biology glucose + oxygen carbon + water + energy dioxide C 6 H 12 O 6 6O 2 6CO 2 6H 2 OATP water + energy glucose + oxygen carbon dioxide 6CO 2 6H 2 O C 6 H 12 O 6 6O 2 light energy +++ Compare the equations Photosynthesis Respiration\nAP Biology The Great ENERGY Circle of Life sun ATP Photosynthesis Respiration O2O2 glucose sugar CO 2 H2OH2O + + plants animals & plants ATP\nAP Biology Food & water storage plant cells central vacuole contractile vacuole food vacuoles animal cells\nAP Biology Vacuoles & vesicles Function Vesicles - moving materials around cell, little “transfer ships” Vacuoles - storage Food vacuoles phagocytosis, fuse with lysosomes Contractile vacuoles in freshwater protists, pump excess H 2 O out of cell Central vacuoles in many mature plant cells Structure membrane sac\nAP Biology Vacuoles in plants Functions storage stockpiling proteins or inorganic ions depositing metabolic byproducts storing pigments storing defensive compounds against herbivores selective membrane control what comes in or goes out Larger vacuole allows for large surface area to volume ratio\nAP Biology Peroxisomes Other digestive enzyme sacs in both animals & plants breakdown fatty acids to sugars easier to transport & use as energy source detoxify cell detoxifies alcohol & other poisons produce peroxide (H 2 O 2 ) must breakdown H 2 O 2 H 2 O']"	['<urn:uuid:2a9ea185-8988-4250-b5fb-8ef4f121fffb>', '<urn:uuid:c9a665b0-4a9f-484b-9ee5-ccdc4ec6ad97>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T22:03:18.033296	13	113	2864
33	macedonian tavce gravce spices paprika differences	In Macedonian baked beans (tavče gravče), paprika is a key recognition factor but its use varies by region. In Macedonia, sharper and spicier types of paprika are traditionally used, creating a spicier overall taste. In contrast, in Bosnia and Serbia, a sweeter, non-spicy paprika is typically used, which merely accents the natural sweetness of the combined beans and onions.	"['Macedonian baked beans (tavče gravče) are a classic side dish present for every holiday, and on many days in between. Large, flat beans are softened to the just-before-falling-apart level, and then baked with chunks of onion in an abundant amount of sweet paprika.\nThere is a general direction we take to make them, plus the tweaks. Many of those are considered ""the right way."" So it\'s a balancing game to achieve a version that will satisfy the requirements of the tried, true and traditional taste we\'re used to. The trick to do it on our own terms while not straying from the pillars.\nAn earlier version of these Macedonian baked beans featured on the site was a quick way to make this side dish.\nToday we delve into the conventional way.\nIn the earlier recipe I\'ve mentioned the importance of this dish. Macedonia is most known for this delicacy. There they call the dish tavče gravče, or \'beans in a pan.\'\nThe best pan baked beans are made with a special type of regional bean called tetovac. Tetovac originates from Macedonian city of Tetovo. For a long time it was the most known bean in the region. Back in the day it was planted throughout Serbia and parts of Bosnia too.\nTetovac is a large, white, flatter bean. Not completely flat though. There is a certain meatiness to it. When prepared well it achieves the softness of a chestnut. When you bite into this bean, there is a lot of substance.\nThis is a hearty side dish. Eat too much of it, and you\'ll be too stuffed to eat the rest of your meal. Eat too little, and you\'ll be left with a lingering for more.\nThe taste leans towards sweet achieved with paprika and soft, simmered onion. This delicacy\'s recognition factor is the paprika. In Bosnia and Serbia sweet, but not too sweet paprika is used. The kind without the spiciness factor. Paprika just accents the already successful, sweet taste of combined bean and onions.\nIn Macedonia however, sharper, spicier tones of paprika are used. The taste is spicier, and will be more up your alley if you are a lover of spicier things.\nThis translates into more choices for you.\nOriginal tetovac is harder to find today. And if you\'re in the US, it\'ll be near impossible.\nHowever the white butter bean is a great substitution. White lima beans also work. Just look for the flatter, larger version. You\'ll need them dry, not canned.\nFirst clean it from the debris. Soak it overnight in plenty of water, with a pinch of baking soda. The following day strain and wash thoroughly. Transfer to a pan, bring to a boil, then cook until the beans are soft. Strain again and combine with simmered onions. And let them take a journey in the oven together.\nAs I mentioned before, the easiest version is to go with 1:1 bean to onion ratio. However it\'s not strictly so. You can do a little less onion, or a little less beans. The first time you make it, make the ratio even. After that, let your taste guide you.\nMacedonian pan beans are great as a roast side.\nThey make a fantastic sandwich too, along with a glass of beer.\nMacedonian Baked Beans (Tavče Gravče)\n- 1 teaspoon baking soda\n- 24 ounces dry white flat beans lima, butter bean or tetovac\n- 24 ounces yellow onion cleaned, diced into half moons\n- ⅓ teaspoon ground pepper\n- 1.5 teaspoons seasoned salt or vegeta or 1 bouillon cube\n- 3 teaspoons paprika\n- 7 ounces vegetable oil\n- In a large bowl, soak beans overnight in plenty of water and baking soda.\n- The following day strain the beans and wash thoroughly. Transfer to a large pan and cover with 6-8 cups of water. Bring to boil on high, then lower the temperature to medium. Continue cooking beans for the 60 minutes or until they are soft, but not falling apart. During cooking, keep removing the foam that forms on top of the pot and stir.\n- About 40 minutes after you\'ve put beans to cook, heat oil in a large pan on high. Add onions and lower the temperature to medium. Simmer for 15-20 minutes, or until onion crystalizes but isn\'t completely brown. Heat oven to 480°F.\n- Remove beans and onions off heat. Strain beans, however keep 8-10 ounces of water in which it was cooked. Transfer to a baking pan by first layering the onion, then the beans. Add spices on top. Finally, add the bean water. Mix thoroughly and transfer to the oven. Lower heat to 390°F.\n- Bake 30 to 40 minutes. Check once or twice. The beans should be baked and soft with a slight, non-dry crispiness in parts.\n- Serve warm.\nTraditional Macedonian baked pans ask for sharper, spicier paprika. We used the sweet paprika usually used in Bosnia and Serbia.\nGluten-free: Use vegeta or a gluten-free bouillon cube.']"	['<urn:uuid:c76d9b54-7494-4971-b263-7215f8a0279d>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T22:03:18.033296	6	59	831
34	Hey, what kind of material is used in a foldable x-ray machine cover?	The shroud for the X-ray machine extension arm is made of Tyvek®, a type of synthetic paper produced by DuPont.	"['John Varrasi is a senior staff writer for the American Society of Mechanical Engineers (ASME). Featured engineers Mary Frecker and Larry Howell are ASME Fellows. Varrasi contributed this article to Live Science\'s Expert Voices: Op-Ed & Insights.\nOrigami, the Japanese art form that dates at least to the 17th century, creates unique patterns and shapes from paper folding. Today, origami is inspiring engineers to design active materials and smart structures that bend, stretch and curve, overcoming traditional design constraints and rendering products and systems with remarkable performance characteristics and features.\nPerhaps the most ubiquitous example is the amazingly compact automobile airbag, though origami-inspired engineering is also making commercial inroads in the energy, apparel and healthcare industries. Reflecting emerging interest in the field, the U.S. National Science Foundation funded eight U.S. universities to advance origami engineering into the realm of reconfigurable, smart, structures and self-assembling systems.\nFolding, far beyond paper\n""Origami engineering can meet the demand, across multiple industries, for products and systems with very complex applications,"" said Mary Frecker, a professor of mechanical engineering and biomedical engineering at the Pennsylvania State University, one of the schools to receive an NSF research grant. ""Origami enables products with the ability to fold and then unfold on demand — at any time.""\nTo create such products, engineers are experimenting with active materials like magneto-active elastomers, which are comprised of magnetic filler particles — such as barium ferrite — embedded in an elastomer matrix and which display a special ability to curve and rotate when a magnetic field is applied. Polymer synthetic compounds featuring extremely high energy density are also favorites in the origami design community due to the ability of the material to compress and stretch in the presence of a voltage.\nWhile such materials are almost magical in how they morph into different shapes and patterns, the challenge for engineers is create a system that is structurally sound and can be fabricated for practical use. [ Folding Origami Solar Panels Could Be Headed to Space (Video)]\n""Traditional origami art uses paper; however, most engineering applications require materials with finite thickness to provide the necessary strength and stiffness to achieve a desired functionality,"" said Frecker. ""Our analysis of experimental designs shows that different activation processes determine different folds, curves and deformation in the material structure.""\nAs researchers continue to create new structures that stand up to practical use, some companies have placed origami-inspired designs into the commercial marketplace, realizing the benefits of products and systems with unique folding capabilities.\nGE Healthcare recently collaborated with Brigham Young University (BYU) on a cover for the extension arm of an X-ray machine used in hospital operating rooms. The shroud expands and contracts like a musical accordion to shield the sterile field in the OR from the non-sterile environment of the extension arm. The shroud is made of Tyvek®, a type of synthetic paper produced by DuPont. GE required a design improvement over the plastic drapes commonly used to shroud the moveable C-arm; the drapes needed to be replaced every time the device rotated in and out of the sterile field, adding time as well as cost to surgical procedures.\n""GE needed a design for the shroud that maintained the sterile field during all movements and positions of the extension arm,"" said Larry Howell, a professor in the Department of Mechanical Engineering at BYU, also a recipient of one of the NSF grants. ""After several concepts were evaluated and tested, an origami-based design was selected.""\nThe design of the shroud was complex. In order to accommodate the needed motion and geometry of the X-ray machine arm, the BYU team developed an adjustable shroud based on a crease pattern named Miura-ori. Named after the Japanese astrophysicist Koryo Miura, the Miura-ori method has inspired the field of systems design, allowing engineers to fold and unfold rigid and thick surfaces in different directions and continuous motion.\nAnother healthcare application for origami engineering is in the area of surgical probes, forceps and other instruments that have the ability to enter an opening in the body in a narrow and compact state and unfold after insertion. Origami-based engineering may also play a role in improved biomedical stents.\nThe expanding role for origami engineering\nBeyond the medical field, origami-inspired products include telescopes, protective covers for automobiles, sporting goods, kayaks, solar arrays, appliance drawers and temporary shelters. Solar arrays represent an example of deployable membranes that allow engineers a range of design flexibility, including low friction joints, low material volume, controlled buckling, and extensive spanning ability.\nIn the area of sporting goods and apparel, industrial design students at BYU, in collaboration with the consumer products firm Tessel Supply, have adapted origami in the design of a novel backpack. The distinguishing features of the backpack are the triangular meshes that allow the bag to conform to the articles within, reducing jostling of the contents and providing protection against damage. Combining form and function, the backpack matches careful engineering with pleasing aesthetics and comfort.\n""These and other products present evidence that foldable solutions are viable in engineering design,"" said Howell. ""Origami art will continue to inspire products that need to be portable and deployable.""\nAccording to Frecker at Penn State, one area of growing interest is in self-folding structures, as well as deployable large-scale systems for space exploration. There is also an emerging focus on applications that combine electrical, magnetic and thermal means to activate materials to enable origami-based design. There were 28 presentations on origami at the 2014 ASME Design Engineering Technical Conferences — interest in the field is surging.\nFollow all of the Expert Voices issues and debates — and become part of the discussion — on Facebook, Twitter and Google+. The views expressed are those of the author and do not necessarily reflect the views of the publisher. This version of the article was originally published on Live Science.']"	['<urn:uuid:fad53d94-627b-4640-931b-09644cdc6996>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T22:03:18.033296	13	20	976
35	old italian jewellery decoration methods	The Etruscans used several sophisticated jewelry decoration techniques. These included repoussé (hammering relief decoration from behind), filigree (decorative open work made from thin twisted wire), and granulation (decorating smooth surfaces with tiny gold granules). Granulation required particular care to solder the granules onto the surface using a tiny heated point and likely arseniates reduced to impalpable dust. These methods were all accomplished using simple or basic tools.	"['||This article has an unclear citation style. (February 2011)|\nThis article refers to the jewelry of the Etruscan civilization and its differences in various eras.\nDuring the Villanovan Era, there was very little jewelry which has remained for discovery in modern times. The Villanovan Etruscans seem to have had left few items of luxury and thus appear modest. Yet Villanovan jewelry proves and confirms that in Etruria great effort was placed in the production of decorative arts. Jewelry was a status symbol and represented, like in present times, wealth and prosperity.\nThey used the same patterns as on their pottery. Swastikas, zigzags and triangles can be observed on their jewelry.\nGold jewelry started spreading rapidly during the Orientalizing era. It allowed a great deal more stylization and showed splendid workmanship. Geometric design was such a regular motif that archaeologists refer to this motif as the “Orientalizing geometric”.\nEtruscan gold jewelry especially flourished during the Orientalizing period due to the very affluent trading system which had evolved during this time. The Etruscans did not invent their decorative techniques. Indeed, the Mediterranean influences had brought such techniques as granulation. Syro-Phoenician jewelers settled in southern Etruria and taught local apprentices the art of granulation and filigree.\nThese techniques first developed in the South of Etruria. It consisted of working designs onto a surface with tiny granules of gold. Care had to be taken not to melt the little granules onto the surface but instead, to solder them on with a tiny heated point. The various omissions and imperfections, made on purpose, gave the piece of jewelry the artistic character. Soldering was done using (most likely) arseniates and reducing the solder to an impalpable dust. \nThe Syro-Phoenicians brought in other techniques of workmanship. Many jewelers were influenced by their recurrent themes and symbols. Sacred emblems like the solar disc and the half moon were incorporated in the Etruscans’ fast-growing repertoire. Later Etruscans loosened up their very stern geometric standards and added in their designs floral and figurative elements of oriental inspiration. The finest jewelry was still mainly centered and focused in the southern city-states such as; Cerveteri, Tarquinia and Vetulonia.\nGorgons, pomegranates, acorns, lotus flowers and palms were a clear indicator of Greek influence in Etruscan jewelry. The modelling of heads, which was a typical practice from the Greek severe period, was a technique that spread throughout the Etruscan territory. An even clearer evidence of new influences is the shape introduced in the Orientalizing era: The Bullae. A pear shaped vessel used to hold perfume. Its surface was usually decorated with repoussé and engraved symbolic figures.\nIn the northern city-states however, jewelry was more sober and refined pieces from Vetrulonia, for example, are decorated with minute particles known as pulviscolo (gold dust).\nMuch of the jewelry found was not worn by Etruscans, but were made to accompany them in the after world. Most, if not all, techniques of Etruscan goldsmiths were not invented by them as they are dated to the third millennium BC.\nThese practices came to them from the (at the time) distant Middle East, along with imported objects who inspired them to widen their range of jewelry. The Etruscans perfected these techniques, and in turn lead them to the very stylized jewelry of the 7th and 6th centuries. Often these pieces are considered the peak of their abilities.\nSome etruscologists believe that the most complete form of acquired art was that of the Etruscan jewelers. Their technical accomplishments are astonishing. From the beginning, gold workers mastered the most sophisticated of techniques. They count amongst their acquired skills: hammering relief decoration otherwise known as repoussé, filigree and granulation. All of these methods were done using simple or basic tools.\nRepoussé literally translated means to push back in French. The technique consists of hammering the design behind the ornament with the relief on the other style.\nGranulation was the art of decorating smooth surfaces of gold jewelry with patterns composed of tiny granules of gold. It was invented in Mesopotamia in the third millennium BC and was subsequently introduced to Anatolia, Syria, Egypt, Cyprus and Mycenaean Greece. The collapse of the Bronze Age civilization brought with it the disappearance of such sophisticated arts in Greece, but they survived in the Near East and from there they were reintroduced in Greece in the 9th century and transferred to Italy during the second half of the 8th century.\nFiligree is a decorative open work made from thin twisted wire mainly in silver and gold but also in other type of metals.\nThe combination of metals was a basic technique. Essentially, there is no such thing as 24 carat gold jewelry. Massive gold is the most malleable metal. It is almost like wax. In order to have jewelry that can pass the test of time and gravity, the Etruscans had to combine their gold with other metals.\nThe most commonly used metal is copper in these cases. Most Etruscan jewelry is 18 karat gold but it varies - going as low as 15 karat. While pure gold is 24 karat, 18 and 15 karat gold benefit from their alloys. 18 karat gold is much more durable and harder than 24, and 15 karat is so much more durable and \'hard\' than 18 karat. Some jewelry of the Regolini-Galassi tomb was too; thin, delicate and big to have been worn while more robust and less poutre gold ornamentation was solid enough to have been worn in life.\nEtruscans would also mold their gold and jewels into stone-carved molds.\nAmong the jewelry found in tombs of the Archaic period were large disc earrings. The techniques here are difficult to define but they actually used granulation in concentric patterns. They also used filigree and glass paste. This was probably a trend of the time as they have been found in several tombs. The disc earring is originally a Lydian type of jewelry and became a fashionable trend during the archaic period with the strong East Greek influence spreading in the second half of the 6th century BCE.\nValise-type earrings were mostly made in Vulci and were very widespread. The heavy pendants started becoming fashionable along with the Middle Eastern floral elements and all the other types of influence the Etruscans received from elsewhere in the Mediterranean.\nThe fibulae became an item closer to jewelry in the archaic period. During the Villanovan era they were mainly bronze. Progressively though they turned into a subject of ornament.\nUnfortunately the classical era was a period of crisis for the Etruscans. During the 5th century, Etruscan jewelry suffers a regression. Such techniques as filigree and granulation gradually disappeared. Others, like repoussé are used to decorate thin funerary bands, necklaces and lockets (or bullae). During this period, a different kind of earring comes into fashion: the grape cluster shaped earring. These kinds of earrings would cover the whole ear and sometimes hung down to the neck.\nDuring the archaic period, ""bigger was better"". Large hanging earrings, long necklaces and heavy pendants or bullae were in style and worn by both men and woman alike. Woman, by modern standards started looking like ""Christmas trees"" as the fashion of the time was really large brooding figures. Women were excessively adorned and wore large diadems, bracelets and circlets, hair spirals, heavy earrings in the shape of grape clusters, large heavy pendants (also worn by men and children).\nThese were the fashion all throughout the 5th and 4th centuries. Necklaces were paired up. They would wear big necklaces with assorted beads, then another one with a big hanging pendant. Earrings with a long oval pendant and a smaller one hanging below were also very well received in the Etruscan community. In the early 3rd century, bead necklaces and bullae remain popular as do torcs, which were rings of color hair (of an animal) or feathers around the neck. In the late classical era body jewelry became more and more popular as the fashions tended towards a progressive state of undress. The body jewelry was the adornment of the body and was paired with other artifacts such as shoes, holding mirrors, etc.\nThese styles stayed popular throughout the Hellenistic period as well as the Roman period. During the Hellenistic periods, technical decline and excessively complex shapes and decoration characterized the jewelry.\nJewelry becomes omnipresent during the Hellenistic period. It becomes unisex and is worn by people whether they are naked or dressed. In images, women were often represented with only slippers and a torque or necklace. The heavy necklaces of the classical era are now replaced by strands of beads and torques.\n- * Jewelry of the ancients: Estruscan Jewelry Castellani was one of the leading Italian jewelry designers (in a famous family lineage) of the 19th century. When the major Etruscan tombs were found, they were asked to analyze the Etruscan jewelry. They could not immediately reproduce the soldering, even when checking all ancient (Italian) sources. However at the time, there were still some traditional goldsmiths, in remote areas of the former Etruscan area, that used similar techniques with similar results as the Etruscan jewelry. They deduced the techniques of the Etruscans from these artisans. Such an approach is not uncommon, and anthropological analysis is often applied to explain archaeological finds, especially in \'traditional societies\' where little has changed over the centuries\n- Bonfante, Larissa. Etruscan Life and Afterlife. Detroit, Michigan, Wayne state university press 1986.\n- Brendel, Otto J. Etruscan Art. New Haven,Yale University Press 1978\n- Haynes, Sybille. Etruscan Civilization.\n- Andrea Cagnetti ""Experimental survey on fluid brazing in ancient goldsmith\'s art"" – International Journal of Material Research (2009) DOI 10.3139/146.101783 \n|Wikimedia Commons has media related to Etruscan jewellery.|']"	['<urn:uuid:09ae9082-5b02-4755-8b1a-a169d5dfc7a2>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T22:03:18.033296	5	67	1613
36	safe endophyte grass options prevent staggers dairy cows grazing risks effects	There is no risk of ryegrass staggers from tall fescue, whether it is endophyte free or contains the MaxP endophyte. The MaxP endophyte produces the alkaloids peramine and loline and is safe for cattle. There are no additional metabolic issues when grazing tall fescue with dairy cows, other than those experienced with ryegrass pastures. However, due to tall fescue typically having high levels of clover, bloat management needs attention.	['- Tall fescue has greater summer growth than perennial ryegrass, with at least similar nutritive value.\n- Grazing of tall fescue needs to be more strictly managed than perennial ryegrass to achieve its potential nutritive value.\n- It is better adapted to hot and dry conditions than perennial ryegrass due to its deeper root system and higher temperature ceiling. This gives it a potential role in low rainfall regions.\n- It can grow in less-fertile soils, is tolerant to a wider range of pH and waterlogging conditions, and can achieve higher persistency than perennial ryegrass.\nTall fescue is likely to be most successfully integrated into a dairy system where a significant proportion of the farm (25% or more) is established in fescue rather than one or two paddocks.\nPaddocks that are summer dry, have suffered insect damage, or where ryegrass has failed to persist, are ideal candidates for renovation with tall fescue.\nSome cultivars are available with an endophyte (MaxP) which can enhance persistence of tall fescue where insect pests such as Argentine stem weevil or black beetle are present. MaxP produces the alkaloids peramine and loline and is safe for cattle.\nTall fescue is slow to establish compared to ryegrass, and is sensitive to soil temperature.\nSow in early autumn (February/early March) when soil temperatures are over 12°C. Lower soil temperatures (5-100C) will result in slower germination (28 days) and slower winter growth allowing weeds to smother the tall fescue.\nTall fescue is sensitive to sowing depth. The ideal depth is 10mm and when sown more than 20mm few seedlings will emerge.\nSow tall fescue as the main grass in a pasture seed mix at 25-30 kg/ha with clover(s).\nIt should never be sown in mixture with ryegrass as ryegrass will dominate and the fescue will disappear.\nFirst grazing should be when plants are 15-20cm in height and checked for pulling. For the first 2-3 grazings if should not be grazed lower than 7cm, preferably with light stock (calves).\nIt is recommended that the same post-grazing residual target be applied to tall fescue pastures as ryegrass pastures (e.g. 1500-1600 kgDM/ha or 7-8 clicks on the rising plate meter set on the formula “clicks” x 140 + 500).\nThe feed quality of tall fescue declines when pasture mass exceeds 3000 kgDM/ha in the spring and summer period, and it can reach this more quickly than ryegrass, and once feed quality is lost it is difficult to graze well.\nIt is not recommended that dairy farmers carry autumn-grown tall fescue through to the spring as it has a low palatability. It is better to graze early winter to provide fresh growth for the spring.\nThere is no risk of ryegrass staggers from tall fescue either endophyte free or with the MaxP endophyte.\nFacial eczema risk is typically lower than ryegrass, but invasion by ryegrass will increase the risk.\nDue to tall fescue typically having high levels of clover, bloat management needs attention, especially in the first spring after autumn establishment when clover can be dominant.\nThere are no additional metabolic issues, other than those experienced with ryegrass pastures, when grazing tall fescue with dairy cows.\nMilk production from fescue\nWhile DM yields of fescue are higher than ryegrass, its lower feed value means there is little advantage in potential milksolids production on many New Zealand dairy farms, or in wet summers.\n- Tall Fescue establishment and management (FarmFact 1-28)']	['<urn:uuid:a43a91c7-b879-4138-9ea0-acfc4e0a765f>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T22:03:18.033296	11	69	568
37	what industries can use augmented reality save money time human life	Augmented reality can be used in industrial assessment (like HVAC, maintenance, and weight-bearing wall identification), remote technical assistance for malfunctioning equipment, research and development, medicine, and military applications. In these fields, AR can be used for training fleets, simulating high-risk surgeries, or preparing employees for pharmaceutical manufacturing, which can save money, time, and prevent the loss of human life.	['Be innovative and prepare for what will become “reality” in the coming years\nThe United States Geospatial Intelligence Foundation had the honor of hosting Amy Peck, the Founder and CEO of EndeavorVR, as the closing keynote speaker for their 2021 GEOINT Community Forum, The Geospatial Metaverse – Infrastructure, Tradecraft, and Applications, last week.\nShe opened her discussion with a question: “What exactly is the AR Cloud and why should you care?” Peck explained that augmented reality (AR) is intended to enhance life by serving as a duplicate, or “digital twin,” of the world around us. Though augmented reality and mixed reality (MR) is currently a smaller market compared to that of virtual reality (VR), Peck anticipates a tremendous shift in focus to AR and MR once headsets become more wearable, affordable, and commonplace.\nShe noted that an expansion of interest in all types of extended reality (XR) products is rooted in the global transition to remote workforce environments in the wake of the COVID-19 pandemic but also underscored that she expects consumer AR to remain a driving force of enterprise AR growth. “Consumers are the workforce, and the workforce are consumers,” said Peck.\nRight now, the consumer is limited to mobile AR. Peck described mobile AR interior design applications and Try On/Magic Mirror as valuable examples that also widely understate the capabilities AR can have in the hands of the consumer. “There’s so much more. This is truly the next computing platform, and everything that we do on our mobile devices [is something] we will eventually be doing with these wearables,” she said.\nPeck also highlighted the current utility of AR in marketing, autonomous vehicles, robotics, product information, and E-sports gaming. “It’s essentially anchoring all these digital experiences to a place… but also allowing us to interact with one another in a global multiplayer mode,” Peck added. “At any given time within the next 4-5 years, there might be 10,000 to 100,000 invisible digital experiences… we will have a system to be able to decide what we see when, and where, and in what context. We’re going to really merge… and [all versions of reality are] going to blend together.”\nAR can also be leveraged in industrial assessment (e.g., HVAC, maintenance, weight-bearing wall identification) and remote technical assistance (e.g., finding solutions to malfunctioning equipment) in ways that save significant money and time.\nPeck mentioned that AR can be paired with other technologies like artificial intelligence (AI) to create an efficacy loop that allows the technology to improve itself.\nPeck further emphasized that research and development, medicine, and military can also benefit from the integration of augmented reality. Training a fleet, simulating a high-risk surgery, or preparing employees to work as a part of a pharmaceutical manufacturing line that is still under construction can save things beyond money and time: it can prevent the loss of human life.\nPeck then shifted her attention to working with data from the inside out. “Data now is really a two-way street,” Peck held. “We are collecting an entirely new and different level of data from within these headsets… how are we merging the digital and physical worlds? On the flip side, we can bring data into these 3D environments and be able to manipulate them and work with them in this organic – and even physical – way.”\nAfter concluding her vibrant discussion on use cases of AR, Peck reminded the audience that while those applications are already operational in one form or another, she believes the spotlight should have a much broader diameter – one that goes beyond use cases.\n“There’s a much bigger strategy that we should all consider when we’re thinking about this technology, and really, all technology as a whole,” Peck urged. “There is a tsunami of emerging technology coming, and it’s all moving forward and progressing at a tremendous pace… we need to start to harness it now, today, before we do actually lose control of it.”\nShe contined: “Imagine what you want your products and services to be. What do you want your business to look like? What do you want your personal life to look like?”\n“What do we want the world to look like?”\n“In order for us to really be effective, we need to think about… convergence… Each of these technologies is simply an ingredient in a larger recipe, but we need to define what that recipe is,” she continued.\nHuman-Centric Identity is what Peck sees as the most critical component of navigating the digital world. “It’s how we are able to start to manage our own data, how we move in and out of digital realms… I envision our avatar as being our digital asset in the digital realm. We can actually build that interface to control our own data to give permissions and potentially even monetize it,” Peck said. “We need to be cross-functional.”\nIn response to a variety of audience member questions, Peck identified her intentions to bridge the digital divide while also putting forth a set of standards to protect users, data, and information as it develops. “We’re building the bridge under our feet as we’re crossing the canal… it’s critical we look at the standards around not just the AR cloud, but also Blockchain and AI in particular.”\nShe encouraged viewers to be innovative, accented the delicate balance between local and global thinking, and underscored the importance of being prepared for what will become “reality” in the coming years.\nShe ended with a quote by Alan Kay: “The best way to predict the future is to create it.”\nUSGIF thanks Amy Peck for her passion, enthusiasm, and inspiring thought leadership in both this dimension and beyond.\nJoseph Rouge, Deputy Director of Intelligence, Surveillance, and Reconnaissance, Headquarters, U.S. Space Force, discussed the Space Force’s vital purpose, unique structure, and future promise as it engages with burgeoning ISR activities.']	['<urn:uuid:64178f8b-75d0-4dee-bd06-4ca6168d8231>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T22:03:18.033296	11	59	977
38	french press drip coffee maker time temperature differences	A French press takes 4-7 minutes to brew coffee and requires separately boiling water first. The water temperature should be 195°F-205°F, and preheating the press is important for heat retention. Drip makers take 4-10 minutes to brew but have the advantage of a hot plate base that keeps coffee warm longer, though extended heating can deteriorate the taste.	"[""How to brew with your french press, using an explanation of extraction, strength, and the variables you can use to make better coffee.\nThere aren’t many purer representations of the wonders of full-immersion coffee brewing than the french press. Resulting in big body and rich flavors, the brewer’s simple design obscures the more complex process happening inside—the interaction of ground coffee, water, and time all work together and, with your influence and know-how, produce a drink suitable for even the most discerning coffee drinker. This article is made to take you beyond simply having a recipe that you use every day, and instead will propel you into understanding how you can influence extraction and strength in your mug and brew up the perfect cuppa joe with any bag of beans you like.\nIf you haven’t already, check out our Basic Brewing Principles guide where we detail the fundamentals of good coffee, good water, and good recipes. Or, if you’re interested in these concepts in a general sense instead of specifically for your french press, take a gander at our Intermediate Brewing Principles. If you're on the market for a french press, make sure to check out our Comparison of French presses.\nBrewing is Extraction\nWhether by immersion or percolation, brewing coffee is actually the process of extraction. A series of physical and chemical interactions are set in motion when you mix coffee and water, starting with washing insoluble materials from the surface of your bed of coffee grounds and quickly moving into dissolving soluble flavor compounds from inside of those coffee particles. All of this material, both soluble and insoluble, contributes to your resulting cup of coffee—those coffee oils and superfine particles that were washed from the surface of your coffee grounds will help define the body and texture of your beverage, while the combination and ratio of dissolved flavor compounds will determine the flavor of your fresh brew.\nThe concentration and mixture of different compounds that end up in your brewed coffee will change constantly throughout the extraction process—this will always be the case because we know that different chemical compounds in your roasted coffee (like organic acids, simple sugars, and more complex carbohydrates) all dissolve and extract at different but predictable rates. This means that in the earliest stages of brewing in your french press you can expect the beverage to be sharp, dilute, and acidic in flavor, growing more balanced over time until eventually extracting astringent compounds and taking on some characteristics that we associate with “over-extraction”. This predictability in the extraction rates of different flavors gives us a way to have some influence over what our brewed coffee tastes like, as well as giving us a baseline to be able to assess our extraction as either under-extracted (we removed too little of the soluble material), over-extracted (we removed too much of the soluble material), or ideally extracted (we removed just the right amount of the soluble material).\nHow to Judge Your Extraction\nThere are lots of ways that you could try to assess how your brew went, and if you look for very long at the science of brewing you’ll come across all kinds of measurements, numbers, and formulas made to inform that assessment. Though quantitative data can be a helpful way to communicate precise information from one person (with the necessary tools to measure and understand the measurement) to another person (with the same tools and understanding), any coffee professional should be able to tell you that you don’t taste numbers, measurements, or extraction percentages, you taste coffee! Since we know that the flavors in our mug are influenced by the relative extraction we achieve, how do we know if our brew is over-extracted, under-extracted, or just right? By taste! There are some flavors that tend to consistently come through at different levels of extraction that you can look out for as you’re learning to dial in your recipes.\nUnder-extracted coffee will likely taste sour, salty, or overbearingly acidic. The early stage of extraction contains most all of the acids that will be extracted, as well as some bitter compounds (like caffeine) that can be overwhelming when left unbalanced by the flavors that will extract later in the brew.\nOver-extracted coffee will likely taste bitter and feel unpleasantly astringent. Many of the flavors extracted late in a brew are bitter in taste, pushing the flavor balance in that direction quickly.\nIdeally extracted coffee should be balanced and flavorful, achieving both a satisfying texture and an acidity that complements the flavors of your brew. This can mean sweet and brimming with fruit flavors, deep and rich with chocolatey goodness, or complex and alluring with savory complexion.\nIf you’re not a confident taster right now that’s totally okay! Tasting is a skill you can learn, and with a little trial and error you’ll be able to start judging your extraction accurately enough to make educated decisions about how to adjust your recipe for a better brew.\nThe Importance of Strength\nAn ideally extracted dose of coffee is the first goal in brewing, but ensuring that you brew coffee with good strength is a very close second. Our ability to taste coffee well relies on the concentration of its flavors being just right—too much flavor in solution and our taste buds become overwhelmed, while too little flavor will leave a beverage to come across as dilute and disappointing.\nThe balance of strength and extraction in your press pot can get especially interesting because the wire mesh filter used in most french presses allows a much greater volume of insoluble material (oils and fines) to end up in your brew. These insoluble materials contribute little flavor but add lots of weight to the body of your brew, which in turn makes it more difficult to taste flavors clearly. Ultimately, this means that in order to achieve a similar level of flavor clarity as in a paper-filtered brew you’ll need to brew coffee the results in a higher overall strength, making it possible to discern the flavors in your brewed coffee through the veil of the heavy body that your brewer excels at producing.\nWith a dose, grind setting, and brew time all within a typical range for immersion brewing, it’s pretty straightforward producing coffee at a palatable strength with your french press. But, if you’re looking for more control over your new brewer, we’ll cover below the variables that you can adjust in order to fine-tune your recipe to brew up your ideal cup of coffee time and time again.\nMastering Your French Press\nLearning to succeed with your french press is all about understanding how to tweak just a few variables in order to achieve the best extraction and strength for any bag of beans that you’ve got. So much of the beauty of immersion brewing is in its simplicity, meaning that more so than in any pour-over brewer you can focus strictly on these four inputs without worrying about other extraneous (and more difficult to manage) variables like pour stages, flow rate, channeling in the brew bed, and loads more.\nBrew ratio is the amount of ground coffee and brew water you use to make a single extraction. Immersion brewers like the french press tend to perform better with a higher ratio of coffee to water than most pour over or auto brewers do, so I’d recommend starting at 1g of ground coffee per every 14 or 15g of water used. Extraction and strength will be affected greatly by this ratio—the more coffee you use the higher your strength will be and the lower your total extraction will be, and vice versa. Expect to make adjustments here to impact your total strength first and foremost, but typically finding a consistent ratio will help you learn to adjust the other variables with more precision quickly.\nThe grind setting that you choose will also greatly impact your total extraction by affecting the rate at which coffee flavor will dissolve and extract out of your grounds. The commonly accepted coarseness for french press has been “very coarse” for a long time now, but more and more lately loads of pros are recommending a setting closer to a standard drip grind. Regardless of which grind you decide to go with, be sure that it pairs well with your chosen contact time.\nMany immersion brewers, like the french press, give you the ability to directly and consistently control your total contact time in a way that you can’t with pour over brewers. While you’ll always need at least a certain amount of time to reach an appropriate extraction level, it seems as though very long steep times are becoming more and more common (as evidenced by a couple of the links above). Whatever your process, remembering that more time always means more extraction will help you dial in how long you let your french press steep before either breaking the crust, depressing the plunger, or decanting the coffee.\nThe Temperature of your brew water is the final control variable, and it will relate directly to your rate of extraction (hotter water causing faster extraction and vice versa). The general standard for brewing says to use water 195°F–205°F (90.5°C–96°C), but there are lots of folks who prefer hotter or colder water for their own brews. Remember that your “temperature” isn’t necessarily the temperature in your kettle, but rather the temperature of the water throughout the brewing process in the brewer itself. Preheating your french press to aid in heat retention will be important no matter the design of your brewer, but it might behoove you to use a slightly higher temperature if your brewer is single-walled glass or metal as opposed to a double-walled or otherwise well insulated brewer.\nAll of the reading in the coffee world won’t make a difference without investing a little time into brewing and tasting, so get to it! Hopefully now with a little understanding of extraction and strength, guidelines for how to taste and assess your brew, and an overview of the variables you can adjust in your french press, you’re ready to dive in and make a little magic in the pursuit of the perfect mug of coffee.\nHappy brewing, y’all."", 'French Press vs Drip\nCoffee enthusiasts have always entangled in a fierce debate of which between the French Press and Drip Coffee Maker is the best when it comes to making the best cup of coffee. Although it all trickles down to preferences, having a clue why the debate has remained endless will enable you to know on which side on the fence you sit on as well as help you make an informed choice on the best coffee maker for you.\nFrench Press Vs Drip Coffee Maker Comparison\n- Variety of Brews\nBoth the French Press and the Drip are well-suited with any kind of coffee that you may want to brew. However, what brings the difference is the consistency of the grind you prefer on each of them. For the French Press, a coarse grind is the most preferred while the Drip is ideally designed for a finer grind.\nRegardless of this, most coffee enthusiasts prefer to grind their coffee beans in the comfort of their homes as opposed to using pre-ground varieties. As such, they have higher control over the grinding process while it enables you to use both the Drip machine and the French Press interchangeably – depending on what you prefer on a given day!\nIt is difficult to settle on a particular product based on the variety of brews as both of these allow you to brew your favorite variety of coffee. Therefore, whichever you pick, you can be sure to enjoy your cup of coffee to the fullest.\n- Brewing Speed\nThe brewing speed of these two coffee makers will depend on the time you have at hand. On average, it should take you from 4 to 7 minutes to brew your favorite cup of coffee using a French Press while the Drip Coffee Maker will take you from 4 to 10 minutes to get your cup of coffee.\nMoreover, it is important to keep in mind that the French Press will require you to boil the water first on a stove and this will eat some of your time and effort as well. On the other hand, some coffee lovers prefer the set it and forget it nature of the Drip Coffee Maker even though it is a bit slower on average.\nOverall, the French Press will deliver you a cup of coffee faster than the Drip especially if you are to consider its affordable price.\n- Capacity and Sustainability\nWhen it comes to capacity, both the French Press and the Drip tend to come in a variety of sizes even though they tend to curb it at 10 cups per brew. So depending on the capacity you are looking for in a coffee maker, you will find one in either product.\nHaving said that, which of the two products can sustain the hotness of your coffee the longest? In this aspect, the Drip Coffee Maker has a clear advantage over the French Press. The Drip comes with a hot plate at its base which allows it to keep the coffee a bit warmer for much longer than the French Press. However, bear in mind that the longer your coffee sits in there after brewing is done the more the taste deteriorates.\n- Necessary Brewing Skills\nWhen using the French Press, you will require some brewing skills as it requires you to determine the brewing time, water temperature the consistency of your grind, and plunge rate. However, the Drip Coffee Maker only requires you to put water and your ground beans and switch it on. If you don’t have prerequisite brewing skills, you might find it difficult to make a perfect cup of coffee when using the French Press. Beginners may find it hard to brew the cup of coffee that they will truly enjoy in the first days.\nSo, if you want to have higher control over what you brew, the French Press could be your best pick, but if you prefer simplicity, the Drip could be your number one.\n- Coffee Taste\nThis is ultimately the most important factor. Getting the best coffee taste every time you brew your coffee is the most fulfilling experience as you kick start your day or cool down after a long day at work. The coffee taste is however more subjective and personal. While some people consider bold and exciting flavors for the best cup of coffee, others consider light and more direct cup of coffee. So the question is which are you?\nYou will have more control over the kind of coffee taste you will get by taking full control of the brewing process with the French Press. Besides, most coffee lovers who have used the French Press contend that the coffee brewed with the French Press is fuller and richer because it preserves much of the natural oils found in the coffee beans. On the other hand, the Drip Coffee Maker delivers a little ‘weak’ taste as it tends to filter out the natural oils found in coffee beans.\nWhen it is about coffee taste, it is your opinion that counts. If you prefer a lighter coffee taste and you want it stress-free, then the Drip machine is ideal for you. But if you want to explore exclusive coffee tastes, smells, and experiences, then the French Press will certainly come handy.\nSummary of Verdicts\n- Allows more control over the brewing\n- Lower initial cost\nDrip Coffee Maker\n- Keeps coffee warmer for longer\n- Convenient for most coffee enthusiasts\nWhich one is Better?\nGetting a clear winner in this comparison is difficult as both products come with almost even after weighing the pros and cons of each. The one that will work best for you will be determined by how much time you have to brew your coffee, the effort you intend to dedicate to your coffee brewing experiences and the ultimate coffee taste you will want to enjoy.\nBoth of these products are great in different aspects. The difference only lies in what kind of coffee taste you want and what your lifestyle is.']"	['<urn:uuid:e13cb441-0ec7-427b-a6f3-fb61ddcc6178>', '<urn:uuid:8f8ccc1f-93b6-4364-84ed-d36aeb76557c>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T22:03:18.033296	8	58	2719
39	what are special nights islam ten nights meaning significance	The 'Ten Nights' refers to the first ten nights of Zul-Hajj, which witness the largest and most devoted gathering of Muslims in the world. Some also interpret them as the last ten nights of Ramadan (when the Qur'an was revealed) or the first ten nights of Muharram. These nights hold great importance as evidenced by Allah swearing by them in the Quran.	['An Enlightening Commentary of the Holy Qur’an\nIn The Name of Allah, The Beneficent the Merciful!\nSura Fajr – Break of Dawn\nNo. 89 (30 Verses)\nContents of the Sura:\nThis Sura, like many other Meccan Suras, contains short,\nmeaningful, awakening verses with abundant warnings.\nThere are several oaths mentioned in the first part of the Sura\nthat have no precedent and are primarily for warning the transgressors\nabout the divine punishment.\nAnother part of this Sura points to some of the ancient rebellious\nnations, such as the ‘Ad and Thamood people; and also to Pharaoh, the\ntransgressor, and the divine punishment that destroyed them, which is a\nlesson for all arrogant powers so that they may take careful account of\nIn the next part of the Sura, which relates to the previous parts,\nMan’s trial is mentioned and his neglect, in doing good deeds, is very\nIn the last part of the Sura the discussion is about the Hereafter\nand the fate of the sinners and unbelievers, and presents a contrast in\nregard to the great rewards that the believers will receive; those whosc\nsouls are at rest.\nThe Virtue of Studying Sura Fajr:\nRegarding the virtue of studying this Sura, a tradition from the\nProphet (pbuh&hf) says:\n“Allah forgives the mistakes of whoever recites Sura Fajr on the\n‘Ten Nights’ (i.e., the first ten nights of Zul-Hajj), and it will\nbecome a light on Dooms Day for the one who recites it at other\ntimes (of the year).”\nAlso, a tradition from Imam Sadiq (a.s.) says:\n” Recite Sura Fajr, which is Hosaibn ibn Ali’s Sura, in your prayers,\nwhether they be obligatory or optional. He who recites it wiil be\nwith him (Hosain-ibn-Ali) at the same place in Heaven on the Day of\nIntroducing this Sura as Hosain-ibn-Ali’s Sura may be for the\nreason that the clear example of the ‘serene soul’, mentioned at the end\nof the Sura, is Hosain-ibn-Ali (p.b.u k.), as the idea has also been cited\ntrom Imam Sadiq (pbuh&hf) about the same verses.\nOr, perhaps, it is for the same reason that one of the\ncommentarics about the ‘Ten Nights’ means the first ten nights of\nMuharram, (the first month of the Muslim new year), which is closely\nrelevant to Hosain-ibn-Ali (pbuh&hf).\nAt any rate, these great rewards and outstanding merits are for\nthe ones who recite the Sura as a preparation for their own\nself-improvement and self-perfection.\nIn The Name of Allah, The Beneficent the Merciful!\n1. “By the Break of Dawn,”\n2. “And the Ten Nights;”\n3. “By the Even and the Odd;”\n4. “And by the Night when it departs;”\n5. “Is there (not) in this an oath for those who have sense?”\nAn Oath to the Dawn!\nThere are five awakening oaths at the beginning of the Sura.\nAt first, it says:\nQURAN: “By the Break of Dawn,”, “And the Ten Nights;”\nThe term / fajr / originally means ‘break open’, and since the light\nof the dawn breaks the gloom of night it is called / fajr /.\nWe know that the meaning of / fajr / is two fold: (1) / fajr-i-kaib /\n‘the false dawn’ which rises without extending laterally and appears to be\nblack, presenting itself like an obstacle on the horizon, and is compared\nto the tail of a Fox whose narrow end is onto the horizon and its conical\nshaped end is in the mid-sky, (2) /fajr-i-sadiq/ ‘the true dawn’ and is\nlike a stream with white water that becomes visible, rising, filling the\nhorizon with its whiteness and, thereafter, spreads throughout the sky\nwith a special brightness by which the night ends and the day begins.\nIt is the time when everything by which fasting would be broken\nbecomes unlawful to the faster, and when the morning prayer can, then,\nSome commentators have carried the term / fajr /, in this verse, to\nits absolute meaning, that is; whiteness, which is certainly one of the\nsigns of Allah’s Greatness. It is a reference point in the lives of\nhuman beings and all earthly creatures, and the prime glory of the\nvictorious light and the end of faded darkness when the calm sleep ends\nand the movement of living creatures begins. It is for this very life that\nAllah swears by it.\nHowever, some commentators have said that it means ‘ break of\ndawn at the beginning of Muharram ‘.\nStill, others have commented on it as meaning ‘ break of dawn on\nthe day of the Feast of Sacrifice ‘ where the important rituals of the\npilgrimage to Mecca are fulfilled and it follows close to the Ten Nights.\nFinally, there are some who have commented on it as meaning ‘ break of\ndawn for the month of Ramadan’, or ‘ break of dawn on Fridays’.\nAll in all, the verse has such a broad meaning which involves all\nthe above commentaries, though some of its examples are more clear\nand more important than the others.\nSome have considered its meaning even beyond this and have said\nthat the objective point for the term ‘ break of dawn ‘ is ‘ any light that\nglitters in the dark ‘.\nTherefore, the glitter of the light of Islam and Mohammad\n(pbuh&hf) in the gloom of ignorance, at that time, is onc of the examples\nof / fajr / ‘ break of dawn ‘ . Also, the glitter of the break of dawn for\nthe Rise of Hazrat Mahdi (the twelfth Imam) (pbuh&hf), when the world\nwill be completely darkened by corruption, transgression and injustice, is\nconsidered to be another example of this.\nThe rise of Imam Hosain on the bloody plains of Karbala, is\nanother example, when he pulled back the black curtains of deceit\nrevealing the tyrannies of the Ummayides and unveiling the real face\nand nature of those devils.\nFurthermore, all the true revolutions that have occurred in the\nhistory of the world, against disbelief, ignorance, transgression and\ncruelty are, also, examples of / fajr /.\nEven the first light of wakefulness that appears in the darkened\nhearts of sinners and makes them move to repent, is / fajr / ‘ break of\ndawn ‘. Of course, this is an expansion on the concept of the verse,\nwhile the apparent meaning of the verse has the same meaning / fajr /\nwith the sense of ‘ the break of dawn ‘.\nThe oath ‘ By the Ten Nights ‘ is generally understood to be the\nfirst ten nights of Zul-Hajj; nights which are witness to the largest and\nmost devoted gathering of Muslims in the world, and this is an idea\nwhich is narrated by Jabir-ibn-‘Abdillah in a tradition from the Prophet\nSome have also commented on them to mean ‘ the last ten nights\nof Ramadan ‘ in which the nights of /qadr/, (when the Qur’an was\nrevealed) are hidden.\nSome have meant ‘ the Ten Nights ‘ to be the first ten nights of\nMuharram; the first Arabic lunar month.\nIt is also possible to combine these three commentaries together.\nSome narrations, containing the hidden meanings of the Qur’an,\nsay that / fajr / refers to the existence of Imam Mahdi (pbuh&hf), and\n/layalin-‘ashr/ ‘ ten nights ‘ refers to the ten sinless Imams who came\nbefore him (pbuth), and / saf’/ ‘ even ‘ refers to Hazrat Ali and\nFatimah Zahra (pbuth); the latter term is mentioned in the next verse.\nIn any event, the oath to these ten nights is an evidence of their\ngreat importance, since oaths are always made to very important things.\nQURAN: “By the Even and the Odd.”\nCommentators have cited many different meanings for the terms\n/saf’/ and / watr / ‘ even and odd ‘ mentioned in this verse Some of\nthem have given twenty meanings, while some others have gone\nfurther and narrated over thirty-six meanings.\nThe following are the most important ones:\n1. The objective point, here, is the ‘ even ‘ and ‘ odd ‘ numbers.\nAccording to this commentary, Allah has sworn by all even and odd\nnumbers. They are figures around which all calculations and regularities\nrevolve, and cover the existing universe. It seems as if He had said:\n‘By the regulation and calculation’ . In fact, in the world of existence\nthe most important things are regularity, calculation and numbers which\nshape the main foundation of Man’s life.\n2. The objective meaning of /saf/ is ‘ creatures ‘, because they are\nall in pairs, and the objective point of /watr/ ‘ odd ‘ is ‘Allah’,\nWho is unique and has no equal. In addition, created things are all\ncombined by substance and existence which, in philosophy, are called\n‘ combined pairs ‘ . The only infinite entity, which is non-material,\nis Allah. (This meaning has been used in some narrations from the\n3. The objective idea of ‘ even and odd ‘ is that all the creatures of\nthe world, in one respect or another, are either ‘ even ‘ or ‘ odd ‘.\n4. The meaning refers to prayers, some of which are ‘ even ‘ from\nthe point of the number of ‘ rak’at ‘, and some are ‘ odd ‘. (This idea\nhas heen narrated from the sinless Imams (pbuth), too.) Or it means the\nelective prayers of ‘Shaf’ (two rak’ats) and ‘ Watr ‘ (one rak’at) at\ntheÂ end of ‘ the night prayers ‘.\n5. The meaning of /saf/ is the day of / tarwiyah / (the eighth day\nof Zul-Hajj when the pilgrims, in Mecca, travel to ‘Arafat), and / watr/\nis the ‘Arafah Day, when the pilgrims in Mecca remain at ‘Arafat. Or\n/saf/ is the day of the ‘ Feast of Sacrifce ‘, (the tenth of Zul-Hajj)\nand /watr/ is the day of ‘Arafah. (This commentary is also mentioned in\nnarrations from the sinless Imams).\nThe main thing is that if the Arabic sign /al/, in these two terms, is\nused in general, all the above meanings can be applied, as each of these\ncommentaries indicates only one example from the given examples of\n/saf’/ ‘ even ‘ and /watr/ ‘ odd ‘ . Mentioning each of them does not\nmean that it is the exclusive interpretation, but that it is one clear\nexample among the rest.\nHowever, if / al / refers to a particular even and odd number,\nhere, concerning the former oaths, two meanings are the most\nappropriate. The first is that the objective point is the day of the ‘\nFeastÂ of Sacrifice ‘ and the day of ‘ ‘Arafah ‘, which corresponds with the first\nten nights of Zul- Hajj, and the most important pilgrimage rituals are\npractised, then. Or, pertaining to the oath ‘ By the break of Dawn ‘; the\nobjective point is concerning the prayers which are said at the end of\nthe night and before the break of day, which is the most suitable time\nfor praying and supplicating to Allah?; particulary when both of the\ncommentaries are mentioned in the narrations cited from the sinless\nFor the final oath in this group, it says:\nQURAN: “And by the Night when it departs.”\nWhat an interesting concept! The movement of the night is in\nrelation to night, itself, (the term / yasr / based on / sura / ‘ to walk\nat night ‘ here, is written as an analogy instead of /yasri/ due to the pause\nat the end of the verse) as if ‘ night ‘ were a living crcature with senses\nand movement and travelled in the darkness, by itself, moving toward a\nYes, the oath is taken to the darkness which moves to the light; a\nmoving darkness, not a stationary one. Darkness is frightening when it\nbecomes fixed and immobile, but when there is movement unto the\nlight, it becomes valuable.\nSome people have said that the gloom of night is moving over the\nface of the earth and, basically, it is that very moving night which is\nuseful and livable, that is, night alternates continuously with day. So it\nnight stopped permanently on one half of the globc, both the dark and\nthe sunlit halves would die.\nWhat does ‘Night’ mean here? Does it mean every night or is it a\nspecial definite night? Again, there is no agreement among the\ncommentators. If /al/, the Arabic definite article, is used in a general\nsense, it refers to all the nights, which, itself, is a blessing from the\ngifts of Allah and is a phenomenon from the great phenomena of creation.\nAlthough, if it refers to a definite night, relating to the previous\noaths, it means the night before ‘ the Feast of Sacrifice ‘ when the\npilgrims of Mecca go from ‘AraNat to the Sacred Monument,\n/muzdalafah/, and, spending the night in that sacred place, go toward\nMina at sunrise. (T place on the night before ‘ the Feast of\nSacrifice ‘ and sees, with his own eyes, the exact meaning of this verse:\nQURAN: “And by the night when it departs.”\nIn any case, night, with either meaning it has (general or definite),\nis one sign among the many signs of the Divine Dignity, and is one of\nthe very important factors in the existing world. It moderates the\ntemperature of the weather. It gives calmness to every creature, and\nprepares a still and quiet atmosphere for worship and supplication to\nAllah. The night before ‘ the Feast of Sacrifice ‘, which is called ‘ the\ngathering night ‘, is also one of the most wonderful nights of the year at\nthe Sacred Monument, /muzdalafah/.\nFurthermore, if these five items (the oath by break of dawn, the\nTen Nights, Even, Odd, Night when it departs) can be considered as\nrelating to the specific days of Zul-Hajj and the great rituals of Hajj,\ntheir relationship will be clear.\nAnd, if it is not clear yet, a collection of great events of the divine\ncreation and divine religious rituals are pointed out, which are signs of\nthe Dignity of Allah and are a wonderful phenomenon in the existing\nAfter expressing these meaningful awakening statements, it says:\nQURAN: “Is there (not) in this an oath for those who have sense?”\nThe term / hijr /, here, means ‘ sense, understanding ‘ and\noriginally means ‘ anything forbidden ‘, for instance, it is said: ‘ The\njudge forbade him to use his wealth ‘. A skirt is also called /hijr/,\nmeaning protection and forbidding others entrance, and since /’aql/\nwisdom ‘ also forbids man from doing wrong deeds it has been rendered\n/hijr/ . The word /’aql/, itself, means ‘ keeping back ‘; therefore, the\nrope with which the leg of a camel is hobbled is called / ‘ iqal /.\nWhat are these oaths for? There are two possibilities. The first is\nthat they are for the sentence of verse 14: “Surely your Lord is ever\nThe second is that what they are for is not mentioned, but means\nthat they are about the punishment of the wrong doers. The meaning\nmay be found in verse 13, in that, a scorge of diverse chastisement will\nbe poured on the unbelievers and transgressors. In this way, the oaths\nand what they are for can be made clear.']	['<urn:uuid:8be691a6-18ad-4fc1-89ff-5b3e561a071e>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T22:03:18.033296	9	62	2553
40	I'm working on memory management systems and noticed both pmalloc and SGen use block-based allocation - what are the key differences in how they handle pinned objects in their respective block management approaches?	In pmalloc, pinned objects remain in place while other data can be write-protected, leaving unused space in blocks that cannot be reused until objects become unpinned. In contrast, SGen's mark-and-sweep collector specifically exempts blocks with pinned objects from its evacuation process, while handling individual objects being freed and new objects filling spaces. SGen's approach deals with many small holes between objects, while pmalloc expects only a small number of pinned objects leaving larger free regions.	"[""Date: Tue, 30 Jan 2018 17:14:45 +0200 From: Igor Stoppa <igor.stoppa@...wei.com> To: <jglisse@...hat.com>, <keescook@...omium.org>, <mhocko@...nel.org>, <labbott@...hat.com>, <hch@...radead.org>, <willy@...radead.org> CC: <cl@...ux.com>, <linux-security-module@...r.kernel.org>, <linux-mm@...ck.org>, <linux-kernel@...r.kernel.org>, <kernel-hardening@...ts.openwall.com>, Igor Stoppa <igor.stoppa@...wei.com> Subject: [PATCH 5/6] Documentation for Pmalloc Detailed documentation about the protectable memory allocator. Signed-off-by: Igor Stoppa <igor.stoppa@...wei.com> --- Documentation/core-api/pmalloc.txt | 104 +++++++++++++++++++++++++++++++++++++ 1 file changed, 104 insertions(+) create mode 100644 Documentation/core-api/pmalloc.txt diff --git a/Documentation/core-api/pmalloc.txt b/Documentation/core-api/pmalloc.txt new file mode 100644 index 0000000..934d356 --- /dev/null +++ b/Documentation/core-api/pmalloc.txt @@ -0,0 +1,104 @@ +============================ +Protectable memory allocator +============================ + +Introduction +------------ + +When trying to perform an attack toward a system, the attacker typically +wants to alter the execution flow, in a way that allows actions which +would otherwise be forbidden. + +In recent years there has been lots of effort in preventing the execution +of arbitrary code, so the attacker is progressively pushed to look for +alternatives. + +If code changes are either detected or even prevented, what is left is to +alter kernel data. + +As countermeasure, constant data is collected in a section which is then +marked as readonly. +To expand on this, also statically allocated variables which are tagged +as __ro_after_init will receive a similar treatment. +The difference from constant data is that such variables can be still +altered freely during the kernel init phase. + +However, such solution does not address those variables which could be +treated essentially as read-only, but whose size is not known at compile +time or cannot be fully initialized during the init phase. + + +Design +------ + +pmalloc builds on top of genalloc, using the same concept of memory pools +A pool is a handle to a group of chunks of memory of various sizes. +When created, a pool is empty. It will be populated by allocating chunks +of memory, either when the first memory allocation request is received, or +when a pre-allocation is performed. + +Either way, one or more memory pages will be obtained from vmalloc and +registered in the pool as chunk. Subsequent requests will be satisfied by +either using any available free space from the current chunks, or by +allocating more vmalloc pages, should the current free space not suffice. + +This is the key point of pmalloc: it groups data that must be protected +into a set of pages. The protection is performed through the mmu, which +is a prerequisite and has a minimum granularity of one page. + +If the relevant variables were not grouped, there would be a problem of +allowing writes to other variables that might happen to share the same +page, but require further alterations over time. + +A pool is a group of pages that are write protected at the same time. +Ideally, they have some high level correlation (ex: they belong to the +same module), which justifies write protecting them all together. + +To keep it to a minimum, locking is left to the user of the API, in +those cases where it's not strictly needed. +Ideally, no further locking is required, since each module can have own +pool (or pools), which should, for example, avoid the need for cross +module or cross thread synchronization about write protecting a pool. + +The overhead of creating an additional pool is minimal: a handful of bytes +from kmalloc space for the metadata and then what is left unused from the +page(s) registered as chunks. + +Compared to plain use of vmalloc, genalloc has the advantage of tightly +packing the allocations, reducing the number of pages used and therefore +the pressure on the TLB. The slight overhead in execution time of the +allocation should be mostly irrelevant, because pmalloc memory is not +meant to be allocated/freed in tight loops. Rather it ought to be taken +in use, initialized and write protected. Possibly destroyed. + +Considering that not much data is supposed to be dynamically allocated +and then marked as read-only, it shouldn't be an issue that the address +range for pmalloc is limited, on 32-bit systems. + +Regarding SMP systems, the allocations are expected to happen mostly +during an initial transient, after which there should be no more need to +perform cross-processor synchronizations of page tables. + + +Use +--- + +The typical sequence, when using pmalloc, is: + +1. create a pool +2. [optional] pre-allocate some memory in the pool +3. issue one or more allocation requests to the pool +4. initialize the memory obtained + - iterate over points 3 & 4 as needed - +5. write protect the pool +6. use in read-only mode the handlers obtained through the allocations +7. [optional] destroy the pool + + +In a scenario where, for example due to some error, part or all of the +allocations performed at point 3 must be reverted, it is possible to free +them, as long as point 5 has not been executed, and the pool is still +modifiable. Such freed memory can be re-used. +Performing a free operation on a write-protected pool will, instead, +simply release the corresponding memory from the accounting, but it will +be still impossible to alter its content. -- 2.9.3\nPowered by blists - more mailing lists\nConfused about mailing lists and their use? Read about mailing lists on Wikipedia and check out these guidelines on proper formatting of your messages."", 'In the last post of my series on SGen, Mono’s new garbage collector, I talked about the young generation, called the nursery, and its collector. In this post I shall talk about the old, or major, generation and the two different collectors that implement it.\nThe older of the two, the copying collector, is mostly used for reference nowadays. It has been supplanted by the Mark-and-Sweep collector in practical use because it performs better in most cases and still has room for improvement.\nThe Mono man page gives details on how to specify which collector to use and how to change their parameters with the environment variable\nThe copying major collector is a very simple copying garbage collector. Instead of using one big consecutive region of memory, however, it uses fixed-size blocks (of 128 KB) that are allocated and freed on demand. Within those blocks allocation is done by pointer bumping.\nAt a major collection all objects for which this is possible are copied to newly allocated blocks. Pinned objects clearly cannot be copied, so they stay in place. All blocks that have been vacated completely are freed. In the ones that still contain pinned objects we zero the regions that are now empty. We don’t actually reuse that space in those blocks but just keep them around until all their objects have become unpinned. This is clearly an area where things can be improved.\nWhy two collectors?\nThe copying collector was implemented when SGen was still young. It shares most of the copying machinery with the nursery collector, so it was easy to implement. As a production collector, however, it is not suited for most workloads. The old generation is expected to be large and composed of objects that have a long life, so copying them is bad from the perspective of memory usage (it might double for the duration of the collection) as well as of cache behavior.\nMark-and-Sweep is SGen’s default major collector and is actively being developed and improved.\nIn contrast to the copying collector a mark-and-sweep collector has to deal with individual objects being freed and new objects filling up that space again. (This is different from pinned objects leaving holes in blocks for the copying collector because in that case it is expected that there will only be a tiny number of remaining pinned objects, so the free regions will be large, whereas here it is expected that many objects survive, leaving lots of small holes). This creates the problem of fragmentation – lots of small holes between objects that cannot be filled anymore, resulting in lots of wasted space.\nThe basic idea of SGen’s Mark-and-Sweep collector is to have fixed-size blocks, each of which is divided into equally sized object slots of a variety of different sizes – this is similar to what Boehm does. One advantage of this over allocating objects sequentially is that the holes are always the same size, so if there are new objects arriving at that size the holes can always be filled.\nOf course it is not feasible to support blocks with slots of every conceivable object size, so we space out the different supported sizes so as not to waste too much space (in the future we might want to choose the sizes dynamically, to fit the workload). Another thing that works to our advantage is that, since the slots have a known size, we don’t have to keep tabs on where they start and end. All we need is one mark bit per object which we keep in a separate region for better memory locality (we could use unused bits in the first object word, like the nursery collector does). For faster processing we’re actually keeping one bit per potential object start address, i.e. 1 bit per 8 bytes.\nSince we only have a limited number of different slot sizes, we can also keep free lists, so allocation is more efficient because it doesn’t involve search.\nDuring garbage collection it is necessary to get from an object to its block in order to access its mark bit. The size of each block is 16 KB, and they are allocated on 16 KB boundaries, so to get from an object that is inside a block to the address of its containing block a simple logical\nAND is sufficient. Unfortunately, large objects live outside of blocks and for them this operation would yield a bogus block address. To determine whether an object is large we need to examine its class metadata which is referenced from its VTable, requiring three loads (the “fixed heap” variant of the collector solves this problem and is described below.)\nEach block has a “block info” data structure that contains metadata about the block. Apart from the size of the object slots and several other pieces of data it also contains the mark bits.\nIn the standard Mark-and-Sweep collector the blocks are allocated on demand, so they are potentially scattered all over the process’s address space. The fixed heap variant, on the other hand, allocates (using\nmmap) a heap of a fixed size (excluding the large object space) on startup and then assigns blocks out of that space. The advantage here is that we can check whether a pointer is within a block simply by checking whether it falls within the bounds of the allocated space, so no loads are necessary to get to its mark bit.\nIn standard Mark-and-Sweep the first word of each block links to its block info. Fixed heap can forego this load as well because the block infos are also allocated sequentially, so all that is needed to get to a block info is its index which is the same as its block’s index, which can be determined from the block address.\nThe obvious disadvantage of fixed heap Mark-and-Sweep is that the heap cannot grow beyond its fixed size. On some workloads fixed heap yields significant performance improvements. I will try to give some benchmarking results in a later post.\nObjects that don’t contain references don’t have to be scanned, which implies that they also don’t have to be put on the gray stack. To handle this special case more efficiently Mark-and-Sweep uses separate blocks for objects with versus without references. The block info contains a flag that says which type the block is.\nFor the fixed heap collector this separation means that we don’t even have to load a single word of an object or its metadata if it doesn’t contain references.\nEven despite the segregation of blocks according to fixed object sizes one form of memory waste can still occur. If the workload shifts from having lots of objects of a specific size to having only a few of them, a lot of blocks for that object size will have very low occupancy – perhaps a single object will still be live in the block and no new objects will arrive to fill the empty slots.\nUnlike Boehm, SGen has the liberty to move objects around. During the sweep stage of a collection we keep statistics on the occupancy of blocks of all slot sizes. If the occupancy for a given slot size falls below a certain (user definable) threshold then we mark that slot size for evacuation on the next major collection.\nWhen blocks of a slot size are being evacuated Mark-and-Sweep acts like a copying collector on the objects within those blocks. Instead of marking them they are copied to newly allocated blocks, filling them sequentially and emptying all the sparsely occupied ones which will then be freed during the following sweep.\nBlocks with pinned objects are exempt from this whole process since they cannot be freed anyway.\nNote that the occupancy situation might change between a sweep and the following major collection – the workload could change in the mean time and produce lots of live objects of that size which would all be copied, without actually saving any space. SGen cannot tell which objects will be live without doing the mark phase because that’s the mark phase’s job, so we take the risk of that happening.\nAs a side-note: Sun’s Garbage-First collector solves this problem by running the mark phase concurrently with the mutator, so it is actually able to tell at the start of a garbage collection pause which blocks will contain mostly garbage.\nThe mark phase can be parallelized with reasonable effort. Essentially, a number of threads each start with a set of root objects independently. Each thread has its own gray stack, but care must be taken that work is not repeated. Two threads can compete for marking the same object, and if they both put the object into their gray stacks, the object will be scanned twice, which is to be avoided.\nSome care must also be taken with handling objects that still reside in the nursery. They have to be copied to the major heap and a forwarding pointer must be installed. The forwarding pointer can only be installed after the space on the major heap has been allocated (otherwise where would it point to?), so two threads might do that independently but only one will win out and the other will have done the allocation in vain.\nIt is not actually sufficient to just partition the set of root objects between the worker threads – some threads might end up having a far smaller share of the live object graph to work with. SGen currently uses a shared gray stack section buffer where worker threads deposit parts of their gray queues for the others to take.\nParallel mark achieves quite impressive performance gains on some workloads, but on most the gains are still somewhat disappointing. I suspect the work distribution needs improvement.\nThe sweep phase goes through all blocks in the system, resets the mark bits, zeroes the object slots that were freed and rebuilds the free lists.\nNone of those data structures are actually needed until the next nursery collection, so the sweep lends itself to be done in the background while the mutator is running, which is what Mark-and-Sweep can do optionally. The next nursery collection will in that case wait for the sweep thread to finish until it proceeds.']"	['<urn:uuid:a415ed5e-823f-4849-abb9-3460a9e2bf79>', '<urn:uuid:4effd7fb-a313-4d16-ae94-caf37800486f>']	factoid	with-premise	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-12T22:03:18.033296	33	75	2579
41	effective methods boost student learning engagement	To motivate students for better learning, teachers should: make lessons interesting and appealing, develop a state of need by exposing students to lesson objectives, structure learning materials to allow success in earlier steps, provide continuous feedback, use incentives judiciously, create healthy competition, make learning relevant to student experiences, develop curiosity by widening students' experiential horizon, make learning pleasant and worthwhile, and develop positive attitudes toward learning.	['Another important factor in determining an individual’s tendency to learn is motivation. Motivation is concerned with an individual’s internal as well as external energies in the direction of an intended goal.\nAn individual cannot learn effectively if he is not motivated to do so. Hence, teachers need to understand the concept of motivation and how it applies to teaching and learning processes. This is the focus of this unit.\nWhat is Motivation\nThe term motivation has been defined in several ways. In a nutshell, motivation answers the question: why a behavior? That is, why does a person behave in the manner he behaves.\nAn answer to this question is pertinent for teachers. If teachers can understand the reason for the actions or responses of their students, then they can be able to facilitate learning in the classroom.\nA more simplified definition of motivation is as follows:\n– Motivation is the force or condition within the organism that impels it to act or respond.\n– Another definition claims that motivation refers to the biological, social and learned factors that initiate, sustain and stop goal-directed behavior.\nFrom these definitions three types of motivation may be discerned:\n1) Extrinsic Motivation:\nThese include those forces outside an individual that regulates his behavior. The energizing motive is something external to the individual (e.g. food, water etc.). Teacher use of this type of motivation needs to be judicious and commensurate with effort put in. Teachers should avoid use of expensive things to motivate students.\n2) Intrinsic Motivation:\nThese include forces within the individual. It has nothing to do with the external world. It is a sort of motivation that comes as a result of an inherent feeling of self-satisfaction as one participates in learning activity.\nThis kind of motivation is tied to the learning task itself. Therefore, teachers need to provide relevant simple and appropriate learning task to the learners if this type of motivation is to be sustained.\n3) Achievement Motivation:\nThis refers to the motivation for achievement in the school, place of work, profession etc. It implies the craving for accomplishment, conquest, competence etc.\nAchievement of success in learning is thus determined by student’s motive to achieve. Thus, if a learning task that has low probability of success is given little incentive, achievement will likely be curtailed than if the incentive is high.\nTheories of Motivation\nThere are numerous theories of motivation. Some of these are briefly stated as follows:\n1) Hedonistic Theory\nThose who subscribe to this theory are of the view that human actions are mainly triggered by a desire for pleasure and the avoidance of pain. In other words people engage in activities for no other reason than the pleasure associated with them.\n2) Physiological Equilibrium Theory\nThis theory holds that human beings are supposedly by a balanced state of physiology. This balanced state is called ”Homeostasis”. Ideally, as long as this state of equilibrium is maintained, it seems there will be no need for human activity.\nHowever, because this equilibrium is inevitably distorted presumably from birth, human activity begins with the motivating force being the desire to recapture the physiological equilibrium. Thus, hunger, thirst, etc., signify a disequilibrium that leads to need, drive and consequently activities to satisfy them.\n3) Drive Reduction Theory\nThis theory postulates that human beings act in order to reduce drive. A drive is a state of tension which occurs whenever a need (want or deficiency) exists. The need could be physiological or social.\nThe motivational force wherever a drive exists is reduction of the accompanying tension, which is usually unbearable.\n4) Cybernetic Theory\nThis theory sees motivation as being intrinsically attached to human activity. The theory claims that motivational force flows from within the individual rather than being a result of pursuing outside incentives.\nForces such as curiosity, competence, and reciprocity are three forms of intrinsic motivation.\n5) Conditioning Theory\nThis theory sees motivation purely in terms of antecedent factors or consequence of behavior. Human beings act in anticipating positive consequences of their actions.\n6) Maslow’s Growth Theory\nThis theory views motivation as a growth process moving from a lower to higher level. All human activities are motivated by the desire to pass through what he calls the hierarchy of motives. Human beings act to satisfy hierarchy of needs hierarchy is as follows.\nHigh 5 Self Actualization\n3 Belongingness and love affiliation\n2 Safety needs\nLow 1 Physiological\nWays of Motivating Students for Better Learning\nThe following ways are recommended for teachers on how to motivate students for effectively learning.\n- Make your lesson interesting and appealing.\n- Develop a state of need in the students by exposing them to the objectives of the lesson.\n- Structure your learning situation and materials in a way that allows success in earlier steps, reinforce later steps.\n- Provide adequate feed back to students on a continuous basis.\n- Make judicious use of incentives. Evolve a token economy system.\n- Develop an atmosphere for healthy competition among students.\n- Make all learning meaningful and relevant to the experiences of the students.\n- Develop curiosity by widening the experiential horizon of the students.\n- Make all learners pleasant, desirable and worthwhile.\n- Develop positive attitude and intentions toward learning in students.\nOne of the basic learning condition is motivation. Without motivation, human beings cannot act or function properly. Equally important is motivating students for effective learning. Without applying effective strategies for motivation, students learning cannot take place.\nRead Also: Meaning of Educational Psychology\nHave you visited our Market Place Today? Follow this link to visit Globalinfo247.com Market Place now to check out our affordable products & services that might interest you and solve your current needs at a very cheap price. You can also Advertise your Own Products & Services at the Market Place for Free by clicking on this link to Get Started!\nCreate a thread for all your related questions to get answers from other members and professionals in the field. Click here on the “Questions & Answers” Section to view or submit your Questions or Answers to previously asked related questions.']	['<urn:uuid:2c53296b-c7df-4839-9c48-a504ce6394de>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T22:03:18.033296	6	66	1015
42	As a legal historian, when did Athens start using written laws, and what was their impact?	Written laws in Athens were introduced by Draco around 621/620 BC, marking a shift from arbitrary oral laws to publicly displayed laws engraved in wood or stone. While this change made laws accessible to all literate citizens instead of just the elite, Draco's written code proved extremely harsh, with death penalties for minor offenses like stealing fruit or sleeping in public places. This system only lasted about 30 years before being largely abolished and replaced by Solon's reforms.	['The death penalty in Classical Athens\nThis entry was posted on April 6, 2017.\nThe latest issue of Ancient History has an article by Murray Dahm on the death of Socrates. We don’t really go into much detail in the issue as far as Classical Athenian law is concerned. For this blog post, I want to use the death of Socrates to briefly discuss how the Athenians of the fifth and – especially – the fourth centuries BC handled the most extreme form of punishment: the death penalty.\nThe law in Classical Athens\nWhen writing became widespread, laws were recorded to ensure that justice and punishment were no longer arbitrary. Laws weren’t scribbled onto papyrus rolls and hidden; instead, they were engraved into wood or stone and put up for public display. As Douglas MacDowell, a foremost authority on ancient Greek law, put it: ‘A Classical Greek could think of a law as a physical object’ (The Law in Classical Athens , p. 42).\nThe earliest known Athenian lawgiver was Dracon, who wrote down a set of laws in 621/620 BC. The punishment for breaking almost any law was supposedly death (according to Plutarch), and it’s perhaps no surprise that we derive our word ‘draconian’ from this man. It’s difficult to gauge whether or not Plutarch’s summary is accurate. The idea of the Athenians accepting laws that would sentence people to death for relatively minor offences like stealing apples or being convicted of ‘idleness’ seems odd to say the least.\nSince we’re relatively well informed about the laws of Classical Athens, there are a few books available for those who want to learn more. Aside from the book by MacDowell cited earlier, you can also check out his Athenian Homicide Law in the Age of Orators (1963), A.R.W. Harrison’s two-volume study The Law of Athens (1968 & 1971), and S.C. Todd’s more modern The Shape of Athenian Law (1993). MacDowell also wrote a very handy little book called Spartan Law (1986) that offers a nice contrast with the works on Classical Athens.\nThe death penalty was the most severe form of punishment in Athens and usually reserved only for the most severe offenders of the law. Socrates’ case is, in fact, rather peculiar, and Murray Dahm does a good job in issue 9 of summarizing how the sentence came to be. The death sentence was generally reserved for those who had been found guilty of intentional homicide. By contrast, the punishment for unintentional homicide was generally exile, and the person found guilty of that was allowed to keep his property.\nIn an Athenian homicide case, each side was allowed to make two speeches, with the prosecutor and the defendant speaking in turn. At the end of his first speech, the defendant was allowed to go into exile, which was a convenient way for someone who feared conviction of saving his own life. Once the speeches were over, the jury did not leave to deliberate, but immediately cast their votes. The presiding magistrate (called a basileus, ‘king’) pronounced verdict based on the vote. Someone found guilty of intentional homicide would be sentenced to death and his property would be confiscated.\nOf course, these rules applied to Athenian citizens who had been found guilty of murdering a fellow Athenian citizen. We’re not as well informed about murder cases involving the slaying of a foreigner, a resident alien, or a slave, but the general assumption is that punishments were less severe, ranging from possible exile to almost certainly the payment of a fine. In the case of a slave, the fine would have had to be paid to the slave’s owner; in the case of an alien, to that person’s relatives.\nThe death penalty in Classical Athens\nThe punishment was usually meted out immediately after the trial. Again, the case of Socrates was exceptional. We know of three different forms of execution. The first involved throwing the sentenced person down a deep chasm or pit (barathron or, in the fourth century BC, orygma). Later, it seems that people were first killed and their remains tossed into the pit. MacDowell suggests that throwing living people down a chasm for them to die – on impact or later – was no longer practised in the fourth century BC (op. cit., p. 255).\nThe second method was through the use of a tympanon. This has been the source of some confusion. A late lexicographer, as explained by MacDowell in his Athenian Homicide Law (pp. 111 – 12), explained that the tympanon was shaped like a club, and that it was later replaced by a sword, suggesting that criminals were beaten to death. MacDowell and other writers, including Todd, suggest that it was a board stuck into the ground to which the victim was fastened and left to die of exposure and thirst, in a manner roughly similar to crucifixion.\nThe third method was death by drinking hemlock. This was how Socrates ultimately met his end. Supposedly, the Thirty Tyrants in 404/403 BC had sentenced a few other men to drink hemlock. But these instances are rare, and limited to the years around 400 BC. Death by hemlock appears not to have been used before 404/403 or after 399 BC, except in the case of suicides. As such, it seems unlikely that it was ever very common.\nMacDowell writes that it ‘is remarkable that neither hanging nor decapitation seems to have been used.’ I’m not sure that this is so remarkable. Murdering a person caused religious pollution (miasma), from which a person had to be ritually cleansed. This is the reason why, in so many stories of Greek mythology, unwanted infants are left to die from exposure rather than simply killed – the act of killing a person would have had serious consequences on the executioner and whoever aided him (possibly including the jury and the presiding magistrates).\nInstead, the ancient Greeks in general preferred to sentence people to death in indirect ways: by throwing them into a precipice, tying them still alive to a board to die of exposure, or indeed by having the convicted criminal drink a cup of hemlock. In this way, the people involved in carrying out the sentence did not run the risk of being regarded as murderers themselves.', 'Written in Human Blood: Draconian Laws and the Dawn of Democracy\nIn ancient Greece, since laws were unwritten at first, the social pyramid’s elite of nobles and rich men interpreted and twisted the aforementioned laws for their own exclusive benefit. Of course, as usually happens in such cases, this elite claimed the laws were god-given, and subsequently, holy and to be blindly followed by all members of society… except themselves. At this time, however, something somewhat new to recorded history started to happen- certain people consistently began to think more logically about such laws, and finally initiated protests against the so-called “noble and holy habit” of creating and interpreting laws. The common people, thus, demanded a single, written and organized legal system that would be fair and practical for every social class.\nIn this way, Draco, the first legislator of Athens in ancient Greece, is introduced to us. Though, Draco wasn’t the first person to document a complete system of laws, he was the first democratic legislator in recorded history, given that he was the first lawgiver to write a codes of law following overwhelming request from the citizens of any ancient city-state. Simultaneously, the initiation and realization of written law came with an extremely high cost for local Athenian society, and actually worked to the disadvantage of the same citizens who had petitioned Draco to record a legal system.\nNowadays, we may not hear the expression, “Draconian Laws,” very often, but for much of recorded history, this term in various languages has been synonymous with harshness, severity, brutality, and unmitigated terror. However, what were the Draconian Laws? Where does the name originate from, and most importantly, why did it inspire so much fear and intimidation?\nAs briefly mentioned, the Draconian Laws were ancient Athens’ established code of law, reportedly introduced to local Athenian society by lawmaker Draco around 621 BCE, from whom the laws take their name. Draco, one of the ancient and most prominent tyrants, also lived strictly and had a decorated professional career, making him the ideal lawmaker in the eyes of fellow Athenians. Although usually regarded as the first lawmaker in Athenian history, in reality, six men preceded him in institutionalizing unwritten laws and codes.\nIn 624 BC, Draco’s fellow Athenian citizens commissioned him to write Athens’ code of law. By 621 BC, he delivered a comprehensive and thorough legal system, written on plates to be placed publicly in the Athenian Agora. Instead of oral laws arbitrarily applied and interpreted, as well as known only to a limited number of citizens who belonged to the upper social class, all laws now were publicly written, and thereby known to all literate citizens.\nThese laws, though fundamentally fair and applicable for every citizen regardless of heritage, social class, or wealth, proved to be extremely strict. The punishments for minor offenses were ridiculously harsh. For instance, the theft of a piece of fruit or sleeping in a public place were punishable by death! Similarly minor offenses could easily see a person turned from a free citizen to a slave.\nThe Draconian Laws’ harshness and inhumanity, as a result, caused a lot of controversy and resentment within diverse circles of Athenian society. Furthermore, according to Aristotle, these laws were first written in human blood rather than ink. In addition, one of the greatest minds and philosophers of the time, Plutarch of Athens, openly challenged Draco’s choices and claimed Draco’s laws were the work of an insane man. Plutarch thus wrote in the Life of Solon,\nIt is said that Draco himself, when asked why he had fixed the punishment of death for most offenses, answered that he considered these lesser crimes to deserve it, and he had no greater punishment for more important ones.\nDespite its many flaws and brutal punishments, Draco’s Code of Laws became the first in known history to differentiate between unintentional homicides, usually punished by sentence of exile, with intentional murders, in contrast, punished by the death penalty.\nThis contribution of Draco to Athenian law and society, however, wasn’t enough to maintain his harsh written code of laws very long. Only thirty years later, most of the Draconian Laws were abolished and replaced successfully by Solon’s reforms and new laws. Solon, widely considered the father of Modern Western Law, did not manage to fully pass legislation against the political and moral deterioration of his age. Still, he led Athenian society out of the dark and harsh “Draconian Era” and most historians credit him as the Athenian who laid the foundations for the city-state’s later “perfect” democracy.\nIn the end, Draco’s story and his infamous Draconian Laws, offers us much food for thought and teaches us numerous political lessons. Draco, though commissioned by the majority of his co-citizens, and not the “gods” like his predecessors, failed to deliver a people-friendly legal system. Instead, he became notorious for delivering one of the most inhumane and harshest legal codes the world has ever known. Nevertheless, from the ashes of that failure a revolutionary system slowly emerged which would soon set the foundation for a significant portion of modern law and politics in the Western world.\n- Nero Didn’t Fiddle While Rome Burned\n- The Amazing Acoustics of the Epidaurus Theatre\n- While the Roman Colosseum is More Famous Today, Its Predecessor, the Circus Maximus, Could Hold About 3 to 6 Times More People\n- Julius Caesar was Once Kidnapped by Pirates Who Demanded a Ransom of 20 Talents of Silver, Caesar Insisted They Ask for 50\n- The word “democracy” comes from the Ancient Greek “dēmos”, meaning “the people”, and “-kratia”, meaning “power/rule”.\n- Draco’s death is also of special interest. Athenian citizens could not hide their appreciation for the famous lawmaker, despite the excessively harsh nature of his laws. Such “excessive” appreciation was destined to be fatal for Draco’s life, literally. As he visited the island of Aegina to be honored before a large crowd during a theatrical event, Draco was covered in so many of the caps, cloaks, and other articles the audience continuously threw at him, that he suffocated to death. Some considered the accident horrible and ironic, while others assume hidden motives were behind his death, and claim Draco was intentionally killed.\n|Share the Knowledge!|']	['<urn:uuid:d192781a-0677-403f-95af-83928b397824>', '<urn:uuid:a63949db-2ea9-4c0b-868d-e6520d4ba860>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T22:03:18.033296	16	78	2087
43	recovery time dbs tavr hospital stay comparison	For DBS, patients usually stay in the hospital for a few days for preoperative tests, planning, implant procedure, and initial recovery. For TAVR, patients typically spend a day or more in the intensive care unit (ICU) and another day or two in a patient room.	"[""Procedure: What to Expect Deep Brain Stimulation for Dystonia\nSo that you can receive DBS Therapy, a device similar to a pacemaker is placed under the skin in your chest. Very thin wires connect the device to your brain to enable the signals to reach the source of your symptoms. Here's what to expect during and after the procedure that makes therapy possible.\nThe duration and steps of the implant procedure can vary, and the procedure typically lasts several hours. The hospital stay is usually a few days for the preoperative tests, planning, implant procedure, and initial recovery before home care.\nPeople who have had the procedure usually describe it as demanding and exhausting rather than painful. Afterwards, you may have some discomfort and soreness that can be managed with pain medication.\nDuring the surgery to implant your DBS system for dystonia*, your doctor will use the following techniques to determine the area in your brain where the leads will be placed.\nThe neurostimulator may be implanted the same day or later. You will be sedated and asleep for this part of the procedure. The surgeon begins by checking to see that the leads are properly positioned.\nThe neurostimulator is placed under the skin of your chest just below the collar bone. The surgeon will also connect the lead to the neurostimulator with extensions that are placed under the skin, leading up from the chest to your neck and head.\nPeople usually go home a few days after the surgery. Healing can take several weeks. Discomfort or pain at the incision sites can be managed with medication. When you are sent home to heal, typically your device will not be turned on until your first programming session.\nFor several weeks you will avoid strenuous activity, arm movements over your shoulder, and excessive stretching of your neck. You may gradually want to try activities that were difficult before your surgery. Talk about this with your doctor first, and be sure to follow all of your doctor's instructions.\nAfter you have healed from the procedure, your doctor will program the device to best control your individual symptoms while minimizing side effects. You will return for follow-up sessions to further adjust the settings. Periodic adjustments are a routine part of DBS Therapy.\nThe full effect of the therapy may not be immediate. You will see the best results after the system has been fine-tuned for your specific symptom control needs. It may take several months to reach maximum effect.\nDepending on the system and your therapy needs, you may have a controller that will allow you to turn the system on and off, adjust the stimulation, and check the battery.\nPlacing the DBS system requires brain surgery, which could have serious and even fatal complications including coma, bleeding inside the brain, stroke, seizures, and infection. Once implanted, the system may become infected, parts may wear through the skin, and the lead and/or extension connector may move. Medtronic DBS Therapy could stop suddenly because of mechanical or electrical problems. Any of these situations may require additional surgery or cause symptoms to return or worsen, which may be life-threatening. Medtronic DBS Therapy may cause new or worsening neurological or psychiatric symptoms.\nIn patients receiving Medtronic DBS Therapy, depression, suicidal thoughts, and suicide have been reported.\nThis therapy is not for everyone. This therapy should not be used for patients who will be exposed to diathermy (deep heat treatment) or transcranial magnetic stimulation. Magnetic Resonance Imaging (MRI) should only be performed as described in the product labeling. The DBS system may interact with other medical devices and electromagnetic interference.\nTalk to your doctor about the risks that may be applicable to your specific situation.\n*Humanitarian Device - Authorized by Federal Law as an aid in the management of chronic, intractable (drug refractory) primary dystonia, including generalized and/or segmental dystonia, hemidystonia, and cervical dystonia (torticollis), in patients seven years of age or above. The effectiveness of the devices for treating these conditions has not been demonstrated.\nInformation on this site should not be used as a substitute for talking with your doctor. Always talk with your doctor about diagnosis and treatment information."", 'Transcatheter aortic valve replacement (TAVR) is a minimally invasive procedure to replace the aortic valve in patients with severe aortic stenosis. TAVR is less invasive than open heart surgery, and the procedure typically takes one to two hours. Your care team will determine if you should have a mild sedative or general anesthesia.\nAt the start of the procedure, your doctor will make a small cut in the groin (1), the neck (2), or a space between your ribs (3).\nThe doctor will guide a thin, flexible tube with the heart valve into your artery and to your diseased valve. Throughout the procedure, your doctor will be viewing images of your heart.\nThe TAVR heart valve will be placed in your diseased valve. Your new valve will work immediately. Your doctor will remove the tube and close the incision.\nAfter your procedure, you may spend a day or more in the intensive care unit (ICU) and another day or two in a patient room. Most patients begin walking within a day of their procedure.\nBefore you leave the hospital, your doctor will explain what kinds of activities you can do, if you need to take medication, and when you will need to see your doctor again. You will also be given an information card about your new TAVR heart valve.\nKeep your Medtronic TAVR valve information card with you at all times.\nShare this card with your family members and all members of your healthcare team, including your dentist. If you need an MRI, tell your doctor that you have a Medtronic TAVR heart valve.\nMost patients report they start feeling better right away, but it can take a little longer for others. Many Medtronic TAVR patients report benefits such as:\nYou will be asked to return to the valve clinic to have your heart valve checked at 30 days and one year after your TAVR procedure, and as recommended by your physician.\nIf you have concerns, discomfort, or changes in your health, be sure to let your doctor know right away.\nThere are significant risks associated with TAVR procedures. Each patient is different, and the potential risks and benefits are based on individual treatment and health conditions. Speak to your doctor for more information about what you can expect.\nThe Medtronic TAVR heart valve is designed to work like your own heart valve.\nThe metal frame is a blend of nickel and titanium. This material allows the frame to shape itself to your anatomy.\nThere are two types of Medtronic heart valves — the Evolut™ R valve and the Evolut PRO+ valve — that come in different sizes.\nYour doctor can help you decide which Medtronic TAVR heart valve is right for you.\nThe Evolut PRO+ valve has tissue leaflets and an outer wrap made from pig heart tissue.\nYou should start feeling better right away. This is because your heart valve is now working properly. Some patients may take longer to feel better.\nMost patients feel less pain and less anxious. They can take care of themselves better and go back to everyday activities.\nThe Medtronic TAVR procedure is currently approved for:\nYour doctor can determine your risk category based on several factors, including age and other medical conditions that might make surgery more dangerous for you.\nMost medical procedures have risks. The Medtronic TAVR procedure’s most serious risks are:\nThe chance of an adverse event from the TAVR procedure depends on many factors, including your underlying medical conditions.\nThe Medtronic TAVR valve should not be used in patients who:\nIf the Medtronic TAVR valve is used in these patients, it may not work right. This could make you feel sick or cause death.\nFor some patients, the Medtronic TAVR procedure risks may outweigh the benefits. Please talk to your doctor to decide whether this therapy is right for you.\nYour doctor will check your valve during your regular follow-up visits.\nDiscuss this with your doctor. He or she can help you decide what activities are safe for you.\nIf you need an MRI, tell your doctor that you have a Medtronic TAVR heart valve. Not doing so could result in injury or death.\nThe Medtronic TAVR heart valve cannot be used for patients who:\nAll medical procedures come with risks. Although serious or major complications from the Medtronic TAVR procedure are rare, they can include:']"	['<urn:uuid:0545b683-2f22-4bc7-95d7-af05c4e13c20>', '<urn:uuid:0b4049e4-1506-4ff4-8a88-38db457d7d4e>']	factoid	direct	short-search-query	similar-to-document	comparison	novice	2025-05-12T22:03:18.033296	7	45	1423
44	cost advantages rapid prototyping manufacturing drawbacks limitations finished products	Rapid prototyping offers significant cost advantages in manufacturing, as identifying a design defect with a CNY 1,000 prototype can prevent potential losses of up to CNY 10 million in production and testing. It enables effective control of packaging, transportation and material costs, while allowing timely adjustments to meet cost targets. However, 3D printed products have notable limitations regarding finished products - they typically require extensive post-processing including sanding off layer lines, waterproofing, removing support structures, and manual refinement. This makes them less efficient than traditional manufacturing methods like injection molding for producing seamless, uniform products at an industrial scale.	['Study shows that 80% of the production cost is determined at the design stage. Therefore, the design stage is of great importance to product cost control. Rapid prototyping constitutes an important part in the industrial design process. From the fully manual prototyping (hand sampling) to the semi-mechanical prototyping, then to the modern 3D printing of prototypes, the progress epitomes the technological progress of the modern manufacturing industry. Among them, though Rapid prototyping is an advanced supportive design technique, it doesn’t receive enough attention.\nIt has been proved th Rapid prototyping not only plays a very important role in product design decision, but is also an effective tool for protecting product information and marketing.\nNow, let’s talk about the importance of Rapid prototyping from different stages of the industrial design process\n- Design Optimization\nMechanical parts that are more focused on geometric shapes can be designed directly with the application of computer graphics. For products with curvy surfaces, such as small-sized household appliances, on the one hand, the reverse engineering technology can be leveraged for 3D mapping, to obtain the 3D modeling data. And, after adjustment, the 3D data obtained can be directly copied with the help of the computer-aided rapid prototyping technology. The benefits of this approach are more evident when designing artworks that feature varied curvy designs. Firstly, the designer directly creates the shape with materials such as sludge, and then parameterizes the prototype through a 3D scanning device to obtain the computer data of the original prototype, which is then copied to a rapid prototype, to greatly improve the design efficiency\n- Design Discussion\nDuring the product design process, discussion plays a very important role. Visualization is an important part in the expression of product design, as well as the cornerstone for design discussion. Modeling products quickly and efficiently is able to ensure a higher design efficiency within the same period of time. During the design creative stage, what is used the most is often the 2D expression based on drawings. However, in the middle and late stages, the design expression will be more specific, so a more intuitive prototype should be adopted as the subject of discussion. Compared with the 2D expression, the hand sample enjoys an irreplaceable advantage, because it is able to make the details of the product clearer and the visual perception of the product more intuitive. Since everyone in the design team is able to see these designs, obvious advantages are demonstrated in the coordination of teamwork among all members.\n- Functional Test\nRapid prototyping is able to help with certain functional tests and simulate the final form of the product, including functions and curves, etc. Prototyping allows the designer to obtain the most intuitive understanding of the dynamic simulation and internal structure of the prototype. When materials with a certain strength are used, a functional prototype containing an internal structure can be made to verify whether the product structure is reasonable, whether the wall thickness meets the requirements, and whether the moving parts are smooth enough, etc. A prototype can also be used for aerodynamic testing. For example, in the design of fast-moving objects, such as cars, high-speed trains and spacecrafts, a 1:1 prototype can be made and placed into a wind tunnel for intuitive aerodynamics research.\n- Cost Cut\nA design defect that is identified with a CNY 1,000 worth of prototype may cause a huge loss of up to CNY 10 million to produce and test, if the product is put into production and launched into the market. Through hand sampling, the final shape, size, structure and color of the product can be obtained, or product ergonomic testing can be performed at a lower cost. Also, important product information, such as product volume and quality, can be predicted, which can effectively help with the control of the packaging, transportation and material costs during production. Also based on this, the sales price and profit of the product can be estimated. If the cost exceeds the assumptions at the beginning of design, adjustments can be made in a timely manner until the specified target range of cost is reached. Therefore, Rapid prototyping is an effective means for an enterprise to control the cost.\n- Market Research\nIn a trade fair, the 2D display board or the 3D animation you show to your potential customers will not be as attractive as the prototype. The prototype can most intuitively indicate the shape, color, size, structure and function of the product. You can also use the rapid hand sampling technology to reproduce the prototypes in small batches, and then send them to different regions for investigation, so as to obtain differentiated survey data. The hand sampling technology is able to help predict the actual consumer mindset and application effects of the product before mass production.\nGenerally speaking, with the advancement of modern processing technologies, the Rapid prototyping technology is continuously improved with a declined cost. On such a basis, more attention should be paid to the role the rapid prototyping plays in the whole process of industrial design, to make full use of its advantages, thus improving design efficiency and reducing R&D risks.\nBenefits of the 3D Printed Prototype:\n- It takes only a few to tens of hours to finish the production of one or several prototypes, so the product development cycle can be reduced by more than 40%;\n- There is no need for mold machining, or mold development. You can print prototypes directly and quickly, to greatly reduce product development costs;\n- Dimensional accuracy is able to meet the requirements of industrial grade assembly. The dimensional accuracy of a plastic sample can reach up to ±0.1mm, while that of a metal sample can reach ±20μm;\n- The plastic sample is made of the high-quality engineering plastic material, the nylon 12, which boasts such excellent properties as high strength, high toughness, as well as high resistance to wear, corrosion and temperature.\n- Rapid prototypes are able to meet a wide range of demanding test requirements, such as wind tunnel testing (3,000rpm), water pressure and flow testing (10,000+ cycles);\n- A variety of complex curves and special structures are available at one stroke.', 'With all the parts, models, and designs available today, you’d think that 3D printers can pretty much do anything. After all, they’re being used by everyone, from hobbyists to prosthetics experts. Still, there are some things that just don’t work – well, not yet, anyway.\nHere are 7 things that cannot be 3D printed:\n- Perfect spheres without supports\n- Flammable natural materials\n- Hard natural materials, such as stone\n- Certain metals, including gold and silver\n- Large and intricate objects\n- Complicated designs\n- Completely finished products\nLet’s get into the details and talk about why these materials and shapes aren’t always compatible with 3D printing. I’ll also discuss the exceptions to these rules and tell you about some of the newest ways engineers work around the limitations of 3D printing.\n1. Perfect Spheres Without Supports\nSpheres and perfectly round objects are among the most challenging things to 3D print. This is because, as with every design and model, there must be a base onto which you print.\nIn most cases, there is a flat base that filament is built upon. Without it, anything round would roll away, interrupting the print.\nThat said, it is possible, provided you use some kind of support. Unfortunately, these supports will leave residue behind, which needs to be sanded away once the print is finished.\nCheck out this video to see how it can be done:\nSo, if you want to print a sphere, you will need to ensure that you use proper support structures and prepare for quite a bit of sanding and processing to even out the shape.\nAs of right now, it doesn’t seem like this problem with spheres will ever go away since you will almost certainly need a print bed to print anything, and to keep a globe on the print bed, you will need something flat to stabilize it.\n2. Flammable Natural Materials\n3D printers use heat to melt down materials and shape them into objects. As a result, they cannot print using materials that don’t tolerate heat.\nThese materials include:\n- Organic textile fibers\nStill, engineers have developed some filaments that contain small amounts of these organic materials. Although they will never compare to the real thing, they often look similar to the original material.\nWood filament usually includes about 30% wood particles (sawdust), an exception to the rule of 3D printing with flammable materials. However, this wood usually burns a bit during the printing process.\nThe wood particles are also always mixed with another filament such as PLA, making it less durable and lighter-feeling than natural wood. Wood filaments also usually contain something like cork, which is fire-retardant, to keep your printer from going up in flames.\nFabrics, too, are possible to 3D print, but materials such as cotton, hemp, and linen are far too flammable to be used.\nInstead, textile printing experimenters use materials such as flexible TPU and TPE.\nStill, this intelligent option for fabric production hasn’t made it off the runway yet. That’s because the processing time and materials necessary to print these designs aren’t any better than the traditional processes used to make cotton and other natural fabrics.\nIn addition, most 3D printed fabrics aren’t tightly knit, which usually leaves TPU and TPE fabrics looking like a holey fishnet. So, right now, making clothing and other fabrics with 3D printing methods isn’t possible.\nPaper is far too flammable to print like wood and other natural fibers.\nSo, 3D printing is not the way to go for natural materials. Although 3D printing can make convincing dupes to materials such as wood, fabric, and paper, it can’t manufacture these items efficiently or competently. As a result, the finished product doesn’t often compare to the original.\n3. Hard Natural Materials, Such As Stone\nLike the other natural materials on this list, stone is impossible to 3D print. Rock is far too hard and heat-resistant to turn into a filament, so, by and large, you can’t use it in 3D designs.\nHowever, there are two workarounds:\nStone-filled filament usually contains PLA and stone powder, making your models look a bit like concrete. These are generally made with softer stone varieties such as:\n- Terra cotta\nThese filaments are much like wood ones because they only contain small quantities of natural materials, and the end products still have a plastic-like feel. Likewise, the durability of stone-filled filament prints does not compare to raw, pure stone.\nStone-mimic filaments don’t contain any stone particles, but they can give your 3D prints a stone look. Such prints are not fireproof, which makes 3D printing with stone-inclusive and stone-look filaments more of an aesthetic choice than anything.\nAs of right now, it is frankly impossible to 3D print something as durable and heat-resistant as stone, and it will likely be a very long time before we can make convincing stone objects with fire resistance and durability using a 3D printer.\n4. Certain Metals, Including Gold and Silver\nWhile 3D printers recently added metal filaments such as aluminum, stainless steel, cobalt, titanium, and nickel alloys to their repertoire, you can’t 3D print with all metals. Instead, you will have to use metal clay or metal powders.\nMetal clay is just what it sounds like – clay with metal powders incorporated into it. Metal powders are usually pure metal fragments.\nPrinting With Metal Filament\nWhen you make a metal 3D print, a specialized printer will deposit a layer of hot metal powder or clay onto the print bed. Then, it will use a laser to heat and melt the metal.\nIn another printing process called metal binder jetting, the 3D printer deposits chemicals onto the metal powder or clay, which causes a chemical reaction that bonds and hardens the metal to itself.\nMetals That Won’t Print\nHowever, not all metals are compatible with 3D printing. According to metal manufacturing experts at Equisphere, when 3D printing metal objects, you need “a metal with high fluidity, high distribution density, low moisture absorption and high chemical stability for when the laser heats up.”\nBecause of these limitations, it is difficult and often impossible to print with materials such as gold, silver, tungsten, molybdenum, and several other common metals.\n3D printed metals also often have issues with structural integrity that make them unsuitable for use in heavy-duty machinery such as manufacturing equipment, automobiles, and airplanes. In addition, metal 3D printed parts usually have high porosity, making them prone to crumbling, cracking, and warping.\nHowever, you can likely expect metal 3D printing to get much more advanced in the coming years since it is a relatively new invention in itself, with the first metal 3D printer coming out in 1994.\n5. Large and Intricate Objects\n3D printers are small, and even the largest of them cannot tackle a project like an extensive and intricate car or a big piece of machinery.\nWhile they can print large or complex objects, they cannot usually do both.\nLarge Printers Need Larger Nozzles\nFor example, let’s look at the largest 3D printer in the world, a $2.5 million printer at the University of Maine. This printer is massive, with a 100-foot (30.48 m) print bed that is suited for making items such as boats and architectural components. But it is not ideal for creating a working machine.\nA larger printer comes with a larger nozzle, and it will accordingly use thicker filaments, making tiny details harder to print with accuracy.\nPrinting a finished product, complete with fine-tuned mechanisms, is impossible right now. With a printer this immense, you could create the casing for a machine, but it’s almost impossible to ensure finer details.\nInstead, you’d need to use a large printer for bigger pieces and several smaller printers for the more intricate parts.\nHowever, the investment in machinery usually isn’t worth it. That’s because the processing time will be longer, the cost of materials will be higher, and the parts won’t be as durable as molded or cast parts.\nStill, with this vast 3D printer comes promise for a more advanced future in 3D printing, and one day, likely soon, we will see printers that can produce seamless, fully-functional machines.\n6. Complicated Designs\nAlthough some complicated designs, from small, humanistic 3D figurines to scale models of ancient artifacts, are possible to 3D print, not every design is feasible.\nIntricate Spirals Need More Structure\nFor example, I recently read this article, which discusses a specific design for drone propellers. Unfortunately, they found that the design was far too complicated for even professional 3D printing services to attempt.\nThis small, round, spiral-shaped propeller, conceived after Leonardo Davinci’s propeller designs from the renaissance, wasn’t anything special at first glance. It had smooth surfaces and a coiling seashell-shaped design. However, the perimeter’s width-to-height ratio, the spiraling shape, and the lack of scaffolding were too much for a 3D printer to produce accurately.\nThis model is a perfect example of the limitations of 3D printing. 3D printers often have difficulty printing round objects since each layer of the print builds upon the previous layer. Therefore, the printer will likely struggle to make the layers meet up properly when printing a spiraling tower.\nPieces With Overhanging Areas Often Won’t Print Properly\nIn addition, the overhanging edges of the print, which had no support towers or bridges, created areas where the 3D printer could not connect each perimeter. Without any scaffolding, infill, or support, the 3D printer would have failed. If anyone tried to make it, the print would have looked like a shapeless lump on the bottom of the print bed.\nIn this case, the drone enthusiast had to make a mold and cast the propeller themself, but they were able to use a 3D printed prototype, complete with infill and scaffolding, to make the mold.\nSo, even if something looks simple, that doesn’t mean that it will be possible to 3D print.\nSometimes, even the most miniature, smoothest design is too intricate for a 3D printer, especially when it comes to overhanging parts, spiral designs, and hollow interiors.\n7. Completely Finished Products\nWhen you 3D print a design, nine times out of ten, you’ll need to finish it by hand. This might include:\n- Sanding off layer lines\n- Waterproofing the object\n- Clipping off support structures\n- Removing rafts depending on your design and the desired use for your product\nPost-processing is something that goes hand-in-hand with 3D printing, and very few printers can create seamless, smooth, airtight models without some manual labor.\nSo, when you want to make something once and be done with it without refining your product, 3D printing isn’t your best option.\n3D Printing Isn’t Always Consistent\nIn addition, though 3D printing is ideal for small parts and homemade objects, when it comes to making reliable, high-quality prints every time, 3D printing isn’t as efficient as other methods.\nSince each part that you 3D print needs some processing before using it, printing large machines, cars, phones, laptops, and other completed items with one design is impossible with a 3D printer.\nFor example, if you printed a small gear mechanism with two gears and a crank handle straight from the printer, you wouldn’t be able to sand off the gears. Without sanding, the layer lines could interfere with the smoothness of your mechanism, making it challenging to turn and use.\nFor now, we will have to be satisfied printing small parts, smoothing them out, then assembling them later.\n3D Printing Isn’t Always Precise Enough for Manufacturing\nThis limitation to 3D printing significantly impacts manufacturing, where efficiency is everything. However, when you are printing small parts at home, it likely won’t stop you from making your own designs.\nIn a factory setting, finishing a 3D-printed object takes care and time. Unfortunately, in industrial applications, that care and time mean more money, which makes the roughness and unreliability of 3D printed objects undesirable.\nSo, in a factory setting, most manufacturers still use tried and true methods such as injection molding to ensure that their products have a seamless, uniform surface without any need for sanding or refinement.']	['<urn:uuid:259d5043-513c-4435-9f4f-01563a20f0c1>', '<urn:uuid:4c4aafc1-7b0a-426f-b00d-9cd79159df1a>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T22:03:18.033296	9	99	3043
45	Can people from Boyton rent homes in Bawdsey through PVCLT?	Yes, a resident from one parish will be eligible for a property in another parish within the area that forms PVCLT.	['An introduction to Peninsula Villages Community Land Trust\nWhat is a CLT?\nA Community Land Trust (or CLT) is formed when people from a village, street, neighbourhood or town get together to create a not-for-profit organisation for the good of the community. Run completely by volunteers, a CLT strives to meet the long-term needs of its community by creating permanently affordable housing which remains owned by the community forever. A CLT is a ‘not-for-profit’ organisation, it acts as a long-term steward of housing, ensuring that it remains genuinely affordable, based on what people actually earn in their area, not just for now but for every future occupier.\nCurrently four villages are involved, supported by the Parish Councils of Bawdsey, Boyton, Hollesley and Shottisham, these villages have come together to form a CLT – Peninsula Villages Community Land Trust (PVCLT). By joining into one group it gives PVCLT a greater voice with local and national government.\nPVCLT has been set up as an effective means of resolving some of the housing needs of local people. The area has a high proportion of holiday lets and second homes, which has resulted in high rents for local people who wish to stay close to their community and families. PVCLT will work to identify sites for affordable homes, engage with landowners, housing associations, developers and local authorities, and oversee a project from inception to completion.\nCLTs have the flexibility to do many things not just provide homes. Other assets and amenities could include workspaces, recreational facilities, a shop, a café or pub or anything that the community is in need of. Ultimately CLTs allow communities to take more control of the future development of their area. In setting up PVCLT we have allowed this flexibility enabling the CLT to react to potential future needs.\nPeninsula Villages CLT main purpose is:\nTo develop affordable housing for rent or shared ownership to enable local people to live and work in our villages making our communities more sustainable.\nWho is involved in PVCLT?\nPVCLT is run by a Board of Trustees, who comprise a group of local people.\nHow does it work?\nCLTs form partnerships with landowners, developers and housing associations, the CLT is in place to ensure that genuine affordable housing is built as specified.\nHomes and any other community assets are secured by the CLT in perpetuity via a statutory ‘asset lock’. As such, CLT homes fall outside of the ‘Right to Buy’ scheme.\nPVCLT gives some control and influence over what is built and who it is for. The planning applications process remains unchanged, Parish Councils retain the legal responsibility to consider planning applications when presented. A CLT has no such power.\nPVCLT will influence the selection of prospective tenants. There is an allocation policy to ensure priority is given to people living in the area who are eligible for this housing. On PVCLT owned schemes, local connection will be considered first and applicants will be prioritised to reflect their housing need.\nIt is important to note that a resident from one parish will be eligible for a property which is in another parish within the area that forms PVCLT.\nFunding will depend on each project but could come from various bodies such as grants, national and local authorities, charities or commercial mortgages. Any surplus funds, should they happen, go back into new and future projects of PVCLT.']	['<urn:uuid:a499464d-4cb8-4472-90f6-cc73bb2abc6e>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T22:03:18.033296	10	21	564
46	hi im building a house what are pros cons of using straw bale walls for insulation and what other eco options exist	Straw bale walls offer excellent insulation with an R-Value of more than 10 (comparable to fiberglass), provide great sound insulation, are fire resistant when packed tightly, and are cost-effective. At the Lama Foundation, they're often used in hybrid approaches - particularly on north-facing walls to keep places warm. Alternative eco-friendly options include sheep's wool, which creates natural temperature buffering and moisture control, and recycled cotton/denim insulation, which offers superior soundproofing and is safe to handle, though it costs twice as much as fiberglass.	['Dancing on a wooden floor in an adobe dome built the 1960s was not really how I had envisaged my visit to the Lama Foundation . Yet this collective moment of fun somehow helped connect me to this community in a way I had not expected.\nOpen day dance inside the dome (built 1968) and the dome from the outside at The Lama Foundation\nClutching to the steep hillside of the Sangre de Cristo mountains north of Taos the Lama Foundation has been building since 1968. Principally a spiritual centre – following the teachings of Ram Dass and his infamous Be Here Now book which was compiled and published onsite – it has an eclectic mixture of eco-houses. These houses have changed quite radically over the years not least because on 5th May 1996 a fire destroyed the majority of the site, leaving rather miraculously the central dome and the old wooden octagonal kitchen. As much loved as the initial dwellings were there was a view that many had been in poor repair (much of the wood had rotted) and ill-equipped to cope with the extremes of the New Mexico climate (long winters, heavy snow and constant winds).\nThe old wooden kitchen and carved wooden door\nNow the range and diversity of the houses is inspirational. There is a log cabin, a straw-bale house, a new base for their cottage industry being made from straw-bale and adobe combined, some yurts for visitors, small vault homes, a hybrid house and many more. Over the years different visitors and residents have experimented with a variety of methods and styles – particularly hybrid approaches. This is when different methods such as straw-bale are used alongside adobe blocks or stone walls. Thus they make use of straw-bale in the north-facing walls (to keep the place warm) and adobe in a south-facing wall (because it allows a building to heat up more quickly than straw).\nHybrid eco-house at The Lama Foundation, and a inside wall – half adobe half straw\nUse of adobe on site was originally inspired by the nearby Pueblo constructions in Taos and many local Indigenous Americans came to the site in the early years to teach the newcomers how to build with earth. The adobe prayer room near the main done is particularly interesting – with a very small entrance (you have to crawl inside) and a sunken circle in which to sit it is completely peaceful. Many of the houses are also deliberately small – such as the vaults designed by Shay Salomon – which are thus cheaper to build and easy to heat. The vaults are straw-bale with aluminium shingle roofs which hang over to the ground each side. Moreover under New Mexico building code very small buildings do not have to comply. The community setting encourages the building of small individual houses and the collective use of the large communal space. There are communal bathrooms, kitchen, library, music room, winter meeting room, and outdoor sheltered eating area.\nThe adobe prayer room and communal bathroom\nThe small vault houses\nBuilding here is a collective process and part of a spiritual practice for many, one resident said they ‘build with clay, mud and love’. Another noted ‘building a house is so human and it has been taken away from us … it is so satisfying being able to build a house’. Some of the ‘special places’ like the stone hermitage have been built in silence and others such as the two vaults were built just by women. In fact Lama has hosted several women-only build workshops which were deemed necessary in order to create a safe space in which women could experiment in building and expand their confidence without the intrusion of men. Nevertheless the majority of building on site is still done by men.\nStraw bale house at The Lama Foundation and prayer flags\nThe way in which the Foundation has been set up limits residents to a maximum stay of seven years. Resident numbers are limited to the number of houses available on site (currently eight) and the site is most active in the summer months when a large number of volunteers (summer stewards) come and stay. However because the main focus of the site is spirituality rather than eco-building then the co-ordination of building or the skills available is very much dependent on who happens to be around. This has created maintenance problems for the core full-time residents who are there all year because of the sheer number of buildings and the use of natural materials which require regular attention and patching. This tension between a lack of time and skills and the focus being primarily elsewhere has led to the use of concrete and stucco in recent buildings which is not very environmental. They have had at times to make compromises. These compromises have been less about saving money and more about reducing labour requirements.\nOne resident noted that permaculture principles which were increasingly being integrated into community life suggest ‘you should start small and then work your way out, and so we should make sure we can cope with maintaining the buildings we currently have before we build more’. Thus the place seems to be in a constant flow of moving forward and correcting earlier mistakes. The main dome is quite cold (lacking insulation) and some damage to buildings is the result of a lack of sufficient roof overhang or ‘boot’ and the harsh weather. But more recent structures learn from these mistakes and a new straw-bale house has a foot stone boot to protect it.\nThe whole community is off-grid; generating electricity through photovoltaic cells, using compost toilets, wood for heat, and water from an on-site spring (and some rainwater is collected). Water is heated in the main through a propane heater because their solar capacity is limited.\nSolar power in the roof\nThere is undoubtedly something magical at the Lama Foundation, less in a spiritual sense for me personally, but something about its location and the freedom in which eco-building has been experimented in. I left longing to stay and to move into my own little vault house on the mountain side.\nThe Lama Foundation regularly hold open days and events. Please see their website for further information: http://www.lamafoundation.org/\nAlso see Cobb, A (2008) Early Lama Foundation, published by Lama Foundation and Salomon, S (2006) Little House on a Small Planet, published by The Lyons Press, Guilford, Connecticut, USA.\n[4th February 2011]', 'When choosing insulating materials for your home or building, the options are more numerous than they first appear.\nA visit to the hardware store will yield only a few options: foam board, fiberglass rolls, and perhaps spray foam. These materials, although widely available, contain many synthetic chemicals that you may not want to have in your home or around your family.\nIf you’re attempting to construct your building with all-natural materials, these conventional options may seem disheartening. But don’t lose hope! There are several eco-friendly, chemical-free alternatives for insulating your home that will be effective and safe for both you and your family.\nThe first option we’ll explore is sheep’s wool. Sheep live in some of the harshest and coldest climates in the world. They thrive there because they have a thick coat of wool that has natural properties to retain heat, even when damp. You’ll often find native sheep living high in the mountains, where it is extremely wet and cold, and yet they’re perfectly content.\nFor the same reasons that sheep’s wool does a great job protecting sheep from weather conditions, a roll of wool insulation will do wonders to insulate your home. When wool fibers are compressed down into a roll of insulation, the crimped nature of the fibers creates millions of tiny air pockets, which provides great insulation, keeping warmth in the winter and out in the summer. In addition, wool has a tremendous amount of breathability, as it absorbs and releases moisture in the air. When wool absorbs moisture, it actually generates heat, preventing condensation in cavities by keeping the temperature above the dew point. This property creates a natural buffering effect, using relative humidity to stabilize the building temperature.\nOld denim and cotton clothes also can be a great alternative to conventional insulation. Many cotton insulation rolls made today are constructed from recycled blue jeans and other textiles. Some companies even will let you donate your own old denim. Although cotton insulation made from recycled textiles is about twice as expensive as fiberglass insulation, it is incredibly safe to handle, has a longer useful life than fiberglass, and has superior soundproofing qualities. For some homeowners, the knowledge that they aren’t putting potentially harmful chemicals in the walls of their living space is worth the extra cost.\nIf you are constructing your home from scratch, using straw bales to construct the walls is a great insulation option. Straw bale homes typically have an R-Value (a measure of materials’ resistance to heat flow; the higher the number, the higher the insulation value) of more than 10. This is comparable to the insulation value of fiberglass. This, in addition to the huge cost savings compared to traditional insulation materials like fiberglass, makes straw bale walls an excellent option when building natural structures.\nStraw bale walls can be finished with a wide variety of materials to make them look and feel exactly like a normal home wall. Most people who visit your home will have no idea that the straw bales are there! Straw bales also provide great sound insulation for walls and are very fire resistant when packed tightly and covered with an appropriate skin.\nDon’t be fooled by the lack of variety when you visit the insulation aisle of your local hardware store. There are great alternatives – if you just know where to look.\nAryn Young lives in Homer, Alaska, running a small farm and sustainable land clearing operation.\nlink to the article content:']	['<urn:uuid:df2c7502-fb13-4b86-a736-0861ff189ea1>', '<urn:uuid:c9fd987b-a861-41bf-8282-1ad8cb01ed9a>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T22:03:18.033296	22	83	1658
47	home studio sound treatment ceiling walls floor	For optimal sound treatment in a home studio, you need to address multiple surfaces. The ceiling should have thick absorption - ideally at least 3 inches of fiberglass material to absorb frequencies down to 250Hz effectively. The walls should be treated with acoustic absorption panels at early reflection points, particularly behind monitors and on side walls nearest to them. For flooring, while carpet provides some absorption, it only works effectively down to about 1kHz - a better option is wood flooring with a thick area rug and underlay at reflection points. Additionally, bass traps should be placed in room corners to control low-end response.	['Building the Acoustic Frontiers Demo Room – Part III: Paint, Star Ceiling, Lights and Carpet\nIt’s been quite a while since the last blog update on the Acoustic Frontiers Demo Room (Part I, Part II). However I am happy to say that there has been a lot of progress, although the room is still not finished! The room is already ‘up and running’ in the sense that it can be used for music and movies but there are still some areas we are working on (acoustic treatment in particular) and a couple of key items that still need to be installed (blackout blinds, stretch fabric on screen wall, etc).\nToday I am going to give a update on the interior decoration and fit out of the room.\nNeutral wall and ceiling paint colors\nFor home theater use it is important that the walls and ceiling are painted a dark color in a non-reflective finish. A front projection screen works by reflecting light back towards the projector. Some of this light hits the side walls, ceiling and floor. Glossy paint finishes causes distracting ghosts and reflections to appear as the light is reflected from the wall back to the viewer. Flat or Matte colors reduce this significantly. Light colors also cause a reduction in perceived contrast. The best finish is flat black but that can be quite imposing when used everywhere. We recently calibrated a home theater in Dallas where the owner had mixed ‘sparkles’ into the flat black paint. He also had a heavy ‘orange peel’ type drywall finish. That combination actually worked very well and made the room a lot less foreboding. For our room however we wanted to make the colors welcoming yet fit for purpose. To this end we selected Benjamin Moore Andes Summit in Matte for the walls and Benjamin Moore Royal Wave (a midnight blue color) in Matte for the ceiling.\nA twinkling fiber optic star ceiling\nAnother design element in this room was the inclusion of a Numinus StarTile ceiling. The Numinus ceilings are some of the best in the industry and create a twinkling semi-random or real world starfield using multi-sized fiber optic strands. See their gallery for a ton of great install pictures. The star ceiling provides a very low amount of ambient light and can be left on when movies are playing. From the factory the StarTile’s substrate is a 1″ thick fiberglass panel wrapped in Guilford of Maine FR701 Black. As anyone with a passing interest in acoustics knows 1″ fiberglass is generally bad news, especially when applied over large areas such as in a star ceiling. 1″ fiberglass, for example, only has constant absorption down to 800Hz or so. A 1″ fiberglass panel only absorbs 25% of the incident energy at 250Hz compared to 100% at 800Hz. This means that the reflected sound from a 1″ absorber will be ‘spectrally distorted’ compared to the direct sound. Ideally any fiberglass based absorber used at primary reflection points should be at least 3″ thick. This is to maintain constant absorption down to the transition frequency at around 250Hz where what we hear becomes dominated by the room rather than the speakers. To illustrate this take a look at the absorption curves for three different thicknesses of fiberglass absorber. The data is for Primacoustic’s Broadway panels but applicable to any fiberglass absorber:\nSo to make the ceiling a full range absorber we used furring strips and 2″ of OC703 fiberglass to space the star ceiling away from the drywall.\nColor changing LED lights\nEvery room needs lights! In our demo room we decided on using a large number of 12V LED lights from NuLEDs. These LED lights are very shallow (<2″) which makes them ideal where sound isolation is a concern since you don’t have to penetrate your ‘isolation shell’ like you would need to with standard recessed lights. We used 4 high output white MiniCans over the seating area, 2 Mini-G gimbal spots and 9 RGB color changing SpectraCans. The other key advantages of NuLED’s 12V LEDs are the fact that they are controllable via RS232. This means that you can set individual RGB levels, turn the color loop on or off, dim the lights, create lighting scenes and more all from a universal remote control or your smartphone. A further advantage is that they do not require an electrician, since they are low voltage. The LEDs were mounted into the star ceiling by carefully cutting holes in it; a job that was made a little more difficult than expected due to the fiber optic strands everywhere!\nAnd a Red Carpet!\nFor home theater use it is important that the floor be non-reflective to prevent ghosts of the moving image being displayed on your floor. Carpet, however, isn’t the best material to use from an acoustical perspective as it doesn’t have consistent absorption. Most carpets only absorb down to about 1kHz with any consistency. A better material is wood, and for a two channel listening room the optimum is probably a wood floor with a very thick area rug with underlay placed at the floor reflection points for the left and right speakers. For a mixed use room you have to make compromises, so carpet it is! To improve the absorption profile of carpet you generally want to use the thickest carpet you can find with a thick fibrous rug pad underneath. We chose a 100% wool carpet from Nourison – the Ashton Velvet in red. It has a very nice underfoot feel to it due to the high pile and the subtle pattern makes it much more interesting to look at than a solid color.', 'Chances are, unless your home studio was specifically designed for recording and mixing music, it’s a less than ideal listening environment. Professional studios are carefully planned out to eliminate naturally occurring acoustic issues in a way that residential rooms are not. A well designed one will have no parallel surfaces and ample acoustic treatment.\nTo get the most out of your home studio, adding acoustic treatment can minimize many of the major problems that misrepresent what recordings and mixes actually sound like.\nProblems to Consider in Your Studio\nStanding waves are produced when a wall’s dimension is equal to, or a multiple of, the wavelength of a particular frequency. They occur when sound waves bounce between parallel surfaces, such as between opposite walls and between the floor and ceiling.\nStanding waves are a form of interference. As reflected waves collide with the original sound source, they can either combine or cancel each other out. This is referred to as constructive and destructive interference, respectively. The result of constructive interference is an unnaturally loud frequency, while the opposite is true of destructive interference. This typically occurs in the low end with frequencies around 300 Hz and below–those with longer wavelengths.\nSuch interference can create an inaccurate representation of the low end material in a mix. If your room has a constructive standing wave at, say, 100 Hz, you may compensate by carving out that particular frequency in the mix. When played in a different environment, your mix may be too thin.\nEarly reflections are those sounds first reaching our ears after bouncing off of a single surface in our listening environment. Psychoacoustically, our brain quickly determines the level and directionality of a sound source.\nCombining with the direct sound leaving our speakers, early reflections can alter our perception of where a sound is placed in the stereo field and how loud it is.\nJust as with standing waves, early reflections at higher frequencies can move through cycles of constructive and destructive interference with the direct sound. The result is a phenomenon known as comb filtering–a frequency response with sharp peaks and dips resembling a fine-tooth comb.\nAcoustic Treatment Options\nEven though home studios are often problematic listening environments, a little bit of acoustic treatment from DIY acoustic panels can make a drastic difference.\nWhile it’s nearly impossible to treat every acoustic issue without building a pro studio from the ground up, working on standing waves and early reflections is a fantastic start. Most importantly, it can be done on a budget, as well. The following are the three most common types of acoustic treatment used to combat the issues mentioned above.\nAcoustic absorption is used to tame the mids and highs in a given environment. You’ll want to use absorption to treat the early reflection points in your room: behind your monitors; on each side wall nearest the monitors; and on the ceiling above your monitors.\nFinding the early reflection points in a room can be as simple as running a mirror along the wall and placing an absorptive panel where a monitor is reflected in the mirror. Again, these points will be somewhere on the back wall behind your monitors, on each side wall nearest the listening position, and on the ceiling above your listening position.\nMany companies produce absorption commercially, but building them yourself is a much more cost-effective alternative if you have the tools and time.\n1″ x 4″ lumber and R13 denim insulation is easy to work with and leaves you with effective 4″-thick absorption! If your handy, check out our guide to building your own acoustic panels.\nPlaced in the corners of rooms, bass traps are used to control the low end response of a space. In small rooms especially, unwanted buildup of low end material can be a huge problem.\nIdeally, floor-to-ceiling bass trapping straddling each corner of the studio at a 45-degree angle is ideal. If you are looking for more guidance on building these out on a budget check out our guide to DIY bass traps for your home studio.\nIf you don’t have the space or resources to fill each corner entirely, don’t hesitate to straddle each corner with a panel like the ones mentioned previously in a size that suits your environment.\nWhile bass traps and absorption are used to absorb frequencies, acoustic diffusion is designed to evenly scatter frequencies throughout a space.\nIn most cases, we don’t want an entirely dead mixing environment. Diffusion, placed on the back wall of a home studio, takes sound waves and scatters them throughout the space.\nWhile we haven’t lost any acoustic energy as we have with absorption, the scattered sound makes it much harder for our ours to determine where reflections are coming from, and how loud they are.\nDiffusion essentially de-concentrates sound waves in the listening environment. Diffusers are commonly made of a hard, reflective material like wood, and may be of the “skyline,” “triangular,” or “spherical” variety.\nFor further information on the basics of acoustic treatment, please enjoy this interview with renowned educator and engineer, Bobby Owsinski!']	['<urn:uuid:161d802c-e985-4887-957f-91271b84ba2e>', '<urn:uuid:849b2582-0ac3-4934-adca-8269290b6b9d>']	open-ended	with-premise	short-search-query	similar-to-document	three-doc	novice	2025-05-12T22:03:18.033296	7	104	1791
48	what is stitch density knitted fabric	Stitch density is the total number of loops in a measured area of fabric, specifically the number of needle loops in a square inch. It can be calculated as Wales density multiplied by Courses density (or WPI x CPI).	['Knitting terms and definition\nFabric forming process\nWeaving is the interlacement (Fig.a) of two sets of threads (i.e. warp and weft) at a right angle.\nInterlooping (Fig.c) consists of forming yarn(s) into loops, each of which is only released after a succeeding loop has been formed and intermeshed with it.\nIncludes a number of techniques, such as braiding and knotting, where threads are caused to intertwine (Fig.c) with each other at right angles or some other angle.\nKnitting is a technique to turn thread or yarn into a piece of cloth. Knitted fabric consists of horizontal parallel courses of yarn which is different from woven cloth. The courses of threads or yarn are joined to each other by interlocking loops in which a short loop of one course of yarn or thread is wrapped over the another course.\nKnitting is a process of fabric forming by the intermeshing the loops of yarns. When one loop is drawn through another, loops are formed in horizontal or vertical direction.\nTypes of Knitting\nThere are two of knitting-\n- Weft knitting\n- Warp Knitting\nWeft Knitting is a method of forming a fabric in which the loops are made in horizontal way from a single yarn and intermeshing of loops take place in a circular or flat form on across wise basis.\nWarp Knitting is a method of forming a fabric in which the loops are made in vertical way along the length of the fabric from each warp yarns and intermeshing of loops take place in a flat form of length wise basis.\nThe series of loops those are connected horizontally, continuously are called as course.\nThe horizontal row of loops that are made by adjacent needles in the same knitting cycle.\nThe series of loops that intermeshes vertically are known as Wales.\nVertical column of loops that are made from same needle in successive knitting cycle\nFeatures of wales\nWhen loop transfer occurs it is possible to transfer a wale of loops from one needle A to another B.\nIn warp knitting a wale can be produced from the same yarn if the same warp guide laps the same needle at successive knitting cycles.\nWales are connected together across the width of the fabric by sinker loops (weft knitting) or underlaps (warp knitting).\nWales show most clearly on the technical face and courses on the technical back of single needle bed fabric.\nParts of a loop\nEach knit stitch (knit loop) is a basic unit for the knitted goods\nH: Head or Crown or top arc\nL: Side limbs or Legs\nS: Bottom arc or Sinker loop\n→A complete loop=Needle loop+Sinker loop\nNeedle loop is the upper part of the loop produced by the needle drawing the yarn.\nThe lower part of the knitted loop is technically referred as sinker loop. It is the connection of two legs belonging to neighboring stitches lying laterally.\nThe open loop is one in which the loop forming yarns do not cross at the bottom of the loop.\nIn closed loop the legs cross at the bottom, so that the loop closing takes place.\nFace loop and technical face\nDuring loop formation, when the new loop emerges through the old loop from back to the face side then it is termed as face loop or weft knitted loop.\nThe side of the fabric which contains all face loops or weft knit loops is known as technical face.\nBack loop and Technical Back\nDuring loop formation, when the new loop passes from the face side to the back of the previous loop then it is termed as back loop or purl loop.\nThe side of the fabric which contains all back loops or purl loops is known as technical back.\nThe knitted stitch is the basic unit of intermeshing. It usually consists of three or more intermeshed needle loops. The centre loop has been drawn through the head of the lower previously-formed loop and is, in turn, intermeshed through its head by the loop above it.\nKnitting notation is a simple, easily-understood, symbolic representation of a knitting repeat sequence.\nIts resultant fabric structure that eliminates the need for time-consuming and possibly confusing sketches and written descriptions.\nNotations are used to express the knitted fabric structure and design.\nNotations are of four types:\n2.Line diagram/ Looping diagram\n3.Diagrammatic notation/ Chain notation\nThe length of yarn required to produce a complete knitted loop (i.e Needle loop and sinker loop) is known as stitch length or loop length.\nStitch length is a length of yarn which includes the needle loop and half the sinker loop on either side of it.\nGenerally, the larger the stitch length, the more extensible and lighter the fabric and the poorer the cover and bursting strength.\nA course length\nIn weft knitted fabrics, a course of loops is composed of a single length of yarn termed a course length.\nDefinition: The length of yarn required to produce a complete knitted course is known as course length.\nCourse length= No. of loops per course X Stitch length\nCourse length= No. of needles X Stitch length\nStitch density refers to the total number of loops in a measured area of fabric.\nIt is the total number of needle loops in a given area (such as a square inch\nStitch density= Wales density x Courses density\n=Wales per inch X Courses per inch\n= WPI X CPI\nUnit: number of loops per square inch\nOr, SD= CPC x WPC (cm scale)\nA knitted fabric is technically upright when its courses run horizontally and its wales run vertically, with the heads of the needle loops facing towards the top of the fabric and the course knitted first situated at the bottom of the fabric.\nSingle jersey & Double jersey\nSingle jersey or plain fabric: The weft knitted fabric (Tubular/flat) which is produced by one set of needles.\nDouble jersey: The weft knitted fabric (Tubular/flat) which is produced by two sets of needles.\nThe fabric loops are always drawn from the needles on the side remote from their hooks. When two sets of needles are employed, the fabric will be produced and drawn away in the gap between the two sets.\nNeedle bed or needle carrier\nNeedle bed or needle carrier is the place where the needles are located or mounted in a knitting machine. Needle moves up and down in the trick of a needle carrier.\nTwo types of needle carrier:\n- Cylindrical or circular\nCylinder and Dial\nCylinder is a circular steel bed having grooves / tricks /cuts on it’s outer periphery into which the needles are mounted.\nWith reference to the tricks, the needles move vertically up and down by their butt being in contact with the cam track.\nThe diameter of the cylinder also varied based on the type and width of the fabric.\nMaximum diameter of the cylinder: 46 inches.\nDial is the upper steel bed used in double knit circular machines.\nInto the grooves of the dial, the needles are mounted horizontally and are allowed to move radically in and out by their dial cams.\nThe number of grooves per unit space conforms with the cylinder gauge in most of the cases.\nMachine gauge or Needle gauge\nThe number of needles present in one inch of a needle bed is called machine gauge. It is denoted by G or E.\nThe distance between two neighboring needles\nSpace required for each needle.\nFeeders are the yarn guides placed close to the needles to the full circumference of the knitting zone.\nThe feeders feed the yarn into the needle hooks and control the needle latches in their open position while the needle attain their clearing position.\nA selvedge fabric is one having a “self edge” to it and can only be produced on machines whose yarn reciprocates backwards and forwards across the needle bed so that a selvedge is formed as the yarn rises up to the next course at the edge of the fabric.\nCut edge fabric usually produced by slitting open a tube of fabric produced on a circular machine. A slit tube of fabric from a 30” diameter machine will have an open width of 94”(πd) at knitting & before relaxation.\nThis may be produced in double faced or single faced structures on circular machine, or in a single faced from on a straight machines with two sets of needles provided each needle set only knits at alternate cycles and that the yarn only passed across from one needle bed to other at the two selvedge needles at each end, thus closing the edges of the tube by joining the two single faced fabrics produced on each needle set together.\nMain features of a knitting machine\n- The frame, normally free standing and either circular or rectilinear according to needle bed shape, provides the support for the majority of the machine’s mechanisms.\n- The machine control and drive system co-ordinates the power for the drive of the devices and mechanisms.\n- The yarn supply consists of the yarn package or beam accommodation, tensioning devices, yarn feed control and yarn feed carriers or guides.\n- The knitting system includes the knitting elements, their housing, drive and control, as well as associated pattern selection and garment-length control device.\n- The fabric take-away mechanism includes fabric tensioning, wind-up and accommodation devices.\n- The quality control system includes stop motions, fault detectors, automatic oilers and lint removal systems.\nHow to write a machine specification?\nWeft knitting machine\n→Single jersey circular knitting machine.\n→Rib circular knitting machine.\n→Interlock circular knitting machine\n→Socks circular knitting machine.\n→Double cylinder purl knitting machine\n→Flat knitting machine\n→ V-bed flat knitting machine\n→ Straight bar frame\nPowered by Facebook Comments']	['<urn:uuid:5251662e-079d-47bf-bd23-bd6935466349>']	factoid	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T22:03:18.033296	6	39	1619
49	As an ornithologist doing field research in Maine, I was surprised to hear what sounded like a cactus wren. Could this be possible or am I mistaken in my identification?	You are likely mistaken in your identification, as cactus wrens exclusively live in the Southwest near cacti. This is a common error that appears even in popular media - for example, The Shawshank Redemption incorrectly featured cactus wren sounds in a Maine setting. When identifying birds, it's crucial to consider their geographic range and habitat requirements.	"[""This article is part of the Science in Sci-fi, Fact in Fantasy blog series. Each week, we tackle one of the scientific or technological concepts pervasive in sci-fi (space travel, genetic engineering, artificial intelligence, etc.) with input from an expert. Please join the mailing list to be notified every time new content is posted.\nThe Expert: Rebecca Mowry\nToday’s expert is wildlife biologist Rebecca Mowry. Over the past 10 years, she’s spent countless hours in the field with wolves, woodpeckers, mountain lions, river otters, and a big salamander called a hellbender. Here’s how I know she’s the real deal: yesterday at work her pickup got stuck somewhere and had to be towed out; she came home covered in mud. We’ve had fascinating conversations about deer and predators and the evolution of migration patterns. You should follow her on Twitter.\nToday, she offers some tips on getting wildlife biology right in your fiction.\nWildlife Misconceptions in Fiction\nAs a wildlife biologist, I encounter a lot of misconceptions about wildlife in books or film/TV that irritate me to no end. In many cases, even a little bit of research could clear these things up. I can think of a dozen species-specific misconceptions off the top of my head (mostly behavioral in nature), but I’m going to pick a few general examples.\n1. I can put a cactus wren in Maine, no problem.\nThis remark stems from one of my favorite films, The Shawshank Redemption. You know that scene where Red finds Andy’s money under a rock wall in Maine? Yeah, that bird singing in the background is a cactus wren, and cactus wrens live in the Southwest. Near cacti.\nAnother classic example is the Coca-Cola advertising campaign featuring polar bears interacting with penguins. Sure, it’s cute, and Coke can get away with it, but I’m fairly certain it drove every zoologist bonkers.\nWhat’s the problem? I like a little anthropomorphism as much as the next person, and I can shrug off the image of one the most aggressive carnivores on the planet sitting next to a family of penguins as the lies of advertising. After all, nobody really thinks that happens (at least, I hope not). What I can’t stand is that Coke ignored the fact that polar bears live only in the Arctic, and penguins live only in the Antarctic.\nI am forever irritated by simple mistakes like this in film and TV. But it applies to novels too. For the love of all that is furry and feathered, look up a species’ habitat/geographic range before you put it in your story. Keep desert animals in the desert, forest animals in the forest, and so on.\nThis happens so often, especially with birds. The problem is that people love animals – which is, in my opinion, a great thing – but it makes it more likely that your readers will know more than you expect them to. Birdwatching is becoming more and more popular with the public, increasing the odds that your lack of research will be noticed and scorned.\nOh, and to add to the confusion, lots of animals migrate. So don’t try to tell me about the turkey vultures circling your lost MC in Montana in the winter, because they’re only there in the summertime.\n2. The geographic range and habitats of my species never changed\nIf you’re writing any kind of historical fiction, you should be aware that the range and habitats and even appearance of a species can change over time. For example:\n- Prior to the red/gray wolf’s extirpation from the eastern U.S., there were no coyotes there.\n- Until a few hundred years ago, jaguars lived throughout most of Arizona, New Mexico, Texas, and even Louisiana.\n- Horses were brought to North America in the 1400s, but they actually evolved here, before being wiped out by the ice age 10,000-12,000 years ago.\n- Recent climate change is causing shifts in migratory patterns of a wide variety of birds, as well as changes in plant communities\nJust like any other facet of historical fiction, it’s worth it to research what the wildlife community would have looked like back when your story takes place. In North America, most habitats have changed drastically since the advent of Europeans (and even since the advent of the first humans, period). You can probably get away with a lot of stuff, but you want your novel to be authentic, don’t you?\n3. All wildlife biologists are park rangers, game wardens, or TV show hosts.\nMy friends and I all got tired of the “Wildlife biology? What are you going to do with that, be a park ranger?” comment in college. In fact, I was just watching an episode of the West Wing where C.J. Cregg gets a visit from a park ranger. This park ranger proceeds to tell Cregg’s assistant that he studied shrub/range ecosystems, and that it was a good thing the Park Service hired him, because he wouldn’t have had anything else to do. Clearly the writers of the show fell victim to the myth that “Park Ranger” is our only career path.\nI’ll admit that I felt this way for my entire childhood. I grew up on Steve Irwin (may he rest in peace) and Jack Hanna, and I didn’t even know you could study wildlife biology in college until I was 17. I thought that if you loved animals, you had to be a veterinarian.\nNow, of course, I know that the field is incredibly diverse. Wildlife biologists do lead tours in natural areas, arrest poachers, and educate the public on TV. But they also trek into remote jungles to document rare and unknown species. They survey deer and turkeys to set yearly harvest quotas for hunters. Thousands of university professors research wildlife behavior, evolution, habitat, and threats to conservation. Each state has an agency (like Texas Parks and Wildlife) dedicated to wildlife research and management, and there are several federal entities that do this as well (the US Fish and Wildlife Service, the US Forest Service, the US Geological Survey). Then there are non-profit organizations that do their own work toward species and habitat conservation, like the Sierra Club, the Nature Conservancy, and the World Wildlife Fund. They all employ wildlife biologists.\nAnd we’re also not all tree-hugging, granola-eating hippies. Many biologists love game animals and hate predators. Many biologists love predators and hate hunters. Many biologists love everything. And some biologists are tree-hugging, granola-eating hippies.\n4. Forensics is only for people.\nThe US Fish and Wildlife Service Forensics Laboratory in Corvallis, Oregon begs to differ.\nIllegal harvesting and animal trafficking (for medicinal uses and the pet trade) are just two common wildlife-related crimes. To combat these issues, scientists have developed a variety of tools – as well as employing traditional forensics – to uncover evidence and catch the culprits. Almost any forensic analysis that can be done on human samples can be done on wildlife samples, but I’m more familiar with the molecular side.\nMolecular ecology and conservation genetics are growing fields in wildlife biology, and the sky’s the limit when it comes to the power of DNA in unraveling ecological mysteries – and solving wildlife-related crimes. I actually got my master’s degree on a genetics project involving “counting” river otter populations in Missouri from scat samples. Yes, scat. There’s DNA in there, did you know?\nWith these and other methods, biologists can use DNA from elephant ivory to determine where the elephant came from (a legal harvest zone versus a protected habitat). They can examine the DNA of fish meat at the market to make sure protected species are not being harvested and sold. Incorporating these types of tools could make for an interesting crime novel, I think, or provide an unexpected source of evidence.\n5. We know just about all there is to know about wildlife and ecosystems.\nWrong, wrong, wrong! We’re still learning. Constantly.\nScientists are continuing to unravel the evolutionary history of wildlife species. Advances in genetic analysis have a lot to do with this. That’s why taxonomists (the people who classify animals into taxonomic groups) are always changing the scientific names of stuff, much to the dismay of people like me who had to memorize them in college.\nIn addition, the effects of contemporary environmental challenges make up a large proportion of current wildlife research. A lot of what is happening in our world today is unprecedented, and in most cases, we can only speculate how these changes will affect wildlife species (and potentially humans!).\nBirds are changing the timing of reproduction and migration due to climate change. Grizzly bears and polar bears may be hybridizing because of range constriction (again due to climate change). Overfishing may be causing trophic cascades running all the way down the food chain; e.g. seals and sea lions decline due to lack of food, orca then run out of seals and switch to sea otters, and sea otter declines result in an overpopulation of sea urchins, which damage kelp forests, which are an important habitat for a number of diverse marine organisms. Other challenges include habitat degradation affecting animal behavior and food habits, and wildfire suppression preventing naturally occurring fire cycles to which plants and wildlife had evolved.\nWe’re still learning just how important these issues are, and whether we need to intervene to prevent species extinction, catastrophic wildfires, and other negative effects.\n6. There will be no animals in the future.\nThis applies especially if you want to write futuristic science fiction. While I can’t say with certainty that this statement isn’t true (I can’t see into the future), I find it highly unlikely.\nCertain animals and plants can survive in any number of tough situations; that’s part of the beauty of mutation and evolution. I’m simplifying this a lot, but consider the Cretaceous-Paleogene mass extinction 66 million years ago that killed off the then-predominant terrestrial vertebrates, the dinosaurs. Many small mammals were able to survive the extreme environmental conditions that the very large reptiles couldn’t. That ushered in the explosion of mammalian diversity which allowed mammals to evolve into many of their current forms (including humans).\nHumans may be one of the most devastating forces of “nature” when it comes to species extinction, but don’t forget that there are many species that thrive in human-dominated landscapes, like pigeons, feral dogs, rats, and cockroaches. There are even some that aren’t quite as “nasty”, like white-winged doves, peregrine falcons, red foxes, raccoons, and coyotes; in fact, I worked on a project in urban Orange County, California where we observed bobcats and coyotes making their home in culverts and office-park drainage systems.\nAnimals adapt, and evolution is still occurring. We may not be able to predict exactly what this will mean for the future, but it means you, as a science fiction author, have a lot of artistic license here.\nHow to handle wildlife biology in your novels\nHere are some tips for handling these topics well in your fiction.\n1. Do your research.\nIf you want to make your story as authentic as possible, make sure you’re describing the appropriate ecosystems and animal communities. There are tons of resources on the internet for this: ebird.com for birds, iNaturalist.org for everything, and even Wikipedia gets things right most of the time.\n2. Have fun with it!\nBecause we’re always learning, there’s a lot of wiggle room for using wildlife in fiction. For example, G.R.R. Martin decided to use ravens as messengers in his fictional world, and while I’ve never heard of them being used as such, Corvids (the taxonomic group of which ravens are a member, and includes crows, jays, and magpies) are known to be highly intelligent. Recently, scientists have observed New Caledonian crows not only using tools, but using sequences of tools and showing ability to reason. Other crow species have been known to use passing cars as tools to crack nuts. In Washington, scientists (using Dick Cheney masks) demonstrated that crows can recognize human faces.\nHere’s a fun TedTalks video about crow intelligence.\nThere’s a lot of license when it comes to using animals in fiction, and while I’m not saying you need to be 100% ecologically accurate, I’d at least like your details to be believable. In The Hunger Games, Suzanne Collins bred genetically-altered jays with wild mockingbirds to create the mockingjay. Jays and mockingbirds are classified into different taxonomic families (which makes hybridization less likely), but I still find this to be an excellent incorporation of wildlife behavior into a futuristic story. Use your research on species’ habitat needs, evolution, and behavior to come up with a mind-blowing prediction of futuristic animal communities.\n3. Contact a wildlife biologist if you have any questions.\nSeriously. There are so many of us. Find a biologist at your state agency, a federal biologist, or a non-profit. We’ll probably be very happy to help you.\nThe Discovery Channel said it best: The world is just awesome.\nAbout the Author\nRebecca Mowry works as a biologist with the Texas Parks and Wildlife Department, where her world revolves primarily around deer, but she still gets to do other fun stuff. About a year ago she began writing The Front Range, a story which hints at an ancient connection between humans and the world we live in, and was inspired by her experiences.\nPlease Share this Article\nHere are three ready-made tweets for you to share this article on Twitter. Thanks fo reading!\n|Click to Tweet Wildlife misconceptions in fiction with biologist @RebeccaMowry. http://bit.ly/1pPqvVA Part of the #ScienceInSF series by @DanKoboldt|\n|Click to Tweet Wildlife problems in fiction by @RebeccaMowry: #1: Putting a cactus wren in Maine. http://bit.ly/1pPqvVA #ScienceInSF by @DanKoboldt|\n|Click to Tweet Common wildlife biology mistakes in fiction, with expert @RebeccaMowry. http://bit.ly/1pPqvVA #ScienceInSF series by @DanKoboldt|\nFollow me and you'll never miss a post:""]"	['<urn:uuid:4950dc00-3453-4c45-b3dc-11cfe9add0fe>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T22:03:18.033296	30	56	2309
50	main purpose data loss prevention software	A Data Loss Prevention (DLP) suite consists of a network scanner and tools that allow IT departments to collect information on what data is being used and by whom, helping protect against internal data loss threats.	['When you think about someone trying to make off with your company’s private data, what comes to mind? Some wily Russian hacker who sneaks into your company’s network through the backdoor? Perhaps you need to update your thinking. A recent report from Cisco revealed that the real threat is coming from insiders. What’s a CIO to do?\nIdentifying The Threat\nBy now all CIOs realize that their corporate networks and data are under almost constant assault. However, most of the steps that CIOs have taken to secure their networks have been designed to defend themselves against the attacker who comes from the outside.\nInformation that was revealed in the Cisco report included that workers are sharing corporate information with outsiders for a variety of reasons. These include sharing data simply in order to get an outsider’s opinion on something, to show off work that they’ve done to others, etc.\nOn top of the active taking of corporate data, Cisco’s report revealed that some 66% of those who responded admitted to engaging in activities that would allow someone else to access corporate data (things like not logging off and then leaving their computers on at work overnight!)\nData Loss Prevention\nIf a CIO ever wants to get to sleep again, something has to be done to solve the data loss threat that insiders pose to the firm. There is no magic bullet, but one approach to dealing with this problem is to deploy a data loss prevention (DLP) suite of tools.\nIn true “big brother” fashion, a DLP suite generally consists of a network scanner coupled with multiple tools that allow an IT department to collect information on what data is being used and by whom.\nBefore moving forward with implementing a DLP solution, CIOs need to take the time to prepare to use this new set of tools. The steps involved include:\n- Secure The Important Stuff: before you go worrying about trying to secure how data is used throughout the enterprise, first identify the most important data and ensure that it is locked down.\n- Close Your (Network) Doors: before you can worry about insiders doing you harm, you need to make sure that outsiders can’t get in. This requires analyzing both your network ports and the protocols that the company’s network is using to make sure that they are secure.\n- Create A Baseline: in order to detect when the wrong things are being done, you need some way to detect them. Creating baselines such as point-in-time content signatures for sensitive data stores is a first step in doing this.\n- Start Inspecting Traffic: the way that you can prevent information from going to internal sources that don’t have a need to know is by installing automated network traffic inspectors. Setting parameters so that notifications of data breeches are flagged will do a great deal to prevent data loss by internal threats.\nThe value that a CIO brings to a firm is that he / she is able to harness IT resources in order to help the company succeed. As part of this task, the CIO is also responsible to make sure that sensitive corporate data remains secure from both external and internal threats.\nCIOs that learn how to deploy DLP solutions in order to protect against the data loss threat from insiders will be better at finding ways to apply IT to enable the rest of the company to grow quicker, move faster, and do more.\nWhat We’ll Be Talking About Next Time\nSince most firms have no idea about what to do with their corporate research facilities, responsibility for the labs often falls under the control of the CIO (because most firms don’t know what to do with IT either). Great. So what’s a CIO to do when he/she is responsible for a corporate R&D lab?']	['<urn:uuid:16253f5e-93f1-4656-9ae6-285a486ab29b>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T22:03:18.033296	6	36	640
51	mortgage process timeline penalties fees	The mortgage process typically takes 30-45 days from application to closing, with several key milestones: loan opening within 5 days of application, appraisal taking 7-14 days, underwriting review and loan commitment 10-14 days before closing, and final closing activities taking 3-5 days. Regarding fees and penalties, particularly for closed mortgages, lenders charge prepayment penalties if borrowers want to pay off early. These penalties are typically calculated as either three months of interest on the remaining amount or the interest rate differential (IRD), whichever is higher. For example, on a $200,000 mortgage at 6% interest, the three-month penalty would be $3,000, while the IRD could be higher depending on current market rates.	['Once a mortgage application has been completed, the mortgage loan is prepared for opening. Once again, this is a less than a heroic effort on our part as Lender’s to create ambiguity around the mortgage process. Many borrowers justifiably confuse the term “loan opening” with what is technically known as the “loan closing”: when the borrower becomes both a homeowner and mortgage payment owner! In truth, the loan opening is the formal term given to the moment at which the loan terms are disclosed to the borrower. In most cases, loan opening occurs within five days after the mortgage application has been completed.\nThe “Initial Loan Disclosure” (aka “Loan Estimate”), as it is formally defined, provides the borrower with key information regarding the loan amount, the payback period (30 years, etc.), the interest rate and other important elements of the mortgage. Most of what is included with the initial loan disclosure package is information pertaining to the consumer’s (or borrower’s) rights. The loan is “opened” with the dual purpose of informing the borrower they are entering into a promise to pay back a loan and begin the formal loan processing activities. At this point in typical mortgage loan proceedings, the borrower can transition focus to fulfilling inspection obligations and other aspects of the purchase transaction. The borrower should anticipate 1 week between application and the loan opening milestone to be completed.\nAs I mentioned earlier, the home buyer’s involvement with most of the early loan processing activities may be limited in scope. In most cases, your competent and forward-looking loan officer has already procured from you the documentation required to facilitate the loan processor’s role in making sure your application is “complete”. There may be some additional requests for supporting documents from the borrower, such as a recent pay statement or bank statement. But, the majority of the “up-front” documents have likely been satisfied and submitted to your loan officer’s processing team without much ado. What happens next is, without question, exactly what Tom Petty was referring to when he penned The Waiting (“is the hardest part”): the wait for the coveted appraisal inspection and report! Undeniably, one of the more stress-prone periods of the mortgage lending process for buyers, the appraisal inspection usually takes 7 to 14 days complete, depending on the property features. In most (legal) cases, your mortgage loan officer is likely in connect with the appraiser or appraisal management company (AMC) through his underwriter. At the time an appraisal is ordered, a due date is assigned for the appraiser to agree to before accepting the request for the appraisal. In most cases, that is 7 to 10 days from the date the appraiser accepts the appraisal. At the point the appraisal has been provided to the lending team, you can expect to be less than 24 days into the loan process.\nRecapping, to this point, we have completed the application, opened the loan, and ordered the appraisal. What follows is the “back half” of the mortgage lending process, and hopefully the most productive! Technically, the underwriting review begins well before the appraisal report has been provided, but we are addressing it as a subsequent “step” in our six-step process. The underwriting review is where the “rubber meets the road” on the mortgage lending journey. The underwriter is the ultimate approver (or denier <gasp!>) of the loan application for the homebuyer. During underwriting, the homebuyer may be asked to provide substantiating evidence supporting their loan eligibility. These requests will typically come from your loan officer or processor and are usually document related in nature. In most cases, the borrower is transparent to the intensity of the underwriting process on the lender’s side, as most of the work involves verifying information pertinent to submitting a “complete” loan package to the investor who will fund the purchase.\nUltimately, the goal of the underwriting process is to issue the “loan commitment”. The loan commitment provides a high level of assurance to the homebuyer, the buyer’s real estate agent and, perhaps most importantly, the seller’s real estate agent that the borrower is considered a “good” applicant to whom the lender is willing to provide the funds for the purchase of their home. This is often referred to as “closing” the loan. The loan commitment will include a statement of the borrower’s creditworthiness, as well as a comprehensive list of the outstanding “conditions” that will need to be satisfied before the loan can close. These conditions will include evidence of an acceptable appraisal report; verification of sufficient funds in the borrower’s account to pay for closing costs and down payment; and other items that the underwriter deems necessary in order to provide the investor with a complete and quality loan file. The loan commitment is typically due 10 to 14 days before closing. Your lender’s processing and underwriting team will ensure they are working towards meeting your loan commitment date.\nWith the underwriter’s job complete, your lender’s closing department takes the baton in driving your loan package to the closing table, on time and ready for signing. In some cases, there may be additional requests for documents prior to the “clear to close” being issued. But, rest assured, once the closing team is managing your loan, you are in the home stretch! Pre-closing activities are typically concluded in 3 to 5 days and mostly occur between the closing team and the title and abstract company who will be facilitating your closing. You can expect approximately 1 week between the time your clear to close is issued and your closing date.\nSo, looking at our process “end-to-end”, it should be evident there are a significant number of factors that contribute to how long it takes for your mortgage application to be completed. As we have also learned, the mortgage application is more so a journey than it is an event. Depending on the complexity of your own purchase scenario, it is safe to expect a 30 to the 45-day application cycle. However, as with most things in life, there are scenarios where that cycle may be substantially shortened and possibly even lengthened. Regardless, your knowledge of the process and the timing between major milestones will help prepare you to close on your schedule!', 'Table of Contents\n- What Is A Closed Mortgage?\n- Costs Involved In Paying Off A Closed Mortgage Early\n- Pros And Cons Of Breaking A Closed Mortgage Contract\n- How To Reduce Or Avoid Prepayment Penalties?\nPaying off an open mortgage early is certainly possible. But what about a closed mortgage contract? Is it possible to exit from a closed mortgage contract early? The answer is YES!\nBut paying it off early is a little bit trickier than you might have hoped. Be it an open or closed mortgage, paying early in both cases tags along with some costs.\nIn the case of a closed mortgage, the prepayment penalty, breakage costs, and other charges could be higher–sometimes to the extent that paying off early becomes a costly affair. Therefore, a thorough calculation before deciding to end a closed mortgage contract is a must! Visit this site to know the differences between an open and closed mortgage.\nWhat Is A Closed Mortgage?\nA closed mortgage, also known as a closed-end mortgage, is a limiting mortgage that prohibits borrowers from paying off, renegotiating, or refinancing the mortgage early. Specific prepayment penalties, breakage costs, and other charges are involved in renegotiating or paying off the mortgage early.\nA closed mortgage is preferred over an open-end mortgage due to lower interest rates. But it limits the borrower’s ability to use the house’s equity as collateral to refinance before its term ends.\nGenerally, you should opt for a closed mortgage if:\n- You have a long-term vision\n- You’re not going to move or sell early\n- You’re searching for attractive interest rates\nPros Of A Closed Mortgage\n- It offers lower interest rates to homebuyers.\n- Borrowers have the assurance that the interest rates will not vary till the end of the term.\n- It offers substantial time to repay the amount.\n- It adds a sense of security to people with fixed, regular incomes.\nCons Of A Closed Mortgage\n- It limits homebuyers to pay off, negotiate, and refinance the mortgage before the term ends.\n- Repayment penalties, breakage costs, and other charges make it too costly.\n- It is challenging to refinance a closed mortgage.\n- It is a less flexible model.\nReasons To Break A Closed Mortgage Contract\nSeveral factors influence a homebuyer to pay off a closed mortgage early. The most popular driving factors are:\n- A homebuyer wants to sell the home before the mortgage term ends.\n- Market stability and growth have cut down the interest rates.\n- A borrower’s income has increased, so they want to pay off the mortgage early.\n- Refinancing to find a better rate.\n- Fulfilling the purpose of buying the home.\n- Moving out of the property for some reason.\nCosts Involved In Paying Off A Closed Mortgage Early\nDeviating from the closed mortgage contract could impose a prepayment penalty, charge, or breakage cost. The lender will charge a prepayment penalty if a borrower:\n- Wants to pay more than what is in agreement.\n- Opts to break a closed mortgage contract.\n- Transfers the mortgage to another lender.\n- Wants to renegotiate the terms of the contract.\nIf not planned or in case of an uncalculated decision, a borrower may end up paying more on breaking the close mortgage contract than they’d have otherwise paid on abiding by the contract.\nTo safeguard the interests of borrowers, an option called prepayment privilege is available. Prepayment privilege is a powerful tool that eliminates prepayment penalties for breaking a closed mortgage contract. It offers:\n- Flexibility to increase the regular payments up to a certain limit\n- Make lump-sum payments of up to a certain percentage\nCalculation Of Prepayment Penalties\nThe calculation of prepayment penalties depends upon several factors like repayment amount, remaining term, and lender. Most established lenders, financial institutions, and banks provide a prepayment penalty calculator on their official websites.\nGenerally, lenders charge a penalty equal to three months of interest on the remaining mortgage amount or the interest rate differential (IRD), whichever is higher. They calculate IRD when:\n- The current mortgage rate is lower than your current mortgage interest rate.\n- Your mortgage is not older than five years.\nExample Of Prepayment Penalty Calculation\nLet’s assume the outstanding amount is $200,000 and the current interest rate is 6 percent. So, the prepayment penalty will be three months’ interest.\n200,000 x 0.06 = 12,000\n12,000 / 12 = 1,000\nSo, 1,000 x 3 = $3,000 will be the prepayment penalty.\nNote that the IRD calculation comes into effect if 36 months are left from the entire term of five years.\nSuppose the current interest rate is 4 percent. Here, the difference between rates becomes 2 percent.\n$200,000 x 0.02 = $4,000\nNext, divide the term left with 12:\n36/12 = 3.\nFinally, $4,000 x 3= $12,000 will be the prepayment penalty as per the IRD method.\nLenders will charge $12,000 since it’s the highest of the two figures.\nPros And Cons Of Breaking A Closed Mortgage Contract\nGenerally, a borrower will break a closed mortgage contract if he comes across a better interest rate. Therefore, it’s advisable to weigh its pros and cons before going through with it.\n- Better mortgage rate with an extended-term.\n- You get peace of mind if you pay off the mortgage early.\n- Homeowners can use their house’s equipment to refinance their other needs.\n- You get an option to renegotiate the mortgage rate and term.\n- The idea of paying off early or renegotiating is to put off the burden and save the remaining interest. But the hidden charges and prepayment penalty associated with breaking the contract could cost you more in the long run.\n- There’s a possibility that a homebuyer fails to secure a new mortgage or refinance under stringent new economic conditions.\nHow To Reduce Or Avoid Prepayment Penalties?\nThe first step should be to understand the terms and conditions of the contract before inking it. Some useful tips to avoid prepayment penalties are:\n- Include the prepayment privilege option in the contract.\n- Utilize the prepayment privilege option fully to repay the mortgage amount early–this helps reduce the mortgage amount significantly. Keep in mind that lenders calculate a prepayment penalty on the remaining balance.\n- You can port your mortgage or blend and extend the mortgage to avoid prepayment penalties.\n- Wait till the term ends to reduce the prepayment penalty.\nAlways calculate the potential costs associated with breaking a closed mortgage contract. If ignored, you end up paying more in the form of prepayment penalties and other charges. Lenders are advised to research, compare, and seek expert advice before signing or breaking a closed mortgage contract.']	['<urn:uuid:5b8ced35-4272-45a0-851d-d143d75e81a6>', '<urn:uuid:888eb12d-9fdc-43f2-8aed-866d42378502>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T22:03:18.033296	5	111	2158
52	I oversee both aviation and construction safety, and I wonder what's the recommended minimum safe distance from dangerous weather conditions for workers at a construction site compared to aircraft in flight?	For aircraft, pilots must maintain a minimum distance of 20 miles from known thunderstorm activity because turbulence and lightning can extend beyond the storm's edges. For construction workers, while there's no specific minimum distance mentioned, they must seek shelter immediately upon hearing thunder, as lightning strikes are possible even if there's no visible lightning or rain yet, and they must remain sheltered for at least 30 minutes after hearing the last thunder.	['While distracted driving, speeding, and impaired driving play a big role in construction work zone accidents, as a construction worker you should also take steps to protect yourself. Hazards can change from minute to minute, so learn to recognize and control them for the safety of yourself and your crew., and ultimately follow the safe work rules and practices established by your employer. Brian Turmail, Vice President of Public Affairs & Strategic Initiatives at Associated General Contractors of America shares tips every on-road construction crew must know to keep themselves and their coworkers safe.\n1. Wear the right PPE\nThe Occupational Safety and Health Association (OSHA) mandates the use of personal protection equipment in construction for good reason. PPE is the last defense between you and an accident when risk can’t be eliminated through engineering and other administrative controls. Hard hats, high visibility gear, gloves, work boots, and fall arrest systems can protect you from injury. Know what PPE is required for the job at hand and insist it is not only provided but that it fits properly. It’s also up to you to ensure that your PPE is in good condition and working properly. Check it daily for signs of wear.\n2. Be aware\nYou’re especially at risk when working on foot on a highway construction site near moving vehicles and heavy equipment. The largest number of vehicle strikes don’t come from the traveling public but from contractor vehicles. Maintain constant awareness of your surroundings and always plan where you will run or jump to get out of the way.\nTake care to avoid blind spots and remain in clear line of sight of coming traffic and other machinery. Workers near the edge of work zones should never turn their backs to oncoming traffic.\nHeavy equipment operators should never move equipment without making visual contact with workers on foot near the equipment. Set parking brakes when leaving equipment unattended and chock wheels when equipment is parked on a slope.\nObtaining a basic orientation to the work site is essential to safety. It’s important to understand internal construction traffic flow, how vehicles are expected to enter and exit the work zone, emergency escape path designations, required PPE, and emergency signals for vehicle intrusions into the work area. This information could save your life.\nAGC’s Highway Workers Safety Program includes important advice to guide workers: Before beginning any task ask yourself, “How can I do this without injury or accident?“ Educational materials are designed to assist in mitigating highway worker safety exposures by increasing awareness of the hazards and the appropriate control measures.\n3. Protect yourself against severe weather\nSevere weather can not only wreak havoc on a jobsite, it can put you in danger. From lightning strikes and flying debris to heat and cold exposure, workers need to be prepared for a wide range of hazards.\nOSHA requires all employers to develop a written emergency action plan to protect their workers during any type of emergency, including bad weather. As a construction worker, you should know what this plan entails, including where to find emergency shelter, how to describe the work site location to emergency personnel, and who to contact in an emergency.\nAccording to OSHA, workers should pay attention to early weather signs indicating potential lightning strikes, such as high winds, dark clouds, or distant thunder or lightning. If you hear thunder, immediately unplug electrical equipment or tools, put them away, and move to a safe place. By the time thunder is audible, it’s possible to be struck by lightning — even if it isn’t raining yet or there is no visible lightning. Seek safety in a building or hard-top vehicle with the windows rolled up. Avoid tall structures, open spaces conductive materials, and explosives. Remain sheltered for at least 30 minutes after hearing the last sound of thunder.\nBoth motorists and construction workers can help reduce work zone accidents, but more needs to be done, as safety is everyone’s responsibility. According to Turmail, Oklahoma recently acted to require a one-hour training class on Work Zone safety for all new drivers. In Washington state, contractors are now able to park decoy patrol cars with flashing lights in the work zone. AGC would like to see the same social stigma around speeding in the work zone as there is for drunk driving.\nWe need everyone to get home safely.', 'Flying in Thunderstorms (What to Do & What Not to Do)\nVisually stunning and filled with deadly power, thunderstorms are one of the most fascinating and dangerous weather phenomena you are likely to encounter as a pilot. An estimated 40,000 thunderstorms pop up around the globe on a daily basis, so your odds of having to deal with one are reasonably high.\nWhen this happens – think when, not if – you will be grateful for everything you have learned about flying in thunderstorms.\nOf course, the best advice about flying in thunderstorms is – not surprisingly – don’t do it. Improve your knowledge of how thunderstorms develop, your ability to read the weather patterns and your skills to avoid severe storms. Do that, and with luck, you may never find yourself with a front row seat to one of Mother Nature’s biggest shows.\nOf course, we can’t count 100% on luck, so step 2 is to know what to do if you find yourself in a thunderstorm and how to get out of it safely and quickly.\nKeep reading for all this information plus answers to some of the most commonly asked questions about flying in and around severe weather.\nWhat Causes Thunderstorms?\nLet’s start off with a little basic meteorology. We have a general working knowledge of what a thunderstorm looks like, so let’s talk about how they develop. If you know what conditions give rise to thunderstorms, you will be better able to spot the early warning signs when you see them.\n3 Conditions that are necessary for a thunderstorm to form\nA thunderstorm develops from a combination of 3 things. Take away any one of these variables and you may still have inclement weather, but you won’t have a thunderstorm.\n- Moisture (Measured by the humidity level in the air)\n- Unstable air\n- Lifting force (can be caused by a wind, heat rising off the ground, electrical charge, air fronts)\nHow to Avoid Thunderstorms\nLet’s say you notice that all 3 thunderstorm causing conditions are present in your area or on your flight path. What do you do next?\nIf you are on the ground when storm conditions develop, great. Stay there until the storm passes.\nNot so lucky and you are in the air when the storm develops? The first thing to keep in mind is that when avoiding a thunderstorm, the standard recommendation is to give it a berth of at least 20 miles. This is because turbulence and lightning can extend beyond the edges of the storm itself.\nCheck the latest broadcast from the Hazardous In-Flight Weather Advisory Service (HIWAS) to find out what dangerous weather conditions may have been reported in your area. You may also consult the En Route Flight Advisory Service (EFAS) and an FAA Flight Service Station (FSS) for specific information that will help you navigate around the storm.\nATIS, ASOS and AWOS advise on weather conditions at each airport, so remember to consult with them as well.\nYou will need to know the current and predicted flight conditions at your originating and destination airports plus what flight rules apply along your proposed flight path before you can make an informed decision.\nRemember, sometimes thunderstorms can hide inside a cloud and you will be unable to see these embedded thunderstorms until you are right on top of them.\nWhich brings us to…\nWhat Happens if You Fly into a Thunderstorm?\nFasten your seatbelt – it’s time to talk about the worst-case situation. Let’s say you were unable to avoid a thunderstorm and you end up flying into it. What should you expect?\nVisibility will be limited, and if it is nighttime and you see flashes of lightning, that will degrade your night vision.\nYou will experience wind shear as well as updrafts, downdrafts or a combination of both. This depends on what stage of formation the thunderstorm is in.\nIn their development or cumulus stage, moist air is being drawn into the developing storm and heat is being released. This creates updrafts.\nAt its mature stage, conditions are the most variable and you will face a combination of updrafts and downdrafts.\nFinally, an anvil shape appears on the top of the storm indicating that it has reached the dissipation stage. At this point, expect mainly downdrafts.\nHow to Get Out of a Thunderstorm\n- Slow Down\nThe first thing to do if you find yourself in or dangerously near a thunderstorm is to slow down to maneuvering speed for your aircraft. The strong fluctuations of the storm’s vertical forces will be less likely to damage your aircraft if you are flying at a reduced speed.\n- Disconnect autopilot\n- Focus on maintaining attitude control\n- Do not turn around\nTurning around is not advised because the maneuver increases already heightened structural stress on your aircraft.\nGive them your information and request the best path through the storm\nAvoid the temptation to fly under a thunderstorm even if you have good visibility. Remember that turbulence is usually the worst below the cell.\nTo help protect your vision from temporary blindness due to lightning, keep your eyes on your instruments.\nAnswers to Your Severe Weather Questions\nTime to field some of the most common and burning questions that new pilots and the general flying public have about how severe weather affects aircraft.\nDo planes get struck by lightning?\nWe all want to know what our odds are of being struck with lightning while in the air. For commercial jets, the FAA has estimated a likelihood of one lightning strike per 1,000 flight hours, or an average of once a year. In theory, your odds of a lightning strike is lower in a small aircraft, although there are fewer hard numbers to back this up.\nThe effects of a lightning strike depend on the size of the plane and whether it has been shielded and designed to withstand a lightning strike. The aluminum skin of a passenger jet keeps the current restricted primarily to the outside of the aircraft. A lightning protection engineer also installs shielding, grounding and devices to suppress surges that could otherwise damage the plane’s electrical equipment.\nThe FAA also has regulations in place to ensure manufacturers of critical aircraft components build their products to withstand a lightning strike.\nCan you fly over a hurricane?\nThe short answer to this interesting question is “no.” The longer answer is that for commercial aircraft, it does depend a little on the size of the hurricane and the situation, but for the most part, the answer is still “no.”\nWhile you may fly over the very outer bands of a small hurricane in an emergency, the wind shear and extreme vertical height of a hurricane (up to 50,000 feet) make it virtually impossible to fly over a fully developed, category 4 or 5 hurricane.\nThe best action to take when it comes to aircraft and thunderstorms is to keep the two separate. Know the conditions that can lead to storm development and avoid flying in the area of known thunderstorms if possible. Remember to maintain a minimum of 20 miles from known thunderstorm activity. Consult all your available weather resources for the most up to date information when planning your flight.\nIf you do find yourself in a thunderstorm, follow best practices to get through the situation. Slow down to maneuvering speeds and avoid turning around. Disengage auto pilot and maintain attitude control. Consult flight following services for course guidance.\nThe best action you can take is to learn, plan and train so that you are prepared for whatever weather situation you may encounter.\n- PilotMall.com Editor']	['<urn:uuid:d2f83d9f-089f-461b-9f9b-0f3aa8850cfa>', '<urn:uuid:e28348c6-0932-4414-9c9b-58c128c796df>']	factoid	with-premise	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T22:03:18.033296	31	72	2005
53	maximize storage memory card quality tradeoff	You can set your camera to take lower resolution pictures to fit more images onto one memory card, but this sacrifices image quality in the process. Lower settings should only be used when you are certain the images will never be printed on paper.	['More and more people are experimenting with photography as a hobby and a way to express themselves. Photography can be both very interesting and frightening due to the complexity of what professional photographers are able to do. You can begin to cultivate better photographic skills by taking a look at the advice below.\nDecide what aspects of your subject you want to capture in your photograph. Many good photos show only a carefully chosen portion of the subject, rather than the whole thing. If you try to put too much into one photograph, you’ll overwhelm yourself and your viewers. If you want to give a more general impression of something, take a series of pictures, rather than a single photograph with no real focus or details.\nOne way to make the subjects of your picture pop out is to have a background that is not as well defined. When everything in the picture is focused including the background, it will make the picture a bit busy and it will be hard for the viewers to specifically focus on the subject of the picture. You can do this by having your background farther away from your subject.\nMake sure the subject you are photographing is close enough to clearly see. Not doing so can result in photos that are too distant or blurred for any clarity. Do everything you can to make sure your subject stands out and is seen clearly.\nYou need not stand directly in front of an object to photograph it. Shoot from several different heights and vantage points to open up possibilities you may not have otherwise envisioned.\nYou will have a better picture if your subject is off-center a little. Pre-focus your camera, and move a little to one side. A centered picture is usually not as interesting. Shooting a photo that is not exactly centered on the subject may produce more interesting results for your viewers.\nIt is important to combine your ISO, aperture, and correct shutter speed. These are how you determine your picture’s exposure. You don’t want to overexpose or underexpose a picture, unless you are going for a certain look. Take some time to experiment using these features so that you learn how they interact, and which combination you like best.\nInspire your own creative photographs by joining photography clubs or taking photos with another photographer. You could learn a lot from other people, but do not let their style influence your pictures. Compare the stylistic differences between photos you both took of the same object.\nThough people tend to think that white is a desirable color to wear when having their photograph taken, in reality it is one of the worst. Autofocus is a feature that many cameras have. This setting determines all the shades in the picture. The contrasting background will make the white clothes stand out instead of looking washed out.\nMake sure you frame all of your shots. Not a physical frame, but one that is totally natural. It is possible to build a natural frame by looking for items in the picture. This helps to build your compositional skills.\nYou may want to set your camera to take lower resolution pictures so that you can fit more images onto one memory card, but realize that you are sacrificing some quality in the process. Lower settings should only be used when you are one hundred percent confident that the images you are capturing will never see printing on paper.\nTo turn photography into a lifelong hobby, it is vital that you invest in the best camera equipment. Most professionals can afford name brands, but you can find many quality cameras for a good price.\nYour camera can be a fantastic way to get beautiful shots if used correctly. Try blurring the background and focusing on the main subject by decreasing your field depth.\nLighting will not always be on your side when you are shooting landscape. At times, you may be unable to locate a preferable area. What should you do in this situation? You can use a program, such as Photoshop, for creating a gradient filter that can counterbalance the contrasting light.\nIf you squat or bend down to take the picture, you will get the best results. This simple technique significantly improves the quality of your shot.\nMake sure you use optical zoom instead of digital zoom for macro shots. Many cameras let you zoom in on your subject as much as you’d like, but keep in mind that image quality decreases when the camera switches from optical zoom to digital zoom. Digital mode will add pixels to your image and make the image quality very poor. If you have a look inside your camera’s manual, you will be able to discover how to turn the feature off.\nGet closer to your subject. When framing a picture, zoom in or move physically closer to the focal point of your shot. So that you know, the subject you are trying to capture takes up the frame. Too much scenery or visual noise, no matter how interesting, distracts the eye from where the focus should be: the subject. When you get closer to a subject, the interesting details and character of a shot pop out.\nYour camera gear needs to be protected with sturdy cases. Most of the time, the reason why photography accessories break is due to the owner’s negligence. Cases that stand up to protecting your gear are widely available wherever cameras are sold.\nCrop pictures to make them better. You may find that you have captured a great image of the subject, but that the objects in the background take away from it. Sometimes, you may have taken a perfect picture that is slightly off-center. These are problems that can be easily altered by using the cropping feature, and cutting the unwanted item out of the picture.\nAs mentioned earlier, photography is a hobby that appeals to a vast number of individuals. Some people may give up their photography dreams due to the vast amount of knowledge that is required for composing a great shot.']	['<urn:uuid:cf381963-dbec-424b-8b0b-c75ebc84cb80>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T22:03:18.033296	6	44	1020
54	what business department handles worker complaints	The human resources department is responsible for managing employee complaints, along with other duties such as entering and storing employee information, processing new hires, handling employee outreach, promoting employees, and terminating employees.	['To be successful, businesses must track costs and income streams to build an accurate business plan and budget. An important factor influencing budgets is the cost center, something that the business needs to spend money on that doesn’t directly generate revenue of its own. A better understanding of cost centers provides insight into how businesses are organized, along with how to read accrual-based accounting financial statements. Read on to learn more about what cost centers are, why they are important, and how they work.\nWhat Are Cost Centers?\nCost centers refer to roles or departments, also known as business units or service centers, within a company that cost money but do not generate revenue. These include administrative, service, and support services. These parts of the organization cannot be eliminated to save money because they are an essential part of keeping operations running as smoothly as possible.\nCost centers are listed separately to keep their resource usage easier to monitor. Cost center managers are responsible for making sure their cost centers run efficiently and within budget.\nCost centers may be roles, such as human resource staff or janitorial services, or they may be entire departments, such as the warranty department, the marketing department, or the IT department. The number and size of cost centers a company may have will depend on industry and company size.\nFor instance, small companies may have a cost center for bookkeeping or office management, where there is a single person who manages everything. But, for a large multinational corporation, there may be separate cost centers for the human resource department, the administrative staff, and the accounting department. The wages of each employee in those departments will be included in the cost center budget, for the cost center they work.\n“Cost centers can be thought of as responsibility centers that influence the profitability of a business.”\nWhy Cost Centers are Important\nCost centers are important because they add value to the company. Without them, it would be difficult, if not impossible, for the rest of the company to generate profit.\nImproved Customer Experience\nCustomer service departments help customers resolve complaints or other issues, locate products, and understand company policy and warranties. Providing excellent customer service increases company value and house to build a loyal customer base.\nResearch and Development\nResearch and development departments seek to find innovative solutions to consumer issues and create new products.\nMaintenance workers, janitorial staff, and IT departments work to make sure that all of the company’s equipment is running efficiently and that facilities remain safe and clean. These workers ensure you can generate profit and keep your staff healthy.\nData Analysis and Business Strategy\nMarket and data analysis departments make it easier for you to understand consumer trends and industry changes. These departments provide information that helps you see how effective your current business strategy is and changes that you need to make moving forward.\nAdministrative Task Management\nThose working in your office, managerial accounting, and human resource departments ensure that your company is submitting reports and forms accurately and managing budgets, and office organization and payroll. They also complete many other important administrative tasks necessary to keep the business running\nHow Cost Centers Work\nCost centers have their own categories in the general ledger so that accounting teams can track costs and resource allocation. Each cost center within your company should have a manager who is held accountable for the department budget and the staff assigned to that cost center.\nCost centers work to:\n- Define management responsibilities\n- Define cost center processes\n- Use monitoring to lower costs\n- Assign budgets\n- Help your finance and accounting department understand your business\nThe financial accounting department assigns cost centers based on your business needs and how you spend your money. They often have a broader perspective of budgeting needs than individual cost center managers and therefore help structure resource allocations for improved efficiency over time.\nIdeally, cost centers are created with a set of parameters that includes the scope of work and details about managing accounts, products, and vendors. Be clear about your purchase order process is, what vendors are approved for ordering, and how invoices are handled.\nThe manager of a cost center needs to be held responsible for managing the budget of their cost center. This responsibility needs to be listed in their management objectives. It’s best to avoid assigning a manager multiple cost centers without good reason. Cost center managers need to have a clear idea of the cost center’s primary goal so they can effectively manage the budget they’re given to reach those goals.\nCost centers need to have clear budgets and their managers need to track that spending. This allows the managers and finance and accounting department to understand the resources that each cost center uses, how the resources are allocated within the cost center, and what changes need to be made for the next fiscal year.\nBy tracking your cost centers for staff and efficiencies, overspending, and other challenges, you can lower your overall cost. The data you collect from your cost center structures can help guide reorganizations and inform future budget allocations. Communicating your cost management techniques and goals with your managers helps greatly.\nExamples of a Cost Center\nThe best example of a cost center is the human resources department. The human resources department doesn’t generate revenue because it is an administrative department. However, it is crucial in ensuring an organization runs smoothly because they are responsible for entering and storing employee information, managing employee complaints, processing new hires, handling employee outreach, promoting employees, and terminating employees. The human resources department may also play a role in recruiting, training, and payroll.\nWhen organizing your human resources cost center consider the:\n- Duties the department will be responsible for\n- Number of employees you will need to manage the assigned duties\n- Pay rate and benefits cost for each position in the department\n- Cost of office equipment and software\n- Different vendors the department will have access to\n- Projected Department maintenance cost\n- Overall projected budget for the department\nThe human resources manager needs to be experienced in the department so that they can efficiently guide and help the staff as needed. Ideally, they should be able to balance employee needs also managing costs.\nOther examples of cost centers include:\n- Customer service department: Staff in this department address customer complaints, manage warranties and rebates, and work to improve the overall customer experience.\n- IT department: This department ensures that software, hardware, and networks are working correctly, securely, and are updated appropriately.\n- Janitorial: Works to keep the building safe and clean so that staff and visitors can be productive\n- Project managers: These people plan, budget, schedule, and monitor projects to ensure they’re completed correctly and as efficiently as possible.\nNone of these departments generate profit, but they are crucial to business function and play a role in cash flow and investment decisions.\nCost Centers vs. Profit Centers\nWhere the cost center costs a business money but doesn’t generate direct profit, the profit center focuses on generating revenue for a company. Profit centers are subunits within a business that are responsible for profits, with a focus on maximizing revenue and profit. Profit centers are more complex because they focus on revenue, profit, and costs. Each profit center is backed by cost centers to ensure the business continues to operate.\nPLANERGY can help you monitor and control spending for all your cost centersFind Out How']	['<urn:uuid:5cad821c-1047-48ab-9ac6-54b94b5413d5>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T22:03:18.033296	6	32	1253
55	How did ancient societies handle the artistic and religious aspects of figurine production, considering both the Upper Paleolithic cave paintings and the Cycladic marble figurines?	In the Upper Paleolithic, cave paintings showed sophisticated artistic skills, with understanding of perspective and animal behavior, and were likely used for rituals rather than decoration. Similarly, Cycladic marble figurines demonstrated artistic sophistication through their proportional systems and detailed craftsmanship. While both had religious significance, the Cycladic figurines evolved to have dual meaning - they maintained their religious symbolism but also developed an artistic value, as evidenced by repaired specimens and multiple figurines found in single graves. Both types of art required specialized skills - Paleolithic painters could render perspective and animal behavior, while Cycladic figurines were made by specialized craftsmen, indicating significant investment of time and resources in their production.	"['The Upper Palaeolithic (Upper Paleolithic or Late Stone Age) is the third and last part of the Palaeolithic period. It lasted from about 40,000 to 10,000 years ago. Humans used tools for hunting and fishing. They also developed cave paintings. In this period, the Neanderthal man completely disappeared, leaving Homo sapiens as the only surviving species in the human genus.\nThe first modern humans found in Western Europe date back to about 36,000 years ago. Those fossils were found in the south-west of Romania. The founds were made in a stone cave called Peștera cu Oase.\nEvidence for belief in the afterlife in the Upper Palaeolithic: appearance of burial rituals and ancestor worship.\nCulture[change | change source]\nVenus figurines[change | change source]\nPossibly among the earliest traces of art are Venus figurines. These are figurines (very small statues) of women, mostly pregnant with visible breasts. The figurines were found in areas of Western Europe to Siberia. Most are between 20,000 and 30,000 years old. Two figurines have been found that are much older: the Venus of Tan-Tan, dated to 300,000 to 500,000 years ago was found in Morocco. The Venus of Berekhat Ram was found on the Golan Heights. It has been dated to 200,000 to 300,000 years ago. It may be the one of the earliest things that show the human form.\nToday it is not known what the figurines meant to the people who made them. There are two basic theories:\n- They may be representations of human fertility, or they may have been made to help it.\n- They may represent (fertility) goddesses.\nScientists have excluded that these figurines were linked to the fertility of fields, because agriculture had not been discovered at the time the figurines were made.\nThe two figurines that are older may have mostly formed by natural processes. The Venus of Tan-Tan was covered with a substance that could have been some kind of paint. The substance contained traces of iron and manganese. The figurine of Berekhat Ram shows traces that someone worked on it with a tool. A study done in 1997 states that these traces could not have been left by nature alone.\nCave paintings[change | change source]\nCave paintings are paintings that were made on the walls or roofs of caves. Many cave paintings belong to the Palaeolothic Age, and date from about 15,000 to 30,000 years ago. Among the most famous are those in the caves of Altamira in Spain and Lascaux in France.p545 There are about 350 caves in Europe where cave paintings have been found. Usually, animals have been painted, like aurochs, bisons or horses. Why these paintings were done is not known. They are not simply decorations of places where people lived. The caves they were found in usually do not show signs that someone lived in them.\nOne of the oldest caves is that of Chauvet in France. Paintings in the cave fall into two groups. One has been dated to around 30,000 to 33,000 years ago, the other to 26,000 or 27,000 years ago.p546 The oldest known cave paintings, based on radiocarbon dating of ""black from drawings, from torch marks and from the floors"". As of 1999, the dates of 31 samples from the cave have been reported. The oldest paintings have been dated from 32,900±490 years ago.\nSome archaeologists have questioned the dating. Züchner believe the two groups date from 23,000–24,000, and 10,000–18,000 years ago. Pettitt and Bahn believe the dating is inconsistent. They say the people at that periods of time painted things differently. They also do not know where the charcoal used to paint some things is from, and how big the painted area is.\nPeople from the Palaeolithic era drew well. They knew about perspective, and they knew of different ways to draw things. They also were able to observe the behaviour of animals they painted. Some of the paintings show how the painted animals behaved. The paintings may have been important for rituals.\nRelated pages[change | change source]\nReferences[change | change source]\n- ""Upper Paleolithic from Academic Press Dictionary of Science and Technology"". credoreference.com. 2011 [last update]. http://www.credoreference.com/entry/apdst/upper_paleolithic. Retrieved July 21, 2011.\n- ""Paleolithic Period (anthropology) -- Britannica Online Encyclopedia"". britannica.com. 2011 [last update]. http://www.britannica.com/EBchecked/topic/439507/Paleolithic-Period. Retrieved July 21, 2011.\n- ""The Stone Age"". history-world.org. 2011 [last update]. http://history-world.org/stone_age.htm. Retrieved July 21, 2011.\n- Trinkaus, Erik (2003). Early modern human cranial remains from the Pestera cu Oase, Romania. (45 ed.). Journal of Human Evolution. pp. 255–259.\n- ""\'Oldest sculpture\' found in Morocco"". BBC News online. 23 May 2003. http://news.bbc.co.uk/1/hi/sci/tech/3047383.stm.\n- Alexander Marshack (1997). ""The Berekhat Ram figurine: a late Acheulian carving from the Middle East"" (pdf). http://www.utexas.edu/courses/classicalarch/readings/Berekhat_Ram.pdf.\n- Klein, Richard G. 2009. The human career: human biological and cultural origins. 3rd ed, Chicago.\n- Quotes from Clottes 2003b p214.\n- Archaeologists sometimes use the phrase ""B.P."" (before the present day) to mean ""years ago""\n- Clottes 2003b p33. The oldest is sample Gifa 99776 from ""zone 10"". See also Chauvet (1996 p131, for a chronology of dates from various caves. Bahn\'s foreword and Clottes\' epilogue to Chauvet 1996 discuss dating.\n- Züchner, Christian (September 1998). ""Grotte Chauvet Archaeologically Dated"". Communication at the International Rock Art Congress IRAC ´98. http://www.rupestre.net/tracce/12/chauv.html. Retrieved 2007-12-23.\nClottes (2003b), pp. 213-214, has a response by Clottes.\n- Pettitt, Paul; Paul Bahn (March 2003). ""Current problems in dating Palaeolithic cave art: Candamo and Chauvet"". Antiquity 77 (295): 134–141. http://www.antiquity.ac.uk/ant/077/Ant0770134.htm.', 'Cycladic marble figurines are particular objects that for their style and shape already in ancient times have produced a great interest on them, still present after their rediscovery in the last century  . In Minoan Crete there was a production of imitations, while the originals are widespread in the whole Aegean area attesting their use as merchandise. The figurines were luxury objects in the same Cyclades; they were not very common, at least in marble  . However, they must have had a role in the society that produced them originally. Paradoxically, whereas we understand the significance of their presence in contexts outside Cyclades, as art objects and exchange goods, we have some difficulties to understand their meaning and purpose in the society, which produced them.\nThe production of these figurines was certainly stimulated by the wide presence of marble in the area, and probably firstly were shaped marble figurines and then their possible imitations in wood or other perishable material. The emery from Naxos and the obsidian from Melos were extremely useful to work the marble, the first as an abrasive in powdered form, the other as cutting and incising tool. In the island of Thera there was also another abrasive agent, perhaps less used: pumice. The most important centres of production were in Paros and Naxos, where it is possible to find the best Cycladic marble. These two islands were particularly prosperous in Early Cycladic certainly also for the production of marble objects, not only figurines.\nThe figurines appear for the first time in Early Cycladic period, precisely in the second stage of Early Cycladic I in a schematic form  , an ancient moment of the Cycladic culture, while the first naturalistic ones appear in the third stage of the period. There are no clear antecedents, even if many have been suggested. Among these, probably Neolithic material from Saliagos  , Kephala and the mainland could really have influenced them. Renfrew thought to the Bulgarian figurines as prototypes for the folded-arm position; anyway certainly there is an high degree of originality in the Cycladic figurines: they are really considered as examples of prehistoric art. The newest parallels are from Sardinia, but even if similar, it is impossible to do a strict comparison with any other figurine from other cultures. Another singular thing is their size: it varies from 5 cm to 1.5 m. The figurines are anthropomorphic and the majority has been found in graves, but the existence of repaired exemplars and the large quantity found as offerings on Keros, as well as the single figurines found in settlements, demonstrate that they were not made specifically for the graves. Unfortunately many figurines, too many, do not come from archaeological contexts, and a few at least could be a false, while others could be hidden in private collections. In any case, there was a loss of precious archaeological data that constrict us to wait for new unearthing to have definitive answers, or at least valid hypotheses. The type of marble cannot help us to distinguish among the varieties: similar fine marble is present in the whole Aegean area, from Paros and Naxos to Crete and the mainland.\nThe earliest figurines are schematic, stylised, with a body, a neck without head and arms minimally traced. Since the beginning there are various types attested. Among the schematic figurines, particularly famous and suggestive is the ""violin form"". There are also more naturalistic types in which there are attempts to define the head, the legs and a few details in the body. Three are the most important earliest groups: Pelos, Plastiras and Louros. These names derive from the cemeteries, where firstly these groups of figurines were found, but a figurine of one type could be present also in a site where the common group is another. In particular both males and females, in standing position with an head and face, compose the Plastiras type, while the Louros type is extremely schematic.\nThe folded-arm figurine is the most common type, originally it belongs to the Syros group, but there are many variants. Exemplars reached Asia Minor and western Greece, while in Crete and Attica they were imitated. Renfrew suggested a relative chronology for the folded-arm figurines, basically analysing the stylistic differences. The face is upturned with only the nose indicated; the neck is short; the figurines generally represent a female, always naked with the pubic triangle signed by incision. The arms are folded in the middle of the body, while the legs are always held together and often the feet, except in the Kapsala and Chalandriani varieties, are inclined suggesting a lying position. Fingers are indicated by incisions, but there is a lack of many other details. Also the back is extremely simple. The result is a very graceful, slim figurine. Sometimes, for the dimensions, we should speak of these as statues, probably could have been existed also exemplars bigger than 1.5 m in height, the maximum dimension known for these objects. However what we see today is not how the figurines looked in the past: many details, such as facial ones, hair, and sometimes jewellery were painted. When the folded-arm figurines appeared, the schematic ones continued to survive and some of these were found in contexts later than EC, even if after the end of Early Bronze Age it is difficult to think to a continued, though reduced, production according to the evidence we have. For this reasons we can presume that the production of marble figurines has ceased in Early Cycladic III A.\nThere are also more complex and varied compositions, like several musicians, a cupbearer, hunters-warriors  and others. The meaning of these figurines can be clearer, as in the case of hunters-warriors, but also more cryptic, in the other cases. All the figurines, comprised the complex ones, use a system of proportions since the earliest period. The schematic figurines could be divided generally in three parts, while the naturalistic ones in four. A simple form of compass was probably used to mark these divisions on the block of marble. The complex ones, instead, use a less definite sum of geometrical rules.\nThe cemeteries in the Cyclades were grouped in base of their characteristics: it is useful therefore to provide a brief description of founds for each group. In the Lakkoudhes group there is an entire absence of any kind of figurines and marble objects. In the Pelos  group the common type of figurines is schematic, sometimes they present few anatomical details. The figurines in the Plastiras  group are the most naturalistic, they constitute a new type, and they are often associated with Pelos type. In graves, Plastiras figurines are associated with obsidian. In the Kampos group graves it is possible to find the Louros type and the folded-arm figurines. In one grave of this group a figurine was particularly connected with a frying-pan: a vessel traditionally connected by archaeologists to religion. There was also obsidian as blades and cores. In the Syros  group there are folded-arm figurines and the complex figures, as well as schematic figurines. In the Kastri group there are a few figurines, without particularities, they come from other groups. To be noticed tomb 14 in the Amorgos cemetery, part of the homonymous group, where there were two folded-arm figurines classed by Renfrew as Dhokathismata. In Phylakopi I group are attested folded-am figurines, schematic ones and a few typical of the Pelos and Syros group.\nFigurines were found in settlements, particularly Ayia Irini and Phylakopi, in the ritual site of Dhaskaleio Kavos on Keros and in a few graves out the many found in the Cyclades. These graves generally present rich goods, but it is important to notice that not every rich grave contains figurines; the reality is far from this affirmation. Doumas thought to the figurines as offerings present in the richest tombs, but if this idea is true when we have graves with figurines  , certainly it is not a rule. However, the lack of documentation from one hand  , and the possibility of the presence of figurines in perishable materials on the other hand, could permit us to imagine that figurines could have been really common in that society, but in a ""poorer type"". I think that also ancient people was fascinated by the shape of these objects, as the imitations demonstrate, but the costs of the production in marble should have been extremely high, so people probably used wood painted, that at the end looked exactly as the marble figurines. If this is true, we could also think, but it is an hypothesis, that the original production was done for living people and only in few cases people agreed to put in the tomb a so beautiful and expensive object, preferring to substitute it with a copy in wood, when not using a wood imitation also during the life! Not everybody probably had the economic possibility to have a marble figurine. Supporting the argumentation of a long life for figurines, with several different uses, is also the fact that a few examples show sign of repairs. This is a clear confirmation that figurines were considered precious objects, to use during all the life or probably more, from generation to generation. It is difficult to imagine why these figurines were repaired; possibly because of their cost or more probably for a special value or symbolism they carried. The figurines had probably also a role in burial practices, but not important as the lack of figurines from the majority of tombs suggest. At least they were not an essential part of the ritual. Wood figurines cannot be considered as replacement in the ritual, because generally a rite has strict rules. A confusing thing is that sometimes archaeologists have found more than one figurine in a grave; this means that these objects were not so strictly personal, probably they were used by an entire family, and for different purposes. We do not know if a difference in the style had a particular symbolic meaning or it was just a variant produced by craftsmen. However four uses are at the moment attested: as objects for ceremonial exchanges, as objects used in ceremonies, as cult images and as part of the funerary ritual. What we know about the ritual is that there was a limited space available and for this reason when there were later inhumations in the same grave, the bones were removed while the skull was always left in its original position. Offerings were left to the dead, probably for its life after death. Among the offerings, we find sometimes marble figurines, different in style and probably sometimes also they carried a different symbolism. We do not know the meaning of figurines in the various uses, nor if there were really other uses outside religion, as it is highly suspected, for examples as art objects and/or personal amulets. Figurines may simply have been felt appropriate in the tomb to bring personal possessions along with the dead, because of their intimate connection with him or her. If there was belief in an afterlife, the buried figures could have been intended as servants or concubines. Otherwise they could have been intended as gifts to the deities who controlled the destinies of people. In this case they could be representations of duties.\nIn particular, the marble figurines of female form found in graves are often thought to have a special religious significance, to be figures of a deity or perhaps her worshippers. The use of female figurines with explicit sexual characteristics could be easily explained thinking to an agricultural society, where fertility played an important role. Most likely these figurines, found in graves, were used during the life by the dead; they were not made specifically for the funerary ritual and should have had not a great part in the ceremony, even if they certainly had an important symbolism and meaning, so that they were left for the important function of accompanying the dead in its new world. During the life probably the most common place was in household shrines. In this case a possible meaning for their deposition in graves could be the devotion of the owner, the meaning of figurines would be limited to a religious sphere, as point of contact between deity and owner. The time lost in the production of these figurines, in a prehistoric society, is another indication of their importance: there were evidently many difficulties to permit to specialised craftsmen  to produce objects whose usefulness was not immediate.\nIn conclusion, these fascinating figurines remain mysterious for some aspects, especially they meaning and symbolism, but with this shade of mystery figurines are also more fascinating. Only a general description and a few hypotheses are possible at the moment. Only new discoveries could bring new light on this theme. In my opinion, they were used in funerary contexts with a double meaning: as personal belonging loved by the owner with also a religious symbolism that was probably the original reason when the production of figurines began. But soon their artistic shape may have changed their original religious meaning to a non religious one, even if the original symbolism remained. Repairing figurines makes sense in an artistic collection: I do not think that a prehistoric society decided to offer to the deity a ""second-hand"" object. The Cycladic society could have discovered a primitive sense of art. Yet connected with religious matters, figurines were probably appreciated also because ""beautiful"". This was a new and confusing feeling for the same ancient people, but it will explain why sometimes we find repaired figurines (they were something more than an idol or an amulet: a loved personal possession), why there are graves with more than one figurine (probably there were collectors of these nice things, interpreted by the rest of community as a very religious person for the original meaning that figurines still carried), why sometimes we do not find them (they were not a fundamental object for the funeral ritual and as some people decided to have them in the grave, others could have thought to conserve the figurines in the family\'s house, as a prestige object, something that could have distinguished a little group from the others, in a community where there was only a little space for things not directly connected with survival). Figurines in this hypothesis carried a new meaning after a certain time of their production (a primitive sense of art), while their symbolism become less and less important. Other hypothesis are however possible.\nBarber, R. 1985. The Cyclades in the Bronze Age.\nDoumas, C. 1977. EBA Burial Habits in the Cyclades. Studies in Mediterranean Archaeology.\nRenfrew, C. 1969. The Development and Chronology of the Early Cycladic Figurines. American Journal of Archaeology 73.\nRenfrew, C. 1972. The Emergence of Civilisation.\n The first figurines were unearthed and studied by Christos Tsountas at the end of 19th century. They seem creations of some tendencies of modern art, so they look very modern and original. This is one reason of their popularity, attested also by the quantity looted.\n Probably these figurines were also produced in wood or in other perishable material. The difference between the two versions was covered at a first look by the painted decoration, which surely was used in the marble figurines as the traces visible in our days clearly show.\n The earliest schematic type, made by a flat piece of marble shaped in an anthropomorphic outline, and sometime incised in one surface, is called ""Brettidolen"". At least fourteen figurines of this type were found in grave 103 in the Pyrgos cemetery in Paros. The majority however come from Paros, Antiparos and Dhespotikon cemeteries.\n Though the Neolithic figurines from Saliagos, schematic and very similar in the form to the Cycladic ones, do not appear at the later site of Kephala, where there are other types.\n This type is constituted by male figurines with a dagger or a similar object, and are interpreted as images of people that had to hunt and offered this image to the deity for protection and good results or as images of warriors offered to the deity before the war to ask protection.\n A cemetery in Melos island.\n Plastiras cemetery in Paros.\n In Syros particularly important is Chalandriani cemetery.\n It is an hypothesis.\n There are more figurines in museums all around the world than figurines quoted in reports of regular excavations.\n The stylistic analysis has shown that all the figurines were created by few persons that were evidently specialised in their production.\n© Andrea Vianello - Sheffield, 1999']"	['<urn:uuid:c36dd9f4-9e71-4d92-b916-49935ced55ed>', '<urn:uuid:ceaea508-ae22-4894-80e2-33a39252eb02>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T22:03:18.033296	25	111	3655
56	Curious about waterfront structures - what's the history and underwater maintenance of riverside buildings?	Taking Readyville Mill as an example, it was built in 1812 along a river and served multiple functions including as a grist mill and power plant. For waterfront structures, underwater maintenance requires three inspection levels: general visual inspection, close-up examination with cleaning, and detailed evaluation using specialized testing methods. Inspectors must consider water quality, requiring coordination with local monitoring agencies and appropriate safety precautions for divers conducting these evaluations.	['Readyville Mill Serves History and Hospitality\nFor those who like a side of history with their meals, Goodness Gracious at the Mill in Readyville is made to order. From the creaking of the original hardwood floor to the grain scale and dispensary in the dining room, a visit to the grist mill-turned-restaurant is a delicious trip back in time.\nBuilt in 1812 by town founder Charles Ready, the Readyville Mill served many functions – grist mill, lumber operation, ice house and even a small power plant, generating electricity for the families in the small, rural town. The mill burned in the 1860s but was rebuilt around 1870, and for the next 100 years changed owners several times. In 1973, the mill was placed on the National Register of Historic Places but closed in 1978 after an unsuccessful preservation fundraising campaign.\nThe mill sat dormant and in disrepair until 2006, when Readyville native and millstone collector Tomm Brady bought and revived it. Brady spent years restoring the mill to its original condition before selling to current owner Karen Ford in spring of 2014.\n“Tomm had the mill for five years. He collected millstones, and someone directed him to the river to find millstones, and that’s how he found this place,” Ford recalls. “It was in serious disrepair. Tomm found an original board in the attic and copied the color of the board when he painted the buildings. Over the years, he acquired all sorts of antiques and treasures to add to the décor. People have even dropped off antiques. I don’t know how Tomm had the vision to take [this project] on, but he did an amazing job, and it’s really a treasure now.”\nA longtime restaurateur and caterer in neighboring Murfreesboro, Ford says she and her husband Bob were drawn to the mill’s history and tradition, as well as its breathtaking location.\nSEE ALSO: More Mills in Tennessee\n“We love the area. It’s beautiful. I like the calm, country setting along the river, and it’s close enough to town to be able to survive in this day and age,” Ford says. “I like the history, and you can’t help but feel like you’ve stepped back in time when you walk in here. I like that we’re able to carry on a tradition. We’re just caretakers carrying it on for the next set of people to come along.”\nThe original mill comprised several structures linked together, Ford says. The restaurant’s main dining room once served as the granary, and a secondary dining area was the mill’s ice house. The property also features the original miller’s cabin, a working grist mill, a restored smokehouse that was donated by a Cannon County resident, and a woodshop that Brady built for his son.\nGoodness Gracious is open Friday through Sunday for breakfast and lunch. The mill itself has become a favored destination for history buffs, but those in the know come for the food – a hearty sampling of Southern goodness, including made-from-scratch biscuits, whole-wheat pancakes made from flour freshly ground in the mill and homemade corn cakes smothered in locally produced fruit preserves. In fact, Goodness Gracious sources from a number of local and regional growers and producers.\nIf You Go...\nLocation: 5418 Murfreesboro Rd., Readyville, TN\nPhone: (615) 563-6455\nHours: 10 a.m. to 2:30 p.m. on Friday; 8 a.m. to 2:30 p.m. on Saturday; 10 a.m. to 2:30 p.m. on Sunday.\n“We go to Orlinda to get our corn and wheat. Then we mill the corn and wheat and use the meal in most of our recipes,” Ford says. “That’s one thing that’s so great about being located here. I don’t have to go looking for anything. Farmers come to us with their eggs, tomatoes, cucumbers, green beans, squash, blackberries – all these amazing things. They come, pull up their trucks and bring it right to my door. And I just love that we’re able to help each other out.”\nFord’s son, Eric, serves as chef and miller, having studied under Brady, and her daughter Danielle coordinates events. The restaurant doubles as a venue for weddings, church luncheons, bridal showers and other special occasions.\n“We’re more of a destination,” Ford says. “When we were on the square [in Murfreesboro], foot traffic brought people in. Now people come here on purpose. People enjoy observing the history here. [The mill] is very well-preserved for its age, and it’s been well taken care of through the years. People marvel at that.”', 'Underwater Inspection and Evaluation\n“A Critical Component” for Preventative Maintenance, Operation and Relicensing\nWilliam J. Castle, P.E.\nW.J. Castle, P.E. & Associates,\nP.C. 693 Main Street, Building B, Suite 1\nLumberton, New Jersey 08048 USA\nE. Levels of Inspection\nThere are three basic types or levels of inspection used while inspecting dam facilities:\nLevel I – General Visual\nThis type of inspection involves no cleaning of any structural elements and therefore is the most rapid of the three types of inspections. The purpose of the Level 1 inspection is to confirm as-built structural plans, provide initial input for an inspection strategy, and detect obvious major damage or deterioration due to overstress, impacts, severe corrosion, or extensive biological growth and attack.\nLevel II – Close-Up Visual\nThis level is directed toward detecting and identifying damaged/deteriorated area which may be hidden by surface biofouling or deterioration and towards obtaining a limited amount of deterioration measurements. The data obtained should be sufficient to enable the gross estimates of the facilities load capability. Level II examinations will often require cleaning of structural elements. Since cleaning is time-consuming, it is generally restricted to areas that are critical or which may be representative of the entire structure. The amount and thoroughness of cleaning to be performed is governed by the necessity to determine the general condition of the overall facility.\nLevel III – Highly Detailed\nThis level will often require the use of Non-destructive Testing (NDT) techniques. It may also require the use of partially destructive techniques such as core sampling into concrete and wood structures, physical material sampling, or surface hardness testing. The purpose of this type of evaluation is to detect hidden or interior damage, loss in cross-sectional area, and material. A Level III examination will usually require prior cleaning. The use of homogeneity NDT techniques are usually limited to key structural areas, areas that may be suspect, or to structural members which may be a representative of the underwater structure. Level III inspections will require considerably more experience and training than Level I or Level II inspections and should be accomplished by qualified engineering or nondestructive testing personnel. This type of inspection is classified as a specialized inspection within the United States. (U.S. Navy Standards)\nSpecial Testing, Level III Examination\nThe types of testing described in this section are among those which may also be used for Level III examinations. Level III examinations are employed when Level I and Level II examinations cannot conclusively determine the structural condition of the underwater items. Findings of previous inspections or the age of the structure may also dictate the level of examination needed. A Level III examination is not required for all inspections.\nIn steel structures, the inspector is often concerned with measuring the remaining thickness of corroded members. This can be done with a graduated scale, caliper and ultrasonic thickness measuring devices.\nGraduated Scales – For measuring the exposed edges of flanges, a rule, or graduated scale is the most basic tool.\nCalipers – Another simple method of thickness measuring is to use a set of calipers. Calipers are compact and easy to use under most conditions.\nUltrasonic Measuring Devices – Ultrasonic devices are also available for measuring remaining steel thickness. The device sends a sound wave through the member. It then measures the travel time of the sound wave and calculates the thickness of the steel. An advantage to this device is that it only needs a transducer to be placed on one side of the member.\nSeveral nondestructive tests can be performed on concrete; however, the nondestructive testing instruments must be modified for underwater use.\nV-Meter – The V-meter is an ultrasonic testing device. Using ultrasonic to check the condition of materials such as concrete requires two transducers. When taking measurements, the transducers can be arranged in three different positions. The direct transmission method provides the most accurate data. The semi-direct and indirect methods require correction factors to interpret the data. The V-meter measures the time it takes a sound wave to pass through a material. Location of discontinuities in the material, such as cracks and voids are determined by abnormal velocities. Data has to be interpreted by a trained technician or information may not be accurate. The Schmidt Hammer is a mechanical device which measures the compressive strength of in-place concrete. For underwater use, the hammer is placed within a housing and the equipment modified somewhat, including a special scale.\nCoring is a partially destructive test method. It can be used alone or to verify and correlate data from non-destructive test methods. Cores obtained underwater can be tested in a laboratory in accordance with standard procedures. The actual levels of inspection to be used for a particular task must be decided early in the planning phase. This cannot be overemphasized, because the time and effort required to carry out the three different levels of inspection are quite different. The time required will also depend on environmental factors such as visibility, currents, wave action, water depth, severity of marine growth, and the skill and experience of the inspector.\nInspector-divers usually move from dive site to dive site. Often divers must enter waters of unknown quality. Local water quality monitoring agencies should be contacted to determine the degree of hazard the water presents, and appropriate precautions must be taken. These may include additional immunizations and the employment of special diving equipment to provide complete diver encapsulation. In some cases, it may be necessary to obtain water samples and have the samples tested prior to diving. Diving managers must ensure that proper precautions are observed. The long term effects on the diver of these and other occupational hazards may result in future liabilities.\nF. Modes of Diving & Inspection Equipment\nWithin air diving, two principle modes are used: SCUBA, in which the diver carries his air supply with him in a tank, and surface-supplied diving in which the diver’s air source is in a boat or on shore. While some organizations may be predisposed to one mode over the other, both modes are permitted by OSHA standards and both have a place in dam inspections. In some situations, one mode may have significant economic benefits over the other, while providing all the inspection information required without in any way compromising safety. Their appropriateness for any specific diving situation depends on a number of factors including depth, bottom time, and the experience and capability of the diver. Each mode has unique operational advantages and disadvantages.\nScuba is an acronym for Self-Contained Underwater Breathing Apparatus. Scuba is generally recognized today in the open-circuit form: air is inhaled from supply tanks and the exhaust is vented directly to the surrounding water.\nSCUBA is well suited for inspection work because if its portability and ease of maneuverability in the water where there are many dives of short duration at different depths than one sustained dive. SCUBA equipment weighs about 75 pounds and requires no elaborate support operation. It has the advantage that the diver does not have to drag an air hose behind him. The use of SCUBA is limited by OSHA to depths of 130 feet and the bottom time is limited by the amount of air the diver can carry with him.\nSurface to diver communication is possible using SCUBA with either hardwire or wireless systems. Communication may be desirable in deep water or for complicated structures. Wireless systems are also available which allows for greater mobility.\nSurface Supplied Air\nThere are two types of surface-supplied equipment: deep-sea (hard-hat) and lightweight. Deep-sea equipment consists of a helmet and breastplate, diving suit and weighted shoes. The equipment worn by the diver alone can weigh more than 200 pounds. Add to this the air compressor, hoses, lines and possibly a diving launch to work from, and the problems of mobility and transportation become significant. This equipment is cumbersome, and generally not considered economical for most modem diving operations since the development of lightweight equipment. The deep-sea hard-hat has changed little in the last 150 years.\nLightweight equipment usually consists of a full face mask or helmet, safety harness, weight belt, boots or fins, back-up air supply, and an exposure suit (a wet or dry suit). Early helmets were free-flow air hats in which a constant stream of air was supplied to the diver. Today, demand regulators similar to those used in the second stage of SCUBA equipment have been incorporated in helmets and full face masks.\nIn a surface-supplied system, the air is supplied by a high volume, low-pressure compressor or from a bank of high-pressure cylinders equipped with a regulator to reduce the high pressure.\nThe diver’s umbilical (Life-Line) is a combination air hose, safety line, communication cable, and pneumofathometer hose.\nThe primary advantage of surface-supplied air diving is the “unlimited” air supply. Longer bottom times can be obtained on surface-supplied dives if decompression schedules are used. Dives in accordance with OSHA standards can be conducted to a depth of 190 feet or, if bottom times are less than thirty minutes, to depths of 220 feet. The major disadvantage of Surface Supplied diving is the lack of mobility. Inspection work generally requires the diver to constantly change depth or travel around structures or obstacles. In doing so, the diver using Surface Supplied equipment may become entangled in his umbilical. As a minimum, he has the added effort of dragging it after him.\nThis is part 2 of a 4-part series.\nYou can find part 1 here.\nRead part 3 here.\nRead the 4th of this 4-part series here.']	['<urn:uuid:e723659b-6cf7-43a1-84b2-332266e6d63c>', '<urn:uuid:92538add-1520-4508-875a-4a2f7a29a361>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T22:03:18.033296	14	69	2336
57	Which works better for blood sugar: prebiotics or probiotics?	Based on the available information, probiotics appear more effective for blood sugar control. Research shows that people following the DASH diet with probiotics achieved a 10.7% reduction in fasting blood sugar and 8.9% reduction in A1C, compared to much smaller reductions without probiotics. While prebiotics can help ensure normal blood sugar levels, the specific research demonstrating probiotics' effectiveness through the butyrate compound and improved insulin sensitivity makes probiotics the better choice for blood sugar management.	['You’ve no doubt heard the term “biotics” used quite a bit in advertising lately, as manufacturers tout the health benefits of various versions. Most of these ads are for prebiotic and probiotic products, but there are other kinds of biotics as well. Here’s some information that will answer the question, “What are biotics?” so that you have more of an idea of what you’re seeing on television.\nThe term “biotic” simply refers to various living things that are not only in the environment, but in our bodies as well. There are several types, and all of them play a role in creating an ecosystem.\nWhen it comes to the human body, biotics are bacteria. There are good bacteria and bad ones, and the bad ones get all the press, because they can cause so many potentially devastating health problems. But the beneficial bacteria are just as important, because they help to keep the harmful ones in check.\nDifferent Types of Biotics\nWhen you ask, “What are biotics?” the depth of the answer may be surprising. These are just some of the biotics – many of which you’ve heard of, but others that don’t come to mind as quickly.\n- Antibiotics – These are, as the name implies, substances that are “anti-biotic,” meaning they exist to kill biotic materials, namely bacteria. Just about everyone has taken an antibiotic at one time or another to kill some type of internal bug, and these medications have saved countless lives. But they not only kill harmful bacteria, they kill good ones as well. So, if you take them, you should seriously consider ways to replenish your supply of beneficial bacteria.\n- Prebiotics – There are certain foods that contain fibers the body can’t digest. These fibers are known as “prebiotics,” because they provide food for the bacteria within the digestive tract. We can’t digest those fibers, but our bacteria can.\n- Probiotics – Probiotics are substances designed to increase the number of good bacteria in the body. They are found in certain types of food, such as yogurt, sauerkraut, and many others, but they are also available in supplement form. These include capsules, powders, and drinks.\n- Macrobiotics – You’ve obviously heard of the first three entries on this list, but this one might leave you scratching your head a bit. Many people follow a macrobiotic diet that emphasizes locally grown vegetables, raw fruits, beans, and whole grains.\n- Synbiotics – These are simply products that contain prebiotics as well as probiotics.\nDifferentiating Between Prebiotics and Probiotics\nSince prebiotics and probiotics have gained so much notoriety in recent years, let’s focus on the differences between the two as well as some of the benefits they are purported to provide.\nAgain, prebiotics are the main fuel source for bacteria. Unlike probiotics, prebiotics are not actually living things. Rather, they stimulate probiotics, so they can do their thing in the digestive system. Beneficial bacteria are constantly battling with harmful ones, and need prebiotics so they can continue to fight. Research indicates that not only do prebiotics help good bacteria thrive, they also benefit overall health.\nProbiotics tend to be touted more for their health benefits (more on these in a bit), but studies show that prebiotics can provide substantial benefits as well. For example, research shows they can boost the immune system, support healthy metabolism, and also help the body properly absorb nutrients and utilize vitamins.They have also been shown to help ensure the body has normal levels of sugar in the blood.\nBut prebiotics affect everyone in a different fashion. One of the main factors to consider if you choose to take a prebiotic supplement is your overall intestinal health. Research indicates that if you have irritable bowel syndrome or Crohn’s disease, taking prebiotics in high doses may exacerbate your symptoms.You need to check with your doctor to make sure your digestive tract will be able to handle a prebiotic.\nProbiotics, on the other hand, consist of living microorganisms. They are intended to help achieve a balance between good and bad bacteria in the “gut,” or the gastrointestinal tract. A lot of people have an imbalance due to a combination of factors, such as stress, illness, processed foods, and even indoor and outdoor pollutants. Serious health issues can result when harmful bacteria outnumber the beneficial ones.\nThere has been a great deal of study devoted to just how beneficial probiotics can be for the body. They seem to not only support good digestive health, but can also have a positive impact for people of all ages, including newborns.Probiotics are thought to help reduce inflammation that can lead to intestinal issues, and they have even been associated with improved oral health.Many people who take antibiotics are also given probiotics to help replace beneficial bacteria.\nResearch also suggests that probiotics can help inhibit certain mechanisms within the body that can increase our sensitivity to allergens such as dust and different types of pollen.They can also help make sure we ,..\nHow Do You Get More Prebiotics and Probiotics in Your System?\nThere are a lot of natural sources of both prebiotics and probiotics, and you can easily find many of them on the shelves of your local grocery store. For example, prebiotics are found in foods such as beans, garlic, and oats, while you can find an ample supply of probiotics in pickled vegetables, as well as the aforementioned sauerkraut and yogurt. It’s very important, however, that you make sure any food item you’re purchasing for its probiotic benefits hasn’t been pasteurized. The reason is that pasteurization kills both good and bad bacteria, much like antibiotics.\nIf you want to go the easy route and still ensure you get enough prebiotics and probiotics, you can always purchase supplements. Capsules are extremely popular, but do a little research and make sure the ones you buy are of the highest quality. If they’re not, they may not survive the passage through your stomach to your intestines. Lower quality capsules dissolve in stomach acid, making it impossible for beneficial bacteria to get where they need to go.\nHopefully we’ve answered the question, “What are biotics?” Again, though, if you choose to take a prebiotic or probiotic supplement, talk to your doctor first to make sure you can do so safely.', 'Probiotics are live bacteria that stimulate the growth of helpful microorganisms, especially those in your gut.\nWhile we may recognize bacteria as a bad thing that can cause disease, it also occurs naturally in the human body. There are the “bad” bacteria we immediately think of, but there are also “good” bacteria.\nProbiotics contain these “good” bacteria. Lactobacillus and Bifidobacterium are the two most common probiotics.\nLactobacillus can be found in yogurt and other fermented foods like sauerkraut and kimchi. It is best known for helping ease diarrhea and assisting in the processing of lactose (the sugar found in milk.)\nBifidobacterium can also be found in some dairy products, and may help ease symptoms of various digestive issues, including Irritable Bowel Syndrome (IBS.)\nProbiotics help move food through the digestive system. When your gut is working at peak performance, you can expect not only improved digestion, but better odds of fighting off food-borne illness, even helping to heal diseases like IBS and Crohn’s.\nNot only will your digestive function benefit, but you will also experience a boost in immunity. An astounding 80% of the body’s immune system is located in the gut! Probiotics can help you fend off all sorts of illnesses, from the common cold to the flu.\nThe digestive system is also a huge part of the body’s neurological system. Chronic fatigue, joint pain, thyroid disease, even psoriasis are initiated in the gut. With improved gut health, some symptoms of these illnesses could be alleviated.\nAre you beginning to see how important gut health really is? Well, thanks to the Cambridge Cardiac Care Centre in Ontario, Canada, there is now another potential benefit to taking probiotics.\nNew research suggests adding probiotics to your health regimen may help to lower blood sugar.\nThe 80 participants in the study were following the DASH diet, a diet recommended for individuals with high blood pressure. About 15% of the participants also had prediabetes.\nAll participants had their blood pressure, hemoglobin A1C levels, and fasting blood sugar levels checked prior to beginning the study.\nSome of the participants followed the DASH diet only, while the others followed the DASH diet with the addition of probiotic-rich foods.\nAfter three months, both groups experienced similar drops in blood pressure. Therefore, the addition of probiotics did not appear to affect that aspect of the study. However, the probiotics did affect participants’ blood sugar levels.\nParticipants who followed the DASH diet only lowered their A1C by 3.4% compared to an 8.9% reduction in A1C for those who had added probiotics.\nThe probiotics also affected participants’ fasting blood sugar. The DASH diet only group lowered fasting blood sugar by 3.3% while the DASH plus probiotic group enjoyed a 10.7% reduction.\nHow can researchers explain these results?\nThere is a naturally-occurring compound in the gut called butyrate. Butyrate may positively impact insulin sensitivity. When insulin sensitivity is improved, the body does a better job of regulating blood sugar and is able to keep blood sugar levels lower.\nWhile this single study does not prove cause-and effect, it is a starting point for additional research.\nSo how do you find the right probiotic?\nWhile probiotics do naturally occur in several foods, some find it helpful to take a probiotic supplement.\nProbiotics are safe for most people, although individuals with immune disorders or other serious health conditions may need special consideration. You should always consult with your physician prior to starting new supplements or medications.\nWhen first starting probiotic supplements, you may experience temporary bloating, upset stomach, gas, or diarrhea. These symptoms should resolve within a few days as your digestive system regulates.\nMake sure the bottle label states “shelf stable” or “refrigeration not required.” If not, you will have to keep your probiotics in the refrigerator to prevent the bacteria from expiring.\nAlso check the label for potential allergens like dairy, wheat, soy, and gluten. Make sure you are getting a quality probiotic.\nHopefully, you will soon be on your way to a happier gut and a healthier body with the help of probiotics!']	['<urn:uuid:6c69ea2f-5486-4fdf-9007-a2a8849b52bb>', '<urn:uuid:1eea98cb-57e6-40bb-9cc1-46c1bfe831c1>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T22:03:18.033296	9	75	1724
58	formula calculate point straight line	A straight line satisfies the equation y = mx + b, where m is the slope and b is the y-intercept (the point where the line crosses the y-axis). Once you know the values of m and b, you can calculate any point on the line by plugging in the x- or y-value into that equation. For multiple ranges of x-values, the equation becomes y = m1x1 + m2x2 + ... + c	"['The LINEST funtion returns the slope and intercept values for a best fit straight line.\nThis function includes an option to include additional regression statistics (stats = true).\nThis function uses the ""least squares"" method to calculate a best fit line.\ny = mx + b\nStraight lines satisfy the equation ""y=mx+b"" when you have one independent x-value.\nb is the value where the line crosses the y-axis and corresponds to the ""const"" argument.\nWhen ""const"" is left blank (or True) the slope and intercept is calculated and returned.\nEnter the following data arranged in two columns. One column for the x-values and one for the y-values.\nHighlight cells ""B10:C10"".\nYou must select two cells in the same row. Not two cells in the same column.\nEnter the following formula as an Array Formula (using Ctrl + Shift + Enter) into the cells ""B10:C10"".\nThe slope of the best fit straight line has been added to cell ""B10"".\nThe intercept of the best fit straight line with the y-axis has been added to cell ""C10"".\nPlotting the Graph\nWe can check the results by plotting an XY Scatter chart.\nThe slope of the line is 2.\nThe intersection of the line with the y-axis is at -1.\nUsing INTERCEPT and SLOPE\nconst = False\nWhen ""const"" is FALSE the slope is calculated with an assumption that the Intercept = 0.\nThis function now only returns a single value and does not have to be entered as an array formula.\nThis is the only situation when this function does not return more than one value.\nWhen the intercept is zero, the y-values need to be recalculated using the formula ""y=mx""\nThis formula is completely different to ""y=mx+c"" and therefore whether ""const"" is true or false has enormous implications on the results returned by this function.\nstats = True\nWhen ""stats"" is TRUE this function will return additional regression statistics.\nInstead of 2 numbers, this function now returns 10 numbers arranged as two columns and five rows.\nThis table explains what the 10 numbers actually tell us:\nAlso known as Beta, written as ""m""\nAlso known as Alpha, written as ""c"" or ""b""\n|Standard Error for the slope coefficients||Standard Error for the intercept|\n#N/A when const=False\nThe coefficient of determination, written as r2\nStandard error for the y estimate\n|The F statistic|\nThis is the ratio of the variance in the data explained by the linear model divided by the variance unexplained by the linear model.\nAn even better test for a ""good fit"" is to use the Fisher F-statistic.\n|Degress of Freedom, written as df|\n|The Regression Sum of Squares||The residual Sum of Squares|\nThese additional statistics tell you how good the best fit line is.\nThe correlation coefficient gives a ""rough"" indicator of a good fit. Values close to 1 are good.\nThe uncertainties in the slope and intercept are a much better indicator for a ""good fit""\nIn this case the uncertainties in the slope and the intercept are not as good as the ""r2"" might have indicated\nDetermining new Values\nOnce you know the values of m and b, you can calculate any point on the line by plugging the y- or x-value into that equation. You can also use the TREND function.\nIn order to determine new x-values for a given set of data points you need to solve the following linear equation\nYou can use the LINEST and INDEX functions to solve this equation.\nThe residual value is the difference between the value predicted by the equation and the value observed or collected.\nIf the equation is a close fit then we would expect the residuals to be randomly scattered around zero.\nIf they display a pattern then it is likely that a better equation exists.\nIf there are more than one range of x-values then the straight line will satisfy a slightly different equation\ny = m1x1 + m2x2 + . . + c\nx1-values | x2-values | y-values\nPlotting a graph and then adding a trendline gives you slightly more control as you can provide the value for the intercept.\nYou can quickly enter the squared symbol by using (Alt + 0178).\nYou can quickly enter the cubed symbol by using (Alt + 0179).\n© 2021 Better Solutions Limited. All Rights Reserved. © 2021 Better Solutions Limited TopPrevNext']"	['<urn:uuid:97d150b1-937a-4cfe-96c7-21373c239887>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T22:03:18.033296	5	73	724
59	network reliability improvement solutions cost	Different solutions exist for improving network reliability at varying costs. The single-busbar scheme is the lowest-cost solution but offers limited availability. The double-busbar system provides more flexibility and reliability but requires additional equipment. The most expensive option is the double circuit-breaker scheme, typically used for very important feeders. For urban distribution networks, interconnected ring main networks offer a balance between reliability and efficiency, with additional options like capacitor addition for optimization during component failures.	['High voltage substations are points in the power system where power can be pooled from generating sources, distributed and transformed, and delivered to the load points. Substations are interconnected with each other, so that the power system becomes a meshed network. This increases reliability of the power supply system by providing alternate paths for flow of power to take care of any contingency (by selecting the most suitable circuit schemes), so that power delivery to the loads is maintained and the generators do not face any substation outage.\nThe high voltage substation is a critical component in the power system, and the reliability of the power system depends upon the substation. Therefore, the circuit configuration of the high voltage substation has to be selected carefully.\nBusbars are the part of the substation where all the power is concentrated from the incoming feeders, and distributed to the outgoing feeders. That means that the reliability of any high voltage substation depends on the reliability of the busbars present in the power system.\nThis leads to tripping of these lines, and the cascading effect goes on until there is a blackout or similar situation. The importance of busbar reliability should be kept in mind when taking a look at the different busbar systems that are prevalent.\n- Single-busbar scheme (1 BB)\n- Double-busbar scheme (2 BB)\n- Double circuit-breaker scheme (2 CB)\n- One-breaker-and-a-half scheme (1.5 CB)\n- 3-phase busbar scheme (3 BB)\nThe applications of this simple scheme are distribution and transformer substations, and feeding industrial areas (see figure 1). Because it has only one busbar and the minimum amount of equipment, this scheme is a low-cost solution that provides only limited availability.\nIn the event of a busbar failure and during maintenance periods, there will be an outage of the complete substation. To increase the reliability, a second busbar has to be added.\nThe more complex scheme of a double-busbar system gives much more flexibility and reliability during operation of the substation (see figure 2).\nIt is possible to control the power flow by using the busbars independently, and by switching a feeder from one busbar to the other. Because the busbar disconnectors are not able to break the rated current of the feeder, there will be a short disruption in power flow.\nTo have a load change without disruption, a second circuit- breaker per feeder has to be used. This is the most expensive way to solve this problem. In very important feeders, the 2 CB solution will be used (see figure 3).\nThe one-breaker-and-a-half is a compromise between the 2 BB and the 2 CB scheme. This scheme improves the reliability and flexibility because, even in case of loss of a complete busbar, there is no disruption in the power supply of the feeders (see figure 4).\nFor important substations at the nodes of transmission systems for higher voltage levels, the 3-phase busbar scheme is used. It is a common scheme in Germany, utilized at the 380 kV level (see figure 5).\nReference // Power Engineering Guide by SIEMENS (Download)', 'Interconnected ring main network\nDue to low voltage on the distribution side of the power system, high currents and power losses are experienced. The pressure on utilities to improve the overall efficiency in the power system has forced them to concentrate on reducing the power loss on the distribution level, since it contributes most power losses in the power system.\nResearchers have discussed and developed different algorithms to solve the power loss reduction problem in distribution networks. However, these methods were developed for radial network operations. However, most large urban distribution networks are operated as ring networks.\nFurthermore, although ring main networks are believed to have less power losses compared to radial networks, loss of an important component in a ring main network can cause an increase in power losses leading to power shortage in the network. To improve the state of a network is the task of optimal load flow studies.\nThis thesis focuses on developing an efficiency plan that can be used by network operators in response to a failure of a major component such as a substation infeed or a primary U/G cable. The developed efficiency plan is using the capacitor addition and network reconfiguration techniques to give the optimal configuration for power loss reduction.\nDevelopment of the efficiency plan work flow diagram\nFigure 1 shows the one-line diagram of the large urban interconnected ring main distribution network that was used to evaluate and develop the efficiency plan (methodology).\nSuch a network is typical in industry and consists of multiple ring main networks.\nThe network shows two interconnected ring main networks namely Ring main Network 1 (RM1) and Ring main Network 2 (RM2). Both ring main networks, RM1 and RM2, have a voltage source of 66∠0° kV and a frequency of 50 Hz. The two networks are interconnected via U/G Cables 13, 14 and 15.\nRM1 has a step down transformer rated at 7 MVA, 66/11 kV with two primary U/G Cables, 1A and 2A. U/G Cables, 3A, 4A, 5A, 6A, 7A and 8A complete the ring main system of RM1.\nRM2 has a source step down transformer rated at 10 MVA, 66/11kV with three primary U/G Cables 1B, 2B and 3B and secondary U/G Cables 4B, 5B, 6B, 7B, 8B, 9B, 10B, 11B and 12B. The load ratings, transformer ratings and U/G cable ratings for both networks are shown in Tables A.1, A.2 and A.3, in Appendix A.\nThe studies will mainly be focused on the RM1 network. The RM2 network will however be considered in the case of network configuration when extra power is needed at RM1.\nEfficiency plan Methodology\nFigure 2 shows an efficiency plan work-flow diagram. The work-flow diagram depicts an empirical approach for finding the optimal configuration that will minimise the power losses in a ring main network that is under a contingency condition, while satisfying the given constraints.\nThe network was analyzed using DIgSILENT powerfactory and Power Systems Computer Aided Design (PSCAD) software to obtain steady-state and time domain results, respectively. The parameter models for DIgSILENT and PSCAD are shown in Appendices A and B respectively.\nThe load flow procedures of the proposed method are mainly composed of power loss and efficiency calculations, busbar voltages and cable/feeder loading determination and empirical approach application.\n|Title:||Power loss reduction in large urban ring main networks (optimisation, capacitor addition) – Julia Dhimbulukweni Iindombo – Thesis submitted in fulfilment of the requirements for the degree Master of Technology: Electrical Engineering in the Faculty of Engineering at the Cape Peninsula University of Technology|\n|Download:||Right here | Video Courses | Membership | Download Updates|']	['<urn:uuid:bc0eadee-1c1a-4d19-96b0-a1b0981380fd>', '<urn:uuid:d3e78d77-38cd-44cf-a213-f4775a8d0d64>']	factoid	with-premise	short-search-query	similar-to-document	three-doc	novice	2025-05-12T22:03:18.033296	5	74	1107
60	As someone working in both fields, I'm curious - do crime scene photographers and dental hygienists require similar educational credentials for professional licensing?	No, crime scene photographers and dental hygienists have different educational requirements for licensing. Crime scene photographers are recommended to have a degree in criminal justice, though they typically start by joining a police force and working their way onto a crime scene team. In contrast, dental hygienists must have graduated from a Commission on Dental Accreditation (CDA) accredited dental hygiene program, typically earning an Associate Degree. They must also pass the National Board Dental Hygiene Exam and state licensing examinations to become a Registered Dental Hygienist (RDH).	"['Crime Scene Photography Jobs\nWhen crimes occur, there is always an investigation. Investigations can last minutes or years, but a crime scene has to be cleaned up relatively quickly. The only way to freeze time at the crime scene is with the help of a crime scene photographer.\nCrime scene photographers, also known as forensic photographers, take photos of crime and accident scenes for investigations and as court evidence. Photographers visit every crime and use a high quality camera to capture the scene.\nCrime scene photographers are some of the first people to arrive at the scene of an incident. They are professional members of law enforcement who take accurate, detailed photographs of absolutely anything at any hour.\nBefore any scene can be cleaned up, photos must be taken to create a visual record of the events. It is easy to overlook things in the heat of the moment. Photos can lead to clues and evidence that are only captured on film or digital images.\nIt’s the photographer’s job to freeze the moment in time. They have to take precise images of the entire scene -victims, vehicles, wounds, weapons, skid marks, bodies. Nobody knows what details may help solve the crime. They will take photos of the pavement, grass, road, weather, and overall surroundings. A trained photographer will have photos from all angles, perspectives, viewpoints and distances, even in the most bloody and gruesome scenes. No details can be overlooked.\nA crime scene photographer may snap hundreds of photos at a crime scene. Taking quality photos and using SLR cameras must be second nature to any crime scene photographer. Usually a crime scene photographer will carry a lots of equipment – several camera bodies, measuring devices, light meters, flashes, rulers, bags, tripods, and wide, mid and zoom lens.\nEvery photographer has a vast knowledge of photography – focal points, exposures, depth of field, and sharpness. They are all important elements to master in order to consistently produce clear, focused, colored photos. Every photo must have a ruler or some way to establish scale for the image to be of any value.\nCrime scene photographers develop or download their own photos. But their photos can never be altered. No cropping, enhancing, or Photoshopping is allowed. It’s a good idea to jot notes at the crime scene to remember what photos are what.\nCrime scene photographers work as part of the crime scene investigation team, just like on CBS’s hit show CSI. They must be law-abiding citizens, have a clean criminal record, and be skilled in photography. Many obtain degrees in criminology, criminal justice, forensic science, or photography. Usually they start by joining a police force and work their way on to a crime scene team.\nOnce on the team, they travel to crime scenes and take photos. Often they will notice things in the photos that will help solve the crime – a bit of blood on a car window, a silhouette in a distant window, or the hilt of a knife in the bushes. Photos are used as evidence in court and are often the first tangible evidence a jury will see.\nCrime happens at all hours of the day in small towns and big cities. Crime scene photographers earn an average of $29,130 per year. The top 10 percent of photographers make up to $63,400 per year (Bureau of Labor Statistics). The exact figure will vary, depending on location and whether the photographer is working on a full-time or freelance basis.\nIf you enjoy photography and want to put criminals behind bars, then consider a career as a crime scene photographer. Maybe you will take the photos that get the bad guys off the street.\nQuick Facts About Crime Scene Photography Work\nJob Title: Crime Scene Photographer\nOffice: At crime or accident scenes with camera\nDescription: Take photos of crime scenes for investigations and court evidence\nCertifications/Education: Degree in criminal justice recommended\nNecessary Skills: Knowledge of crime scenes, Attention to detail, Good with camera\nPotential Employers: Police Departments, Law Enforcement Agencies\nPay: Average $29,130 per year\nHelpful Crime Scene Photographer Employment Links:\nSearch Crime Scene Photography and Other Photographer Jobs on JobMonkey\nCrime Scene and Evidence Photography\nInternational Association for Identification\nEvidence Photographers International Council\nJobs in Law Enforcement', ""How to Enroll In a Dental Hygienist School near North Charleston South Carolina\nChoosing the ideal dental hygienist program near North Charleston SC is an essential first step toward starting your new career in dentistry. But before you can make your choice, you need to examine and compare your school options. There is much more to doing your due diligence than choosing the school with the most affordable tuition or enrolling in the school that is closest to your home. There are other significant issues to consider as well, such as the college’s reputation and accreditation. Dental hygienists usually earn an Associate Degree, as compared to a certificate usually earned by assistants, and can take anywhere from two to three years to finish. Naturally with the lengthier training of a hygienist comes more expense. We will talk about all of these issues and supplemental questions that you should be asking the dental hygienist schools you are assessing later in this article. But first, let’s explore the roles of dental hygienists and the training programs available.\nThe Duties of Dental Hygienists\nWhen comparing the duties of a dental assistant to that of a hygienist, the main difference is probably that the hygienist works more on their own. Dental assistants work with and assists the North Charleston SC dentists and the practice. Hygienists, while also assisting the practice, deal with the patients more on a one-to-one basis. They are usually the initial person a patient encounters when called from the waiting area. They examine every patient’s gums and teeth and report their results to the dentists. They may also carry out basic procedures. Depending on state law, a hygienist’s responsibilities can include:\n- Removing tartar, stains and plaque\n- Administering fluoride treatments\n- Polishing teeth and applying sealants\n- Educating patients about oral hygiene\n- Taking and developing X-rays\n- Removing sutures and applying fillings\nIn order to be licensed in nearly all states, dental hygienists must have graduated from a Commission on Dental Accreditation (CDA) accredited dental hygiene program. They also must pass the National Board Dental Hygiene Exam in addition to passing any state licensing examinations. When they have completed these requirements they are considered fully licensed and can add the “RDH” designation to their names, standing for Registered Dental Hygienist.\nDental Hygienist Education Options\nBecause of the increased responsibility as compared to an assistant, dental hygienists working in North Charleston SC dental practices are normally required to hold an Associate Degree in dental hygiene rather than a certificate. These programs can require anywhere from 2 to as long as 3 years to finish and must be accredited by the CDA in almost every state. They are offered in trade and technical schools as well as community colleges. And in addition to classroom studies learning the fundamentals of dental hygiene, there will be a practical aspect to the training as well| Many programs also offer internships with local dental practices or dentists.\nDental Hygienist Online Programs\nEnrolling in an online dental hygienist program may be a good option for getting your education. Just keep in mind that the classes will not be totally online, since there will be a practical component to your training. But the remainder of your classes will be available by means of your desktop computer in the comfort of your North Charleston SC home or anywhere else on your laptop or tablet. For those working while going to college, online dental classes make education far more accessible. Some may even have lower tuition costs than their traditional competitors. And supplementary expenses for items like commuting, books and school supplies may be lessened as well. The clinical training can usually be performed at a community dental practice or in an on-campus lab. With both the clinical and online training, everything needed to obtain the proper education is provided. If you have the discipline for this mode of education, you might find that enrolling in an dental hygienist online college is the ideal option for you.\nPoints to Ask Dental Hygienist Colleges\nNow that you have decided to become a dental hygienist in North Charleston SC, you can start the procedure of comparing programs and schools. As we discussed at the opening of this article, many students begin by checking out the cost and the location of the schools. Perhaps they search for several online options also. Although these may be important initial points to consider, there are several additional questions that you need to address to the colleges you are looking at in order to reach an informed decision. Toward that end, we have included a list of questions to help you with your due diligence and ultimate selection of the best dental hygienist school for you.\nIs the Dental College Accredited? There are many important reasons why you should only pick an accredited dental hygienist school. If you are intending to become certified or licensed, then accreditation is a prerequisite in nearly all states. To qualify to take the National Board Dental Hygiene Examination, your dental program must be accredited by the Commission on Dental Accreditation (CDA). Accreditation also helps establish that the education you get is comprehensive and of the highest quality. North Charleston SC employers often prefer or require that job applicants are graduates of accredited colleges. And finally, if you are applying for a student loan or financial aid, often they are not provided for non-accredited programs.\nIs Plenty of Practical Training Provided? Practical or clinical training is a necessary component of any dental training program. This applies for the online school options also. Most dental hygienist colleges have associations with regional dental practices and clinics that provide practical training for their students. It’s not only imperative that the college you enroll in offers adequate clinical hours but also provides them in the kind of practice that you subsequently want to work in. As an example, if you have an interest in a career in pediatric dentistry, verify that the college you enroll in offers clinical rotation in a local North Charleston SC dental office that focuses on dental care for children.\nAre Internships Available? Find out if the dental schools you are evaluating have internship programs. Internships are probably the ideal way to receive hands-on, practical experience in a professional dental practice. They help students to transition from the theoretical to the practical. They can also help students develop working relationships in the professional dental community. And they look good on resumes as well.\nIs Job Placement Help Provided? Many students that have graduated from dental hygienist colleges require assistance landing their first job. Find out if the colleges you are considering have job assistance programs, and what their job placement rates are. Colleges with high job placement rates probably have excellent reputations within the North Charleston SC dental profession in addition to broad networks of contacts where they can position their students for employment or internships.\nAre the Classrooms Smaller? Check with the colleges you are reviewing how large on average their classes are. The smaller classes usually provide a more personal atmosphere for learning where students have increased access to the instructors. On the other hand, bigger classes often are impersonal and provide little individualized instruction. If feasible, ask if you can sit in on a couple of classes at the North Charleston SC dental hygienist school that you are leaning toward so that you can experience first hand the degree of interaction between teachers and students before making a commitment.\nWhat is the Total Cost of the Program? Dental hygiene colleges can differ in cost depending on the length of the program and the volume of clinical training provided. Other variables, for example the reputations of the schools and whether they are public or private also come into play. But besides the tuition there are other significant expenses which can add up. They can include costs for such things as textbooks and commuting as well as school materials, equipment and supplies. So when examining the cost of schools, don’t forget to include all of the costs related to your education. The majority of schools have financial assistance offices, so make sure to check out what is offered as far as grants, loans and scholarships in the North Charleston SC area.\nAre the Classes Convenient? Before enrolling in a dental hygienist school, you must confirm that the assistant or hygienist program furnishes classes that suit your schedule. This is especially true if you continue working while getting your education and have to go to classes near North Charleston SC in the evenings or on weekends. And even if you select an online school, you will still have to schedule your practical training classes. Also, while making your inquiries, ask what the make-up protocol is if you should have to miss any classes because of work, illness or family responsibilities.\nAttending Dental Hygienist School near North Charleston SC?\nNorth Charleston, South Carolina\nNorth Charleston is the third-largest city in the U.S. state of South Carolina, with incorporated areas in Berkeley, Charleston, and Dorchester counties. On June 12, 1972, the city of North Charleston was incorporated and was rated as the ninth-largest city in South Carolina. As of the 2010 Census, North Charleston had a population of 97,471, growing to an estimated population of 108,304 in 2015, and with a current area of more than 76.6 square miles (198.5 km2). As defined by the U.S. Office of Management and Budget, for use by the U.S. Census Bureau and other U.S. Government agencies for statistical purposes only, North Charleston is included within the Charleston–North Charleston–Summerville metropolitan area and the Charleston-North Charleston urban area. North Charleston is one of the state's major industrial centers and is the state's top city in gross retail sales.\nThe large plantations were subdivided into smaller farms in the late 19th century as the urban population began moving northward. Due to the large labor forces of enslaved African Americans who worked these properties for over two centuries, the population of Charleston County in 1870 was 73 percent black; however, they were freedmen by this time. After the Civil War, phosphate fertilizer plants were developed, with extensive strip mining occurring between the Ashley River and Broad Path (Meeting Street Road). The main route for transportation of these phosphates eventually became known as Ashley Phosphate Road.\nSince the early 20th century, the section of unincorporated Charleston County that later became the city of North Charleston had been designated by Charleston business and community leaders as a place for development of industry, military and other business sites. The first industry started in this area was the E.P. Burton Lumber Company. In 1901, the Charleston Naval Shipyard was established with agreements between the federal government and local Charleston city leaders. Shortly thereafter, the General Asbestos and Rubber Company (GARCO) built the world's largest asbestos mill under one roof.\nIn 1912, a group of businessmen from the city of Charleston formed a development company that bought the E.P. Burton Lumber Company tract and began to lay out an area for further development. The Park Circle area was one of the first to be designed and developed, allocating sections for industrial, commercial, and residential usage. Park Circle was planned as one of only two English Garden Style communities in the US, and most of the original planning concept remains today. Some of the streets in the area still bear the names of these original developers: Durant, Buist, Mixon, Hyde, and O'Hear. During World War II, substantial development occurred as the military bases and industries expanded, increasing the personnel assigned there. New residents moved to the region to be closer to their work.\nSelect the Ideal Dental Hygienist College near North Charleston SC\nEnrolling in the ideal dental hygienist program is important if you intend to take the National Board Dental Hygiene exam or, if required in your state, become licensed. As you now know, there are numerous options available to acquire your training and it takes a fairly short period of time to become a dental hygienist. You can obtain your formal training through dental programs at junior colleges, vocational schools, technical institutes and trade schools. Graduates of these programs typically obtain an Associate Degree. Dental Hygienists usually require about two years of studies prior to entering the job market. When obtaining a degree you can choose to go to classes online or on-campus. Whichever mode of training you choose to pursue, by asking the questions presented in this article you will be in a better position to make the ideal selection. And by doing so, you will be ready to start your journey toward becoming a dental hygienist in North Charleston SC.\nMore Great Cities in South Carolina\nBusiness Results 1 - 10 of 67\nBing: South Carolina Dental Hygienist Schools Search results\nDental Lab Technician Schools & Programs - MTS\nAccording to the Bureau of Labor Statistics (BLS 2021), 30,800 people were employed as dental lab technicians in the U.S. as of 2020. While working on fabricated dental materials is a critical part of the career, it also requires a variety of other skills such as good time management, manual dexterity, and an ability to discern various shades of color.\nDental Hygiene Training | Fortis\nFortis offers dental hygienist training programs in our schools in Pennsylvania, Maryland, Utah, Alabama and Georgia. Dental Hygienist Careers. Valued members of the healthcare team, dental hygienists are licensed oral healthcare professionals. These respected members of the dental team provide preventative, educational and therapeutic services.\nCytotechnologist Schools - Cytogenetic Technologist Programs\nWestern Association of Schools and Colleges (WASC) Career Outlook According to the Bureau of Labor Statistics (BLS 2021), job opportunities in cytotechnology (categorized as medical and laboratory technology) are expected to see 11 percent growth between 2020 and 2030, which is higher than the average increase projected for all occupations (11 ...\nDental License Lookup - Health Guide USA\nDental License Lookup Licensing requirements for dentists , dental hygienists and dental assistants vary from State to State. To facilitate the verification of the license status of practitioners, most States provide Dental License Lookup web tools as detailed below.\nDental Hygiene | University of Michigan School of Dentistry\nDental Hygiene Undergraduate Program (BS) 2-year full-time Dental Hygiene Bachelor of Science with a broad-based education in the social, behavioral, and biological sciences, preparing students for the practice of contemporary dental hygiene and for leadership roles in oral healthcare of the future.\nGlobalShiksha.com - Buy CBSE|ICSE Educational CD|DVD|Pendrive|SD card ...\n2000 | CUSTOMER CARE : +91 8970308999, [email protected] Login; Register; 0. Kid's assessment; Printed Worksheets; Animated Videos\nA dental hygienist shortage has dentist offices struggling to schedule ...\nMorrissey was a co-author of a study last year that estimated 8% of dental hygienists left the workforce in 2020. A year later, some had returned. But hygienist Sarah Crow, who's the president of the Massachusetts Dental Hygienist Association, says many of her older colleagues aren't coming back.""]"	['<urn:uuid:6973fee3-de9f-4eef-82f9-899caeb283af>', '<urn:uuid:52065b4a-164d-4db6-938b-8af1a204d4b6>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T22:03:18.033296	23	87	3207
61	what is special about floor plate dopaminergic neurons	Floor plate-derived midbrain dopaminergic neurons are special because they follow the natural developmental pathway of the brain and are the only ones shown to be engraftable in animals. This is different from other commercially available iPSC-derived dopaminergic neurons which are derived from neural stem cells. These neurons are particularly relevant for studying disorders like Parkinson's disease and schizophrenia.	['Cells Provide a More Relevant In Vitro Model for Neurological Disorders, Including Parkinson’s Disease and Schizophrenia\nMADISON, Wis., April 30, 2014 (GLOBE NEWSWIRE) — Cellular Dynamics International, Inc. (CDI) (Nasdaq:ICEL) today announced the launch of iCell® DopaNeurons, the company’s neural floor plate-derived midbrain dopaminergic neurons generated from human induced pluripotent stem cells (iPSCs).\n- Dopaminergic neurons, specifically those located in the floor plate-derived midbrain, are implicated in neurological disorders such as Parkinson’s disease and schizophrenia; thus iCell DopaNeurons provide a highly relevant in vitro model to investigate these types of pathologies.\n- Midbrain dopaminergic neurons develop from an early developmental stage of the brain called the floor plate. Other commercially available iPSC-derived dopaminergic neurons do not follow this developmental path, but instead are derived from neural stem cells. Only midbrain dopaminergic neurons have been shown to be engraftable in animals (1), indicating the promise for the development of cell-based therapies.\n- In addition to iCell DopaNeurons, CDI’s MyCell® process can be applied to develop and manufacture cells from donors with dopaminergic neuron-based diseases.\n- CDI exclusively licensed the technology, invented by Lorenz Studer, M.D., and his laboratory team at Memorial Sloan Kettering Cancer Center, to create floor-plate-derived midbrain dopaminergic neurons for use in research.\nLorenz Studer, M.D., director, Center for Stem Cell Biology at Memorial Sloan Kettering Cancer Center, said, “We are pleased that Cellular Dynamics has licensed our technology to create dopaminergic neurons for research use that are specific to the substantia nigra, the small area of the brain implicated in Parkinson’s disease. Research in our laboratory has shown that iPSC-derived dopaminergic neurons that follow the midbrain dopaminergic developmental pathway are able to engraft in animal models for Parkinson’s disease. We’re excited that CDI will make these cells widely available for research purposes, enabling researchers to better understand dopaminergic neuron-based diseases and work toward possible patient therapies.”\nChris Parker, chief commercial officer of CDI, said, “Despite decades of effort and resources applied by pharmaceutical companies to treat diseases such as Parkinson’s, there are no cures available. iCell DopaNeurons now offer a human in vitro system that we believe more closely reflects human biology than other model systems available today. CDI’s manufacturing capabilities enable these specialized neurons to be developed at a high purity and quantity with rigorous quality control so that researchers studying dopaminergic diseases have a stable supply source of a relevant in vitro model.”\n(1) Nature. 2011 Nov 6;480(7378):547-51. doi: 10.1038/nature10648.\nAbout Cellular Dynamics International, Inc.\nCellular Dynamics International, Inc. (CDI) is a leading developer and manufacturer of fully functioning human cells in industrial quantities to precise specifications. CDI’s proprietary iCell Operating System (iCell O/S) includes true human cells in multiple cell types (iCell products), human induced pluripotent stem cells (iPSCs) and custom iPSCs and iCell products (MyCell Products). CDI’s iCell O/S products provide standardized, easy-to-use, cost-effective access to the human cell, the smallest fully functioning operating unit of human biology. Customers use our iCell O/S products, among other purposes, for drug discovery and screening; to test the safety and efficacy of their small molecule and biologic drug candidates; for stem cell banking; and in the research and development of cellular therapeutics. CDI was founded in 2004 by Dr. James Thomson, a pioneer in human pluripotent stem cell research at the University of Wisconsin-Madison. CDI’s facilities are located in Madison, Wisconsin, with a second facility in Novato, California. See www.cellulardynamics.com.\nTo the extent that statements contained in this press release are not descriptions of historical facts regarding Cellular Dynamics International, Inc., including statements regarding our products, they are forward-looking statements reflecting the current beliefs and expectations of management made pursuant to the safe harbor provisions of the Private Securities Litigation Reform Act of 1995. Words such as “may,” “will,” “believe,” “expect,” “anticipate,” “estimate,” “intend,” and similar expressions (as well as other words or expressions referencing future events, conditions or circumstances) are intended to identify forward-looking statements. Forward-looking statements in this release involve substantial risks and uncertainties that could cause our product development efforts, actual results, performance or achievements to differ materially from those expressed or implied by the forward-looking statements. Cellular Dynamics undertakes no obligation to update or revise any forward-looking statements. For a further description of the risks and uncertainties that could cause actual results to differ from those expressed in these forward-looking statements, as well as risks relating to the business of the Company in general, see Cellular Dynamic’s annual report on Form 10-K/A filed with the Securities and Exchange Commission on March 11, 2013, which risks are incorporated herein by reference, and as may be described from time to time in Cellular Dynamics’ subsequent SEC filings.\nSenior Director, Marketing & Communications\nCellular Dynamics International, Inc.\nRobert E. Flamm, Ph.D.\nRusso Partners LLC\nGitanjali Jain Ogawa\nThe Trout Group, LLC\nSource: Cellular Dynamics International, Inc.\nNews Provided by Acquire Media']	['<urn:uuid:7f532b10-623d-4790-a0cf-e0191eb193c5>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T22:03:18.033296	8	58	805
62	crispr delivery methods efficiency plants animals	For plant genome editing, researchers developed HI Edit which uses pollen as a natural DNA delivery vehicle, successfully tested in corn, Arabidopsis and wheat. For animal applications, scientists developed CRISPR-Gold using gold nanoparticles to encapsulate and deliver CRISPR machinery to cells, demonstrating success in treating muscular dystrophy in mice.	['Pollination is a natural way to deliver DNA into plant cells. So why not to use pollen as a vehicle for CRISPR machinery to start genome editing? HI Edit, as this approach is called, has been successfully tested by Syngenta in corn, Arabidopsis and wheat in the lab. Please see the paper just published in Nature Biotechnology by Timothy Kelliher et al., Jon Cohen’s piece in Science, and a quick guide to HI (haploid induction) from Current Biology. Leading wheat geneticist Cristobal Uauy of the John Innes Centre also showed high spirit, when asked for comment by email: “The possibility to genome edit any variety is revolutionary given that transformation is so difficult in many species. If I understand this correctly this would be a game changer as it would allow us to alter genes in elite cultivars.”\nResearchers from the International Institute of Tropical Agriculture in Nairobi, Kenya, are using CRISPR to inactivate the banana streak virus DNA in the genome of plantain. Their strategy, reported in Communications Biology today, paves the way for improving banana breeding and getting better varieties of this staple food crop. This project is a welcome addition to the list of CRISPR applications being considered for developing countries’ agriculture, such as maize varieties resistant to lethal necrosis and cassava resistant to brown streak disease.\nChili peppers have happily entered our kitchens with their capsaicinoid content, since Cristoforo Colombo brought then back from Central America. Capsicum species however are labour-intensive and difficult to grow. They are also notoriously recalcitrant to biotechnological intervention. Tomatoes are much handier in comparison. The Capsicum and Solanum clades split at least 19 Mya ago but comparative genomics has revealed that tomatoes retain all the necessary genes for pungency. Why not to harness CRISPR power to turn tomatoes into capsaicinoid biofactories then? Continue reading\nIt’s never too late to learn how to rewrite a genome. So here I am, attending this CRISPR school. Forget the do-it-yourself kits sold over the internet. I am lucky enough to take the first practical course on genome editing organized by the Italian Society of Agricultural Genetics (Siga) in Grugliasco, at the Department of Agricultural, Forestry and Food Sciences of the University of Turin. After writing a lot about CRISPR, it’s time to try the real thing. Continue reading\n“To make a seed it takes a fruit,” pupils use to sing in Italy. Then students learn that there is an embryo inside seeds and it takes a pollen fertilized egg to make it. The dream of plant scientists, however, has always been to be able to produce seeds using only the cell egg. This dream has finally come true: a group led by Venkatesan Sundaresan, at UC Davis, has developed a rice variety capable of cloning its seed. Continue reading\nWhen toasting during Christmas holidays, perhaps with a glass of Italian sparkling prosecco, think about it: viticulture in Europe occupies 3% of the cultivated area, but it accounts for 65% of all fungicides employed in agriculture. The adoption of new wine grape varieties resistant to powdery and downy mildew could significantly cut chemical use. If fairly regulated, advanced biotech tools such as CRISPR could help sustainability without losing anything of the genetic identity of iconic varieties. Continue reading\nJust imagine you could find them all on the supermarket shelves, would you buy rice labeled as CRISPR or GMO, or stick to conventional non-genetically modified rice? And what price would you consider fair? Aaron Shew and colleagues from the University of Arkansas conducted a multi-country assessment of willingness-to-pay for and willingness-to-consume a hypothetical CRISPR-produced food and published their findings in Global Food Security. Continue reading', 'Gene-Editing Successfully Cures Mice Genetic Disorder\nHarrington L.B. et. al. (2017) Nature Communications 8:1424. https://www.ncbi.nlm.nih.gov/pubmed/29127284\nThe CRISPR/Cas systems used for genome editing to date have come from mesophilic bacteria, preferring temperatures of 20-45°C, preventing their use at higher temperatures. Harrington et. al. have identified a Cas9 protein from the thermophilic bacterium Geobacillus stearothermophilus (GeoCas9) that is active in temperature up to 70°C, providing a much wider range of possibilities. Additionally, GeoCas9 showed greater stability as an RNP complex in human plasma, opening the door to possible therapeutic uses.\nGray, B.N and Spruill, W.M. (2017) Nature Biotechnology 35:630-633. https://www.ncbi.nlm.nih.gov/pubmed/28700549\nThe ongoing patent battle between the Broad Institute and the University of California-Berkeley provides difficulties for researchers and companies wishing to develop CRISPR/Cas technology, though this is not the only barrier. This article describes the broad claims that have been granted or that are being investigated and presents the argument that these claims are overly broad and could limit the genome editing field.\nGEN News Highlights, 30 October 2017, https://www.genengnews.com/gen-news-highlights/crispr-drives-out-fungal-resistance/81255106\nGene drives have been described as a way to eliminate pests, notably the mosquito, from the environment. However, they are a powerful research tool as well. Candida albicans can be notoriously difficult to study due to its diploid nature. By combining a newly discovered haploid C. albicans and CRISPR/Cas gene drive technology, researchers have been able to rapidly create diploid knockouts for study (https://www.ncbi.nlm.nih.gov/pubmed/29062088). The creation of these mutants could serve to increase the pace of drug discovery to combat this and other fungal pathogens.\nDavid Ruth, 17 October 2017, PHYS.org, https://phys.org/news/2017-10-genome-efficient.html\nResearchers at Rice University have used computational models to predict the speed at which the CRISPR/Cas system identifies and cleaves the targeted location. The research, published in the Biophysical Journal (https://www.ncbi.nlm.nih.gov/pubmed/28978436), determined that by allowing CRISPR to cut at off-target sites the system could quickly find and cleave the targeted site. By limiting the system’s ability to cleave off-target sites, the dissociation of Cas9 from DNA greatly decreased the speed at which on-target sites were identified.\nSophia Ktori, GEN News Highlights, 04 October 2017, http://www.genengnews.com/gen-news-highlights/crispr-nanoparticles-repair-duchenne-muscular-dystrophy-gene/81255009\nScientists working on CRISPR delivery have developed a gold nanoparticle that encapsulates the CRISPR/Cas machinery for delivery to cells. This new technique, coined CRISPR-Gold, was published in Nature Biomedical Engineering (https://www.nature.com/articles/s41551-017-0137-2). In the paper the authors demonstrated CRISPR-Gold’s ability to correct the mutated dystrophin gene in a mouse model, with mice receiving CRISPR-Gold treatment displaying two-fold improvement in hanging time in a four-limb hanging test, compared to control mice.\nDavid Cyranoski, 02 October 2017, Nature News, http://www.nature.com/news/chinese-scientists-fix-genetic-disorder-in-cloned-human-embryos-1.22694\nA new report in Protein and Cell (https://www.ncbi.nlm.nih.gov/pubmed/28942539) is the latest in a string of human embryo CRISPR publications. Using a modified CRISPR/Cas9 system tethered to a second enzyme that can swap individual DNA bases, the researchers targeted an A to G point mutation that results in β-thalassemia. Eight of the 20 cloned embryos contained a corrected copy of the gene, possibly curing the recessive disorder. The scientists were careful to point out that not all cells in the embryo were modified, which could have unintended consequences.\nChen et. al. (2017) Nature. https://www.ncbi.nlm.nih.gov/pubmed/28931002\nResearchers using FRET to study previously engineered high-fidelity Cas9 (SpCas9-HF1) and enhanced Cas9 (eSpCas9), identified that these versions are trapped in an inactive state when bound to off-target sites. Using this observation and rational protein engineering, the researchers made additional modifications to the REC3 domain to prevent activation of the HNH nuclease domain unless the guide RNA and target DNA match is very close. This new Cas9, coined Hyper Cas9 (HypaCas9) maintains the native Cas9 on target efficiency, but decreases the number of off-target events.\nSharifnia, T., et. al. (2017) Cell Chemical Biology. 24:1075-1091. https://www.ncbi.nlm.nih.gov/pubmed/28938087\nRare cancers have traditionally been difficult to study due to low incidence and limited sample availability. However, new technologies, such as sequencing, have allowed for a greater understanding of the underlying genetic causes. In tandem with sequencing technologies, CRISPR/Cas and small molecule screens have allowed researchers to rapidly screen rare cancers for possible mechanisms and treatments.\nRachael Lallensack, Nature News, 18 September 2017, http://www.nature.com/news/crispr-reveals-genetic-master-switches-behind-butterfly-wing-patterns-1.22628\nTwo new studies in the Proceedings of the National Academy of Sciences (http://www.pnas.org/content/early/2017/08/29/1709058114, http://www.pnas.org/content/early/2017/08/29/1708149114) provide insight into butterfly wing color. The studies identified two genes, WntA is responsible for creation of the coloring pattern and borders, while optix fills the color within the borders. Understanding butterfly coloration could provide insights into adaptations such as mimicry.\nVella, M.R. et. al. (2017) Scientific Reports 7:11038. https://www.ncbi.nlm.nih.gov/pubmed/28887462\nCRISPR/Cas gene drives could be used to eliminate vector-borne diseases such as malaria and Lyme disease. However, release of modified organisms is controversial in part due to unforeseen consequences. Developing strategies for gene drive reversal could prove useful if such problems arise. This paper develops models to evaluate the effectiveness of gene drive counter-measures in order to evaluate their potential use.\nGene-Editing Successfully Cures Mice Genetic Disorder, unlocking new possibilities for genetic editing.\nA peptide nucleic acid developed at Carnegie Mellon is part of a gene editing system that has cured a blood disorder in mice. Credit: Carnegie Mellon University.123\nGene-Editing Successfully Cures Mice Genetic Disorder: In recent years, CRISPR has made waves, sparkling hundreds of studies after researchers understood its huge potential.\nGene-Editing Successfully Cures Mice Genetic Disorder: Clustered regularly interspaced short palindromic repeats (CRISPR, pronounced crisper) are basically segments of prokaryotic DNA which can be cut with a “genetic scissors,” eliminating unwanted elements from genes.\nGene-Editing Successfully Cures Mice Genetic Disorder: The technique could herald a new age for genetic editing, but there’s a catch: it’s really difficult to apply it in complex, living creatures.\nGene-Editing Successfully Cures Mice Genetic Disorder: The difficulty stems from the fact that you need to be careful and apply it in cells all throughout the body, otherwise, there’s a high mutation risk.\nGene-Editing Successfully Cures Mice Genetic Disorder: This new technique on the other hand significantly decreases unwanted, off-target gene mutations making it much easier to apply directly to complex organisms – including humans.\nGene-Editing Successfully Cures Mice Genetic Disorder: The method relies on state-of-the-art technology revolving around peptide nucleic acid (PNA) molecules, a synthetic nucleotide technology. Nucleotides are subunits of DNA.\n“We have developed a system that uses FDA-approved nanoparticles to deliver our PNA molecule along with a donor DNA to repair a malfunctioning gene in living mice. This has not been achieved with CRISPR,” said Danith Ly, professor of chemistry in Carnegie Mellon’s Mellon College of Science and an expert in PNA chemistry.\nGene-Editing Successfully Cures Mice Genetic Disorder: There is also another issue with CRISPR. CRISPR relies on DNA-cutting enzymes to slice open DNA at a target site to edit a specific gene.\nGene-Editing Successfully Cures Mice Genetic Disorder: The thing is, these enzymes are pretty big and hard to administer in situ – in live organisms. What researchers usually do is harvest some cells, administer the enzymes in the lab and then re-implant the cells.\nThe new system consists of biocompatible nanoparticles containing PNAs, small nano-sized synthetic molecules which are much more easily applied to the body.\nIn order to test this approach, aside from lab tests, researchers attempted to cure a genetic blood disorder in mice.\nThey were successful in 7 percent of all cases which doesn’t really sound like much, but it’s a huge improvement from the 0.1% success rate usually seen in genetic editing.\nFurthermore, when you consider that this is still in its early stages, it’s even more impressive.\n“The effect may only be 7 percent, but that’s curative,” Ly said. “In the case of this particular disease model, you don’t need a lot of correction. You don’t need 100 percent to see the phenotype return to normal.”']	['<urn:uuid:c4004627-fef7-49d1-b59a-bc4dcf915062>', '<urn:uuid:0fe63a54-6628-4d70-a65b-1e923934e9a3>']	factoid	direct	short-search-query	distant-from-document	comparison	expert	2025-05-12T22:03:18.033296	6	49	1880
63	what percentage working capital maximum tied manufacturing company stock inventory raw materials	For manufacturing organizations, direct inventory (raw materials, works in progress and finished goods) can account for up to 50 percent of the current assets of the business.	['Spare parts inventory: The forgotten investment?\nInventory management is one of the most important management disciplines in every company that holds inventory. Inventory can provide the capability to meet a customer need, repair a broken machine, assemble products for sale or just keep production going.\nFor manufacturing organizations, direct inventory (raw materials, works in progress and finished goods) can account for up to 50 percent (or more) of the current assets of the business. Current assets are those investments that are expected to be converted to cash within the next 12 months. For retail and wholesale businesses, the figure is even higher. This type of inventory is part of what is referred to as working capital.\nThis working capital is tied up in the inventory, and so by definition not available for any other use until converted from an asset into cash via sales. The goal, therefore, of any business with significant working capital tied up as inventory is to convert that capital to revenue as quickly as possible. They do this by focusing attention on supply chain management, production planning and sales and operations planning.\nFor manufacturing organizations there is, however, another element of working capital that does not get this high level of attention. This is the indirect inventory and more specifically their spare parts inventory.\nThis inventory is not purchased with further processing and sales in mind and so does not represent a potential future stream of revenue like direct inventory. This inventory is purchased to fulfil the functions of maintenance, repairs and operational support. (And hence is sometimes referred to as MRO inventory.) One of the major challenges that companies face in managing this indirect inventory is that if the items are not consumed during their useful life, there is no way to recover the funds invested. Selling excess and obsolete stock rarely yields more than pennies in the dollar.\nIt would, therefore, be natural to expect that a company spending cash on something for which there is little prospect of financial recovery would have in place very tight controls on the purchase of the items.\nYet many companies buying this type of inventory often have fewer controls over the purchase decision-making than they do for their direct inventory. Of course, there will be purchasing authorities but often the decision-making on what to stock and how many to stock is ad-hoc and emotional. Decisions are left to people with little or no training in spare parts inventory decisions, so they buy too many with their initial purchase. Then there is little, or no, end-of-life planning so companies are left with significant obsolete stocks.\nThis type of inventory, it seems, is considered by many senior managers to be too low level to warrant a lot of attention. This means the cash impact of investing in this type of inventory is not always fully appreciated by others in the company.\nThis results in the investment in spare parts inventory being often ignored until the level of cash invested in the inventory gets too big to ignore. By then, however, the money has been spent and there is little prospect of recovery.\nIn many ways, spare parts inventory is the forgotten investment and it is only when senior management shakes loose its apathy toward this inventory that companies can make significant inroads into reducing money wasted on unnecessary inventory. NP']	['<urn:uuid:302b2e21-bfbc-4428-9b28-baf5f93e036a>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T22:03:18.033296	12	27	561
64	cloud data backup key benefits cost speed security	The key benefits of cloud data backup are lower operating costs through pay-per-use pricing, fast recovery times, and enhanced security. AWS enables businesses to scale infrastructure on an as-needed basis while providing comprehensive security measures like unified access policies, encryption, and Cloud Access Security Brokers (CASB). The solution also offers maximum flexibility for faster recovery during disasters without requiring expensive secondary infrastructure.	"[""More about AWS Migration\n- AWS Storage Gateway: Connecting Your On-Premise Storage to the Amazon Cloud\n- AWS Migration: Understanding the Process and Solving 5 Key Challenges\n- Bitbucket on AWS: Ultimate Quick Start Guide\n- AWS Managed Service Provider: Passing the Baton to Certified Experts\n- AWS Case Studies with NetApp Cloud Volumes ONTAP\n- Migrating Physical Server to AWS: Now Free with AWS CloudEndure\n- AWS Cloud Migration Services: Don’t Migrate Alone\n- 5 Steps to the Cloud: AWS Migration Checklist\n- AWS Migration Strategy: The 6 Rs in Depth\n- 5 Optimal Approaches for Your Hyper-V to AWS Migration Strategy\nToday, more than ever, businesses are performing AWS migrations, attracted to the cloud provider's key features and benefits such as lower operating costs, scalability, agility, high availability, and disaster recovery. In this article we’ll provide a five-step AWS migration checklist that covers how to train staff, enhance security and access management, establish cloud performance KPIs, plan for database migration, migrate static websites and unstructured data.\nIn this article, you will learn:\n- Why businesses choose to migrate to AWS\n- How do I migrate to the Cloud? A checklist\n- How to migrate data with Cloud Volumes ONTAP\nWhy Do Businesses Migrate to AWS?\nAmazon Web Services is the leading cloud provider and provides many advantages when compared to on-premise deployments and other clouds. Here are some advantages that cause businesses to migrate to AWS:\n- Cost ━ AWS is famous for its pay-per-use pricing, but in addition, it provides a large variety of services that allow you to consume exactly the level of services you need, to control costs. For example, you can perform storage tiering and can save cold data to a low-cost storage service like Amazon Glacier.\n- Scalability ━ AWS offers infinite scalability and also provides many ways to manage that scalability━from auto-scaling on EC2 instances to containerized deployments, serverless functions, and more.\n- Agility ━ AWS provides very comprehensive automation capabilities, integrates with DevOps workflows and allows teams to instantly spin up development, test and production environments to iterate quickly. Amazon also makes it easy to provide self-service IT and allows organizational units to set up their own infrastructure.\n- High availability and disaster recovery ━ Amazon offers multiple data centers around the world, with a Multi-AZ feature that lets you run cloud services in multiple data centers, so even in the case of a catastrophe that affects one geographical region, your services would continue to run.\nHow Do I Migrate to the Cloud? A Checklist\nTo migrate to the Amazon cloud, follow this five-step checklist:\n1. Train your staff on cloud migration essentials\n2. Consider security needs and access management\n3. Establish cloud performance KPIs\n4. Plan database migration\n5. Plan Data Migration for Static Websites and Unstructured Data\nRead our in-depth post on AWS migration strategy to understand Amazon’s migration framework and the six alternative strategies for migrating an application to the cloud.\nOnce you have done discovery and planning and are ready to start your migration, find more details about each of the stages below.\n1. Train Staff\nMap out staff to cloud services and infrastructure and understand who needs to know what. Use the following methods to get staff up to speed on the AWS services they will be using on an ongoing basis:\n- Classroom training━at least some formal training with an experienced instructor can give teams a good basis for AWS concepts and basic usage. Amazon provides an official training program, including self-paced labs where employees can get practical experience with the technology.\n- Online training━give employees time to consume online resources that can help them learn AWS. Learning can be freestyle or you can purchase training units from vendors like CloudAcademy and Udemy.\n- Practicing in sandboxes━give employees a small budget so they can play around on AWS. They can spin up machines, integrate them with other services, break things and get their hands dirty. This can be the fastest and most effective form of training.\n- Cloud Center of Excellence (CCoE)━many industry experts agree that cloud migration should be supported by an organizational unit that shares knowledge about cloud technology and practices, and promotes learning and experimentation. Identify your most cloud-savvy employees, encourage them to create a Cloud Center and become champions of your cloud effort.\n2. Consider Security Needs and Access Management\nSymantec provides several important steps you can take to ensure security during and after your AWS migration:\n- Unified policies━users may access cloud applications from many different locations and may need to grant third-parties access to data. Legacy applications may be restrictive in the level of access they grant users. Some web or cloud systems should be locked out and not used by any employees or third-parties. Regardless of the situation, there must be a unified policy that governs access and control for all cloud applications.\n- Ensure flexibility━cloud applications make dynamic use of data. Data may be encrypted at rest but may be decrypted when used and transferred between multiple cloud services. Understand data flows in your new cloud deployment and ensure you have security measures in place that do not disrupt essential data flows.\n- Set up security brokers━a Cloud Access Security Broker (CASB) can take log information about user interactions with endpoints and make it available for compliance, security and operations purposes. It provides visibility into activity at important cloud destinations, and lets you set policies to limit access and control shadow IT.\n3. Establish Cloud Performance KPIs\nNewRelic provides a list of simple KPIs you can use to monitor your cloud. Cloud performance KPIs are critical because, in the cloud, performance is not guaranteed and can vary between deployments, data centers, Amazon services and times of the day.\nCloud Performance KPIs examples:\n|User experience||Page load time\n|Application performance||Error rates\n|Infrastructure performance||CPU Usage %\n|Business metrics||Bounce rates and dwell time\nIt is important to:\n- Establish a baseline━measure performance of the current on-premise deployment, to be able to compare it with the performance in the cloud. This can also help diagnose specific performance issues. Set a baseline measure for each KPI, and choose the shortest baseline period that gives you statistically significant data.\n- Test and measure performance━create a realistic production-like environment in the cloud and measure each application for each metric, comparing to the baseline.\n- Diagnose problems━if your performance lags behind your goals or the original performance of the on-premise app, identify where the problem lies: in the machine instance, the performance of the underlying cloud services, data issues, or simply insufficient cloud resource allocation.\n- Optimize and retest━make changes to the environment and test again to ensure performance is up to your standards. Doing this now at the pre-migration stage will save major headaches later.\n4. Plan Database Migration\nAmazon offers a few important considerations for migrating databases to AWS.\nTake the following aspects into account:\n- Data transfer━consider how to manage the actual data transfer, will you need Extract, Transform and Load (ETL) and how will you guarantee consistency. Consider the size of the database, how long data transfer will take, and whether you need specialized services like Amazon Direct Connect. Knowing the layout of your database can help you define a migration project and speed up your data copy phase significantly.\n- Moving to a different database━it is common in cloud migrations to switch to a different database, especially in modernization projects. Alternatively, you may be switching to a service version of the same database, like Amazon RDS. Consider what changes need to happen at the application level and test carefully to ensure your data is there and that your critical application flows are working.\n- Plan time for refactoring━if you do need to refactor applications, this can be time-consuming. Leave time in your migration plan for several iterations of code changes and testing, as this may take weeks to months for complex applications.\n- Very large tables and large objects━if you have tables larger than 200 gigabytes and with hundreds of millions of rows, handle them with care. Amazon Database Management Service can help by loading the data in parallel. If you have large objects, they can be painful to migrate, take into account longer migration time and allocate more data to your replication server.\n5. Plan Data Migration for Static Websites and Unstructured Data\nIn our blog post about AWS data migration, we outlined several important considerations before moving data volumes to the cloud:\n- Amazon S3 bucket name restrictions━every S3 bucket must have a unique name, and for static websites, the bucket name must be the same as the domain name. You must ensure that you keep bucket names available to support all the buckets used in your migration.\n- Set up CDN with Amazon CloudFront━to provide fast global access to data assets, especially those on S3, use CloudFront, in combination with Route53 for DNS. Furthermore, an added benefit is that CloudFront enables SSL, and AWS provides free SSL certificates to be used with its services.\n- Define cache policy━Amazon CloudFront caches your content at the network edge. Define your cache policy smartly to ensure content updates are visible to your users. Use a short Time to Live (TTL) or periodically invalidate all caches, so you can push updates to your users.\n- Data consistency on S3━when you upload new objects they are available immediately, but update or delete operations can take time to resolve due to S3’s eventual consistency mechanism. When updated content doesn’t load for a user, it may not be a caching issue, but a data consistency issue.\n- Setting HTTP headers for web pages━when migrating from IIS to Amazon S3, a major problem is that you can’t set HTTP response headers on all pages in bulk. You can use Amazon’s CloudFront Lambda@Edge and define a custom index page for each folder.\n- S3 security━if you are leveraging CloudFront, it will take care of SSL connectivity. However, you also need to secure access to Amazon buckets. Use Original Access Identity or ACL and set up AWS WAF to monitor and block malicious incoming requests.\n- Select S3 storage type━Amazon S3 offers three storage classes for static website hosting, each with a different SLA and cost. Select storage classes carefully to save costs while ensuring that you provide appropriate performance for cold/warm/hot data.\nLift and Shift Your Data to AWS with Cloud Volumes ONTAP\nThere are many methods for migrating to AWS, however the lift and shift approach remains the quickest, simplest, lowest-risk and most cost-effective.\nA successful lift and shift migration strategy, however, requires the right tools. That is why many companies are using solution providers such as NetApp to help them with migration and to assist them with ongoing data management.\nThe lift and shift migration approach involves migrating your application and connected data to the cloud with little or no changes. Applications are “lifted” from their current environments and “shifted” in their existing state to a new premise, such as the cloud. Typically, there are no marked changes to make to the application data flow, architecture or authentication processes.\nNetApp Cloud Volumes ONTAP, the leading enterprise-grade storage management solution, delivers secure, proven storage management services on AWS, Azure and Google Cloud. Cloud Volumes ONTAP supports up to a capacity of 368TB, and supports various use cases such as file services, databases, DevOps or any other enterprise workload.\nCloud Volumes ONTAP utilizes NetApp SnapMirror® technology, to migrate, replicate, and synchronize files, or any data from on-premise, hybrid or multicloud storage systems architecture. Cloud Volumes ONTAP supports NFS, SMB/ CIFS, and iSCSI file shares with SAN and NAS storage."", 'AWS Disaster Recovery (DR) is a proven approach by Teleglobal International to encounter potential risks. AWS supports many disaster recovery architectures that are robust and highly efficient. Amazon Web Services enables your business enterprise to scale up its infrastructure on an as-needed, also enabling the pay-as-you-go basis.\nAWS Disaster Recovery Approach. There are four standard approaches that are employed for backup and disaster recovery using the AWS cloud. These comprise: Backup and Recovery Approach: This is the simplest approach and is a cost-effective method utilizing services such as Amazon S3 (Simple Storage Service) to backup and restore data.\nTrust your Disaster Recovery to AWS Certified Experts. Data loss and downtime are major worries for any IT team. While you want to take every measure to reduce this risk, budget and resources often restrict you—in some cases leaving you unprotected.\nJul 29, 2016 · The AWS cloud gives you maximum flexibility to enable faster recovery and optimize resources during a DR event. With AWS, you can scale up or down on AWS’ fast, reliable, and secure infrastructure on an as-needed basis – ensuring continuity before, during, and after a disaster. 1.\nMay 14, 2019 · Many options for disaster recovery exist, this article highlights the points required for designing a disaster recovery plan. With AWS you can design a more effective disaster recovery plan using real world scenarios as it allows you to control the maximum building blocks required to build a appropriate DR solution.\nUsing cloud for disaster recovery is a major cloud use case that is growing in popularity and this can only increase with time. Compared to traditional disaster recovery, Cloud disaster recovery is cheap, fast, and scalable both for backup and recovery purposes. AWS provides various services to assist with backup and recovery.\nPilot Light Disaster Recovery Solution on AWS. Disaster Recovery is No longer an Option. Just given the increase in natural disasters, not to mention breaches of systems, ransomware, etc., a disaster recovery plan is a “must” in this new physical and digital environment.\nAWS is the leading public cloud for good reason and companies are continuing to move more and more workloads to the AWS cloud but their model is only half of the story. That’s where we come in, to achieve the vision of the always-available, disaster-immune, cost-optimized cloud.\nCloud Recovery – AWS is a managed solution, negating the need for AWS or recovery skills, providing immediate improved resiliency, with a 4 hour Recovery time Objective SLA. Workloads can be hosted On-Premise or within another AWS Region, and then recovered to the customer’s desired AWS Region. Scale and recover with confidence\nSite Recovery is a native disaster recovery as a service (DRaaS), and Microsoft been recognized as a leader in DRaaS based on completeness of vision and ability to execute by Gartner in the 2019 Magic Quadrant for Disaster Recovery as a Service.\nVMware Site Recovery for VMware Cloud on AWS is a separate service, but it is built on top of Site Recovery Manager and vSphere Replication. Site Recovery provides on-demand disaster recovery as a service to protect your workloads both on-premises and on VMware Cloud on AWS.\nAWS Disaster Recovery Continuity in the cloud. The cloud allows you to extend disaster recovery functionality to a greater number of workloads —– including those where it had been previously unaffordable to do so — with less complexity and the benefit of on-demand, pay-as-you-go infrastructure.\nN2WS Backup & Recovery is a tool designed specifically for AWS that is meant to help you backup and restore your cloud resources quickly and easily. N2WS Backup & Recovery provides automated backup support, allows for a near zero RTO using AWS native tools, and even has cost control features that will provide you with much needed savings.\nDec 21, 2017 · Most commonly used DR approaches requires AWS services such as AWS Data Pipeline for data migration, Route53 DNS failover routing, AWS Glue for Extract Transform Load (ETL), AWS S3 for file storage or third party solutions for AWS disaster recovery and cloud independent data migrations. Using AWS Storage for DR\nAmazon Web Services – HA/DR Options for SAP HANA on AWS November 2015 Page 9 of 23 you configure SAP HANA services to start up automatically after a system restart for a fully automated recovery. HANA System Replication HANA System Replication (HSR) is a high availability and disaster recovery solution provided by SAP.\nHystax transforms the way enterprises approach cloud migration, cutting project timelines from months to days. Hystax Acura is a highly specialized and fully automated solution for migrating workloads from on-premises datacenter to the cloud as well as cross-cloud\nDisaster recovery. Even a minor outage can put you at a competitive disadvantage. So, be ready with a business continuity plan that includes disaster recovery for all your major IT systems—without the expense of secondary infrastructure.\nDisaster Recovery Using AWS Cloud. Disaster recovery as a service (DR) using public cloud is based on virtualization and takes a unique approach to traditional disaster recovery planning (DRP). Delivering faster recovery times and multi-site availability at a fraction of the cost of conventional disaster recovery, DRaaS using AWS cloud\nDell EMC’s Data Domain Cloud Disaster Recovery is a solution that allows users to extend their on-premises data protection to the cloud, and orchestrate the disaster recovery of a protected machine on the cloud. The solution has been available since early 2017 and many customers are already using it for their disaster recovery.\nTo enable faster disaster recovery of the critical IT systems and recover from any event that has a negative impact on the Pragma IT business Romexsoft build Pilot Light Disaster Recovery Solution using reliable AWS cloud infrastructure.\nNov 23, 2018 · S3 cross region disaster recovery 1strategy aws disaster recovery whitepaper certification high availability and disaster recovery options for sap hana for developing your aws disaster recovery plan performing a disaster recovery failover and failback. New Whitepaper Using Aws For Disaster Recovery News.\nSep 10, 2019 · Cloud Volumes ONTAP offers a cloud-based disaster recovery solution for your storage. The solution can be configured for AWS disaster recovery as well as for Azure disaster recovery, which is outlined in this blog post. As a central part of a disaster recovery strategy, it offers better protection and security when compared to traditional\nOnica’s dedicated team of AWS experts can help architect Disaster Recovery as a Service (DRaaS) solutions for your business leveraging Amazon Web Services (AWS) as the hosting platform and Onica’s 24×7 Managed Services Team as the support engine. AWS provides a robust platform to host Disaster Recovery and add additional services on-demand.\nAWS Disaster Recovery. As an AWS Premier Consulting Partner, Mission’s team of certified engineers has extensive experience in developing AWS disaster recovery plans.We’ll help you navigate the options and develop a proactive AWS disaster recovery plan to help you meet your Recovery Time Objectives (RTO) or Recovery Point Objectives (RPO) and remain operational.\nAWS Certified Professional: Do you know on which cloud server your data was hosted? Client: Yeah, it’s Amazon Web Services. AWS Certified Professional: Did your cloud services provider practice disaster recovery for safety and security of your data? Client: Buddy, I am not so tech savvy. I am not sure what they were doing technically with my\nCohesity DataPlatform software is designed to run on Amazon Web Services. Native integration with AWS cloud and AWS Outposts helps you protect and consolidate data over hybrid cloud. Gartner – How to Leverage Public Cloud IaaS for Backup and Recovery Leveraging Cohesity’s solution on AWS allows\nMay 15, 2017 · Companies are using the AWS cloud to enable faster disaster recovery of their critical IT systems without incurring the infrastructure expense of a second physical site. In this 1h webinar, Evgeny\nJul 13, 2018 · Disaster Recovery across regions in AWS. By David Hill on July 13, 2018 in Veeam. Last week I wrote an article on how Veeam Cloud Protection Manager can be used to protect your instances in AWS. This week I want to look at how you can use Cloud Protection Manager for Disaster Recovery across regions in AWS.\nMay 26, 2018 · We have discussed various disaster recovery options available for AWS RDS and their configuration and limitations. Tensult Blogs. Stories on Cloud computing, Analytics, Automation and Security, AWS.\nWhen using AWS, you have disaster recovery options for storage, networking, databases, compute, and deployment orchestration. Wrap up. Whichever service you choose, the cloud provides greater accessibility and cost-efficiency than ever when it comes to implementing an effective disaster recovery\nAnnounced today, a new service from OneCloud Software for VMware-based environments uses the Amazon Web Services Inc. (AWS) cloud as a virtual disaster recovery datacenter. OneCloud Recovery is unique in that it provides “built-in cloud cost optimization” that lowers costs, claimed OneCloud in a statement today.\nDec 18, 2019 · For some, this cost is an accepted part of having a disaster recovery strategy in place. However, there are ways to minimize costs on platforms that can sit idle waiting for DR testing or actual DR events. VMware Cloud on AWS is an integrated cloud offering jointly developed by Amazon Web Services (AWS) and VMware.\nJan 08, 2019 · Amazon has reportedly acquired one of the disaster-recovery service providers within the Amazon Web Services Marketplace, purchasing\nI’m curious to hear from folks who have experience in AWS and who have built traditional enterprise disaster recovery plans. I’m thinking about an engineer who from an enterprise that employs similar technologies such as (EMC Recoverpoint – block level async replication, VMware Site Recovery Manager – recovery orchestration).\nTo facilitate disaster recovery of its ERP database, the company decided on an Oracle Limited disaster recovery optimized solution. “We learned that the Oracle Limited solution was available to us at no cost when in sleep or standby state.” AWS is an authorized cloud platform for Oracle — one of a very small number of approved cloud vendors.\nWe’re fully tested and validated to support AWS Outposts. That means Commvault helps you keep it simple, with unified data management and data protection across AWS in the cloud and AWS Outposts on premises. Now you can relax. Commvault software covers all of your AWS data management and data protection needs.\nJan 10, 2019 · Business continuity and IT resilience are a click away with CloudEndure Disaster Recovery. Our one-touch disaster recovery solution utilizes block-level continuous replication, application stack\nAWS for Disaster Recovery Customers with primary workloads outside of AWS can enable faster disaster recovery of critical systems to the AWS Cloud without incurring the infrastructure expense of a second physical site. Customers with primary workloads already running in the AWS Cloud can use the\nHow to build a successful disaster recovery plan using multicloud technology. When disaster strikes a data center, you need to be sure recovery efforts are as smooth as possible.\nFeb 13, 2020 · However, there are two use cases that seem to be bigger drivers of VMware Cloud on AWS; the need for improved performance and disaster recovery. Aside from on-demand access to infrastructure, another big advantage of AWS is the sheer number of solutions they have created that become available to use in a matter of minutes and can be easily\nThings you should know about Disaster Recovery in AWS. AWS regions have separate control planes, which generally serve to isolate faults to particular regions. That’s why we don’t see global outages of DynamoDB, but rather region-specific outages of particular services.\nStoneFly and AWS: Integration beyond Your Imagination. Businesses have prioritized data backup and disaster recovery with the turn of the century. StoneFly provides customers with backup and replication with the DR365 appliance. However, business continuity demands disaster recovery in the cloud. So, the partnership between StoneFly and AWS\nHystax Disaster Recovery to Azure is the only any-to-any Disaster Recovery and backup software that supports all modern source platforms: AWS Google Cloud Platform\nOct 26, 2015 · When creating a cloud disaster recovery plan, don’t underestimate how an outage will affect your enterprise and its customers. Test your AWS DR plan early and often.\nDec 07, 2016 · Taking care of backups and disaster recovery is the matter you are going to consider sooner or later. Even if you are running your infrastructure on such a\nApr 18, 2019 · Cloud disaster recovery strategy. Disaster recovery strategy does not change dramatically with the addition of the cloud, but there are new factors to consider. Your cloud disaster recovery strategy must still be business-oriented, reflecting the priorities of the business and its operating model. For example, if SAP ERP requires the fastest\nMay 30, 2017 · Cloud computing has made disaster recovery better and cheaper. Here’s what you need to know about how to integrate the cloud into your disaster recovery plan. A disaster can strike at any time, disrupting the delivery of your software applications to your customers and your business’ continuity. Whether it’s due to an outage in your network,\nDisaster risks dictate the necessity to incorporate AWS disaster recovery best practices and have proper AWS disaster recovery tools in place. NAKIVO Backup & Replication was designed with a focus on fast and reliable AWS disaster recovery.\nOct 03, 2017 · I’ll explain why you need disaster recovery and do a deep dive on how to configure all of the core AWS services that are used for disaster recovery,\nBusiness Continuity and Disaster Recovery Solutions Sungard AS is a leading provider of Business Continuity and Disaster Recovery with 40 years proven record. Learn more about our tailored solutions. And we can act as a single point of recovery for virtual machines and data in the Amazon Web Services (AWS) cloud.']"	['<urn:uuid:334855e5-9e6b-461a-966a-727f625f9b32>', '<urn:uuid:dfeac1b9-bcfd-445e-87cb-0441ea8da488>']	factoid	direct	short-search-query	distant-from-document	three-doc	novice	2025-05-12T22:03:18.033296	8	62	4231
65	I'm planning some evening entertainment and noticed both the Kamuy Lumina in Hokkaido and Wings of Time in Singapore have storylines. What stories do these shows tell and how long does each performance last?	The Kamuy Lumina tells a traditional Ainu story about the Owl and Jay Bird who must deliver a message to the Kamuy (gods) to restore harmony between humans and animals, lasting 50-60 minutes. Wings of Time, running for 15 minutes, presents a story about friendship using projections, lasers, and fire effects, culminating in a fireworks display over Sentosa beach.	"['ABOUT KAMUY LUMINA\nKamuy Lumina is an enchanted experience in Moment Factoryʼs Lumina Night Walks series.\nBeginning at nightfall, you are invited to pursue an immersive experience along\na 1.2 km walking path into the land of the Ainu gods,\nthrough the Forest of Lake Akan, in Akan Mashu National Park, Hokkaido.\nYour journey will begin when you receive your “Rhythm staff”,\nwhich you will be called\nupon to use at different moments along the way. After being greeted by your host,\nthe Owl, you will set off on an enchanted journey!\nIn this retelling of a traditional Ainu song,\nyou will play an active role in the unfolding of the story and will be\nemotionally immersed through interactivity,\nlight, sound, scenography and video projection.\nKAMUY LUMINA is an enchanted experience in the Lumina night walks series. Inspired by the local culture and the natural beauty of the site, the Lumina night walks create an enchanting environment by using a universal language. Beginning at night fall, visitors are invited to pursue an immersive experience, where emotions are engaged through lights, soundscape, scenography, video and interactivity. The Lumina night walk series is seen not only in the creator’s home province of Québec but also in other places in North America and Asia where it is receiving great acclaim.\nFor your safety please note the following.\nThe Kamuy are angry and have stopped sending deer and fish\nfor the hunters because humans have forgotten their respect\nfor the land. But the Owl and the Jay Bird are determined to get\nthe humans a second chance by travelling to the land of the Kamuy,\nand delivering a musical message on their behalf.\nThere’s just one problem. The Jay Bird may have the most beautiful\nvoice in the forest, but he can’t follow a beat! He needs your help\nto keep the rhythm.\nJoin him on his journey, and you might even return with the wisdom\nto bring the animals back to the forest!\n- THE OWL PROTECTOR OF THE AINU VILLAGE\nHis low, calm voice emanates a welcoming atmosphere of\nprotection, intelligence and wisdom. He has been watching\nover the forest and the village for a very long time.\nHe seeks the perfect messenger to take his plea on behalf\nof humans to the land of the Kamuy, but ends up with the clumsy\nand excitable Jay Bird. The Owl is skeptical at first, but decides\nto help the Jay Bird realize his heroic potential.\nDespite his endearing personality, the Owl gets a little annoyed\nby the Jay Bird’s chaotic way of doing things. But, their fates\nare intertwined, and regardless of their differences,\nthey make a great duo.\n- THE JAY BIRD THE OWL\'S MESSENGER\nHe is a peppy and funny little bird. Clumsy and excitable,\nhe can’t speak very well, but can sing more beautifully than anyone\nin the forest. His dream is to sing for the Kamuy, but he is timid\nbecause he can’t keep a beat. To realize his dream, he will need\nthe audience’s help to keep the rhythm.\nWhen he takes up the Owl’s mission to travel to the land\nof the Kamuy, he doesn’t fully realize its importance. But through\nthis journey, he grows and learns, ultimately becoming\na distinguished hero.\nEXPLORE YOUR JOURNEY\nClick on the arrows to travel through the world of Kamuy Lumina\n- Enter the forest\n- Grab your ""Rhythm staff"" and enter Kamuy Lumina. The Owl, keeper and the protector of the Ainu village, is waiting for you.\n- The Protector\n- The Owl seeks a messenger to travel to the land of the Kamuy with a plea on behalf of humans. Will you help him?\n- The Messenger\n- Meet the Owl\'s messenger, the Jay Bird. Learn the Owl \'s rhythm and follow the Jay Bird through the forest towards the land of the Kamuy.\n- Escape of the Deer\n- The deer are escaping, fleeing from the humans who have forgotten their reverence for the land. Play the rhythm to encourage them to stay!\n- Message of the Marimos\n- Play along with the musical patterns of the marimos to convince the fish to remain.\n- Land of the Kamuy\n- Follow the Jay Bird into the luminous world of Kamuy as he delivers the Owl’s important message.\n- A Warning\n- The warning of the Kamuy resounds all around. Are you ready to learn their wisdom?\n- A Collective Rhythm\n- Now it’s your turn to show the Kamuy that you have understood their lesson. Tap the rhythm all together!\n- The Return\n- Find out what the Jay Bird has accomplished with your help after meeting with the Kamuy. The Owl will be forever grateful.\n(Junior high school students +)\n- Advance：3,000 yen\n(Elementary school students)\n- Advance：1,500 yen\nTickets are sold on this website 、at hotels in Akan Onsen area、and at Kamuy Lumina ticket booth.\n※Free admission for preschool children.\nKAMUY LUMINA + LOST KAMUY Bundle ticket\n(Junior high school students +)\n- 4,700 yen\n(Elementary school students)\n- 2,100 yen\nLOST KAMUY is a performance program at the ikor theater that combines ancient ceremonial dance,contemporary dance,and 3-D imagery.\nKAMUY LUMINA/LOST KAMUY bundle tickets are sold at the Kamuy Lumina ticket booth,Lake Akan Ainu Theater Ikor and Hotels around Lake Akan.\n※LOST KAMUY is organized by the Akan Ainu Industrial Arts Association.\nPlease check the follow link for details. https://www.akanainu.jp/en/about/ikoro\n5-20, Akankoonsen 1-chome, Akancho, Kushiro City, Hokkaido, 085-0467, Japan\nAkan Adventure Tourism E-mail:email@example.com\n- Program Start Times\n- -Event hours will change depending on the season.\n-The attraction\'s opening time and program start time may vary slightly depending on the day\'s weather conditions. When crowded to the full capacity, the waiting time may take longer.\n- Entering KAMUY LUMINA\n- -To prevent any accidents and confusion and to ensure the smooth flow of people, be sure to follow the instructions given by our staff. People who do not follow our staff\'s instructions may be refused entry or may force us to cancel the program. If it gets very crowded, we may limit the number of people entering the attraction.\n-People entering KAMUY LUMINA may be subject to a bag search for hazardous items.\n- -KAMUY LUMINA does not have parking.\nPlease park at Akan Forest Garden.\n-Eating inside KAMUY LUMINA is prohibited.\n-Since the path follows the natural contours of the park, there may be rough ground and slippery areas. You should wear appropriate clothing and footwear. (High heels, sandals etc., would not be suitable.)\n-KAMUY LUMINA\'s entrance and exit are at different locations. You cannot store any belongings at the entrance.\n-Always look after young children while walking within the path. You should hold hands with smaller children.\n-Utilizing wheelchairs and baby strollers inside KAMUY LUMINA is admitted, you\nmight have some difficulty on rough ground though. Take great care.\n-With the exception of guide dogs, hearing dogs, and service dogs, no pets are allowed in KAMUY LUMINA.\n-No smoking in and around KAMUY LUMINA is allowed, except within designated areas.\n-Photography with a drone, monopod, or tripod is not allowed. Also, do not block the path while taking pictures.\n- Other Cautions\n- -The attraction will be held even in rainy weather. However, the program may be modified or canceled partially or wholly in the case of adverse weather or natural disaster. If the program is canceled or postponed, there will be no compensation for any transportation or lodging expenses. If the program has been canceled or if the attraction has been closed due to any of the reasons above, a refund will be given or your ticket date can be changed.\n-We cannot change your ticket date or refund your ticket due to any delay or cancellation of the mode of transportation used.\n-Once you agree to the ticket purchase agreement, you cannot exchange, change, or cancel the ticket.\n-If you lose or destroy your ticket, a replacement ticket will not be issued.\n-Walk-up tickets may be available for purchase at Ticket Booth at KAMUY LUMINA entrance and hotels in Akan Onsen area. Since space is limited for each departure/group, it is recommended to purchase your ticket in advance from this official website for you to participate at your desired departure time.\n-In principle, your ticket is only valid for the printed departure time. Please speak with customer service or ticketing when you arrive to determine if you will be able to utilize your ticket at a later departure time. Please note that waiting times may vary in busy periods.\n-The organizer may photograph the attraction for publicity and advertising purposes and visitors may be pictured in the photos or video.\n-Anyone hindering the experience for other visitors, the staff, or the attraction may be refused entry or be expelled from the premises.\n-KAMUY LUMINA and its related companies cannot be held liable for any accidents, thefts, etc., occurring in or outside the premises. Take adequate precautions to protect your valuables and personal belongings.\n- Prohibited Acts\n- -Bringing in hazardous items (knives, scissors, etc.); riding a bicycle, kick scooter, etc., in the attraction; or bringing anything that annoys other visitors.\n-Going over a fence.\n-Straying away from the set path.\n-Walking in the reverse direction on the path.\n-Entering the attraction while inebriated.\n-Entry of criminal gangs or gang members.\n-Bringing anything else deemed to be hazardous or exhibiting any other behavior deemed to be undesirable by the attraction\'s operator.\nDiscover other night walksFIND OUT MORE\n- What are your opening hours?\n- The experience will open 30 – 45 minutes after sunset, and the last departure will be at 9:30pm.\n- Who can visit the experience? Is it suggested for children and seniors?\n- The experience is designed for all ages. There are no mobility restrictions on this site. However, the path is 1.2km so guests must be able to walk this distance at a good pace. The Rhythm staff will be distributed to children in primary school and older.\n- How long does it last?\n- The experience will last 50 – 60min.\n- How much time can I spend visiting? Am I limited in time?\n- The experience is designed as a linear path experience, guests are encouraged to walk the path with their dedicated group.\n- Is there a lot of walking involved?\n- Is it accessible to all?\n- The path is fully accessible, with moderate path inclines.\n- Can I take photos?\n- Yes, it is encouraged,, however flash photography is strictly prohibited.\n- How should I dress?\n- Comfortable attire and dress for the seasonal weather temperature. Check weather forecast in advance of your experience date and time.\n- Can we eat there?\n- Food & Beverage is available in the welcome area, however please refrain from eating & drinking within the experience.\n- Are there washrooms?\n- Washrooms are located in the welcome area. There are no washrooms on the experience path.\n- Why should I purchase my ticket ahead of time?\n- Space is limited for each departure/group so it is recommended to purchase your ticket in advance for you to participate at your desired departure time.\n- Can I buy my ticket when I arrive?\n- Yes, however requested departure times may be full.\n- There is a time on my ticket. What does it mean?\n- The time on your ticket is your departure time for the experience. It is recommended that you arrive at the entrance area minimum 15 min early.\n- Where can I buy a ticket?\n- Tickets are available on Kamuy Lumina official website and at hotels in Akan Onsen.\n- Can I visit the experience many times with the same ticket?\n- Your ticket is valid for one experience.\n- What happens if I arrive late?\n- In principal your ticket is only valid for the printed departure time. Please speak with customer service or ticketing when you arrive to determine if you will be able to utilize your ticket at a later departure date.\n- Are tickets reimbursed if it rains?\n- The weather will be monitored to ensure guest safety,. The experience will be held rain or shine, if the weather poses no threat to public safety. If the experience is cancelled due to inclement weather, then tickets will be refunded.\n- Why is it not possible to visit during the day?\n- This multimedia Night Walk has been developed to be experienced after sunset.', 'Make your trip more special by seeing the 3 popular light shows in Singapore, which are located at Gardens by the Bay, Marina Bay Sands, and Sentosa. And these are no other than―Garden Rhapsody, Spectra, and Wings of Time.\nIf you’re around Marina Bay, catch the Garden Rhapsody at Gardens by the Bay’s Supertrees or Spectra – Light and Water Show. And if you are exploring Sentosa Island, you shouldn’t miss the Wings of Time.\nEach of these has different kinds of digital charms and breath-taking presentations. So, it’s best that you see all three shows together with your family and friends. And for sure, you won’t get disappointed with the fantastic displays of vibrant colours, beaming lights, and lively music.\nTo know more about the best Singapore light shows, check out the list below.\n1. Garden Rhapsody – A Mesmerizing Night Exhibition at Gardens by the Bay’s Supertrees\nThere’s more to love with these giant, vertical gardens situated at Supertree Grove of Gardens by the Bay. At night, the Supertrees transform the entire place into a dynamic and festive landscape filled with magical colors and sounds. And that signals the start of the ever-spectacular light show―the Garden Rhapsody.\nThis impressive light and sound exhibit add more life to the already vibrant scenery. Here, you’ll witness the towering gardens dancing to the beat of the music. All the colours surrounding the Supertrees also give extra charm to everyone watching. Without a doubt, the Garden Rhapsody is definitely a must-watch attraction at Gardens by the Bay.\nThere are two shows every night to give a chance to all visitors. Each show runs for about 15 minutes. And the best part? You can watch it for free! Aside from this popular light show, there are other night-time activities in the country. You can simply check out my article on the most famous things to do in Singapore at night for a more comprehensive guide.\nSo, if you’re looking for a memorable way to conclude your Marina Bay trip, don’t forget to include Garden Rhapsody in your Singapore itinerary. Surely, you’ll enjoy a once in a lifetime experience seeing the beautiful and brilliant Supertrees in action.\n7:45 AM and 8:45 PM\nNearest MRT Station:\nBayfront MRT Station, Exit B\nGarden Rhapsody Location\n2. Spectra – Watch Light and Water Show at Marina Bay Sands\nAside from Garden Rhapsody, one of the top Marina Bay attractions at night is Spectra – Light and Water Show. You can find this amazing show in front of The Shoppes at Marina Bay Sands shopping mall.\nMore than its impressive display of digital arts and colours, Spectra actually takes into account the country’s humble beginnings to its modern innovations. It narrates the history of Singapore in four parts through beautiful visual presentations, stunning laser effects, and sparkling fountain jets.\nThe number of shows varies depending on the schedule. From Sunday to Thursday, there are two shows, while there are three shows every Friday and Saturday. Each show lasts up to 15 minutes.\nSo, if you want to see a spectacular combination of light, water, and sound show in Singapore, don’t think twice and watch Spectra.\nSunday to Thursday\n8:00 PM and 9:00 PM\nFriday and Saturday\n8:00 PM, 9:00 PM and 10:00 PM\nNearest MRT Station:\nBayfront MRT Station, Exit D\nMarina Bay Sands Light Show Location\n3. Wings of Time – Fantastic Light Presentation at Sentosa\nOne of my favourite things to do in Sentosa is to witness the Wings of Time. With the picturesque Sentosa beach as the backdrop, it’s hard not to fall in love with the visually incredible presentation.\nThe unique projections, advanced lasers, and fire effects create a brilliant animated show that is suitable for all ages. To make it more magical, Wings of Time revolves around a remarkable story of friendship.\nFollow the characters as they go through different adventures in a span of 15 minutes. Because of its moving narration and majestic visuals, this show might even deserve a standing ovation. You can choose from the two time slots, though the fee is just for one show.\nAnd to top all other light shows, Wings of Time ends the presentation with a bang through its incredible fireworks display. So, if you want a magical light show that has everything, from awe-inspiring animation to majestic soundtrack to stunning fireworks, this is no doubt one of the best contenders.\n7:40 PM and 8:40 PM\nStandard Seat – SGD $18\nPremium Seat – SGD $23\nHow to Get There:\nTake MRT to HarbourFront Station. At HarbourFront Station, walk to the 3rd floor of VivoCity so that you can board the Sentosa Express to Beach Station in Sentosa Island. Once you reach Beach Station, you just need to walk to Wings of Time in 1 minute.\nWings of Time Location']"	['<urn:uuid:d19732e3-29ab-47c3-92f8-9d45ad27ae67>', '<urn:uuid:e3047e0f-c49e-49e3-9a2c-76a437670de2>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T22:03:18.033296	34	59	2886
66	factors consider selecting appropriate respiratory protective equipment workplace hazards	When selecting appropriate respiratory protective equipment (RPE), several factors need to be considered: whether the RPE is appropriate for the hazardous substance (check SDS), if the RPE is suitable for the work involved, whether the RPE will create another risk, and if the RPE works with other PPE the worker uses. It's important to note that RPE should not be the first or only control measure considered, as it can be costly to maintain and replace. Engineering controls, such as exhaust ventilation, may be more cost-effective long term.	['RPE, respiratory protective equipment, is a type of PPE, which helps to protect people\nfrom breathing in substances that are hazardous to health. Some people find RPE a\nconfusing topic as there are a lot of options. Consider these factors when selecting to\nuse RPE as your control against hazardous substances.\nAirborne substances hazardous to health can be found in dust, mist, vapour or gases. You may or may not be able to see it in the air. If these hazardous substances are inhaled, workers or other people (visitors) could become unwell.\nDepending on the substance, the effects can be immediate or long term. Common immediate effects can include headaches, feeling dizzy and sick, and eye and skin irritation. Long term effects include cancer, organ damage and death.\nManaging Risk Using RPE\nTo manage risks arising from respiratory hazards, all appropriate and effective control\nmeasures which are reasonably practical need to be considered. With a preference of\ncontrol measures that protect multiple people at once. PPE, such as RPE, should not\nbe the first or only control measure considered.\nRPE is not a quick and easy solution. RPE can be costly to maintain and replace.\nEngineering controls, such as exhaust ventilation may be more cost effective long term.\nIf RPE is the selected control measure, there are some factors to consider when\nselecting the most appropriate RPE for your circumstance.\n• Is the RPE appropriate for the hazardous substance? (check SDS)\n• Is the RPE suitable for the work involved?\n• Will the RPE create another risk?\n• Does the RPE work with other PPE the worker uses?\nThere are a few types of respirator protection equipment to select from. It is important\nresearch options and identify the best respiratory system for your work.\nRespirators: Use filters to remove contaminants from the air the wearer breathes.\nPowered Air Supply: (PAPRs) Contaminated air is forced by a powered fan through filters\nto provide purified air for the wearer.\nSupplied Air Respirators: Provide a supply of clean air from a source, such as a cylinder\nor air compressor.\nWorkers need to visually check their respirator for signs of damage before each use.\nWorkers must inform management immediately, if there is any damage or defect that\nthey become aware of.\nRPE needs to be kept clean and stored correctly. As respiratory equipment should not\nbe shared having each team member trained of how to use, check, store and clean their\nown RPE is recommended.\nFitting of RPE:\nWith everyone’s faces being different shapes and sizes it is important to have\nemployee’s fit-tested before purchasing the equipment. It is unlikely that one\nmodel will fit every employee’s face characteristics.\nFit testing can be either qualitative (smell or taste tests) or quantitively (involving\nspecialist equipment). Negative pressure RPE must have a tight seal around the\nface to be effective.\nFit-testing can be a useful training exercise to teach workers how to use and\nwear their RPE properly. It is suggested fit-testing is performed once a year,\nespecially if the worker has changed weight or has sustainable dental work\ncompleted. The worker needs to be tested for each piece of RPE they use.\nFacial hair and stubble can make it almost impossible to get a tight seal. If a\nnegative pressure RPE is used please discuss the importance of clean shaven\nskin to workers. Jewelry, glasses, long hair and makeup can also compromise\nA ‘RPE check’ before entering the hazardous zone should be conducted to\nensure tight and correct fit of the RPE. (see attached poster)\nIf a workers glasses fog up when wearing a half-face respirator, this indicates\nthere is a leak at the top of the mask.\nWe recommend that a written record of the respiratory protection programmed ,\nincluding training, selection and use of RPE and any health monitoring are kept\non record. Keeping a record of regular reviews to RPE and any changes that\nresult is also recommended.\nSee in PDF, the below free poster for your workplace.']	['<urn:uuid:b5458e78-be81-4b21-9b63-06694391613a>']	open-ended	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T22:03:18.033296	9	88	666
67	how music affects brain emotions clinical therapy evidence benefits	Music has a powerful influence on the brain, affecting moods and feelings despite being an abstract system of sounds. From a clinical perspective, music therapy is an evidence-based health profession that addresses physical, emotional, cognitive, and social needs. Research supports its effectiveness in physical rehabilitation, emotional support, and providing outlets for expression, with treatments including creating, singing, moving to, and listening to music.	"['A bittersweet symphony3 min read . Updated: 01 Dec 2007, 12:13 AM IST\nA bittersweet symphony\nA bittersweet symphony\nMusicophilia | Oliver Sacks\nThe vast amounts of data mined by the human senses are so well organized by the brain, streamlined without conscious effort into perceptions of such clarity, that most of us never think twice about the wonder of it all. How do our two eyes somehow produce one stable image in the brain? Our two ears, one sound? These are the quotidian miracles of our lives, so familiar that we take them for granted.\nSacks’ new book, Musicophilia, is a survey of the power of music over the human brain. Our aptitude for music, Sacks remarks, is as deeply hard-wired into us as our aptitude for language—a conundrum that is still being investigated by evolutionary biologists. Although an abstract system of sounds, music exercises enormous influence over our moods and feelings. Not only that, our musical memory is extraordinarily tenacious. We have only to hear a song or tune once to be able to “replay"" it in the mind with perfect fidelity to rhythm, pitch and tempo—and can often still recall it years later.\nSacks’ case histories present us with a series of instances of music as both disorder and cure. Among the strangest cases presented by him is that of a doctor called Tony Cicoria who, a few days after being struck by lightning, begins to hear music playing in his head all day long. “It’s like a frequency, a radio band,"" he explains. “If I open myself up, it comes."" Cicoria is suffering from musicophilia, and has been virtually reborn. He had no special interest in music earlier, but now his life is devoted to playing the piano and composing.\nOther cases are about people who are not necessarily patients, but more like medical and musical curiosities: People, for instance, who can hear in perfect pitch (such as the five-year-old musical prodigy who insists that “Papa blows his nose in G"") or those who experience musical synaesthesia or a confusion of sense impressions, in which each note, for instance, is experienced as a different colour. On the other hand, there are conditions that involve not the advance but the retreat of music. Many of Sacks’ case histories are of highly musical people—many of them professional musicians—who find their working lives disrupted after neurological damage of some sort.\nOf these, the most curious and heart-rending case is that of Clive Wearing, an accomplished musician whose capacity for memory is ruined by a brain infection. The effect is not only to destroy Wearing’s capacity for work but, in fact, his entire self. He lives only in the moment (though he is subliminally aware that there is something disastrously wrong about him) and everybody but his wife becomes a stranger. He is incapable of any complex thought, because he cannot keep track of any sequence. Yet, somehow, his musical skills are undiminished—as if stored in a separate bank in his memory—and his acute misery lightens when he plays. Music is a way of filling up his ghastly and eternal present.\nThe reader will note after a while that there is hardly any episode that ends with the patient being “cured"". This is because neurological damage can hardly be reversed; the brain’s balance is so complex that it cannot be reassembled artificially. The cure, where intense suffering is involved, is in helping the patient adapt to his or her condition, make a kind of peace with it. These cures begin with an understanding of the unsettling lack being experienced by the patient and are furthered by the creative use of music.\nThe flow of music, Sacks repeatedly shows—the manner in which it embodies “will"" and intentionality—can have a tonic effect on all kinds of conditions, from Parkinson’s to amnesia to depression. Even where he rummages through complex specialist literature, he synthesizes its meaning in an uncommonly lucid and insightful fashion, and with an eye for the philosophical implications of different biological conditions. Musicophilia is a book that, like a stone thrown into still water, powerfully disturbs our sense of ourselves.\nRespond to this review at email@example.com', 'What is Music Therapy?\nMusic Therapy is the clinical and evidence-based use of music interventions to accomplish individualized goals within a therapeutic relationship by a credentialed professional who has completed an approved music therapy program.\nMusic Therapy is an established health profession in which music is used within a therapeutic relationship to address physical, emotional, cognitive, and social needs of individuals. After assessing the strengths and needs of each client, the qualified music therapist provides the indicated treatment including creating, singing, moving to, and/or listening to music. Through musical involvement in the therapeutic context, clients\' abilities are strengthened and transferred to other areas of their lives. Music therapy also provides avenues for communication that can be helpful to those who find it difficult to express themselves in words. Research in music therapy supports its effectiveness in many areas such as: overall physical rehabilitation and facilitating movement, increasing people\'s motivation to become engaged in their treatment, providing emotional support for clients and their families, and providing an outlet for expression of feelings.\nWhat Music Therapy Is... and Is Not\nThe American Music Therapy Association (AMTA) supports music for all and applauds the efforts of individuals who share their music-making and time; we say the more music the better! But clinical music therapy is the only professional, research-based discipline that actively applies supportive science to the creative, emotional, and energizing experiences of music for health treatment and educational goals. Below are a few important facts about music therapy and the credentialed music therapists who practice it:\n- Music therapists must have a bachelor’s degree or higher in music therapy from one of AMTA’s 72 approved colleges and universities, including 1200 hours of clinical training.\n- Music therapists must hold the MT-BC credential, issued through the Certification Board for Music Therapists, which protects the public by ensuring competent practice and requiring continuing education. Some states also require licensure for board-certified music therapists.\n- Music Therapy is an evidence-based health profession with a strong research foundation.\n- Music Therapy degrees require knowledge in psychology, medicine, and music.\nThese examples of therapeutic music are noteworthy, but are not clinical music therapy:\n- A person with Alzheimer’s listening to an iPod with headphones of his/her favorite songs\n- Groups such as Bedside Musicians, Musicians on Call, Music Practitioners, Sound Healers, and Music Thanatologists\n- Celebrities performing at hospitals and/or schools\n- A piano player in the lobby of a hospital\n- Nurses playing background music for patients\n- Artists in residence\n- Arts educators\n- A high school student playing guitar in a nursing home\n- A choir singing on the pediatric floor of a hospital\nFinally, here are examples what credentialed music therapists do:\n- Work with Congresswoman Giffords to regain her speech after surviving a bullet wound to her brain.\n- Work with older adults to lessen the effects of dementia.\n- Work with children and adults to reduce asthma episodes.\n- Work with hospitalized patients to reduce pain.\n- Work with children who have autism to improve communication capabilities.\n- Work with premature infants to improve sleep patterns and increase weight gain.\n- Work with people who have Parkinson’s disease to improve motor function.\nAMTA’s mission is to advance public awareness of the benefits of music therapy and increase access to quality music therapy services in a rapidly changing world. In consideration of the diversity of music used in healthcare, special education, and other settings, AMTA unequivocally recommends the unique knowledge and skill of board certified music therapists.\nBradt, J., Magee, W.L., Dileo, C., Wheeler, B.L., & McGilloway, E. (2010). Music therapy for acquired brain injury. Cochrane Database of Systematic Reviews, 2010(7), doi: 10.1002/14651858.CD006787.pub2.\nClair, A. A., Lyons, K., & Hamburg, J. (2012). A feasibility study of the effects of music and movement on physical function, quality of life, depression, and anxiety in patients with Parkinson disease. Music and Medicine, 4 (1), 49-55.\nQuotes about Music Therapy\n- Dr. Sanjay Gupta:\n""On this day, I was playing the patient. An intensive, exhaustive seven-hour schedule was presented, full of physical therapy, speech, recreational, occupational and my personal favorite – music therapy."" - CNN, February 2011\n""Music therapy helps speech, but also motor skills, memory and balance. Also emotionally uplifting."" - Twitter, May 2011\n- Jodi Picoult (Author of the bestselling book Sing You Home):\n""Music therapy, to me, is music performance without the ego. It’s not about entertainment as much as its about empathizing. If you can use music to slip past the pain and gather insight into the workings of someone else’s mind, you can begin to fix a problem. ""\n- Michael Greene, President & CEO of NARAS - 1997 Grammy Awards:\n""When we look at the body of evidence that the arts contribute to our society, it\'s absolutely astounding. Music Therapists are breaking down the walls of silence and affliction of autism, Alzheimer\'s and Parkinson\'s disease.""\n- Sen. Harry Reid (D-Nev.):\n- Sen. Harry Reid:\n""Music helps all types of people to remain forever young."" He noted that Congress had never before ""directly addressed the question of music"" as preventive medicine and as ""a therapeutic tool for those suffering from Alzheimer\'s disease and related dementias, strokes and depression.""\n- SUPERIOR, WI Telegram, Aug. 14, 1991.\n- Mickey Hart (Grateful Dead):\n""(Rhythm) is there in the cycles of the seasons, in the migrations of the birds and animals, in the fruiting and withering of plants, and in the birth, maturation and death of ourselves,"" Hart told a Senate panel studying music therapy.\n- REUTERS, Aug. 1, 1991.\n- Ida Goldman (90-year-old testifying at Senate hearings):\n""Before I had surgery, they told me I could never walk again. But when I sat and listened to music, I forgot all about the pain,"" said Goldman, who walked with assistance during the hearing.\n- REUTERS, Aug. 1, 1991.\n- Sen. Harry Reid:\n- Dr. Oliver Sacks (""Awakenings""):\nDr. Sacks reports that patients with neurological disorders who cannot talk or move are often able to sing, and sometimes even dance, to music. Its advocates say music therapy also can help ease the trauma of grieving, lessen depression and provide an outlet for people who are otherwise withdrawn.\n- ST. Louis Post Dispatch.\n- Dr. Clive Robbins (Nordoff-Robbins Music Therapy Clinic):\n""Almost all children respond to music. Music is an open-sesame, and if you can use it carefully and appropriately, you can reach into that child\'s potential for development."" Nordoff-Robbins uses music therapy to help 100 handicapped children learn and to relate and communicate with others.\n- Barbara Crowe (past president of the National Association for Music Therapy):\n- Oliver Sacks, M.D.:\n- Mathew Lee (Acting Director, Rusk Institute, New York):']"	['<urn:uuid:1b0b2f7c-182a-49d7-9cab-324339763a3f>', '<urn:uuid:5965de03-6b3a-4315-acc6-33d3682be80b>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T22:03:18.033296	9	63	1810
68	What's good for controlling the attack of bass notes?	The Sonnox Oxford Transient Modulator (TransMod) plugin specifically controls bass note attack - positive Ratio settings increase pick attack while negative settings reduce it. The Sound Toys Decapitator's Tone control can adjust the edge and presence of notes. Additionally, the Mod Electronics Rock Bottom bass fuzz pedal provides a blend control to fine-tune how the effect interacts with your initial note attack.	"['Kits & Effect Pedals\nAre you a DIY\'er and enjoy building your own gear? Our kits range from beginner to advanced skills, so there is something for everyone!\nKit - Fender® Stratocaster® Mid Boost Kit\nGenuine Fender® Mid Boost upgrade kit for Strats with active electronics. This circuit is used in the signature Eric Clapton and Buddy Guy strats and is commonly referred to as the Clapton Boost. The Mid Boost knob adjusts gain from 0 dB (no boost) to 25 dB (maximum boosted mids). This boost sounds great clean and adds powerful control to distortion.\nLiteIIB 18W Kit - Scratch Build Parts\nPackage of 22GuitarAmplifierPCBs.com 18 Watt LiteIIb ""Baby Will"" Parts List for Scratch Build The Baby Will is a PCB simplified version of the legendary Marshall 18 Watt 1974x circuit. The circuit we laid out on a circuit board is known as the LiteIIb. It\'s capable of tone from clean jazz, crunchy blues, to raunchy classic rock. Crank up the volume and you\'ll be in AC/DC Back In Black heaven. The magic of the LiteIIb is its simplicity.\nLiteIIB 18W Kit - Valve Junior Conversion Parts\nPackage of 20Convert your Epiphone Valve Jr. into a Lite IIb 18 Watt amp using the ""Baby Will"" kit. The Baby Will is a PCB simplified version of the legendary Marshall 18 Watt 1974x circuit. The circuit laid out on a circuit board is known as the LiteIIb. It\'s capable of tone from clean jazz, crunchy blues, to raunchy classic rock. Crank up the volume and you\'ll be in AC/DC Back In Black heaven\nPedal Kit - Mod® Electronics, Erratic Clutch Deluxe\nThe Mod® Electronics Erratic Clutch Deluxe is a unique effect pedal kit that gives you fuzzy square wave distortion as well as a monophonic sub-octave square wave using a total of only four transistors. The two signals can be used individually or mixed together for a raw and sonically rich synthy output. Full of character and quirk, this pedal will give you a truly original sound. The middle knob is the bias control. This adjusts the pulse width in the initial fuzz stage of the pedal. Set this knob to fit your pickups and playing style. The closer to the center the longer the note will sustain but with that comes more chaotic tracking for the divider. Moving it more clockwise or counterclockwise will give you more predictable note tracking on the divider with less sustain.\nPedal Kit - Mod® Electronics, Step Ladder, Input Attenuator\nThe Mod® Electronics Step Ladder is a passive input attenuator using high quality components for excellent reliability. True-bypass mode provides the maximum signal level. The attenuator pot and two toggle switches allow for flexibility in the amount of signal attenuation when not in bypass mode. Treble bleed capacitors in the circuit retain crisp high frequencies even at maximum attenuation. The Step Ladder is ideal for anything from getting a slight boost while playing single-note acoustic guitar leads or for jumping from clean to overdrive in a high-gain electric guitar amp.\nPedal Kit - Mod® Electronics, Suspended Chime, Chorus, Delay\nThe Mod® Electronics Suspended Chime has two powerful effects in one pedal - chorus and chorus/delay. The Suspended Chime features a blend knob which allows you to go from subtle to lush chorus effect in either set up. Using the toggle switch, you can add a 190 millisecond delay to the chorus introducing special depth to your tone. Varying the dry/wet mix can fatten your rhythms or produce a shimmering 12-string sound. The Suspended Chime kit comes equipped with an LED indicator and industry standard 9 volt center negative power jack. MOD® Kits are designed to give both novice and experienced musicians the opportunity to build their own amps and effects pedals. All kits come with easy to follow instructions and use point to point wiring. Pre-drilled enclosure and all parts are included. All you need to provide are hand tools, a soldering iron and solder. All effects pedals operate on a 9V battery.\nPedal Kit - Mod® Electronics, The Contortionist II, Fuzz\nThe Mod® Electronics Contortionist II kit is an all analog, high octane fuzz box. It produces fuzz with layered octave overtones similar to those produced by an electronic-bow. The frequency multiplier circuitry creates harmonics that swell and recede depending on gain setting, pick attack, neck position and pickup. This is a high gain circuit that can put out up to 2 volts and will provide ample amounts of sustain. The Contortionist II has all the features of the original Contortionist with an added tone control for a wider range of sounds, and an LED that lights up when the unit is activated.\nPedal Kit - Mod® Electronics, The Penetrator, Treble Boost\nThe Mod® Electronics Penetrator is a treble boost pedal kit with enough gain to cut straight through any mix. This pedal adds glistening harmonics that get loud enough to saturate your amp’s front end. Adjust the level knob to vary the output so there’s no mistaking when you kick this pedal in. Great for solos or when you need your guitar front and center.\nPedal Kit - Mod® Electronics, The Persuader Deluxe, Overdrive\nThe Mod® Electronics Persuader Deluxe utilizes a Darlington preamp to push cascaded triode vacuum tubes into distortion. A boost switch provides an additional layer of flexibility with added gain and a little extra bottom end. An LED indicator lights up when the Persuader Deluxe is engaged and not in true bypass mode. Operates on a 9V battery or AC adapter, which is not included. When completed, this pedal draws considerable current and battery life will be short. The Godlyke 9-volt adapter (M-PAS-S) may be purchased separately to power your new pedal.\nPedal Kit - Mod® Electronics, The Piledriver, Power Boost\nThe Mod® Electronics Piledriver is a clean-boost kit that will provide up to 38dB of gain to your guitar signal. Use it to push your preamp into overdrive or as a line driver if you are experiencing signal degradation from your effects chain. This is a simple kit and great for beginners!\nPedal Kit - Mod® Electronics, The Rattler Returns, Distortion\nTo celebrate the 10 year anniversary of Mod® Electronics Mod® kits, we are re-releasing a new and improved version of the original Mod pedal - The Rattler. This fan favorite is making its long-awaited comeback as The Rattler Returns. The Rattler Returns offers the same over-the-top fuzz as the original Rattler and features several new upgrades. The circuit has been overhauled for a wider spectrum of fuzz that is steady when used with any pickups, pedals, or amps. The Rattler Returns preserves the diode clipping, a crucial component of the classic raw fuzz/distortion in the original but also features a diode lift switch. With the lift engaged, the pedal can hit volumes that easily drive any tube or pedal with fuzzed out guitar tones. The pedal now operates on +9V, so standard DC power supplies can be used in lieu of a battery if you choose.\nPedal Kit - Mod® Electronics, The Rock Bottom, Bass Fuzz\nThe Mod® Electronics Rock Bottom is a bass fuzz kit that gives you all of the sizzle you\'d expect from a vintage fuzz without sacrificing those low frequencies. Use the blend pot to set your fuzz levels and dial in the volume to get that sound you need. This versatile pedal can be used for the sludgiest bass tones to the cleanest boost and everything in-between.\nPedal Kit - Mod® Electronics, The Saturator\nThe Mod® Electronics Saturator is an overdrive pedal kit that can cover a wide range of gain effects, transitioning smoothly from a massive clean boost to a warm and saturated distortion with the turn of a knob. The saturation is controlled with the Soak knob, which dials in the amount of symmetrical soft-clipping applied to the guitar signal. When fully counterclockwise, the signal is unaffected and clean. Rotating the knob clockwise will incrementally increase the soft-clipping, morphing from a clean signal through subtle-to-heavy saturated overdrive sounds and straight into distortion territory. The Level knob sets the overall volume of the clean or saturated output. Unity gain is at roughly 8 o\'clock. With the Soak set to clean, rotating the Level knob clockwise will increase the level to upwards of 30dB.\nPedal Kit - Mod® Electronics, The Tea Philter, T Filter\nA T-Filter Pedal Kit - the Mod® Electronics Tea Philter pedal produces a fixed wah tone allowing you to lock it in without rocking the pedal back and forth. Every guitar/wah combination has a certain sweet spot that is perfect for lead and chord work. The Tea Philter lets you dial in everything from your favorite vocal mid-range tone to thick lows. Using point-to-point wiring and easy-to-follow instructions you can have a working fixed wah pedal in a matter of hours. The effect pedal operates on a 9V battery. For a longer lasting option, a 9-volt adapter can be purchased separately.\nPedal Kit - Mod® Electronics, The Thunderdrive, Overdrive\nThe Mod® Electronics Thunderdrive is an overdrive pedal kit that will provide a signal boost at low gain settings and its own distortion at maximum gain settings. You can use it to push your preamp into overdrive or as a source of smooth distortion. Adjusting volume control and gain control results in a wide variety of tones in spite of its simple construction. This pedal is capable of overdriving the preamp section of your guitar amp or adding its own layer of distortion at lower volume. Using point to point wiring and easy to follow instructions you can have a working overdrive pedal in a matter of hours. All you need to provide is solder and a soldering iron. Uses 9 volt battery which is not included.\nPedal Kit - Mod® Electronics, The Trill\nThe Mod® Electronics Trill tremolo pedal offers classic tremolo tones in an easy to build kit. The Trill\'s speed knob provides a wide range from a very slow to rapid fire effect. The depth knob varies the ratio between effected and dry signal. Using modern, low noise transistors makes operation of The Trill quiet with no signal drop when engaged. The Trill uses true bypass foot switching so it is out of the signal path when not engaged. All you need to provide is solder and tools! Uses 9 volt battery which is not included.\nPedal Kit - Mod® Electronics, The Verb Deluxe, Digital Reverb\nThe Mod® Electronics Verb Deluxe kit is built around the Belton Digi-Log Mini Module (P-RBTDR-2H-X) for a great digital reverb sound. The Belton Digi-Log module was built as a replacement for traditional spring reverberation units. Features include both Dwell and Mix controls, allowing the dry signal to be blended with the processed signal from a just hint of reverb to deep, cavernous echoes. The dwell control adds an extra flexibility, providing a full palette of sweet reverb sound. The effect pedal operates on a 9V battery. For a longer lasting option, a 9-volt adapter can be purchased separately. The Godlyke 9-volt adapter (M-PAS-S) may be purchased separately to power your new pedal.\nPedal Kit - Mod® Electronics, Thunderdrive Deluxe, Overdrive\nAdjusting volume control and gain controls provides a wide variety of tones in spite of its simple construction. The Turbo switch provides an additional boost in volume and less internally generated distortion. It is capable of overdriving the preamp section of your guitar amp or adding its own layer of distortion at lower volume. The current draw of this pedal is very low at 1.3 milliamps, allowing for long battery life.\nPine Board AM Transmitter components - Ham Nation, Bob Heil\nKit components for the Ham Nation AM Transmitter project.\nPower Supply components - Ham Nation, Bob Heil\nKit components for the Ham Nation Power Supply project. From HAMNation episodes 290, 291, 292, 293, 294, and 295\nRadio Kit - 2 Tube Regenerative Radio\nThis radio kit will cover the AM radio band along with the SW radio band. Power for this receiver is 1.5 V DC for the filaments and 45V DC for the plate current. A D cell and five 9V transistor batteries wired in series will work to power this radio. A 2000 ohm headset or an audio amplifier is required.\nRequires two T-1T4_DF91 tubes (not included).\nRequires two T-1T4_DF91 tubes (not included).\nSnubber Kit - eliminate fuzz from amplifier\nSnubber parts kit for circuits. Used to build a ""snubber circuit"" to remove unwanted high frequencies and high voltages resulting from output transformer ringing in a guitar amplifier, without the ""brick wall"" effect of using diodes.\nStarting at $8.25', 'Featuring IK Multimedia, Sonnox, Sound Toys, Tech 21, Universal Audio and Waves\nWe all know how important bass is to holding down that bottom end. And whether it’s a real bass guitar, synth bass or a combination of both, it’s critical to get that sound pumping. In this installment of PAR Picks 6 — our monthly, recommended software column — I detail six useful plug-ins to help dial in the most killer low end.\n1 IK Multimedia Ampeg SVX Amp Modeling Plug-In\n(Left) IK Multimedia Ampeg SVX Amp Modeling Plug-In; (Right) Sonnox Oxford Transient ModulatorAmpeg and IK Multimedia worked together to re-create a software version of one of the great bass amps of all time. The interface lets you choose from four amp models including the SVT-Classic, B-15R Portaflex Flip Top, BA-500 and SVT-4PRO. Cabinet choices include the B-15R, BA-500, BXT- 410H, PB212H, SVT-410H and SVT-810E, as well as a handful of microphone models and analog modeled stomp boxes. It can work as a standalone application or within your DAW as an AU, VST or RTAS plug-in.\nMy favorite combination is the B-15R fliptop head with B-15R cabinet, which delivers a thick, classic tone. It’s nice to have Gain for some overdrive and Bass, Mid and Treble frequency knobs to customize the sound to fit the mix. The SVT sounds especially delicious on my old 70 Fender P bass, but I’ve also used it to add beef to synth bass parts and even on miked-up bass tracks that sounded less than stellar. Note that you can also use the Mix knob to blend in the SVT signal with your original bass, offering the best of both worlds.\nContact: IK Multimedia | ikmultimedia.com/ampegsvx\n2 Sonnox Oxford Transient Modulator (TransMod)\nThis one’s a little different. The Transient Modulator — or TransMod, for short — does just that: modulates transients, making your bass (or whatever you put through it) have either more or less note attack. It’s got only a few knobs and faders, so it’s super easy to use. Gain will boost or increase gain, Threshold sets the level at which the process will operate above, and Deadband can help control the processing of insignificant transients. Ratio can be set either positive or negative, whereas positive increases the attack and negative reduces it. Overshoot sets the time in which that processing occurs. So with a picked bass part for example, increasing the Ratio and Overshoot drives the pick sound. Decreasing it pulls it back and pushes the groove back in the pocket.\nWith the Overdrive at 100 percent, the process will include peak information up to 6 dB greater than max, without overloads, providing a nice harmonic fullness. RiseTime and Recovery are used to modify the timing and response of the envelope processing, but I don’t use them much. Overall, it’s a powerful effective tool for bass transient control.\nPrice: $115, $195 (Native only, TDM and Native, respectively)\nContact: Sonnox Plug-Ins | sonnoxplugins.com\n3 Sound Toys Decapitator Analog Saturation Modeler\nWhile yes, this has been in our Pick 6 before, it had to come up again when talking about bass. This analog saturation modeler can give bass tracks some nice beef and presence, without dramatically changing the natural bass sound.\nSound Toys Decapitator Analog Saturation ModelerMy most used button is the Tone knob, which is simply labeled Dark or Bright. With a quick twist, you can change the character of the bass to pull some top down or give it an edge. Then you can hit it with a little Drive, which increases gain and distortion. If you flip up the Auto-Gain switch, it will automatically turn down the output even when you increase the Drive. To take it over the top, press the Punish button, which drops an extra 20 dB of gain into your path. Watch out though, it can blast.\nAt the bottom of the plug-in are Style switches for five different saturation algorithms, including an Ampex 350 tape drive preamp, Chandler/EMI TG Channel, Neve 1057 (my favorite), Thermionic Culture Vulture triode and pentode setting. On my old Fender P bass, I use the Thump switch, which adds a few dB of low boost where the Low Cut control is set. This gives a nice, big bottom without the mudflaps. How can you go wrong with that?\nPrice: $179, $349 (Native and TDM, respectively)\nContact: Sound Toys | soundtoys.com\n4 Avid Tech 21 SansAmp PSA-1\nWhen I want distortion on a bass, this is the plug-in I turn to. In the factory presets, there’s a Bass folder with a few patches with names like Insane Bass and Extra Crunchy Bass. They are great places to start, or I have my own presets customized for different kinds of bass tones.\nAvid Tech 21 SansAmp PSA-1What makes this one different from other distortion units is the “in your face” aggressive tone that cuts through tracks, even those with big distorted guitars. Cranking the Punch knob delivers the sound of speakers just breaking up. If you then turn up the Crunch and Drive knobs, it can get savage. The Preamp and Buzz knobs both add their own nasty touch, which can be dialed in as needed. I tend to actually cut the Low knob, as it can overwhelm the mix. If I do add bass back in, I will do it with a parametric EQ, allowing me finer control.\nThe PSA-1 is not just for distorted bass. It can help add just a touch of edge to any bass part, and on clean tracks, the Punch knob does the job. Can’t go wrong with this one.\nPrice: Included with Pro Tools\nContact: Avid | avid.com\n5 Universal Audio Little Labs VOG (Voice Of God) Bass Resonance Tool\nAh, my newest favorite toy for bass sounds. It’s great because it’s not really an EQ, more like a filtering processor. Based on the 500-series hardware unit, VOG features only two knobs and three buttons.\nUniversal Audio Little Labs VOG Bass Resonance ToolThe Amplitube knob adjusts the amount of effect in the overall signal. While its range is 0-10, the numbers do not reflect a specific dB value. Increasing the level boosts the gain frequencies determined by the Frequency knob and two Center switches. The Frequency knob adjusts the actual target frequency of the effect. Frequencies above the setting are boosted and frequencies below are attenuated (by -24 dB per octave).\nThe two Center switches determine the active center frequency, with fixed choices of 40, 42, 100 and 200 Hz. The Flat button bypasses the EQ circuitry, and when it’s on (red LE D), the dry signal path of the hardware is still being modeled. For true bypass, you can use the Power switch.\nI dig the fact that within a few seconds of tweaking, your bass simply sounds better. Also, the low-end cut helps keep the bottom tight and focused, helping to punch the bass through even the smallest speakers. It’s the kind of plug-in you need to experiment with — just twist to taste. But boy, does it taste good when you get it right.\nContact: Universal Audio | uaudio.com\n6 Waves CLA-3A Compressor/Limiter\nA great emulation of the classic ’70s (well, 1969) Universal Audio solid-state unit, this “audio leveler” uses an electro-optical attenuator for program-dependent compression. That basically means you don’t have to do any work to get a killer bass sound. Set the Gain and Peak Reduction and let it go.\nWaves CLA-3A Compressor/LimiterThe CLA-3A delivers a classic, thick compression (approximately 3:1) to bass, but you can also set it to limit (at approximately 100:1 ratio). The Peak Reduction knob sets the amount of desired compression, and the Gain delivers makeup.\nAt the bottom of the plug-in sits the HiFreq, which increases the voltage amplifier gain in the peak-reduction circuit, leaving frequencies below 1 kHz unaffected. There are Analog buttons for controlling the noise floor and hum from the original power supplies, with a choice of 50 Hz, 60 Hz and Off.\nPrice: $150, $200 (Native and TDM, respectively)\nContact: Waves | waves.com']"	['<urn:uuid:433e091b-f61b-41c3-a7f0-550d605ceb31>', '<urn:uuid:9c3434a6-7f4a-4746-8487-99e934bf77a1>']	open-ended	with-premise	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T22:03:18.033296	9	62	3443
69	cultural significance traditional food preparation methods inuit communities historical contemporary practices	Historically, Inuit food preparation involved using natural materials as cooking utensils and cooking over open fires, with preservation methods including smoking, drying, and encasing food in melted animal fat or whale blubber. The harvest and consumption of country foods remain fundamental to Inuit cultural identity and food security. Traditional practices included utilizing every part of hunted animals for food, clothing, or shelter. In contemporary times, while Inuit have incorporated processed and convenience foods, they maintain their hospitable customs of having stews or teas available for guests. Traditional feasts are still held to celebrate harvests or important life events, such as large salmon catches or a youth's first seal or caribou hunt. The preservation of these food practices is celebrated through modern events like potlatches and National Aboriginal Day festivities.	"['Public Health Nutrition: page 1 of 13\nDietary sources of energy and nutrients in the contemporary diet of Inuit adults: results from the 2007–08 Inuit Health Survey Tiff-Annie Kenny1, Xue Feng Hu1, Harriet V Kuhnlein2,3, Sonia D Wesche4 and Hing Man Chan1,* 1\nDepartment of Biology, University of Ottawa, 30 Marie Curie, Ottawa, ON, Canada, K1N 6N5: 2Centre for Indigenous Peoples’ Nutrition and Environment, McGill University, Montreal, QC, Canada: 3School of Dietetics and Human Nutrition, McGill University, Montreal, QC, Canada: 4Department of Geography, Environment and Geomatics, University of Ottawa, Ottawa, ON, Canada\nPublic Health Nutrition\nSubmitted 20 June 2017: Final revision received 7 November 2017: Accepted 16 November 2017\nAbstract Objective: To characterize the major components of the contemporary Inuit diet and identify the primary sources of energy and essential nutrients. Design: Dietary data were derived from the 24 h recall collected by the Inuit Health Survey (IHS) from 2007 to 2008. The population proportion method was used to determine the percentage contribution of each group. Unique food items/ preparations (ninety-three country foods and 1591 market foods) were classiﬁed into eight country food groups and forty-one market food groups. Nutrient composition of each food item was obtained from the Canadian Nutrient File. Setting: Thirty-six communities across three Inuit regions of northern Canada. Subjects: A representative sample (n 2095) of non-pregnant Inuit adults (≥18 years), selected through stratiﬁed random sampling. Results: Despite their modest contribution to total energy intake (6·4–19·6 %, by region) country foods represented a major source of protein (23–52 %), Fe (28–54 %), niacin (24–52 %) and vitamins D (up to 73 %), B6 (18–55 %) and B12 (50–82 %). By contrast, the three most popular energy-yielding market foods (i.e. sweetened beverages, added sugar and bread) collectively contributed approximately 20 % of total energy, while contributing minimally to most micronutrients. A notable exception was the contribution of these foods to Ca (13–21 %) and vitamins E (17–35 %) and C (as much as 50 %). Solid fruits were consumed by less than 25 % of participants while vegetables were reported by 38–59 % of respondents. Conclusions: Country foods remain a critical dimension of the contemporary Inuit diet.\nFrom the Arctic to the South Paciﬁc, Indigenous Peoples have experienced a rapid nutrition transition(1,2) characterized by the adoption of a ‘western’ diet (i.e. high in saturated fats, sugar and processed foods) and the decline of traditional/subsistence-based ways of life(3–6). Across the globe, this dietary shift has been paralleled by an increase in the prevalence of obesity, diabetes and other diet-related chronic diseases(3–6). Inuit are a traditionally semi-nomadic Indigenous Peoples residing across the circumpolar north(7). In the latter half of the 20th century, Inuit endured signiﬁcant lifestyle changes, involving the settlement into permanent communities, the development of a wage economy and the introduction of market foods to remote northern communities(8). The diet transition among Inuit is characterized by the decreased consumption of ‘country foods’ (foods harvested from northern *Corresponding author: Email [email protected]\nKeywords: Inuit Indigenous People Arctic Country foods Mixed diet\necosystems, through cultural practices, traditions and detailed environmental knowledge) and the increased presence of ‘market foods’ (foods shipped to northern communities from the south and purchased in stores) in the diet(5,9,10). The harvest and consumption of country foods remain fundamental to Inuit cultural identity(11–13), food security and dietary adequacy(14–16). However, the transition towards higher intakes of market foods has led to excessive intakes of energy, carbohydrates and fat, coupled with inadequate intakes of several micronutrients (i.e. dietary ﬁbre, Ca, folate and vitamins A, D and E)(5,9,10,17,18). This transition is associated with high rates of food insecurity(10), has been linked to increasing incidence of obesity, and bears important risks for the development of diabetes and chronic disease(5,19). © The Authors 2018\nPublic Health Nutrition\nInuit experience moderate and/or severe food insecurity at almost four times the rate of non-Aboriginal Canadians (27 % relative to 7 %, based on data from males aged 12 years or older)(20). With few exceptions, research on the Inuit diet in Canada generally consists of community-level studies, involving small sample sizes. While there are unique qualities that deﬁne Inuit communities at the local level, food system disturbances (e.g. environmental change) are often expressed, and modelled by scientists, at larger scales(21). Likewise, strategies and interventions to improve food security and nutrition in Inuit communities may necessitate broader regional, territorial or federal support (see, for example, the federally administered Nutrition North Canada Program(22) and the Nunavut Food Security Strategy and Action Plan(23)). Kuhnlein et al. (2008) provide a comprehensive description of dietary adequacy in three populations of Arctic Indigenous adults (n 3329) across Canada between 1993 and 1999(24). More recently, several authors have reported diet, nutrition and food security results from the 2007–08 International Polar Year Inuit Health Survey (IHS)(10,19,25–27). The IHS was developed in response to the disparity in available information regarding the health status of Inuit residing across the Canadian Inuit Nunangat (homeland of Inuit of Canada). The IHS collected comprehensive baseline data for 2595 Inuit adults in thirty-six communities, spanning three jurisdictions of Inuit\nT-A Kenny et al.\nNunangat (Nunatsiavut, Nunavut and the Inuvialuit Settlement Region (ISR); Fig. 1). Health status for Inuit in Nunavik (the fourth Inuit region in Canada) was assessed during the 2004 Qanuippitaa? How are we? Nunavik Health Survey(28). The purpose of the present study was to describe region-level population diets for a large sample of Inuit adults across the Canadian north. Speciﬁcally, our objectives were to: (i) identify principal dietary sources of energy and selected nutrients; and (ii) examine the relative contribution of country foods and market foods to energy and nutrient intakes.\nMethods Study design and sample Dietary data were derived from the Canadian International Polar Year IHS. The IHS, conducted between the late summer/autumn of 2007 and 2008, collected comprehensive baseline data about the health and living conditions of Inuit adults across three Inuit regions (Nunatsiavut, Nunavut and the ISR) spanning the Canadian north (latitude of 54°10′N to 76°25′N). Complete methodology and design for the 2007–08 IHS have been published elsewhere(29). The survey was cross-sectional, employing a stratiﬁed random sampling of households in thirty-three coastal\nFig. 1 Map of the participating Inuit regions* of the 2007–08 Inuit Health Survey. *Nunavut is comprised of the Kitikmeot, Kivalliq and Qikiqtaaluk regions\nNutrient sources in the Inuit diet\nPublic Health Nutrition\ncommunities and three inland communities (Fig. 1). A total of 2796 households were randomly selected to participate. From the households, non-pregnant Inuit adults aged 18 years or older were eligible to participate. Dietary assessment As described elsewhere(10,15), dietary assessments were conducted in-person by trained interviewers in English and Inuit Languages. Diet was assessed by administering a single 24 h dietary recall (beginning at midnight and ending at midnight), based on an adapted form of the US Department of Agriculture’s (USDA) Automated MultiplePass method(30). Three-dimensional graduated food model kits(31) were available to aid participants in the estimation of portion sizes. Due to survey logistical constraints, a single 24 h recall was collected from each participant. While this method does not capture interindividual variations in dietary intake, it is appropriate for estimating population mean intakes(32). Dietary data were entered using CANDAT Software (Godin London Inc.). A total of 1591 (including ninety-three country foods) unique food and beverage (hereafter referred to as ‘food(s)’) items and/or preparations, corresponding to unique food codes in the Canadian Nutrient File (CNF), were reported in the dietary recalls. Alcoholic beverages (twelve unique items), which are legislatively prohibited in some Inuit communities, were excluded from all analyses. All foods and beverages reported as consumed in the IHS were coded hierarchically, by item similarity and food group (major and sub groups; see online supplementary material, Supplemental Tables 1 and 2). Similar food items in each recall were collapsed into a single item (e.g. ‘potato chips’ aggregated all potato chips of various seasonings) and compiled as a daily sum for the item (g/person per d). Food groups were based on the classiﬁcation scheme of the USDA’s Food and Nutrient Database for Dietary Studies (FNDDS) 5.0(33), with some exceptions to reﬂect culinary usage (e.g. butter was categorized as a ‘fat and oil’ as opposed to a dairy product) and the dietary habits of Inuit. Due to database limitations, foods reported as mixed dishes/recipes (e.g. pizza, sandwiches) could not be disaggregated into component ingredients. Thus, a ‘mixed dishes’ grouping was included and classiﬁed according to the dish’s primary ingredients (e.g. primarily meat dishes; primarily grain dishes). Potatoes were excluded from the vegetables grouping and included with ‘grains and starches’. Efforts were made to group market foods based on nutritional similarities (e.g. high-sugar beverages, such as fruit drinks and cola, were collectively grouped). However, food fortiﬁcation practices in Canada(34), such as the mandatory fortiﬁcation of ﬂour (with thiamin, riboﬂavin, niacin, folic acid and Fe), as well as the fortiﬁcation of fruit-ﬂavoured drinks with vitamin C (mandatory), folic acid (voluntary) and Fe (voluntary), can complicate these relationships.\nCountry foods were classiﬁed by species (e.g. caribou, beluga whale) and body part (e.g. meat, fats, organs). Bannock, a homemade biscuit (often considered traditional), was included with market grains and starches. The importance of food items/subgroups to total diet was characterized by: (i) mean population consumption (averaged for all participants, by region); and (ii) the percentage of recalls reporting consumption of a particular food. Dietary sources of energy and nutrients The CNF national food composition database(35) was used to calculate energy and nutrient intakes. Nutrient composition information for foods not included in the CNF was available from an additional in-house food ﬁle (McGill School of Dietetics and Human Nutrition), as described elsewhere(10). Missing nutrient values for all foods were imputed following procedures described by Schakel et al.(36). The USDA FNDDS 5.0 was used to supplement missing nutrient information for market foods(33,37). Missing nutrient values for country foods (not in the FNDDS) were imputed and/or calculated manually, based on similar food items (considering the species, body part and preparation method). Analysis Data management and statistical analyses were performed with SAS statistical software package version 9.4. The percentage contribution of each food subgroup to total energy and nutrient intakes was calculated for the entire population, according to the population proportion method(38,39). Nutrients analysed include energy, selected macronutrients (protein, fat, carbohydrates, total sugar, saturated fat), dietary ﬁbre, vitamins (A, C, D, E, B12, thiamin, riboﬂavin) and minerals (Ca, Fe, Mg, Zn, Cu, Se, Na). Dietary supplements, which were consumed by 65 % of recalls), sweetened beverages (44–64 % of recalls) and bread (50–80 % of recalls; Table 3). Consumption of dairy products was reported by the majority of participants (62–72 %, by region), but region-level differences were observed in the consumption of ﬂuid milk v. powdered milk and non-dairy coffee whitener, with higher consumption of the former in Nunatsiavut. Store-bought meats (including other proteins) were reported by 68–90 % of participants, according to region. Solid fruits were consumed by less than 25 % of participants across all regions (Table 3), while solid vegetables (onions, carrots and other root vegetables) were reported by 38–59 % of respondents on the day prior to the interview.\nTop dietary sources of energy and nutrients The top ten food sources of dietary energy and nutrients, including the percentage contribution of each food to total intake, are presented by region in Tables 4–6.\nContribution of country foods to dietary intake The contribution of country foods to total diet energy (TDE) for individuals ranged between 0 % TDE (43·5 % of all respondents) to over 50 % TDE (10·0 % of all respondents), with less than 1 % of respondents consuming 90 % or more of TDE from country foods (data not presented). The contribution of country foods to TDE for the population differed by region, and was stratiﬁed by sex and age (Table 1). In general, the contribution of country foods to TDE was lowest in Nunatsiavut, particularly among younger (75 %) and saturated fat (>80 %) was provided by market foods (Table 4); however, country foods were a major source of cholesterol (19·6–47·3 %; Fig. 2). Country foods collectively contributed less than 20–25 % of total MUFA in Nunavut and the ISR (Fig. 2), with beluga ranking as the principal source of MUFA in both regions (7–8 % of total intake; data not presented). Country foods contributed 18·5 % of PUFA in Nunavut and 13·6 % in the ISR (Fig. 2), which were\nNutrient sources in the Inuit diet\nTable 2 Mean* consumption (g/person per d) of country foods by adult Inuit respondents of the 24 h recall (n 2095), by region; Inuit Health Survey, 2007–2008 Nunavut (n 1568) Country food By part Country food – meat† Country food – fat‡ Country food – organs§ By animal/species Birds Fish (country food only) Caribou Other land mammals Seal and walrus Beluga whale Narwhal Berries\nInuvialuit Settlement Region (n 267)\nNunatsiavut (n 260)\n% of recalls\n% of recalls\n% of recalls\n47·6 21·6 2·1\n116·4 45·7 2·0\n194 134 25\n34·8 12·4 2·2\n82·4 25·7 1·9\n156 142 16\n24·6 1·2 1·5\n49·6 0·3 0·7\n124 4 7\n1·5 14·3 39·3 1·5 9·1 11·0 4·3 3·1\n3·1 42·9 97·4 2·8 18·4 30·6 12·0 6·2\n29 143 187 28 85 116 72 44\n5·2 22·1 29·2 1·1 1·1 9·7 0·7 0·7\n11·7 61·8 68·1 4·9 0·8 23·4 1·1 2·2\n68 153 144 50 10 141 13 24\n5·0 7·3 18·1 0·4 1·9 0·0 0·0 1·5\n10·4 20·1 36·0 0·3 3·9 0·0 0·0 0·2\n59 104 109 5 33 0 0 2\nPublic Health Nutrition\n*Population mean (consumers and non-consumers), by region. †Total meat does not include fish. ‡Total fat includes muktuk (whale blubber and skin). §Total organs includes bone marrow and offal.\nderived principally from caribou and local ﬁsh. Country foods did not contribute signiﬁcantly to carbohydrates (', '1 GEOGRAPHIC SETTING AND ENVIRONMENT\nThe phrases ""Native Canadians"" or ""Aboriginals"" describe the descendants of the people who were living in what is modern-day Canada before European colonists, explorers, and traders arrived in the 1600s. Giving labels to these groups is complicated by emotional and historical issues. Aboriginals inhabited all regions of Canada and the United States, and dozens of tribal groups, lived, hunted, fished, and foraged (gathered native plants) all across North America. The provinces of modern-day Canada obviously did not exist when the Europeans arrived on the east coast of Canada.\nThe Inuit inhabit the northernmost parts of Canada. On April 1, 1999, Nunavut (pronounced NOON-ah-voot) became Canada\'s newest territory, created from about half the land that made up the Northwest Territories.\n2 HISTORY AND FOOD\nIn general, most Canadians agree that the approximately 800,000 Aboriginal Canadians counted by the 1996 Census of Population may be identified as belonging to one of three groups: First Nations (554,000), Métis (210,000), and Inuit (Innu, 40,000). The First Nations people are members of the approximately 50 recognized ""First Nations"" or tribal groups in Canada, and they inhabit all parts of Canada. The Métis are descendants of the intermarriages that occurred between the men employed by the early European fur trading companies (Hudson\'s Bay Company and Northwest Fur Company) and Native Canadian women.\nThe Inuit are the descendants of the Thule people who migrated from the Canadian arctic 700 to 800 years ago. They have been inhabiting the territory of modern Canada for thousands of years. They were historically hunters and fishers. Because of the\nPemmican is a nutritious, high calorie food that can be prepared in quantities and stored. The French and English explorers, trappers, and traders, bought large quantities of pemmican from the Aboriginals, and even learned to make pemmican. Pemmican would be sealed inside an animal skin or stomach cavity to preserve it. Europeans carried these pemmican stores on long furtrading expeditions.\n- 1 package beef jerky\n- 1 cup dried berries, such as dried blueberries, cranberries, or cherries\n- 1 cup chopped nuts or sunflower seeds\n- ¼ cup beef suet or vegetable shortening\n- Honey to taste (1 to 3 teaspoons)\n- 12-cup muffin tin\n- Line muffin cups with paper liners (or grease cups well).\n- Grind or chop beef jerky into confettisize pieces to make about 1 cup. Melt suet or shortening in a saucepan.\n- Remove from heat, stir in beef jerky, dried berries, and seeds. Stir in honey.\n- Spoon about ¼ cup of the pemmican mixture into each muffin cup. Press down firmly to make a cake, smoothing the top.\n- Refrigerate until well set.\n3 FOODS OF NATIVE CANADIANS\nThe traditional diet of Aboriginal people was made up of the animals and plants found on the land and in the sea around them. Seal, whale, buffalo, caribou, walrus, polar bear, arctic hare (rabbit), all kinds of fish and many species of bird were hunted or fished. Raw blubber (fat) was enjoyed or mixed with meat or berries. Every part of the animal was consumed or used to make clothing or shelter. Because the foods were eaten raw or with minimal processing, the Aboriginal people were generally well nourished.\nModern-day First Nations, Métis, and Inuit people have added processed foods and convenience foods to their traditional diet, and are experiencing the health problems that come from consumption of foods rich in sugar and additives (such as tooth decay and obesity).\nTheir traditional diet was nutritious and high in calories, but the calories were needed to help keep their bodies warm through the long, frigid winters. During the short summers, Aboriginals (mainly the women) would plant small gardens and gather wild berries and seeds. Corn, beans, and squash were common vegetables grown in the small gardens of Manitoba and Alberta. These vegetables were often simmered to make soups or stews, such as Three Sister Soup (the ""sisters"" are corn, beans, and squash).\nSnacks were often enjoyed right on the trail—a few berries or dried seeds plucked from the wild plants. Some were eaten right on the spot, and some may have been carried home to share or save for another day.\nSaskatoon Berry Snack\nSaskatoon berries, similar to blueberries, have been picked and eaten in the wild by Aboriginal Canadians for centuries. In the late twentieth century, commercial fruit growers began planting crops of these tasty berries to sell to grocery stores.\n- 1 pint berries (may be blueberries, raspberries, strawberries, or other fresh berries or\n- 1 package dried berries (blueberries, cranberries, or other berries)\n- If using fresh berries, rinse them under running water.\n- Divide berries into several waxed paper bags or plastic baggies. Carry these along for snacks during the day or to share with a friend.\nServes 8 to 10.\nThree Sisters Soup\n- 3 cans chicken broth\n- 2 cups frozen corn, thawed\n- 1 cup green beans or yellow wax beans, washed and ends trimmed off\n- 1½ cups of butternut squash (or pumpkin)\n- 2 bay leaves\n- Salt and pepper to taste\n- Optional spices: ½ teaspoon red pepper flakes or 1 teaspoon each fresh (or ½ teaspoon each dried) parsley, basil, and oregano\n- Pour the chicken broth into a large saucepan or kettle. Heat until the broth begins to boil.\n- Add the corn, beans, squash, and bay leaves.\n- Lower heat and simmer for 45 minutes.\n- Add optional spices if desired, and simmer 15 more minutes.\n- Remove the bay leaves, and transfer the soup in batches to the blender to puree if desired. Serve with bannock (bread).\nServes 8 to 10.\nAboriginal peoples who lived on the prairies of western Canada consumed buffalo (and used buffalo skins for clothing and shelter). In central Canada, Ojibwa people would gather wild rice from the waters of Ontario and Manitoba, allowing it to dry and then roasting it. The fur traders, who came into contact with Aboriginal peoples all across Canada, introduced a bread similar to the Scottish scone. It became known as bannock . Bannock may be baked (Aboriginal people would lay it on hot rocks near a campfire) or twisted onto a stick and cooked over hot coals.\nBannock may be baked in the oven or over a charcoal or open fire (recipe for Bannock on a Stick follows).\n- 4 cups all-purpose flour\n- 1 Tablespoon sugar\n- 2 Tablespoons baking powder\n- ½ teaspoon salt\n- 2 cups milk (or water)\n- Combine flour, baking powder, sugar, and salt in a large mixing bowl.\n- Measure the milk (or water) and add it to the flour mixture, stirring with a fork to combine. A dough should form. If the mixture seems too dry and crumbly, add more liquid, one Tablespoon at a time.\n- Turn the dough out onto a surface lightly coated with flour. Knead for about 3 minutes. (To knead, press down the dough, turn it clockwise, fold it in half and press it down. Repeat.)\n- Preheat oven to 350°F (180°C).\n- Pat the dough into a circle about ¾-inch thick. Transfer the dough to a well-greased cookie sheet. Prick the surface of the dough all over with a fork.\n- Bake about 20 to 30 minutes, or until golden brown.\nBannock on a Stick\n- Prepare Bannock dough (see preceding recipe). Have ready several sticks, 3- to 4-feet in length.\n- Divide the dough into balls slightly larger than golf balls. Shape each ball into a rope about 8 inches long by rolling it between the hands.\n- Wrap each dough rope around a stick. Hole the dough over a a bed of red hot coals (charcoal, wood, or gas grill flame set at medium.) Turn the stick frequently to bake the dough evenly.\nServes 10 to 12.\nMan-O-Min (Ojibwa Wild Rice)\n- 1 cup wild rice\n- 4 cups\n- 1 teaspoon salt\n- Wash the wild rice in a colander or bowl, changing the water two or three times.\n- Measure water into a large saucepan; add salt. Heat the water to boiling.\n- Slowly add the rinsed rice to the boiling water. Lower heat to medium and simmer the rice, undisturbed, for about 40 minutes. (Do not stir the rice.)\n- The rice grains will swell to four times their original size.\n- Serve hot or at room temperature.\nAboriginal peoples living in the region of modern-day British Columbia enjoyed foods such as salmon cooked over an open fire, a popular modern-day delicacy.\n4 FOOD FOR RELIGIOUS AND HOLIDAY CELEBRATIONS\nThe traditional feasts held by Aboriginal peoples usually revolved around a harvest, or seasonal excess of food. For example, if there was a large salmon catch, a feast would be held. When a youth killed his first seal or caribou, a celebration feast might be held.\nIn western Canada, Aboriginal peoples held ceremonial parties called potlatches to celebrate the birth of a child, a young woman reaching puberty, or the marriage of a son. Modern-day potlatches are held to celebrate and preserve Aboriginal culture.\nIn 1996 an annual National Aboriginal Day was proclaimed, to be celebrated on June 21 each year. There is no specific menu associated with the celebration of this holiday, but many traditional foods, such as salmon, wild rice, and even buffalo, are enjoyed during the festivities staged by many of the Aboriginal groups.\nWild Rice Cakes\n- 1 cup wild rice\n- 4 cups water\n- 1 teaspoon salt\n- ¼ cup cornmeal\n- 1–2 Tablespoons bacon drippings (or butter)\n- Rinse the wild rice in a sieve under cold running water and drain.\n- Measure the 4 cups of water into a saucepan and add rice and the salt. Heat until the water boils, reduce heat, and simmer for about 30 minutes. The rice should be tender but not soft.\n- Add the cornmeal slowly, stirring constantly with a wooden spoon and cook for 3 or 4 minutes. Remove from the heat.\n- Melt bacon drippings (or butter) in a skillet.\n- Shape the rice mixture into pattie-like cakes about 1½ inches in diameter.\n- Sauté the patties until they are brown on one side (about 5 minutes). Carefully turn the cakes over to brown the other side. Drain on paper towels.\nServes 12; may be served hot or at room temperature.\n5 MEALTIME CUSTOMS\nAboriginal peoples are hospitable, and always have stews or teas simmering and available to serve to guests. Historically, cooking utensils were fashioned from natural materials and cooking was done over an open fire. Food preservation methods included smoking, drying, and encasing in melted animal fat or whale blubber.\n6 POLITICS, ECONOMICS, AND NUTRITION\nThe substitution of packaged and fast foods for the nutrient-rich traditional Aboriginal diet has contributed to health problems among Aboriginal children.\nThe Canadian Government\'s Minister of Indian Affairs and Northern Development continues to work to address the concerns of the Aboriginal peoples. Addressing historic wrongs and developing modern-day programs is challenging, and representatives of the government and the First Nations, Métis, and Inuit peoples are striving to communicate and design goals to meet the needs of all Canadians.\n7 FURTHER STUDY\nAlexander, Bryan and Cherry Alexander. What Do We Know About the Inuit. New York: Peter Bedrick Books, 1995.\nBruemmer, Fred. Arctic Memories: Living With the Inuit. Toronto: Key Porter Books, 1993.\nHarper, Judith E. Inuit. Mankato, MN: Smart Apple Media, 1999.\nJackson, John C. Children of the Fur Trade: Forgotten Métis of the Pacific Northwest. Missoula, MT: Mountain Press, 1996.\nLutz, Norma Jean. Nunavut. Philadelphia: Chelsea House, 2000.\nMercredi, Morningstar. Fort Chipewyan Homecoming: A Journey to Native Canada. Minneapolis, MN: Lerner Publications, 1997\nMorrison, David A. Arctic Hunters: The Inuit and Diamond Jenness. Hull, Quebec: Canadian Museum of Civilization, 1992.\nSantella, Andrew. The Inuit. New York: Children\'s Press, 2001.\nAboriginal Awareness Week. [Online] Available http://www.aboriginalawarenessweek.gc.ca/ (accessed August 17, 2001).\nLiboiron, Henri and Bob St-Cyr. ""Making Pemmican."" [Online] Available http://collections.ic.gc.ca/notukeu/pemmican_e.htm (accessed April 17, 2001).\nMétis Nation. [Online] Available http://www.metisnation.ca (accessed August 17, 2001).\nMétis Nation Ontario. [Online] Available http://www.metisnation.org (accessed August 17, 2001).\nNational Aboriginal Day. [Online] Available http://www.ainc-inac.gc.ca/nad (accessed August 17, 2001).']"	['<urn:uuid:3a00dd72-0c60-4d64-91c4-5800f28f74c1>', '<urn:uuid:ad1374c6-5531-4270-93d9-d24db1809fa7>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T22:03:18.033296	11	129	4315
70	I've been struggling with my exam preparation and need some effective tips - could someone tell me the best ways to prepare for the emergency medical services certification?	You should keep and highlight your textbooks, take practice tests repeatedly, review AHA Guidelines for CPR, study with groups, focus on understanding rather than memorizing, avoid cramming, and don't worry about learning the computer system - just focus on the EMT material.	['Everything you need to know about completing your NREMT certification\nSo you’ve decided to become an EMT. Great! You’re now one EMT boot camp and exam away from an exciting new career as an emergency medical technician. Unfortunately, deciding to become an EMT is as far as some people get, often because they let themselves become intimidated by the idea of taking a test.\nDon’t let that be you! There’s no reason a test should stand between you and your dream, and we’re here to help ensure it doesn’t.\nWhen it comes to taking the NREMT (National Registry of Emergency Medical Technicians) exam, there’s no need to panic. Your EMT training program (such as our 14-day EMT Bootcamp) should more than prepare you for the test. But to help make sure that you’re ready for all aspects of the exam, we’ve compiled everything you need to know about completing your NREMT certification.\nWhat is the EMT test?\nThere are actually two parts of the NREMT exam—a cognitive (written) test and a psychomotor test. Both are required to apply for your final EMT certification.\nThe cognitive portion of the EMT test assesses your general knowledge of emergency medical care across a full spectrum of topics, including:\n- Respiration & Ventilation\n- Cardiology & Resuscitation\n- EMS Operations\nThe psychomotor test, on the other hand, measures your ability to physically complete the responsibilities of an EMT. This test is normally taken as part of your EMT training course—for example, students in our 14-Day EMT Bootcamp take the psychomotor exam on the final day of training.\nThis portion of the EMT test will assess your skills regarding:\n- Assessment/management of trauma & medical patients\n- Cardiac arrest management\n- Bag-valve-mask ventilation of an apneic patient\n- Spinal immobilization (seated and supine patients)\n- Long bone fracture immobilization\n- Joint dislocation immobilization\n- Traction splinting\n- Bleeding control / shock management\n- Upper airway adjuncts and suction\n- Mouth-to-mouth ventilation with supplemental oxygen & supplemental oxygen administration to a breathing patient\nYou can find a complete checklist of the NREMT required skills here.\nSome of that may sound complicated—and it can be. After all, these are the skills EMTs must master in order to save lives in a wide variety of situations. But by the time you finish your EMT training course, you should know these steps backward and forward.\nHow many questions are on the NREMT exam?\nWhen you sit down to take your EMT written exam, you can expect anywhere from 70 to 120 questions. 10 of these questions (called “pilot questions”) won’t impact your final score and are simply there to help test possible questions for future versions of the exam. You probably won’t be able to tell which questions are pilot questions, though, so don’t waste time guessing and give every question your best shot.\nHow long is the EMT test?\nIn the end, the length of the test is up to you. You’ll be given two hours to complete the exam, but many test takers finish far earlier while others work until the last minute. The important thing is to budget your time, work at your own pace, and if you’re stumped on a question, move on and come back to it later.\nAnd don’t panic—two hours is plenty of time. In fact, less than 1% of test takers are unable to finish the exam according to the NREMT.\nHow to study for the EMT exam?\n- Keep Your Textbooks And Workbooks – Highlight definitions and any specifics that stand out, then read back over the highlighted portions until you feel confident enough in the concepts that you could teach someone else.\n- Take the Practice Tests – Many EMT courses provide practice exams. Take these as often as you can and retake them until you are happy with your score.\n- AHA Guidelines – Review the AHA Guidelines for CPR and Emergency Cardiovascular Care. This absolutely will be on the test.\n- Study with a Group – Your classmates might have picked up on something you missed.\n- Understand – Memorizing material is helpful, but truly understanding the material is even more so. Try and learn a concept so well that you could explain it to someone who has never been in an EMT class in their lives.\n- Don’t Cram – Spread out your study periods, get plenty of breaks and rest in-between, and don’t exhaust yourself with a cram session the night before the test. Take your time, do it right, and you’ll have your best shot at acing the exam.\n- Ignore the Computer – You’ll be taking your test on a computer, but the exam process itself is very simple—even for those who aren’t computer-friendly. Don’t worry about trying to learn the exam system or how the software algorithms work—just learn the EMT material.\nWhat score do you need to pass the EMT test?\nIt’s a fact, no matter how badly we want one, a perfect score on any exam is rare. You’re going to miss a question here or there but it’s not the end of the world—do your best, but at the end of the day, the most important thing is just that you pass.\nIn order to pass the NREMT cognitive exam, you’ll need a 70% or higher as your final score.*\nHow do I register for the EMT exam?\nRegistration for the NREMT exam starts on the www.nremt.org website. Here, you’ll go through the following steps:\n- Create an account\n- Update your personal profile and contact information\n- Create a new application\n- Pay the application fee\n- Verify you’ve been approved to test\n- Print your ATT (exam approval) letter\n- Follow the scheduling instructions on your ATT letter within 90 days\nThe NREMT has created a handy checklist (with additional helpful links) here.\nRemember, if you miss your exam date, you’ll have to re-apply and pay another application fee!\nHow hard is the NREMT exam?\nThe EMT exam is passable… but don’t make the mistake of thinking you can skate by with minimal effort. According to NREMT.org, 72% of first time exam takers passed the NREMT exam in 2019—which means that nearly three of every ten test takers scored lower than a 70% and failed.*\nThe difficulty of the EMT exam also depends on your grasp of the subject materials. The NREMT exam uses the CAT model (computerized adaptive testing), which means the test will do its best to identify any areas of weakness. There’s no skipping questions and no going back to answer missed questions, and if you do miss a question, the CAT is probably going to rephrase the question and ask it again.\nSo, if you feel a little shaky in any areas, dedicate some extra study time to fully understanding them. The majority of test takers will pass the exam on the first attempt, but be sure to study hard (and pay close attention during your EMT training program) to make sure you’re in that group.\nWhat is on the EMT exam?\nAccording to NREMT.org, the average exam spans the following topics:\nBecause the exam operates on the CAT model, the exact number of questions under any given topic is going to depend entirely on you. The computer wants to be absolutely certain whether you know (or don’t know) a subject, so if you miss a question, you’ll probably see another version of the same question pop up later.\nAs mentioned previously, there will also be 10 “pilot questions” that will not impact your score, although you probably won’t know which questions are the pilot questions.\nAnother helpful tip: the test will primarily focus on adult patient care—85% to be exact—while the remaining 15% will focus on pediatric care.\nCan I bring anything with me to the EMT exam?\nPersonal items are not allowed within the testing room. This includes (but is not limited to) mobile devices, watches or smart watches, weapons, hats, bags, jackets, notes, pens, pencils, wallets, or purses. And to make certain none of these make it into the room, be aware that the testing administrators are authorized to pat you down, ask you to pull your hair back, or ask you to roll up your sleeves.\nThis may sound extreme, but ask yourself—if you were injured and your life depended on the EMT giving you medical treatment, would you want someone who had cheated to pass the NREMT? None of us would. Test administrators take their job very seriously, because the stakes are literally life and death once the test is complete.\nWhere can I take the NREMT exam?\nNREMT exams are given at Pearson VUE testing centers across the country. Your exact testing location and scheduled exam time will be given only after you complete your NREMT registration and pay your registration fee.\nAfter registration, you’ll print your ATT (Authorization To Test), which will include all the important information about where to go and when to be there.\nREMEMBER, the ATT is only valid for 90 days and no extensions are available, so schedule your test quickly after registering.\nThe ATT will also tell you what identification documents to bring with you, so read carefully.\nHow much does the NREMT exam cost?\nThe EMT exam registration fee is $80 per attempt. However, if you enroll in our EMT Bootcamp program, the fee for your initial exam (and a second exam attempt, if needed) is included in course tuition.\nHow many times can you take the NREMT exam?\nIt’s tough to think about, but some candidates just don’t pass on their first attempt. Remember, three out of every ten won’t get above that necessary 70% grade on their first try.* If this happens to you, don’t panic and don’t get discouraged! You can try again!\nYou can apply to retake the EMT exam 15 days after your last exam, and you have 6 chances total to pass the NREMT cognitive. After three failures, you will have to pass a remedial training course before trying again.\nWhen will I know if I passed the EMT cognitive exam?\nYour NREMT official scores aren’t tallied at the testing facility (so don’t bother bugging your test administrator for a sneak peek at your results… they don’t have them). The NREMT receives your exam electronically and typically will have the final score within two business days.\nAfter two days, log onto your NREMT account and click on “View All Certification Applications”. If you’ve passed, congratulations! You’re now free to apply for EMT certification under a local agency. You won’t see any additional information about your exam results.\nIf you failed, the NREMT breaks down the exam for you, highlighting the different subject areas as Above Passing, Near Passing, or Below Passing to help you figure out where you need to study before retesting. Remember, you can try again in 15 days.\nWhat happens after I pass the NREMT exam?\nFirst of all, congratulations! Now that you’ve passed the NREMT exam, you can officially apply to a local emergency services agency. In other words, it’s time to put on that uniform and get to work saving lives.\nBut you’re not completely done with tests. You’ll need to recertify every two years—which means taking the NREMT recertification exam or showing that you’ve completed a specific number of continuing education hours.\nGraduates of the Unitek 14-Day EMT Boot Camp have a leg up in the recertification department and can attend future EMT bootcamps for free (provided you have your text book, uniform, and have met all school policies).\nThat’s it! You now know the ins and outs of becoming a certified Emergency Medical Technician, passing your NREMT exam, and maintaining that certification for years to come. And as always, if you run into any questions not answered here, our Unitek staff is always standing by to help. Give us a call or send us a message and we’ll be happy to get you whatever information you need.\n*According to the NREMT, this percentage is subject to review and potentially changes every three years.']	['<urn:uuid:4127cdef-8747-4e1a-ac89-2c1317137b13>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T22:03:18.033296	28	42	2016
71	compression factor djvu compared jpeg gif pdf black white documents	For black-and-white documents, DjVu compression produces files that are 10-20 times smaller than JPEG files, five times smaller than GIF files, and about 5 times smaller than PDF files.	"['Majority of the image compression techniques are usually associated with type of image. Enterprises or individuals who are interested to share high resolution documents in public domain require an effective lossy compression technique that can provide high rate of compression. Normally such documents are viewed in a web browser. To support display of such high resolution document in a web browser, the compression technique used should have fast decoding rate. DjVu is an option for such compression requirement.\nIn order to read DjVu files in a web browser, a decoder is implemented in the form of plug-in that is compact-able with all available standard web browsers. It is pretty much easy to zoom and pan a file with image with the help of this decoder. A user can see the initial version of a file very quickly and the quality of the file gradually improves when more bits arrives, Because of this, the DjVu compression technique is considered to be progressive in nature.\nThe DjVu techniques make use of the fact that various elements of a scanned file, such as text, images, and drawings, possess different perceptual characteristics. Digitized text and line drawings can be coded at high spatial resolution and little color resolution whereas high color resolution and at lower spatial resolution is required for images and backgrounds. Text usually requires one color but images and drawings requires more than one colour.\nDjVu Compression Rate :\nDjVu offers a compression factor up to 1000. This level of compression is 5 to 10 times superior compared to similar compression techniques available. For example, full color pages scanned at 300 dpi can be compressed from 25 Mb to 30-60Kb without compromising quality. The color documents having image and text compressed using DjVu method are mostly 5 to 10 times smaller when compared to JPEG files of similar quality and black-and-white documents compressed using DjVu method are mostly 10–20 times smaller when compared to JPEG files and five times smaller when compared to GIF files. Similarly files compressed using DjVu technique is about 5 times smaller compared to PDF files.\nDjVu Components :\nIn DjVu compression technique, the document is decomposed into three components and they are mask, foreground, and background. The pixels that represent the image and the paper background are categorized as background component. The text and the lines in bi-level form are considered as mask component. The three components are decoded by the decoder. After that, the resolution of the background and foreground components is increased back to 300 dpi, and this will generate final decompressed image pixel based on the mask. A mask pixel value 0 implies the respective image pixel is decoded from the background. Similarly a mask pixel value of 1 implies the respective image pixel is created in the color of the foreground pixel.\nAttention reader! Don’t stop learning now. Get hold of all the important CS Theory concepts for SDE interviews with the CS Theory Course at a student-friendly price and become industry ready.\n- Difference between Lossy Compression and Lossless Compression\n- Basic Network Attacks in Computer Network\n- LZW (Lempel–Ziv–Welch) Compression technique\n- Data Compression With Arithmetic Coding\n- Shannon-Fano Algorithm for Data Compression\n- PGP - Compression\n- Compression of IPv6 address\n- Difference between Inter and Intra Frame Compression\n- Difference between Storage Area Network (SAN) and Network Attached Storage (NAS)\n- Differences between Wireless Adhoc Network and Wireless Sensor Network\n- Difference between Next Generation Network and Traditional Network\n- Difference between Software Defined Network and Traditional Network\n- Computer Network | Leaky bucket algorithm\n- HMAC Algorithm in Computer Network\n- Introduction of Firewall in Computer Network\n- Circuit Switching in Computer Network\n- Packet Switching and Delays in Computer Network\n- Routing v/s Routed Protocols in Computer Network\n- Collision Domain and Broadcast Domain in Computer Network\n- Difference between Unicast, Broadcast and Multicast in Computer Network\nIf you like GeeksforGeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to firstname.lastname@example.org. See your article appearing on the GeeksforGeeks main page and help other Geeks.\nPlease Improve this article if you find anything incorrect by clicking on the ""Improve Article"" button below.']"	['<urn:uuid:f274c38c-4cb7-4ce6-8c43-fedd34b562db>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T22:03:18.033296	10	29	703
72	schrodinger equation quantum key distribution applications	The Schrödinger equation, which describes quantum wave functions and their probabilistic interpretation, provides the theoretical foundation for quantum mechanics. This fundamental physics is now being applied in practical quantum key distribution (QKD) systems for secure communications. While the Schrödinger equation explains how quantum states evolve and how measurement affects them, QKD leverages these principles to create unbreakable encryption by encoding information in quantum states of photons. This technology has moved from theory to real-world implementation, with China establishing a 1,200-mile quantum network between major cities and demonstrating satellite-based quantum key distribution over 1,120 kilometers using the Micius satellite, though current key exchange rates remain too low for practical widespread use.	"['The Schrödinger Equation1. The Schrōdinger Equation. The fundamental equation of motion in quantum mechanics is Erwin Schrōdinger\'s famous wave equation that describes the evolution in time of his wave function ψ.\niℏ δψ / δt = H ψ (1)Max Born interpreted the square of the absolute value of Schrōdinger\'s wave function |ψn |2 (or < ψn | ψn > in Dirac notation) as providing the probability of finding a quantum system in a particular state n. As long as this absolute value (in Dirac bra-ket notation) is finite,\n< ψn | ψn > ≡ ∫ ψ* (q) ψ (q) dq < ∞, (2)then ψ can be normalized, so that the probability of finding a particle somewhere < ψ | ψ > = 1, which is necessary for its interpretation as a probability. The normalized wave function can then be used to calculate ""observables"" like the energy, momentum, etc. For example, the probable or expectation value for the position r of the system, in configuration space q, is\n< ψ | r | ψ > = ∫ ψ* (q) r ψ (q) dq. (3)2. The Principle of Superposition. The Schrōdinger equation (1) is a linear equation. It has no quadratic or higher power terms, and this introduces a profound - and for many scientists and philosophers a disturbing - feature of quantum mechanics, one that is impossible in classical physics, namely the principle of superposition of quantum states. If ψa and ψb are both solutions of equation (1), then an arbitrary linear combination of these,\n| ψ > = ca | ψa > + cb | ψb >, (4)with complex coefficients ca and cb, is also a solution. Together with Born\'s probabilistic (statistical) interpretation of the wave function, the principle of superposition accounts for the major mysteries of quantum theory, some of which we hope to resolve, or at least reduce, with an objective (observer-independent) explanation of irreversible information creation during quantum processes. Observable information is critically necessary for measurements, though observers can come along anytime after the information comes into existence as a consequence of the interaction of a quantum system and a measuring apparatus. The quantum (discrete) nature of physical systems results from there generally being a large number of solutions ψn (called eigenfunctions) of equation (1) in its time independent form, with energy eigenvalues En.\nH ψn = En ψn, (5)The discrete spectrum energy eigenvalues En limit interactions (for example, with photons) to specific energy differences En - Em. In the old quantum theory, Bohr postulated that electrons in atoms would be in ""stationary states"" of energy En, and that energy differences would be of the form En - Em = hν, where ν is the frequency of the observed spectral line. Einstein, in 1916, derived these two Bohr postulates from basic physical principles in his paper on the emission and absorption processes of atoms. What for Bohr were assumptions, Einstein grounded in quantum physics, though virtually no one appreciated his foundational work at the time, and few appreciate it today, his work eclipsed by the Copenhagen physicists. The eigenfunctions ψn are orthogonal to each other\n< ψn | ψm > = δnm (6)where the ""delta function""\nδnm = 1, if n = m, and = 0, if n ≠ m. (7)Once they are normalized, the ψn form an orthonormal set of functions (or vectors) which can serve as a basis for the expansion of an arbitrary wave function φ\n| φ > = ∑ n = 0 n = ∞ cn | ψn >. (8)The expansion coefficients are\ncn = < ψn | φ >. (9)In the abstract Hilbert space, < ψn | φ > is the ""projection"" of the vector φ onto the orthogonal axes ψn of the ψn ""basis"" vector set. 2.1 An example of superposition. Dirac tells us that a diagonally polarized photon can be represented as a superposition of vertical and horizontal states, with complex number coefficients that represent ""probability amplitudes."" Horizontal and vertical polarization eigenstates are the only ""possibilities,"" if the measurement apparatus is designed to measure for horizontal or vertical polarization. Thus,\n| d > = ( 1/√2) | v > + ( 1/√2) | h > (10)The vectors (wave functions) v and h are the appropriate choice of basis vectors, the vector lengths are normalized to unity, and the sum of the squares of the probability amplitudes is also unity. This is the orthonormality condition needed to interpret the (squares of the) wave functions as probabilities. When these (in general complex) number coefficients (1/√2) are squared (actually when they are multiplied by their complex conjugates to produce positive real numbers), the numbers (1/2) represent the probabilities of finding the photon in one or the other state, should a measurement be made on an initial state that is diagonally polarized. Note that if the initial state of the photon had been vertical, its projection along the vertical basis vector would be unity, its projection along the horizontal vector would be zero. Our probability predictions then would be - vertical = 1 (certainty), and horizontal = 0 (also certainty). Quantum physics is not always uncertain, despite its reputation. 3. The Axiom of Measurement. The axiom of measurement depends on the idea of ""observables,"" physical quantities that can be measured in experiments. A physical observable is represented as an operator A that is ""Hermitean"" (one that is ""self-adjoint"" - equal to its complex conjugate, A* = A). The diagonal n, n elements of the operator\'s matrix,\n< ψn | A | ψn > = ∫ ∫ ψ* (q) A (q) ψ (q) dq, (11)are interpreted as giving the expectation value for An (when we make a measurement). The off-diagonal n, m elements describe the uniquely quantum property of interference between wave functions and provide a measure of the probabilities for transitions between states n and m. It is the intrinsic quantum probabilities that provide the ultimate source of indeterminism, and consequently of irreducible irreversibility, as we shall see. Transitions between states are irreducibly random, like the decay of a radioactive nucleus (discovered by Rutherford in 1901) or the emission of a photon by an electron transitioning to a lower energy level in an atom (explained by Einstein in 1916). The axiom of measurement is the formalization of Bohr\'s 1913 postulate that atomic electrons will be found in stationary states with energies En. In 1913, Bohr visualized them as orbiting the nucleus. Later, he said they could not be visualized, but chemists routinely visualize them as clouds of probability amplitude with easily calculated shapes that correctly predict chemical bonding. The off-diagonal transition probabilities are the formalism of Bohr\'s ""quantum jumps"" between his stationary states, emitting or absorbing energy hν = En - Em. Einstein explained clearly in 1916 that the jumps are accompanied by his discrete light quanta (photons), but Bohr continued to insist that the radiation was classical for another ten years, deliberately ignoring Einstein\'s foundational efforts in what Bohr might have felt was his area of expertise (quantum mechanics). The axiom of measurement asserts that a large number of measurements of the observable A, known to have eigenvalues An, will result in the number of measurements with value An that is proportional to the probability of finding the system in eigenstate ψn. Quantum mechanics is a probabilistic and statistical theory. The probabilities are theories about what experiments will show. Experiments provide the statistics (the frequency of outcomes) that confirm the predictions of quantum theory - with the highest accuracy of any theory ever discovered! 4. The Projection Postulate. The third novel idea of quantum theory is often considered the most radical. It has certainly produced some of the most radical ideas ever to appear in physics, in attempts by various ""interpretations"" to deny it. The projection postulate is actually very simple, and arguably intuitive as well. It says that when a measurement is made, the system of interest will be found in (will instantly ""collapse"" into) one of the possible eigenstates of the measured observable. We have several possibilities for eigenvalues. We can calculate the probabilities for each eigenvalue. Measurement simply makes one of these actual, and it does so, said Max Born, in proportion to the absolute square of the probability amplitude wave function ψn. In this way, ontological chance enters physics, and it is partly this fact of quantum randomness that bothered Einstein (""God does not play dice"") and Schrōdinger (whose equation of motion for the probability-amplitude wave function is deterministic). The projection postulate, or collapse of the wave function, is the element of quantum mechanics most often denied by various ""interpretations."" The sudden discrete and discontinuous ""quantum jumps"" are considered so non-intuitive that interpreters have replaced them with the most outlandish (literally) alternatives. The famous ""many-worlds interpretation"" substitutes a ""splitting"" of the entire universe into two equally large universes, massively violating the most fundamental conservation principles of physics, rather than allow a diagonal photon arriving at a polarizer to suddenly ""collapse"" into a horizontal or vertical state. 4.1 An example of projection. Given a quantum system in an initial state | φ >, we can expand it in a linear combination of the eigenstates of our measurement apparatus, the | ψn >.\n| φ > = ∑ n = 0 n = ∞ cn | ψn >. (8)In the case of Dirac\'s polarized photons, the diagonal state | d > is a linear combination of the horizontal and vertical states of the measurement apparatus, | v > and | h >. When we square the (1/√2) coefficients, we see there is a 50% chance of measuring the photon as either horizontal or vertically polarized.\n| d > = ( 1/√2) | v > + ( 1/√2) | h > (10)4.2 Visualizing projection. When a photon is prepared in a vertically polarized state | v >, its interaction with a vertical polarizer is easy to visualize. We can picture the state vector of the whole photon simply passing through the polarizer unchanged. The same is true of a photon prepared in a horizontally polarized state | h > going through a horizontal polarizer. And the interaction of a horizontal photon with a vertical polarizer is easy to understand. The vertical polarizer will absorb the horizontal photon completely. The diagonally polarized photon | d >, however, fully reveals the non-intuitive nature of quantum physics. We can visualize quantum indeterminacy, its statistical nature, and we can dramatically visualize the process of collapse, as a state vector aligned in one direction must rotate instantaneously into another vector direction.', 'In 2019, a team of Chinese technicians, engineers, and scientists sent pairs of photons from a single satellite called Micius to two ground stations in China separated by over 1,120 kilometers. The photons were prepared in such a way that they carried information that remained perfectly correlated in spite of the distance between them. In addition, the two receiving stations in China were able to ensure that the two receivers could not be disrupted or deceived by any third party. The experiment demonstrated the ability to share secret cryptographic keys between the two locations in China, with no known means for a third party to covertly observe or copy them. Although the rate of the key exchange was too low for practical use, the achievement represented a step toward secret communications guaranteed by the laws of physics.\nSeveral countries have spent decades trying to find ways of moving data that are both cost-effective and secure by investing in quantum communication technology. The surge in China’s work in the field dates to 2013, when the release of classified information by Edward Snowden detailing U.S. intelligence capabilities caused deep concern in Beijing. “This incident has been so fundamental to Chinese motivations that Snowden has been characterized as one of two individuals with a primary role in the scientific ‘drama’ of China’s quantum advances, along with Pan Jianwei, the father of Chinese quantum information science,” the researchers Elsa Kania and John Costello concluded in a 2017 report.\nThe national-security implications of China’s interest in space-based quantum communications cuts several ways. The development of impenetrably secure communications links in China would be a loss for American intelligence organizations. On the other hand, China’s intensive efforts in using space for secure quantum-based communications may lead that nation to consider international agreements governing space activities as in their national interest. This strategic interest might be leveraged as part of a future U.S.-China agreement in managing competition in space. There are ample opportunities for collaboration in this field among the United States, Europe, Canada, Japan, Australia, and other democratic allies. China’s leading position in quantum data security suggests that U.S.-China collaboration—at least on basic science—would be a net benefit for the United States in understanding the state of the art.\nQuantum computing and secure communications\nData transmitted between two parties over the internet is subject to unwanted interception. The value of the internet depends on the fact that data sent between the sender and receiver can be securely encrypted. Encryption methods are based on exchanging secret keys used to encode data in a way that reveals minimal information to someone lacking the key. Keys can be as simple as a long random string of 0’s and 1’s. Common methods for sharing secret keys over the insecure internet are based on numerical computations that are easy to make in one direction but very difficult to make in the reverse direction—on our current computers. For example, multiplication of two prime numbers is easy, but given a very large integer, determining which primes were multiplied together to yield that integer is a difficult problem, and it gets rapidly more difficult the larger the integer. This is true even on today’s most powerful conventional computers.\nFundamentally new types of computing architectures based on interactions of quantum systems were first proposed in the 1980s. Within a few years, Peter Shor and other theorists proved that quantum computer algorithms, running on sufficiently large quantum computers, could in principle solve the extremely time-consuming numerical problems like factoring large integers much more quickly than a classical computer algorithm could.\nIn short, working quantum computing systems threaten to make useless current methods of encryption that provide the basis of internet commerce and digital communication. Though such systems are generally thought to be unachievable before 2030, when such quantum computers are available, decrypting some communications streams may become feasible if nothing is done to protect those streams. What’s more, any encrypted data that has been intercepted and stored will be vulnerable to decryption. That means any country that attains a quantum computing system of sufficient power in the future will be able to decrypt stored data from the current era that would otherwise remain impossible to decode. And the data at risk goes beyond national-security information to include genomic, medical, and financial data.\nThese concerns have spurred efforts in the United States to develop new encryption algorithms that are more resistant to known quantum computing-based decryption methods. These post-quantum cryptography (PQC) methods are being designed and evaluated to be run on current classical computers. The National Institute for Standards and Technology (NIST) is leading an evaluation of PQC alternatives and has recently published its latest list of top contenders.\nWhat is perfect security?\nIn discussions of cryptography, the sender and receiver of a message are usually referred to as Alice and Bob, respectively. They are assumed to have a public channel and an encrypted channel over which to send data. Proofs of security assume that an adversarial eavesdropper, called Eve, has access to both channels, as well as powerful computers at her disposal.\nIf Alice wants to transmit a message to Bob at some future time over a network on which Eve lurks, Bob can meet at Alice’s totally secure office. There they create two identical copies of a long string of completely random binary digits, called a key, and securely package one copy of the random key so that Bob can take it to his secure office. When Alice wants to send a message to Bob over the compromised channel, Alice converts her message to a string of bits using an agreed upon encoding scheme that does not need to be secret. She then chooses the first segment of the random key the same length as the binary form of her message, aligns the key bits with the message bits so that they are paired, and computes a bitwise exclusive “or” operation (XOR). A bitwise XOR operation of the two bits is a simple function that outputs a 0 if both inputs are 0, a 1 if one bit is 0 and the other is 1, and the “exclusive” part means that the output is 0 if both inputs are 1. The result is an encrypted string of bits that is equally likely to be any message. Alice sends this string of bits to Bob. Bob XORs the encrypted message with his copy of the key, and then he can convert the result back to text using the public encoding. Then they both discard the random key.\nThe procedure above provides no new information to Eve, even if she captures the entire string of encrypted bits. However, the requirements of this ideal scheme are immense: The random string of bits in the key must be truly random, a new key must be generated for each message since it is discarded after one use, the key must be as long as the message, and the key must be shared in perfect secrecy. This makes data exchange very inefficient. The requirement for perfect randomness cannot be met using readily-available computer random number generators: Only physical systems such as radioactive decay, or other quantum systems can generate truly random numbers. Encryption methods in use today make compromises to the idealized algorithm above in order to trade perfect security for efficiency.\nQuantum key distribution may offer secrecy with fewer such compromises. Quantum key distribution methods transmit random keys by encoding these strings of 0’s and 1’s into sequences of photons whose quantum states obey the rules of quantum mechanics. For single photons, these rules allow for photons to exist in a combination of two quantum states until they are detected by a device that can measure the states. Once detected by a particular kind of device, the photon will take on a definite state that is in part determined by the device itself. This close relationship between the photon and the measurement device is at the heart of QKD methods. Other QKD methods use pairs of photons which are generated to have perfect correlation between their states, regardless of their separate travel paths. Common to all QKD methods is the fact that an eavesdropper that detects the photons will either gain no information about the keys, or will signal to Alice and Bob that they have successfully intercepted the data. This allows Alice and Bob to make adjustments in order to complete the key exchange. In any case, the eavesdropper can never copy the quantum information. In classical information exchange over the internet, an eavesdropper can detect, copy, retransmit the 0’s and 1’s without changing how this information is later observed by Alice and Bob, thereby remaining invisible.\nThe ability to replicate classical digital data without error is a key enabler of the current internet, as it allows the same information to travel to multiple places for use. Since the quantum states of photons cannot be copied, this creates special challenges for quantum networking. However, by compromising on the perfect security of quantum information exchange at a few, well-trusted sites, quantum networks have been built.\nThe guaranteed secrecy of QKD systems threatens to make it impossible to spy on communication channels use by adversary countries. Whether these are channels that are already tapped, or ones that would be useful to tap in the future, improvements in communication security can potentially cut off information that might be useful in statecraft or to gain advantage in a military crisis.\nThis gives rise to two important reasons for pursuing QKD research. First, by understanding the weaknesses of QKD devices, one can guard against attacks on the integrity or reliability of one’s own QKD system. Second, if one can deny an adversary the secure use of QKD, it may drive that adversary to use less secure communications means, which may then be exploited. Thus, the development of QKD systems between geopolitical rivals will take on a measure-countermeasure character, in much the same way as military communications and sensing measures must overcome sophisticated electronic warfare countermeasures.\nNational efforts at QKD and QKD networks\nThe United States, Japan, Canada, Singapore, and Europe initially led the efforts in quantum key distribution. Initial research involved point-to-point QKD, but networks of quantum-secured information exchange is the real goal of these efforts. The first QKD network was established in Boston by DARPA in 2003 and, by 2004, ran between Harvard University, Boston University, and the offices of the research firm Bolt, Beranek, and Newman. Between 2008 and 2009, the European FP6 project integrated several QKD systems into one QKD backbone in Vienna. In 2010, researchers in Tokyo demonstrated a QKD network with encryption for video.\nToday, China has taken the lead in quantum key distribution: The largest demonstrated network is one that began operating in 2017 in China within the cities of Beijing, Jinan, Hefei, and Shanghai with a 1,200 mile quantum backbone network connecting them. In the United States, meanwhile, the U.S. firm Battelle, together with the Swiss company ID Quantique, is constructing a 400 mile link between Columbus, Ohio, and Washington, D.C..\nQKD research and development continues today, as part of broader developments in quantum technologies in Canada, the European Union, South Korea, Japan, the United Kingdom, the United States, Russia, China and other countries. Over the past 20 years, emphasis within the overall field of quantum technology has shifted, with the United States and other Western countries tending to focus on quantum computing and China putting greater emphasis on QKD. Though there are efforts underway in China to build advanced quantum computers, this difference of emphasis reflects the deep concern about internet security at the highest levels of Chinese leadership, while in the United States, quantum computing advances have been driven by large companies. As China, the United States, and other countries build a larger workforce with the experience in designing and building quantum information systems, it may be that all countries converge to a more broad-based quantum information technology base.\nCompanies based in China dominate applications for patents in quantum cryptography in the most recent period when data is available, from 2012 to 2016. Companies based in the United States and Japan dominated quantum cryptography patent applications between 2002 and 2010, but have since slowed considerably. In the field of quantum computing on the other hand, the United States, Japan, and Canada have applied for the great majority of patents from the period 2001 to 2016 and far more than China.\nAs commercial QKD component offerings grow, benefits will likely accrue to companies that can innovate while meeting or establishing industry standards. South Korea Telecom and ID Quantique have worked through the International Telecommunications Union to establish standards for quantum communications tools. The competition to help set standards is perhaps as consequential in this field as is any particular technological development. Companies that can adapt to voluntary international standards for technical devices and data can establish a market advantage as other companies and countries around the world begin to integrate that technology into their own infrastructure. Industries cooperate in setting standards by consensus, and to the extent that Chinese companies can bring real expertise and experience in quantum technology to international standards organizations, they will have a better chance that their technical approaches will be integrated into the standards and that they will be competitive in the long run.\nFinally, it is always useful to keep in mind throughout technical discussions of data security, that the weakest points in technological systems is often humans. Greed, fear, carelessness, lack of training or darker motives can open the most technically secure systems to risks. The methods of social engineering—manipulating the perceptions and behaviors of human users—that are core to cyberattack methods transfer directly to future quantum secure communications. Proper training and monitoring for insider threats will remain a key element of information security, regardless of any particular technology implementation.\nThe global quantum race\nThe need for varying levels of data security, up to and including near-perfect security, is driving countries around the world to invest in improvements in encryption based on both mathematics and on quantum physics. With quantum computers now in active production around the world, the risk to current internet encryption may arise a decade from now. This also raises immediate concerns about the long-term security of sensitive data that is already being intercepted. One possible future for information security is one that involves a hybrid of post-quantum cryptography based on mathematics and QKD based on physics, with the former providing security for authentication required over classical data channels.\nAs the United States increases its reliance on the secure flow of data, QKD will probably play a significant role. Therefore, it is important for the United States to develop a mix of technical infrastructures, such as satellites and fiber links. It also critical to understand the vulnerabilities of those QKD links. In cryptography, open analysis of methods has always led to better security, and the same is likely to be true for QKD. The United States would be best served through collaboration on quantum information among the national governments, business, and academic groups within the United States and between the US, Europe, South Korea, Australia, Japan, and other countries. This will minimize strategic surprise by maximizing the breadth and depth of U.S. understanding of quantum information science and engineering.\nChina has a demonstrated lead in demonstrations of several specific QKD technology areas, including space-based quantum key distribution using entangled photons launched from space. Since this method has some distinct advantages for very long-range secure information, China could become increasingly dependent on space-based QKD for securing data over long distances. This could provide the basis for a common interest in preserving the stability of satellite-based communications between the United States, China and other countries that are increasingly dependent on space.\nThe United States should also continue to engage in technical exchange and collaboration with China in the area of quantum information science for several reasons. First, China has a demonstrated lead in several QKD technology areas, including space-based QKD using entangled photons and large terrestrial quantum networks. The United States is likely to learn something about the engineering issues if not the physics. The second reason for collaboration has to do with the nature of QKD itself: It is provably secure. Unlike the codebreaking of WWII, which was so important to the Allies, certain QKD systems are impervious to eavesdropping. Understanding of the technology does not create a security vulnerability for either side. The final reason for collaboration is to maintain expert exchange between the two countries in the critical nexus of information technology, cybersecurity, and the uses of space. China could become increasingly dependent on space-based QKD for securing data over long distances. The United States has long been dependent on space for collecting and moving data. This convergence of needs could provide the basis for future agreements on activities in space that are mutually beneficial, such as limitations on disruptions of satellite communication systems.\nTom Stefanick is a visiting fellow at the Brookings Institution.']"	['<urn:uuid:fd2321d4-139a-468e-bdf6-8e22af8de92f>', '<urn:uuid:d0983495-8a6b-4fe3-8a25-e09a08c097fb>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T22:03:18.033296	6	110	4594
73	I live in a neighborhood that feels a bit run down and needs some improvements. What kind of changes can be made to boost such areas?	A neighborhood can be improved through physical interventions and adjustments in the social sector. To determine the most effective changes, it's important to closely examine the area and survey the users to understand their current concerns. This approach helps gain first-hand insight into where changes are needed. Any adjustments should take into account trends and developments to ensure the area becomes future-proof in its design and organization.	"[""Urban Regeneration is a minor at the School of Architecture & Built Environment, specifically belonging to the major in Spatial Development. This minor was developed in conjunction with the School of Social Studies and Minerva Art Academy and is taught in English by lecturers from these three institutes.\nWe have all seen neighbourhoods or districts that could perhaps do with a boost. This boost could take the form of physical interventions and/or adjustments\nin the social sector. Closely examining the area and surveying the users will enable you to gain first-hand insight into their current concerns and where a\nchange is perhaps needed to boost the area. This adjustment, taking into account any trends and developments, will ensure a future-proof design and/or\norganisation of the area.\nOn completion of this minor, you will be able to answer the following question: How can we adaptively design and/or organise an existing area in the built\nenvironment, while taking into account the use and context of the location? You will also know how and when it will be effective to help local stakeholders in\nthese processes of spatial transformation.\nThe Urban Regeneration minor allows students to think in an innovative way, to conduct research while designing a solution, to work with members of various\ndisciplines and cultures, to gain insight into and experience with the way people and places interact and to design a future-proof adaptive environment for\npeople to use.\nI&I.1: The student can identify their own ambitions for personal /professional development (of skills) within the context of this module.\nI&I.2: The student can compare and\nmatch their own personal qualities and roles within a team with those of other team members.\nI&I.3: The student can identify objective and subjective values of a place in relation to their own values.\nI&FV.1: The student is able to identify and describe a 'problem gap' in the context of a given need or spatial issue.\nI&FV.2: The student can associate through broadening, connecting and restructuring to cultivate new insights\nand gain radically different perspectives.\nI&FV.3: The student can formulate research outcomes from different lines of investigation.\nI&FV.4: The student can reflect on selflearning, the process followed and deliver reflective peer feedback.\nI&FV.5: The student is able to demonstrate appropriate\ncommunication to the different target groups.\nI&FV.6: The student is able to demonstrate constructive cooperation with group members and other participants/actors.\nRC.1: The student is able to recognise and distinguish between the different social and / or economic issues within a neighbourhood\nRC.2: The student is able to carry out research specific to the topic and retrieve information regarding the\nissues in the neighbourhood related to context solutions and feasibility\nRC.3: The student is able to relate global scale issues to the context (motive, relevance, trends) of neighbourhood\nissues and compare these to spatial or social transformations.\nRC.4: The student is able to involve other processes, developments, values and / or ideas and assess their usage or application\nRC.5: The student is able to reflect on his or her own performance according to self-formulated criteria and is able to outline and revise his or her functioning for to achieve improvements\nST.1: The student can identify, classify and explain separate components of the urban environment\nST.2: The student is able to conceptualise and visualise the essence of spatial issues and describe them in\nthe appropriate form (e.g. plans, drawings and models).\nST.3: The student can produce and present their understanding / vision of the dynamics within the urban\nST.4: The student can translate theory, principles and policy with stakeholder needs to produce solutions for the\nregeneration of the urban environment.\nST.5: The student can compare, contrast and match his or her work with that of colleagues to establish interdisciplinary connections.\nRtD.1: The student recognises the different theoretical and practice based methodologies involved in RtD processes.\nRtD.2: The student demonstrates different observational skills and classifies relevant abstract and physical\nqualities and issues from technical, environmental and social arenas.\nRtD.3: The student is able to conceptualise and visualise the essence of researched phenomena by formulating an annotated portfolio of alternative and improved prototypes.\nRtD.4: The student evaluates research outcomes.\nRtD.5: The student reflects systematically on the learning\nmoments, the process followed and the decision-making process.\nA&E.1: The student can articulate his/her ideas, concepts, work, approach and vision for professionals and public\nboth within and outside of the field of work.\nA&E.2: The student is able to take objectively critical positions regarding product, societal context and the real\nA&E.3: The student evaluates the results of implementing the product and reflect critically upon them.\nA&E.4: The student evaluates and can debate subjects related to his/her discipline with other experts.\nA&E.5: The student carries out peer to peer feedback with fellow students\nThe different modules comprise assignments, all of which relate to part or whole of the Vinkhuizen neighbourhood where we will be actually located for all lessons, workshops and presentations (for more\ninformation, see http://www.backbone050.eu/). Numerous civic initiatives organised in the neighbourhood take place in this former school building, which is\nnow a kind of creative breeding ground.\nassignments and assessments""]"	['<urn:uuid:4f4eee40-4223-4bf5-94fe-dbcc5facc036>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T22:03:18.033296	26	67	850
74	Which Latin music figure has more Grammy awards - Intocable or Rudy Pérez?	Intocable has won more Grammy awards. The group has received two GRAMMY Awards (in 2005 for 'Íntimamente' and in 2011 for 'Clásico'), seven Latin GRAMMYs, while Rudy Pérez's works have been nominated for 19 Grammy Awards with five wins.	['INTOCABLE Group Members:\nRicardo Javier “Ricky” Muñoz – (Lead Vocalist, Accordion)\nRené Orlando Martínez – (Drums, Chorus)\nSergio Serna – (Percussion)\nDaniel Sanchez – (Second Vocals, Bajo Sexto)\nJosé Juan Hernández – (Group Motivator and Rhythms)\nAlejandro Gulmar – (Bajo Sexto)\nFelix Salinas – ( Bass )\nINTOCABLE is one of the most influential groups in the Tejano/Norteño music scene today. Their road to success began twenty years ago in Zapata,Texas, when friends Ricky Muñoz and René Martínez dreamed of bringing their songs to the world as they crafted a unique and extraordinary style of music which expertly fuses Tejano conjunto music, Norteño folk rhythms, pop ballads and rock.\nAfter years of dedication and passion, the dream became an indisputable reality. With #1 hits such as “Robarte un Beso,” “Prometí,” “Te Amo (Para Siempre),” “Culpable Fui (Culpable Soy),” their latest “Tu Ausencia,” as well as fan favorites like “Dame Un Besito,” “Eres Mi Droga,” Enséñame a Olvidarte,” “Sueña,” “Contra Viento y Marea,” “Llueve,” “Arrepiéntete,” among others, INTOCABLE continues to entertain and delight audiences in both the United States and Mexico. Their songs are heard in radio stations throughout the American Continents and the band has performed live in world-renowned venues such as the Auditorio Nacional in Mexico City, the Monterrey Arena in Mexico, and the Reliant Astrodome Stadium in Houston, with a historic record of more than 70,000 fans singing along and enjoying one of the best shows in the industry. And to top off an already exceptional career, INTOCABLE made history by being the first and so far only act of their genre to sing the halftime show at the Dallas Cowboys vs. Washington Redskins game at the famed AT&T Stadium, home of the Dallas Cowboys.\nToday, the group INTOCABLE is the standard bearer of “la música norteña.” Its achievements have been recognized with two GRAMMY Awards (in 2005 for their album “Íntimamente (Intimately)” and in 2011 for the album “Clásico (Classic),” seven Latin GRAMMY and multiple nominations, thirteen Premios Lo Nuestro, numerous Latin Billboards and two Billboard Regional Mexican Awards, as well as many other nominations, including one from the American Music Awards. And to top it off, in 2010 Billboard Magazine named them “Group of the Decade”, and they were honored by Billboard with a Lifetime Achievement Award in 2012.\nMaking history and pole vaulting through industry standards has become second nature to the group Intocable. On October 12th, 2015, Intocable made history, yet again, by streaming a showcase for the whole world, in which more than 22,000 people enjoyed a unique evening from the comfort of their homes. 100% of the proceeds were donated to St. Jude Hospital to help children affected with cancer; there the inception of #AyudaAAyudar was formed and will continue to do events to raise funds for this noble cause.\nContinuing with the notion that “music is the best medicine for the soul”, On January 23rd 2016, Intocable humbly announced their partnership with St.Jude, a pledge first of its kind in the music industry. After witnessing firsthand the groundbreaking research done at St. Jude, Intocable knew they had to help the organization in any capacity possible. Today, their support will not only create awareness for the cause, but their pledge of support will contribute to assure that families pay for nothing and can focus on letting their child live.\n2016 has proved to be revolutionary in the career of Intocable as they solidify their position as pioneers in the industry. In the prestigious SXSW musical festival, Intocable headlined SXSW 2016 SXAmericas All Latino Showcase, making Intocable the first Latino artist to headline this three-day series of charity beneficiary concert events. The group took over the largest SXSW stage, which attracted more than 50,000 attendees over the three-day period.\nAlways striving towards creating music that is relevant and transcends the boundaries of genre, national borders and even languages, INTOCABLE returned to the studio for their 19th career album. “Highway” referencing the journey of which is Intocable throughout almost three decades of exceptional music paying homage to the unique sound that has made them a mainstay and innovators of Tejano/Norteño music. With the rhythms, expert arrangements and the excellence and precision of a group of musicians at the top of their art form, this album will surely go down in history as one of the greats.\nIn this album, under Good I Music and Universal Music Group, INTOCABLE remained loyal to their team and used the same writers and musicians that have made then a mainstay in the industry since their first album in 1994. When selecting the songs that would ultimately make the cut to the album, Intocable chose 14 songs that truly resonated with the group while keeping the fans in mind. The track list speaks for itself featuring their first hit single off the album, “Tu Ausencia” along with “Me Faltas Tu”, “Quiereme (Amame)”, “Cuando Me Vi En Tus Ojos” and many many more.\nYear after year, INTOCABLE demonstrates their passion for music and their desire to share their creations with a loyal audience that keeps growing with every word and every note.\n• Simplemente…Intocable (Simply…Untouchable) (1993)\n• Fuego Eterno (Eternal Fire) (1994)\n• Otro Mundo (Another World) (1995)\n• Llévame Contigo (Take Me With You) (1996)\n• IV (4) (1997)\n• Intocable (Untouchable) (1998)\n• Contigo (With You) (1999)\n• Es Para Tí (This Is For You) (2000)\n• Sueños (Dreams) (2002)\n• La Historia(2003)\n• Nuestro Destino Estaba Escrito (Our Destiny Was Written) (2003)\n• Íntimamente (Intimately/Live album) (2004)\n• X (10) (2005)\n• Crossroads: Cruce de Caminos (2006) (2007-Fan Edition)\n• 2C (12) (2008)\n• Classic (“Clásico” México) (2009)\n• Super #1’s (2010)\n• INTOCABLE 2011 (2011)\n• En Peligro de Extinción (Endangered) (2013)\n• XX Aniversario (XX Anniversary) (2015)\n• Highway (Highway) (2016)', 'Rudy Pérez is an International Songwriter, Producer, Arranger, Entrepreneur and Philanthropist. During the last 30 years, he has composed over 1,000 songs with over 300 of them have been #1 or top ten hits. He is one of the Founders of the Latin America Recording Association (LARAS) The Latin Grammys as well as the Co Founder of the Latin Songwriters Hall of Fame.\nRudy Pérez was named Producer of the Decade by Billboard Magazine for having more top ten hits than any other producer from 2000-2010.\nHe has written and produced tracks for artists including Beyonce, Julio Iglesias, Christina Aguilera, IL Divo, Natalie Cole, Michael Bolton, Luis Miguel, Jose Feliciano, Marc Anthony, Jon Secada, Cyndi Lauper, Roberto Carlos, Los Temerarios, Juan Gabriel, Alejandro Fernandez, Marco Antonio Solis, Jose Luis Rodriguez (El Puma) Alexandre Pires, Arturo Sandoval, Chayene, Luis Fonsi, Dima Bilan, Leonid Agutin and Jaci Velasquez, Juan Luis Guerra among many more. He is the first Latin record producer to win the Billboard Hot Latin Tracks Producer of the Year for four consecutive years. Perez has also been awarded ASCAP’s Songwriter of The Year 5 times.\nIn 2010 Billboard Magazine named Rudy “Producer of The Decade” for having more # 1 and Top Ten Hits chart from 2000 to 2010. More than any other Latin Producer in their history. He has been called by industry leaders, “the Producer, Songwriter with the midas touch”. It’s no wonder why he has received over 500 gold and platinum album awards, and his works have been nominated for 19 Grammy Awards, with five wins.\nBillboard Magazine’s, Leila Cobo, wrote “A look at the 10 of the 13 tracks this year that placed Rudy Perez at the top of the year-end Hot Latin Tracks producers list reveals a versatile producer/songwriter capable of successfully handling a variety of music genres.” Perez’s strength lies in his chameleon-like capabilities; not only does he easily navigate through assorted styles, he also traverses Record Labels, with credits on each of the Majors as well as Independents. Perez has emerged atop one of the hottest trends to surface in the Latin music Industry: English- speaking Artists like Beyonce, who decide to cross over into the Spanish market. Chairman and Presidents of all Major Labels have been quoted saying: “Rudy’s got this bicultural understanding of the music” ” It’s not about language, it’s about culture, and Rudy seems to understand that very, very well”. He has his thumb on the pulse of the Latin movement. Since he has this unique understanding of the market, it makes it a very smart decision to bring him into all our projects! Mauricio Abaroa former SVP and Executive Director of the Latin Academy of Recording Arts and Sciences, says: “If we analyze the work he’s done in the past years, we find the songs and productions that are the most representative of the contemporary Latin music movement” Music Executive and Producer Ron Fair, former President of A&M Geffen Records says about Rudy’s work with Christina Aguilera: “From a creative standpoint and from a musical standpoint, Rudy Perez was the best possible Songwriter/Producer to bring Christina into the Spanish language world”. Music Moguls like Simon Cowell call upon Rudy to work with his biggest artists like IL Divo. Mr. Perez has a vast experience in music composition and production for Commercials, TV, and Film.\nIn 2009, Rudy launched his Label, Bullseye Music Entertainment, a unique boutique indie Latin Label whose streamline operation brings together top independent professionals under his direction. Rudy teamed up with friend and music Industry veteran Rick Stevens for this venture. Their first release was with the NBC/Telemundo Telenovela Superstar, Jencarlos Canela who Rudy has been developing since he was 17 years old. Jencarlos is the star of hit Novelas like, PECADOS AJENOS, MAS SABE EL DIABLO, and MI CORAZON INSISTE. In what may be the highest-ever first week position on the Billboard Top Latin Albums chart from a debut artist with independent distribution, Jencarlos’ first album “Buscame” debuted at # 2. Under Bullseye, “Buscame” set a record as the highest charting Latin debut album ever on an Independent Label in the history of the charts. Bullseye became the Independent label of the year, and was nominated for a Billboard Music Award as the LATIN POP LABEL OF THE YEAR…the only non-major label so-recognized. In 2011 Bullseye does it again. This time, Bullseye Music Entertainment and Jencarlos solidify their standing once again by coming in at #1 in the first week of releasing Jencarlos’ new album “Un Nuevo Dia”.\nRudy and his label Bullseye focused is on the fastest growing segment of the USA population…the YLA’s (Young Latinos In America). This is the generation that is dominating the general market because they are the general market. Rudy is determined to create artists and a lifestyle that are from this demographic. This is the reason he teamed up with the ASCAP Foundation and created the Rudy Perez Songwriting Scholarship to help aspiring Latino Songwriters since 2003 go to college. In his quest to mentor and develop new artists, Rudy teamed up with longtime friend David Frangioni to create a great mentorship program called IDA (Inspire & Develop Artists) to find out more visit: www.idaprogram.com\nMore of his projects include, his production of Natalie Cole’s last Album before her untimely passing in which Perez produced a Historic duet with Natalie and her Dad Nat King Cole in Spanish. He produced a very special single for Jon Secada that featured a duet with Cyndi Lauper of a song that Rudy co-wrote with Dr. Maya Angelou and Joel Diamond entitled “Still I Rise”. Rudy is in the studio working with Portuguese Superstar Mickael Carreira on his first Spanish crossover Album. Rudy is also working on Tony Carreira’s Portuguese album.\nRudy recently produced the Soul Man himself, Mr. Sam Moore for his new album “Sam Moore An American Patriot”. His expert Production skills brought together the soulful sound of Sam Moore on the most patriotic songs of all time. It’s a unique and wonderful tribute to America!']	['<urn:uuid:44413874-0c8f-444b-9e55-bf897b1ec73b>', '<urn:uuid:73333b95-76c4-4a0f-8f0f-5c307b393cd3>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T22:03:18.033296	13	39	1974
75	museum tour guides digital content role importance physical versus virtual experience differences	In physical museums, tour guides provide detailed knowledge of artifacts and offer interactive opportunities for questions, enhancing visitor understanding through direct narration. In the virtual space, museums have adapted by offering online guided tours and lectures, reaching wider audiences globally and securing speakers who might not have been able to participate in person.	['Museums often have a quiet, reverential feeling to them on your first visit. Many are also large and expansive with a variety of specialty exhibits and artifacts. Getting the most out of the experience requires a bit of planning, and the most enjoyable museum experiences are enlightening without the need to spend hours walking through every floor or exhibit. Regardless of their size or subject matter, museums are valuable resources of science, history, art and knowledge. They also provide excellent ways of immersing yourself in a subject matter that’s meaningful to you.\nThe following tips will help you move past any feelings of being overwhelmed and take full advantage of all the educational fun on your next trip to the best LA museums.\nPlan and book ahead if possible\nThe majority of LA museums now allow you to buy your admission tickets ahead on their websites and even have virtual previews of their latest exhibits. Buying your tickets online will save you time that would’ve been spent waiting in long lines during peak hours. Museums with specialty, limited-time exhibits often charge separate admission, and the same applies to subject matter films shown in museums’ large-screen movie theaters. Souvenirs from museum gift shops also cost extra, another factor for your museum visit budget.\nLook for special museum events\nMany LA museums have events each month for different age groups, such as free admission for kids under a certain age or even adults-only evenings once or twice per month. These events are additional fun ways to learn new things and meet new people who share similar interests in the same abject matter.\nWhile you’re at the museum\nDuring your planned LA museum visit, you’ll want to be engaged with the curiosities and exhibits that interest you. Keep your cell phone use to a minimum and be sure to follow any rules regarding photography inside the museum. While it might be tempting to look up more information online using your phone, you can most often learn more by reading through exhibit literature and asking questions of museum volunteers. Many of them have completed training in detailed knowledge of the artifacts and will be able to provide even more insight.\nIf you enjoy writing or drawing, it can be fun to take a notebook and write or sketch out your impressions of the exhibits that appeal to you the most. This is an excellent way to share what you think with your friends and family members who are also interested.\nAnother great way to enhance your museum experience in Los Angeles is to take advantage of guided tours, many of which are available at low or even no additional cost. The information and detail you’ll get from a tour guide will teach you more than you’d learn from exploring on your own. Having a guide narrate the history or science behind each exhibit will give you more context, and tours most often end with additional time to ask the guide questions. Take advantage of this opportunity to learn even more and to give museum staff members to talk further about their subject matter.\nOverall then, planning your visit ahead of time and taking advantage of extras will help you have the most enjoyable experience on your next museum visit. Take advantage of guided and self-guided tours, choose the exhibits that intrigue you the most, and continue the discussion after your visit.\nQuality most often takes precedence over quantity when it comes to the time you spend in LA museums, and engagement with what interests you is also a key to enjoying them the most.', 'Virtual Programming is Here to Stay—and so are Its Audiences\nMuseums have evolved during the past twelve months. Because of the ongoing coronavirus pandemic, an extraordinary number of museums have temporarily shut their doors and have had to find a way to meet their visitors online. While some museums were already engaged in virtual programming, others had to catch up–and quickly–if they wanted to survive. If these responses can lead to real and fundamental institutional changes, then museums may have just tapped into their true audience potential.\nBefore the pandemic began, the Knight Foundation commissioned HG&Co to conduct a survey about museums’ digital strategies and readiness. This report looks at 480 museums of all types and sizes across the United States. While things have certainly changed since quarantine began, it does offer a snapshot of what the situation was like heading into the pandemic.\nFor example, the report found that only 25% of museums had an established digital strategy pre-pandemic. While leadership support was high, digital projects often were not being tracked and insights were shallow, if they were even collected at all. Quarantine has forced many museums to reckon with digital content in a way that they hadn’t before as programming shifted to online.\nFor many museums, like Musée d’Orsay in Paris, this change means virtual tours. For others, like the National Gallery of Art in Washington, D.C., online lectures and events. Some, like the Charles Dickens Museum in London, have worked on digitizing their collections or hosting exhibits online. Some museums are engaged in all three as well as other creative forms of virtual programming, like art classes or scavenger hunts.\nSome of these offerings include museums adapting their already planned in-person events to a digital format. Others are specially designed to be online content. Museums have been able to secure speakers for online events—speakers who might not otherwise have been able or willing to travel in order to participate.\nOne of the most exciting aspects of this shift to virtual programming has been museums’ ability to reach audiences far outside of their normal geographic ranges. While tourists obviously make up a large portion of a museum’s normal visitors, online content has removed geographic and financial barriers to experiencing lectures, exhibits, and tours.\nFor example, a few months ago, from the comfort of my own bedroom in North Carolina, I logged on to a virtual guided Zoom tour of the Jane Austen House in Chawton, England. At one point, those of us attending used the chat feature to chime in and share where we were. While there were some people only a short drive away from the house museum, others were tuning in from other continents around the world.\nWhen we interviewed Ken Howard, he told us that the North Carolina Museum of History is focusing on creating more digital content, partially because of the ability to reach audiences who might never set foot in the museum. He said, “The benefit of that is we’re now reaching people around the world! We’ve had people who have been accessing our resources from England, and Australia, and France, and Canada, and all over the place. Having these digital resources was a huge benefit because now our reach has grown beyond the borders of North Carolina.”\nThe Museum of Natural History in London is purposefully reaching out to Asian audiences, who would normally be a major part of their tourist traffic. They’ve partnered with Fliggy, an online travel brand, to livestream virtual tours to China. It’s a fantastic example of a museum realizing they must find new ways to reach audiences during this unprecedented time, but that those audiences can be a lot further away than normal.\nKimberley Floyd, site manager of the Zebulon B. Vance Birthplace in Asheville, North Carolina, told us they’re able to reach wider audiences with their virtual events than they have in the past with their in-person programming. For example, an embroidery workshop that would normally be attended by ten people had three times that many attend it virtually–and from several different states.\nMany museums seem to plan to continue their virtual programming even as the pandemic begins to wane. Both Floyd and Howard said their organizations plan to continue, and many museums have kept up their virtual programming as they begin to reopen. Obviously, it may not endure at the same level that is has during the pandemic, but it seems to be a trend that is here to stay.\nOf course, this not only affects museum visitors, but museum staff as well. It creates new work for those employed at museums and demands they are well-versed in skills that were not previously required, from video editing to handling large Zoom calls. For state-owned historic sites, it also may influence how funding is allocated as many budgets are based on visitation numbers. Floyd said, “How many staff we have is based on our visitation. The digital content is opening it up in a way and may complicate that.”\nWhat is clear now is that virtual programming is likely to be one of the most lasting effects of the coronavirus pandemic for the museum industry. Not only has it been practical over the last year, but it also increases accessibility to museums. It allows people who might not have been able to visit – whether because of physical accessibility issues or because of barriers to travel – to experience exhibits, collections, and events.\nAs virtual programming continues to grow, it is important that museums incorporate it into their strategic plans and collect demographic information about the people with whom they engage online. This shift towards virtual programming might have been coming all along and perhaps it was only accelerated by a global pandemic. Regardless of why or how it occurred, it is something that all museums should strongly consider embracing in the interest of realizing their true audience potential.']	['<urn:uuid:7914f20c-6752-49fc-bc76-fdefc20d0939>', '<urn:uuid:f24496c5-d599-450b-97ce-ce6ed1e283c2>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T22:03:18.033296	12	53	1583
76	What are the similarities in how indigenous youth in Guatemala and Mongolian pastoralists are impacted by modernization, and how do both groups struggle with preserving their traditional ways of life?	Both groups face significant pressures from modernization that threaten their traditional lifestyles. In Guatemala, Ixhil youth are increasingly abandoning traditional subsistence agriculture due to land scarcity and the allure of consumer culture, with many seeking migration to the US to 'get ahead in life' rather than maintaining their ancestral farming practices. Similarly in Mongolia, pastoralism is declining as more people move to urban areas, with predictions showing only 5% of the population will be rural by 2030. Both groups are seeing their youth prefer modern alternatives - in Guatemala through migration and wage labor, and in Mongolia through urbanization. The preservation of traditional ways is challenged in both cases - the Ixhil's connection to communal farming and the Mongolian pastoralists' nomadic herding lifestyle that traces back to 3000 BC are both at risk of being lost to modernization pressures.	"['mala-Farming-300×196.jpg” alt=”” width=”270″ height=”176″ />Mario Benedetti, the Uruguayan novelist and poet who best expressed the aspirations and dreams of the oppressed of Latin America, once said: “Publicity is a formidable dream-seller. But I don’t want to buy the dreams of someone else. I simply want to accomplish my own.”\nFor the last six years, I have worked in Central America with the Mennonite Central Committee (MCC) in a variety of projects aimed at helping youth in the region accomplish dreams of their own. According to Antony Sánchez and Irma Solano, regional directors for MCC in Guatemala and El Salvador, “MCC`s vision is to work with youth towards the construction of a future where youth can dream with dignity, recognize their own holistic potentials, and become active political subjects in their communities.”\nDespite these lofty intentions, reality has shown that all too often the dreams of Central American youth are thwarted by external forces that impede the creation of a different type of future, or by the learned inability to dream beyond the imposed limits of our society.\nIn the northern highlands of Guatemala, the Mayan-Ixhil people have lived as subsistence farmers in relative isolation for the last 2500 years. Only during the last 30 years have the Ixhil people suffered significant contact with outsiders. In the 1980`s, during the Guatemalan Civil War, the Ixhil people suffered 114 massacres at the hands of the Guatemalan Army. Since 2009, the region has been at the epicenter of multinational corporate greed, as dozens of international mining and energy companies have arrived seeking land to launch mining and mega-hydroelectric projects, all of which will contaminate local ecosystems and leave zero benefits to the Ixhil people.\nAccording to Fernando Morales, a young father and fisherman who lives downstream from the recently inaugurated Palo Viejo hydroelectric dam, “The worst part of the dam is that we can longer fish in the river. We used to be able to catch up to 20 pounds of fish a day, but now we have nothing. It´s clear to us that the (Guatemalan) government only protects the interests of big business while ignoring the voice of the people.” The arrival of these multinational corporations, however, is only one of many factors that have contributed to a context that is becoming ever more detrimental to the livelihoods of youth in the region.\nThe traditional livelihood of subsistence agriculture is becoming increasingly unsustainable for Ixhil youth due to unjust land concentration and population growth. In 1893, the population of Nebaj, the largest municipality in the Ixhil region, had a population of 5,945 people and every person had an average of 5.5 hectares (approximately 13 acres) of land to farm. In 2011, the population rose to 98,666 people and the average person had just a quarter of a hectare (approximately 0.6 acres) to farm, though there is a large percent of landless families that seek temporary work on coastal plantations and the urban centers. The lack of sufficient agricultural land and insufficient employment opportunities in the Ixhli region has led to a massive migration of youth to the United States in search of opportunities. Migration, in turn, has had the effect of skyrocketing land prices, making life all the more difficult for those that stay.\nFacing the reality of injustice and poverty that characterizes youth in the Ixhil Region, MCC has sought to support youth through vocational training and through organizing small cooperative enterprises. In 2011, one group of youth started a small bakery in an effort to provide local employment opportunities for youth in their village.\nAfter months of training and capacity building, the youth had converted an abandoned house in their village into a lively bakery that became the social center of the community. After various months of working and despite the enthusiasm and hope that the small bakery brought, the youth began to perceive that their small cooperative business was incapable of competing with the cheaper prices of more industrialized bakeries from the urban centers of Guatemala. The youth took out a small loan to try to increase their production and lower prices to compete with the imported products, but in the end the profit margin was too low to continue their enterprise.\nThe inability to sustain small, community oriented businesses due to the overwhelming presence of larger competitors brings to light another factor that is detrimental to the dreams and hopes of youth in the region. The ethic of competition inherent in the economic system in which we live overwhelmingly benefits the wealthy and powerful as it discriminates against the poor and powerless. When youth have insufficient land to farm, insufficient employment options, and insufficient funds to start businesses that can survive in a competitive economic system, then their dreams are either degraded into accepting demeaning work as sweatshop laborers or dependent upon the luck of a successful migration to the north; something that is more dangerous and improbable every day.\nJosé Corio is a 22 year old youth from a remote village in the Ixhil region of Nebaj who tried to migrate to the United States in 2009, but was caught at the US border and sent back. He is planning to try again in the next couple of months. “I am going to try to go to the North (USA) because I want to do something with my life. I want to “get ahead” and here there aren`t any opportunities for that.”\n“Getting ahead” (salir adelante in Spanish) is the most commonly used term by Ixhil youth to explain their goals and aspirations in life.\nWhy did you decide to migrate?\n“To get ahead in life.”\nWhy did you decide to continue your education?\n“To get ahead in life.”\nBut why do you work as a farmer?\n“Because there aren´t any other opportunities for work.”\nIt seems that to many youth in the Ixhil region, “getting ahead” means finding ways to live differently than did their grandparents. “Getting ahead” seems to imply the ability to obtain more commodities, make more money, and buy more stuff. “Getting ahead” seems to be synonymous with the supposed ease of living a consumer life.\nThe community of Xeucalvitz in northern Nebaj is home to 50 Ixhil families that have traditionally lived an agrarian lifestyle. In 2009 a Spanish company began buying up land along the local river in order to build a mega-hydroelectric dam. The majority of community leaders and Mayan ancestral authorities vehemently opposed the dam. With the beginning of the construction of the dam in April, however, local youth have been given sporadic opportunities to work for $10 USD per day.\nToday, there is a bitter generational division between the youth and the elders over the hydroelectric project in their communities. The elders maintain that the communities were never consulted as stipulated by international law and that the dam will ruin their lands and natural resources as have done other dams in the region. The youth, enticed by the rare opportunity to earn a small salary, affirm that the hydroelectric company is bringing “development” and “progress” to their community.\nJacinto Ceto Brito, ancestral authority of the community of Xeucalvitz remarked, “We don´t understand why the youth in our community are so determined to sell out and work for the (hydroelectric) company. Here (in our community) we have everything we need. We can get two corn crops a year. We can grow coffee and bananas and fruits. No one in our community has ever gone in need of food. The land provides for us, but the youth are determined to make more money by working with the company.”\nAfrican American author James Baldwin said: “Anyone who has ever struggled with poverty knows how extremely expensive it is to be poor.” Poverty in the Ixhil region is not a simple matter as close to 80% of children are malnourished. It would be unfair to overlook the callousness and suffering of poverty and its effects on the Ixhil people.\nHowever, there is no doubt that we live in a globalized, consumer driven world. The enticement of easy riches through migration, the desire for newer and better cell phones, nicer homes, fashion clothes, etc. is an influential part of youth mentality today. The question arises: Is that mentality compatible with the agrarian lifestyle traditionally lived by the Mayan people?\nRainer Maria Rilke once said: “If your daily life seems poor, do not blame it; blame yourself that you are not poet enough to call forth its riches.”\nThe Ixhil region and the traditional Mayan lifestyle undoubtedly offer a life full of riches: forested mountains, plentiful rain, fertile soil, abundant rivers, peaceful communities (compared to the rest of Central America), and a vivacious, rooted cultural legacy. But these are not the riches so coveted by the society we live in. They are not the riches that require on to “get ahead” to be able to obtain. They are not riches that can be quantified, or deposited into banks, or selfishly accumulated, or spent in meaningless and superficial ways. They are riches that are simply part of the land and part of the community. It is a wealth that is freely given to those who are content to accept them.\nThere are very few youth in the Ixhil region who are “poet enough” to appreciate and revere the riches of their lands and their communities. Gaspar Cobo, a youth from the region, is one of the few. When asked about his dreams and aspirations for his life, he remarked, “All I want is to have enough land to build a small house, raise my corn, potatoes and sheep, and enjoy the peacefulness of the mountain.”\nAnd what about “getting ahead” in life? “For me, there is no reason to “get ahead.” I am happy here. This place offers all that I need and want.”\nUndoubtedly, Gaspar represents a dying mentality amongst youth today who prefer the allure and enchantment of the infinite wonders of the western, consumer-driven world. But it is precisely the mentality that Gaspar embodies that MCC is trying to help preserve.\nIn the struggle against poverty and the chronic lack of opportunities that affect youth in the region, MCC supports small income-generating projects, advocates for more governmental programs benefitting youth, and supports communities in their resistance to the greed of multinational extractive and energy corporations.\nYet perhaps the best starting point in the struggle against poverty and injustice is to help youth discover the riches that exist in their traditional lifestyles and to learn to value those riches. The American author and poet Wendell Berry says, “The past is our definition. We may strive, with good reason, to escape it, or to escape what is bad in it, but we will escape it only by adding something better to it.” Life in the Ixhil region is filled with its share of adversities, and youth have good reason to seek ways to surmount those adversities. The American Dream of an easy, consumer lifestyle however, apart from being unsustainable and ecologically devastating, is hardly an improvement to traditional Mayan lifestyles. The challenge is to encourage youth not to “escape”, but to discover ways to “add something better” to a way of life that is filled with hidden riches.\nThe most important work then, is to help youth refuse to buy the dreams of others, as they simply seek to accomplish their own.\nTobias Roberts lives in Nebaj, Guatemala where he and his wife Yasmin Méndez are on assignment with Mennonite Central Committee, a development and peace agency of the Anabaptist churches in Canada and the U.S. He is a native of Bowling Green, Kentucky.', 'The country of Mongolia, sandwiched between Russia and China, is located in Northern Asia. Since ancient times, Mongolians have been engaged in pastoralism (Marin, 2008). Today, half of the inhabitants of the country still rely on pastoralism as both an economic activity and a way of life (National Statistics Office of Mongolia, 2009). Approximately 74 percent of land in Mongolia is used for pastoralist purposes, with most of the excluded land being forest and desert (MEGDM, 2014). The most common livestock used in this practice are camels, sheep, cows, goats, and horses (Neupert, 1999). A geographically diverse country, Mongolia, and the pastoralists within its borders, face an immense number of threats. The ways in which these threaten those that live off the land are numerous, but so are numerous the number of ways in which these hazards can be adapted to.\nHazards & Pastoralist Exposure\nIn Northern Asia, it is predicted that temperature has already risen by over 2℃ in the past 50 years, and is expected to increase 6℃ by the end of the 21st century (Hijioka et al., 2014). Landlocked countries like Mongolia are more likely to experience increases in temperature because they lack the temperature regulation that oceans provide land masses (Hijioka et al., 2014) and because of polar amplification due to high elevation and latitude of the country (Wang et al., 2014).\nTemperature rise can have a plethora of impacts on pastoralists. For instance, it has already decreased rangeland productivity by 20-30% in the past fifty years (Angerer et al., 2008). The decline in productivity has caused the average weight of livestock to decrease and thus both the animals\' economic value and use value to the pastoralists (Asian Development Bank, 2014). Higher temperatures in the winter also cause ice sheets to form over rangeland which prohibits animals from grazing which further deteriorates livestock health (Batima et al., 2005).\nMongolia is naturally susceptible to extreme winter weather events known as dzuds. These events are attributed with heavy snow, frozen groundwater, severe drops in temperature, and high winds (MEGDM, 2014). While dzuds occur naturally, an increase in these events can be attributed to changes in precipitation caused by climate change (Rao et al., 2015). Droughts in the summer before a dzud makes the event much more deadly to livestock because they are more weak due to lack of resources (Marin, 2010). Even without a preceding drought, dzuds can cause massive die off because the severe weather prevents access to food and water (MEGDM, 2014). Specifically, the extreme weather prevents livestock from foraging, weakens the animals, and makes water accessibility difficult (Rao et al., 2015).\nDzuds over two winters alone have been attributed to the mortality of 20 million livestock (Rao et al., 2015).\nChange in Precipitation\n- To the west and east precipitation is increasing, while it decreases in the south and central areas of the country (Batima et al., 2005).\n- In the northern, mountainous region of Mongolia aridity and droughts are expected to increase (Hijioka et al., 2014).\n- Mongolian pastoralists in the southern desert steppe reported “patchier rains” and an increase in droughts over their lifetimes (Marin, 2010).\nThis is a concern for pastoralists because they rely on precipitation patterns for their own migration patterns (MEGDM, 2014). Rain that occurs in patches creates islands of good foraging land that can be difficult to access which creates the risk of livestock mortality (Marin, 2010). The timing of precipitation has also changed to be later in the summer which means that pasture plants that herders rely on are not present during the expected season (Marin, 2010). Overgrazing coupled with decreased precipitation and rising temperatures causes desertification of previously fertile lands (MEGDM, 2014). For example, in the southern part of the country, the Gobi desert expands 150 kilometers every 20 years (Vernooy, 2011).\nSurface water in Mongolia is already drying up as a result of increased temperature and decreased precipitation (Wang & Zhang, 2012). Groundwater, the source of 80% of Mongolia’s water supply, is at risk of drying up due to overuse and climate change (Hasiniaina et al., 2010). The alternative to groundwater is the use of wells; however, pastoralists find that this limits mobility and prevents fattening of livestock (Wang & Zhang, 2012). Alternatively, the other major source of water for pastoralists are glaciers which have decreased by 30% in Mongolia since the 1940s (MEGDM, 2014). The finite nature of the glaciers supply poses a threat for future flows of freshwater once the glaciers and permafrost have entirely melted away (Hasiniaina et al., 2010).\nVulnerability of Pastoralists\nEconomic pressure on pastoralists is a recent concept in Mongolia. Prior to 1990, Mongolia was a communist country during which there was an abundance of state resource support for pastoralists and animals were under collective ownership. This limited the number of animals on the livestock to sustainable amount. Under capitalism Mongolia privatized herds which resulted in a significant increase in herd size (Rao et al., 2015). Simultaneously most government support for pastoralists was ended as the government shifted its focus on urbanization and industrial activities (Janes, 2010). The government has also began to encourage pastoralists to enter other activities and fail to acknowledge the key role pastoralism plays in the country\'s identity (Marin, 2008). Due to lack of support, inequality amongst pastoralists became more apparent as poorer pastoralists had a more difficult time obtaining necessary permits (Rao et al., 2015). Increased herd size has exacerbated desertification and increased water use, which further contributes to the hazards of climate change (Janes, 2010).\n“This is our life. We and our pastures are one body.”\n- One Khazakh herder on the importance of pastoralism (Fernández-Giménez et al., 2017)\nPastoralists are heavily reliant on the climate sensitive resource of livestock as both a food and cash source. Livestock die off as a result of climate change hazards such as dzuds, shifting precipitation, water scarcity, and temperature rise not only means that there is less food, it also results in less income. This income is necessary to pay for education, health care, and other basic needs, and without it, public health is severely threatened (Janes, 2010). It has been found that, among the poor pastoralists, there has been a decrease in height in children and an increase in emotional and physical problems among adults (Janes, 2010).\nPastoralism in Mongolia traces back to the Bronze Age of humanity, approximately 3000 BC, a heritage that is the pride for many pastoralists (Fernández-Giménez et al., 2017). The hazards of climate change, in conjunction with capitalist pressure on pastoralists, pose a risk to the continuance of the practice of pastoralism. Interviews with pastoralist parents have revealed their desire for their children to pursue ambitions outside of livestock husbandry (Fernández-Giménez et al., 2017). This loss of a pastoralist livelihood could be detrimental to the culture that has developed around a spiritual connection to the land (Fernández-Giménez et al., 2017). Thus, not only is a livelihood vulnerable to effects of climate change, but the culture that is intrinsically tied to it is too. More positively, this connection to the land can play a role in the resilience of the country as well.\nCoping with a Changing Landscape\nIn the face of climate change, more and more pastoralists are coping by seeking economic prospects in cities rather than in their native areas. This strategy has not only caused overcrowding and a significant rise in air pollution, but a loss of the pastoralist culture. The pastoralist life is a nomadic one, and so being settled in one place does not serve the traditions built around following the animals from season to season (Sarlagtay, 2004). Urbanization is only expected to increase and by 2030 it is predicted that only 5% of the country\'s population will be rural (Fernández-Giménez et al., 2017). The image to the left depicts the sprawl that has come with rapid urbanization (Batima et al., 2005).\nAdaptation and Resilience\nImprove breeding practices of livestock to select for animals that are best adapted to varied precipitation and decreased range productivity (ADB, 2014).\nCapture and store snowmelt and rainwater through use of small dams to help save precious water supplies (ADB, 2014)\nDecentralize rangeland decisions to give the power to control these decisions over the communities that are most affected by them (Fernández-Giménez, 2000)\nThe federal government must allocate more funds to rural communities to support development and capacity to maintain control over rangeland (Ahearn, 2018). These resources can also be used to recover from disaster such as dzuds (Fernández-Giménez, 2000)\nPractice sustainable pasture use by using rotational grazing methods (ADB, 2014) and by practicing co-management of rangeland (Vernooy, 2011)\n""Together, we collect hay and forage for the winter. We grow vegetables, comb goats, sheer sheep and ensure our river remains clean. These activities are quicker when carried out together.""\nBatkhuyag Tseveravajaa, head of the Uvurkhangai community in southern Mongolia, on the importance of pooling community resources in the face of climate change (DW, 2017).\nThe resilience of pastoralist people in Mongolia comes from the strength of immense local knowledge of the land that is at risk of being lost to the need to continue to grow herd size to reduce economic hardship. When outsiders call to preserve a cultural practice like pastoralism, it is important to not romanticize an ancient version of it but to preserve the practice in a way that is determined by the community (Marin, 2008). Based on this, an adaptation plan should focus on strengthening the resilience of the people through empowerment. Finance of the rural economy would allow local power over risk management to develop (Ahearn, 2018). Decentralizing decisions and money can also allow for co-management of communities over pasture lands. This would allow the decisions to be in the hands of those with the most ecological knowledge and thus restore a more sustainable system of livestock ownership (Vernooy, 2011). Additionally, more local management would allow for the most vulnerable, low-income pastoralists to be more resilient to the effects of extreme weather events and shifts in precipitation (Ahearn, 2018).\nAbout the Author\nEliza Gillilan graduated from St. Lawrence University in 2019 as an Environmental Studies and Sociology combined major with minors in Biology and African Studies. Her interest in pastoralism was first piqued after learning about the Maasai during a semester abroad in Kenya. Thanks to Dr. Jon Rosales\' Adaptation to Climate Change course, she was able to continue to explore her interest in pastoralists across the world who are connected to the Maasai by their fight to preserve a way of life in the face of climate change.\nAhearn, A. (2018). Herders and hazards: covariate dzud risk and the cost of risk management strategies in a Mongolian subdistrict. Natural Hazards, 92(1), 165-181. Retrieved from: https://doi.org/10.1007/s11069-017-3128-4\nAngerer, J., Han, G., Fujisaki, I., & Havstad, K. (2008). Climate Change and Ecosystems of Asia with Emphasis on Inner Mongolia and Mongolia. Rangelands, 30(3), 46-51. Retrieved from: https://www.jstor.org/stable/25145393\nAsian Development Bank. (2014). Making Grasslands Sustainable in Mongolia Herders’ Livelihoods and Climate Change. Ministry of Environment and Green Development of Mongolia, Asian Development Bank, Philippines. Retrieved from https://www.adb.org/sites/default/files/publication/31213/mongolia-making-grasslands-sustainable-herders-livelihoods.pdf\nBatima, P., Natsagdorj, L., Gombluudev, P., Erdenetsetseg, B. (2005). AIACC Working Paper No.12: Observed Climate Change in Mongolia. Assessments of Impact and Adaptations to Climate Change. Retrieved from http://www.start.org/Projects/AIACC_Project/working_papers/Working%20Papers/AIACC_WP_No013.pdf\nFernández-Giménez, M.E. (2000). The Role of Mongolian Nomadic Pastoralists’ Ecological Knowledge in Rangeland Management. Ecological Applications, 10(5), 1318-1326. Retrieved from https://www.jstor.org/stable/pdf/2641287.pdf.\nFernández-Giménez, M.E., Venable, N.H., Angerer, J., Fassnacht, S.R., Reid, R.S., & Khishigbayar, J. (2017). Exploring linked ecological and cultural tipping points in Mongolia. Anthropocene, 17, 46-69. Retrieved from https://doi.org/10.1016/j.ancene.2017.01.003\nHasiniaina, F., Zhou, J., & Guoyi, L. (2010). Regional assessment of groundwater vulnerability in Tamtsag basin, Mongolia using drastic model. Journal of American Science, 6(11). Retrieved from http://www.jofamericanscience.org/journals/am-sci/am0611/09_3069am0611_65_78.pdf\nHijioka, Y., Lin, E., Pereira, J.J., Corlett, R.T., Cui, X., Insarov, G.E., Lasco, R.D., Lindgren, E., & Surjan, A. (2014). Asia. In: Climate Change 2014: Impacts, Adaptation, and Vulnerability. Part B: Regional Aspects. Contribution of Working Group II to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change [Barros, V.R., C.B. Field, D.J. Dokken, M.D. Mastrandrea, K.J. Mach, T.E. Bilir, M. Chatterjee, K.L. Ebi, Y.O. Estrada, R.C. Genova, B. Girma, E.S. Kissel, A.N. Levy, S. MacCracken, P.R. Mastrandrea, and L.L.White (eds.)]. Cambridge University Press, Cambridge, United Kingdom and New York, NY, USA, pp. 1327-1370.\nJanes, C.R. (2010). Failed Development and Vulnerability to Climate Change in Central Asia: Implications for Food Security and Health. Asia-Pacific Journal of Public Health, 22(3), 236S-245S. Retrieved from: https://journals.sagepub.com/doi/pdf/10.1177/1010539510373008\nMarin, A. (2008). Between Cash Cows and Golden Calves: Adaptations of Mongolian Pastoralism in the ""Age of the Market."" Nomadic Peoples, 12(2), 75-101. Retrieved from http://www.jstor.org/stable/43123826\nMarin, A. (2010). Riders under storms: Contributions of nomadic herders’ observations to analysing climate change in Mongolia. Global Environmental Change, 20(1), 162-176. https://doi.org/10.1016/j.gloenvcha.2009.10.004\n(MEGD) Ministry of Environment and Green Development. (2014). Mongolia Second Assessment on Climate Change 2014. [Dagvadorj, D., Barjargal, Z., & Natsagdorj, L. (eds.)]. Retrieved from: http://www.jcm-mongolia.com/wp-content/uploads/2015/10/MARCC-Final-Bk-2014-book-lst.9.17-ilovepdf-compressed.pdf\nNational Statistics Office of Mongolia. (2009). 2009 Mongolia Statistical Yearbook. National Statistics Office of Mongolia, Ulaanbaatar, 2010.\nNeupert, R.F. (1999). Population, Nomadic Pastoralism and the Environment in the Mongolian Plateau. Population and Environment, 20(5), 413-441. Retrieved from: https://doi.org/10.1023/A:1023309002127\nRao, M.P., Davi, N.K., D’Arrigo, R.D., Skees, J., Nachin, B., Leland, C., Lyon, B., Wang, Shih-Yu, & Byambasuren, O. (2015). Dzuds, droughts and livestock mortality in Mongolia. Environmental Research Letters, 10. doi:10.1088/1748-9326/10/7/074012.\nSarlagtay, M. 2004. Mongolia: managing the transition from nomadic to settled culture, in J. Rolfe (ed.), The Asia-Pacific: A Region in Transition: 323-34). Honolulu (HI): Asia Pacific Center for Security Studies.\nWang, Q., Fan, X., & Wang, M. (2014). Recent warming amplification over high elevation regions across the globe. Climate Dynamics, 43(1-2), 87-101. Retrieved from: https://doi.org/10.1007/s00382-013-1889-3\nWang, X., & Zhang, Q. (2012). Climate Variability, Change of Land Use, and Vulnerability in Pastoral Society: A Case from Inner Mongolia. Nomadic Peoples, 16(1), 68-87. Retrieved from: https://www.jstor.org/stable/43123900\nVernooy, R. (2011). How Mongolian Pastoralists are Transforming Nomadic Pastoralism. Solution, 2(5), 82-87. Retrieved from https://dlc.dlib.indiana.edu/dlc/bitstream/handle/10535/8368/How%20Mongolian%20Herders%20Are%20Transforming%20Nomadic%20Pastoralism.pdf?sequence=1']"	['<urn:uuid:59b9dc60-76b1-4598-8f6b-ca2e96243ea0>', '<urn:uuid:c3c07c40-4e20-408e-8da5-286c95b2eb02>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T22:03:18.033296	30	139	4265
77	modern wine trends organic techniques impact	Modern wine trends show a significant shift toward natural and organic production methods across Canadian wine regions. Wineries like Southbrook and Okanagan Crush Pad have embraced organic and biodynamic farming, wild fermentation, and minimal intervention techniques. This trend is driven by growing consumer interest in sustainable products and unique flavors, particularly among wine enthusiasts seeking pure, terroir-driven expressions. The natural wine movement has transformed from a niche market to influence mainstream production, though it requires more careful attention in both vineyard management and winemaking processes. These methods result in distinctive wines that may display unconventional characteristics, challenging traditional wine expectations while promoting environmental sustainability.	['Oh, baby, baby, it’s a wild world\nIt’s hard to get by just upon a smile\nOh, it’s wild, all right. And for a growing number of Canadian winemakers, wild is the new cash cow as they venture further and further beyond the tried and true methods of making wine to create something that is, well, beautifully and wondrously weird.\nAnd wine geeks are lapping it up.\nOnly a scant few years ago being a wine baron was such a predictable vocation. Grow Riesling, Cabernet Franc, Chardonnay and Pinot Noir in the right places in Niagara, and, boom, you’re in business. Add Syrah and Pinot Gris to that list in the Okanagan, and, boom, you have a well-rounded portfolio. In Prince Edward County, it’s Pinot and Chard, that’s all you really need, right?\nHang on, not so fast.\nThere is a primordial shift occurring across the wine regions of this nation that cannot be ignored. No longer, as Cat Stevens sang, can you get by just upon a smile.\nYou need some boom-boom, more intriguing arrows in the quiver, if you want to capture the taste buds of an ever-growing geeked-out segment of wine lover, weaned on the Brett-infused, hopped up and psychedelic world of craft beer, who crave something new and completely off-the-wall different. They don’t want smiles. They want giggles. They want to be blown away, weirded out, with whatever kind of funk you can throw at them.\nA surprising number of winemakers are only too happy to oblige.\n“We’re trying to provoke a conversation,” Hinterland Wine Company’s Jonas Newman tells me as we sip fresh crushed Gamay straight from the tank at his winery in Prince Edward County winery.\nJonas, winemaker and co-founder of Hinterland Wine Company, acknowledges that finding an identity in a crowded market is the first priority for a young industry. He and his wife Vicky Samaras chose sparkling wine — from traditional bubbles, to charmat, to the ancient method (that pre-dates Champagne) of ancestral — to establish their identity. “But there will always be room for the kink,” Newman says, “as long as we are honest about it.”\nThat “kink” for Newman comes in the form of his Hinterland Ancestral, a vintage-dated sparkling Gamay with a cult-like following that clears the shelves nearly as quickly as the winery makes it.\nNamed for the original method of making sparkling wine, the ruby-hued Ancestral is produced using similar techniques first employed by the monks of Saint-Hillaire abbey in 1531 and is the tradition most famously implemented in Jura mountain region of Bugey-Cerdon in eastern France. The carbon dioxide is captured during primary fermentation, which produces a lightly spritzed, sweeter sparkling wine that’s lower in alcohol.\nBella Wines’ Jay Drysdale, on the Naramata Bench in the Okanagan Valley, wild ferments his organically grown, single-vineyard Gamay Ancestral and uses no sulphur. It is essentially an early-pick Gamay with sparkle; an austere, pure, fresh, tangy and intense wine that is as true an expression of the vineyard as you can get.\nWhile sparkling wines in Canada are evolving and being reinvented by winemakers, from low to zero dosage wines (no added sugar), to extended lees aging, to pétillant-naturel (pét-nat), to luxurious and decadent bubbly Icewines made from Vidal or Riesling or even “ice cuvées” crafted with a shot of Icewine added to the sparkling wine, there’s a far greater shift occurring in the edgier wineries, which find breaking from tradition (or returning to simpler times, depending on how you look at it) a more attractive and/or adventurous path to take.\nBack in Niagara, Southbrook winemaker Ann Sperling is presiding over a carboy of a roiling, gurgling mass of organically- and biodynamically-grown Vidal grapes that are happily fermenting naturally in the warmth of a late fall day.\nThis is Sperling’s second vintage of “orange” wine, a 100% natural (nothing added, including sulphur) white wine that gets its glowing amber colour from skin contact during fermentation. “For our first batch we wanted to gauge response,” she says. Sommeliers in Toronto were only too eager to jump on board and curious consumers quickly followed suit.\n“When you’re talking ‘no additives’ a segment of people gets excited. They are looking for something pure,” Sperling says. Only 150 cases of the Southbrook Orange Wine were produced in 2014; that has quadrupled for the 2015 vintage.\n“It’s like today’s music,” she says. “With better and better ways to reach niche consumers.”\nIn BC’s Okanagan Valley, one winery has taken the term “niche” mainstream.\nThe Okanagan Crush Pad, which includes the Haywire, Narrative and Samantha brands (among others), has transformed its entire portfolio into one of the most progressive in the country.\nOwned by Steve Lornie and Christine Coletta, with winemaking duties falling to Matt Dumayne, OCP has matured into one of the most exciting self-contained wineries in BC, dedicated to natural, organic, and pure wines using minimalist intervention in both the vineyard, and winery. What has been remarkable is the determination to turn the OCP brands into 100 per cent organic while using natural methods to craft the wines.\n“It has been an amazing amount of work,” says Coletta. “We were like parents of newborns: over-anxious, overbearing and hovering. Perhaps we are a little more relaxed about the process now and have built up confidence in our conversion process and the outcome.”\nAs of the 2015 vintage, oak barrels have been fully replaced by concrete tanks and amphorae for fermentation and aging. Sulphur dioxide is also used sparingly, if at all, and no additional additives including nutrients, tartaric, tannins and enzymes are used at all in the winemaking process. All the wines at OCP are wild fermented using only native (wild) yeast that is naturally occurring on grape skins.\nThat all adds up to a stunning portfolio from stem to stern of some of the most unique wines being made in Canada right now, not the least of which are two naturally-made “orange” style wines, a red and a white called Free Form, with no sulphur at all and no filtering.\nEvery bottle in the OCP stable has a story to tell, from concrete or amphora aged to natural, organic and wild fermentation. There’s even an “ancient method” sparkling wine made under the Narrative label that is produced using the pétillant-naturel (pét-nat) method that’s bottled during fermentation without filtering, fining and stabilizing to show the more delicate and interesting characters unique to the vintage.\nOh, baby, it’s a wild world, all right, and it’s getting wilder. To wit:\nRoad 13 Vineyards in the Okanagan Valley pulled out staples such as Pinot Noir and Chardonnay to make room for more Rhône plantings. The future is being reflected in the new VRM 2013, a Rhône blend of Viognier, Roussanne and Marsanne, the 2014 Marsanne and 2013 Syrah Mourvèdre. Other BC wineries, Stag’s Hollow and Pentage, for example, have also placed an emphasis on Rhône varietals.\nIn Niagara, where weather limits the varieties that can be grown, a long list of wineries has added appassimento- (drying of grapes) style wines to their portfolios. Foreign Affair, Rennie Vineyards, Big Head, Reif, Colaneri and Pillitteri all use some method of drying grapes to add complexity and concentration to many of their reds (and even a few whites).\nThen there are the “fringe” varieties: Albariño (Stag’s Hollow loves this Spanish grape so much they have sent Sauvignon Blanc to the junk pile and are replacing it with this), Sémillon (a beautiful wine on its own or blended with Sauvignon Blanc … it suffered greatly in Niagara from two bad winters but finds consistent success in BC), Gruner Veltliner (Culmina makes a fabulous wine out of this Austrian staple), and a whole whack of experimental and established plantings including (but not limited to): Malbec, Petit Verdot, Sangiovese, Tannat, Tempranillo, Corvina, Rondinella and Molinara … the list is endless and ever growing.\nMore profound, perhaps, and more controversial, for sure, is the stylistic approach being pursued by some modern winemakers trying to etch their names not only on the Canadian wine-scape but also on a global scale.\nTake renegade winemaker François Morissette, for example.\nShunned by many of his own peers in Niagara, who argue he doesn’t support the local wine industry, but loved by many outside his own region for the unique approach he takes at the Pearl Morissette Winery in Jordan, Morissette doesn’t have time to deal with all the drama surrounding his style of winemaking.\nThe truth is he doesn’t really care what his critics think; no, not one bit.\nHe’s been a pain in the ass for VQA, viewed as a snake oil salesman by many veteran Niagara winemakers and an outspoken cheerleader for the style of wines he wants to make.\nNiagara, for Morissette, is just a place in the world to make his wines. In his mind, his wines have no borders and he’s just as thrilled to sell a couple of cases to some obscure wine shop in Germany as he is getting listings at the LCBO and SAQ, perhaps more so.\nHe loves the fact that his wines are sought out almost exclusively by geeks, collectors, and somms, either here or in far-flung places where his wines are, shall we say, more appreciated.\nI have had more people tell me they dislike his wines than I have heard from fans. Which, to him, is totally cool; he doesn’t make wine for the masses, only to a niche audience that appreciates the nuances and the pure, minimal-interventionist, low sulphur and reductive style of winemaking he employs. It’s not for everyone.\n“I’m not making wine for Ontario, I’m making wine for the world,” he goes on to say.\nMorissette is an island onto himself. A lone wolf with some crazy ideas that manifest themselves in the wines that he makes. You either dig them, or you don’t. And a lot don’t.\nIt is texture that defines his wines, texture achieved through oxidative winemaking, the catalyst for skepticism over his wines.\nMorissette is an experimental winemaker, pushing wild ferments to the limit, while stretching the boundaries of what VQA deems acceptable and teasing consumers’ palates. “From Day One we have not introduced anything to the wines but a little bit of sulphur,” he says. So, most of his wines are exposed to oxygen, very little filtration and fining (if any) takes place and sulphur is used only if he thinks a wine needs it. His ferments take place naturally in large neutral oak foudres, concrete eggs and increasingly in clay amphorae.\nIn his words, “you can have some VA (volatile acidity), you can have some brett (brettanomyces) but it can’t take over, it can’t be a justification for being lazy.”\nHis wines garner a lot of attention, which rankles traditionalists in Niagara to no end. “Being different is what our industry needs. We need the troublemakers, the misfits, the crazy ones,” a Niagara winery industry person, who remains anonymous here, told me.\n“But there always seems to be drama around each one of their wines. It’s not a good or bad thing, just an observance. There’s always a story and a drama around their stuff.\n“Oddly enough they have become an iconic winery here in a very short time. And this drama is how it’s been done. But this happens everywhere. Marcel Deiss? Joly? Piuze? Bourdy. Wine would be so boring without it. So, I welcome it, our region now has relevance because of them but at the end of the day it’s drama.”\nOnly time will tell if all this drama will succeed at the box office, but as long as there are those willing to go along for the ride, the script will continue to be written.\nTake a Walk on the Wild Side (Five wines that are like nothing you’ve tasted before)\n94 Inniskillin Sparkling Vidal 2014, Niagara ($80/375 mL)\nThe nose is a gorgeous mélange of orange marmalade, honeycomb, peach and mango. It has a vigorous mousse on the palate to go with sweet, compoted fruits, fresh and delicate notes of pineapple, tangerine and peach and a luxurious texture that’s accentuated by the tiny bubbles that tickle the palate through the finish. A rich, exotic treat that is well worth the high price.\n92 Road 13 VRM 2013, Okanagan ($29)\nThe blend is 72% Viognier, 25% Roussanne and a splash of Marsanne with fermentation and aging in older (neutral) oak barrels. Love the nose of poached pear, apricot, soft vanilla, spice and minerality. It is defined by its texture on the palate with round and fleshy fruit and a broad range of pear, exotic tropical fruits and subtle spice notes. Delicious stuff.\n92 Haywire Free Form Red 2014, Okanagan ($55)\nThe Red Free Form is another natural wine made with no intervention in the cellar. It’s fermented with wild yeast in amphora where it aged on the skins for 8 months. No sulphur was added. It has a pure and attractive nose of extracted cherry, raspberry and black currants. It is a beautiful wine on the palate with a rich broth of red fruits, crushed currants, raspberry, bramble and lovely silky tannins.\n91 Pearl Morissette Cuvée Mon Unique Gamay 2013, Niagara ($29)\nThis is wild-yeast fermented in open wood fermenters and is made with zero added sulphur. It shows savoury cherry, boysenberry, blueberry and bramble fruit on the nose. It’s so fresh and alive on the palate with a range of purple fruits and cherries in a juicy, yet vibrant, style.\n90 Narrative Ancient Method 2013, Okanagan ($40)\nThis pétillant-naturel (pét-nat) style of sparkling wine was bottled during fermentation without filtering, fining and stabilizing to show the more delicate and interesting characters unique to the 2013 vintage. It is made with 100% Chardonnay and each bottle will display different nuances depending on when it is opened. The nose shows lemony biscuit, racy grapefruit and cream. It has a gentle mousse followed by a vibrant core of citrus, melba toast and vanilla cream.\nPhoto: Francois Morissette from the Pearl Morissette Winery in Jordan', 'When it comes to wine making, there are various techniques that winemakers employ to create their desired styles and flavors. One approach that has gained popularity in recent years is natural wine making. Natural wine making is a process that focuses on minimal intervention, allowing the grapes to express themselves fully while showcasing the unique characteristics of the terroir. In this article, we will delve into the details of natural wine making and explore the various techniques used in the process.\nThe Importance of Organic and Biodynamic Farming\nOne of the key components of natural wine making is the use of organic and biodynamic farming methods. These methods prioritize the health of the vineyards and promote sustainability. Organic farming avoids the use of synthetic chemicals, pesticides, and fertilizers, focusing on natural and sustainable practices. Biodynamic farming takes it a step further by utilizing the lunar calendar and incorporating holistic agricultural practices. By implementing these farming techniques, winemakers can ensure that the grapes are grown in a way that minimizes harm to the environment and maximizes the quality of the fruit.\nGentle Grape Harvesting and Sorting\nOnce the grapes have ripened, winemakers practicing natural wine making techniques employ gentle harvesting methods. Manual picking is preferred to ensure that only the healthiest grapes are selected. The grapes are carefully sorted to remove any damaged or unripe fruit. This meticulous process helps maintain the integrity of the grapes and prevents any unwanted flavors from being introduced into the wine.\nMinimal Intervention in the Winery\nAfter the grapes have been harvested and sorted, they are transported to the winery for fermentation. Natural winemakers believe in minimal intervention during the winemaking process, allowing the grapes to ferment naturally without the use of commercial yeasts. Instead, the naturally occurring yeasts present on the grape skins initiate the fermentation process. This approach allows for the expression of the unique flavors and aromas of the grapes, resulting in a wine that truly reflects its terroir.\nNo Added Sulphites and Low Sulfur Levels\nIn conventional winemaking, sulfur dioxide (SO2) is often added as a preservative to prevent oxidation and microbial spoilage. However, natural winemaking techniques avoid or limit the use of added sulfites. Sulfur dioxide can mask the flavors and aromas of the wine, so minimizing its presence allows the true character of the grapes to shine through. This approach may result in wines that are more vibrant and expressive, although they may require careful storage and handling.\nAged in Neutral Containers\nAnother technique used in natural wine making is aging the wine in neutral containers, such as old oak barrels or concrete tanks. Unlike new oak barrels, which can impart strong flavors and aromas to the wine, neutral containers allow for a more subtle aging process. This approach preserves the natural flavors and aromas of the grapes without adding any additional elements to the wine.\nThe Rise of Natural Wine\nIn recent years, natural wine has experienced a surge in popularity among wine enthusiasts. This trend can be attributed to the growing interest in sustainable and organic products, as well as a desire for wines that showcase the unique characteristics of their origin. Natural wine offers a different experience from conventional wines, with a focus on purity and minimal intervention. The natural wine movement has inspired winemakers around the world to explore these techniques and create wines that are both delicious and environmentally friendly.\nAs natural wine making gains traction, more winemakers are adopting these techniques and incorporating them into their practices. By understanding and appreciating the detailed process behind natural wine making, wine enthusiasts can develop a deeper understanding of the wines they enjoy and support the growing movement towards sustainable and authentic wine production.']	['<urn:uuid:4aa58d42-e0a0-41d2-99b5-14286407f445>', '<urn:uuid:7825dc97-3172-461d-b279-201bd25a93c1>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T22:03:18.033296	6	104	2957
78	How do dark periods affect cannabis flowering and greenhouse light control?	Cannabis requires sufficient dark periods for flowering, typically 12 hours of darkness and 12 hours of light, with some Indica varieties needing 8 hours and Sativas up to 13 hours of darkness. Greenhouse growers must use blackout shields or curtains to contain light and prevent light pollution, installing them at roof, sidewalls, and gable walls while maintaining 30-50cm distance from light fixtures.	"['It is legal for adults over 21 years of age to grow limited quantities of marijuana in states such as Colorado, Alaska, Washington DC, Oregon (after July 15, 2015) and Washington state (medical marijuana only).\nIt’s a complex task and one that requires step- by-step guidance if good quality is expected. One of the most important ingredients for a successful grow is good quality seeds.? That said, here is an overview of the basics:\nSoil is required with sufficient nutrients. Commercial potting soils usually indicate the percentages of the fundamental nutritional elements, i.e., nitrogen, phosphorus and potassium. Nutrients are often provided to the soil via fertilizers but such practice requires caution. The pH should be between 5.9 and 6.5. This pH can be adjusted and commercial fertilizers (even organic) tend to make the soil more acidic (ie decreasing its pH).\nThe optimal day temperature range for cannabis is 24 to 30 ?C (75 to 86 ?F). Temperatures above 31 ?C (88F) and below 15.5 ?C (60F) decrease THC potency and slow growth. At 13 ?C the plant undergoes a mild shock, though some strains withstand frost temporarily.\nLight can be natural for outdoor growing or artificial for indoors. Under artificial light, the plant typically remains under a regime of 16?20 hours of light and 4?8 hours of darkness from the germination until flowering, although the plant can use a full 24 hours of light without harm. There is an ongoing debate over the importance of the “dark period”. It has been shown that, when subjected to a regimen of constant light without a dark period, most types of flora, including cannabis, will begin to show signs of decreased photosynthetic response, lack of vigor, and an overall decrease in vascular development. There is no defined flowering “stage”, unless you are discussing an auto-flowering variety (Cannabis Ruderalis). Typically, flowering is induced by changing the light schedule to 12 hours of light and 12 hours of darkness. Flowering in cannabis is triggered by a hormonal reaction within the plant that is initiated by an increase in length of its dark cycle, i.e. the plant needs sufficient prolonged darkness for bract/bracteole (flowering) to begin. Some Indica varieties require as little as 8 hours of dark to begin flowering, whereas some Sativa varieties require up to 13 hours.\nWatering frequency and amount is determined by many factors, including temperature and light, the age, size and stage of growth of the plant and the medium’s ability to retain water. A conspicuous sign of water problems is the wilting of leaves. Too much water can kill young cannabis plants. The additives in tap water can reduce plant growth and can be removed with reverse osmosis filtration but is expensive and produces poorer results than well or spring water.\nNutrients are taken up from the soil by plants. Nutrient soil amendments (fertilizers) are added when the soil nutrients are depleted. Fertilizers can be chemical or organic, liquid or powder, and usually contain a mixture of ingredients. Commercial fertilizers indicate the levels of NPK (nitrogen, phosphorus, and potassium). During the vegetative stage, cannabis needs more N than P and K, while during the flowering stage, P is more essential than N and K. The presence of secondary nutrients (calcium, magnesium, sulfur) is recommended. Micro nutrients (e.g. iron, boron, chlorine, manganese, copper, zinc, molybdenum) rarely manifest as deficiencies. Because Cannabis’ nutrient needs vary widely depending on the variety, they are usually determined by trial and error and fertilizers are applied sparingly to avoid burning the plant.\n*The above text is compliments of WikipediaGrow MJ', 'The rapidly expanding cannabis cultivation market has experienced many growing pains including excessive energy and water use that can cause negative environmental impacts. One little-known consequence of greenhouse marijuana cultivation, in particular, is marijuana light pollution. Greenhouse growers have many advantages over indoor and outdoor cultivation methods but also require artificial lighting systems to create a dark photoperiod.\nCannabis plants depend on a light-dark cycle to know when to switch from vegetative growth to flowering. Starting in late summer, daylight hours decrease signaling the end of the plant\'s life cycle and the start of the flowering period. Greenhouses that employ lighting systems at any point of a plant\'s life cycle can not only drive up energy bills but also disturbing neighbors and wildlife, if not properly contained.\nMarijuana Light Pollution\nGenerally, cannabis plants begin to flower when they are receiving 12 hours of daylight and 12 hours of darkness. Many other photoperiod plants also require certain daylight hours to start different life cycle stages. Some growers may opt for more hours of daylight depending on the strain. Greenhouse cultivators will use artificial lighting to compensate for the lack of sunlight through the vegetative state into the flowering stage. If there\'s no covering, the lights will shine out into the sky contributing to light pollution. Marijuana light pollution can end up harming the surrounding wildlife and humans, too.\nAccording to the International Dark-Sky Association, light pollution is ""the inappropriate or excessive use of artificial light."" Light pollution is broken down into multiple components including glare (blinding light), skyglow (brightening of the night sky in populated areas), light trespass (bad control of lighting), and clutter (grouping of lights). The 2016 ""World Atlas of Artifical Night Sky Brightness"" found that 80 percent of the world\'s population lived under skyglow, and 99 percent in the U.S. and Europe. In terms of marijuana light pollution, light-abatement techniques are a necessity to reduce the consequences of ecosystems and humans.\nThe Consequences Of Marijuana Light Pollution\nMarijuana light pollution can negatively affect plant life, birds, insects, and other nearby animals. Light pollution can affect nocturnal prey that are left exposed and more vulnerable to predators hunting at night. A complete ecosystem can be affected if certain prey become more scarce. One study found that frogs reduced their croaking or mating call when exposed to artificial light. Artificial light also has the power to confuse long-distance migratory birds and affect their reproductive physiology.\nMarijuana light pollution also affects neighbors\' circadian rhythms, the body\'s natural sleep/wake cycle. Sleep cycles can be dramatically affected by increased exposure to light. For example, the artificial blue light emitted from smartphones has been linked to heart disease, obesity, and diabetes. Another study on the negative effects of artificial light found that nurses who worked more rotating night shifts had a moderate increased risk in developing breast cancer.\nMarijuana light pollution can sometimes be confused with a wildfire. When the cloud cover and fog is low enough, the moisture in the air can enhance the glow of the light. Wildfires are a real risk for many cannabis cultivators. Depending on the topography of the location, some neighbors may believe the glow of artificial lights is a wildfire. Distressed calls to firefighters can waste resources by pursuing a false alarm.\nMarijuana Light Pollution Hits The Countryside\nMarijuana light pollution from greenhouse sites is turning up more and more in rural locations, which can disrupt neighbors who enjoy looking up at the stars. Whether it\'s the lack of regulations or lack of knowledge about the light pollution issue, many cultivators are facing backlash from neighboring residents. Last year, Canopy Growth heard complaints from Langley, B.C. residents about the bright lights emanating from its facilities. In response, Canopy put up shades on the sides of their greenhouse facilities to reduce the light pollution.\nGreenhouse growers can incorporate similar methods to reduce marijuana light pollution, but it may require a few weeks or months to install the shades or covers. Some areas don\'t have regulations for greenhouse cultivation, which can not only disturb neighbors, but also contribute to other negative effects on the environment including increasing energy use, damaging the ecosystem and wildlife, and harming humans. In the rural areas, the increased energy use can also increase the risk of diesel spills and wildfires from generators.\nHumboldt County, for example, has regulations that state that ""cultivators using artificial lighting for mixed-light cultivation shall shield greenhouses so that little to no light escapes. Light shall not escape at a level that is visible from neighboring properties between sunset and sunrise. Residents can complain to the Humboldt County Planning Division and cultivators have 10 working days to fix the light shielding issue.\nHow To Prevent Marijuana Light Pollution\nGreenhouse growers are using blackout shields to reduce marijuana light pollution. When cannabis plants begin the 12/12 photoperiod, greenhouse growers can employ rubberized fabrics or portable awnings, for example, to contain light. Lighting curtains are helpful not only to reduce marijuana light pollution but also reflect light back to the cannabis plant. Blackout curtains increase the lighting efficiency by reducing the amount of light that goes out of the greenhouse.\nGreenhouse Product News (GPN) suggests installing blackout curtains at the roof, sidewalls, and gable walls to reduce the light going out. They also suggest using roll-up screens or solid screens to avoid light pollution from coming out of the sides. Minimal gapping of the curtain can be used to reduce the humidity levels caused by the contained heat from lights.\nEach municipality will have different requirements to reduce marijuana light pollution and maintain dark sky compliance. Some screens can block over 99 percent of light by using white reflective layers around a starting black layer. Lighting screens also enable greenhouse cultivators to use whatever lighting they please without concern for disrupting neighbors. Growers can make the daylight longer depending on the crop they\'re growing.\nIssues With Blackout Curtains\nThe complementary use of blackout curtains and lights like high-pressure sodium (HPS) lights can generate a lot of heat. With bulbs emitting over 1,000 watts each, greenhouse cultivation sites that use blackout curtains can have an increased chance of fire damage. The distance between blackout screens and lights is vital. Depending on the type of screen used, the heated greenhouse can create a fire risk.\nCultivators recommend installing light fixtures as high as possible. GPN suggests installing the blackout curtains ""as close to the bottom cord of the truss as possible to provide a flat surface to mount to."" It\'s recommended to keep curtains between 30 to 50 cm away from the light fixture. Measuring from the ballast may still be too close and increase the chance for curtains to burn or melt. Growers can ask their screen manufacturer to determine melting points or ensure that their screens have fire retardants.\nMarijuana light pollution doesn\'t have to cause widespread ecological damage before cultivators and regulators step in to prevent more consequences. Light pollution is not restricted to marijuana cultivation, but has gained more notice. Unlike other ecological impacts like threatening wildlife and damaging forests, marijuana light pollution has a fairly easy fix. Educating cultivators and regulators on the importance of blackout screens can reduce pollution from lights in greenhouse facilities.']"	['<urn:uuid:b9612154-e1c6-469f-950c-fcc4456677cc>', '<urn:uuid:cc8be24e-a985-4f50-8049-97d8d2497dff>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T22:03:18.033296	11	62	1800
79	why is vega magnetic field important discovery	The discovery of a magnetic field on Vega is significant because it suggests the existence of a previously unknown type of magnetic stars in the intermediate-mass domain. Vega may be the first confirmed member of a larger, unexplored class of weakly-magnetic stars that can now be studied with current stellar spectropolarimeters.	"[""|Reference : The rapid rotation and complex magnetic field geometry of Vega|\n|Scientific journals : Article|\n|Physical, chemical, mathematical & earth Sciences : Space science, astronomy & astrophysics|\n|The rapid rotation and complex magnetic field geometry of Vega|\n|Petit, Pascal [> >]|\n|Lignières, F. [Laboratoire d'Astrophysique de Toulouse-Tarbes, Université de Toulouse, CNRS, France]|\n|Wade, G. A. [Department of Physics, Royal Military College of Canada, PO Box 17000, Station Forces, Kingston, Ontario, Canada]|\n|Aurière, M. [Laboratoire d'Astrophysique de Toulouse-Tarbes, Université de Toulouse, CNRS, France]|\n|Böhm, T. [Laboratoire d'Astrophysique de Toulouse-Tarbes, Université de Toulouse, CNRS, France]|\n|Bagnulo, S. [Armagh Observatory, College Hill, Armagh BT61 9DG, Northern Ireland, UK]|\n|Dintrans, B. [Laboratoire d'Astrophysique de Toulouse-Tarbes, Université de Toulouse, CNRS, France]|\n|Fumel, Aurélie [Université de Liège - ULg > Département d'astrophys., géophysique et océanographie (AGO) > Astrophysique et traitement de l'image]|\n|Grunhut, J. [Department of Physics, Royal Military College of Canada, PO Box 17000, Station Forces, Kingston, Ontario, Canada]|\n|Lanoux, J. [Centre d'Étude Spatiale des Rayonnements, Université de Toulouse, CNRS, France]|\n|Morgenthaler, A. [Laboratoire d'Astrophysique de Toulouse-Tarbes, Université de Toulouse, CNRS, France]|\n|Van Grootel, Valérie [Laboratoire d'Astrophysique de Toulouse-Tarbes (LATT) > > > >]|\n|Astronomy and Astrophysics|\n|Yes (verified by ORBi)|\n|[en] stars: magnetic field ; stars: rotation ; stars: atmospheres ; stars: imaging ; stars: individual: Vega|\n|[en] Context. The recent discovery of a weak surface magnetic field on the normal intermediate-mass star Vega raises the question of the origin of this magnetism in a class of stars that was not previously known to host detectable magnetic fields.\n<br />Aims: We aim to confirm the field detection reported by Lignières et al. (2009, A&A, 500, L41) and provide additional observational constraints about the field characteristics, by modelling the large-scale magnetic geometry of the star and by investigating a possible seasonal variability of the reconstructed field topology.\n<br />Methods: We analyse a total of 799 high-resolution circularly-polarized spectra collected with the NARVAL and ESPaDOnS spectropolarimeters during 2008 and 2009. Using about 1100 spectral lines, we employ a cross-correlation procedure to compute, from each spectrum, a mean polarized line profile with a signal-to-noise ratio of about 20 000. The technique of Zeeman-Doppler Imaging is then used to determine the rotation period of the star and reconstruct the large-scale magnetic geometry of Vega at two different epochs.\n<br />Results: We confirm the detection of circularly polarized signatures in the mean line profiles. The signal shows up in four independent data sets acquired with both NARVAL and ESPaDOnS. The amplitude of the polarized signatures is larger when spectral lines of higher magnetic sensitivity are selected for the analysis, as expected for a signal of magnetic origin. The short-term evolution of polarized signatures is consistent with a rotational period of 0.732 ± 0.008 d. The reconstruction of the magnetic topology unveils a magnetic region of radial field orientation, closely concentrated around the rotation pole. This polar feature is accompanied by a small number of magnetic patches at lower latitudes. No significant variability in the field structure is observed over a time span of one year.\n<br />Conclusions: The repeated observational evidence that Vega possesses a weak photospheric magnetic field strongly suggests that a previously unknown type of magnetic stars exists in the intermediate-mass domain. Vega may well be the first confirmed member of a much larger, as yet unexplored, class of weakly-magnetic stars now investigatable with the current generation of stellar spectropolarimeters.\n|9 pages, 8 figures|\n|File(s) associated to this reference|\nAll documents in ORBi are protected by a user license.""]"	['<urn:uuid:f752eb6a-118a-421b-bc45-0e23a5f3e645>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T22:03:18.033296	7	51	583
80	What are the key differences between how radar receivers and cell phone signal boosters handle signal strength reduction to prevent interference?	Radar receivers use sensitivity time control (STC) circuits that automatically reduce receiver gain to zero when the transmitter fires and gradually increase it to prevent interference from transmitted pulse leakage, with effects limited to approximately 50 miles. In contrast, cell phone signal boosters use attenuation, which can be automatic or manual, to reduce signal strength. The automatic attenuation in cell phone boosters reduces both incoming and outgoing signals equally for all antennas, while manual attenuation allows separate control of uplink and downlink signals, providing more flexibility in preventing interference from feedback loops between outdoor donor antennas and indoor broadcast antennas.	"['RECEIVER SPECIAL CIRCUITS\nThe performance efficiency of radar receivers is often greatly decreased by interference from one or\nmore of several possible sources. Weather and sea return are the most common of these interference\nsources, especially for radar systems that operate above 3,000 megahertz. Unfavorable weather conditions\ncan completely mask all radar returns and render the system useless. Electromagnetic interference from\nexternal sources, such as the deliberate interference by an enemy, called jamming or electronic counter\nmeasures (ECM), can also render a radar system useless. Many special circuits have been designed to\nhelp the radar receiver counteract the effects of external interference. These circuits are called VIDEO\nENHANCEMENT FEATURES, ANTIJAMMING CIRCUITS, or ELECTRONIC COUNTER-\nCOUNTERMEASURES (ECCM) CIRCUITS. This section will discuss, in general terms, some of the\nmore common video enhancement features associated with radar receivers.\nAutomatic Gain Control (AGC)\nMost radar receivers use some means to control the overall gain. This usually involves the gain of\none or more IF amplifier stages. Manual gain control by the operator is the simplest method. Usually,\nsome more complex form of automatic gain control (agc) or instantaneous automatic gain control (iagc) is\nused during normal operation. Gain control is necessary to adjust the receiver sensitivity for the best\nreception of signals of widely varying amplitudes. Agc and iagc circuits are designed with, a shut-off\nfeature so that receiver gain may be adjusted manually. In this way, manual gain control can be used to\nadjust for best reception of a particular signal.\nThe simplest type of agc adjusts the IF amplifier bias (and gain) according to the average level of the\nreceived signal. Agc is not used as frequently as other types of gain control because of the widely varying\namplitudes of radar return signals.\nWith agc, gain is controlled by the largest received signals. When several radar signals are being\nreceived simultaneously, the weakest signal may be of greatest interest. Iagc is used more frequently\nbecause it adjusts receiver gain for each signal.\nThe iagc circuit is essentially a wide-band, dc amplifier. It instantaneously controls the gain of the IF\namplifier as the radar return signal changes in amplitude. The effect of iagc is to allow full amplification\nof weak signals and to decrease the amplification of strong signals. The range of iagc is limited, however,\nby the number of IF stages in which gain is controlled. When only one IF stage is controlled, the range of\niagc is limited to approximately 20 dB. When more than one IF stage is controlled, iagc range can be\nincreased to approximately 40 dB.\nSensitivity Time Control (STC)\nIn radar receivers, the wide variation in return signal amplitudes make adjustment of the gain\ndifficult. The adjustment of receiver gain for best visibility of nearby target return signals is not the best\nadjustment for distant target return signals. Circuits used to adjust amplifier gain with time, during a\nsingle pulse-repetition period, are called stc circuits.\nSensitivity time-control circuits apply a bias voltage that varies with time to the IF amplifiers to\ncontrol receiver gain. Figure 2-29 shows a typical stc waveform in relation to the transmitted pulse. When\nthe transmitter fires, the stc circuit decreases the receiver gain to zero to prevent the amplification of any\nleakage energy from the transmitted pulse. At the end of the transmitted pulse, the stc voltage begins to\nrise, gradually increasing the receiver gain to maximum. The stc voltage effect on receiver gain is usually\nlimited to approximately 50 miles. This is because close-in targets are most likely to saturate the receiver;\nbeyond 50 miles, stc has no affect and the receiver operates normally.', ""Manually Adjustable Gain Signal Boosters & Amplifiers allow cell phone signal booster or repeater kit owners to manually adjust gain of each band by increasing or decreasing its incoming signal strength into the home, office, or building. This is how to tune your cell phone signal booster whether made by Wilson, SureCall or any other manufacturer. Automatic Gain Control signal boosters do not allow ability to adjust Gain manually - It adjusts Gain automatically to compensate for signal fluctuations to prevent oscillation due to overpowering.\nThe AGC or Auto Gain Control can be found in many SureCall boosters including Fusion5x, Fusion5s (with attenuator dials to adjust attenuation or signal being sent from booster to interior antennas), and Flare. MGC or Manual Gain Control is a feature in several SureCall signal boosters including Fusion4Home, Fusion7, Force5, Force5 Industrial, and Force7. In manual gain signal boosters, the settings arrive from the factory as ideal in most circumstances but can be adjusted to fine-tune reception depending upon specific signal strength characteristics at a particular location.\nWatch the two videos below to find out what is attenuation, the differences between AGC and Attenuation, advantages and disadvantages of both Automatic and Manual Gain Control signal boosters, and how to perfom manual attenuation to reduce signal going from amplifier to internal antenna(s) and prevent oscillation due to proximity of external and internal antenna(e). Find out exactly what is manual attenuation and how to manually manipulate dip switches. See how helpful it is in certain circumstances to adjust attenuation for enjoying peak performance of your SureCall signal boosters. These questions and answers will definitely help you understand this topic better, and maybe even make you an expert so you can help others understand the differences, disadvantages, and benefits of choosing Automatic or Manually Adjustable Gain Signal Boosters.\nVideo # 1: Auto OR Manual Gain Adjusting Signal Boosters?\nThe differences, advantages, and disadvantages of choosing Auto (AGC) or Manual Gain Control (MGC) Signal Boosters.\nAttenuation is a very important word when it comes to properly setting up any cellular signal booster. But what exactly is attenuation? Attenuation is a general term that refers to any reduction in the strength of a signal. It is also referred to as loss.\nHow does a reduction in signal help us ensure that a signal booster will work at its best? In SureCall systems, there's a difference between automatic attenuationand automatic gain control or AGC. AGC adjusts the booster automatically to compensate for signal fluctuations. Automatic attenuation automatically reduces the booster’s incoming and outgoing signal reception, depending on what the software tells it to do.\nManual and automatic attenuation have different advantages and disadvantages. In any booster system, there's an indoor antenna known as the broadcast antenna and an outdoor antenna known as the donor antenna. In an ideal scenario, each antenna is both receiving and projecting signal at their maximum power level. If these two types of antennas are installed too closely, they will create a feedback loop that will ultimately force the booster to shut down. When the booster automatically adjusts its attenuation, both the outside donor antenna and inside broadcast antenna attenuate in equal amounts. This is great for ease of install but may affect coverage as all of the antennas inside will be equally affected.\nOn a booster where there are dip switches that control the uplink and downlink separately, you have the ability to create what is called asymmetrical gain. This means you can reduce the amount of incoming signal without sacrificing indoor coverage to remedy things like overpowering and inadequate separation.\nIt can be intimidating to see switches or dials on a booster. It is also time consuming to configure the booster if it requires manual attenuation, however SureCall has a way to remotely monitor and adjust the booster’s attenuation levels called the century remote monitoring system. Automatic attenuation is accomplished through software. This makes installing boosters with automatic attenuation quick and easy. The disadvantage to relying on software to automatically adjust both gain and attenuation is that you cannot control how much coverage area you get with the booster. Also, if your booster automatically attenuates, proper separation is a must.\nManual attenuation is the way to go if you are working in an area where there are many cell towers that could overpower the booster. It is also great for areas where you need to adjust the coverage areas. Many experienced installers like having the ability to control the gain of the booster. Manual attenuation is also important if you can't get enough separation between the outside and the closest inside antenna. Keep in mind that increasing the distance between the internal antennas and the outside antenna is often a better solution than manually adjusting the booster.\nAutomatic attenuation works well in more rural areas where the cell towers are more spread out. Open coverage areas like warehouses and factories are a nice fit for boosters with automatic attenuation as well. Many installers working in the residential market like automatic boosters because of the quick and easy nature of their installation. They are also great if you know you can get enough space between the installed antennas.\nWhile there are advantages and disadvantages to both manual and automatic attenuation, it's nice to know that there are choices when it comes to cellular signal boosters for larger applications. The ability to configure the booster is a distinct advantage for those interested in installing boosters themselves. Automatic attenuation might be a better fit. As always, if you have questions, don't hesitate to get in touch with someone here at SignalBooster.com.\nVideo # 2: Manual Attenuation, Manual Gain Controls, and How to Adjust your Signal Booster?\nConfiguring your signal booster based on the environment it is installed in.\nHello SureCallers and certified installers, today we're going to be talking about manual attenuation. Now something you may find on your SureCall cellular signal booster is something called MGC or manual gain control. Now, manual gain control can be either dip switches or dials but it is a way for you to be able to configure your booster based on the environment that it is being installed in.\nLet us talk a little bit about what those manual gain controls do, and how to adjust your signal amplifier. As with any signal booster, uplink and downlink are important but so are separation and balance. You can have really great signal getting into the booster but unless you know how to configure the booster properly, you could be sacrificing coverage and overall performance.\nThere are boosters out there that have automatic attenuation which makes installation relatively simple. But their drawback is that they do not provide configurability. SureCall boosters have automatic gain control or AGC as well as manual gain control or MGC, so you are able to adjust the gain of the booster to fit your needs.\nFirst, let us talk about what to do when you see MGC on your booster. The dials will control the gain for the uplink and the downlink at the same time, much like what automatic attenuation does. That means that as you reduce the gain on the booster, the uplink and downlink are reduced at the same time. This is great if you cannot get the required separation between the outside and closest inside antenna, or if you only need a specific amount of coverage area.\nThe MGC switches may seem intimidating but they are actually pretty easy to use. Think of them like knobs on your kitchen faucet. If the water is too hot, you simply turn down the heat. If it is too cold, you turn up the heat. The same principle applies to the manual gain controls on a signal booster. Each set of manual gain controls will create as much as 31 decibels of resistance to the signal. Each switch has its own value and as you switch multiple switches on, these values combine.\nFor example, if you turn the third switch on which has a value of 4 dB as well as the fifth switch with a value of 16, maybe you will have a total of 20 dB of attenuation for that particular band of frequencies. The thing we want to remember is that the uplink or UL is what is going from the cell phone into the booster, and then out of the booster to the cell tower. The downlink is what is coming from the tower into the booster and then from the booster to the indoor antennas.\nTherefore, if you are seeing red lights flashing on any of the bands, simply add attenuation on that band’s downlink. This will not affect the indoor coverage. If you cannot get enough separation between the outside and the closest inside antenna, you should adjust the manual gain controls for the uplink on the bands that are flashing yellow and red. This will allow you to achieve separation without sacrificing overall coverage once you make adjustments to the booster.\nIt can take thirty seconds to a minute to see those changes come into effect so it is often a good practice to do a power cycle to reset the unit. The best way to tell if you need to adjust the booster is by looking at the LED behaviors. Here's a quick guide to get you going in the right direction. If you see red lights flashing on three or more bands, you should consider using an inline attenuator. However, make sure you check the bands that are not flashing to make sure you have enough signal to accommodate the signal reduction. If you see only one or two bands flashing red, you will attenuate the downlink on the manual gain controls of those bands.\nStart with 16 dB. Do a power cycle and if the red flashing light is still being activated, add some more attenuation. Remember, putting all of the manual gain controls into the on position will shut the band off completely. If you are seeing a red and yellow flashing light, this indicates oscillation. For this, in order to not sacrifice coverage, you should adjust the uplink MGCs. This will reduce the propagation of the donor antenna while allowing the indoor antennas to provide maximum coverage.\nHopefully, this video was helpful. Thank you so much for taking the time to watch, and as always, thank you for using SureCall to help raise your bars.\nShare this post""]"	['<urn:uuid:505b47d3-412d-4a39-9548-e21f47d03076>', '<urn:uuid:42cb8d3b-b386-4858-8aba-9c796249ad47>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T22:03:18.033296	21	100	2347
81	urban farming workshops content topics	The Food Skills workshops cover four main areas: Planting (including Container Gardening), Picking (including Seed Saving and Edible Wild Plants), Preparing (including Preparing Local Vegetables and Using Culinary Herbs), and Preserving (including Root Cellars). These workshops are introductory learning experiences designed to teach participants about planting, picking, preparing, and preserving food.	['Adults and Seniors\nThe Regional Nutritionist supports healthy eating through various programs. These programs provide an opportunity for participants to enhance food skills. Participants will also increase their knowledge of buying, preparing and storing healthy foods.\nA Community Kitchen is a community-based cooking program where small groups of people come together to prepare meals and take food home to their families. In a community kitchen every member contributes by planning, preparing, and cooking food. Community kitchens are great opportunities for learning about the importance of healthy eating and developing the skills to prepare healthy and affordable meals.\nWestern Health can support communities interested in offering a Community Kitchen. Western Health can provide resources and training to Group Facilitators. The Wellness Facilitator can assist groups with applying for funding opportunities and offer other support as needed.\nA Community Garden is a shared space where people gather together to grow fruits, vegetables, small livestock, and/or flowers collectively. Community Gardens vary widely in their structure, purpose and format. They can consist of collective plots, individual plots, or a combination of the two. Food may be grown for the garden’s members, for a local organization such as a community kitchen or for the community at large. The most common type of Community Garden is one in which garden plots are rented to community members on an annual basis to plant vegetables for their own consumption.\nWestern Health can support communities interested in offering a Community Garden. Western Health can provide resources and training to Group Facilitators. The Wellness Facilitator can assist groups with applying for funding opportunities and offer other support as needed.\nColour it Up Program (ages 19-50) (50 years and over)\nThe Colour it Up Program is a six week program designed to help women and their families to eat more vegetables and fruit. Participants use goal setting to address barriers to eating vegetables and fruit. It is recommended that the program should be held with groups between 9 and 15 women as it allows for meaningful discussion, exchange and participation. The Colour It Up Program can be offered by various organizations through volunteer trained facilitators.\nFood Skills Workshops\nThe purpose of the Food Skills workshops is to assist community groups across the western region in fostering knowledge, capacity, and engagement with healthy, traditional food skills in their communities. The workshops are meant to be introductory learning experiences, both for participants attending the workshops and facilitators preparing them.\nThe workshop kit outlines what community groups will need to know in order to successfully host their own workshops. Once participants take part in a workshop they are given knowledge and skills on a particular topic of planting, picking, preparing, and preserving.\nPlanting: How to Grow Food\nPlanting includes all aspects of growing healthy food up until the time when plants are ready to be harvested. Planting can refer to household or community gardening, sprouting, and agricultural cultivation of products like vegetables, fruits, and herbs. The two workshops under Planting are:\n1. Container Gardening\nPicking: How to Harvest Cultivated & Wild Foods\nPicking includes all aspects of harvesting food, both foods that have grown in the wild and those that are cultivated in a garden or on a farm. Picking includes harvesting foods derived from animals as well as plants. Raising backyard chickens and other livestock, beekeeping, hunting, fishing, as well as harvesting vegetables, or gathering wild plants all fall under Picking. The two workshops under Picking are:\n3. Seed Saving\n4. Edible Wild Plants\nPreparing: How to Prepare Healthy Meals\nPreparing describes the process of making healthy meals that will be eaten soon after they are prepared. Cooking skills, meal traditions, and information about ingredients are all included under Preparing. The two Preparing workshops are:\n5. Preparing Local Vegetables\n6. Using Culinary Herbs\nPreserving: How to Store and Preserve Food\nPreserving incorporates the various methods for storing and preserving food to keep it for future consumption. Root cellars and cold storage, canning (bottling), pickling, drying, salting, freezing, fermenting, and smoking are all techniques under Preserving. The two workshops for Preserving are:\n8. Root Cellars\nWestern Health can support communities interested in any of the programs or workshops listed above. The Wellness Facilitator can assist groups with applying for funding opportunities and offer other support as needed. If you are interested or have any questions, please contact the Regional Nutritionist (709-784-5257) or contact a Wellness Facilitator in your area:\n- Bonne Bay and Area; Port Saunders and Area:\n- email: Wellness Facilitator\n- Tel: (709) 458-2381 ext. 268\n- Bay of Islands and Area; Deer Lake, White Bay South and Area:\n- email: Wellness Facilitator\n- Tel: (709) 632-2920\n- Port aux Basques and Area; Burgeo, Ramea and Area; Bay St. George and Area:\n- Email: Wellness Facilitator\n- Tel: (709) 646-3728']	['<urn:uuid:04b59669-39f9-4814-b2c8-c49ed4712a8f>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T22:03:18.033296	5	51	798
82	What historical secrets were found in ancient tombs?	Ancient tombs have revealed fascinating historical secrets through various discoveries. In Maeshowe, archaeologists found Viking runic 'graffiti' on the inner walls, confirming historical accounts that Norsemen had entered the tomb in the 12th century. At Bryn Celli Ddu, excavations uncovered significant finds including rock art showing connections to Ireland, and remarkably, five Mesolithic postholes dating to the sixth millennium BC, along with an undated animal burial near the entrance passage. These discoveries show how these monuments were used and revisited over thousands of years, containing evidence of multiple phases of human activity and cultural connections.	"['is] one of the greatest architectural achievements of the prehistoric\npeoples of Scotland"".\nThe parish of Stenness, in Orkney’s West Mainland, is home to some of the county’s best-known monuments.\nAmong these is the prehistoric chambered cairn, Maeshowe.\nThought to date from around 2700BC, Maeshowe is one of the monuments that make up the Heart of Neolithic Orkney World Heritage Site.\nApproximately 500 metres from the south-eastern shore of the Harray loch, Maeshowe is, by far, the largest and most impressive of Orkney’s many chambered cairns.\nAppearing as a large grassy mound, it is clearly visible for miles around, including the nearby Standing Stones o\' Stenness, the Barnhouse Settlement and the Watchstone.\nArchaeologist James Farrer first excavated the cairn in 1861, prior to which the mound had a distinctly different shape than it has today. As can be seen in the illustration (right), Maeshowe was once conical, with a deep depression in the top. It had a diameter of around 30m (100 ft) and stood 11m (36 ft) high.\nThe cairn was taken into state care in 1910, at which time a concrete roof was added to the structure. At the same time, the outer mound was sculpted to give it is present ""rounded"" dimensions of 7.3m high and a 37m diameter.\nA Neolithic elite?\nMaeshowe was built in the Neolithic period. Constructed on a platform of levelled ground, like the nearby stone circles of Brodgar and Stenness, the monument is surrounded by a ditch and raised bank.\nArchaeological work in recent years hints that the cairn was built on top of an earlier structure - perhaps an early Neolithic house. It has been suggested that this house was replaced by a stone circle - four of the stones of which came to be incorporated into Maeshowe.\nAn excavation outside the chamber, in 1996, led to the discovery of a socket-hole on a platform to the rear of the mound. This added weight to the theory that the site had one housed a stone circle. The massive stone slabs used to line the entrance chamber may also have once been part of this stone ring.\nAt the same time, it was suggested that the chamber\'s encircling ditch was originally intended to be filled with water. This would have had the effect of further isolating the world of the living from that of the dead.\nMaeshowe is made up of a large central chamber, with three side chambers built into the walls. It is accessed by a low, long entrance passage (see illustration below).\nThe complexity of the chamber\'s architecture, and the grandness of its scale, has led to the idea that Maeshowe was built to demonstrate the power of a ""social elite"" within the prehistoric tribal systems of the time.\nEstimates for the labour required to build Maeshowe have been placed at 100,000 man-hours, compared to 10,000 hours required for its lesser contemporaries. This, suggest some, shows a society where the emphasis had shifted from the community as a whole, to one elevated class, or individual.\nThe midwinter connection\nPerhaps one of Maeshowe\'s most famous attributes is its midwinter alignment - something it shares with the chambered tomb of Newgrange, in Ireland.\nFor a few days each year, as the midwinter sun slips below the horizon, its last rays shine directly through Maeshowe\'s entrance passage to illuminate the rear wall of the central chamber.\nFor more on this work of Neolithic engineering, click here.\nStone ring links?\nAt the nearby Standing Stones o\' Stenness, stand two angular slabs, standing side by side, with a large prone stone beside them.\nis intriguing, although perhaps mere coincidence, that when viewed from the centre of the stone circle, Maeshowe is aligned to the gap between the two\nThis could indicate that the\nstones formed some sort of symbolic link, or connecting ""portal"", between\nthe chambered cairn and the stone circle.\nDuring the 1861 excavation, Maeshowe’s entrance passage was inaccessible, so an access shaft was driven down through the top of the mound. Once inside, however, the archaeologists discovered that they were not the first to break into the tomb.\nRunic ""graffiti"" found on the inner walls confirmed the Orkneyinga Saga account that several groups of Norsemen had entered the tomb - known to them as ""Orkahaugr"" - in the middle of the 12th century and recorded their presence on the ancient stone.\nFor more on these Viking invaders, and the Maeshowe runes, click here.', 'Bryn Celli Ddu passage tomb revisited\nBryn Celli Ddu, Anglesey, UK (OS 250759 370184) is a passage tomb, a type of monument found in Ireland (e.g. in County Meath and County Sligo) and Britain, on Orkney and Anglesey, as well as on mainland Europe. Despite the single, shared name for these sites—and similarities in decorative motifs and spatial organisation—passage tombs exhibit significant variability, in form, materials and construction. Bryn Celli Ddu is significant in late Neolithic north-west Atlantic Europe because of its finds—including rock art indicative of Irish connections (Lynch et al. 2000)—and its complex, multi-phase development, including five sixth-millennium cal BC Mesolithic postholes (Burrow 2010).\nExcavated in 1929 (Hemp 1930), Bryn Celli Ddu comprises the heavily reconstructed remains of a passage tomb with at least two phases of construction. The site’s development has been variously interpreted (O’Kelly 1969; Eogan 1983; Bradley 1998; Figure 1), with differing assessments of the encircling ditch. O’Kelly (1969; Lynch 1991) suggested that the first Neolithic phase comprised a henge followed by the construction of the passage tomb—a sequence that inverts established chrono-typologies. Burrow (2010; cf. Bradley 1998), however, drawing on discussion by Eogan (1983), has questioned the identification of an early henge. O’Kelly’s henge thesis rests on whether or not the ditch was accompanied by an external bank, for which there is no evidence, either as a positive feature, or from fill layers in the ditch (Burrow 2010). A deposit within the ditch has been suggested to be the stabilisation layer within a feature open for some time, with the ditch spoil forming an exterior bank (O’Kelly 1969; Lynch 1991; Burl 2000).\nOur new survey aimed to identify evidence for an external bank and to locate other unrecorded archaeological features within the scheduled area. Evidence for a bank would potentially inform understanding of the chrono-typological development of later Neolithic monuments across north-west Atlantic Europe, in Ireland, Wales and Scotland.\nMethod and results\nAn electrical resistance tomography (ERT) survey of the mound and its immediate surroundings was undertaken. ERT is based upon the principles of resistivity, producing transects of varying depths. We surveyed half the monument, with transects at parallel 2m intervals (Figure 2). The array and specification are detailed in Table 1.\nExcluding the mound transects, all meaningful variations in resistivity occurred in the upper three metres, with green/blue responses representing variation in bedrock (Figure 2). Anomalies A–K have been identified. Only anomalies associated with the ditch, bank and mound are discussed here.\nAnomaly A, located in transects 1 and 2, represents a potential outer bank. Its position and width are congruent with this interpretation. The slightly higher resistance of this anomaly compared to the topsoil outside and within the ditch would also be consistent with a denuded bank, which now exists only below ground level, comprising compacted humic topsoil. The distance from the centre of the ditch to the crest of the ‘bank’ is approximately 2m. The ditch itself is well represented in many transects as anomaly B. Near the entrance, the ditch shows as an area of higher resistance, probably as a result of twentieth-century reconstruction. The reconstructed mound (anomaly E) shows clear differentiation from other features. The survey detected probable stone elements around the entrance (anomaly I) and the artificial southern opening (anomaly J).\nDiscussion and conclusion\nAs the first application of ERT on this type of monument in the UK, this survey has demonstrated the technique’s effectiveness. The results include an anomaly, a possible outer bank that might have been a component of a henge or other earthwork enclosure. If this is the case, both the standard interpretation that henges post-date British passage tombs and the conventional late typo-chronology may require revision. This could indicate important and dynamic relationships between north Wales and Ireland during this period. Along the north-west Atlantic seaboard, stone circles, passage graves, henges and other earthwork enclosures may have had more variable traditions within more closely related sets of practices than previously anticipated (cf. Bradley 1998).\nThat said, the survey was limited by both access and time; further work to characterise this tantalising anomaly must be undertaken. The potential for a more complicated monument sequence than the single or double phases advocated might be suggested by the monument’s relatively late construction date, which is towards the end of the timespan of dated Irish examples (Griffiths 2015). The presence of an undated animal burial near the entrance passage, and the Mesolithic postholes, demonstrates that other aspects of the monument are also atypical. The ERT evidence from Bryn Celli Ddu is another indication of the variability that may exist within the passage tomb class of monuments.\nWe thank Ian Halfpenney, Regional Inspector of Ancient Monuments and Archaeology, Cadw, for permission to conduct the survey, and Richard Bradley for comments on a draft.\n- BRADLEY, R. 1998. Stone circles and passage graves—a contested relationship, in A. Gibson & D. Simpson (ed.) Prehistoric ritual and religion: 2–13. Stroud: Sutton.\n- BURL, A. 2000. The stone circles of Britain, Ireland and Brittany. New Haven (CT): Yale University Press.\n- BURROW, S. 2010. Bryn Celli Ddu passage tomb, Anglesey. Proceedings of the Prehistoric Society 76: 249–70.\n- EOGAN, G. 1983. Bryn Celli Ddu. Antiquity 57: 135–36.\n- GRIFFITHS, S. 2015. Chapter 10. Beside the ocean of time: a chronology of Neolithic burial monuments and houses in Orkney, in C. Richards & R. Jones (ed.) The development of Neolithic house societies in Orkney. Oxford: Windgather.\n- HEMP, W. 1930. The chambered cairn of Bryn Celli Ddu. Archaeologia 80: 179–214.\n- LYNCH, F. 1991. Prehistoric Anglesey: the archaeology of the island to the Roman conquest. Llangefni: Anglesey Antiquarian Society.\n- LYNCH, F., S. ALDHOUSE-GREEN & J. DAVIES (ed.). 2000. Prehistoric Wales. Stroud: Sutton.\n- O’KELLY, C. 1969. Bryn Celli Ddu, Anglesey. Archaeologia Cambrensis 118: 17–48.\n* Author for correspondence.\n- S. Griffiths*\nUCLan, Archaeology Department, School of Forensic and Applied Sciences, UK (Email: firstname.lastname@example.org)\n- B. Edwards\nDepartment of History, Politics and Philosophy, Manchester Metropolitan University, Geoffrey Manton Building, Rosamond Street West, off Oxford Road, Manchester M15 6LL, UK (Email: email@example.com)\n- A. Wilson\nSchool of Computer Science, Bangor University, Dean Street, Bangor, Gwynedd LL57 1UT, UK (Email: firstname.lastname@example.org)\n- F. Reynolds\nCadw, Welsh Historic Environment Service, Plas Carew, Unit 5/7 Cefn Coed, Parc Nantgarw, Cardiff CF15 7QQ, UK (Email: email@example.com)\n- A. Stanford\nAerial-Cam Ltd, Harrowfields, Eckington WR10 3BA, UK (Email: firstname.lastname@example.org)']"	['<urn:uuid:5a003df7-1883-4399-b0c8-5e58f880082e>', '<urn:uuid:a81ee573-f693-4163-ba6d-9900099245ea>']	open-ended	with-premise	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T22:03:18.033296	8	95	1800
83	fda safety standards pet food criminal charges	The FDA's food safety standards now apply to pet food processing, and the agency has legal authority to press federal criminal charges against food manufacturers or processors if unsanitary plant conditions cause human illness, such as when people contract salmonellosis from handling contaminated pet food.	['From Feed to Food, Part 1\nWhat does the Federal Drug Administration (FDA) case against the now-bankrupt American Peanut Corporation have to do with dog kibble? Plenty, if a dog or cat owner contracts salmonellosis or another food-borne illness from handling tainted kibble. FDA food safety standards now apply to pet food processing, and this agency has the legal authority to press federal criminal charges against a food manufacturer or processor if unsanitary plant conditions cause human illness.\nDon’t think of the product as animal feed, or even pet food, anymore. It’s food, period.\nPet food manufacturers have watched consumers abandon well-known brands in droves after a pet food recall, and know that their flight impacts both current and future profits. Yet, in the evolution from feed to food production, many companies have not fully addressed all of the issues associated with food safety. Pet food manufacturers might consider lessons learned from human food manufacturers when it comes to designing and operating processing plants to ensure food safety.\nHere is a look at some of the strategies that pet food manufacturers can apply to improve food safety through more effective site selection, facility design, storm run-off and waste water management, work flows, waste management, and safety training.\nAvoid Sites Located in the Shadow of a Volcano\nIt is essential to select a piece of land that limits the potential contaminants. Sounds obvious, but there have been plenty of cases in which food manufacturers have been lured by cost-saving site deals, including one that was literally located in the ash plume of a volcano.\nThe right location is one that minimizes contamination risks from water and air. A good start would be a relatively flat site. Ideally, it would be elevated above the local estuaries. Elevated and flat sites minimize the cost of storm run-off management, which can be significant in many areas.\nIf a low-level site is the only option, avoid the use of open water ponds. One strategy that has proven effective in the food industry is to have the site drained efficiently, conveying water away from the building, then construct a significantly raised building pad. This will cost more initially, but will save a great deal in storm run-off management.\nSometimes the purchase of the gross acreage depends on a government entity draining the building site for the pad. In other cases, industrial park managers provide regional storm run-off management.\nChoosing the site presents cost trade-offs that must be weighed carefully, such as when the location is near a power utility station, waste treatment facility, or other manufacturing neighbors. Drawbacks presented by the surrounding structures can also invoke some creative design advantages to overcome them.\nFor example, a developer conceptualized constructing a ship-out facility for an existing food manufacturing plant utilizing the established rail yard for transporting large quantities of packaged finished goods. With the rail yard accessible to both the manufacturing plant and the new ship-out facility, goods could be trucked a short distance to the ship-out facility, sorted and marshaled into rail cars.\nDue to the physical constraints of the site, the ship-out facility’s receiving docks would face the prevailing winds. Not an ideal scenario. To complicate matters, a neighboring recycler already occupied part of the rail yard up wind. A completely exterior operation, the recycler loaded open top rail cars with “fluff” ( i.e., all the crushed non-metal components from auto recycling). Dust and debris from that operation would blow directly into the open doors of the new ship-out facility.\nTo control contamination, the ship-out facility’s receiving docks would need to be rotated to face away from the most direct means of access, as well as require an additional investment in sheltered, sealable, access-controlled doors. The cost required to control contamination from neighbors would have to be factored in to the Return On Investment (ROI) calculations to determine project feasibility and payback.\nDon’t Become the Local Watering Hole\nWater attracts and sustains wildlife and microorganisms. Pet food manufacturers should keep as much as possible off the site, and get rid of the rest pronto. The optimal storm run-off management solution depends on local building codes and hydrogeology.\nIn most cases, the solution involves a retention pond (which intermittently contains water) or detention pond (which always contains water), depending on the hydrogeology of the site and surrounding region. Regardless, locate it downwind and at least 500 feet away from the facility. Dig it shallow to better blend in with the surrounding landscape and fill it with granular material that allows water to percolate more quickly into the soil beneath.\nLandscaping at the facility site can absorb storm run-off, but should be limited to trees and shrubs that do not bear flowers or fruit or produce sap. An insect and vermin barrier should also surround the building perimeter. This is typically an uninterrupted pea-gravel band at least 3 feet wide and 3 feet deep. Landscape installers must position major plantings, such as trees, at least 30 feet from the structure and maintain branching growth to no closer than 6 feet from the structure.\nFor part two of this two-part series, please visit www.chem.info/Articles/2012/07/Plant-Operations-From-Feed-to-Food-Part-2/. For more information, please visit www.ssoe.com.']	['<urn:uuid:f9ec63e8-e91a-4122-876a-37f89c76552f>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T22:03:18.033296	7	45	862
84	When did the Median Empire first appear?	According to the Greek historian Herodotus, the Median Empire was founded by King Deioces in 728 BC.	['Foundation of the Median Empire\nThe inhabitation of Iran dates as far back as 3000 BC, but it was not until the rise of the Medes in the ninth century BC, that a powerful empire came into prominence in this land. The celebrated Greek historian, Herodotus, believes that King Deioces founded the Median Empire in 728 BC. He established the capital city of the Median Empire, Ecbatana. Deioces ruled until 675 BC, and Phraortes, Deioces son, took over the reigns of the kingdom after his father.\nThe Medes and the Persians had lived in relative peace although the Medes far outnumbered the Persians and were more widespread as well. Assyrian records show that the Medes had occupied all of the eastern regions including Ecbatana and most of the western Zagros Mountains. Phraortes dreamed of ruling a vast empire of Medes. He conquered the Persians and invaded Assyria. Phraortes ambition remained unfulfilled as he died in his Assyrian mission.\nThe Reign of Cyaxares\nCyaxares ruled the Medes from 625 BC to 585 BC, and has been hailed as the greatest among the Median kings. In a dramatic intrigue, Cyaxares invited the Scythian lords to a party where they were murdered. The Scythians withdrew, leaving the Medes unrivaled.\nCyaxares was a diplomat. He forged alliances with Babylonia and betrothed his granddaughter to Nebuchadnezzar II. He thus secured the military might of Babylonia and proceeded to deal with the Assyrians.\nNebuchadnezzar II reorganized the Median Army and created specialized divisions for the cavalry and infantry. He then attacked Ashur. The fall of Ashur was a great blow to the Assyrian might. He captured parts of eastern Anatolia, but this brought the Medes into conflict with the Lydians. The Median Wars with the Lydians came to an end in 585 BC, and the Kizil River was agreed upon as the boundary separating the two kingdoms.\nDecline of the Medes\nCyaxares was succeeded by his son, Astyages, who reigned from 585 BC to 550 BC. Deteriorating relations with Babylonia and the rise of Cyrus II spelled the end of the Median Empire in Iran.\nHistorical Records and Corroboration\nMuch discrepancy is found between the records of Herodotus and the records of Assyria with regard to the rule of Deioces. The Assyrians believed that Herodotus record was simply a local legend. Some historians believe that Deioces was a captive and slave of Sargon II, who was banished to Assyria and went on to rule as Daiaukku. Others consider this theory far-fetched and highly unlikely.\nPhraortes invasion of Assyria, however, closely corresponds with Assyrian court records. The Assyrian records talk of a Median king, Kashtariti, who waged war on the Assyrians and was repelled by King Esarhaddon. Historical evidence proves with certainty that the rise of the Scythians and the dominance of the Medes did pose a threat to the Assyrians in the seventh century BC.\nZoroastrianism and the Medes\nHistorical records show that Zoroastrianism has its origins in the Median Empire. The worship of fire that forms the core of Zoroastrianism was popular in the Median Empire. Fire altars found in Tepe Nush-e Jan are testimony to the religious rites performed by the Medes.\nA priestly cult called the Magi performed the priestly duties for the Medes and the Persians alike. The occupation of the Magi was passed on from father to son. Historians and theologists, however, are divided in their opinions about the origin of Zoroastrianism in Media. Some believe that the religion practiced by the Medes was Mithra, which was similar to Zoroastrianism.']	['<urn:uuid:cb9082f7-27b5-42ae-843b-cb16779155e2>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T22:03:18.033296	7	17	589
85	I'm trying to understand modern interpretations of the rock cycle theory - how has our understanding evolved since the basic model, particularly with respect to plate tectonics?	Modern geologists have come to view the basic rock cycle model as oversimplified. While the traditional model is cyclical without any apparent direction or trend, the incorporation of plate tectonics theory has led to a different understanding. The current view suggests there may actually be a trend toward greater diversity of rock types over time, rather than just a simple circular process. This represents a more complex and sophisticated understanding of how rocks form and transform compared to the basic rock cycle model.	"[""1. Crayon Rock Cycle\nWhat's the big deal about rocks? They don't move, aren't flashy, and seem pretty useless to the untrained eye. To discover the beauty of rocks, one must look closer and ...\nWhat's the big deal about rocks? They don't move, aren't flashy, and seem pretty useless to the untrained eye. To discover the beauty of rocks, one must look closer and learn how to read them. Geologists are rock detectives, discovering clues to the ancient past. If you know how to read them, rocks can tell an observant scientist about what a place looked like millions and even billions of years ago. This activity introduces the 3 main types of rocks and the processes that form them. Wax crayons are eroded into sediment, compacted into sedimentary rock, partially melted and pressed into metamorphic rock, and finally melted and cooled into igneous rock. This understanding is the basis of the rock cycle. In the Going Further section, there is a recipe for making your own sandstone, siltstone and conglomerate using sediments and a sodium silicate solution.\nCan describe the 3 major types of rock (sedimentary, metamorphic, and igneous) and discuss the relationships between them\nCan diagram the rock cycle\nGiven one of the three major types of rock, can describe the geologic processes that formed it\n1. Crayon Rock Cycle - Logistics\nEach student needs:\n- Copy of the Rock Cycle Diagram\n- 1 wax crayon (Crayola brand crayons works well)\n- 1 plastic knife\n- 6 inch square of aluminum foil\n- Styrofoam or other cup to hold hot water\n- Source of hot water (I use a hot pot style electric water heater - see image)\n32 oz Hot Pot Express by RivalA\nFor the whole class:\n- Samples of sedimentary, metamorphic, and igneous rock, sorted into boxes of the same rock type\n- 3-6 large containers or water pitchers for water that has cooled down in the students’ cups\n1. Crayon Rock Cycle - Background\nThe rock cycle is perhaps the most basic, fundamental principle of geology. All rocks are related to each other and may be transformed from one kind to another. In its simplest form, the rock cycle describes the relationships between the 3 major types of rock:\n- Igneous Rocks - Formed from the cooling of molten rock (magma).\n- Sedimentary Rocks - Formed from layers of sediment as the pressure of overlying layers compacts the sediment into rock. Sometimes, a cementing agent, dissolved minerals such as silica or carbonates, helps bind the sediment particles together.\n- Metamorphic Rocks - Formed from other rocks that are exposed to intense heat and pressure and thus change their physical and/or chemical form.\nMolten rock or magma solidifies either rapidly at the Earth’s surface or slowly under the Earth’s surface into igneous rock (this is the whole crayon we start with). As these rocks are exposed to erosion and weathering, they are broken down into sediment (a pile of crayon shavings). The grains of sediment may be transported long distances by water, wind or gravity, and eventually deposited in layers. As more and more sediment layers build up on top of each other, the sediments are compacted and sometimes cemented together into sedimentary rock (squishing the crayon shavings together) in a process called lithifaction. With heat and pressure (partial melting in hot water), the rock will undergo a physical and/or chemical change into metamorphic rock. If the rock is melted completely and cooled, you once again have igneous rock.\nThe rock cycle is attributed to James Hutton (1726-1797), the “father of geology” who meticulously explored and documented the landscape of the British Isles. Hutton proposed the principle of uniformitarianism, the idea that the processes that shape the world today also operated in the past. His idea brought about the revolutionary notion that given how long it takes for geologic processes to occur today, the Earth must be very very old for all the existing landforms to have been created, not merely the 6000 years allowed by tracing Biblical genealogy. One of his most famous quotes states that with respect to the Earth there is “no vestige of a beginning, and no prospect of an end”.\nPortrait of James Hutton, 1726-1797, by Sir Henry Raeburn. From the collecctions of the National Galleries of Scotland.\nWith greater scientific sophistication and the plate tectonics revolution, many geologists now believe that the basic rock cycle described in this lesson is too simple. The basic rock cycle is cyclical, with no apparent direction or trend. Instead, if plate tectonics is taken into account, there may indeed be a trend towards greater and greater diversity of rock types over time. For more information, see the Tectonic Rock Cycle\nIt helps the discussion of sedimentary rocks if students are familiar with soil separation and identifying different sediments (gravel, sand, silt, clay) by size. See Soil Analysis Lesson, Erosion Patterns Lesson and/or the Sediment Study Project.\n1. Crayon Rock Cycle - Getting Ready\n- Rip sheets of aluminum foil into 6 inch squares\n- Set out remaining materials – crayons, knives, cups, and hot water source\n- Copy Rock Cycle Diagram handout\n- Sort the rock samples into the 3 main categories of rocks if they aren’t already sorted\n1. Crayon Rock Cycle - Lesson Plan\n- Pass around samples of sedimentary rock. Ask students to observe the rocks and describe some of the similarities between them. As students offer their ideas, write them on the board in one column. You should end up with a list like: First rock type: can see grains, grains can be of different sizes, has layers or streaks, grains come off if you rub it.\n- Collect the sedimentary rocks then pass around samples of metamorphic rocks. Ask students to observe the rocks and describe how these rocks are different from the sedimentary rocks. Again, write their ideas in a column on the board. You should end up with a list like: Second rock type: no grains, has crystals, many colors, very hard, swirly patterns.\n- Collect the metamorphic rocks then pass around samples of igneous rocks. Ask students to observe the rocks and describe how these rocks are different than the other two types of rocks. Again, write their ideas in a column on the board. You should end up with a list like: Third rock type: no grains, some have crystals, some have lots of holes, uniform texture and pattern throughout the rock, no layers or swirls.\n- Tell the students that they have been observing and categorizing the 3 major types of rocks: sedimentary, metamorphic, and igneous rocks. Explain that today, they will be using crayons to model the processes that create each of these 3 types of rock.\n- Pass out the handout, crayons, foil, and knife.\n- Tell students that they have been given a sample of a crayon rock. Looking at the 3 descriptions on the board, which one is this sample most similar to. It doesn’t have grains, layers or streaks. Thus it is an igneous crayon rock! On their handout, in the box at the top of the circle, have students write “igneous rock”.\n- The first step is to create sediment. Have students unwrap their crayons then create a pile of crayon shavings on their piece of aluminum foil by scraping it with the knife. They may trade crayons among themselves to acquire a mixture of colors. Give them around 5 minutes to build up a decent sized pile.\n- On the diagram, the arrow from “igneous rock” can be labeled “erosion”. The next box can be labeled “sediment”.\n- Now fold over the foil to wrap up the sediment pile. Press down on the pile as hard as you can. Gently unwrap it. The sedimentary crayon rock will be fragile but should hold together in a packed layer.\n- Discuss the similarities between the sedimentary crayon rock and the real sedimentary rocks the students observed earlier.\n- On the diagram, the arrow from “sediment” can be labeled “lithifaction – compacting and cementing sediments together”. The next box can be labeled “sedimentary rock”. Discuss this process as it occurs in the real world with layers being squeezed under other layers.\n- Now get a helper to pass out the cups and go around yourself to fill each cup with hot water. Have another helper place containers for cooled water near each table or cluster of desks.\n- Each student should create a little boat for their sedimentary crayon rock and float his or her boat on the hot water. Watch as the heat from the water melts the crayon. Remove the foil when the wax is soft to the touch and the colors have swirled together but not so much that the colors are indistinguishable. Let the metamorphic crayon rock cool.\n- Discuss the similarities between the metamorphic crayon rock and the real metamorphic rocks the students observed earlier.\n- On the diagram, the arrow from “sedimentary rock” can be labeled “metamorphism – heat and pressure transforms the rock”. The next box can be labeled “metamorphic rock”. Discuss this process as it occurs in the real world with rocks being subjected to intense heat and pressure beneath the surface of the Earth.\n- At this point, the temperature of the water the cups may have cooled. Ask students to dump their water into the containers. Go around and refill each cup with hot water.\n- Each student should put their metamorphic crayon rock back in the foil boat and float it on the hot water. This time, allow the wax to melt until a smooth pool of liquid wax forms and the colors blend together uniformly. Carefully remove the foil and let the igneous crayon rock cool.\n- Discuss the similarities between the igneous crayon rock and the real igneous rocks the students observed earlier.\n- On the diagram, the final arrow from “metamorphic rock” can be labeled “melting into magma then cooling”. Discuss this process as it occurs in the real world with rocks being melted deep within the Earth then extruded again as volcanoes or bubbles of magma that do not reach the surface.\n- Ask the students if they think this igneous rock could be turned into sedimentary rock? How? Could it be turned directly into metamorphic rock? How? Could a metamorphic rock be turned directly into sedimentary rock? How?\n- Add additional arrows across the middle of the rock cycle to illustrate that any type of rock can turn into any other type of rock.\n- If there is time, students can experiment with turning their igneous crayon rock into a new sedimentary, metamorphic, or igneous crayon rock.\n- Clean up! Students can keep their crayon rocks.\n1. Crayon Rock Cycle - Assessments\n- 9th grade teacher Marcie Krech, has a list of great extension activities related to the rock cycle. They include a vocabulary cut & paste, a lab, a whole class puzzle, a game and a comic strip activity. In fact, Marcie has put her whole Earth Science curriculum online for others to learn from. Thank you!\n- Give students rocks to classify as sedimentary, igneous, or metamorphic.\n- Make sedimentary rocks! Any sediment (powdered clay, silt, playground sand, or a sand and gravel mixture) can be turned into a sedimentary rock with the addition of a dilute sodium silicate solution. See the Sources section for where to purchase sodium silicate. The recipe:\n- 15 ml dilute sodium silicate, dilute full strength sodium silicate with water in a 1 to 1 ratio (a 20 ml syringe is a great measuring tool and dispenser for this viscous solution)\n- 6 tablespoons of sediment (playground sand works great although kids like to mix and match sediments for their own special rock type)\nMix the sediment and sodium silicate in a clear plastic 9 oz cup with a disposable stirrer like a popsicle stick. Be careful not to get sodium silicate on your hands or in your eyes. Smooth out the surface of the mixture with the stirrer. Set aside for 2 days. Once the mixture is completely dry, it can be popped out of the cup and examined up close. If you plan on doing the Layers Upon Layers lesson, consider adding layers of a different sedimentary rock on top of the first before removing the rock from the cups. You are, in effect, creating a permanent version of the depositional cups formed in the Layers Upon Layers lesson.\n- Try the History of Rock lesson where students research a rock and discover the story of its formation.\n- The National Parks Service has a great collection of teacher lesson plans related to rocks and the rock cycle called Geodetectives. There are individual activities for each of the 3 main rock types, a candy rock cycle activity, and a brilliant idea for comparing rocks to identify which rocks are best used for what purposes – building a house, tools, jewelry, etc.\n1. Crayon Rock Cycle - Sources and Standards\nThe best write up for the crayon rock cycle activity is available from Eric Muller of the Exploratorium’s Teacher Institute. Go to The Crayon Rock Cycle. Eric has developed many other fantastic activities, particularly for Earth Science.\nFor additional information about the rock cycle, go to:\nSodium silicate solution (also called water glass) can be purchased from most science supply companies such as Flinn Scientific and Science Kit & Boreal Labs. A 500 ml bottle costs $5-6. Sometimes it can be found at marine supply stores in quart sized containers for sealing the outside of boats.\nPlate Tectonics and Earth's Structure\nPlate tectonics accounts for important features of Earth's surface and major geologic events. As a basis for understanding this concept:\na Students know evidence of plate tectonics is derived from the fit of the continents; the location of earthquakes, volcanoes, and midocean ridges; and the distribution of fossils, rock types, and ancient climatic zones.\nShaping Earth's Surface\nTopography is reshaped by the weathering of rock and soil and by the transportation and deposition of sediment. As a basis for understanding this concept:\na Students know water running downhill is the dominant process in shaping the landscape, including California's landscape.\nb Students know rivers and streams are dynamic systems that erode, transport sediment, change course, and flood their banks in natural and recurring patterns.""]"	['<urn:uuid:5dfa424c-c673-46a2-87c3-e0e55d7c49f6>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T22:03:18.033296	27	83	2400
86	professional audio setup guide signal routing clean boost noise reduction methods	To achieve clean signal routing with noise reduction, several methods can be combined: 1) Use a high fidelity gain stage for clean signal boost with low noise, 2) Employ a noise gate with properly adjusted threshold to eliminate background noise, only allowing desired signals through, 3) Apply filters - the Low Pass Filter can remove unwanted high frequencies like string noise, while the High Pass Filter can filter ultra lows for more clean headroom. The parallel outputs should be buffered to prevent loading down the instrument, and when using both Line out and DI simultaneously, a Ground Lift switch can be used to isolate grounds and reduce buzz.	"['This revision of the Messenger has a few changes from the previous releases, including:\n- Input impedance optimized for upright and electric bass\n- Phase switch added at the input\n- Parallel output (labeled Tuner) is now buffered so that any parallel devices do not load down your instrument\n- The EQ Bypass switch has been replaced with a Mute switch\n- The stomp switches are now soft touch switches that activate relays\nA high fidelity gain stage cleanly boosts your signal with very low noise. A switchable clean boost gives you extra volume for when you need to stand out. The four band EQ with variable mid frequencies shapes your tone with precision and clarity. The low and high pass filters smooth the lows and highs for consistent tone for any venue. Separate level controls for the line out and DI allow you to set the perfect signal level for driving an amplifier and mixing console simultaneously. The DI features a transformer balanced out with a ground lift switch for true isolation. The internal 30 volt supply provides plenty of clean headroom for virtually any incoming signal.\nThe Messenger requires a regulated DC power supply. The voltage must be no less than 9 volts and no greater than 18 volts. The supply must be rated for at least 250 mA. The supply must use a 2.1mm barrel connector with center-negative polarity. You should choose a supply that is intended to be used for musical devices to ensure it will be low noise. There is no difference in performance when using 18V compared to 9V.\nInput and Outputs\nThe input to the Messenger is a 1/4"" unbalanced jack. The input impedance is 10MΩ and will work well for both upright bass and electric bass. The Phase button retains the phase when pressed in, or inverts the signal when the button is out . The Tuner jack is a buffered output, and may be used for sending your unaffected signal to another device. The line output is capable of driving preamps, power amps, effects loops, and other effect pedals. The transformer coupled DI output is specified to drive 600Ω loads or greater. Phantom power will not power the Messenger, however it will not damage the unit if applied. The Ground Lift switch isolates pin 1 of the XLR connector from ground.\nThe front end gain stage is capable of up to 30 dB of clean gain. Be careful when setting this…start with lower gain settings and only increase when necessary.\nThe boost is capable of up to 15 dB of additional clean gain. This is active when the Boost stomp switch is on.\nThe EQ is a boost/cut type, providing +/-15 dB of range and is flat with the knobs at noon.\n- The Bass is a shelving type EQ, boosting frequencies below 80 Hz.\n- The Treble is a shelving type EQ, boosting frequencies above 3 kHz.\n- The Low Mid and High Mid EQ boost/cut at the frequency set by the associated Freq knobs.\n- The Low Pass and High Pass filters are 12 dB per octave active filters with a smooth response in the pass band.\nThe Line Level knob provides up to 10 dB of additional gain. The DI Level knob is a passive attenuator, ranging from unity gain to completely off. Both Line Out and DI signals are affected by Boost, Mute, and the stomp switches which toggle them on/off.\nTips and Other Info\n- Depending on how strong your incoming signal is, it may be advantageous to use low to moderate Gain to allow maximum headroom in the EQ section, and then boost the signal with the Line Out knob if needed.\n- With so much gain available, a compressor may be necessary to protect your equipment from large signal spikes that may be present on the attack of your signal. Peaks can easily go above 10V when using higher gain levels, which may cause other devices to distort.\n- Typical electric bass and guitar signals need between 15 to 30 dB of overall boost to achieve line level, but this can vary dramatically depending on many aspects such as pickups, effects, string setup, and playing style, to name a few.\n- Gain in dB is additive. For instance, if you have Gain set for +15 dB, Boost set for +10 dB, and Line Level set for +10 dB, you have a total of +35 dB of gain. If you boost the Low EQ by another +15 dB, this means your low frequencies have a total of +50 dB of gain compared to your input signal.\n- Using the HPF to filter your ultra lows can allow more clean headroom, especially when using the Low EQ to boost your bass frequencies.\n- The LPF can filter out very high frequencies that may not be desired, such as string noise and fret buzz.\n- With the HPF at 25 Hz and the LPF at 20 kHz, they are effectively ""off"".\n- A simple approximation of a speaker cabinet\'s frequency response can be set up with the filters. A typical setting would be 100 Hz for the HPF and 5 kHz for the LPF, although you should experiement with different frequencies to find your desired tone.\n- A DI signal is often expected to be ""instrument level"" or ""microphone level"" in terms of signal strength. It is recommended you set the DI Level to -15 dB and adjust from there if needed.\n- When the Ground switch in pressed in, the Ground is connected to the XLR pin 1. This is the preferred setting. However, if using both the Line out and DI simultaneously, a buzz may occur in the audio signal due to a difference in ground potential between the connected equipment. In this case, lifting the Ground will isolate the grounds and reduce the buzz.\nThe messenger is 5.62"" wide x 4.62"" long\nAll orders will be shipped via Canada Post. Your shipping cost will appear at check-out.', ""Having a good microphone is essential for any successful voice chat, but ensuring that the microphone is properly adjusted is equally important. Adjusting the microphone sensitivity correctly can make a huge difference in how well your audio comes across to your audience. In this article, we'll provide you with some tips and tricks for adjusting the microphone sensitivity so you can get the most out of your voice chat experience. When adjusting the sensitivity of a microphone, it is important to keep in mind that the goal is to find a setting that will produce the best sound quality while still providing enough gain to ensure that your voice is audible. In order to do this, you will need to experiment with different settings until you find one that works for you.\nHere are some tips to help you get started:1.Make sure that the microphone is positioned correctly. The ideal position for the microphone should be directly in front of your mouth, at a slight angle.2.Adjust the gain or sensitivity setting on the microphone. This setting determines how much sound is picked up by the microphone. If the gain is too low, your voice may not be audible; if it’s too high, the sound quality may be distorted.3.Experiment with different settings until you find one that works for you.\nTry adjusting the gain in small increments and listening to the results.4.Test your settings in different environments. Different environments have different noise levels, so you may need to adjust your settings in order to get the best sound quality.5.Use a noise gate if necessary. A noise gate can be used to eliminate background noise from a recording by only allowing sound above a certain threshold to be recorded. This can be helpful if you are recording in a noisy environment.6.Monitor your recordings.\nOnce you have adjusted the settings on your microphone, it is important to monitor your recordings to ensure that they are of good quality.\nMonitor Your RecordingsOnce you have adjusted your settings, it is important to monitor your recordings to ensure that they are of good quality. This can be done by recording a short sample and listening back to it. Listen for any distortion, static, or other audio artifacts that may be present.\nIf you hear any of these, try adjusting the microphone sensitivity again until the recording sounds clear and crisp. Additionally, pay attention to the volume of your recordings; if it is too quiet or too loud, adjust the microphone sensitivity accordingly.\nPositioning Your MicrophonePositioning your microphone correctly is essential for getting good sound quality. Make sure that the microphone is placed directly in front of your mouth, at a slight angle. This will ensure that the microphone can capture your voice clearly and without distortion.\nAdditionally, be sure to maintain a comfortable distance from the microphone. If you are too close, you may experience feedback or a muffled sound. You can also experiment with the microphone's gain setting to find the right balance for your environment.\nTesting Different SettingsTesting your settings in different environments is an important step to ensure that your microphone is providing the best sound quality. Different environments can affect the sound quality of your microphone, such as an echo in a room or too much background noise.\nIt is important to test your settings in different environments, such as indoors and outdoors, to make sure they are producing the best sound quality. When testing your microphone's settings in different environments, it is important to pay attention to the volume levels and any unwanted background noise. Adjust the sensitivity of your microphone accordingly to ensure that you are getting the best sound quality possible. Additionally, if you are using a headset, make sure that the microphone is close to your mouth so that you can be heard clearly.\nUsing Noise GatesNoise gates are an effective way to reduce background noise in a recording. A noise gate works by only allowing sound above a certain threshold to be recorded.\nThis can be especially useful if your microphone is picking up too much background noise or you are recording in a noisy environment. To use a noise gate, you will need to adjust the settings to match the level of the sound you want to record. When setting up a noise gate, it is important to ensure that the threshold level is set correctly. Too high and your voice chat may be distorted. Too low and it may not be able to filter out the background noise.\nYou can also adjust the attack and release times for the noise gate. The attack time determines how quickly the noise gate will react when it detects a signal above the threshold and the release time determines how long the gate will remain open before closing again. Using a noise gate can be an effective way to improve your voice chat experience by eliminating background noise and ensuring that your microphone is picking up only the sound you want it to. If you are having trouble with background noise, adjusting the sensitivity of your microphone and using a noise gate can help you get the most out of your voice chat.\nAdjusting Gain SettingsGain settings are an important factor in determining how much sound your microphone will pick up. Adjusting the gain can help you find the optimal sound quality for your voice chat experience.\nWhen adjusting the gain, it is important to remember that the higher the gain, the more sensitive the microphone will be. This means that if you increase the gain too much, it can cause your microphone to pick up background noise or other distractions. If you want to find the best gain setting for your microphone, experiment with different settings until you find one that works for you. You may also want to try adjusting the microphone's noise cancellation and echo cancellation settings to help reduce background noise. By taking the time to adjust the sensitivity of your microphone, you can ensure that your voice chat experience is as enjoyable as possible. Adjusting the sensitivity of your microphone is essential for optimizing your voice chat experience.\nProper positioning, gain settings, and noise gates can all help you achieve the best sound quality and clarity. Experiment with different settings and environments, and take the time to monitor your recordings, until you find the right balance that produces good sound quality while still providing enough gain to ensure that your voice is audible. Having the right microphone sensitivity can make a huge difference for your voice chat. With the right tools, adjustments, and a bit of patience, you can get the most out of your voice chat experience.""]"	['<urn:uuid:f943d2fe-4b95-457d-9670-a3112427c365>', '<urn:uuid:3c331f50-5921-4918-a78b-4999d8ceb4a5>']	factoid	with-premise	long-search-query	distant-from-document	three-doc	expert	2025-05-12T22:03:18.033296	11	108	2117
87	want to join polymer chemistry group what student benefits does ACS POLY division offer	POLY offers networking events at ACS meetings, webinars on polymer applications, opportunities for internships, research opportunities, graduate school connections, and professional development events. The first year of POLY membership is free to new members.	['Meet the ACS Division of Polymer Chemistry\nWhen Katrina Knauer was an undergraduate at Florida State University, one problem loomed large in her mind: plastics. Single-use plastics. From take-out containers to shampoo bottles, they were everywhere. As a chemical engineering major, she was amazed by the chemistry that made these plastics possible. But she worried about environmental impacts. “We have this huge source of waste that we don’t know what to do with,” she recalls thinking.\nToday, Knauer is a polymer scientist beginning a new job at the National Renewable Energy Laboratory in Golden, CO. She develops technologies for recycling plastics and works with industry leaders to make those innovations scalable.\nKnauer credits landing this dream position, in part, to the relationships she’s built through the ACS Division of Polymer Chemistry (POLY), a community of professionals dedicated to advancing the polymer enterprise and addressing global challenges. As the member-at-large on the POLY executive committee, Knauer encourages ACS members to join—especially students.\nStudents who join POLY can explore how their own interests in polymer chemistry can lead to a wide range of impactful careers while building connections and gaining professional skills.\nPotential in polymers\nPolymer structures vary widely, from long molecules that are twisted and tangled together like a bowl of spaghetti strands to repeating blocks or branching chains, resulting in a broad range of properties, says Philip Costanzo, a chemistry professor at California Polytechnic State University, San Luis Obispo. “They have a lot more properties than just a single small molecule because they are so long and they can interact with each other.”\nBecause of their diverse physical properties, polymers lend themselves to many uses—and career paths. Before beginning her current position, Knauer was a senior scientist at Novoloop, a chemical recycling startup in the San Francisco Bay Area. She helped devise methods for upcycling polyethylene waste from single-use plastics like sandwich bags, turning it into polyurethanes designed for more long-lasting products like running shoes and performance raincoats.\nMeanwhile, Costanzo designs new polymers that work as “re-healable” materials. His team modified polymers in soybean oil to create a topcoat for cars that “heals” (or spontaneously self-repairs) with heat. “If you got your car scratched, instead of getting it repainted,” he explains, “you could heat it up with a hair dryer and melt the scratch away.”\nPolymer scientists also tackle other challenges, such as drug delivery and sustainable energy. They work in academia, industry, and government. But while the applications for polymer science are expansive, the POLY division helps the community of polymer scientists remain close-knit. “Being involved with POLY really has given me a wonderful support network of colleagues,” says Knauer.\nCostanzo is confident that POLY will do the same for students. “It will open up doors for internships, research opportunities, graduate school opportunities,” he says. “The sooner you plug in and interact with people, the more opportunities will pop up.”\nPlugging into POLY\nCurious about polymers but not sure where that interest could take you? POLY hosts networking events at local and national ACS meetings. They also offer webinars on cutting-edge polymer applications—from fighting the pandemic to 3D printing.\nThrough these events, you can connect with polymer enthusiasts in a wide range of professions and career stages who can offer insights into their own professional paths. “Especially if you do want to go to grad school, you’ll want to start knowing what people are doing, what kind of research they are working on, what universities are focusing on,” says Knauer.\nSome schools also have POLY student chapters jointly supported by the Division of Polymeric Materials: Science and Engineering (PMSE), another ACS division for polymer chemists. POLY/PMSE student chapters are typically formed by graduate students and postdocs but are open to undergrad involvement as well. One of Knauer’s main goals is to encourage more universities to establish student chapters. “I try to get funding to set up networking events and professional development events for the students,” says Knauer.\nOne such event, Random Walks from a PhD, is a symposium series created by the recently formed POLY/PMSE student chapter at the University of Massachusetts, Amherst. “Random Walks is a classic polymer physics term to describe random trajectories of polymer chains,” explains Chapter President Hazel Davis, a PhD candidate in polymer science and engineering whose research focuses on how polymers can be used to deliver proteins for various therapeutic applications. The organizers invite polymer scientists to speak about the twists and turns of their own career trajectories. The first speakers at the symposium, which was launched last summer, included Knauer, a patent agent, and a materials scientist at Nike.\nAbout 25 students joined each of the hybrid virtual/in-person sessions, including numerous undergraduates. Geneva McElvaine, a third-year chemistry major at Rhodes College in Memphis, TN, attended while conducting REU research at UMass Amherst on soft material interfaces. McElvaine isn’t sure what she wants to do after graduation, but the symposium helped her see polymer science as a real possibility. “If I really wanted to make myself known in the polymer science industry, or participate in it, I would know where to start,” she says.\nZachary Ahmad, a fourth-year polymer science and engineering major at the University of Southern Mississippi, also attended the symposium while conducting summer research at UMass, Amherst. Afterwards, Ahmad connected with all of the speakers and many of the attendees on LinkedIn. None of the speakers work in Ahmad’s particular research area: creating flexible electronics for medical applications. But he values these connections for the unexpected ways that their careers may someday overlap. “It made bridges that I can take in the future if I need to,” he says.\nMACRO and more\nPOLY can also help you develop professional skills for advancing your polymer science career—and the skill building might come with some prizes along the way. POLY recently sponsored a three-minute elevator pitch competition for graduate students at Case Western Reserve University’s ACS POLY/PMSE Student Chapter. Irlaine Machado, a graduate student in polymer engineering, who recently passed her dissertation defense, won $500 for a pithy description of her research on eco-friendly flame retardants. Hitting all of the key points in three minutes was challenging, recalls Machado, who is earning a joint degree from Case Western and the Federal University of Rio de Janeiro in her home country of Brazil. “I needed to practice a lot.”\nPOLY members can also draw on the many resources provided by the Macromolecular Alliance for Community Resources and Outreach (MACRO), a POLY/PMSE committee that supports curriculum building, community outreach, and professional development. Interested in sharing your love of chemistry with elementary school students? MACRO is compiling a collection of lesson plans certain to thrill the very youngest of chemists, like how to make DIY silly putty and slime.\nMACRO’s website can also help you develop professionally by identifying internship opportunities, for example, or learning tips for strengthening your grad school application. “We’re adding more and more things—like how to prepare a résumé, how to write a cover letter, how to write your first paper,” says Costanzo, who leads MACRO along with PMSE’s Dominik Konkolewicz of Miami University in Oxford, OH.\nIn about a year, MACRO will launch a mentorship program designed to support members at each career stage. Graduate students, for example, will mentor undergraduates and, in turn, be mentored by postdocs. Postdocs will similarly be both mentors and mentees.\nMaggie Kumler, a fourth-year chemistry major in Costanzo’s lab with an interest in sustainable polymers, has already found that being both a mentor and a mentee is greatly rewarding. “In our lab, the older students are used as mentors for the first and second years,” she says. “That’s something that I’ve gained a lot of experience in that I’m really grateful for.”\nAs with other networking opportunities, your POLY mentors and other colleagues can give you heads-ups about openings, write recommendation letters, or even refer you to hiring committees at their own companies. It’s not so much that a company will hire you just because you are a POLY member, but having the connections will make it much easier to get your foot in the door, says Davis. “Really these networks are incredibly vast.”\nThe first year of POLY membership is free to new members. Members keep tabs on POLY events by checking out the POLY website and following the group on Twitter (@POLY_ACS), Facebook (@ACSPOLY), and LinkedIn. If you’d like to meet other POLY members, a great way to start is by volunteering. “Students can reach out to me at any time to find out how to get involved,” says Knauer. “We are always looking for volunteers to help at the national meetings and regional meetings.”\nAnd if you’d like to volunteer to help with any of POLY’s other offerings, just contact the organizers. POLY members are passionate about helping students grow professionally, says Knauer. “That’s my favorite part of POLY, the really strong focus on educating the students and giving them as much opportunity as possible.”']	['<urn:uuid:e81e8c93-2b61-4092-8c90-293c8ec8fc54>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T22:03:18.033296	14	34	1494
88	die casting ejector mechanism energy efficiency	Die casting ejection works through pins connected to an ejector plate that force the casting from the cavity without causing deformation. For energy efficiency, NADCA provides resources including an Energy Webinar and Energy Treasure Hunt guide focusing on low/no cost process improvements to reduce energy consumption throughout die casting facilities.	"['High Pressure Die Casting (HPDC) technology\nFAIST Light Metals specialises in high pressure die casting (HPDC) technology for the production of aluminium components. Here we are going to explain how this process works and its features.\nSo, casting means forcing molten metal under high pressure into reusable metal dies. It is often described as the quickest route between raw material and finished product. The finished product also called “die casting” is an accurately dimensioned, sharply defined, smooth or textured-surface metal part.\nThe process has a number of phases:\n- the production of a steel mould able to produce tens of thousands of castings in a few seconds, which is divided into at least two sections to allow the removal of the castings.\n- Mounting of the two sections onto a specific machine where one will be stationary (fixed die half) while the other is moveable (injector die half). They are then clamped tightly together.\n- Injection of molten aluminium into the die cavity where it quickly solidifies.\n- The two sections are drawn apart and the casting is ejected.\nOf course, depending on the complexity of the final part, die casting dies can have moveable slides, cores, or other sections. The complete process is the fastest currently known able to produce precise non-ferrous parts.\nLet’s focus now on the die castings die composition. They are made of alloy tool steels and they have at least two sections:\n- The fixed die half, which is mounted on the side toward the molten metal injection system. It is specifically designed to contain the sprue hole through which molten aluminium enters.\n- The ejector die half, which is mounted on the moveable platen of the machine. It adheres to the other section and it is removed when the die is opened. Usually, it contains the runners (passage ways) and gates (inlets) which route molten metal to the die cavity (or cavities). It is also connected to an ejector box, which holds the mechanism to eject the casting from the die.\nHow ejection works?\nThe opening stroke of the machine involves the pins which are connected to the ejector plate moving forward thus they force the casting from the cavity. They must be carefully arranged so that any force placed upon the casting during ejection will not cause deformation.\nThen, when the die closes, return pins attached to the ejector plate return it to its casting position.\nThe die casting can be adjusted dependent on requirements. If the side of a die casting design requires a depression, one or more slides can be used to obtain the desired result without affecting the ejection of the casting.\nIndeed, if the slides and cores aren’t carefully fitted and securely locked into position during the process, molten metal could be forced into their slideways causing a disruption of operations.\nFixed and moveable cores are often used in dies. If fixed, the core axis must be parallel to the direction of the die opening. If moveable, they must be attached to core slides.\nIn conclusion, even though slides and cores increase the complexity and the cost of die construction, they allow adaptation of die castings to a wide variety of configurations, usually more economically than any other metalworking process.', ""NADCA has developed several energy resources to aid you. These web pages contain a wealth of information, including free videos, tutorials and statistics.\nOnline Energy Webinar\nThe hour long NADCA Energy Webinar will walk through energy usage in die casting and provide methods for reducing energy consumption.\nReduction Of Die Casting Energy Usage Through Process Optimization from NADCA Video News on Vimeo.\nEnergy Star published the Energy Treasure Hunt Guide to provide instructions on internally assessing energy waste. This guide focuses on building a culture of energy awareness by using a continuous improvement process to incorporate employees in assessing and improving energy use throughout the facility. The main area of focus is on low/no cost process improvements. NADCA is working on tailoring this process to the die casting industry. A recording of the webinar discussing how an Energy Treasure Hunt is performed and its use in die casting is provided. Contact NADCA if you have questions, comments or interest in facilitating an Energy Treasure Hunt.\nEnergy Saving Manual\nNADCA's Energy Saving Manual is designed to assist the die caster in establishing a strong, profitable energy management program for his or her organization. The book utilizes information and results obtained from several recent projects and studies performed in the metal casting industries to help the die caster determine efficient methods to utilize and conserve energy and to reduce some of the cost associated with producing die castings.\nDownloadable Melting Cost Calculator and the Energy Assessment\nThe calculator tabulates the amount of energy needed for melting and holding and indicates whether the die caster’s energy use is better than or below “best standards.” It also tells a die caster how much more or less energy should be used to operate at the most efficient levels.\nThe energy assessment software allows a die caster to evaluate its entire operation. Based on the user’s input, the program recommends immediate improvements, small capital expenditures, and improvements that necessitate a change in process or equipment.\nGreenhouse Gas Regulations and North American Die Casting Industry\nAs global climate change has received significant discussion for several years there has been continued discussion of nadca house gas regulations. This report gives background on the global warming debate. It also provides an in depth discussion of current regulatory proposals. The possible impact of these regulations on die casters is presented. Finally, the report provides further discussions of major sources of CO2 in an average die casting facility and ways to reduce emissions. The goal of reducing emissions being to reduces regulatory costs.\nThe Carbon Footprint of Aluminum and Magnesium Die Casting Compared to Injection Molded Components\nThis report was created in response to numerous questions from NADCA members. It is a comparative analysis of the carbon foot of aluminum and magnesium die castings with the carbon footprint of plastic injection molded components. The life cycle assessment procedures come from a section of the ISO 14000 environmental standards. A number of conclusions and recommendations are given including the largest NADCA house gas sources and methods to reduce their effect.""]"	['<urn:uuid:4af0ee8f-5753-429d-ae1c-0dee481c4fe3>', '<urn:uuid:c414cf49-d6a6-4eb6-8b67-952bb2913670>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T22:03:18.033296	6	50	1049
89	As someone interested in nature and environmental issues, I'd like to understand how educational institutions approach sustainability. What is the role of schools in teaching environmental awareness, and what responsibilities do universities have in addressing sustainability challenges?	Educational institutions have dual responsibilities regarding environmental awareness and sustainability. At the school level, there's an emphasis on inculcating a love for nature and respect for all forms of life, helping students develop a global outlook and concern for mankind. Moving to higher education, universities have been directed to take a leading role in addressing the sustainability crisis since Agenda 21. Universities have a special responsibility because they educate future leaders and decision-makers, influencing their knowledge, skills, and values. They also have a moral responsibility, as they've contributed to sustainability challenges through fragmented approaches to knowledge. Their role includes guiding policy development, exchanging knowledge, supporting communities, and using their academic freedom to drive innovation for sustainability. However, their current knowledge base is considered insufficient to solve contemporary global environmental problems, indicating the need for more integrated approaches.	"[""J. Krishnamurti's philosophy of education, reflected in the activities of Rishi Valley Education Centre, is best expressed in his own words:\n‘The purpose, the aim and drive of these schools, is to equip the child with the most excellent technological proficiency so that the student may function with clarity and efficiency in the modern world. A far more important purpose than this is to create the right climate and environment so that the child may develop fully as a complete human being. This means giving the child the opportunity to flower in goodness so that he or she is rightly related to people, things and ideas, to the whole of life. To live is to be related. There is no right relationship to anything if there is not the right feeling for beauty, a response to nature, to music and art - a highly developed aesthetic sense.\nI think it is fairly clear that competitive education and the development of the student in that process . . . are very, very destructive. We must be very clear in ourselves what we want - clear that a human being must be the total human being, not just a technological human being. If we concentrate very much on examinations, on technological information, on making the child clever, proficient in acquiring knowledge while we neglect the other side, then the child will grow up into a one-sided human being. When we talk about a total human being, we mean not only a human being with inward understanding, with a capacity to explore, to examine his or her inward state and the capacity of going beyond it, but also someone who is good in what he does outwardly. The two must go together. That is the real issue in education: to see that when the child leaves the school, he is well established in goodness, both outwardly and inwardly. (Krishnamurti on Education)’\nThe intention of the school, in other words, is to awaken the intelligence and the generosity of spirit in students so that they are able to meet an increasingly complex world without losing their humanity. The cultivation of a global outlook, a love of nature and a concern for mankind are all part of our educational aims. More specific goals of the educational philosophy of Rishi Valley School are:\n- To educate students so that they are able to explore both the world and their inner being\n- To inculcate a love for nature and respect for all forms of life\n- To create an atmosphere of affection, order and freedom without either fear or license\n- Not to condition the students in any particular belief, either religious, political or social, so that their minds may remain free to ask fundamental questions, enquire and learn.\nBased on this philosophy, teachers attempt to create an atmosphere of space, freedom, care and security in which students are helped to enlarge their horizons and grow.\nThe school is concerned with developing the individual talent and intelligence of each student.\nWe feel that it is important for parents to be aware equally of their children's talents as well as limitations, and allow them to develop in their own way, and not on the basis of predetermined expectations."", 'Society increasingly expects higher education to play a leading role in addressing the sustainability crisis. This trend seems to have started since Agenda 21, which directed universities worldwide into the driving seat of society’s quest for sustainability.\nWhat’s more, there seems to be a moral responsibility angle, based on the argument that higher education has to some extent contributed to society’s sustainability crisis in the first place. For example, on one hand universities are organised around disciplines, resulting in what is termed as ‘knowledge silos,’ without much conversation between disciplines. On the other hand, sustainability issues are complex and require a holistic approach to knowledge generation and dissemination. This indicates a mismatch between society’s needs and higher education’s provision in teaching and research.\nSuch issues have led to analyses like that of UNESCO (2012), ‘Our current knowledge base does not contain the solutions to contemporary global environmental, societal and economic problems.’ The sustainability crisis is therefore usually interpreted as ‘a knowledge crisis’ (Parker 2010).\nIn modern society university is the pinnacle of knowledge generation and dissemination. Because of their crucial role in educating and training future leaders and decision-makers, they can have great influence on their knowledge, skills and values. Besides the relative freedom that higher education enjoys gives it room for innovation for sustainability. So, not only higher education should play a leading role in sustainability, it has great potential to do so.\nThere is, however, no firm agreement on what higher education’s sustainability leadership role should be. In general, it could be viewed as comprising ‘developing future citizens, guiding policy development, exchanging knowledge, supporting communities, and using academic freedom to fuel further enterprise and innovation’ (Ryan et al 2010: 116) with a view of building a sustainable world.\nWhat has been higher education’s response to this sustainability challenge, especially in the UK? The next post will look at that.\nResources on higher education’s role in sustainability\n2012Greening Universities Toolkit (draft) (UNEP)\nThe role of higher education in addressing sustainability (Defra)\nHow can higher education institutions become more sustainable? (The Guardian)\nSustainable development in higher education: 2008 updateto strategic statement and action plan (HEFCE)\nSustainable Development in Higher Education: CurrentPractice and Future Developments (The Sustainability Information Teaching Exchange)\nSustainable Development on Campus (International Institute for Sustainable Development IISD)\nSustainable Development (HEFCE)\nSustainable Development (SFC)\nPeople’ssustainability treaty on higher education (University of Gloucestershire)\nReferences & Bibliography\nBeringer, A. and Adomssent, M. (2008) Sustainable university research and development: inspecting sustainability in higher education research. Environmental Education Research, 14(6), pp.607-623.\nChalkley, B. and Sterling, S. (2011) Hard times in higher education: the closure of subject centres and the implications for education for sustainable development (ESD). Sustainability, 3, pp. 666-677.\nDjordjevic, A. and Cotton, D.R.E. (2011) Communicating the sustainability message in higher education institutions. International Journal of Sustainability in Higher Education, 12(4), pp. 381-394.\nFerrer-Balas, D; Lozano, R; Huisingh, D; Buckland, H; Ysern, P; Zilahy, G (2010) Going beyond the rhetoric: system-wide changes in universities for sustainable societies, Journal of Cleaner Production, 18, pp.607-610\nMinguet, P.A., Martinez-Agut, M.P., Palacios, B., Pinero, A. and Ull, M.A. (2011) Introducing sustainability into university curricula: an indicator and baselines survey of the views of university teachers at the University of Valencia. Environmental Education Research, 17(2), pp. 145-166.\nParker, J. (2010) Competencies for interdisciplinarity in higher education. International Journal of Sustainability in Higher Education, 11(4), pp.325-338.\nRyan, A., Tilbury, D., Corcoran, P.B., Abe, O. and Nomura, K. (2010) Sustainability in higher education in the Asia-Pacific: developments, challenges, and prospects. International Journal of Sustainability in Higher Education, 11(2), pp.106-199.\nScott, W. and Gough, S. (2007) Universities and sustainable development: the necessity for barriers to change. Perspectives: Policy and Practice in Higher Education, 11 (4), pp. 107-115.\nStephens, J. C. and Graham, A.C. (2010) Toward an empirical research agenda for sustainability in higher education: exploring the transition management framework. Journal of Cleaner Production, 18(7), pp. 611-618.\nUNESCO (2012) Education for Sustainable Development [Online] Available from: http://www.unesco.org/new/en/education/themes/leading-the-international-agenda/education-for-sustainable-development/education-for-sustainable-development/ (Accessed 27 November 2012)\nWaas, T., Verbruggen, A. and Wright, T. (2010) University research for sustainable development: definition and characteristics explored. Journal of Cleaner Production, 18, pp. 629-636.\nWright, T.S.A. and Wilton, H. (2012) Facilities management directors’ conceptualizations of sustainability in higher education. Journal of Cleaner Production, 31, pp. 118-125.\nPhotograph by Asitha Jayawardena']"	['<urn:uuid:8e9dcd96-c1ca-403c-9102-b108d76e556f>', '<urn:uuid:472e1ccb-0d1b-44f3-8ec0-31cb525865e3>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T22:03:18.033296	37	137	1251
90	where is publisher big brother mouse located in the asian country	Big Brother Mouse is located in a rickety building on Hiaphong Street in Luang Prabang, the old royal capital of Laos.	['Highs and Laos of being a bestselling author\nI first heard about Colin Cotterill on a family holiday in the old royal capital of Laos, called Luang Prabang.\nIt’s a small and atmospheric city of softly spoken residents and half-hidden architectural treasures.\nWeather-beaten temples and French colonial-era mansions dot the streets and side-streets and down at the river bank, the waters of the Mekong slide silently by.\nThere are pockets of activity here, on the main street where the tourists congregate and in a rickety building, on a dusty alleyway, that the locals call “Big Brother Mouse”.\nVisitors who venture through the front door are greeted with a large book shelf packed with picture books and children’s stories – and usually, there is a good supply of youngsters making use of them in a room off the side.\nThe books are written and produced here, for the ramshackle building on Hiaphong Street is an unlikely looking publishing house.\nIt was founded by an American called Sasha Alyson, a retired businessman from Boston.\nWhile holidaying in Laos in 2003, he failed to spot a single book in the country – so he decided to do something about it.\nThree years later, ‘Big Brother Mouse’ produced the first batch of children’s books written in the local language. The idea behind the project is simple – raise literacy levels and promote the idea that reading can be fun.\nThe land of no books\nYet the task before the Big Brother Mouse team is huge, for this isolated and impoverished nation continues to be, in many ways, the land of no books.\nFew people could afford the $7 or so required to buy a popular novel – but then again, they are unlikely to find anything in Lao on the shelves anyway.\nA government-approved six volume set called “The History of Laos” did make it into a handful of shops this year, but we were told by one book seller that ‘it’s not selling very well.’\nWhen I asked Mr Alyson about the paucity of reading material for adults, he did offer a few words of encouragement however; “you do know that Colin Cotterill is trying to get his books published here, don’t you?”\nI didn’t – but I decided to find out more.\nMr Cotterill is the critically acclaimed author of an eight book crime-fiction series set in Laos. The London native got to know the country in the early 1990’s when he took a job re-writing the national university’s English course – it had been designed some years earlier by a team of East Germans and was in need of some fairly major repair.\nA shaky start\nStill, the author told me this rather exotic sounding opportunity got off to a shaky start : when I arrived in the country someone on the airplane leaned over and said: “do you realise that you have hepatitis?”\n“I went into the bathroom and looked at this big yellow glowing person and I realised that I was arriving in Laos sick. So on my first day I went to the hospital and spent three months there for treatment – and I liked the place so much that I moved in.”\nCotterill spent two years at the Mahosot Hospital in Lao capital, Vientiane and the experience serving as the inspiration for books about “Dr Siri Paiboun”.\nThis doc is a curmudgeonly figure, dreaming of a long and uninterrupted retirement when, to his undisguised disgust, officials from Laos’ one and only political party – the Communists – order him to become the national coroner.\nStill, Dr Siri does as he is told, casting a wry eye at “revolutionary” Laos as he solves all manner of crimes and mysteries.\nThe series is now available in 14 languages and seems ideally suited to the local book-starved market – but making them available here has proven a challenge of almost Mekong-length complexity.\nStruggle to get published\nFor starters, getting the books translated in Laos has been anything but simple. Cotterill told me: “I have been looking for six, seven, eight years for someone to translate my books but I’ve never been able to find anyone who could actually get the jokes.”\nAfter asking “20 to 30 people around the world’”to have a go, Cotterill thinks he has finally found someone who gets the jokes in both languages.\nHis name is Neil Garforth, an electrician from Rochdale, who has been travelling the highways and waterways of Southeast Asia for twenty years.\nGarforth has developed an ear for languages over the years and now works as a legal translator in Vientiane.\nIt is pretty dry stuff he told me – contracts and laws and memoranda of understanding – but at least it is predictable.\nMr Cotterill’s novels offer a different sort of challenge; “Dr Siri Paiboun was described as a short-arsed man,” said Garforth, reading out-loud from the book. “I mean, how do I translate short-arsed into Lao? It’s not like I have got a lot of words to choose from,” he chuckled.\nLess than a dollar a page\nMr Garforth is not doing it for the money – less than a dollar a page he said – and he seems to have inherited a fairly weighty burden; “when I do work on it, sometimes I re-write a paragraph 20 times because I’m not happy.” When I asked Garforth how far he’d actually got, he said he had finished the first chapter of the first book.\nIf the translated text of the first novel, “The Coroner’s Lunch” is produced, Mr Cotterill will need to get the book printed into the Lao language – but he is running into problems in that department as well.\nUnsurprisingly perhaps, the “country of no books” doesn’t have a whole lot of printing factories either.\nIn fact there is only one printer in Laos who uses equipment that could reasonably be described as ‘contemporary’ and he happens to be a devout Christian who insists on reading over the material before it touches his machines.\nHis name is Mana Jangmook, a self-described “unusual businessman”. “I turn down millions of dollars of business every year,” he told me; “We won’t print everything. Like magazines, like sex appeal – we totally deny, we reject completely. Or maybe horoscopes, we deny (that too).”\nQuite possibly, he will also deny Colin Cotterill’s books which contain plenty of references to Buddhism and local animist traditions – the sort of thing Mr Mana doesn’t look particularly favourably on.\nIf you thought that marked the end of this literary obstacle course, there is one more challenge to face – and it is a rather formidable one at that.\nIf the books do get translated and printed, they have to be sent on to the “state censors” for approval before anyone can sell them.\nThe censors work in a curtained-off room at the Laos Ministry of Culture and one of our contacts was kind enough to take a picture of the outer door – but the mere mention of this building in Vientian produces an array of anxious or terrified responses. When I suggested to one local that we would really like to film it for our report, he started to visibly shake.\nColin Cotterill seems to be taking this particular threat in his stride however. “I don’t know how the Ministry will react to these books. I don’t know what will be left or how long it will be. It might be a six page book by the time everyone’s put their hands on it,” he said.\nStill, the London-native is quite sure that he is doing the right thing – even if the final chapter in this particular “adventure-in-publishing” seems a long way off; “I want to see how they will react to a story set in their own country.\n” They watch foreign movies but they can’t imagine that Laos could ever produce anything like that and I want to give them that feeling – the feeling that their lives and their stories are just as important as anything they see in Hollywood.”\nFollow @c4sparks on Twitter.']	['<urn:uuid:112fbc5f-da80-4c65-91c4-2ccb9f395ec4>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T22:03:18.033296	11	21	1354
91	I'm planning to visit East Devon and I love viewing landscapes. Can you tell me what kind of views I can expect to see from the high points in the area?	From the open upland ridges, particularly in the north, you can enjoy extensive views across East Devon. There are outstanding views southwards to the coast and northwards into the Blackdowns. When looking from these ridges, you can often see over the tree tops on the steep greensand scarps to other ridges beyond and observe the strong field patterns on the valley slopes.	['This landscape comprises a narrow, elevated, rolling ridge fringed by steep scarp slopes and upper valley farmland. The main ridge forms a distinctive spine to East Devon, fanning out into a series of narrow fingers that have distinctive southerly aspect and are flanked to the south by wooded slopes, which give way to small scale farmland in the upper and tributary valleys of the Axe and Sid. The northern finger of the ridge sits between the upper Otter valley and the Yarty valley. The river valleys drain southwards, forming steep sided and often remote-feeling valleys, whose sides are clothed with ancient oak woodlands. The Sid valley is the major landscape feature of the southern part of the area; further east the valleys are shorter, steeper and narrower. Views from the open upland ridges, particularly in the north, are extensive, often looking over the tree tops on the steep greensand scarps to other ridges beyond and into the visually strong field pattern of the valley slopes. The area is sparsely settled, with individual farmsteads and small hamlets in the valleys and vernacular buildings mainly of stone and red brick. Seen from the south, the area forms a backdrop to the valleys, separating the coastal plateau and Axe valley from the Blackdown Hills to the north.\nThis landscape shares many of the characteristics of the Blackdown Hills and is physically separated from that area by the upper Otter valley. It comprises a greensand ridge which forms a central spine to East Devon south of the A30 and Honiton. The north-western boundary of this area is marked by the abrupt wooded scarp overlooking the River Otter and Honiton. To the east there is a more gradual transition to lower lying farmland and the River Axe’s tributary valley the Yarty. To the south is a clear boundary with open plateau farmlands of the Sidmouth and Lyme Bay Coastal Plateau. To the north where the ridge narrows, the landscape becomes transitional with the Blackdown Hills and the boundary between the two areas, although difficult to perceive, is drawn along the Stockland Hill road.\n|Constituent LCTs:||1A: Open Inland Planned Plateaux, 2A: Steep Wooded Scarp Slopes, 3A: Upper Farmed and Wooded Valley Slopes|\n|Part of NCA:||147: Blackdowns|\n- Elevated narrow ridge with a rolling topography underlain by clay-with-flints or Upper Greensand.\n- Landscape crossed by streams and springs draining into the adjacent steeply incised wooded valleys.\n- Higher ground sparsely wooded with grown-out beech hedgebanks and some pine shelterbelts.\n- Steep scarp slopes that are densely wooded – ancient oak with bluebells and primroses; and some conifer plantations which extend onto the ridges.\n- Regular modern and Parliamentary fields of large and medium scale on the ridges, reflecting late enclosure of common land (e.g. Stockland Hill), contrasting with smaller curving fields of medieval origin on valley slopes.\n- Low narrow earthbanks with hedges on the ridgetop, with wider historic banks in the upper farmed valleys and more species-diverse Devon hedges (e.g. beech, sycamore, ash, hazel and gorse) with flower, fern and moss-rich banks on lower slopes.\n- Mainly pasture (often improved) and dairy farming with some mixed farming on heavy brown soils.\n- Remnant areas of gorse, heathland and bracken on the ridges; ancient semi-natural and broadleaved woodlands on the scarp slopes; semi-improved and unimproved acidic and neutral grassland and springline mires occurring mainly on the scarp slope and in the upper farmed valleys.\n- Bronze Age barrows in elevated positions on the ridges; hillfort sites on upper slopes of valleys; ancient settlement remains that also lend strong time-depth.\n- Clustered hamlets and villages at road crossings with little 20th century development. Farmsteads scattered throughout, nestled in dips with beech shelterbelts.\n- Straight roads with verges on ridges change abruptly to sinuous lanes defined tightly by close hedgebanks, with steep decents.\n- Very distinctive building tradition that uses local cherts (flint-like nodules occurring occurs within greensand) with red brick detailing and slate roofs.\n- Strong overarching perceptions of tranquillity and remoteness in many areas.\n- Distinctive, unspoilt, and very exposed skylines.\n- High scenic quality reflected in the north half of the area forming part of the Blackdown Hills AONB and the southern half part of the East Devon AONB.\n- Outstanding views across East Devon, mainly southwards to the coast but also northwards into the Blackdowns.\n- Sense of isolation, tranquillity and remoteness, enhanced by natural qualities of the rivers and scarp woodlands.\n- Many CWS sites comprising ancient semi-natural woodland; semi-improved and improved acidic and neutral grasslands; springline mires; and extensive areas of heathland and gorse scrub particularly on Gittisham Hill and Broad Down and Dumpton Hill.\n- Prominent Bronze Age barrows on Grittisham Hill and Farway Hill; earthwork remains of former castles e.g. Stockland Little Castle, Stockland Great Castle and Farway Castle hillforts.\n- Combe House Registered Park and Garden, including historic wood pasture and parkland.\n- Picturesque villages with traditional buildings linked by narrow winding lanes crossing historic stone bridges; many listed buildings.\nForces for Change and Their Landscape Implications:\n- Past planting of small coniferous plantations forming conspicuous features on higher ground and blurring the distinction between open ridge and steep wooded scarps.\n- Decline in woodland management including coppicing, and a spread of non-native species affecting the appearance and biodiversity of the landscape’s woodlands.\n- Hedgerow removal in the drive for agricultural intensification; replacement with post-and-wire fencing, affecting the integrity of field patterns.\n- Lack of hedgerow management (laying and coppicing) leading to grown out sections of beech, now susceptible to windthrow and storm damage.\n- Conversion of former areas of moorland to pasture, beginning in the late 18th century and intensifying after the Second World War.\n- Recent lack of grazing on land on ridgetop edges and steep valley sides, leading to a spread of gorse and bracken.\n- Prominent telecommunications masts on Gittisham Hill and television transmitter at Stockland Hill.\n- Growing demand for facilities such as caravan parks, holiday accommodation, golf courses and visitor attractions – many as farm diversification enterprises.\n- Recreation pressures and growing traffic levels on rural roads, particularly at holiday time.\n- Heavy farm traffic leading to vehicular damage to roadside hedges and woodland.\n- Removal of vegetation by Highways Authority leading to a change in character of roads.\n- Peace and tranquillity interrupted by main roads radiating from Honiton that transect the area i.e. the A375, A35 and A30.\n- Increase in the area of coniferous plantation and woodland planted to filter water, minimise downstream flooding, store carbon and provide low carbon fuel (through coppice management).\n- Potential loss of or change in oak- and beech-dominated valley woodlands due to spread of Phytophthora, intolerance of water level extremes and more frequent storm events.\n- Uncertain future for the agricultural economy – levels of future agri-environment support and market prices for farmed products unknown.\n- Longer growing season and faster growth of bracken, gorse and secondary woodland resulting in a decrease in remaining areas of heathland and rush pasture.\n- More intense periods of drought, as a result of climate change, leading to the drying out of important wetlands including spring line mires and rush pasture.\n- Increased autumn and winter precipitation leading to higher water levels and consequent increases in poaching of river banks and flood risk in lower catchments.\n- Development pressure within the area and along the main A375 corridor and north of Sidmouth, particularly due to its attractiveness of the area as a place to live.\n- Increased demand for wind turbines and communications masts on higher ground as well as for domestic and community-scale solar panels and small wind turbines, with cumulative impact on landscape.\n- Uncertainty over future agri-environment scheme funding putting conservation of species-rich grasslands and heaths at risk.\nTo protect the distinctive and highly scenic interrelationship of narrow ridge, wooded scarp and intimate upland valley. Opportunities are sought to restore conifer plantations to broadleaved and heathland habitats reinforcing the distinction between the landscape types that make up this area and strengthening the network of valuable heathland habitat. Field patterns are reinforced through the restoration and management of distinctive hedgebanks. Scarp woodlands are managed and valley side wetlands expanded to help prevent downstream flooding and protect water quality. The landscape’s time-depth continues to have a strong influence, whilst opportunities for sustainable recreation and limited low-carbon development are sensitively accommodated. The peaceful and historic character of the valley settlements is enhanced whilst providing recreational spaces in less prominent locations.\n- Protect the distinctive, unspoilt, and exposed skylines of the central spine and its outstanding views across East Devon.\n- Protect and appropriately manage the rich cultural heritage of the area’s hilltops, such as Bronze Age barrows, hillforts, through livestock grazing at appropriate levels and recreation management.\n- Protect the character and setting of Combe House park and garden, ensuring any new development does not encroach into the historic landscape or views to it.\n- Protect the sparse settlement pattern of clustered hamlets, villages and farmsteads maintaining the settlements’ characteristic form and peaceful character.\n- Protect traditional building styles and materials, particularly local chert with red brick detailing, utilising the same styles and materials in new development wherever possible (whilst seeking to incorporate sustainable design).\n- Protect the landscape’s network of quiet lanes enclosed by woodland and species-rich hedgebanks, resisting unsympathetic highways improvements or signage.\n- Manage and enhance the scarp semi-natural woodlands through traditional techniques including coppicing and control access by livestock to promote natural regeneration. Explore opportunities for community utilisation of coppice residues as a low-carbon fuel source.\n- Use extensive grazing to promote the species diversity of woodland ground flora.\n- Manage the landscape’s distinctive beech hedges on higher ground to strengthen the strong square field pattern. Reinstate coppicing to mature sections and grown-out trees to ensure the future survival of these characteristic features.\n- Respect the traditional methods and styles of construction, including stone facing on banks.\n- Within the valleys, manage and extend species-rich mires and neutral and acidic grasslands through appropriate grazing and traditional land management regimes – both to enhance their wildlife value and functions in flood prevention.\n- Manage heathlands and acidic grasslands on hills such as Dumpton by appropriate grazing and other works to control scrub and bracken.\n- Plan for long-term restoration of the more prominent conifer plantations and shelterbelts to semi-natural habitats (where their role in timber production has ceased) including re-creation of heathland on the ridges and broadleaved woodland on steep scarp slopes.\n- Create, extend and link woodland and wetland habitats to enhance the water storage capacity of the landscape, reducing soil erosion, agricultural run-off and downstream flooding and improving water quality.\n- Extend and link fragmented woodland sites through natural regeneration and new planting (using appropriate native climate-hardy species).']	['<urn:uuid:bf298246-4e95-494f-88b7-83cc18f60e83>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T22:03:18.033296	31	62	1794
92	environmental impact natural gas leaks wind farm effects bird population both issues explain	Regarding natural gas, the EPA is underestimating methane leaks from the industry by 25-75%. Methane is a potent greenhouse gas responsible for about one-fifth of global warming since the 18th century, with atmospheric levels rising rapidly since 2007. As for wind farms' impact on birds, while experts believe they may negatively affect bird populations, no statistically significant evidence of negative impacts on breeding bird populations has been found. However, wind farms did show significant negative effects on local populations of specific non-breeding birds like geese, Wigeons, Golden Plovers and Lapwings. Collision rates vary between 0 and 50 birds per turbine per year, with gulls and raptors being the most frequent victims.	"[""America’s natural gas system is super-leaky, and that’s bad news for the climate\nLike a free-riding bus passenger whose expired ticket gets overlooked by the driver, the natural gas industry has been getting a free pass from the EPA for its global warming impacts for well over a decade.\nA new mega-analysis of 20 years worth of research suggests that the EPA is underestimating the fossil fuel’s climate impacts by 25 to 75 percent.\nThe problem with the EPA’s math doesn’t concern the burning of natural gas, which produces less carbon dioxide than other fossil fuels (but way more than solar panels or wind turbines). The problem is in the leaky systems that extract and transport the fuel.\nThe EPA relies on 1990s estimations to calculate the climate-warming effects of the natural gas industry. A new paper published in the journal Science concludes that the EPA is severely underestimating the amount of natural gas that leaks into the atmosphere during drilling, processing, and distribution.\nAnd those leaks are important, because natural gas is basically just methane. Methane is a potent greenhouse gas, responsible for perhaps a fifth of global warming since the 18th century. Levels of methane detected in the atmosphere have been rising rapidly since 2007, although scientists aren’t quite sure why. Some of it appears to be oozing out of the ground, in some cases because global warming is breaking down icy sheaths that had helped keep it in place, but the livestock and fossil fuel industries are also playing major roles.\n“People who go out and actually measure methane pretty consistently find more emissions than we expect,” said Stanford University researcher Adam Brandt, the lead author of the new paper. “Atmospheric tests covering the entire country indicate emissions around 50 percent more than EPA estimates. And that’s a moderate estimate.”\nThe scientists didn’t just rely on studies of methane levels in the atmosphere. They also looked at studies that attempted to quantify the amount of natural gas that escapes from fracked wells, pipelines, and other infrastructure.\nThe findings were so stark that the researchers dispute the widespread assumption that natural gas–burning buses are better for the climate than those that burn diesel.\nStill, when it comes to power plants, the authors say it is better to switch over to natural gas than to continue burning coal. That’s partly because coal burning releases a lot of carbon dioxide, and partly because coal mining releases a lot of methane. “Natural gas is very likely to provide climate benefits compared to coal for electricity generation,” Garvin Heath, a National Renewable Energy Laboratory researcher who coauthored the new paper, told Grist.\nThe study concluded that fracking “is unlikely to be a dominant contributor to total emissions,” but it reported elevated methane levels near some gas and oil fields.\nUltimately, this synthesis of 20 years’ worth of research highlighted the desperate need for more research. One priority area needs to be the identification of a presumed small number of “superemitters” that are believed to be responsible for a large proportion of methane leaks. “Identifying superemitters will take investment by a combination of governmental and industrial research and development to reduce the cost of methane detection and leak source identification. Some approaches exist, but radical improvement is needed in their speed and cost for widespread adoption,” Heath said. “We note this as a priority.”\nMethane Leaks from North American Natural Gas Systems, Science.\nAmerica's natural gas system is leaky and in need of a fix, new study finds, Stanford University.\nDonate now to support our work."", 'Impacts on biodiversity of exploitation of renewable energy sources: the example of birds and bats – facts, gaps in knowledge, demands for further research, and ornithological guidelines for the development of renewable energy exploitation.\nThe purpose of this report is to collect and to evaluate the available information on the impacts of exploitation of renewable energy sources on birds and bats. The focus is on wind energy as there is only little information on the impact on birds and bats of other sources of renewable energy. The report aims at better understanding the size of the impact, the potential effects of re-powering (exchanging small old wind turbines by new big turbines), and possible measures to reduce the negative impact on birds by wind turbines. In addition the need for further research is highlighted.\nThe evaluation is based on 127 separate studies (wind farms) in ten countries, most of them in Germany. Most studies were brief (not more than two years) and did not include the preconstruction period. Before-After Control Impact studies that combine data collection before and after, in this case construction of a wind farm, on both the proposed development site and at least one reference site were rare. In only a few cases, would the design of the study and the length of the study period theoretically allow statistically significant effects of wind farms on birds and bats to be found at all. Assessments of impacts, therefore, are usually based on few studies only. This report includes all studies readily available to the authors, irrespective of the length of the study period and the quality of the study design. In order to base the assessments on as many independent samples as possible even rather unsystematic observations were included. The information of the data was reduced to a level that justified the application of sign tests. The compilation of many different individual studies gave the following results:\nThe main potential hazards to birds and bats from wind farms are disturbance leading to displacement or exclusion and collision mortality. Although there is a high degree of agreement among experts that wind farms may have negative impacts on bird populations no statistically significant evidence of negative impacts on populations of breeding birds could be found. There was a tendency waders nesting on open grounds to be displaced by wind farms. Some passerines obviously profited from wind farms. They were probably affected by secondary impacts, e.g. changes in land management or abandonment from agricultural use next to the wind plants.\nThe impact of wind farms on non-breeding birds was stronger. Wind farms had significantly negative effects on local populations of geese, Wigeons, Golden Plovers and Lapwings.\nWith the exceptions of Lapwings, Black-tailed Godwits and Redshanks most bird species used the space close to wind turbines during the breeding season. The minimal distances observed between birds and pylons rarely exceeded 100 m during the breeding season. Some passerines showed a tendency to settle closer to bigger than to smaller wind turbines.\nDuring the non-breeding season many bird species of open landscapes avoided approaching wind parks closer than a few hundred metres. This particularly held true for geese and waders. In accordance with published information disturbance of geese may occur at least up to 500 m from wind turbines. For most species during the non-breeding season, the distances at which disturbance could be noted increased with the size of the wind turbines. For Lapwings this relationship was statistically significant. There was no evidence that birds generally became „habituated“ to wind farms in the years after their construction. The results of the few studies lasting longer than one season revealed about as many cases of birds occurring closer to wind farms (indications for the existence of habituation) over the years as those of birds occurring further away from wind farms (indications for the lack of habituation).\nThe question whether wind farms act as barriers to movement of birds has so far received relatively little systematic scientific attention. Wind farms are thought to be barriers if birds approaching them change their flight direction, both on migration or during other regular flights.\nThere is evidence for the occurrence of a barrier effect in 81 bird species. Geese, Common Cranes, waders and small passerines were affected in particular. However, the extent to which the disturbances due to wind farms of migrating or flying birds influences energy budgets or the timing of migration of birds remains unknown.\nCollision rates (annual number of killed individuals per turbine) have only rarely been studied with appropriate methods (e. g. with controls of scarvenger activities). In particular, such studies are missing for Germany. Collision rates varied between 0 and more than 50 collisions per turbine per year for both birds and bats. Obviously the habitat influenced the number of collisions. Birds were at high risks at wind farms close to wetlands where gulls were the most common victims and at wind farms on mountain ridges (USA, Spain), where many raptors were killed. Wind farms in or close to forests posed high collision risks for bats. For both birds and bats, the collision risk increased with increasing size of the wind turbine. The relationship, however, was not statistically significant.\nGulls and raptors accounted for most of the victims. In Germany the relatively high numbers of White-tailed Eagles (13) and Red Kites (41) killed give cause for concern. Germany hosts about half of the world population of breeding Red Kites and has a particular responsibility for this species. Bird species that were easily disturbed by wind farms (geese, waders) were only rarely found among the victims. Bats were struck by wind turbines mostly in late summer or autumn during the period of migration and dispersal.\nPopulation models created by the software VORTEX revealed that significant decreases in size of bird and bat populations may be caused by relatively small (0,1 %) additive increases in annual mortality rates, provided they are not counter acted by density dependent increases in reproduction rates. Short-lived species with high reproductive rates are more affected than long-lived species with low reproductive rates. The latter, however, are less able to substitute additional mortality by increasing reproductive rates.\nThe effects of „repowering“ (substitution of old, small turbines by new turbines with higher capacity) on birds and bats is assessed by the available data and by simple models. There is no information, however, on the effects of the newest generation of very large wind turbines. According to current knowledge, repowering will reduce negative impacts on birds and bats (disturbance and mortality) if the total capacity of a wind farm is not changed (many small turbines are replaced by few big turbines). In a scenario in which the capacity of a wind farm is increased 1.5 fold, negative impacts start to dominate. In case of a doubling of wind farm capacity, repowering increases the negative impacts of the wind farm. Repowering offers the chance to remove wind farms from sites that are associated with high impacts or risks for birds and bats. New turbines could be constructed on sites that are likely to be less problematic with respect to birds and bats.\nEffective methods of reducing the negative impacts of wind energy use on birds and bats include:\n- choice of the right site for wind farms (avoidance of wetands, woodlands, important sites for sensitive non-breeding birds and mountain ridges with high numbers of raptors and vultures),\n- measures to reduce the attractivness of wind farm sites for potential collision victims,\n- configuration of turbines within wind farms (placement of turbines parallel to and not accross the main migration or flight directions of birds),\n- construction of wind turbines: replacement of lattice towers, wire-cables and overhead power lines.\nMeasures to increase the visibility of wind turbines and to reduce the effects of illumination remain to be studied.\nIn spite of many publications on windfarms and birds there still is a great demand for further research. First of all there is an urgent need for reliable data on collision rates at wind turbines of birds and bats in Germany. This holds true particularly for the new and big turbines which will replace the present generation of wind turbines.\nIt is still unclear whether these big and necessarily illuminated turbines pose a high collision risk to nocturnal migrants which have not yet been greatly affected by smaller turbines. The high collision rates of Red Kites in Germany also merit urgently study. The aim of the research has to be a quick reduction of the collision rates. The sensitivity to wind farms of many other bird species that are of particular nature conservation interest (storks, raptors, Cranes) has not yet been sufficiently studied.\nThere is hardly any information on the impacts of arrays of solar panels on birds and bats. Studies should be initiated as soon as possible.']"	['<urn:uuid:e4680def-00b8-4dbc-9f4b-e22389aa02b1>', '<urn:uuid:94aae7dd-018c-48dc-b84c-619735491aee>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T22:03:18.033296	13	111	2063
93	i have a bin for food scraps what foods go into the bin like fruit pasta rice beans	You can put fruit and vegetable scraps (fresh or cooked), bread, pasta, rice, grains, cereal, baked goods, nuts, beans, seeds (including shells/hulls), egg shells, coffee grounds, filters, and tea bags (but no foil products) into your food scraps bin.	"['Feed The Green Bin\nTo SIGN UP or see if you are in the collection area, please click here.\nAccording to a recent Howard County study, almost 30% of what we send to the landfill could be composted and turned into a valuable product!\nInterested households in certain areas (who receive County yard trim collection) can participate in the Feed The Green Bin program. This new program collects the same types of material you have always set at the curb, the difference is a separate container!\nParticipants receive a special food scrap/yard trim cart that is emptied weekly.\nIf you\'re in the collection area and want another green cart, here\'s the form to purchase additional carts. Please remember that green carts are to be used for food scraps and yard trim ONLY.\nWatch our help videos to learn indoor & outdoor tips for collecting food scraps. Neat!\nWhat is Accepted?\n- Fruit and vegetable scraps (fresh or cooked)\n- Grass, leaves, yard trim\n- Bread, pasta, rice, grains, cereal, baked goods, etc.\n- Nuts, beans, seeds (including shells/hulls)\n- Egg shells\n- Coffee grounds, filters, tea bags (no foil or foil-backed products)\n- Paper products (paper towels, napkins, paper plates)\n- Pizza boxes (remove non-food items)\n- Houseplants, cut flowers\n- Chopsticks and popsicle sticks\nWhat is NOT Accepted?\n- Meat, fish, shellfish (including bones)\n- Dairy products (cheese, butter, ice cream, etc.)\n- Fats, oils, grease\n- Facial tissues\n- Pet waste\nBenefits of collecting food scraps for composting:\n- Reduces household trash. Food scraps are a large part of what people throw away.\n- Reduces greenhouse gases - food scraps in the landfill create methane (a powerful greenhouse gas) as they decompose.\n- Reduces the need for garbage disposals. By using garbage disposals to get rid of unwanted food, excess nutrients are sent to treatment plants. They are costly to process and remove (about 10 times more expensive than curbside collection and processing).\n- Creates a useful product – compost is a great soil amendment that returns nutrients to gardens and produces healthy plants.\n- Saves money. Removing food scraps from trash reduces trash tonnages and therefore money spent on disposal.\n- Keeps the food scraps local. Food scraps are processed locally at Alpha Ridge Landfill into a soil amendment.\n- Provides public awareness. Food scraps are a significant part of household waste that shouldn\'t be wasted in the landfill.\n- Promotes opportunities for local business growth which may enable food scrap collection to be expanded throughout the region.\nFrequently Asked Questions\nWhere can I buy compostable bags?\nYou are not required to use bags, but if you prefer to, please make sure the bags contain the BPI logo. The BPI logos means they are compostable and they have been tested and proven to breakdown (DO NOT use regular plastic bags). Many local retailers including Roots, My Organic Market, Kendalls Hardware, Home Depot, and Target carry the bags. They are also available online from many different websites.\nWhy collect food scraps separately?\nTo reduce waste and save money on disposal costs. Howard County residents could reduce trash sent to the landfill by 30%, simply by composting their food scraps. Food scraps and yard trim that is collected will be composted and turned into a soil amendment.\nWhat will the cart look like and how big is it?\nResidents can choose from 2 green carts: a 35 gallon cart (39” tall x 20” wide x 23” deep) or a 65 gallon cart (40.5"" tall x 27"" wide x 23"" deep). Remember, you can add yard trim to the green carts too!\nHow often is the cart emptied?\nThe cart will be emptied on your recycling day by the same truck that collects yard trim. No additional collection trucks are needed!\nMy household doesn’t generate many food scraps. Should I still participate?\nYes! You can make a difference, no matter how big (or small!) your household. Composting even small amounts of food scraps reduces trash. Unexpected items such as herbs/spices, cereal, corn cobs/husks, cookies and paper egg cartons can be included.\nWill green bin collection smell?\nPlease keep in mind you will be setting out the same material for collection; it will just be in a different container. Meat, fish and dairy products, often the most odor causing items, are not accepted for food scrap collection.\nYou can easily incorporate paper bags, pizza boxes, grass, leaves or baking soda to absorb liquids and reduce any smells. Rinsing and cleaning the cart may also help.\nWhat if I rely on my garbage disposal?\nUsing a garbage disposal can lead to clogs and backups in your pipes. Disposals also send nutrients that are expensive to treat to the water reclamation facility. Composting food scraps in your backyard or using the green bin for collection are better options. Watch Bill Nye\'s quick video about garbage disposal use.\nWill the 40-pound weight limit apply to my food scrap cart?\nNo, not for the county-issued cart. Additional containers should weigh less than 40 pounds.\nWhat about leftovers that contain meat, oil, butter or cheese?\nSmall amounts of these items used as an ingredient can be included in the green bin. However, meatballs or other easily removed meat, fish or dairy products should be placed in the trash.\nWhat should I use to collect food scraps indoors?\nA large tub or plastic container is all you need, but if you prefer a food scrap specific container, check online or your local stores. You may also use compostable bags (with the BPI logo) or paper bags. Do not use plastic bags because they don\'t break down with the material. You can also make an easy indoor bin liner out of newspaper.\nHow do I learn more about the new ARL pilot compost facility?\nReport from the mini food scrap collection pilot in Howard County.\nFood scrap pictures from the mini pilot.\nSee us on the news!\nFood Scrap Composting at Work\nFood Scrap Composting at Events']"	['<urn:uuid:9ccdf372-210e-4b13-9458-953326583945>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T22:03:18.033296	18	39	999
94	how many species of mammals in costa rica	There are 208 species of mammals in Costa Rica.	"['|Costa Rica Page in My World-Map-Software|\nCOSTA RICA :\nOn his final voyage to the New World in 1502, Christopher Columbus came ashore on Costa Rica\'s Caribbean coastline (near Limon) and the indigenous Indians presented him with an offering of gold.\nThe lure of riches in the Americas was a strong magnet for European adventures of-the-day, but those that ventured to Costa Rica were confronted by an inhospitable landscape of coastal swamps and hostile Indians tribes - and most did not survive.\nThe Spanish persisted, and by the mid-16th century a few small settlements were successfully established; Cartago was declared the capital, and the colony was ruled by a governor appointed by the Spanish empire’s regional capital of Guatemala.\nUnlike neighboring colonies, Costa Rica was slow to grow, and remained a collection of subsistence farmers for the next two centuries. With very few exports of real value, it quietly remained off the radar screen of the Spanish Crown.\nAfter the overthrow of the Spanish King by Napoleon, Costa Rica and others declared their independence from Spain in 1821. Then, Costa Rica, as well as El Salvador Guatemala, Honduras and Nicaragua formed the United Provinces of Central America, but that federation quickly dissolved, and Costa Rica became an independent state in 1838.\n|Juan Mora Fernandez|\nJuan Mora Fernandez (the first head of state) encouraged the cultivation of coffee, and over time the powerful coffee barons helped finance a political revolution that would change the country forever.\nOver the next century an assortment of presidents and ambitious military dictators came and went, but for the most part, Costa Rica avoided the chaos of bloody coups, rebellions and citizen revolts endemic to other Central American countries.\nIn the mid-1900s, democracy was the order-of-the-day, but when the United Social Christian Party refused to leave power after losing the 1948 general election, civil war was in the wind.\nThe opposition in that brief 40-day uprising was led by Jose Mar¡a (Don Pepe) Figueres Ferrerhe, and in the end he founded a new republic; banned the Communist Party; women could vote; blacks gained citizenship; and in a brilliant stroke of governing he abolished the armed forces and established a term limit for all future presidents.\nWhen Don Pepe died in 1990 (a justifiable national hero to this day) his innovative leadership had earned Costa Rica an enviable international reputation as a peaceful, stable democracy in a region of instability.\nFun Facts of Costa-Rica:\n1) Costa Ricans refer to themselves as Ticos (males) or Ticas (females).\n|Guanacaste is Costa Rica\'s national tree|\n|The national flower is the guaria morada, an orchid|\n2) The Guanacaste is Costa Rica\'s national tree and the clay-colored thrush is the national bird. The national flower is the guaria morada, an orchid.\n|clay-colored thrush is the national bird.|\n3) Costa Rica has a population of 4.5 million and a life expectancy of almost 77 years, one of the highest in the world.\n|There are 800 miles of coastline in Costa Rica between the Pacific and Caribbean coasts.|\n4) There are 800 miles of coastline in Costa Rica between the Pacific and Caribbean coasts.\n5) Costa Rica is divided into seven provinces: San Jose, Alajuela, Heredia, Cartago, Guanacaste, Puntarenas and Limon.\n6) There are more than 121 volcanic formations in Costa Rica, and seven of them are active.\nPoas Volcano has the second widest crater in the world (nearly one mile in diameter), and\nArenal Volcano is one of the ten most active volcanoes in the world.\n7) Chirripo Mountain is the highest point in Costa Rica at 12,500 feet. The country\'s lowest point is 790 feet below sea level in the caves of Barra Honda National Park.\n|caves of Barra Honda National Park|\n8) Irazu is the tallest volcano in Costa Rica at 11,259 feet.\n9) Costa Rica hosts more than 5% of the world’s biodiversity even though its landmass only takes up .03% of the planet\'s surface. There are more than:\n130 species of freshwater fish\n160 species of amphibians\n|Poison Arrow Frog|\n208 species of mammals\n|White-faced Capuchin (Cebus capucinus)|\n220 species of reptiles\n|common chunk headed snake.|\n|hatchling-Olive-ridley sea turtle.|\n850 species of birds\n1,000 species of butterflies\n1,200 varieties of orchids\n|bee approaching a bird of paradise flower.|\n9,000 species of plants\n34,000 species of insects\n|Black Honey Bee|\n|Giant brown long-horned beetle|\nand still counting as new species are discovered every day\n10) More than 25% of Costa Rica’s land is dedicated to national parks, reserves and wildlife refuges. There are more than 100 different protected areas to visit.\n|A soda is a small, informal restaurant that serves traditional meals like chicken, rice, beans|\n11) A soda is a small, informal restaurant that serves traditional meals like chicken, rice, beans and salad for $2-$3 a plate.\n12) A pulperia is a neighborhood store that sells essential foodstuffs like canned goods, eggs, milk, bread and some produce.\n13) For most of Costa Rica, the dry season runs from December through April. During these months, the Pacific coast and most of the Central Valley receive little rain, or the odd afternoon shower. May through mid-November is considered the rainy season, when days are marked by sunny mornings and late afternoon thunderstorms. The Caribbean coast, Monteverde and Arenal areas are exceptions to this rule, and have no distinct dry season.\n|The Tempisque River Bridge|\n14) The Tempisque River Bridge (known locally as El Puente de Amistad) that connects the mainland to the Nicoya Peninsula was a gift from Taiwan.\n15) Costa Rica\'s official language is Spanish, though a large number of its citizens are bilingual. English, due to its status as the international language of tourists, is the most common second language in Costa Rica.\n|Today, driven by agricultural products (especially bananas and coffee)|\nToday, driven by agricultural products (especially bananas and coffee), and strong tourism and technology industries, Costa Rica enjoys a high standard of living, and remains the true success story of Central America.\nCosta Rica Industry:\nCosta Rica is one of the world\'s largest per capita producers of hydroelectric power with big installations at Lakes Arenal and Cachi.\n|hydroelectric power with big installations at Lakes Arenal and Cachi.|\nFor years it exported electricity to Nicaragua and El Salvador, but domestic use increased so that much is still generated with expensive, imported oil. Recently Daylight Saving was tried in order to save power usage, but was abandoned due to public pressure and concern for children walking to school on narrow roads in the dark. Though there has been some oil exploration and talk of a trans-Costa Rican pipeline to transport Alaskan and Venezuelan oil, presently no oil is produced in the country, which has made balancing the economy difficult.\nCosta Rica offers tax incentives to labor-intensive industries producing items for export. Many plants import textiles and other raw materials, hire Costa Ricans to assemble them, and export finished products such as underwear. My uniform shirts on the Alaska ferry were labeled ""assembled in Costa Rica"". One successful operator imports used typewriters from North America, reconditions them in a modem plant, and exports them wholesale all over Latin America. There\'s plenty of room for imagination here! Costa Rica would rather produce many products internally than buy with foreign exchange, but with a well-scattered population only as big as a large city, there isn\'t the market to make it pay.\nCosta Rica\'s major non-agricultural industry, aside from government, is tourism. Ecotourists can find more species of birds, wildlife, and plants growing naturally here than in any other similar area. With planning, tourism can make preserved tropical forests and beaches more profitable than any other use of the same land-and keep it so forever.\n|Costa-rican-Marriott Hotel-san jose.|\nRecently Costa Rica has attracted travelers from all over the world and their numbers have grown rapidly. Hotels near San Jose and at the beaches and tours and lodges for nature lovers are increasing though sometimes still are less than the demand. Costa Rica has so much to offer the tourist, and does it so well, that it is rapidly being discovered by thousands who ride jets instead of tossing Spanish galleons.\n|Picture of Coffee Tree|\n|Shoveling coffee beans inside the truck.|\nThe main products of Costa Rica are agricultural...fruit and coffee exports are way up on the list. As far as industries are concerned, the tourism industry had over 2 million visitors last year, and it is expected to be a bit more than that this year\nCosta Rican Sports\nCosta Ricans are avid sports fans and interested in a variety of spectator sports.\n|The national game of Costa Rica is undoubtedly, soccer|\nThe national game of Costa Rica is undoubtedly, soccer! Sports in Costa Rica form an integral part of the Costa Rican social life.\nSome of the sports that can be practiced are tennis, running, sport fishing, trekking, boxing, motocross, swimming, baseball, basketball, diving, snorkeling, and of course surfing.']"	['<urn:uuid:72bd376a-9722-44ef-bf0c-f324af56fd59>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T22:03:18.033296	8	9	1487
95	How can training help with stress, and what activities help officers cope?	Training can help by 'inoculating' individuals against stress through exposure to realistic but manageable stress levels, similar to how vaccines work. The training should combine high-intensity physical activity with job-relevant tasks, helping prepare for violent encounters that require functioning under both physical and psychological stress. As for coping activities, officers should maintain plans and goals outside of policing, associate with non-law enforcement friends, participate in satisfying activities like coaching or community groups, and seek advice from senior officers who have effectively managed stress. These non-police activities help officers decompress and gain different perspectives, which is crucial for maintaining resilience against stress.	['As seen in Night Stick the official publication for New Hampshire Police Association (Volume 77 Spring 2012 edition)\nAs your brain is your most important weapon, the effects of physical stress is often underappreciated for its effects on cognition for those who go into harms way, such military personnel, law enforcement, and other groups.\nIt’s been a generally held belief that cerebral blood flow (CBF) remains relatively steady during exercise, but recent studies (1,2) suggest that’s not the case. CBF appears to depend on a multitude of factors, including exercise intensity. With lower intensity steady state forms of exercise – all things being equal, such as hydration, etc, – CBF may increase, but during high intensity intermittent forms of exercise (like wrestling with another human being in a life or death struggle for example), appears to decrease. This may partially explain the cognitive decline people experience during high intensity exercise.\nThis information very much applies to law enforcement as it does for the military. Under psychological stress, demand for CBF increases, while supply may decrease, resulting in additional cognitive decline.\nIf a person has not experienced that cognitive decline under training conditions – and so has some experience and understanding of its effects – the results could prove fatal during a “real life” encounter.\nIf one has not experienced some job related stress training, they often find even simple directions difficult to follow during their first exposure to it. For example, simple directions regarding which targets to engage in which order, or other simple instructions, fail to happen…\nObviously, this will differ greatly with the persons experience, training, fitness, etc., but it’s well established that physical + psychological stress = decline in cognitive abilities and that’s old news for most and common sense.\nThe specific effects of stress, physical and psychological, on cognitive abilities is a large and growing topic of interest and research for the military and law enforcement.\nBasic take home: some form of stress training, where higher intensity, intermittent, training combined with firearms- hopefully following some job/task relevant movement patterns – will help best prepare those who are likely to face violent encounters that require an ability to function under physical and psychological stress simultaneously.\nAdditionally, such training will add to an ability to physically deal with the effects of stress and will improve overall fitness and general physical preparedness (GPP) of law enforcement and mil populations.\nMost people realize how physical and emotional stress will negatively impact performance, but few make the effort to utilize some form of stress training to help ‘inoculate’ the person to that stress, which could very well mean the difference between life and death in a highly stressful encounter not uncommon to law enforcement personnel.\nFor example, an article entitled “Assessment of Humans Experiencing Uncontrollable Stress: The SERE Course” by Dr. C.A. Morgan III and Major Gary Hazlett (3) goes into depth on the effects of stress on performance, and the importance of properly applied stress to help “inoculate” the individual to future stressors:\n“Physical and psychological stress are unavoidable in military operations, yet the negative effects of stress can make it difficult or impossible for individuals or teams to operate effectively. Stress is an essential element of warfare, and individual responses to combat-related stress have often been the determining factor between victory and defeat on the battlefield.”\n“Having recognized the effects of stress, the U.S. military designs its training scenarios to be both rigorous and realistic in their stress intensity. Rigorous training improves a person’s ability to perform on the battlefield, and exposure to realistic levels of stress can protect or “inoculate” a person from some of the negative effects of operational stress. The concept of stress inoculation is very much like the concept of preventing a particular disease through vaccination: When stress inoculation occurs, an individual’s performance will likely be better the next time he is stressed. Like immunization, which occurs only when the vaccine is given in the proper dosage, stress inoculation occurs only when the stress intensity is at the optimal level – high enough to activate a person’s psychological and biological systems, but low enough so as not to overwhelm them. If the stress level is not high enough, inoculation will not occur; if the stress level is too high, stress sensitization will occur, and the individual will probably perform less effectively when he is stressed again.”\nThe above is focused on the military specifically, but the same principles apply to other professions where a high level of physical and mental stress is likely to occur; such as law enforcement. The correct application of some form of practical job related stress training could pay long term dividends for the law enforcement community.\nHowever, it needs to be designed with the population in mind (patrol LEO vs. Tactical LEO, mil, etc), and correctly applied to that population. Some in the LE tactical community have found my Practical Applied Stress Training (P.A.S.T) is a tool in the tool box that can help there.\n(1) J Appl Physiol. 2009 Nov;107(5):1370-80.\nCerebral blood flow during exercise: mechanisms of regulation.\n(2) Sports Med. 2007;37(9):765-82.\nRegulation of cerebral blood flow during exercise.\n(3) 2000 Edition of the The Professional Bulletin of the John F. Kennedy Special Warfare Center and School\nWill Brink is the owner of the Brinkzone Blog. Will has over 30 years experience as a respected author, columnist and consultant, to the supplement, fitness, bodybuilding, and weight loss industry and has been extensively published. Will graduated from Harvard University with a concentration in the natural sciences, and is a consultant to major supplement, dairy, and pharmaceutical companies.\nHis often ground breaking articles can be found in publications such as Lets Live, Muscle Media 2000, MuscleMag International, The Life Extension Magazine, Muscle n Fitness, Inside Karate, Exercise For Men Only, Body International, Power, Oxygen, Penthouse, Women’s World and The Townsend Letter For Doctors.\nHe’s also been published in peer reviewed journals.\nYou can also buy Will’s other books on Amazon, Apple iBook, and Barnes and Noble.', 'Why your off-duty life is important for stress management\nOver time, if occupational stress is not properly managed, officers may experience life-threatening consequences\nLaw enforcement is one of the most stressful occupations in existence. Over time, if occupational stress is not properly managed, officers who are currently employed, as well as those who are retired, may experience life-threatening consequences. These consequences could be physiological, psychological, or behavioral (Patterson et al., 2014).\nPolice officers face many threats to both their physical safety and mental health that the general population does not. Officers are exposed to traumatic events and must regularly respond to crime scenes and domestic disputes. They also experience stress from things like leadership and management challenges as well as the difficulties associated with long hours and shift work. These different factors often contribute to officers suffering from chronic stress.\nThe Consequences of Unmanaged Stress\nDue to the acute stress that officers experience while on-duty, combined with the chronic stress that occurs over time, officers can suffer from both physical or mental problems. Officers are at a greater risk of suffering from mental disorders. In one study, 24 percent of the officers examined reported symptoms of post-traumatic stress disorder (PTSD), 9 percent had depression symptoms, and 19 percent reported alcohol abuse (Fox et al., 2012). Other research by Austin-Ketch et al. (2012) and June-Hee et al. (2016) found prevalence rates of PTSD among police officers at 35 percent and 41 percent respectively.\nIn addition to concerns involving mental health, they are at higher risk of physical problems. Police officers are at risk of developing cardiovascular disease due to high-levels of stress (Thayyil, Jayakrishnan, Raja, & Cherumanalil, 2012). They are also more likely to have higher alcohol consumption caused by exposure to work-related violence as they continue to accumulate months or years of experience in policing (Leino et al., 2011). Alcohol abuse can then lead to many additional problems for officers including isolation, psychological, physical, and personal problems (Chopko et al., 2013; Leino, Eskelinen, Summala, & Virtanen, 2011).\nTo promote resilience against the psychological, physiological, and behavioral problems resulting from stress, officers need to apply effective stress management strategies. Ways that officers spend their time off-duty is an important part of this.\nWhile on-duty, officers see the worst side of society and are regularly exposed to traumatic events. The stress that this causes can be countered by engaging in activities and maintaining a lifestyle that is unrelated to policing. All officers need to deliberately work to establish a personal identity that’s distinct from their career as an officer. This enables officers to truly separate themselves from the uniform while off-duty.\nTo minimize the risks associated with police stress and to promote resilience from the stress experienced on-duty, officers should:\n- Maintain plans and goals outside of being a police officer. This should include long-term goals following retirement and other aspirations that increase an officer’s personal satisfaction. Giving thought to the future, post-policing, is important because it requires officers to reflect on topics that are not associated with policing.\n- Associate with society outside of law enforcement. It is common for police officers to congregate with other officers, even while off-duty. However, having friends outside of policing is important because it can provide insight into how people in other occupations solve problems. Having civilian friends can also help officers socialize and be part of their community in a role that is outside of law enforcement.\n- Have activities outside of work that provide satisfaction. Engaging in activities outside of policing is also important in maintaining a proper work-life balance. These activities can include regular participation in things like coaching, community groups, sports teams or anything else that officers personally enjoy. These activities also provide opportunities for officers to take their mind off the stresses of policing.\n- Accept the advice of senior officers who have effectively managed stress throughout their police career. Regardless of career stage, officers should take advantage of the excellent resource found in the experience and support of other officers. Take the time to have discussions with fellow officers about how they cope with stress and what works for them. This can be especially insightful if veteran officers have developed coping strategies that are unique to the stressors officers face in specific regions, within certain agencies due to politics or processes, or other factors that may be unique to that area.\nPolice stress is a major concern and can have serious psychological and physiological consequences if it is not properly managed. It is important that steps are taken to promote officer resiliency even while off-duty. Maintaining an identity aside from the badge enables an officer to decompress through personal interests and gives them an opportunity to gain experiences and perspectives that are not police-related. Officers should strive to maintain a network of friends outside of work who are not associated with law enforcement and can provide problem-solving strategies from the perspective of other occupations. In addition, officers should also engage in off-duty activities that promote relaxation. Lastly, officers should talk with other officers at various career stages to gain insight about coping mechanisms and what they do off-duty to mitigate the stress associated with policing.\nAbout the author\nDr. Jarrod Sadulski is an adjunct professor with American Military University. He has spent more than two years studying police stress and its influence on the lives of police officers. Sadulski has conducted a review of approximately 300 peer-reviewed scholarly articles that focused on topics associated with police stress and officer wellness. In addition, he conducted a two-year qualitative study on how successful police officers effectively manage stress throughout their law enforcement career. Each participant in Sadulski’s study reflected upon the value of life and having an identity outside of policing as an effective stress management strategy. With a Ph.D. in Criminal Justice, he continues to research effective stress management strategies for police officers to promote resiliency. In addition, Sadulski has 20 years of policing experience between both federal and local law enforcement. Sadulski is available for speaking engagements and assisting agencies with the development of peer-support and stress management implementation programs. You can contact him at firstname.lastname@example.org.\nAustin-Ketch, T. L., Violanti, J., Fekedulegn, D., Andrew, M. E., Burchfield, C. M., & Hartley, T. A. (2012). Addictions and the criminal justice system, what happens on the other side?? Post-traumatic stress symptoms and cortisol measures in a police cohort. Journal of Addictions Nursing (Taylor & Francis Ltd), 23(1), 22-29 8p. doi:10.3109/10884602.2011.645255\nChopko, B. A., Palmieri, P. A., & Adams, R. E. (2013). Associations between police stress and alcohol use: Implications for practice. Journal of Loss & Trauma,18(5), 482-497. doi:10.1080/15325024.2012.719340\nFox, J., Desai, M. M., Britten, K., Lucas, G., Luneau, R., & Rosenthal, M. S. (2012). Mental-health conditions, barriers to care, and productivity loss among officers in an urban police department. Connecticut Medicine,76(9), 525-531. http://csms.org/publications/connecticut-medicine/\nJune-Hee, L., Inah, K., Jong-Uk, W., & Jaehoon, R. (2016). Post-traumatic stress disorder and occupational characteristics of police officers in Republic of Korea: a crosssectional study. BMJ Open, 6(3), 1. doi:10.1136/bmjopen-2015-009937\nLeino, T., Eskelinen, K., Summala, H., & Virtanen, M. (2011). Work-related violence, debriefing and increased alcohol consumption among police officers. International Journal of Police Science & Management,13(2), 149. doi:10.1350/ijps.2011.13.2.229\nPatterson, G. T., Chung, I. W., & Swan, P. W. (2014). Stress management interventions for police officers and recruits: A meta-analysis. Journal of Experimental Criminology, 10(4), 487-513. doi:10.1007/s11292-014-9214-7\nThayyil, J., Jayakrishnan, T. T., Raja, M., & Cherumanalil, J. M. (2012). Metabolic syndrome and other cardiovascular risk factors among police officers. North American Journal of Medical Sciences,4(12), 630. doi:10.4103/1947-2714.104313']	['<urn:uuid:aff672a5-a8dd-475d-b1f6-b8163a2c9398>', '<urn:uuid:ed607d6c-791f-4caf-810d-ac3a1bac1198>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T22:03:18.033296	12	101	2265
96	sugar industry tactics to hide health risks and brain chemistry changes from sugar consumption what evidence exists	Evidence shows the sugar industry used PR tactics to deflect health concerns, including sponsoring scientific research and silencing critical media coverage, while scientific evidence demonstrates sugar causes changes in brain chemistry through dopamine pathways. Documents from the 1970s reveal the industry spent $230,000 on research to defend sugar, while neuroscience shows sugar consumption leads to dopamine down-regulation and desensitization of natural reward circuits, similar to drug addiction. The industry actively worked to hide these biological effects and maintain public opinion that sugar was safe.	"['When Cristin Couzens went on the hunt for evidence that Big Sugar had manipulated public opinion, she had no idea what she was doing. She was a dentist, not an investigative reporter. But she couldn\'t let go of the nagging suspicion that something was amiss.\nHer obsession started in an unlikely place, at a dental conference in Seattle in 2007 about diabetes and gum disease. When one speaker listed foods to avoid, there was no mention of sugar. ""I thought this was very strange,"" Couzens said. And when a second speaker suggested sugary drinks were a healthy choice, she chased him down at the end of the conference to make sure she\'d heard him correctly. ""How could you possibly recommend sweet tea as a healthy drink?"" she asked the speaker, who paused just long enough to say, ""There is no evidence that links sugar to chronic disease,"" before he bolted out the door.\n""I was so shocked by that statement,"" she said, ""I felt obligated to do a little bit of research, thinking perhaps the sugar industry had somehow had an influence over the lack of advice to limit sugar intake to prevent and control diabetes. That\'s what set me off.""\nShe quit her job, exhausted her savings and spent 15 months scouring library archives. Then one day she found what she was looking for, in a cardboard box at the Colorado State University archives.\n""The first folder that I opened jumped right out at me,"" she said. ""It was on the Sugar Association letterhead which is the trade association in Washington for cane and beet sugar producers. And the word ""confidential"" was right under the letterhead. So the first document I saw was a confidential Sugar Association memo talking about their PR strategies in the 70s.""\nWhat Couzens found was something food industry critics have been seeking for years — documents suggesting that the sugar industry used Big Tobacco tactics to deflect growing concern over the health effects of sugar.\n""So I had lists of their board reports, their financial statements, I had names of their scientific consultants, I had a list of research projects they funded, and I had these memos where they were describing how their PR men should handle conflict of interest questions from the press,"" she said.\nThe documents survived in the Colorado University Library Archives only because they helped explain a photograph of three men and a trophy. When the Great West Sugar Company went out of business in the 1980s, someone put the files in a box so that librarians would know who the men were and why they were being honored. So who were they?\n""That was a picture of sugar industry executives being awarded the Silver Anvil, which is like the Oscars of the PR world,"" Couzens said. In the 1976 photo, the president of the Sugar Association and its director of public relations smile as they pose with their prize for their successful campaign ""forging public opinion,"" in the face of mounting consumer and government concern over the health risks of sugar.\n""It\'s a little bit shocking to me that an industry would be rewarded for manipulating scientific evidence,"" Couzens said. ""At the time the award was given in 1976, there was a controversy. Many people thought sugar was harmful, the sugar industry wanted to turn public opinion toward thinking sugar was safe so they forged public opinion on how the public viewed the effects of sugar,"" she said.\nAs Couzens sorted through the documents, the full extent of that campaign to forge public opinion emerged. The documents describe industry lobby efforts to sponsor scientific research, silence media reports critical of sugar, and block dietary guidelines to limit sugar consumption.\nThe Sugar Association\'s president reported to the Board of Director\'s meeting in October, 1976 that, ""in confronting our critics we try never to lose sight of the fact that no confirmed scientific evidence links sugar to the death-dealing diseases. This crucial point is the life blood of the association.""\nAnd the Sugar Association was clear about its intention to use science to defeat sugar’s critics. To support ""our desire to maintain research as a main prop of the industry\'s defence,"" the member companies were told that $230,000 was being reserved to fund scientific research, according to a report from the Sugar Association\'s annual meeting of the board of directors, in Chicago in 1977.\nAt the next meeting, a year later in Washington, the Sugar Association updated the board of directors on the research program, listing 17 different scientists at some of the world\'s most prestigious universities, including M.I.T., Harvard, and Yale, receiving sugar industry money\nSome of the money was supplied by ""contributing research members,"" heavy sugar users including ""Coca-Cola, Hershey, General Foods, General Mills and Nabisco"" who contributed the funds specifically for the lobby group\'s scientific effort. The document reports that those companies each ""contributed $10,000 to our general research fund.""\nTargeting media coverage\nThe documents also reveal efforts to manipulate media coverage. The Sugar Association had been stung by a New York Times headline: ""The Bitter Truth About Sugar,"" written by a prominent nutritionist in June 1976. ""It\'s bad for the health, bad for the teeth, and we eat more of it than we think"", the article declared.\nThe Sugar Association president warned the board of directors that this ""shoddy piece"" was being followed up by ""naive writers working for other papers around the country."" But the association had saved the day. Thanks to an inside tip, they were able to keep Reader\'s Digest from running a condensed version of the same article.\n""Thanks to friendly sources who alerted us, a progressive approach to research by the Digest and persistence on our part, we were able to persuade them to cancel the story,"" the document reports. ""We did it in two stages. We failed in our initial conversations, but succeeded when we took our case to the editor-in-chief. Our telegraph to him is included in your folders and might be helpful should you be confronted by criticism.""\nThe documents Couzens found in that cardboard box also reveal that the Sugar Association was busy trying to block dietary guidelines that would recommend limits on sugar consumption. At the time, the US Senate Select Committee on Nutrition and Human Needs, headed by Senator George McGovern, had released ""Dietary Goals for the United States,"" which recommended that Americans should reduce their sugar intake by 40 per cent.\nThe Sugar Association had been warned by a committee insider that ""the final conclusions would hang sugar,"" the association\'s president reported to the board in 1977. And now that the committee\'s report had been released the results ""certainly bear this prediction out,"" he added.\nBut the lobby group had a plan. ""The McGovern Report has to be \'neutralized\'"" that document reports, assuring the members that the Sugar Association would fight back, because ""the consequences of losing this battle and permitting dietary goals to become a basic reference are too grave to be taken lightly.""\nSugar industry committed to sucrose consumption\nWhen Couzens approached the sugar industry for comment, she was told the documents were ancient history. ""They gave a comment like, that was in the past and does not reflect the mission of the sugar association currently,"" she said. But then she found one more document, an internal Sugar Association e-newsletter from August, 2003 that announced ""the Sugar Association is committed to the protection and promotion of sucrose consumption. Any disparagement of sugar will be met with forceful, strategic public comments and the supporting science.""\n\'It\'s very, very difficult to find those kinds of …[documents]\n, so it was a real treasure.\' — Marion Nestle\nCouzens said that document showed ""the sugar industry is still very active in nominating scientists to serve on the dietary guidelines advisory committee, and it is still publishing research through connections with the World Sugar Research Organization, based in London. These scientific reviews that are published by the sugar industry are still considered in the evidence review for the dietary guidelines, so they’re still serving to balance out the evidence,"" she said.\nArmed with all of these documents, Couzens\' next challenge was to make them public somehow. She sought out author Gary Taubes, an American science writer who has reported extensively on the health effects of sugar. He listened to her story and offered to help her get the documents published in the independent news magazine Mother Jones, under the provocative title ""Big Sugar\'s Sweet Little Lies."" What happened when this bombshell was finally revealed, after months of research and sacrifice? Nothing, Couzens said. The story failed to be picked up by the mainstream media.\nCouzens blames it on Hurricane Sandy. The media was distracted. But, she said, the story ""has gotten particular attention from food policy folks here in the U.S. and also from researchers who had been studying the tobacco industry who see the parallels,"" she said.\n""I thought it was just wonderful and I got in touch with her right away,"" said Marion Nestle, author of Food Politics, a professor at New York University and vocal food industry critic. ""Those kinds of things are very, very hard to come by. One of the things that brought down the cigarette companies was finding enormous documentation of the efforts of cigarette companies to cover up the fact that they knew that cigarettes were unhealthful. It\'s very, very difficult to find those kinds of things, so it was a real treasure.""\nWhen CBC News asked the Canadian Sugar Institute for comment on the revelations that the sugar lobby group influenced science and public opinion, it didn\'t answer the question. Instead we received this reply: ""The Canadian Sugar Institute (CSI) is the national, non-profit association representing Canadian sugar manufacturers on nutrition and international trade affairs. I am providing you with a summary of the scientific consensus on some common misconceptions about sugars and health.""\nChiara DiAngelo, the Canadian Sugar Institute\'s coordinator of nutrition communications, also offered the names of several university professors to talk to for more information about industry-sponsored research.\nThis is the second of two special features by CBC health reporter Kelly Crowe on how industry designs food so that we crave them.', 'What’s going on in the brain behind addiction?\nLet’s explore the brain chemistry behind pleasure and addiction. While addiction is a complex condition that we still don’t know everything about, there are certain neuroscience concepts that I’ve summarized to help better explain what we do know about this mental illness.\nWhat Is Addiction?\nThere are various definitions of addiction. The National Institute on Drug Abuse (NIDA) defines addiction as a chronic, relapsing brain disease that’s characterized by compulsive drug seeking and use despite harmful consequences. Brain changes caused by addiction can be long lasting and can lead to many harmful, often self-destructive, behaviors.\nWhat Can You Become Addicted To?\nWhen we think of addiction, we tend to think of drugs and alcohol. However, one can become addicted to anything that stimulates the brain’s dopamine reward pathway. Video games, shopping, sugar, thrill-seeking, and social media are just some potential sources of addiction. In the case of social media, the variable reward system associated with receiving “likes,” comments, and followers boosts dopamine release, fostering feelings of pleasure and perpetuating a cycle of dependence. While many things have the potential to become addicting, how do we determine when dependence is a problem?\nIn order to officially diagnose a substance use disorder, clinicians use the Diagnostic and Statistical Manual of Mental Disorders (DSM-5). The DSM-5 was created by hundreds of international experts on behavioral health with the purpose to improve diagnoses, treatment, and research. The essential feature of a substance use disorder is a group of cognitive, behavioral, and physiological symptoms that show a person continues using a substance despite significant substance-related problems.\nChanges in Brain Chemistry\nAlcohol and drugs produce their psychoactive effects by altering brain chemistry. More specifically, neurotransmitters, or chemical messengers in the brain, responsible for daily functions (e.g., learning, memory, appetite, pleasure, pain) are altered by substance use. A key neurotransmitter in the process of pleasure and addiction is dopamine.\nDopamine and Reward\nIn addition to its primary role in pleasure and addiction, dopamine is also involved in processes like learning, cognition, memory, and movement. Most of your dopamine is generated deep in the midbrain and released in three pathways across the brain as shown below.\nIt’s natural for people to seek out experiences that result in pleasure by stimulating the dopamine reward pathway. For example, natural processes that stimulate the dopamine reward pathway include exercising, eating, meditating, and having sex.\nNatural Reward vs. Substance-Induced Reward\nWhen people seek out experiences that feel good, these experiences boost dopamine. When people use drugs, dopamine transmission can be 3-5x greater than natural sources of pleasure. As you can imagine, this is not sustainable.\nDown-Regulation of Dopamine\nContinual activation of the dopamine pathway reduces the availability of dopamine in the brain. This reduction, or down-regulation, in dopamine availability has a blunting effect on the natural reward circuit. This partly explains why those who abuse substances may find it difficult to enjoy natural pleasures that they previously did. Something that once naturally caused pleasure no longer does as the reward circuit has been desensitized.\nIs Sugar Addiction Real?\nAs mentioned earlier, one can become addicted to any substance or activity that stimulates the brain’s dopamine pathway. Sugar, for example, can be a highly addictive substance that can change one’s brain chemistry. Neuroscientists have shown that sugar leads to dopamine release just as drugs do (e.g., cocaine, heroin). Also, sugar can lead to changes in dopamine receptors, increasing tolerance and requiring more sugar over time for the same effects.\nSince the dopamine pathway is involved with both sugar consumption and drug use, research has shown that those addicted to drugs can have increased cravings for sugar when they’re in early sobriety. This effect, known as cross-tolerance, illustrates that addiction to one substance makes it easier to become addicted to another substance that may use the same brain mechanisms. For example, marijuana is often referred to as a “gateway drug” as it may introduce the dopamine pathway to seek further experiences of drug use.\nSelf-Medicating Mental Illness\nAs discussed in the neuroscience of depression and anxiety, mental illness can be caused by an imbalance in the brain’s neurotransmitter activity. Mental illness and substance abuse are often comorbid and bidirectionally influence each other. Feelings of depression, for example, can lead to self-medication with substance use as it may the depression by stimulating dopamine release. External dependence on regulating one’s mood can quickly develop into a vicious cycle that leads to dopamine depletion and dependence.\nAddiction as a Brain Disease\nSubstance abuse can cause lasting changes to brain chemistry, especially the dopamine reward pathway. Limited availability of dopamine in certain areas of the brain may desensitize the reward circuit to natural sources of pleasure. People may choose continued substance use to compensate for the deficit in dopamine availability, thus perpetuating the cycle of addiction.\nIt’s possible for one’s brain to return to its baseline availability of dopamine after substance abuse. Research has shown that extended periods of sobriety, lasting at least twelve months, may be required for partial recovery of normal dopamine function. The time it takes for one’s dopamine functioning to revert to its normal state may be longer than some people are able to abstain from substance abuse.\nHowever, some people who receive treatment may not be able to return to normal dopamine pathway functioning depending on one’s history of substance abuse. As it can be incredibly challenging to retrain the brain’s cravings for substance-induced pleasure, and avoid triggers for substance use, rates of relapse are greatest in early sobriety.\nWithdrawals: a Price of Addiction\nHave you ever experienced irritability, fatigue, or cravings after you stopped eating sugar, drinking caffeine, or smoking marijuana? Most substance use results in withdrawal symptoms that can range from mild headaches to severe flu-like symptoms — depending on the substance in question and how chronic your dependence was. You feel irritable, sick, and fatigued because your brain is struggling to cope without the substance it became dependent on to feel normal. Also, contrary to popular belief, chronic substance users don’t solely continue using a drug to get “high,” but rather to prevent withdrawal symptoms that can interfere with daily life.\nMedication-assisted treatment, or “MAT,” is the use of medications in combination with counseling and behavioral therapies to treat substance use disorders. It can allow people to live satisfying, functional lives by relieving withdrawal symptoms and psychological cravings. For example, in the case of opioid addiction, methadone, is an approved MAT. Methadone “tricks” the brain into thinking it’s still getting opiates that it’s become dependent on. When used as prescribed, methadone mitigates withdrawal symptoms and does not cause one to feel “high.” People may be on methadone for weeks, months, years, or a lifetime. When they wean from methadone depends on factors such as their biology, environmental stability, and psychological readiness to cope with potential cravings and triggers for drug use. For more on opioid dependence and narratives on substance abuse, visit my series on Untold Stories of Addiction.\nAddiction is a chronic, relapsing brain disease that’s characterized by compulsive drug seeking and use despite harmful consequences. Brain changes caused by addiction, such as dopamine depletion, can be long lasting and can lead to many harmful, often self-destructive, behaviors. Also, addiction goes far beyond alcohol and drugs as humans can become dependent on anything that stimulates the brain’s dopamine reward pathway. Video games, shopping, sugar, thrill-seeking, and social media are just some potential sources of addiction. Understanding the process of addiction is critical for recovery. While cravings, withdrawal symptoms, and self-medication can pose barriers, recovery is possible and there is hope.\nIs there something you’ve become dependent on? Do you experience cravings and compulsive behaviors? These are questions we must all periodically ask ourselves, especially in a society that normalizes and profits from addiction.\nAddiction Treatment Resources\nIf you or a loved one is struggling with addiction of any kind, please know that there is treatment available. Utilizing SAMHSA’s (the Substance Abuse and Mental Health Services Administration) website or helpline is a great place to start for more information.\nSAMHSA’s National Helpline, 1-800-662-HELP (4357): The National Helpline is a free, confidential, 24/7, 365-day-a-year treatment referral and information service (in English and Spanish) for individuals and families facing mental and/or substance use disorders.\nI’d like to express a huge thanks to the professor of my Addictions course in graduate school, Dr. Eric Devine. The knowledge gained from his class was an impetus for my work as a clinician in the field of addiction. To my readers, thank you for being a part of my journey as a mental health counselor.\nOne Comment Add yours']"	['<urn:uuid:48bcd4db-c62d-4874-bc56-21046f526bc8>', '<urn:uuid:80f0cf85-8a77-415c-b1ed-c8fa6cc71c4b>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T22:03:18.033296	17	84	3146
97	modern techniques ancient wisdom benefits indoor farming buddhist mindfulness	Modern vertical farming techniques and Buddhist mindfulness principles both offer ways to manage resources efficiently and reduce suffering. Vertical farming uses scientific methods like hydroponics, aeroponics, and aquaponics to maximize crop yields while minimizing resource use. This aligns with Buddhist teachings about being mindful and heedful - carefully managing thoughts and actions to achieve better outcomes. While vertical farming addresses physical sustenance through controlled environments and reduced resource consumption, Buddhist mindfulness provides mental sustenance through controlled awareness and reduced mental suffering.	['We shouldn’t have fear.\nWe should be fearless.\n(I’ll admit with the last one that fear can short-circuit thinking in ways that kill the mind and that can later harm us.) But let’s try and change the way we look at fear.\nThere are three qualities that the Buddha listed as the roots of unskillful behavior: greed, aversion, and delusion. Some psychotherapists have asked why he didn’t list fear as the fourth, because psychotherapy tends to see neurotic fear as the primary source of mental illness. Why didn’t the Buddha have the same understanding?\nBecause he saw that fear has its uses. It’s not always unskillful. If you go into a forest, it’s right to be fearful. If you weren’t fearful, you’d get complacent and careless. You could die. When you think about your own mortality — how fragile your life is, how fragile your health is, how fleeting your youth is — it’s right to feel a certain amount of fear for the future: How are you going to fare when aging, illness, and death hit you? Think of the Buddha when he was still a young prince, and how he saw an old person, a sick person, a dead person. Think of the fear he felt in realizing that all of the areas in which he looked for happiness in life were subject to aging, illness, and death as well.\nThe feeling he felt on realizing that is called samvega, which is sometimes translated as urgency, sometimes as a sense of dismay. But it can also be translated as terror: looking into the abyss and seeing you’re about to fall into it. But the story doesn’t stop there. The fourth person he saw was a forest mendicant. And the feeling he felt on seeing the mendicant was pasada, confidence: If there’s a way out, this is it.\nThis dynamic between terror and confidence informs all of the Buddha’s teachings, all of the Buddha’s practice. Which means that a sense of fear is a legitimate part of the practice. It’s a legitimate motivation for wanting to get your mind to settle down, for wanting to gain some insight into why you are suffering. You realize that if you don’t gain control over your mind, then when aging, illness, and death come, you’ll be at a total loss. At the same time, you have the confidence that if the mind is trained, then you can handle these things and not suffer.\nSo fear is a legitimate reason for coming to the practice. In fact, it’s probably the most legitimate of all. We don’t like the feeling of fear. The experience of fear is very uncomfortable. We feel small, weak, and threatened. This feeling can become unskillful when it gets mixed up with greed, aversion, and delusion. But a clear-sighted sense of fear combined with confidence that there is a way out can actually get you on the path.\nThis combination of fear and confidence is what translates into what the Buddha said is the root of all skillful behavior: heedfulness. You realize that there are dangers, but if you’re careful, you can avoid them. If the dangers were inevitable, there’d be no reason to be heedful, for nothing you might do could make any difference. If there were no dangers at all, there’d be no reason to be heedful, either. But there are dangers in life. And it turns out that the dangers lie not so much in aging, illness, and death, but in the way we think about things. Our greed, aversion, and delusion: These are the dangers. But the care with which we learn how to manage our thoughts, our words, and our deeds provides the way out.\nSo heedfulness reminds us of the dangers but also says, “If you’re careful, if you’re mindful, if you’re alert, if you’re discerning, you can gain release from those dangers.” That’s why we’re here meditating, learning how to train the mind so that it can recognize greed, anger, and delusion when they come. A large part of the problem is that we don’t recognize these qualities for what they are because delusion by definition can’t see itself; often it gets mixed up with the greed and the anger so we don’t recognize them either.\nTo get past that, you have to learn how to observe your own mind to sense what you’re doing that’s skillful, and what you’re doing that’s not. And to do that you have to observe your thoughts to see where they lead: to pleasure or pain. This is something we don’t normally do. We prefer to get involved in a thought world, totally in that world, trying to shape it whatever way we like. Then, for one reason or another, we drop that, move to another one, and then to another one. It’s like hopping trains. If you’ve ever tried to trace the trains of your thought, you know that they’re a lot more complicated than the railroad network here in America. You hop on a train of thought and find yourself in Burma, England, in the middle of Russia, up to the North Pole, down to the South Pole, out to Mars and Saturn, with brief stops along the way when you’re feeling hungry, tired, or hot.\nIt’s back-and-forth all over the place. And when our thoughts are totally out of control like this, no wonder they cause suffering. They can latch onto any object and worry it to death — and worry us to death. Unless the mind is trained, it has very little ability to step back and see what’s going on. You need to learn how to see where your thoughts go. In other words, you step out of the thought and see it as a part of a causal process. This thought leads to that reaction, that reaction leads to that thought, that thought leads to that reaction, and so on. To get out of these trains of thought, you also want to see how each thought gets put together. Why do thoughts arise to begin with?\nWhen you understand these processes, then you can step back and — when you notice that a particular thought is leading toward suffering — you can drop it. You can disband it. The more alert, the more mindful you are, the more quickly you can do this until you get to the point where there’s just a brief stirring of a thought — even before it becomes a coherent thought — and you can zap it. You recognize that it’s going to go off in an unskillful direction and you stop it in its tracks by breathing right through the little knot or bundle of energy around which the thought was about to coalesce.\nThese are some of the skills you develop as you meditate. This is one of the reasons why we start with the breath. We start by thinking about the breath, because if you keep your thoughts concerned with something right here in the present moment, you can start to see the processes of thinking, what’s called fabrication, in action. The breath is called bodily fabrication. It’s what helps to create your sense of the body, the way you feel the body from within. And then you combine that with directed thought and evaluation, which are called verbal fabrication. In other words, you keep directing your thoughts to the breath and then you evaluate it: How does this breath feel? Is it comfortable? If it’s not comfortable, how can you make it better?\nThis brings in the other level of fabrication, which is mental fabrication: feelings and perceptions. Your perceptions are the labels you apply to things. In the case of the breathing process, this has to do with how you perceive what’s going on when you breathe. When you visualize the breathing process to yourself, what is that visualization like? Is it helpful or does it actually cause harm? If you think of the body as a bellows — pulling the breath in, pushing it out — it’s going to make the breathing process tedious, tiresome. If you learn how to perceive the breathing process more as an energy flow, not just the air in and out of the lungs, but the quality of the energy in the body as a whole — from the top of the head down to the face, down to the torso and down to the legs, and down the shoulders and out the arms — then the breathing is more pleasurable. The whole body is involved in this quality of breath, breathing, energy flow.\nThe body is wired in such a way that it can actually pick up energy from within itself, one part feeding another. The energy doesn’t have to come in with the air. In fact, the air coming in and out is simply a byproduct of the energy flow in the body. Try holding that perception in mind and see what it does for the breathing. See which parts of the body’s energy can feed the parts that feel starved. If that gets too complicated, just get back to directing your thoughts to the in-and-out breath, evaluating the in-and-out breath, and leave it at that. But as you get more sensitive to the full process of fabrication, you begin to realize what you’re doing is creating a thought world here that includes all forms of fabrication: breath, which is bodily fabrication; directed thought and evaluation — verbal fabrication; and your feelings and perceptions — mental fabrication. They’re all right here.\nWhen they’re all right here, you’re in a better position to see how thoughts and emotions form, how they disintegrate, where they lead. Because it’s inevitable as you’re trying to focus on the breath that other things will come up. In the beginning you realize this only after they’ve taken you far away. You find yourself on the coast of Norway: “How did I get here?” But in the beginning, don’t try to trace it back just yet. Just say, “Okay, I’ve got to go back to the breath.” And fortunately you don’t have to travel every inch of the way from Norway back here. Just drop Norway and you’re here, back with the breath. With the next thought you’re in Africa. Okay, drop that, and come back to the breath. With the next thought you’re thinking about tomorrow’s meal: Drop that, come back to the breath.\nAn unskillful reaction to all this is to get frustrated. The skillful reaction is to realize that this is what the mind’s been doing all along, so it’s going to take time to change its habits. The important lesson to draw is not to be surprised when the mind wanders off like that. Learn to anticipate it. You realize, “Okay, it’s going to wander off again, so watch for the warning signs.” How does that happen? A sudden curtain falls over the mind and, when the curtain is raised, you’re off someplace else, as in a play. The curtain drops on Act One and when it rises again, you’re in Act Two, off someplace else. How and why does the mind hide these things from itself? And how do you know that it’s about to happen?\nWhen you can anticipate that it’s about to happen, you’ll notice it’s because of a sense of irritation or boredom or antsiness in the mind. Even though you’re standing with the breath, the mind is beginning to look someplace else. When you can catch that happening, remind yourself that it’s a sign the breath isn’t interesting and comfortable enough. Start asking yourself more questions about the breath. How could it be more comfortable? What kind of breathing would feel really, really good, gratifying, refreshing right now? You can ask the different parts of the body. “Hand, what kind of breathing would feel good for you? Left hand, right hand, stomach, legs, chest, abdomen: What kind of breath would you like?” And then let them breathe in whatever way they like.\nThe more interesting the breathing process — the more you can see the good impact it’s having on the body — then the less likely that the mind will wander off. And the more easily it’ll come back. At the same time, you’re learning some important lessons about how the mind creates thought worlds, and how it creates suffering in the process. This way you can learn how not to engage in those processes, developing the skills that will protect the mind from its own worst habits.\nSo as you’re practicing breath meditation like this, you’re doing something concrete about all your realistic fears: If death comes, aging comes, illness comes, if somebody drops you off in the middle of nowhere in the dark, how can you keep your mind under control so you don’t suffer? By doing what you’re practicing right now. You’re giving yourself some concrete skills that can underlie a realistic sense of confidence that you can manage your mind, that you can learn how to train the mind, regardless of the situation. This combination of fear and confidence constitutes the heedfulness that underlies the whole path. You become heedful to try to develop skillful qualities, i.e., qualities of mind that will lead to good results, leading you away from suffering; and to abandon and avoid unskillful qualities, the ones that cause suffering. If you develop your mindfulness, your alertness, your concentration, you can do this.\nSo fear isn’t necessarily a bad thing. It’s an important part of wisdom, recognizing that there are dangers in life. It’s a necessary function of the mind, anticipating that dangers are going to happen. The important thing is not to let the fear get tied up in greed, aversion, or delusion. You want to bring more mindfulness, more clarity to the issues you fear, and to gain more skill in the qualities that will help you avoid those dangers.\nThat’s the important message of the Buddha’s teaching. After he saw the forest mendicant, he became a forest mendicant himself to test and see if the confidence he had placed in that way of living was really well placed. And his awakening proved that it was: It is possible to find a happiness that’s not touched by aging, illness, death, or separation. And as the Buddha said, this realization came not through any special qualities on his part. It came through developing qualities of mind that we all have, that we all can develop, such as ardency, alertness, and resolution, but especially heedfulness: the skillful sort of fear that can get you on the path and see you through to the end.\nSo don’t hate your fears or fear your fears. Learn how to educate them. When they’re educated and trained, they’re part of the path to the end of suffering. This is part of the Buddha’s genius: He took things that many of us don’t like about the mind, things that actually cause trouble in the mind, and learned how to tame them, to train them, so that they actually become part of the path to the end of suffering. In this way, you can reach a place in the mind where there really is no more reason to fear. As Ven. Ananda said, you use desire to come to the end of desire. In the same way, you can use fear, treating it wisely, to bring yourself to the end of fear. And as it turns out, that’s the only way you can get there.', 'As urban populations continue to grow, entrepreneurs are going beyond traditional farming to find new ways to feed everyone while minimising the effect on our land and water resources. Vertical farming is one such method that has been used all around the world. Food crops may be conveniently farmed in urban settings using Vertical Farming by planting in vertically stacked layers to conserve space and require little energy and water for irrigation.\nVertical farming is the process of producing crops in layers that are vertically stacked. Controlled-environment agriculture, which tries to maximise plant development, and soil-less farming techniques such as hydroponics, aquaponics, and aeroponics, are frequently used.\nBuildings, shipping containers, tunnels, and abandoned mine shafts are among popular structures used to host vertical farming systems. There are approximately 30 hectares (74 acres) of functioning vertical farms around the globe as of 2020. Vertical farming, in conjunction with other cutting-edge technology such as customised LED lighting, has resulted in crop yields that are more than ten times greater than those obtained by standard agricultural methods.\nVertical farming is still in its early stages in India, but there are a few entrepreneurs and agri-tech enterprises aiming to revolutionise the area.\nVertical Farming Background and Concept\nGilbert Ellis Bailey originated the phrase “vertical farming” and published a book named “Vertical Farming” in 1915. William Frederick Gerick pioneered hydroponics at the University of California, Berkeley, in the early 1930s.\nke Olsson, a Swedish ecological farmer, devised a spiral-shaped rail system for growing plants in the 1980s and proposed vertical farming as a method of raising vegetables in cities.\nProfessor Dickson Despommier invented the concept of vertical farming in 1999. His idea was to grow food in urban areas, utilising less distance and saving time in transporting food produced in rural regions to cities.\nHe aimed to produce food in urban areas in order to have fresher goods available sooner and at a reduced cost. As a result, vertical farming is defined as the cultivation and production of crops/plants in vertically stacked layers and vertically inclined surfaces.\nThe plants are vertically piled in a tower-like form in the physical arrangement. This reduces the amount of space needed to cultivate plants. Following that, a combination of natural and artificial lighting is employed to ensure an ideal atmosphere for the plants’ effective growth. The third component is the plant’s growth medium. Aeroponic, hydroponic, or aquaponic growth media are employed instead of soil as the growing medium.\nAs the methodology gets more scientific, the process’s efficiency grows, and as a result, vertical farming becomes more sustainable, consuming 95 percent less water than previous agricultural methods.\nAlso Read, Oxagon: The World’s First Floating City in the World\nVertical Farming Techniques\nIt is a method of producing food in water without the use of soil by employing mineral fertiliser solutions.\nThe primary benefit of this strategy is that it lowers soil-related cultivation issues such as soil-borne insects, pests, and illnesses.\nAeroponics was inspired by NASA’s (National Aeronautical and Space Administration, USA) endeavour in the 1990s to develop an effective technique to grow plants in space. There is no growth medium in aeroponics, hence there are no containers for growing crops. Instead of water, mist or nutrient solutions are utilised in aeroponics. Because the plants are attached to a support and the roots are sprayed with nutritional solution, there is very little space, very little water, and no soil required.\nThe name aquaponics is derived from the combination of two words: aquaculture (fish farming) and hydroponics (the process of growing plants without soil in order to develop symbiotic interactions between the plants and the fish). The symbiosis is established by feeding nutrient-rich waste from fish tanks to hydroponic production beds called “fertigate.”\nIn turn, the hydroponic beds act as biofilters, removing gases, acids, and chemicals from the water, such as ammonia, nitrates, and phosphates. Furthermore, the gravel beds serve as a home for nitrifying bacteria, which aid in nutrient cycling and water filtering. As a result, the newly cleansed water may be recirculated back into the fish tanks.\nThe Benefits of Vertical Farming\nVertical farming offers various advantages, making it promising for agriculture’s future. The land need is fairly minimal, water usage is 80% less, water is recycled and stored, pesticides are not used, and in the case of high-tech farms, there is no true reliance on the weather.\nA vertical farm makes farming possible within the constraints of a metropolis. When the farms are close by, the food is delivered swiftly and is always fresh, as opposed to the chilled stuff commonly seen in stores. Transportation reduction minimises the cost of fossil fuels and the accompanying emissions, as well as transportation spoilage. Vertical farming, like anything else, has its limitations. The biggest issue is the initial capital expenses for building the vertical farming system.\nThere are further expenditures associated with building the structures as well as their automation, such as computerised and monitoring systems, remote control systems and software, automated racking and stacking systems, programmable LED lighting systems, temperature control systems, and so on.']	['<urn:uuid:9bca0e5b-d811-4f08-a642-137e820b5866>', '<urn:uuid:38abc93f-c15d-442d-abe3-4bcfbd141419>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T22:03:18.033296	9	81	3440
98	need know profit pattern company launches new product market phase	During the introductory stage, company's profits show a negative figure because of huge costs but low sales volume. This is due to high costs incurred in product development, distribution, and heavy promotion, while there are few customers and low sales volume at this stage.	['Product Life Cycle (PLC)\nA new product passes through set of stages known as product life cycle. Product life cycle applies to both brand and category of products. Its time period vary from product to product. Modern product life cycles are becoming shorter and shorter as products in mature stages are being renewed by market segmentation and product differentiation.\nCompanies always attempt to maximize the profit and revenues over the entire life cycle of a product. In order to achieving the desired level of profit, the introduction of the new product at the proper time is crucial. If new product is appealing to consumer and no stiff competition is out there, company can charge high prices and earn high profits.\nStages of Product Life Cycle\nProduct life cycle comprises four stages:\n- Introduction stage\n- Growth stage\n- Maturity stage\n- Decline stage\nProduct is introduced in the market with intention to build a clear identity and heavy promotion is done for maximum awareness. Before actual offering of the product to customers, product passes through product development, involves prototype and market tests. Companies incur more costs in this phase and also bear additional cost for distribution. On the other hand, there are a few customers at this stage, means low sales volume. So, during introductory stage company’s profits shows a negative figure because of huge cost but low sales volume.\nAt introduction stage, the company core focus is on establishing a market and arising demand for the product. So, the impact on marketing mix is as follows:\nBranding, Quality level and intellectual property and protections are obtained to stimulate consumers for the entire product category. Product is under more consideration, as first impression is the last impression.\nHigh(skim) pricing is used for making high profits with intention to cover initial cost in a short period and low pricing is used to penetrate and gain the market share. company choice of pricing strategy depends on their goals.\nDistribution at this stage is usually selective and scattered.\nAt introductory stage, promotion is done with intention to build brand awareness. Samples/trials are provided that is fruitful in attracting early adopters and potential customers. Promotional programs are more essential in this phase. It is as much important as to produce the product because it positions the product.\nIn this stage, company’s sales and profits starts increasing and competition also begin to increase. The product becomes well recognized at this stage and some of the buyers repeat the purchase patterns. During this stage, firms focus on brand preference and gaining market share. It is market acceptance stage. But due to competition, company invest more in advertisement to convince customers so profits may decline near the end of growth stage.\nAffect on 4 P’s of marketing is as under:\nAlong with maintaining the existing quality, new features and improvements in product quality may be done. All this is done to compete and maintain the market share.\nPrice is maintained or may increase as company gets high demand at low competition or it may be reduced to grasp more customers.\nDistribution becomes more significant with the increase demand and acceptability of product. More channels are added for intensive distribution in order to meet increasing demand. On the other hand resellers start getting interested in the product, so trade discounts are also minimal.\nAt growth stage, promotion is increased. When acceptability of product increases, more efforts are made for brand preference and loyalty.\nAt maturity stage, brand awareness is strong so sale continues to grow but at a declining rate as compared to past. At this stage, there are more competitors with the same products. So, companies defend the market share and extending product life cycle, rather than making the profits, By offering sales promotions to encourage retailer to give more shelf space to the product than that of competitors. At this stage usually loyal customers make purchases.\nMarketing mix decisions include:\nAt maturity stage, companies add features and modify the product in order to compete in market and differentiate the product from competition. At this stage, it is best way to get dominance over competitors and increase market share.\nBecause of intense competition, at maturity stage, price is reduced in order to compete. It attracts the price conscious segment and retain the customers.\nNew channels are added to face intense competition and incentives are offered to retailers to get shelf preference over competitors.\nPromotion is done in order to create product differentiation and loyalty. Incentives are also offered to attract more customers.\nDecline in sales, change in trends and unfavorable economic conditions explains decline stage. At this stage market becomes saturated so sales declines. It may also be due technical obsolescence or customer taste has been changed.\nAt decline stage company has three options:\n- Maintain the product, Reduce cost and finding new uses of product.\n- Harvest the product by reducing marketing cost and continue offering the product to loyal niche until zero profit.\n- Discontinue the product when there’s no profit or a successor is available. Selling out to competitors who want to keep the product.\nAt declining stage, marketing mix decisions depends on company’s strategy. For example, if company want to harvest, the product will remain same and price will be reduced. In case of liquidation, supply will be reduced dramatically.\nLimitations of Product Life Cycle (PLC)\nProduct life cycle is criticized that it has no empirical support and it is not fruitful in special cases. Different products have different properties so their life cycle also vary. It shows that product life cycle is not best tool to predict the sales. Sometimes managerial decisions affect the life of products in this case Product Life Cycle is not playing any role. product life cycle is very fruitful for larger firms and corporations but it is not hundred percent accurate tool to predict the life cycle and sales of products in all the situations.']	['<urn:uuid:aa60928f-ab20-432c-8d85-73d3ea96cb19>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T22:03:18.033296	10	44	988
99	evidence based tobacco programs protection factors	Evidence-based tobacco prevention programs work by targeting both risk and protective factors. They aim to build protection within children by developing social and emotional skills, including goal-setting, decision-making, and peer-pressure refusal abilities. The Too Good program specifically focuses on increasing protective factors such as self-efficacy, positive attitudes, and healthy beliefs, while addressing risk factors like early initiation of problem behavior and lack of commitment to school. This comprehensive approach is showing results in real-world applications, as demonstrated in Kentucky where youth-led efforts have successfully influenced smoke-free policies and contributed to significant decreases in tobacco use rates among students.	"['The Surgeon General of the United States officially declared tobacco as harmful to human health in 1964, and despite declines in tobacco use among Americans in the decades since, there are still approximately 1,300 tobacco-related deaths each day in the United States. In Kentucky, smoking rates remain stubbornly high at around 25 percent compared with 15 percent in the nation overall.\nFor many health officials, the best approach to reducing these rates starts with educating children about the dangers of tobacco use. In the documentary Tobacco-Free Kentucky Kids, leaders in Kentucky’s teen smoking prevention field discuss successful public awareness campaigns that are ongoing around the commonwealth.\nThe program also examines teenagers’ use of smokeless tobacco and e-cigarettes, and interviews high school students who are using the TATU (Teens Against Tobacco Use) platform to educate both younger students and adults about smoke-free policies and tobacco cessation. This program is funded in part by a grant from the Foundation for a Healthy Kentucky.\n“We know that 90 percent of people who start smoking, start before the age of 18, before they’re really even legally able to buy cigarettes” says Audrey Darville, PhD, APRN, a tobacco treatment specialist with University of Kentucky HealthCare. “And, by the age of 25 or 26, nearly everyone has either decided not to smoke, and will not smoke going forward in their life, or become tobacco users.”\nKentucky ranks second behind West Virginia in the teenage smoking rate. Tobacco was one of Kentucky’s primary agricultural crops during the 19th and 20th centuries, and in many parts of the state the plant still exerts a hold on the economy and culture.\nBut more than ever before, younger generations of Kentuckians are turning away from tobacco use due to their understanding of the myriad health problems associated with smoking and chewing or dipping. The efforts to spread awareness of tobacco’s harmful effects are being led by both medical professionals and motivated teen advocates who have seen their elder relatives suffer from debilitating diseases after decades of using tobacco.\nTeen Advocates Making a Difference\n“My grandma has smoked my whole life,” says Katelyn McWhorter, a senior at Lincoln Co. High School. “So, I’ve seen the effects of smoking and how bad it can be and the effects it can cause. And so, I just really want to prevent other people from putting that upon their families and upon themselves, because I’ve seen how bad it can get.”\nMcWhorter and some of her fellow high school students in Lincoln County are members of a program called Teens Against Tobacco Use (TATU). The program was developed by the American Lung Association, the American Cancer Society, and the American Heart Association. TATU enlists local health department officials to first educate teens about tobacco addiction, and then supervise them as they lead meetings with students in middle and elementary schools.\nThe TATU model turns high school students into mentors for younger kids, and instructs the teens on the basics of tobacco use and how nicotine addiction works. TATU organizers in Lincoln County and other school districts feel that anti-tobacco messages to young children are more effective coming from their high school peers, who are often seen as role models.\n“It’s so important to teach them at a younger age,” says youth services coordinator at the school, Mindy Stevens. She notes that in the current media-saturated culture, children are being exposed to tobacco products at an early age compared with previous generations. “So we’re trying to reach them now at even kindergarten levels, before they make those decisions, because if we wait until they are in high school, it’s too late, they’ve already made those choices.”\nIn addition to the TATU counseling groups, high school students around the commonwealth are engaging in public relations campaigns targeted at their own peers and even at adults. The Casey Youth Coalition is one example. Motivated by persistent smoking in the high school bathrooms, the students at Casey County High School created a campaign called Photo Voice. For Photo Voice, the students took photographs of smoking’s toll on their high school campus and community, added captions, and then displayed their collection and also posted it online. The campaign has been well-received, and other school groups in Kentucky and from out of state have inquired about copying the format.\nJacob Steward, a sophomore at Bourbon County High School, recently gave a speech at the launch of Smoke-Free Tomorrow, an advocacy group featuring some of Kentucky’s prominent health care organizations. Steward is a member of a group called Students Making a Community Change (SMACC), and his speech in Frankfort drew rave reviews from health officials and politicians alike.\n“This is as much an educational opportunity as it is really a volunteer or service opportunity, in my opinion,” Steward says, “in improving the lives of Kentuckians as well as learning about the dangers and effects of tobacco and the risks that Big Tobacco poses to teenagers.”\nChanging Kentucky’s Tobacco Culture for the Next Generation\nRecent national television and online public service campaigns such as the “Real Cost” campaign from the Food and Drug Administration and the “Finish It” series of PSAs from the Truth Initiative have inspired health officials and teen groups to produce their own media spots. In western Kentucky, officials with the Barren River District Health Department started a Facebook campaign that promotes Kentucky’s Quit Line (1-800-QUIT-NOW, www.quitnowkentucky.org). Meanwhile, tech-savvy teens in the same area have created their own PSAs for local television. Their message: “We don’t have time for tobacco!”\nIncreasingly, advocates are directing their efforts to curb usage rates for both smokeless tobacco and the relatively new product of electronic cigarettes. Kentucky’s rate of teenage chewing tobacco use is 17 percent, compared with the national average of 8 percent. The habit is one that is often passed down through generations in the commonwealth.\n“Smokeless tobacco use invariably leads to what we call ‘poly-tobacco use,’ which is using more than one tobacco product,” says Donald Helme, PhD, associate professor in the Department of Communications at the University of Kentucky. Helme has recently conducted research into what “is a primary rural phenomenon.”\nSmokeless tobacco users are overwhelmingly male, Helme says, and from his many interviews he’s gained an understanding about how ingrained tobacco chewing is within communities. “These young men would be out working on the farm with their father or their grandfather, and he would offer a piece of chew. And they all described [the offer] as an honor.”\nElectronic cigarettes have become popular among teenagers during this decade, and studies into their health effects are just starting to be published. But according to Elizabeth Hoagland, policy analyst with the tobacco prevention program at the Kentucky Department of Public Health, officials already know enough about “vaping” to be very concerned about how it is being marketed to teenagers.\n“E-cigarettes do contain nicotine, which is an addictive product,” she says. “We know that teenagers who start using e-cigarettes are about 30 percent more likely to start smoking conventional cigarettes, so they could be an initiation product, they could be a starter product.”\nIn November, the new 2017 Kentucky Youth Behavior Risk Survey was released, and its data offers fresh, new evidence that the persistent and dedicated efforts by anti-tobacco advocates are working. The survey showed that among high school students in the commonwealth, cigarette and cigar use decreased from 23.4 percent in 2015 to 18.2 percent in 2017. Cigarette use among middle school students dropped from 22.5 percent in 2015 to 12.1 percent in 2017, and e-cigarette use among middle schoolers dropped from 12.1 to 3.9 percent.\n“Really, we are very encouraged,” Hoagland says. “I think what the numbers show is that tobacco control efforts really do work. I think these new lower tobacco use numbers really do reflect the impact of youth efforts in this state. We see time and again that youth are able to move policy and we know that these policies like smoke-free schools, smoke-free communities, they really do reduce youth tobacco use.\n“If we are able to really double down on these efforts, then we can envision a future that is tobacco-free.”', ""Choosing an Evidence-Based Program (EBP) to address a community need or to add capacity to prevent problem behaviors is an exciting enterprise. The goals of the Exploration Phase of Implementation are to identify a need for change and to select the right EBP to meet that need.\nUse the information on this page to determine if Too Good is the best program fit to meet your needs and if your organization has the capacity to reach and sustain Too Good's intended outcomes.\nToo Good is a comprehensive family of evidence-based substance use and violence prevention interventions designed to mitigate the risk factors linked to problem behaviors and build protection within the child to resist problem behaviors.\nToo Good develops a framework of social and emotional skills through the development of goal-setting, decision-making, emotion management, and effective communication skills in addition to peer-pressure refusal, pro-social bonding, and conflict resolution skills.\nToo Good builds the basis for a safe, supportive, and respectful learning environment.\nSetting and Target Population\nThe Too Good programs are universal prevention interventions that target children and adolescents Grades K-12. The Too Good programs are designed for delivery in a classroom style setting that supports individual, paired, and group learning. Too Good programs align with frameworks for Response to Intervention (RTI) or Positive Behavioral Intervention and Support (PBIS) to aid students with varied needs in whole group, small group, or individualized settings.\nThe programs are successfully implemented by:\nEffective prevention programs like Too Good promote the development of social emotional skills and educate youth about the effects and consequences of risky behaviors like engaging in substance use and violence. Because the delivery of information alone rarely changes behaviors, and because other informative or emotional appeal strategies like didactic lectures, scare tactics, alarming statistics, or infrequent or single event presentations have proven to be ineffective in reducing or deterring risky behaviors in children and youth, Too Good takes a skills-based approach to prepare children and adolescents to make healthy responsible decisions.\nToo Good programs are based in sound development and prevention theory. They use a strengths-based approach that builds on strengths and wellness as part of a strategy to address risk aiming to build protective factors. Too Good programs are interactive and hands-on providing opportunities for skill building and skill application.\nTargeted Risk Factors and Protective Factors\nToo Good targets the following risk factors and protective factors associated with substance use or antisocial behavior in children and adolescents:\nProtective factors targeted for increase\nRisk factors targeted for decrease\nIn random control trials Too Good programs have been shown to reduce substance use and antisocial behavior.\nTGFD Short-Term Outcomes\nTGFD Long-Term Outcomes\nFollowing program implementation, students report that as a result of exposure to TGFD, they intend to abstain from/reduce use of alcohol, tobacco, and marijuana. In schools that have the ability to measure these behaviors, TGFD leads to fewer incidents of ATOD use.\nTGFVSP Short-Term Outcomes\nTGFVSP Long-Term Outcomes\nFollowing program implementation, students report that as a result of exposure to TGFVSP, they intend to resolve their conflicts non-violently. In schools that have the ability to measure these behaviors, TGFVSP leads to fewer incidents of violence.\nToo Good Fidelity Model\nToo Good programs are evidence-based universal interventions that have undergone rigorous evaluation to establish their effectiveness. Once a program has established effectiveness, program implementation determines how likely the intended outcomes are achieved. An intervention delivered with fidelity to the implementation model refers to the degree to which facilitators remain faithful to the program design throughout program delivery.\nThe program design includes the delivery methods, activities and materials, intensity and dosage of the program, and other core components that make the program effective.\nEvaluation instruments to assess the fidelity of your implementation, including instructor performance, and to collect student outcomes data, including knowledge, attitudes, perceptions, and behavior, are provided at every program grade level.\nAdaptations to the Fidelity Model\nAdaptations or modifications to the program design are sometimes necessary for implementation in real-world settings. Proper and minor adaptations that can enhance effectiveness include:\nImproper adaptations can reduce or weaken effectiveness or have a counter effect.\nAdaptations should only be made to enhance effectiveness. Adaptations should not be made for convenience or to suit the implementation style of the facilitator. Adaptation decisions should be made collaboratively with the site Implementation Coordinator or other responsible party and a Mendez Foundation Implementation Adviser.\nSuccessful implementation of an evidence-based program like Too Good requires planning and preparation and trained staff at all phases.""]"	['<urn:uuid:ef23803a-96b0-4166-b55b-7201a50f3515>', '<urn:uuid:c6a71304-1587-4ca3-bfca-d26a6cf624b1>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T22:03:18.033296	6	98	2104
100	chemical exposure prevention personal protection equipment	Chemical exposure prevention involves multiple routes: absorption through skin, ingestion, inhalation, and injection. Protection measures include engineering controls like fume hoods and local exhaust systems, plus mandatory personal protective equipment (PPE) such as safety glasses and chemical-resistant gloves. When handling chemicals, workers must inspect PPE for defects, and facilities must provide access to Material Safety Data Sheets (MSDS) for all chemicals used.	"['While chemical safety includes many individual elements, ranging from right-to-know training, to personal protective equipment (PPE), to material safety data sheets (MSDS), these elements should be part of a larger, overall system. This system\'s aim is to protect all employees from chemical hazards they may encounter at work.\nChemical safety is a system of standard operating procedures that receives, transports, stores, uses and disposes of chemicals in a safe and prudent manner. This process actually begins before a chemical is even ordered, and it doesn\'t end until the substance is properly disposed of.\nThe system is a living system, growing, changing and adapting to the potential dangers of the substances that are being used. It is also a partnership - one between labor and management designed to produce what all parties want: a safe work environment.\nLet\'s look at the some of the operating procedures involved in a chemical safety program.\n1) Prior to orderingBefore a chemical is ordered, the person requesting the material should attempt to substitute a less hazardous substance. In some facilities, prior approval from management and the safety department is needed if the desired chemical is a known or probable human carcinogen, acutely toxic substance, or presents a pronounced physical hazard, such as an explosion.\nAn MSDS for the substance should be on hand before the chemical is received. After reviewing this document you can decide if additional/specialized engineering controls, PPE, storage facilities and spill control measures are needed.\nAlso, having the MSDS prior to receipt of the chemical allows time for employee training in the safe handling and use of the material.\n2) Receipt and transportAll packages containing chemicals, including gas cylinders, must be inspected for leaks and damage before being transported to the assigned storage site. The personnel that inspect these packages should be wearing the appropriate level of PPE: safety glasses and chemical-resistant gloves at a minimum. Things to look for:\n- physical damage: dents, tears, scrapes, dirt, paint/ink from other containers;\n- evidence of tampering;\n- signs of leaks or spills: discoloration, odor, wet packaging, powder coming from the container.\nIf there are any indications of a spill or leak, evacuate the immediate area and call for aid.\nIf there are no signs of damage or leaks, the material may be transported to its assigned storage area. If the material is a particularly hazardous substance, notify the safety department and the person requesting the material. If a chemical requires refrigeration or freezing, and transport to the designated storage site is delayed, make arrangements for proper preservation of the compound.\nDo not hand carry chemical packages. Packages containing chemicals should be transported to the appropriate storage site on a cart that has raised sides. If the package has been opened, place the substance in a secondary containment vessel and then on the cart.\n3) Storage/inventoryChemicals are stored according to properties. Acids are stored separate from bases, flammables are stored in special cabinets, oxidizers are stored away from organics, and so on. After storing according to chemical properties, then and only then may chemicals be sorted alphabetically.\nRefrigerators/freezers used for chemical storage must be labeled to that effect. Food or beverages must be kept out unless they are being used for testing; they should bear a prominent label that states they are not for human consumption. Refrigerators/freezers being used for storing flammable chemicals should be explosion-proof.\nThe chemical inventory in a stock room or other storage area must be inspected at least once a year. During these inspections, examine each bottle to see if the chemical is still safe to use and that the container hasn\'t been breached. (See ""Warning Signs"" sidebar.)\nCertain classes of chemicals must be checked more frequently. Peroxide-forming compounds that have been opened should be checked every three months at a minimum. Visual inspection of the container and contents using a non-hazardous light, such as a flashlight, can detect the presence of very high levels of peroxides in organic solvents stored in glass bottles. Check to see if the clear solvents contain: suspended wisp-like structures, crystal formation, cloudiness or gross contamination. Contact your safety department or hazardous waste contractor immediately if they do.\nIf none of these conditions exist, the solvent may be tested using a commercially available test strip. These strips can measure up to 100 mg/L peroxide as H2O2. Some experts recommend that this concentration be used as the maximum safe concentration and that solutions containing higher values than this be disposed of.\n4) Point of useWherever chemicals are used, measures must be taken to reduce personnel exposure. Among these measures are engineering controls, including: fume hoods, glove boxes, local exhaust systems and robotic equipment.\nThese systems need to be inspected and tested on a regular basis, with the date of the last inspection being posted on the device. When working with a particularly hazardous substance, have the device tested immediately prior to use.\nAnother safety requirement is the posting of appropriate safety signage. Additionally, in areas where carcinogens, reproductive toxins, and chemicals of high toxicity are used, access to the area should be limited to only those people who need to be there.\n5) DisposalChemical disposal must conform to the local hazardous waste regulations. Most regulations require secondary containment and storage in a secure area. Only authorized personnel should be allowed into this part of the facility. Spill equipment should be stored outside the secured area to allow easy and safe access in case of an emergency.\nAn empty container is not truly empty. Until it has been cleaned, there will always be chemical residue inside it. PPE should always be worn when handling these ""empties"" until decontamination/cleaning has taken place.\nSIDEBAR: Warning SignsWhat to look for when inspecting your chemical inventory:\n- Is the label discolored? Something in the cabinet may have leaked.\n- Are there crystals around the cap? For some chemicals, such as potassium chloride solutions, this is not abnormal.\n- Is the container or cap bulging?\n- Is the cap cracked?\n- Does the container have a ""sucked in"" look or show signs of corrosion?\n- Is there a film on the container?\n- Is the bottle warm to the touch?\n- Is there a precipitate in an otherwise clear reagent?\n- Has the reagent turned color? This is normal in the case of open nitric acid bottles but a danger sign for most other reagents.\n- Is there obvious contamination, such as discoloration or specks in the substance?\n- Is the chemical past the expiration date assigned by the manufacturer?\n- Is the substance giving off a strange odor? If so, initiate a spill response.', ""Right to Know & Blood Borne Pathogens\nPlease read and understand the follow information concerning chemical hazards and blood borne pathogen safety. When you are finished you must complete the training by submitting answers to our online quiz. The information you submit will only be accessible by the program administrator in the Health & Safety office at the Jefferson-Lewis BOCES and your school's training administrator.What is the Hazard Communication Standard and the Right-To-Know Law and why are they important?\nRight To Know\nFirst, the Hazard Communication Standard CFR 1910.1200 is a Federal O.S.H.A. standard that\nprovides employees with access to information on chemical hazards and\nprovides that employers provide training to minimize employee exposure\nto potentially hazardous chemicals and injury from these chemicals.\nUnder the federal standard the employer must:\nSecond, the Right To Know law is a New York State Law that also protects workers from potentially hazardous chemicals in the workplace.\nUnder the New York State law the employer must:\nYour Responsibilities - As an Employee...\nIt is Also Your Responsibility to...\n*Right-To-Know regulations are enforced by the NYS DOL (New York State - Department of Labor) Public Employee Safety & Health (PESH) unit.\nPhysical & Health Hazards\nThere are six main chemical hazard areas\nFlammables & Combustibles - Gasoline, spray cans, paper products\nAcute Exposure - Effects after the exposure occur immediately (minutes or hours)\nChronic Exposure - Effects from the exposure may take years or longer to appear\nRoutes of Chemical Exposure\nChemicals can get into your body in four different ways.\nAbsorption - This exposure from a chemical is when it seeps through your skin and causes a reaction of some kind.\nIngestion - This exposure to a chemical can be from drinking it.\nInhalation - The chemical gets into your lungs by breathing the chemical in as a gas or a vapor.\nInjection - The chemical gets into your system by force like a needle or stepping on a nail.\nLabels and Material Safety Data Sheets\nThe OSHA Hazard Communication Standard requires that ALL hazardous materials be labeled, identifying the material and warning employees of the potential hazards.\nMany chemical products have the NFPA Diamond on its label.\nNFPA Diamond = Four Hazard Classes\nHealth = Blue\nFlammability = Red\nReactivity = Yellow\nSpecial Hazards = White\n*There are numbers inside of these diamonds (1,2,3,4).\n*Remember - The greater the number, the higher the hazard.\n*Check with your facility if you are unaware or unsure of where your MSDS’s are located.\nschools are enrolled in the MSDS Online program. This program allows\nanyone who needs an MSDS sheet for a chemical to get one by fax. They\ncan be reached 24 hours a day, 7 days a week, 365 days a year. It is\nrecommended that you should have information about the chemical before\nyou call. This information can include: Product Name & Number,\nManufacturer Name, Manufacturer Phone Number, and UPC Code.\nEnd of RTK\nBlood Borne Pathogens\nWhat is the Blood Borne Standard and why is it important?\nThe Blood Borne Standard (29 CFR 1910.1030) is an O.S.H.A. federal law that requires employer must:\nBlood Borne Pathogens are pathogenic micro-organisms that can be present in human blood and can cause disease in humans.\nFacts about Viruses and Bacteria\nIts estimated that 1 out of 20 people in the US are infected with some type of Hepatitis and do not realize it.\nThere are various types of Hepatitis (Hepatitis A - G).\nThis training covers Hepatitis A - C.\nSymptoms of Hepatitis\nincludes - Vaccination, Hand washing, 10:1 bleach solution on\ncontaminated surfaces, or minimize exposure with PPE & proper food\nHepatitis B - Is one of the most infectious blood borne pathogen known. It causes liver inflammation and can lead to conditions like cirrhosis and liver cancer.\nIt is transmitted\nby blood transfusions and organ donations before 1992, being born to an\ninfected mother, intravenous drug use and sexual contact.\nVaccine - None available. Medical prescriptions can include Interferon Alpha and Ribvirin.\n*80% of people have no symptoms for Hepatitis C and if the person does exhibit symptoms, it feels like the flu.\nHIV - Human Immune Deficiency Virus / AIDS - (Acquired Immune Deficiency Syndrome)\nHIV (Human Immune Deficiency Virus) – An infection that progresses in stages and leads to AIDS (Acquired Immune Deficiency Syndrome). The AIDS virus attacks the body’s immune system and ultimately causes death.\nIt is transmitted by contaminated blood, blood-to-blood or blood-to-mucus membrane contact, contaminated needles, and sexual contact. Infected blood used in transfusions or infected organs donated before March 1985.\nThere is No Cure for this virus, but medications can slow the progress of it.\n*It is important to remember that symptoms can develop from 2-15 years after initial infection.\nWork practice controls to keep you safe from Blood Borne Pathogens\nProper Hand-Washing Techniques:\nWays to Control the Spread of Disease\nWear Personal Protective Equipment (PPE)\nAlways wear gloves to address open wounds, to clean blood and/or other bodily fluids.\ngloves to make sure that there are no holes or rips before using them.\nIf you find that the gloves are defective, throw them away.\nIf you become Exposed to Blood or Body Fluids :\nEnd of BBP\nClick Here to take the required Right To Know / Blood Borne Pathogens quiz.""]"	['<urn:uuid:c976c5fe-ee49-41e5-b956-4c093cf7f0aa>', '<urn:uuid:41d8f7d9-c49f-4b6c-9984-0cfdafb1985e>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T22:03:18.033296	6	62	1988
