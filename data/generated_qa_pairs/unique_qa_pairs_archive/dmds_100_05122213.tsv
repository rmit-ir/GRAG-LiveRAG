qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	Could you explain what happened with the Dutch people who moved to Indonesia during colonial times and what became of their descendants?	During colonial times, Dutch soldiers often intermarried with local Indonesian women and settled in the colonies. Over the centuries, this led to the development of a relatively large Dutch-speaking population of mixed Dutch and Indonesian descent, known as Indos or Dutch-Indonesians. However, following the Indonesian Revolt, there was an expulsion of Dutch people from Indonesia, resulting in most of this mixed-heritage group now living in the Netherlands.	['You may also find older native audio system in Australia, New Zealand, the U.S. and Canada as many Dutch individuals migrated to those nations within the 1950’s. Since World War II, Dutch emigrants have primarily departed the Netherlands for Canada, the Federal Republic of Germany, the United States, Belgium, Australia, and South Africa, in that order. Today, massive Dutch communities additionally exist in the United Kingdom, France, Spain, Turkey, and New Zealand. In the early sixteenth century, the Protestant Reformation began to kind and shortly spread within the Westhoek and the County of Flanders, where secret open-air sermons had been held, referred to as hagenpreken (“hedgerow orations”) in Dutch. The ruler of the Dutch areas, Philip II of Spain, felt it was his obligation to struggle Protestantism and, after the wave of iconoclasm, sent troops to crush the rebellion and make the Low Countries a Catholic region as soon as more.\nRabbi Binyomin Jacobs, President of the Dutch Association of Rabbis, mentioned only about 50 male Jewish infants are circumcised in the Netherlands every year. AMSTERDAM (Reuters) – Dutch docs need politicians and human rights groups to talk out and discourage the apply of male circumcision in the Netherlands as a result of they say it is a “painful and harmful ritual,” and a violation of children’s rights. People of the Netherlands name their country “Nederland”, their language “Nederlands”, and themselves “Nederlanders”.\nThe majority died of disease or made their means back to Europe, however a few of them made the Indies their new residence. Interaction between the Dutch and native population primarily took place in Sri Lanka and the modern Indonesian Islands. Most of the time Dutch soldiers intermarried with native women and settled down in the colonies. Through the centuries there developed a relatively large Dutch-talking population of combined Dutch and Indonesian descent, often known as Indos or Dutch-Indonesians. The expulsion of Dutchmen following the Indonesian Revolt, signifies that currently nearly all of this group lives in the Netherlands.\nWhen you propose a visit to Holland, you really want to enter an artwork museum.\nFrom Amsterdam to Zeeland… and beyond\nThe provinces of North Brabant and Limburg have historically been strongly Roman Catholic, and a few of their folks might nonetheless contemplate the Catholic Church as a base for their cultural identification. Protestantism within the Netherlands consists of a variety of church buildings within varied traditions. The largest of these is the Protestant Church in the Netherlands (PKN), a United church which is Reformed and Lutheran in orientation. It was shaped in 2004 as a merger of the Dutch Reformed Church, the Reformed Churches in the Netherlands and a smaller Lutheran Church.\nCookies on Iamsterdam.com\nDutch is a nationwide language in the Netherlands, Belgium, Suriname in South America and the Dutch Antilles. In Belgium, it’s the official language of Flanders, the Northern area of the country, and can be spoken in Brussels, though the majority of the town’s population communicate French. In Suriname and the Dutch Antilles, Dutch remains to be an official language, but a number of different languages are spoken there too. They probably continued to use their time period of “Black Dutch” to check with swarthy-skinned people or, extra usually, political opponents.\nCycling holidays previous the icons of Holland\nThis comes from the fact that the Netherlands were tolerant of other nationalities and other religions for the reason holland girls that beginning of the Dutch Republic (in 1588). In the 17th century, it was not odd, for example for Muslims to reside in Amsterdam.\nDutch has also turn out to be extra important in tourism, especially within the Ardennes. The Flemish and Dutch are more and more serviced at campsites, motels and attractions in their own language by Dutch-talking staff. There are additionally extra Flemish families now living across the language border because it is cheaper to build, purchase or rent. These developments don’t but result in authorized services for Dutch speakers. At that point, the proportions were 24.24% Dutch-speaking and 70.sixty one% French-talking.\nIn the Netherlands, present-bringing at Christmas has in latest many years gained some recognition too, though Sinterklaas is rather more popular.. TV exports from the Netherlands mostly take the type of specific formats and franchises, most notably through internationally lively TV manufacturing conglomerate Endemol, based by Dutch media tycoons John de Mol and Joop van den Ende. Headquartered in Amsterdam, Endemol has round ninety companies in over 30 international locations. Endemol and its subsidiaries create and run actuality, expertise, and recreation present franchises worldwide, including Big Brother, Deal or No Deal, 1 vs. one hundred and The Voice. During the twentieth century Dutch architects played a leading position within the growth of contemporary architecture.\nSeveral diversified groups of multiracial people have typically been known as or recognized as Black Dutch, most often as a reference to their ancestors. 10 May. 2009;All the towns shaped a brand new, non-feudal factor within the present social construction, and from the beginning merchants performed an essential role. The retailers usually formed guilds, organizations that grew out of service provider groups and banded together for mutual safety whereas traveling during this violent interval, when attacks on service provider caravans were common.\nIn 1898 Dutch was lastly granted official standing, but as change was sluggish, it was determined in 1963 to make it the only language allowed in Flemish schooling and public life. Nowadays, there are 6 million audio system of Dutch in Flanders The southern a part of Belgium, Wallonia, remained entirely French-speaking and the capital, Brussels, is bilingual. Dutch is a member of the West Germanic family tree, and as such, is a cousin of English and German and a sibling to Afrikaans.']	['<urn:uuid:91233f6f-491e-4b2d-a01d-986668cb8cdb>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T22:13:20.996306	22	67	948
2	What are the main differences between how factories treat workers today versus in the 1800s, and what modern approaches are used to handle workplace disagreements?	In the 1800s during the Industrial Revolution, labor legislation favored employers with practices like hire-and-fire policies, resulting in low wages, unhygienic living conditions, lack of security, and poor conditions for migrant workers. This created conflicts between management and workers. Today, it's recognized that workers must be treated humanely and that sustainable development requires considering workers' interests. Modern workplace conflict resolution involves various processes including cognitive resolution (analyzing and understanding the conflict), emotional resolution (addressing feelings about the conflict), and behavioral resolution (addressing actions and behaviors). Contemporary approaches emphasize peaceful resolution through negotiation, mediation, and collective bargaining, with a focus on maintaining positive relationships between management and workers.	['“Labour” is a subject in the “Concurrent List” under the Constitution of India where both the Central and State Governments are competent to enact legislations subject, however, to reservation of certain matters for the Central Government.\n1.The advent of Industry & the beginning of Industrial Relation.\n- Concept of Industrial relation & Labour Laws.\n- Organised & Unorganised Sector\n- Objectives of Industrial Relation & Labour Laws\n- Different approaches of Industrial Relation\n- Labour – Definition & Concepts\n- Labour welfare & its theory\n- History of Industrial relation in India\n- Pre-independence and post-independence Laws\n- Acts applicable to Organised and Unorganised Sector\n- Constitutional provisions related to industrial relation & Labour laws\n- Trade union & Collective bargaining\n- Labour code 2019 & 2020\n- Factory act 1948\n- Contract Labour Regulation & Prohibition act 1970\n- Payment of wages Act 1936\n- Minimum wages act 1948\n- Payment of Bonus act 1965\n- Equal remuneration act 1976\n- National Commission on Labour\n- International Labour Organisation.\n- Importantly asked questions\nSHORTS NOTES OF INDUSTRIAL RELATION AND LABOUR LAWS\nThe advent of Industry & the beginning of\nIndustrial Revolution had begun in 1800 & the importance of Industrialisation emerged out.\nBecause of Rapid industrialisation, Industry and Labour legislation were inclined towards the benefits of the employer. Labour legislation was governed by the doctrine of Laissez-faire, hire & fire policy and Demand & Supply. This result in low wages, non-hygienic habitat, lack of security & poor condition of migrant labourers. This led to a conflict between management & workers. That hampered the growth of a nation.\nFinally, few points had been realised by the world mention below –\n- None of the Nations can have sustainable development without the interest of workers.\n- Workers are the human and they must be treated like humane.\n- Labour is actually a human resource and we must have to utilize it wisely.\n- There must be a harmonious relationship between management and employee for consistent economic growth.\n- And thus, a peaceful relation established between the management and workers for the overall growth of a nation called “Industrial Relation”. However, their contradictory nature is the reason for the enforcement of some government laws that make this relation harmonious and these laws are known as Labour laws.\nRelation between two Societies employer/Owner/Entrepreneur/Contractors/Management and employee/workers/Labourers/contract-labourers. It also involved in organising the workers, its participation in decision making, dispute settlement, collective Bargaining etc. It also focuses on the improvement of the economic conditions of workers.\nNow, this industrial relation is managed by some rules and regulation called labour or employment or industrial relation. The aim of these laws is to protect workers against their exploitation, social justice.\nOrganised and Unorganised Sector….\n|ORGANISED SECTOR||UNORGANISED SECTOR|\n|Regulated by Government||Not regulated by Government|\n|Employees have assured work||Don’t have assured work|\n|Term/Tenure are fixed and regular||Neither fixed nor regular|\n|Follows Government rules and regulations||Doesn’t follow Government rules and regulations|\nObjectives of Industrial Relation & Labour laws…..\n- Enhance the economic status of the workers\n- Regulate production by minimizing industrial conflict\n- Socialize industries\n- Provide an opportunity for the workers\n- Improve workers strength\n- Avoid industrial conflict between labour and management\n- Maintain industrial democracy\n- To improve industrial relation\n- Minimize the industrial disputes\n- Protect workers from exploitation\n- Help workers in getting fair wages\n- Minimise the labour unrest and reduce conflict and strike\n- Ensuer jobs security for workers\nDifferent Approaches of Industrial Relation…..\nBased on the proposition that ” There is only one source of authority and this is the management which owns and controls decision, negotiation and bargaining. Under the unitary approach, industrial relations are grounded in mutual co-operation, individual treatment, team works and shared goals. Workplace conflict is seen as a temporary aberration. Unions cooperate with the management and accept the override power of management. Emphasize reactive industrial relation and its strategy is based on direct negotiations with employers. Participation of government, tribunals and union is not sought or is seen as being necessary for achieving harmonious employee relations. Criticism- seducing employees away from unionism and socialism, manipulation and exploitation.\nBased on the assumption that the organisation is composed of individuals from distinct groups with their own set of aims, objectives, leadership styles and value propositions. It considers the conflict between management and employees are rational and inevitable. The role of management is to mediate amongst the different interest groups. Trade unions as legitimate representatives of employee interests. Based on the assumptions that the organisation is composed of individuals from distinct groups with their own set of aims, objectives, leadership styles and value propositions. It considers the conflict between management and employees are rational and inevitable. The role of management is to mediate amongst the different interest groups.\nMARXIST OR RADICAL PERSPECTIVE / APPROACHES…..\nIt is based on the proposition that the economic activities of production, manufacturing and distribution are mainly governed by the objective of profit. It also regards that conflict between employer and employees as inevitable. Conflict arises not only because of competing interests within the organization but also because of the division within society between those who own or manage the means of production and those who have only their labour to offer. Industrial conflicts are thus seen as being synonymous with political and social unrest.\nAccording to this approach differences in the perceptions of employers and workers give rise to problems of employer-employee relations. Employer and employee consider each other less appreciative of the others’ position and less dependable than himself. The perceptions in addition dissatisfaction with pay, working condition, nature of the jobs etc. Causes frustration and aggression on the part of workers. These in turn lead to strikes, Gherao, Boycott and Works to rule, etc. Similarly, employers resort to lockout and others forms of protest on account of frustration over market conditions, Government policies and other constraints.\nThe industry is part of society and it is a community made up of individual and groups with different family background, educational level, personalities, emotions, likes and dislikes, etc. These differences in individual attitudes and behaviour create problems of conflict and cooperation in the industry. The value systems, customs, status symbols and institutions of the society in which industry functions affect relations between the parties involved. Urbanisation, housing and transport problems in industrial areas, the disintegration of the joint family system, and other social problems cause stress and strains along with workers. The social and cultural changes shape behaviour patterns and cause an adjustment in employer and employee relations. There cannot be harmony and peace in the industry when society is in turmoil.\nHUMAN RELATIONS APPROACH….\nThe industry consists of living human beings who want freedom of thought and expression and control over their lives. When employers treat workers as inanimate objects and encroach upon their interests and desires, conflict and dispute arise.\nWorkers want the security of service, good pay and working conditions, recognition for a job well done, opportunity to participate in decision-making. Employers must understand the needs, attitudes and aspiration of workers.\nThe human relation approach explains the behaviour of individuals and groups at works and helps in modifying or utilizing such behaviour towards the achievement of organisational objectives. If management and labour both understand and apply the human relation approach to their mutual relations industrials conflict can be minimised. The human relation approach is inter-disciplinary in nature because knowledge drawn from several disciplines likes psychology, sociology, anthropology, economics and political sciences is used in it.\nAccording to Shri V.V.Giri, collective bargainings and mutual negotiation between management and labour should be used to settle industrial disputes. He suggested that there should be bipartite machinery in every industry and every unit of the industry to settle difference from time to time with the active encouragement of the government.\nIn this approach employer-employee relations is based on the fundamental principles of truth, non-violence and non-possession. If the employers follow the principle of trusteeship, there is no scope of conflict of interests between them and labour.\nGandhi ji accepted the worker’s right to go on strike but they should exercise this right in a peaceful and non-violent manner. workers should resort to strike for just cause and after the employers fail to respond to their moral appeals.', 'Meaning Of Conflict Resolution\nConflict resolution is a conceptualize soft skill because of the methods and processes involved in facilitating the peaceful ending of conflict and retribution. Dedicated group members always try their best to solve group conflicts. They do so by actively communicating, sharing information about their conflicting motives to the remainder of the group (e.g., intentions and reasons for holding certain beliefs), and engaging in collective negotiation. Dimensions of resolution typically parallel the size of conflict within the way the conflict is processed.\n1. Cognitive Resolution\nCognitive resolution is the way in which disputants understand analyze and examine the conflict, with beliefs, perspectives, understandings, and attitudes.\n2. Emotional Resolution\nEmotional resolution is within the way disputants feel a couple of conflicts, the emotional energy.\n3. Behavioral Resolution\nBehavioral resolution is reflective of how the disputants act, their behavior.\nUltimately a large range of methods and procedures for addressing conflict exist, including negotiation, mediation, mediation-arbitration, diplomacy, and artistic peacebuilding.\nThe term conflict resolution can be used interchangeably with the term ‘dispute resolution’, where arbitration and litigation processes are critically involved. The concept of conflict resolution is often thought to encompass the employment of nonviolent measures by conflicted parties in a trial to push the effective resolution.\nTheories And Model Of Conflict Resolution\nThere is a plethora of various theories and models that link to the concept of conflict resolution.\n1. Conflict Resolution Curve\nThere are many samples of conflict resolution in history, and there has been a debate about the ways to conflict resolution whether it should be forced or peaceful. Conflict resolution by peaceful means usually appears to be a higher option. The conflict resolution curve derived from an analytical model that provides a calm solution by motivating and appreciating conflicting entities. Forced resolution of conflict might invoke another conflict within the future.\nThe conflict resolution curve (CRC) separates conflict styles into two separate domains: the domain of competing entities and the domain of accommodating entities. There’s a kind of agreement between targets and aggressors on this curve. Their judgments of badness compared to goodness of every other are analogous on CRC. So, the arrival of all conflicting entities to some negotiable points on CRC is vital before peacebuilding. CRC doesn’t exist (i.e., singular) essentially if the prospect of aggression if the aggressor is definite. Under such circumstances, it would cause apocalypse with mutual destruction.\nThe curve explains why nonviolent struggles ultimately toppled repressive regimes from power and sometimes forced leaders to alter the character of governance. Also, this technique has been applied to capture the conflict styles on the Korean Peninsula and the dynamics of negotiation processes.\n2. Dual Concern Model\nAccording to the dual concern model, group members balance their concern for satisfying personal needs and interests with their concern for satisfying the wants and interests of others in numerous ways. The intersection of those two dimensions ultimately leads individuals to exhibit different types of conflict resolution. The twin model identifies five with number four being the target to finish the cycle and illuminate the difficulty at hand. Conflict resolution styles or strategies that individuals may use depend upon their dispositions toward pro-self or pro-social goals.\n3. Avoidance Conflict Style\nCharacterized by joking, changing or avoiding the subject, or maybe denying that an issue exists, a strong dislike for following the foundations the conflict avoidance style is employed when a person has withdrawn in addressing the opposite party, when one is uncomfortable with conflict, or because of cultural contexts. During the conflict, these avoiders adopt a “wait and see” attitude, often allowing conflict to end on its own with none personal involvement. By neglecting to deal with high-conflict situations, avoiders risk allowing problems to fester or spin of control.\n4. Yielding Conflict Style\nIn contrast, yielding, “obliging”, smoothing or suppression conflict styles are enhance by a high level of concern for others and an occasional level of concern for oneself. This pro-social passive approach comes forth when individuals derive personal satisfaction and happiness from meeting the requirements of others and have a general concern for maintaining stable, positive social relationships. When faced with conflict, a person with a yielding conflict style tends to harmonize into others’ demands out of respect for the social relationship.\n5. Competitive Conflict Style\nThe competitive, “fighting” or pressurizing conflict way or style increases to maximum individual assertiveness (i.e., concern for self) and minimizes empathy (i.e., concern for others). Groups consisting of competitive members usually enjoy seeking domination over others. Fighters tend to force others to easily accept their personal views by employing competitive power tactics (arguments, insults, accusations, or maybe violence) that foster intimidation.\n6. Conciliation Conflict Style\nThe conciliation conflict style is typical of people who possess an intermediate level of concern for both personal and others’ outcomes. Compromisers value business and, in doing so, anticipate agreed give-and-take interactions. This conflict style is often consider as an extension of both “yielding” and “cooperative” strategies.\n7. Cooperation Conflict Style\nCharacterized by an energetic concern for both pro-social and pro-self behaviour, the cooperation, integration, confrontation, or problem-solving conflict style is usually used when a private has elevated interests within their own outcomes moreover as in the outcomes of others. During the conflict, cooperators collaborate with others in an attempt to seek out an amicable solution that satisfies all parties involved in the conflict. Individuals using this sort of conflict style tend to be both highly assertive and highly empathetic. By seeing conflict as an ingenious opportunity, collaborators willingly invest time and resources into finding a “win-win” solution. In line with the literature on conflict resolution, a cooperative conflict resolution style is usually recommended specifically others. This resolution could also be achieved by lowering the aggressor’s guard while raising the ego.\n8. Relational Dialectics Theory (RDT)\nRDT, introduced by Leslie Baxter and Barbara Matgomery (1988), explores the ways within which people in relationships use verbal communication to manage conflict and contradiction as critical psychology. This idea focuses on maintaining a relationship even through contradictions that arise and the way relationships are managed through coordinated talk. RDT assumes that relationships are composed of opposing tendencies, are constantly changing, and tensions arise from intimate relationships.\nThe concept is that the contrary has the characteristics of its opposite. People can seek to be during a relationship but still need their space.\nThe totality comes when the opposites unite. Thus, the link is balanced with contradictions and only then it reaches totality\nComprehended through various social processes. These processes simultaneously continue within a relationship in an exceedingly recurring manner.\nProcess Of Conflict Resolution\nThe resolution of conflicts in the workplace typically involves some of the following processes:-\n- Recognition by the parties involved that an issue exists.\n- Mutual agreement to handle the problem and find some resolution.\n- An effort to grasp the angle and concerns of the opposing individual or group.\n- Identifying changes in attitude, behavior, and approaches to figure by each side that may lessen negative feelings.\n- Recognizing triggers to episodes of conflict.\n- Interventions by third parties like Human Resources representatives or higher-level managers to mediate.\n- A willingness by one or both parties to compromise.\n- Agreement on a thought to deal with differences.\n- Monitoring the impact of any agreements for change.\n- Disciplining or terminating employees who resist efforts to defuse conflicts\nConflict Resolution In The Workplace\nAccording to the Cambridge dictionary, an awfully basic definition of CONFLICT is full of life disagreement between people with opposing opinions or principles. Conflicts like disagreements may occur at any moment, being a standard a part of human interactions.\nThe sort of conflict and its severity may vary both in content and degree of seriousness; however, it’s impossible to completely avoid it. Actually, conflict in itself isn’t necessarily a negative thing. When handled constructively it can help people to face up for themselves to evolve and find out how to figure together to realize a mutually satisfactory solution. But if the conflict is handled poorly it can cause anger, hurt, divisiveness, and more serious problems.\nConflict can be avoided?\nIf it’s impossible to completely avoid conflict because it was said, the probabilities to experience it are usually higher particularly in complex social contexts within which important diversities are at stake. Especially due to this reason, speaking about conflict resolution becomes fundamental in ethnically diverse and multicultural work environments, within which not only “regular” work disagreements may occur but within which also different languages, worldviews, lifestyles, and ultimately value differences may diverge.\nConflict Resolution is a process by which two or more parties engaged during a disagreement and reach an agreement solving it. It involves a series of stages, involved actors, models, and approaches that will rely upon the sort of confrontation at stake and therefore the surrounded social and cultural context. However, there are some general actions and private skills that will be very useful when facing a conflict to unravel (independently of its nature)\nExample: an open-minded orientation able to analyze the various point of view and perspectives involved, in addition to the capability to empathize, carefully listen, and clearly communicate with all the parts involved.\nSources of Conflict in Workplace\nSources of conflict are also numerous, counting on the actual situation and also the specific context, but a number of the foremost common include:\nPersonal differences like values, ethics, personalities, age, education, gender, socioeconomic status, cultural background, temperament, health, religion, dogmas, etc. Thus, any social category that serves different people may become an object of conflict when it does negatively diverge with people that don’t share clashes of ideas, choices, or actions.\nConflict occurs when people don’t share common goals or common ways to achieve a specific objective (e.g. different work styles). Conflict occurs also when there’s direct or indirect competition between people or when someone may feel excluded from a specific activity or by some people within the corporate. Lack of communication or poor communication is a significant reason to start out a conflict, to misunderstand a specific situation, and to form potentially explosive interactions.\nFrequently Asked Questions-\nAns. a) Avoiding: Someone who uses a strategy of “avoiding” mostly tries to ignore or sidestep the conflict, hoping it will resolve itself or dissipate.\nAns. Having good conflict resolution skills help employees to reduce the friction that may damage their working relationships. By resolving conflicts in a very more professional and respectful manner, they’re enabled to collaborate better with co-workers, building stronger work relationships. Reduced disruptions.\nAns. a) Communication: Employees must understand how to speak to every other so as to grasp the source of conflict, additionally as alternate points of view but communicating well involves over talking and getting your point across to others.\nc) Stress Management\nd) Emotional Agility\nAns. There are two forms of conflict in literature:\nInternal conflict is within the character’s mind. Internal conflict is described as a struggle between opposing forces of desire or emotions within someone.\nExternal may be a conflict between a personality and an out of doors forces.\nAns. a) Poor communication: Communication relies on clear and complete messages being sent moreover as being received.\nc) Lack of designing\nd) Poor staff selection\ne) Frustration or stress']	['<urn:uuid:c9835956-4847-446f-932e-43d863d88553>', '<urn:uuid:f46a958b-caa2-4889-b83f-86b15542b574>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T22:13:20.996306	25	107	3248
3	How long can tap water be safely stored in the refrigerator?	When water goes directly from tap to fridge, it can be kept safely for up to 10 or 15 days. The refrigerator slows down both chlorine loss and microorganism reproduction.	['It seems totally harmless when you put tap water in your bottle, store it in the fridge (or on top of the kitchen bench, unrefrigerated), and you have the feeling that it will never go bad. You go on vacation and you don’t even think about emptying the water jugs. You leave a glass on the nightstand or office desk and drink it the next day. But be careful, because although it is simply water, it is not eternal.\nTap water is not sterile, of course. Its conservation depends on several conditions. As per the indicative data, you should not keep it out of the fridge for more than one day in summer, and approximately one week in winter.\nTap water and chlorine\nTap water contains chlorine to make it drinkable, a disinfectant that prevents microbiological growth. But that chlorine evaporates and, when it is lost, there is again a risk of contamination with microorganisms.\nOne of the factors on which the conservation of tap water depends is the quality of the water in each area, and the quality of the water treatment plant in each area.\nThe refrigerator causes the loss of chlorine (disinfectant) from the water to be slow and the reproduction of microorganisms to slow down as well, which is why tap water always lasts longer in the refrigerator.\nIn case, if it goes from the tap directly to the fridge, you can keep it for up to 10 or 15 days without problems. The same thing happens with mineral water, once opened, it has the same conservation period as that of the public network.\nThe temperature factor means that in summer there are many more chances of water contamination, although it is not usual. The sources from which the water is collected are at a lower level and the microbes are more concentrated.\nOf course, when the water comes out of the treatment plant it doesn’t have to carry any risk, but the pipes can become contaminated, and in summer it is easier for this to happen, although it is not frequent.\nEffects of plastic bottles\nThe plastic bottles which you buy to drink water at the supermarket are not designed for reuse because the chemicals in the plastic can pass into the food. That is why it is better that you store tap water in hard plastic or glass bottles or jugs. You can opt for glass because it is easier to wash.\nWashing jugs, bottles and tap aerators (where limescale, impurities, and microbes accumulate) at the water outlet are also key. The containers, according to food safety specialists, should be thoroughly washed every week, with hot water or soap.\nIncorporated filters of jar type containers\nThe most dangerous are the filters that are incorporated into jar-type containers. They are designed for room temperature and last for a month. In summer, the chances of creating a biofilm that forms a film for bacteria are high.\nThat is why it is always advisable to follow the recommendations of the manufacturers. The same thing happens with the osmosis filters installed in the taps. If you go over the time of use, there will be more possibilities of contamination in the water than when these filters are not used, because bacteria accumulate.\nAnother piece of advice for conserving water in the fridge is to cover it, to prevent the evaporation of chlorine and the entry of odors. Since as a neutral liquid, water absorbs them. Although sometimes you are assailed by the laziness of going for a glass, do not drink from the bottle, in any case.\nWhat can happen to you if you drink water contaminated with some bacteria\nYou could suffer from gastroenteritis or an intestinal problem. Although it is true that water is not the most dangerous because microbes do not grow easily in it as it does not have nutrients like food. Food is likely to have much more contamination.\nContaminated water and its consequences\nWhen you talk about contaminated tap water, unfortunately, you are talking about bad habits, poverty, diseases, and various ailments that reduce the health status of any person.\nIn order for you to be better informed and to act accordingly, below you will learn about some of the most common diseases that can be generated from the consumption of contaminated tap water and ways how to prevent yourself from them.\nDrinking contaminated water is one of the many causes that can trigger diarrhoea, since spoiled food, food with an excessive amount of fat and even some viruses can trigger this disease. When this disease comes in contact with contaminated water, many electrolytes begin to be lost, and with this comes dehydration.\nSometimes it is identified as a condition similar to diarrhoea with the difference that people who suffer from it usually also have some cramps, fever, and either of the two extremes: either very severe diarrhoea or inability to evacuate with a feeling of constipation. It is usually caused by bacteria that appear in contaminated food or water.\nIt is suggested that this disease can be treated by a doctor with a specific treatment and it usually takes a few weeks for the body to recover 100%. If you want to prevent it, you have to avoid sources of dubious origin both in the case of food and water consumed.\nThis condition is another one that can appear due to the consumption of contaminated water. It usually occurs much more in places with hot climates and it is a disease that is caused by a bacterium that directly attacks the intestines and care must be taken with the treatment.\nThe conditions of this disease are multiple episodes of diarrhoea and intense vomiting. It is said that this disease is a deadly enemy, you must be very careful and if you come in contact with it, in addition to a specific medical treatment, you should take plenty of fluids.\nAmong the symptoms of typhoid fever, as its name indicates, high fevers stand out, as well as severe headaches accompanied by nausea, loss of appetite, and repetitive episodes of stomach discomfort.\nFor this disease, there is no other source of proliferation other than the consumption of contaminated tap water and it must be treated by a health specialist so that the infection can be combated through the consumption of an antibiotic.\nIt is also essential that the source of this contaminated tap water be detected in order to prevent the disease from advancing and the virus from becoming resistant.\nTake your precautions and avoid these diseases\nConsume water from reliable sources, one option is installing filters and purifiers, which are products that have the highest standards for filtering the tap water. There are many ro service centers that offer doorstep services for repair and installation.\nDue to its manufacturing characteristics, it allows you to be confident that the water you are going to consume is up to 99.9% purified.\nAnother advantage of purifiers is that their installation is easy, fast, and of course, that everything you invest in a device like this will pay off in the form of drinkable water in excellent condition, it will be one of your best acquisitions.\nThe Hidden Dangers of Drinking Poorly Preserved Tap Water Infographic:\nPhoto by form PxHere']	['<urn:uuid:255cefd3-f2a0-4095-a09e-5d1ed5db48be>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T22:13:20.996306	11	30	1214
4	how does heat transfer between ice cubes water and glass temperature equilibrium conductor insulator	Heat transfer occurs from warmer to colder systems until thermal equilibrium is reached. In a glass of water with ice, the water gives energy to the ice, cooling down to a minimum temperature. The glass cup, being a conductor, creates a large temperature gradient between its inside and outside (high Biot number of 606), affecting the cooling process through convection and conduction.	['In this week’s blog we will follow up the previous series of espresso thermal analysis with a thermal analysis that is a little more fitting for this time of the year. As summer drags on and the days get warmer, everyone’s primary mission is to stay cool. One of the most common methods for this is a nice cold drink. A drink becomes cold when it gives energy to a colder system, such as ice. Thermal energy is always transferred from the warmer system to the colder system until an equilibrium point is reached. This means that ice cannot infinitely cool water; there is always a limit to how cold the water can get and this limit is known as the freezing point (0°C, 32°F).\nHeat Transfer Experiment\nThe purpose of this experiment is to understand the impact that ice has on the thermal energy of a glass cup of water.\nThe scope of this experiment is to use a glass cup of water and cubes of ice to find the cooling relationship between them. The glass cup will be covered to nullify any losses to evaporation or convection. Each temperature will be measured using an Omega™ HH806AU J-type therm°Couple reader. The glass cup will start at room temperature and then be monitored as it cools until the ice is completely melted.\nThe basic procedure for this experiment will be as follows:\n- Fill the glass cup with 7 ounces of room temperature water.\n- Place a predetermined amount of ice cubes in the glass cup along with the therm°Couple reader and cover.\n- Record temperature until ice is completely melted.\nExperimental Results of Water and Ice Heat Transfer\n- Each ice cube had an approximate size of 1 ¼” x ¾” x ¾”, with a density of 0.9167g/cm3 and a measured volume of 0.5 oz. Each ice cube weighed 13.55 g.\n- With one ice cube the water reached a low temperature of 19.6°C and the ice cube took 5 minutes and 51 seconds to melt. The temperature of the water over time can be seen below in Figure 1.\n- With two ice cubes the water reached a low temperature of 14.2°C and the ice cubes took 8 minutes and 16 seconds to melt. The temperature of the water over time can be seen below in Figure 2.\n- With three ice cubes the water reached a low temperature of 10.3°C and the ice cubes took 12 minutes and 56 seconds to melt. The temperature of the water over time can be seen below in Figure 3.\n- With four ice cubes the water reached a low temperature of 6.5°C and the ice cubes took 15 minutes and 1 second to melt. The temperature of the water over time can be seen below in Figure 4.\n- With five ice cubes the water reached a low temperature of 5.1°C and the ice cubes took 18 minutes and 11 seconds to melt. The temperature of the water over time can be seen below in Figure 5.\n- With six ice cubes the water reached a low temperature of 4.3°C and the ice cubes took 21 minutes and 36 seconds to melt. The temperature of the water over time can be seen below in Figure 6.\n- With seven ice cubes the water reached a low temperature of 4.1°C and the ice cubes took 53 minutes and 26 seconds to melt. The temperature of the water over time can be seen below in Figure 7.\n- The distribution of of the time it took to melt the ice can be seen below in Figure 8.\nAnalysis of Water and Ice Heat Transfer\nThe water with just one ice cube had a theoretical low temperature of 18.5°C. This was found using Equation 1, seen below.\nwhere m is mass, c is the specific heat, T is temperature, n is the molar amount and H is the molar heat of fusion. Using the same equation the theoretical temperatures for the water with 2 through 7 ice cubes was also calculated and these values are seen in Figure 9 below.\nFigure 9: Theoretical Final Temperature per Ice Cube\n[table id=ice9 /]\nHowever this theoretical temperature was not reached in the measurements due mainly to energy losses that could not be avoided. The measured temperatures can be seen in Figure 10 below.\nFigure 10: Measured Final Temperature per Ice Cube\n[table id=ice10 /]\nThe comparison between the theoretical temperatures and the measured temperatures can be seen below in Figure 11.\nWhile the measured temperature does not match exactly with the theoretical calculations, all of the measured points are within the allowable error and follow the same trend. The main losses were through free convection to the air through the walls of the glass cup and conduction to the walls of the class cup. The Biot number is a dimensionless number which is the ratio of the resistance to heat transfer on the inside and outside of the glass cup and is calculated using Equation 2 below\nBi=(h*Lc)/k Equation 2\nwhere h is the heat transfer coefficient, Lc is the characteristic length and k is the thermal conductivity. The glass cup used in these experiments has a very high Biot number (606) meaning that there is a large temperature gradient between the inside of the glass cup and the outside of the glass cup. These differences can be seen in Figure 12 below.\nFigure 12: Inner and Outer Surface Temperatures of Glass Cup\n[table id=ice12 /]\nThe common misconception with iced drinks is the more ice, the better. This experiment proved that this is in fact not true. This final temperatures of the water with 4, 5, 6 or 7 ice cubes were all within 1.6°C. Physically speaking, the water can only be cooled to a certain temperature, so it makes sense that the limit was consistent regardless of the amount of ice. After reaching the cooling limit the ice will affect the time at which the water stays that temperature. When comparing the temperature of the water with 6 ice cubes against the water with 7 ice cubes, their minimum temperatures are only 0.1°C off. However it took the 7 ice cubes 31 minutes and 50 seconds longer to melt completely and 6 ice cubes only took 21 minutes and 36 seconds.\nIn order to use the least amount of ice for the coldest drink the results point to the 4 ice cube sample. This doesn’t mean that you should always use just four ice cubes in your drink, it means that you want about 20% of your drink (by volume) to be ice. A good rule of thumb would be approximately 1 ice cube per every five ounces of beverage.', '•Temperature: average kinetic Energy ofparticles in an objectSolidLiquidGashttp://www.bcssa.org/newsroom/scholarships/great8sci/Matter/Choose_Matter.htmlThe more kinetic Ethe higher the temp.Add heat to changephasesInteractiveLABPHASES•Density: how close the atoms are in the element\nSOLIDS: atoms are tightly packedmovement of atoms is small & rapid because ofthe close bonds-definite volume & shapeLIQUIDS: atoms are farther apart than solids andcan slip around each other faster-definite volume; no definite shape so will takethe shape of containerGASES: atoms are farthest apart and can moveabout freely. (unconstrained gases willspread out indefinitely; confined gasestake shape of container.)-no definite volume or shape\nELEMENTS:Different elements have different properties-•Boiling point: point at which the atoms in a liquidstate vibrate and become a gas (hot)-different for each element!•Melting point: point at which the atoms in the solidstate vibrate and become a liquid (hot)-different for each element!VIDEO: Making molecules with atoms (2 min)Elements LINK SONG (3.5 min)VIDEO: Energy SourcesVIDEO: Energy Flow (2 min)VIDEO: How Electricity is Made (2 min)Molecules in motion SONG (3 min)\nSolubility: amount of a solute that can bedissolved in a solvent-depends on the chemistry of it-depends on the temperatureof it (warm dissolves faster)http://www.chem4kids.com/files/matter_solution.html\n•Conduit: conducts/ allows Energy transfer;water, metal•Insulator: does not allow Energy transfer;fiberglass, cardboard, air, cork, wood,rubber, wood•Heat: Energy transferred between objectsat different temperatures; thermal EnergyIf two objects are connected, there is always transferbetween high temp to low temp•Thermal equilibrium Energy transfer untilthe same temperature on both (balanced)\nPut ‘C’ for Conductor & ‘I’ for Insulator.Curling ironoven mittceramic bowliron skilletstove coilcookie sheetplastic spatulafiberglass insulationcopper pipeWhy would we want an object to be an insulator?Why would we want an object to be a conductor?\n•Conduction transfer of thermal energy throughdirect contact•Thermal (warm object touches a cold object)•Mechanical (kinetic E) 2 objects push or pullon each other•Electrical (current) battery or generator isconnected in a complete circuit to a device•Convection transfer of thermal energy bymovement of liquid/gas (hot up, cold down)•Radiation transfer of thermal energy byelectromagnetic waves (visible light & ultravioletwaves)HOW DO OBECTS HEAT UP?\nQUESTION: Which changes temperature faster:--air, water?Can’t measure transferred E directly; must becalculated:Heat (J) = specific heat x mass x change in tempVIDEO: Heat Transmission (3 min)HSW video: Fiberglass insulationVIDEO: RADIATIONVIDEO: Geothermal Power (2 min)HSW video: THERMAL RADIATION universe (2 min)HEAT experiment 5 card LINK: http://www.wisc-online.com/objects/ViewObject.aspx?ID=sce304\nMaterials:Sugar, hot water, cold water, spoonSugar Solution Lab1. Define solubility: _____________________________________________2. In this experiment, which was the solute? _____________ Which was the solvent?___________3. Which jar dissolved the sugar the fastest? _______________4. Which jar dissolved the most sugar? ______________5. Answer: Was your hypothesis correct? ________6. Explain why the sugar dissolved faster in the ______ water. (Use words from theword bank.) __________________________________________________________________________________________________7. What happened to the solution when you could not dissolve any more sugar in thesolutions?________________________________8. What does kinetic energy have to do with solubility? ____________________________________________________________________________________________9. What would be the best way to make sweet tea? _______________________________________________________________________________________________Variables:Independent variable- hot water, cold waterDependent variable- sugar, size of container, amount of water WORD BANKSolventSoluteSolubilitySaturationMoleculesAtomsKinetic energyDensityQUESTION: Does the temperature of a liquid affect thedissolving of a solute into a solvent?PROCEDURE:1.Prediction: In which solution do you think the sugar will dissolvebest? ____________________2.Add the same amount of water to each jar (1 hot, 1 cold)3.Add a spoon of sugar to each jar. Stir.4.Add equal spoonfuls of sugar to each jar until no moredissolving is taking place.5.Observe & answer the questions below.6.Cleanup.\nLAB: Feel the HeatTrial Mass of nails (g) Volume of water thatequals mass of nails(mL)Initial temp. of water& nailsInitial temp of waterto which nails will betransferredFinal temp ofwater and nailscombined12Procedure:• Bundle the nails together with a rubber band. Record the mass. Tie string around, leaving one end 15cm long.• Put bundle of nails into a cup, letting string dangle out. Fill the cup with hot water, covering nails. Setaside 5 min.• Use graduated cylinder to measure enough cold water to exactly the mass of nails. Record.• Measure & record the temp of the hot water with the nails & temp of cold water.• Use string to transfer the bundle of nails to the cup of cold water. Use the thermometer to monitor thetemperature of water-nail mixture. When the temp stops changing record this final temp.• Empty cups, dry nails. Repeat for trial 2, but switch the hot & cold water. Record data.Conclusion:The cold water in Trial 1 gained energy. Where did the E come from?How does the E gained by the nails in Trial 2 compare with the E lost by the hot water in Trial 2?Which material seems to be able to hold E better… iron or water?p. 430Materials: balance, 2 cups, cylinder, 10 nails, string, rubber band, thermometer, hot water, cold water\n•Wave: disturbance that transmits Energythrough matter or empty space;--as wave travels it does work on everything in itspath•Mechanical waves: need a mediumexamples- sound wave, ocean wave•Some waves don’t need a mediumexamples- visible light, microwaves, tv, radiosignals, x-rays\nWaves:Transverse wave- particles move up & down,perpendicular to direction wave is goingLongitudinal wave- particle move horizontallyalong the wave in the direction the wave is movingSurface wave- combination of transverse &longitudinal waveVIDEO: HSW Wavelength BasicsADVANCE\nTransverse wave- particles move up & down,perpendicular to direction wave is goingBACK\nLongitudinal wave- particle move horizontally alongthe wave in the direction the wave is movingBACK\nProperties of Waves•Amplitude: maximum distance the particles vibrate•Wavelength: distance between two crests orcompressions in a wave•Frequency: number of waves in a given amount oftime•Wave speed: speed at which wave travels (v)\nWave Interactionsreflection- wave bounces back after hitting a barrier;examples:light reflected lets us see itsound echoesrefraction- bending of a wave as it passes from onemedium to another at an angle (because wavechanges speed in a different medium)example:light through a prism (light is dispersed into separate colors)\nDiffraction bending of waves around a barrierInterference two or more waves overlap-constructive interference 2 waves overlapcrests & troughs, combining both waves’ energy…makes it stronger!-destructive interference 2 waves overlapone crest on one trough, cancels out each other’sEnergyHSW: Assignment Discovery: Sound/Interference (1 ½ min)Ripple Tank Simulation\nStanding waves: pattern looks like wave isstanding stillResonance: two objects naturally vibrate at thesame frequency; sound of one causes the other tovibrateVIDEO: Sound/Resonance-Shattering glass (15 min) UnitedStreaming']	['<urn:uuid:e1782f65-d927-4aac-aaf6-4b4334c873eb>', '<urn:uuid:beb7dfa1-2b18-4553-9a2c-b8aaba90a018>']	factoid	direct	long-search-query	similar-to-document	three-doc	novice	2025-05-12T22:13:20.996306	14	62	2123
5	skilled immigrants impact european development economy history examples	There are significant historical examples of skilled immigrants boosting European development. In the 17th century, when 20,000 skilled Huguenots migrated to Brandenburg-Prussia, they had substantial positive effects on textile manufacturing productivity - a 1% increase in Huguenot population share led to 1.4% higher productivity by 1802 through technology transfer. Similarly, Prussia benefited from skilled immigrants when the Hohenzollern dynasty encouraged migration in the 17th century, using tax revenues to build schools and roads. The evidence shows that skilled migration can boost innovation and productivity, but migrants' skills need to complement the host economy and language barriers need to be overcome for benefits to materialize.	"[""What was Prussia?\nPrussia no longer exists. It was a territory adjacent\nto the Baltic Sea in north eastern Europe. The original people\nthat lived in this area were the Prussi or Borussi who were a\nBaltic people related to Lithuanians (they lived around Duchy of\nPrussia in Fig 1). They were conquered and largely\nexterminated by the Teutonic Knights (a German Roman Catholic\ncrusading religious order) in the 13th cent after\nwhich the Baltic area, including Silesia, was effectively\nGermanised with intensive settlement by German peasant\ncolonists. However, in the fourteenth century, with the\nBlack Death and the revival of Slavonic Kingdoms (Hungary,\nPoland and Bohemia), this colonisation ended and thousands of\nfarms and cottages together with large tracts of land were\nabandoned. During the 17th century, a protestant family with the name\nHohenzollern rose to power in northern\nEurope. Besides creating an orderly army and capable government\nsystem, they filled their Treasury with tax money with which\nschools and roads were built and improved. A relatively\nstable 'dynasty' resulted with their leader taking the title of\n'King' of lands that were called 'Prussia'.\nThus Prussia became\na kingdom and a monarchy in north eastern Europe (under\nFriedrich I) from the beginning of the eighteenth century\n(1700). It reached the peak of its power under King\nWilhelm I (1861-1888) and his prime minister, Otto von Bismarck.\nFrom 1871, it became part of a series of German nations: the\nGerman Empire (1871-1918), the Weimar Republic (1919-1933), the\ncentralized German Third Reich (1934-1945)). During the\nThird Reich, the Nazis centralised power with Hitler formally\nbecoming the governor of Prussia. In effect, free states\nsuch as Prussia were dissolved. After World War II, the\nformer state of Prussia was split up, and like Poland in the\neighteenth century, legally erased from the map of Europe.\nWhere was Prussia in the late 1700s?\nSince Prussia no longer exists as a political entity, it is\nnot a simple matter to explain where the Prussia in which the Schaefers lived was located\nsince its boundaries were in a state of flux. Fig 1\nprovides an indication of extent of Prussia prior to 1740 under\nKing Friedrich I. At that time,\nPrussian territory was mostly in parts of what are now Germany\nand Poland (Fig 1). Silesia, where the Schaefers lived, was not a part of Prussia at that\ntime; it belonged to the Austro-Hungarian Hapsburg Monarchy to\nFig 1: Extent of Prussian territory prior to 1740 in\nrelation to the boundaries of Germany, Poland & Czech Republic\nPrussia then become a militaristic power with a large standing\narmy which, in the mid 1700s, consumed 80% of state tax revenue\neven in peacetime. Freidrich II (Frederick the Great:\nincreased the army to 150,000 with the land owning nobility providing an\nelite officer corps. The militarism of Prussia inspired\nfear and hatred from other European peoples\nIn the decades prior to the Schaefers' birth, Frederick the\nGreat had wrested Silesia from the Habsburg Monarchy\n(Austro-Hungarian) in the First Silesian War (1740-42) and\nPrussia became a major expansionist power in Europe from 1742.\nConsequently, after the Second and Third Silesian Wars (1744-5\nand 1756-63), the Austrian Habsburgs were finally convinced that\nthey could not have Silesia back and it formally became part of\nPrussia. It was made a Prussian province as it was\nvaluable as a barrier to Austrian expansionism northwards.\nThus, by 1763, the extent of Prussian territory had grown to\nthat shown in Fig 2. This would have been around the time,\nor a little before, Christian Schaefer Snr's parents were born.\nFig 2: Prussia expansion into Silesia and East Frisia by\nPrussia continued to expand under Frederick the Great.\nVia diplomacy with Russia, he managed to have a part of Poland\npartitioned off and added as West Prussia in 1772 which provided\na joining of the east and western parts of his territory (Fig 3).\nFig 3: Addition of West Prussia and the Netze District to\nthe Kingdom of Prussia prior to 1786.\nConsequently, when the Schaefers were born in the latter half\nof the 1700s, Silesia had become an established Prussian\nprovince and Prussia was in expansionist mode.\nBy the last decade of the century, Prussia purchased the\nprincipalities of Anabach and Bayreuth (Fig 4). In\naddition, more parts of Poland\nwere partitioned off and divided between Russia, Prussia and\nAustria. In effect, Poland was erased from the map and\nPrussia gained South Prussia and New East Prussia. It\nwould have been unthinkable, at the time, that, in the twentieth century, the\ntables would be turned, and Prussia would no longer appear on\nFig 4: Prussia's gain of most of the rest of Poland prior to\nBy the first decade of the nineteenth century (1800s),\nPrussia gained more territory in the east (Fig 5).\nFig 5: Prussian territory gains in the East prior to 1803.\nHowever, the French,\nunder Napoleon Bonaparte, were beginning to increase their\nterritory throughout Europe, devastating what was left of the\nHoly Roman Empire. Prussia remained neutral at first, but\nNapoleon defeated neighbouring Austria in 1804 and crowned\nhimself Emperor of France. A new political order was\nemerging in Europe and Prussia was the only state not dominated\nby France. Until 1805 (the year great great great\ngrandmother Schaefer gave birth to Christian Schaefer Snr), Prussia briefly occupied even more territory in the East (including Hannover) (Fig 6).\nFig 6: Prussian territory gains in the East prior to 1805.\nThat status did not last for long. In 1806, the\nPrussian Army, by then corrupt and poorly organised, was first\ndefeated in a battle with Britain, and then humiliated by\nNapoleon who entered Berlin (then in Prussia) unopposed.\nAs a result, in 1806-7, Prussia was forced to cede about half\nits territory in both the east and west (Fig 7). If Russia\nhad not interceded on its behalf, it may well have disappeared\nFig 7: Prussia after defeat by Napoleon in 1806.\nPrussia was also faced with a French occupation financed by\ncontributions which Prussia had to pay, and with French demands for the\npayment of indemnities.\nThus over the period that this generation of Schaefers would have grown up and\nmarried, Prussia expanded right across the northern part of\nEurope from the border with Russia, across all of Poland, and\nmuch of Germany; and then, when their son Christian Snr was a baby and\ntoddler, their country was invaded by Napoleon's forces.\nAs a result, the Prussian King relocated to Konigsberg\n(between what is now Poland and Lithuania on the Baltic Sea),\nand the Schaefers possibly endured the hardship of French occupation."", 'Can mass migration boost innovation and productivity?\nThe long-term impact of migration on innovation and productivity growth in host countries is a neglected issue in the current debate on refugees. Research shows that these effects can be substantial, but if Europe wants to capitalize on this potential it will need better information systems to match migrants’ skill sets with host environments.\nAs we have seen at various points in history, skilled migrants can have a substantial impact on the host economy through innovation and productivity growth, even when arriving in large numbers. These effects are beyond the direct contribution of the skilled migrants themselves.\nMigrant and host skills need to be sufficiently connected.\nOne important channel of impact is indirect, through the transfer of knowledge from migrant workers to native workers in the host country. However, analysis shows this takes a long time to materialize, and the effects are not obvious.\nMigrants’ skills need to complement those of the host economy, host economies need a strong native human capital set that is able to learn from migrant skills, and migrant and host skills need to be sufficiently connected.\nThe effects of skilled migration\nBorjas (1994) develops a theory model to show how immigrants with high levels of productivity who adapt rapidly to the host labour market can make a significant contribution to economic growth in the host economy. This contribution is not only direct, through the higher productivity of the migrants themselves, but also indirect, by raising the productivity of the native human capital through transfer of know-how.\nBorjas (1995) also explores when immigration can be beneficial for economic growth. When immigrants’ skills are sufficiently different from native workers’ skills, and when their characteristics are complementary to the native factors of production, this can lead to an “immigration surplus”. However, Borjas’ research does not look at the impact of mass migration on host productivity growth.\nWhen the Huguenots were outlawed in France by Louis XIV in the 17th century, thousands of skilled migrants fled to other European countries. Hornung (2014) analyses the impact of this mass skilled migration on host productivity growth.\nAbout 20,000 people went to Brandenburg-Prussia (which had a population of about 1.5 million), and about 5,000 to Berlin, where they represented about 20% of the town’s total population. The Huguenots were known for being well educated and holding skilled occupations.\nThe Prussian King, Friederich Wilhelm I, selected Huguenots according to their skills and assigned them to the Prussians towns depopulated by the Thirty Years’ War and the Black Death.\nHornung uses the historic records of this natural experiment where Huguenots were placed in selected Prussian towns, in combination with firm-level data on the value of inputs and outputs for all 693 textile manufactories in Prussian towns in 1802. He finds that immigration had long-term positive effects on productivity in textile manufacturing, where the Huguenots had specific skills.\nImmigration had long-term positive effects on productivity.\nThe effects found are sizeable: a 1 percentage point increase in the share of Huguenots in 1705 led to a 1.4 percentage point increase in productivity in textile manufacturing in 1802. Most effects were indirect, through technology transfers which increased the productivity of local textile plants.\nEven if the Huguenot manufacturing plants did not survive, technology was transferred to local manufacturing plants by training workers. However, these indirect transfers took a long time to materialize, due to the gap in native textile skills in Prussia at the time and to language issues.\nMass migration from Nazi Germany\nThe Jewish migrants who left Germany for the United States in the 1930s and 1940s are another example of forced mass skilled migration. By 1944, over 133,000 German Jewish émigrés had found refuge in the United States. Most were urban white-collar workers and one fifth were university graduates.\nMoser, Voena and Waldinger (2014) use the research fields in which dismissed German Jewish émigré chemists were specialized pre-1933, and compare changes in U.S. patenting by U.S. inventors in these research fields with changes in U.S. patenting by U.S. inventors in the fields of other German chemists.\nGerman Jewish émigré chemists led to a 71 percent increase in local patenting.\nThe authors find sizeable positive effects. They estimate a 71 percent increase in local patenting. The authors also document the multiple channels through which these effects materialized:\n- The arrival of the migrants encouraged U.S. inventions by helping to attract new domestic inventors to the research fields of émigrés, rather than by increasing the productivity of incumbent U.S. inventors in these fields;\n- Co-inventors of migrants became active patentees in the fields of migrants especially after 1940, and continued patenting through the 1950s;\n- Co-inventors of co-inventors of migrants also substantially increased their inventive activity in émigré fields after 1933, and remained substantially more productive throughout the 1950s and 1960s.\nPotential effects of low-skilled migration\nPreliminary work by Rachel Harris suggests that mass migration may positively impact host economies’ innovation capacity, even if migrants are low-skilled. She studied the Mariel Boatlift, a mass emigration of Cubans to the USA in 1980, as a natural experiment.\nLittle precise information is available on how many people came to the United States during the Mariel Boatlift or exactly where they settled. The most reliable sources (Card, 1990) indicate that between April 1980 and June 1981, 120.000-126.000 Cubans entered the US labour market, about half settling in Miami, and half in the rest of Florida. Many of the migrants were low-skilled and had a low level of English.\nNevertheless, Harris finds that the Mariel Boatlift caused an increase in patents in Florida, in technological categories with low barriers to entry. She suggests that this could be because individual inventors had access to a large supply of low-skilled labourers, and were able to hire them to do housework, child care and other manual work. This allowed these inventors to substitute away from housework and spend more time inventing, leading to an increase in patenting.\nLessons for the current crisis\nFrom these historical studies it can be seen that skilled migration can have a substantial effect on productivity growth in the host economies. But this requires migrants’ skills to be matched to the needs of host economies.\nSkilled migration can have a substantial effect on productivity growth in the host economies.\nUnfortunately, reliable systematic data on the skills of the current migration wave is not widely available. One of the most up-to-date sources of information is from the Swedish Employment Services, where refugees are asked to provide information on their education as part of an ‘establishment programme’.\nIn 2015, most of the refugees accepted onto the programme had less than 9 years of education, as shown in the figure below. Interestingly however, the second biggest share of refugees had higher education. This bodes well for the long term impact of migrants’ skills on the host economy.\nIf Europe wants to capitalize on the potential for long term effects on productivity growth from its migrants, European leaders must better balance migrants’ skills and the needs of host countries.\nEuropean leaders must better balance migrants’ skills and the needs of host countries.\nCurrently it is not possible to match migrants to the skills base of the host regions in Europe, as systematic information on incoming skills is missing.\nA better information system on the incoming migrants’ skills is needed, in order to match migrants better with the hosting environments, and ensure that their potential to boost innovation and productivity growth is not wasted.\nPositive spillover effects from matched skills will also be larger and faster with smaller language barriers. The earlier migrants can take language courses, the quicker they can integrate into the labour market and the faster spillover effects can materialize.\nImplementing this information system, and matching skills and environments at the European level would benefit migrants and host countries on a larger scale. It would reduce fixed set up costs, and allow for better matches.\nRepublishing and referencing\nBruegel considers itself a public good and takes no institutional standpoint. Anyone is free to republish and/or quote this post without prior consent. Please provide a full reference, clearly stating Bruegel and the relevant author as the source, and include a prominent hyperlink to the original post.']"	['<urn:uuid:5e073913-c5ca-4391-b13e-37d5a5b57017>', '<urn:uuid:a497fb3a-e88e-4791-a5e3-8d4963c09bc2>']	factoid	with-premise	long-search-query	distant-from-document	three-doc	novice	2025-05-12T22:13:20.996306	8	104	2473
6	What kind of work do kids in Norway typically do during their summer vacations according to one of the documentaries?	According to the film 'Tongue Cutters', children in Norway traditionally work cutting tongues off of fish as their summer job.	"[""As the Rencontres internationales du documentaire de Montréal turns 20, the festival's audience continues to evolve.\nDocumentary film is alive and kicking in Quebec, as evidenced by the 20th Rencontres internationales du documentaire de Montréal (RIDM), which presents 142 films from 47 countries, Thursday through Nov. 19.\n“I think 20 years for any festival is just amazing,” RIDM executive director Mara Gourd-Mercado said over coffee with festival programming director Bruno Dequen last week. “Twenty editions for a documentary festival in Montreal is a testament to all the teams that built RIDM into something that has lasted through the years and been growing all the time.”\n“What’s most remarkable is that the festival continues to renew its audience, unlike some other events,” Dequen added. “We could have had a niche audience from the start, which would have been the same, year after year. It’s fascinating to see more and more young people coming.”\nA driving force in the fest’s constant renewal is the growing interest in documentaries in general. Added to that is a thriving documentary scene in Quebec cinema, which feeds RIDM with a steady stream of films and prospective filmgoers.\n“The festival has a double mandate,” Dequen said. “On the one hand, it’s about showing the best or most interesting international films of the year; and it’s a great platform for local artists. We always have a huge selection of Quebec and Canadian films.”\n“The Montreal documentary community feels like RIDM belongs to them,” Gourd-Mercado said, “and it does. They feel comfortable enough to come to us with ideas, and we try to put them forward when we can. That exchange is really important, and has kept us going for 20 editions.”\nLast week, Telefilm Canada announced promising results of its initiative to reach gender parity by 2020 in its funding of films by women and men. Halfway through the first year of the mandate, Telefilm announced that out of approximately 60 features that have been approved for funding so far this year, 44 per cent are directed by women, 46 per cent written by women and 51 per cent produced by women.\nThat’s music to the ears of Gourd-Mercado, who notes that gender parity has long been a priority at RIDM. The fiction film world has lagged behind documentary in terms of films written and directed by women; likewise at primarily fiction festivals, from the Toronto International Film Festival to the Festival du nouveau cinéma, where the proportion of female directors generally lingers around 25 to 30 per cent.\n“It’s great that the institutions are catching up,” Gourd-Mercado said. “This year, we’re at 49 per cent women directors. Our opening and closing films are directed by women. It’s also a reflection of what’s happening in the doc world, where there are so many women producers and directors, it’s almost normal to have parity.”\nAs a programmer at the festival since 2011 and director of programming since last year, Dequen has sought to obtain a wide diversity of films by more than doubling the programming team to its current staff of six, each with their own area of specialization. They include Selin Murat, who specializes in Africa and the Middle East; Inuit filmmaker Isabella Weetaluktuk; Portugal-based Brazilian expat Gustavo Beck, who tracks the European festival circuit; former RIDM programming director Charlotte Selb; and former pre-selection committee member Apolline Caron-Ottavi.\n“There’s strength in numbers,” Dequen said. “The more people you bring in to put their personal touch and input, the more you build something interesting. We’re trying to find the perfect mix. The ideal RIDM film is extremely original in its form, while proposing some political point of view, so you can have discussions about esthetics and the subject matter.”\nThe goal, always, is to bring viewers on a journey. That’s where documentary film has an edge on fiction, allowing viewers to experience real people and places in new ways.\n“People go to RIDM to discover things,” Gourd-Mercado said, “to go, ‘Oh, wow, I never thought of that.’\n“There are no movie stars, so what you see is the content of the film. It’s important to go into the festival with une volonté de découvrir des choses.”\nAT A GLANCE\nThe 20th Rencontres internationales du documentaire de Montréal takes place Thursday, Nov. 9 through Sunday, Nov. 19. For tickets and program information, visit ridm.ca.\n10 films to catch at RIDM\nRIDM programming director Bruno Dequen and executive director Mara Gourd-Mercado offer their highlights from this year’s lineup.\nRoom for a Man, directed by Anthony Chidiac (Thursday, Nov. 16 at 6 p.m., UQÀM’s Pavillon Judith Jasmin Annexe). “It’s a first film by a homosexual director who comes from one of the richest, most conservative families in Lebanon. A very personal, autobiographical essay.”\nTaste of Cement, directed by Ziad Kalthoum (Friday, Nov. 10 at 8 p.m., Cinémathèque québécoise; Friday, Nov. 17 at 7 p.m., Concordia’s Alumni Auditorium in collaboration with Cinema Politica Concordia). “A film by a Syrian refugee, shot in Lebanon with Syrian migrant workers at a huge construction site,” Dequen said.\n“It’s very poetic — a real reflection on war and migration,” Gourd-Mercado added.\nDid You Wonder Who Fired the Gun?, directed by Travis Wilkerson (Saturday, Nov. 11 at 5:30 p.m., Quartier Latin; Tuesday, Nov. 14 at 2:30 p.m., Cinéma du Parc). “The director will be there for the first screening. This is a kind of essay-suspense-search film, very autobiographical — an experimental exploration of racism in America.”\nMaman Colonelle, directed by Dieudo Hamadi (Friday, Nov. 10 at 5:45 p.m., Cinémathèque québécoise; Sunday, Nov. 19 at 1 p.m., Cinéma du Parc). “An observational film from the Congo about a police officer who specializes in women’s and children’s rights, with rare access to the subject matter.”\nTan Pin Pin retrospective: Alternative Singapore Chronicles (Friday, Nov. 10 to Tuesday, Nov. 14, Cinémathèque québécoise). “She’s the most famous documentary filmmaker in Singapore. She uses a different esthetic each time. To Singapore, With Love uses talking head interviews with political activists in exile; while her last film, In Time to Come, is a symphony-essay on her city.”\nManic, directed by Kalina Bertin (Friday, Nov. 10 at 8:30 p.m., Cinéma du Parc; Wednesday, Nov. 15 at 8:30 p.m., Cinémathèque québécoise). “Montrealer Kalina Bertin’s film looks at the history of mental illness in her family, beginning with her father.”\nNowhere to Hide, directed by Zaradasht Ahmed (Saturday, Nov. 11 at 3 p.m., Cinéma du Parc; Monday, Nov. 13 at 7 p.m., Concordia’s Alumni Auditorium in collaboration with Cinema Politica Concordia). “Usually we see refugees at the refugee camp. This is about the whole process before: what makes you take the decision to say, ‘OK, now we have to leave (our country) and become refugees.’ This film really struck something in me.”\nTongue Cutters, directed by Solveig Melkeraaen (Friday, Nov. 10 at 3:45 p.m., Quartier Latin; Sunday, Nov. 19 at 6:30 p.m., Cinéma du Parc). “A film about kids in Norway, where their traditional summer job is to cut the tongues off of fish. It’s about something small, but makes you feel very close to the subject.”\nBirth of a Family, directed by Tasha Hubbard (Friday, Nov. 10 at 7 p.m., Concordia’s Alumni Auditorium in collaboration with Cinema Politica Concordia; Monday, Nov. 13 at 3 p.m., Cinéma du Parc). “A film about the reunion of an Indigenous family that got caught up in the ’60s Scoop, was separated and now they’re finding each other as adults. Super powerful.”\nThe Devil’s Freedom, directed by Everardo González (Friday, Nov. 10 at 6:30 p.m. and Saturday, Nov. 11 at 9:15 p.m., Cinéma du Parc). “A film about violence in Mexico. The director films the perpetrators and the victims with masks on. You hear their stories and all you see is their eyes. You understand that they’re all part of a system built on violence. Fantastic.”""]"	['<urn:uuid:959b9821-9883-4dc7-a671-ce2ebcb126be>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T22:13:20.996306	20	20	1305
7	How does climate change affect food production, and what organic farming solutions exist?	Climate change impacts agricultural production through changing temperature and rainfall patterns, affecting food pricing, reducing calorie consumption and nutritional value, and increasing child malnutrition. As a solution, organic farming presents a holistic approach that optimizes biological productivity while protecting the environment. This method prohibits synthetic pesticides and fertilizers, instead relying on crop rotations, cover crops, and composted manure to maintain soil fertility, which helps create a more resilient agricultural system that can better absorb climate change's impacts.	"[""This Article proposes a novel approach to addressing global climate change's impacts on agricultural production and food security. The climate change crisis is the most significant environmental issue facing our planet. The changes predicted to occur as the earth's climate warms include significant impacts to agriculture. At the same time that the planet is undergoing dramatic climatic changes, the global population is increasing, and economic development in many parts of the world is exerting increased demand for a greater and more diverse supply of food.\nThe relationship between climate change and agriculture is a close and complex one, as the current system of agricultural production both contributes to, and will be impacted by, the effects of increased climate change phenomena. On the one hand, fossil fuel-intensive practices associated with the current industrialized agricultural system significantly contribute to the greenhouse gas (GHG) emissions considered to be the major cause of climate change. On the other, the predicted changes in global temperature and rainfall patterns associated with climate change have the potential to dramatically impact agricultural production. Changing weather conditions associated with climate change will likely impact agricultural production, affect food pricing, and reduce both global per capita calorie consumption and the nutritional value of some food, thereby increasing child malnutrition. Because of the close relationship between climate change and agriculture, climate change has the potential to greatly impact global food security as its effects become more prevalent.\nAs the impacts of climate change on worldwide food production become more pronounced in the coming decades, it will become increasingly necessary to shift agricultural strategies away from current industrialized practices and toward a more resilient system of “ecologically-based” agriculture. This Article suggests that a key component to developing an agricultural system that can adapt to the likely impacts of climate change involves applying the concept of ecological resilience to increase agriculture's capacity to absorb climate change's impacts. Ecological resilience considers the magnitude of disturbance a system can absorb before the shock causes it to shift into another state. In an agricultural context, this concept describes a farm ecosystem's ability to adapt to shocks or disturbances, such as drought and pest and disease outbreaks, that threaten its ability to continue to function to produce acceptable crop yields.\nIn this Article, we suggest a new approach that we call “whole-system” agriculture certification, which builds on the successes of eco-labeling and the United States organic certification program by rewarding ecologically-based practices that enhance an agricultural system's resilience in the face of climate change-induced uncertainty. This Article proposes that this whole-system agricultural certification approach be modeled after the successful Leadership in Energy & Environmental Design (LEED) green building certification program, which certifies that buildings and communities are constructed according to specific standards designed to address aspects of environmental sustainability, and demonstrates how such a system would work.\nMary Jane Angelo & Joanna Reilly-Brown, Whole-System Agricultural Certification: Using Lessons Learned From Leed to Build a Resilient Agricultural System to Adapt to Climate Change, 85 U. Colo. L. Rev. 689 (2014), available at http://scholarship.law.ufl.edu/facultypub/465"", ""Introduction to Organic Farming\nOrganic farming is a method of crop and livestock production that involves much more than choosing not to use pesticides, fertilizers, genetically modified organisms, antibiotics and growth hormones.\nOrganic production is a holistic system designed to optimize the productivity and fitness of diverse communities within the agro-ecosystem, including soil organisms, plants, livestock and people. The principal goal of organic production is to develop enterprises that are sustainable and harmonious with the environment.\nThe general principles of organic production, from the Canadian Organic Standards (2006), include the following:\n- protect the environment, minimize soil degradation and erosion, decrease pollution, optimize biological productivity and promote a sound state of health\n- maintain long-term soil fertility by optimizing conditions for biological activity within the soil\n- maintain biological diversity within the system\n- recycle materials and resources to the greatest extent possible within the enterprise\n- provide attentive care that promotes the health and meets the behavioural needs of livestock\n- prepare organic products, emphasizing careful processing, and handling methods in order to maintain the organic integrity and vital qualities of the products at all stages of production\n- rely on renewable resources in locally organized agricultural systems\nOrganic farming promotes the use of crop rotations and cover crops, and encourages balanced host/predator relationships. Organic residues and nutrients produced on the farm are recycled back to the soil. Cover crops and composted manure are used to maintain soil organic matter and fertility. Preventative insect and disease control methods are practiced, including crop rotation, improved genetics and resistant varieties. Integrated pest and weed management, and soil conservation systems are valuable tools on an organic farm. Organically approved pesticides include “natural” or other pest management products included in the Permitted Substances List (PSL) of the organic standards. The Permitted Substances List identifies substances permitted for use as a pesticides in organic agriculture. All grains, forages and protein supplements fed to livestock must be organically grown.\nThe organic standards generally prohibit products of genetic engineering and animal cloning, synthetic pesticides, synthetic fertilizers, sewage sludge, synthetic drugs, synthetic food processing aids and ingredients, and ionizing radiation. Prohibited products and practices must not be used on certified organic farms for at least three years prior to harvest of the certified organic products. Livestock must be raised organically and fed 100 per cent organic feed ingredients.\nOrganic farming presents many challenges. Some crops are more challenging than others to grow organically; however, nearly every commodity can be produced organically.\nThe world market for organic food has grown for over 15 years. Growth of retail sales in North America is predicted to be 10 per cent to 20 per cent per year during the next few years. The retail organic food market in Canada is estimated at over $1.5 billion in 2008 and $22.9 billion in the U.S.A. in 2008. It is estimated that imported products make up over 70 per cent of the organic food consumed in Canada. Canada also exports many organic products, particularly soybeans and grains.\nThe Canadian Organic Farmers reported 669 certified organic farms in Ontario in 2007 with over 100,000 certified organic acres of crops and pasture land. This is an annual increase of approximately 10 per cent per year in recent years. About 48 per cent of the organic cropland is seeded to grains, 40 per cent produces hay and pasture and about five per cent for certified organic fruits and vegetables. Livestock production (meat, dairy and eggs) has also been steadily increasing in recent years.\nThe main reasons farmers state for wanting to farm organically are their concerns for the environment and about working with agricultural chemicals in conventional farming systems. There is also an issue with the amount of energy used in agriculture, since many farm chemicals require energy intensive manufacturing processes that rely heavily on fossil fuels. Organic farmers find their method of farming to be profitable and personally rewarding.\nConsumers purchase organic foods for many different reasons. Many want to buy food products that are free of chemical pesticides or grown without conventional fertilizers. Some simply like to try new and different products. Product taste, concerns for the environment and the desire to avoid foods from genetically engineered organisms are among the many other reasons some consumers prefer to buy organic food products. In 2007 it was estimated that over 60 per cent of consumers bought some organic products. Approximately five per cent of consumers are considered to be core organic consumers who buy up to 50 per cent of all organic food.\n“Certified organic” is a term given to products produced according to organic standards as certified by one of the certifying bodies. There are several certification bodies operating in Ontario. A grower wishing to be certified organic must apply to a certification body requesting an independent inspection of their farm to verify that the farm meets the organic standards. Farmers, processors and traders are each required to maintain the organic integrity of the product and to maintain a document trail for audit purposes. Products from certified organic farms are labelled and promoted as “certified organic.”\nIn June 2009, the Canadian government introduced regulations to regulate organic products. Under these regulations the Canadian Food Inspection Agency (CFIA) oversees organic certification, including accreditation of Conformity Verification Bodies (CVBs) and Certification Bodies (CBs). This regulation also references the Canadian Organic Production Systems General Principles and Management Standards (CAN/CGSB-32.310) and the Organic Production Systems – Permitted Substances List that were revised in 2009.\nThe Canadian organic regulations require certification to these standards for agricultural products represented as organic in import, export and inter-provincial trade, or that bear the federal organic agricultural product legend or logo. (Figure 1) Products that are both produced and sold within a province are regulated by provincial organic regulations where they exist (Quebec, British Columbia and Manitoba).\nFigure 1. Canadian Agriculture Product Legend (logo)\nThe federal regulations apply to most food and drink intended for human consumption and food intended to feed livestock, including agricultural crops used for those purposes. They also apply to the cultivation of plants. The regulations do not apply to organic claims for other products such as aquaculture products, cosmetics, fibres, health care products, fertilizers, pet food, lawn care, etc.\nFood products labelled as organic must contain at least 95 per cent organic ingredients (not including water and salt) and can bear the Canada Organic logo. Multi-ingredient products with 70 per cent to 95 per cent organic product content may be labelled with the declaration: “% organic ingredients”. Multi-ingredient products with less than 70 per cent organic content may identify the organic components in the ingredient list.\nExported products must meet the requirements of the importing country or standards negotiated through international equivalency agreements. Products exported to the U.S. must meet the terms of the Canada-U.S. equivalency agreement signed in June 2009. All products that meet the requirements of the Canada Organic Regime can be exported to the U.S. with the exception that agricultural products derived from animals treated with antibiotics cannot not be marketed as organic in the U.S. Canada is also exploring other international equivalency agreements with other trading partners to enhance trade opportunities for export and to assure the organic integrity of imported products.\nWhen considering organic certification, know the requirements and accreditation(s) needed in the marketplace where your products will be sold. When comparing certification bodies, make sure they have the certification requirements and accreditations needed to meet market requirements. As a minimum certification bodies should be accredited under the Canadian Organic Products Regulations. Some markets may require accreditation or equivalency agreements with countries in the European Union, or with the Japanese Agricultural Standard (JAS), Bio-Swisse or other international organic certification systems. As Canada develops international equivalency agreements the need for the certification body to have these international accreditations will diminish.\nFor more information on certification and links to Canadian regulations and standards see the Organic Agricultural section of the OMAFRA website at www.ontario.ca/organic or the CFIA website at www.inspection.gc.ca.\nThe first few years of organic production are the hardest. Organic standards require that organic lands must be managed using organic practices for 36 months prior to harvest of the first certified organic crop. This is called the “transition period” when both the soil and the manager adjust to the new system. Insect and weed populations also adjust during this time.\nCash flow can be a problem due to the unstable nature of the yields and the fact that price premiums are frequently not available during the transition since products do not qualify as “certified organic.” For this reason, some farmers choose to convert to organic production in stages. Crops with a low cost of production are commonly grown during the transition period to help manage this risk.\nCarefully prepare a plan for conversion. Try 10 per cent to 20 per cent the first year. Pick one of the best fields to start with and expand organic acreage as knowledge and confidence are gained. It may take five to 10 years to become totally organic, but a long term approach is often more successful than a rapid conversion, especially when financial constraints are considered. Parallel production (producing both organic and conventional versions of the same crop or livestock product) is not allowed. Use good sanitation, visually different varieties, individual animal identification and other systems to maintain separation and integrity of the organic and conventional products. Good records are essential.\nIn organic production, farmers choose not to use some of the convenient chemical tools available to other farmers. Design and management of the production system are critical to the success of the farm. Select enterprises that complement each other and choose crop rotation and tillage practices to avoid or reduce crop problems.\nYields of each organic crop vary, depending on the success of the manager. During the transition from conventional to organic, production yields are lower than conventional levels, but after a three to five year transition period the organic yields typically increase.\nCereal and forage crops can be grown organically relatively easily to due to relatively low pest pressures and nutrient requirements. Soybeans also perform well but weeds can be a challenge. Corn is being grown more frequently on organic farms but careful management of weed control and fertility is needed. Meeting nitrogen requirements is particularly challenging. Corn can be successfully grown after forage legumes or if manure has been applied. Markets for organic feed grains have been strong in recent years.\nThe adoption of genetically engineered (GMO) corn and canola varieties on conventional farms has created the issue of buffer zones or isolation distance for organic corn and canola crops. Farmers producing corn and canola organically are required to manage the risks of GMO contamination in order to produce a “GMO-free” product. The main strategy to manage this risk is through appropriate buffer distances between organic and genetically engineered crops. Cross-pollinated crops such as corn and canola require much greater isolation distance than self-pollinated crops such as soybeans or cereals.\nFruit and vegetable crops present greater challenges depending on the crop. Some managers have been very successful, while other farms with the same crop have had significant problems. Certain insect or disease pests are more serious in some regions than in others. Some pest problems are difficult to manage with organic methods. This is less of an issue as more organically approved biopesticides become available. Marketable yields of organic horticultural crops are usually below non-organic crop yields. The yield reduction varies by crop and farm. Some organic producers have added value to their products with on-farm processing. An example is to make jams, jellies, juice, etc. using products that do not meet fresh market standards.\nLivestock products can also be produced organically. In recent years, organic dairy products have become popular. There is an expanding market for organic meat products. Animals must be fed only organic feeds (except under exceptional circumstances). Feed must not contain mammalian, avian or fish by-products. All genetically engineered organisms and substances are prohibited. Antibiotics, growth hormones and insecticides are generally prohibited. If an animal becomes ill and antibiotics are necessary for recovery, they should be administered. The animal must then be segregated from the organic livestock herd and cannot be sold for organic meat products. Vaccinations are permitted when diseases cannot be controlled by other means. Artificial insemination is permitted. Always check with your certification body to determine if a product or technique is allowed in the Permitted Substances List and the organic standards. Organic production must also respect all other federal, provincial and municipal regulations.\nOrganic produce can usually qualify for higher prices than non-organic products. These premiums vary with the crop and may depend on whether you are dealing with a processor, wholesaler, retailer or directly with the consumer. Prices and premiums are negotiated between buyer and seller and will fluctuate with local and global supply and demand.\nHigher prices offset the higher production costs (per unit of production) of management, labour, and for lower farm yields. These differences vary with commodity. Some experienced field crop producers, particularly of cereals and forages, report very little change in yield while in some horticultural crops such as tree fruits, significant differences in marketable yield have been observed. There may also be higher marketing costs to develop markets where there is less infrastructure than for conventional commodities. Currently, demand is greater than supply for most organic products.\nOrganic farming can be a viable alternative production method for farmers, but there are many challenges. One key to success is being open to alternative organic approaches to solving production problems. Determine the cause of the problem, and assess strategies to avoid or reduce the long term problem rather than a short term fix for it.\nCOG – Canadian Organic Growers Inc.\n323 Chapel St., Ottawa ON K1N 7Z2\nPhone: (613) 216-0741, 1-888-375-7383\nEFAO – Ecological Farmers Association of Ontario\n5420 Highway 6 North,\nRR 5, Guelph, ON N1H 6S2\nPhone: (519) 822-8606\nOMAFRA – Ontario Ministry of Agriculture, Food and Rural Affairs\n1 Stone Road W., Guelph, ON N1G 4Y2\nAgr. Information Contact Centre\nOACC- Organic Agricultural Centre of Canada\nNova Scotia Agricultural College\nBox 550, Truro, Nova Scotia, B2N 5E3\nPhone: (902) 893-7256, Fax: (902) 893-3430\nGuelph Organic Conference\nFor information contact:\nTomás Nimmo, Box 116,\nCollingwood, ON L9Y 3Z4\nPhone: (705) 444-0923, Fax (705) 444-0380\nOCO - Organic Council of Ontario\nRR 5 Guelph, ON N1H 6J2\nPhone: (519) 827-1221, Fax: (519) 827-0721\n© Queen's Printer for Ontario, 2015\nThe information on this page was written and copyrighted by the Government of Ontario. The information is offered here for educational purposes only and materials on this page are owned by the Government of Ontario and protected by Crown copyright. Unless otherwise noted materials may be reproduced for non-commercial purposes. The materials must be reproduced accurately and the reproduction must not be represented as an official version. As a general rule, information materials may be used for non-profit and personal use.""]"	['<urn:uuid:314375cb-fc25-49b6-91aa-783a71132b83>', '<urn:uuid:e6ddb9b3-68e1-422b-adc0-49ee5dd47471>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T22:13:20.996306	13	77	3004
8	wondering if trademark dispute settlement possible without legal fight what ways available to fix domain name conflict and past cases outcome	Settlement of trademark disputes without litigation offers several advantages: avoiding high legal expenses, achieving quick resolution, maintaining good relations between parties, and avoiding disclosure of confidential information. Specific options include: amending goods/services descriptions, modifying the trademark, voluntarily abandoning applications, or surrendering registrations. Based on WIPO case precedents, particularly in domain name disputes, panels consistently order domain transfers when someone registers multiple variations of famous trademarks (like Facebook and Instagram) by adding generic terms. This is especially true when the domains are either inactive or used for parking websites, which is considered evidence of bad faith registration and use.	"['Settlement Options for Trademark Disputes\nWhen faced with a trademark dispute or trademark conflict, parties are well advised to consider settlement options. There are many reasons to avoid litigation including, escalating litigation costs and the uncertainty of an unfavorable result. Both court proceedings and Trademark Trial and Appeal Board proceedings are lengthy, expense, and significantly interfere with a brand\'s marketing program and related activities. Besides from avoiding exorbitant legal expenses and achieving a speedy resolution, settlement of a trademark conflict has other advantages as well. An agreement can be prepared that meets the specific needs of the respective parties, you can eliminate the risk of an unfavorable result, you can maintain amicable relations between the parties, you can avoid producing confidential information and documents in discovery, and you can resume day-to-day business activities without too much interruption.Amendment of Goods or Services\nParties are becoming more creative in finding resolutions. However, there are some basic options a party should immediately consider when faced with a trademark dispute at the Trademark Trial and Appeal Board. The first question to ask is can you amend the identification of goods and services in the trademark application or registration to avoid confusion? An applicant or registrant can sometimes clarify or limit (broadening the id is not allowed) the identification of goods and services. This can be done by deleting an item in the identification, restricting trade channels, placing restrictions on classes of consumers, or perhaps other limits could be placed on the goods or the scope of the services. Occasionally, it makes sense to broaden the identification and then, a new trademark application must be filed. Within the settlement documents, the Opposer or Petitioner would agree to refrain from opposing the new application or resulting registration and the applicant or registrant would agree to abandon the pending application or surrender the original registration.Amendment of the Trademark\nAnother option is to amend the trademark. If a party wishes to amend their mark, the amendment must not materially alter the mark and the specimens and drawing previously submitted must support the amended mark; otherwise a new trademark application will have to be filed. An applicant may amend the drawing if the specimens filed or the substitute specimens support the proposed amendment (the mark in the drawing must agree with the specimens). The general rule for ""material alteration"" is the amended mark cannot create a different commercial impression from the original mark (requiring republication for the purposes of opposition). However, there is an exception to this rule, if you add matter that was already the subject of a valid registration owned by the same party and covering the same goods or services, this will not be considered a material alteration.Abandonment of Application, Surrender of Registration, Withdrawal of Proceeding\nAnother feasible alternative for settlement is for the Applicant or Registrant to agree to either abandon its trademark application or surrender its registration. If the proceeding was not commenced yet, then voluntary abandonment is simple. If the proceeding was commenced, then the party is well advised to obtain the written consent of every adverse party to the proceeding, otherwise judgment will be entered against the applicant and it will have preclusive effect. The same rule applies to the surrender of a trademark registration. Likewise, an Opposer or Petitioner may agree to withdraw its proceeding for various reasons. For example, if an acceptable amendment is made to the goods or services in the trademark application or if the mark is modified, then this may prompt a withdrawal. Either proceeding (Opposition or Cancellation) may be withdrawn without prejudice if the answer was not submitted. Another example that may prompt a withdrawal of an opposition or cancellation proceeding would entail the parties entering into a settlement agreement. For more details on trademark settlement agreements see our webpages entitled, Resolving Trademark Disputes Without Litigation and Is A Co-existence Agreement The Right Choice For Your Brand?Assignments, Licenses, & Disclaimers\nParties can also achieve settlement through assignments and licensing. A party that is faced with a prior application or registration can contact the owner of the conflicting mark in an effort to negotiate an assignment of rights or in the alternative, a license. See our webpage entitled Trademark Licensing for more information involving this subject. Lastly, it is possible to achieve a settlement in some cases, if the parties agree that a word or element of the mark is disclaimed. These are some of the settlement options that should be explored before proceeding with litigation. If you have any questions concerning a trademark dispute, please feel free to contact our office for a courtesy consultation.', 'WIPO Arbitration and Mediation Center\nADMINISTRATIVE PANEL DECISION\nFacebook Inc., Instagram, LLC v. randy fierro, angels.net, Domains By Proxy, LLC\nCase No. D2021-1782\n1. The Parties\nThe Complainants are Facebook Inc. and Instagram, LLC, United States of America (“United States”), represented by Hogan Lovells (Paris) LLP, France.\nThe Respondents are randy fierro, angels.net, United States, and Domains By Proxy, LLC, United States.\n2. The Domain Names and Registrar\nThe disputed domain names <facebookadvertisinginc.com>, <facebookadvertisingind.com>, <facebookadvertisingindustry.com>, <facebookangel.com>, <facebookangels.com>, <facebookcorps.com>, <facebookdatabase.com>, <facebookdatabases.com>, <facebookdatacenter.com>, <facebookdatacenters.com>, <facebookincorporated.com>, <facebookindinc.com>, <facebookindustry.com>, <facebookindustryinc.com>, <facebookindustryincorporated.com>, <facebookmanufacturing.com>, <facebookmanufacturinginc.com>, <facebookmanufacturingincorporated.com>, <facebookmanufacturingindustry.com>, <facebookproductsinc.com>, <facebookproductsincorperated.com>, <facebookproductsindustry.com>, <facebookpublisherinc.com>, <facebookpublisherind.com>, <facebookpublisherindustry.com>, <facebooktechnology.com>, <facebookuniverseinc.com>, <facebookuniverseincorporated.com>, <instagramadvertisinginc.com>, <instagramadvertisingincorporated.com>, <instagramadvertisingind.com>, <instagramadvertisingindustry.com>, <instagramangel.com>, <instagramangels.com>, <instagramincorporated.com>, <instagramind.com>, <instagramindustry.com>, instagramindustryincororated.com>, <instagrammanufacturinginc.com>, <instagrammanufacturingincorporated.com>, <instagrampublisherinc.com>, <instagrampublisherincorporated.com>, <instagrampublisherind.com>, <instagrampublisherindustry.com>, <facebookbankinc.com>, <facebookbankincorporated.com>, <facebookcorporations.com>, <instagramcorporations.com>, <instagramsocialmedia.com>, <instagrammanufacturing.com>, <facebookhoodies.com>, <facebookproductscorporation.com>, <instagramproductscorporation.com>, <facebookmanufacturingcorporation.com>, <instagrammanufacturingcorporation.com>, <instagramsocialmediainc.com>, <instagramsocialnetwork.com>, <facebookproductscorp.com>, <instagramcorp.com>, <instagramcorporation.com>, <instagramadvertisingcorporation.com>, <instagramadvertisingcorp.com>, <facebookadvertisingcorp.com>, <facebookadvertisingcorporation.com>, <facebookpublishercorp.com>, <facebookpublishercorporation.com>, <instagrampublishercorp.com>, <instagrampublishercorporation.com>, and <instagramtechnology.com> are registered with GoDaddy.com, LLC (the “Registrar”).\n3. Procedural History\nThe Complaint was filed with the WIPO Arbitration and Mediation Center (the “Center”) on June 7, 2021. On June 7, 2021, the Center transmitted by email to the Registrar a request for registrar verification in connection with the disputed domain names. On June 7, 2021, the Registrar transmitted by email to the Center its verification response disclosing registrant and contact information for the disputed domain names, which differed from the named Respondent and contact information in the Complaint. The Center sent an email communication to the Complainants on June 8, 2021, providing the registrant and contact information disclosed by the Registrar, and inviting the Complainants to submit an amendment to the Complaint. The Complainants filed an amended Complaint on June 9, 2021.\nThe Center verified that the Complaint together with the amended Complaint satisfied the formal requirements of the Uniform Domain Name Dispute Resolution Policy (the “Policy” or “UDRP”), the Rules for Uniform Domain Name Dispute Resolution Policy (the “Rules”), and the WIPO Supplemental Rules for Uniform Domain Name Dispute Resolution Policy (the “Supplemental Rules”).\nIn accordance with the Rules, paragraphs 2 and 4, the Center formally notified the Respondent of the Complaint, and the proceedings commenced on June 15, 2021. In accordance with the Rules, paragraph 5, the due date for Response was July 5, 2021. The Respondent did not submit any response. Accordingly, the Center notified the Respondent’s default on July 7, 2021.\nThe Center appointed William F. Hamilton as the sole panelist in this matter on July 13, 2021. The Panel finds that it was properly constituted. The Panel has submitted the Statement of Acceptance and Declaration of Impartiality and Independence, as required by the Center to ensure compliance with the Rules, paragraph 7.\n4. Factual Background\nThe Complainants provide online social networking services and have rapidly developed considerable renown and goodwill worldwide, particularly in the United States. The Complainants’ websites at “www.facebook.com” and “www.instagram.com” are among the most visited websites in the world. The Complainants own numerous trademark registrations for the terms FACEBOOK and INSTAGRAM (collectively, the two “Marks’) in many jurisdictions throughout the world, including in the United States. The Complainants’ trademark registrations include but are not limited to the following:\n- United States Trademark No. 3041791, FACEBOOK, registered on January 10, 2006, (first use in commerce in 2004):\n- International Trademark No. 1075807, FACEBOOK, registered on 16 July 16, 2020:\n- United States Trademark No. 4146057, INSTAGRAM, registered on 22 May 22, 2012, (first use in commerce on 6 October 2010): and\n- International Trademark No. 1129314, INSTAGRAM, registered on 15 March 15, 2012.\nThe Complainants also own and utilize domain names around the world that incorporate the Marks, for example, with respect the United States <facebook.us> and <Instagram.us>.\nThe Complainants’ Marks are highly distinctive, well-known, and uniquely associated with the Complainants’ services and products. Facebook, Inc. v. He Wenming, WIPO Case No. DCC2013-0004; Instagram, LLC v. lu xixi, PRIVATE, WIPO Case No. D2015-1168.\nThe Respondent registered the following disputed domain names on the dates indicated:\n<facebookadvertisingcorp.com>: December 30, 2020\n<facebookadvertisingcorporation.com>: December 30, 2020\n<facebookadvertisinginc.com>: November 3, 2016\n<facebookadvertisingind.com>: November 3, 2016\n<facebookadvertisingindustry.com>: November 3, 2016\n<facebookangel.com>: July 3, 2017\n<facebookangels.com>: July 3, 2017\n<facebookbankinc.com>: July 16, 2019\n<facebookbankincorporated.com>: July 17, 2019\n<facebookcorporations.com>: April 30, 2020\n<facebookcorps.com>: July 24, 2020\n<facebookdatabase.com>: July 3, 2017\n<facebookdatabases.com>: July 3, 2017\n<facebookdatacenter.com>: July 3, 2017\n<facebookdatacenters.com> July 3, 2017\n<facebookhoodies.com>: June 6, 2020\n<facebookincorporated.com>: January 28, 2016\n<facebookindinc.com>: March 4, 2016\n<facebookindustry.com>: March 3, 2016\n<facebookindustryinc.com>: March 3, 2016\n<facebookindustryincorporated.com>: March 3, 2016\n<facebookmanufacturing.com>: October 19, 2016\n<facebookmanufacturingcorporation.com>: July 27, 2020\n<facebookmanufacturinginc.com>: November 4, 2016\n<facebookmanufacturingincorporated.com>: November 4, 2016\n<facebookmanufacturingindustry.com>: June 3, 2017\n<facebookproductscorp.com>: September 29, 2020\n<facebookproductscorporation.com>: July 26, 2020\n<facebookproductsinc.com>: November 4, 2016\n<facebookproductsincorperated.com>: October 19, 2016\n<facebookproductsindustry.com>: June 3, 2017\n<facebookpublishercorp.com>: January 4, 2021\n<facebookpublishercorporation.com>: January 4, 2021\n<facebookpublisherinc.com>: November 3, 2016\n<facebookpublisherind.com>: November 3, 2016\n<facebookpublisherindustry.com>: November 3, 2016\n<facebooktechnology.com>: July 3, 2017\n<facebookuniverseinc.com>: July 3, 2017\n<facebookuniverseincorporated.com>: July 3, 2017\n<instagramadvertisingcorp.com>: December 30, 2020\n<instagramadvertisingcorporation.com>: December 30, 2020\n<instagramadvertisinginc.com>: November 4, 2016\n<instagramadvertisingincorporated.com>: November 3, 2016\n<instagramadvertisingind.com>: November 4, 2016\n<instagramadvertisingindustry.com>: November 4, 2016\n<instagramangel.com>: July 3, 2017\n<instagramangels.com>: July 3, 2017\n<instagramcorp.com>: September 30, 2020\n<instagramcorporation.com>: December 21, 2020\n<instagramcorporations.com>: May1, 2020\n<instagramincorporated.com>: March 4, 2016\n<instagramind.com>: April 1, 2016\n<instagramindustry.com>: April 1, 2016\n<instagramindustryincororated.com>: April 1, 2016\n<instagrammanufacturing.com>: June 3, 2020\n<instagrammanufacturingcorporation.com>: July 28, 2020\n<instagrammanufacturinginc.com>: November 4, 2016\n<instagrammanufacturingincorporated.com>: November 4, 2016\n<instagramproductscorporation.com>: July 27, 2020\n<instagrampublishercorp.com>: January 4, 2021\n<instagrampublishercorporation.com>: January 4, 2021\n<instagrampublisherinc.com>: November 4, 2016\n<instagrampublisherincorporated.com>: November 3, 2016\n<instagrampublisherind.com>: November 4, 2016\n<instagrampublisherindustry.com>: November 4, 2016\n<instagramsocialmedia.com>: May 28, 2020\n<instagramsocialmediainc.com>: July 29, 2020\n<instagramsocialnetwork.com>: July 29, 2020\n<instagramtechnology.com>: January 29,2021\nThe disputed domain names <facebookadvertisingcorp.com>, <facebookcorporations.com>, <facebookcorps.com>,<facebookindustryinc.com>, <facebookmanufacturingcorporation.com>, <facebookproductscorp.com>, <facebookproductscorporation.com>, <facebookproductsinc.com>, <facebookproductsincorperated.com>, <facebookpublishercorp.com>, <instagramind.com>, <instagramindustryincororated.com>,<instagrammanufacturingcorporation.com>, <instagramproductscorporation.com>,<instagramsocialmediainc.com> and <instagramsocialnetwork.com> resolved to parking websites when the Complainant was served according to Annexes to the Complainant while the remaining disputed domain names are inactive and do not appear have ever resolved to an active website. As of this decision, the disputed domain names that previously resolved to parking websites do not appear to be active.\n5. Parties’ Contentions\nThe Complainants assert Respondent engaged in a pattern and practice over a five-year period of registering and using the sixty-nine disputed domain names which are confusingly similar to the Complainants’ Marks because each of the disputed domain names entirely incorporate one or other of the Marks and merely adds dictionary terms as a suffix. The Complainants assert that the Complainants never authorized the Respondent to use the disputed domain names, that the Respondent is not generally known by any of the disputed domain names, and that the Respondent has never engaged in any bona fide commercial activity in connection with and of the disputed domain names. The Complainants assert that the Respondent registered and used the disputed domain names in bad faith to attract unsuspecting internet users for commercial gain.\nThe Respondent did not reply to the Complainants’ contentions.\nThe Panel finds that the Complainants are related entities and the claims lodged against the Respondent by the Complainants are similar and show a common pattern and practice extending over a five-year period. Thus, the Panel finds that (i) the Respondent has engaged in common conduct that has affected the Complainants in a similar fashion, and (ii) that it would be equitable and procedurally efficient to permit the consolidation. WIPO Overview of WIPO Panel Views on Selected UDRP Questions, Third Edition, (“WIPO Overview 3.0”), section 4.11.1.\n7. Discussion and Findings\nUnder paragraph 4(a) of the Policy, to succeed the Complainants must satisfy the Panel that:\n(i) the disputed domain names are identical or confusingly similar to a trademark or service mark in which the Complainants have rights;\n(ii) the Respondent has no rights or legitimate interests in respect of the disputed domain names; and\n(iii) the disputed domain names were registered and are being used in bad faith.\nA. Identical or Confusingly Similar\nThe Panel finds that the disputed domain names are confusingly similar to the Complainants’ Marks.\nThe disputed domain names are all composed entirely of one of the other the Complainants’ Marks followed by the dictionary terms “advertising,” “corporation,” “angel,” “data base,” “product,” “social media,” “inc” (standing for “incorporation”), “corp” (standing for “corporation”), and “ind” (standing for “industry”), or combinations of these dictionary terms or abbreviations.\nA domain name which wholly incorporates a complainant’s registered mark is sufficient to establish confusingly similarity for the purposes of the Policy when, as here, the addition to the Mark is a generic or dictionary term. WIPO Overview 3.0, section 1.7; and Nomura International Plc / Nomura Holdings, Inc. contre Global Domain Privacy / Nicolas Decarli, WIPO Case No. D2016-1535 (transferring <nomura-bank.com>). See also WIPO Overview 3.0, section 1.8 (“where the relevant trademark is recognizable with the disputed domain name, the additions of other terms (whether descriptive, geographic, pejorative, meaningless, or otherwise) would not prevent a finding of confusing similarity under the first element”); Nomura International Plc and Nomura Holdings, Inc. v. Whois Privacy Protection Service, Inc. / PUK SERVICES, WIPO Case No. D2015‑2036 (transferring <nomuralabuan.com>); and Nomura International Plc. v. Name Redacted, WIPO Case No. D2021‑0654.\nThe generic Top-Level Domain of the disputed domain names, in this case “.com”, may be disregarded for the purposes of assessment under the first element, as it is viewed as a standard registration requirement. See WIPO Overview 3.0, section 1.11.1; and Monster Energy Company, a Delaware Corporation v. J.H.M. den Ouden, WIPO Case No. D2016-1759 (transferring <monsterenergy.world>).\nThe Complainants have met their burden under paragraph 4(a)(i) of the Policy.\nB. Rights or Legitimate Interests\nThe Panel finds that the Respondent has no rights or legitimate interests in the disputed domain names.\nThe Complainants have specifically disavowed providing the Respondent with permission to use the disputed domain names or either of the Marks. There is no evidence that the Respondent has conducted any bona fide business under the disputed domain names or is commonly known by the disputed domain names. The Complainants have established a prima facie case in their favor, which shifts the burden of production on this point to the Respondent. The Respondent, however, has failed to come forth with any evidence showing any rights or legitimate interests in the disputed domain names. Furthermore, the nature of the disputed domain names, being nearly identical to the Mark, carries a high risk of implied affiliation. See WIPO Overview 3.0, section 2.5.1. The disputed domain names either resolve to a parking website or are inactive which further supports the conclusion that the Respondent has no rights or legitimate interests in the disputed domain names. Facebook, Inc. v. Mirza Azim, WIPO Case No. D2016-0950; and Guinness World Records Limited v. Solution Studio, WIPO Case No. D2016-0186. The facts and circumstances presented to the Panel demonstrate that the Respondent does not have any rights or legitimate interests in the disputed domain names.\nThe Complainants have met their burden under paragraph 4(a)(ii) of the Policy.\nC. Registered and Used in Bad Faith\n1. Under paragraph 4(b) of the Policy, bad faith may be established by any one of the following scenarios:\n(i) circumstances indicating that the respondent has registered or acquired the domain name primarily for the purpose of selling, renting, or otherwise transferring the domain name to the complainant who is the owner of the trademark or service mark or to a competitor of that complainant, for valuable consideration in excess of the respondent’s documented out-of-pocket costs directly related to the domain name; or\n(ii) the respondent has registered the domain name in order to prevent the owner of the trademark or service mark from reflecting the mark in a corresponding domain name, provided that the respondent has engaged in a pattern of such conduct; or\n(iii) the respondent has registered the domain name primarily for the purpose of disrupting the business of a competitor; or\n(iv) by using the domain name, the respondent has intentionally attempted to attract, for commercial gain, Internet users to its website or other on-line location, by creating a likelihood of confusion with the complainant’s mark as to the source, sponsorship, affiliation, or endorsement of the respondent’s website or location or of a product or service on the respondent’s website or location.\n2. The Panel finds the disputed domain names were registered and are being used in bad faith.\nA simple Internet search, normally undertaken before registering the disputed domain names, would have disclosed the Complainants’ well-known Marks. The Marks are not common or descriptive terms. Common sense compels the conclusion that the Respondent was quite aware of the Complainants’ Marks when registering and deploying the sixty-nine disputed domain names all of which contain one of the other of the Complainants’ Marks followed by a variety of dictionary terms.\nEven if one were to accept the unbelievable proposition that the Respondent was unaware of the Marks, willful blindness is no excuse and does not avoid a finding of bad faith registration and use. Instagram LLC v. contact Privacy Inc. / Sercan Lider, WIPO Case No. D2019-0419. The registration by the Respondent of the disputed domain names that are currently inactive is an abusive threat hanging over the head of the Complainant. Conair Corp. v. Pan Pin, Hong Kong Shunda International Co. Limited, WIPO Case No. D2014-1564.\nAdditionally, the Respondent registered the disputed domain names under a privacy shield which in itself does not establish bad faith use and registration, but which is a factor the Panel may consider. See WIPO Overview 3.0, section 3.6. It further appears that the Respondent has exhibited a pattern and practice of registering and using domain names composed of other well-known trademarks owned by major corporations, for example <airbusincorporated.com>, <barclaysbankinc.com>, <ebayincorporated.com>, <fordmotorinc.com>, and <googlechrometechnology.com>.\nFinally, it is difficult to conceive of any use that the Respondent might make of the sixty-nine disputed domain names without the Complainants’ consent that would not involve bad faith. Telstra Corporation Limited v. Nuclear Marshmellows, WIPO Case No. D2000-0003; Verner Panton Design v. Fontana di Luce Corp, WIPO Case No. D2012-1909 (where the reputation of a complainant in a given mark is significant and the mark bears strong similarities to the disputed domain name, the likelihood of confusion is such that bad faith may be inferred); DPDgroup International Services GmbH & Co. KG v. Wise One, Wilson TECH, WIPO Case No. D2021-0109 (transferring <dpdcouriercompany.com>); and Monster Energy Company v. PrivacyDotLink Customer 116709 / Ferdinand Nikolaus Kronschnabl, WIPO Case No. D2016-1335 (transferring <monsterenergy.club>).\nThe Complainants have met their burden under paragraph 4(a)(iii) of the Policy.\nFor the foregoing reasons, in accordance with paragraphs 4(i) of the Policy and 15 of the Rules, the Panel orders that the disputed domain names:\nbe transferred to the Complainant Facebook, Inc., and that the disputed domain names:\nbe transferred to the Complainant Instagram LLC.\nWilliam F. Hamilton\nDate: July 20, 2021']"	['<urn:uuid:69a920a7-0de2-45af-9dfe-f0c3a5f7a4de>', '<urn:uuid:644cc0f4-ec11-4ed5-aba2-1bf28ba91f62>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T22:13:20.996306	21	98	3224
9	What are the benefits of modern procurement for companies and what risks does it involve?	Modern procurement helps companies improve their financial performance by reducing fixed assets and debt levels through supply chain management. Instead of owning factories and warehouses, companies orchestrate thousands of suppliers globally, which improves key metrics like Return on Capital Employed and Fixed Asset Turnover, making businesses more attractive to investors. However, this reliance on complex supply chains introduces risks that need management. These risks include potential software coding errors throughout the supply chain, natural disasters, and epidemics. Organizations need automated monitoring systems, supplier transparency, and proper risk management plans to detect and address these threats before they impact operations or profitability.	['How Good Procurement Improves The Balance Sheet\nProcurement wins friends because it can deliver additional profit to the bottom line, but that is not enough. To satisfy investors procurement goes one step further. This is important for sellers to understand, it can help transform a sales proposal into a compelling business proposal.\nWhy Investors Love Procurement\nTo understand the full impact of procurement you have to distinguish between profit and return on investment. You have to look beyond the P&L account to the balance sheet.\nTypically the biggest item on the balance sheet is fixed assets, otherwise known as the amount of money invested in property, plant and equipment (PP&E). This investment has to be funded either through shareholder equity, or debt and it is an area where Procurement can have a major impact.\nReturn On Assets\nAssets or investment don’t come free – they demand a return. If it takes one company 100 million of assets and debt to generate 5 million in profit, while another can generate the same level of profit with an investment of only 50 million, it is clear where shareholders would rather place their money.\nAt its simplest an organisation with assets of 100 million needs to generate twice as much profit as that with assets of 50 million, if they are to be scored equally in terms of Return On Investment or ROCE. Procurement’s ability to re-engineer ROI makes it a darling of the stock exchange.\nInvesting In A Supply Chain\nToday’s large corporations build and manage supply chains, rather than factories and warehouses. Take the Ford Corporation – it once owned the steel mills and rubber plants needed to supply its car manufacturing plans. But such a vertical mammoth is now inconceivable.\nModern procurement orchestrates thousand’s of tier one suppliers and perhaps tens of thousands of tier 2, tier 3 and raw material suppliers located around globe. It is these supply chain partners who own the plant and equipment, perhaps even hold the stock.\nThe move from vertical integration to horizontal supply chain has enabled corporations to reduce debt levels (and the burden of interest repayments) and to fund strategies for growth (such as innovation, new products, new markets, etc.). This is key to procurement’s ability to deliver superior business performance.\nImproving Key Business Ratios\nThe shift from vertically to horizontally integrated organisation, and the rise in outsourcing, global logistics and the supply chain, has enabled organisations to divest themselves of considerable amounts of property, plant and equipment (PP&E).\nReducing PP&E means procurement can improve those metrics used by investors and analysts to evaluate corporate performance:\n- Return On Capital Employed which measures the efficiency of the business based profits expressed as a percentage of total investment\n- Fixed Asset Turnover which tracks the efficiency of growth in the form of net sales expressed as a percentage of PP&E property, plant and equipment (PP&E)\nIn so doing procurement makes the business more attractive to investors\nImplications for sellers\nProcurement’s impact on business performance gives it tremendous clout within the organisation. That is another reason why avoiding procurement doesn’t make sense.\nThe Procurement function is not just on a smash and grab raid in respect of seller margins, it has a deeper strategy in terms of its contribution to the financial performance of the organisation. So, engaging with procurement around numbers is key.\nBut how can sellers help procurement achieve its overall goals in terms of freeing up working capital, de-leveraging the business, and boosting ROI?\nWe recommend you examine the impact of your sales proposal in terms of its impact on all these variables – Is your proposal working capital and asset neutral, or positive for example?\nAddressing the buyer’s primary ratio is key. By doing so, you will be directly addressing the CEO and CFO agenda.', 'Supply Chain Risk Management\nWhat is Supply Chain Risk Management?\nSupply Chain Risk Management Definition: Supply chain risk management (SCRM) involves monitoring for and identifying potential threats to the supply chain. Although there is no way to completely eliminate risk, many issues can be avoided and contingency plans can be put into place to minimize or mitigate the impact on operations and/or profitability.\nSupply Chain Risk Management Examples: Generally speaking, there are two types of risk associated with supply chains. There are things that are impossible to predict and plan for, such as natural disasters, epidemics, and acts of terrorism, and there are other things that can be planned for, provided an organization has the data necessary to observe and act on early indicators. This may include:\nThe Role of Software Risk Management in Supply Chain Reliability\nBecause there are many potential issues that can impact the supply chain, executives have the difficult duty of assessing which problems are likely to cause the most harm to operations and profitability. Although cyber security is a core focus, there are numerous ways software can impact the reliability and profitability of a supply chain and not all of them are apparent, even to those in IT. While there is some risk involved in the hacking or insertion of malicious code in software, most issues surround unintentional coding errors, which routinely go undetected. As the infrastructure and architecture grow, these errors can create vulnerabilities and cause unexpected conflicts. By identifying these issues in advance and taking steps to reduce their impact, the risk is effectively managed prior to breakdown of the chain. Bear in mind, these errors can occur at a local level as well as at any stage in the supply chain, which is why it’s important for organizations to work with vendors and companies that have systems and transparency in place.\nSupply Chain Risk Management Best Practices\nAutomation: Supplier risk management (SRM) processes, including the collection, management, and analysis of data, should be automated. Human eyes cannot always detect errors, let alone conflicting codes within a system.\nSupplier Transparency: Performance information from suppliers should be readily available and included in analysis.\nEarly Detection: The supply chain risk management process should make use of automation software detect potential red flags before they become a problem.\nClassification: All potential risks should be identified, classified, and prioritized in order to determine which corrective actions offer the greatest ROI.\nPlans & Contingency Plans: A supply chain risk management plan should be developed and include methods for managing imminent risks as well as for minimizing the impact of issues that cannot be prevented or would be cost-prohibitive to prevent.\nMonitoring: Supply chain risk management tools should be used to identify new triggers, even after an assessment and corrections have been carried out.\nCollaboration: SCRM metrics and information should be available to various departments and heads at the same time for easy collaboration regarding supply chain risk management strategies.\nLeadership: SCRM should be overseen by one leader, even if duties are delegated to a team. Historically, organizations that assign a lead, usually an executive or VP, to handle SCRM see the greatest ROI. These organizations are also the only ones to see in excess of 100% ROI on their SCRM efforts.']	['<urn:uuid:7ef13a60-3a7f-409f-bd16-0cfe7f7f769d>', '<urn:uuid:08a27c68-0dd9-4293-992b-f4e82c905629>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T22:13:20.996306	15	101	1178
10	For my upcoming construction project, we need to establish clear data requirements. What specific information should be included in the model at each exchange point, and who typically defines these requirements?	The employer defines the level of detail required at each stage in the Employer's Information Requirements (EIR). This includes two components: Levels of model detail (LOD) for graphical content, and Levels of model information (LOI) for non-graphical content. These requirements are typically made contractually binding through a BIM protocol. Data drops or information exchanges occur at key project stages, allowing the employer to verify that project information meets their requirements and to make decisions about proceeding to the next stage. While there's no standardized definition for the timing of data drops, they should align with employer decision points and be consistent across all appointments.	"[""Level of detail for BIM\nThe level of detail of a building information model increases as the project proceeds, often based in the first instance on existing information, then developing from a simple design intent model through to a detailed virtual construction model, then an as-constructed asset information model (AIM). Different aspects of the model may develop at different rates, may originate with different members of the project team, and their development may pass from the employer, to consultants, to the contractor and suppliers and ultimately back to the employer.\nIt is important therefore that the employer defines the level of detail that is required at each stage of development of the project. This not only ensures that the design is developing in sufficient detail, but also that the information required by the client to make decisions about the project development and then to operate the completed project efficiently, is actually provided. It also gives an indication of the reliance that can be placed on information.\nThe employer defines the level of detail that is required in the Employer's Information Requirements (EIR). The Employers Information Requirements might be appended to a BIM protocol, incorporated into the contract by addition of a 'model enabling amendment', making the delivery of required information a contractual obligation.\nPAS 1192-2 Specification for information management for the capital/delivery phase of construction projects using building information modelling (now replaced by BS EN ISO 19650) defines two components to the 'level of definition':\n- Levels of model detail (LOD), which relates to the graphical content of models.\n- Levels of model information (LOI), which relates to the non-graphical content of models.\nIn fact, the two are closely aligned as it is normal for graphical and non-graphical content to develop alongside one another.\nThe levels of model detail and model information are generally defined for key stages of the project, at which 'data drops' (information exchanges) take place, allowing the employer to verify that project information is consistent with their requirements and enabling them to decide whether to proceed to the next stage. This is analogous to a stage report on a conventional project.\nAs present, there is no standardised definition for the timing of data drops or for levels of model detail and model information, other than the suggestion that they should be aligned to employer decision points and should be consistent across all appointments. This is because it is thought they will vary depending on the nature of the project. However, some very broad guidance is given in PAS 1192-2:\n- Brief: If a graphical model exists it is likely to have been developed from an existing asset information model. Other information might relate to existing buildings and structures (there may also be schedules of requirements).\n- Concept: The graphical design may show massing diagrams and 2D symbols to represent generic elements.\n- Definition: Objects are based on generic representations, and specifications and attributes allow the selection of products.\n- Design: Objects are represented in 3D with the specification attached along with information about space allocation for operation, access, maintenance, installation and replacement.\n- Build and commission: Generic objects are replaced with manufacturers objects, with essential information re-linked to the replacement objects and manufacturer information added.\n- Handover and close-out: The model represents the as-constructed project and all necessary information is included in handover documentation, including maintenance and operation documentation, commissioning records, health and safety requirements and so on.\n- Operation and in-use: Performance is verified against the Employer's Information Requirements and the project brief and if changes are necessary, the model is updated. Information about maintenance, replacement dates, and so on may be added.\nThe American Institute of Architects (AIA) has also published a LOD framework for the AIA G202-2013 Building Information Modelling Protocol Form. Here LOD refers to the 'Level of Development' required for model element content. The term 'level of development' is used rather than 'level of detail' in recognition of the fact that a visually very detailed element might in fact be generic and despite appearances might be at a low level of design development.\nThe AIA suggest that the LOD framework recognises that different elements of the project will develop at different rates and '…allows the Project Participants to efficiently communicate to one another the extent to which a Model Element has been developed … It also allows the Project Participants to communicate the extent to which a Model element may be used and relied on…'\n- LOD 100: The Model Element may be graphically represented in the Model with a symbol or other generic representation, but does not satisfy the requirements for LOD 200. Information related to the Model Element (i.e., cost per square foot, tonnage of HVAC, etc.) can be derived from other Model Elements.\n- LOD 200: The Model Element is graphically represented within the Model as a generic system, object, or assembly with approximate quantities, size, shape, location, and orientation. Non-graphic information may also be attached to the Model Element.\n- LOD 300: The Model Element is graphically represented within the Model as a specific system, object, or assembly in terms of quantity, size, shape, location, and orientation. Non-graphic information may also be attached to the Model Element.\n- LOD 400: The Model Element is graphically represented within the Model as a specific system, object or assembly in terms of size, shape, location, quantity, and orientation with detailing, fabrication, assembly, and installation information. Non-graphic information may also be attached to the Model Element.\n- LOD 500 The Model Element is a field verified representation in terms of size, shape, location, quantity, and orientation. Non-graphic information may also be attached to the Model Elements.\n Related articles on Designing Buildings Wiki\n- BIM articles.\n- BIM dimensions.\n- BIM glossary of terms.\n- BIM maturity levels.\n- BIM Task Group.\n- Construction Operations Building Information Exchange.\n- CIC BIM Protocol.\n- Data drops.\n- Employers information requirements.\n- Federated building information model.\n- Information manager.\n- LOD: Linguistics Needs of the Construction Industry.\n- PAS 1192-2:2013.\n- Plain language questions.""]"	['<urn:uuid:cfe44750-bcdd-4e44-b0ef-6e8e9fe7758b>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T22:13:20.996306	31	104	1010
11	how do modified bacteria locate recognize destroy tumors inside body	The engineered bacteria use multiple mechanisms to target tumors. They have an oxygen sensor from E. coli linked to an invasin protein from another bacteria, allowing them to detect tumor tissue's unique characteristics like low oxygen and high lactic acid concentrations. The bacteria then invade cancer cells specifically when these conditions are met. Once inside, they can produce enzymes that convert drug precursors into cancer-killing compounds. The bacteria are also designed to colonize tumor microenvironments and can lyse on demand through genetic clockwork mechanisms to release therapeutic substances.	['Scientists are synthetically engineering E. coli that can target and kill cancer cells.\nImagine a bacterium that, when injected into the bloodstream, would travel to the site of a tumor, insert itself into the cancer cell, and then produce a cancer-killing compound. That’s exactly what scientists at the University of California, Berkeley (UCB) and University of California, San Francisco (UCSF) have set out to do.\nTraditional cancer therapies are limited for two key reasons: little of the drug actually reaches the tumor and the drug is toxic to both cancerous and healthy tissues. Bacteria, however, have the potential to precisely target cells. “In a way, bacteria are the ultimate in smart drugs,” says George Church, a geneticist at Harvard Medical School in Boston (he was not involved in the current work, but will collaborate on the project in the future). “It’s hard to pack a lot of intelligence into a small molecule or protein; but bacteria can have sensors and actuators and can drill into a cell, like a submarine.”\nTo build a cancer-killing bacterium, biologists must create organisms that can perform a series of complicated functions – namely, when in the bloodstream, they have to sense and respond to the tumor environment. Once inside the tumor, the bacteria must infiltrate the cancer cell, and then – and only then – start producing a tumor-killing toxin. The researchers plan to engineer such super-organisms by co-opting parts from different types of bacteria and inserting them into Escherichia coli, a bacterium commonly used in research.\nTumor tissue has unique characteristics, including lower oxygen and higher lactic acid concentrations than surrounding tissue. To create a bacterium that can sense a tumor, Christopher Anderson, a postdoctoral researcher at UCB and UCSF, and colleagues took an oxygen sensor from E. coli and linked it to a special protein, called invasin, from another type of bacteria, which allows the organism to invade cancer cells. In a paper published earlier this year in the Journal of Molecular Biology, the researchers showed in a test tube that the engineered bacterium selectively invades tumor cells.\nAnderson and colleagues are now working on making the system even more specific. To ensure that the bacteria invade only tumor cells, they will create a genetic mechanism that allows the invasin protein to be expressed only when two conditions are met, such as when both the oxygen and lactic acid concentrations are at a certain level. Essentially, it’s a genetic version of what’s known in engineering terms as an AND gate – a regulatory circuit that’s turned on only if two conditions are met.\n“By using multiple cues, we can garner a great deal of specificity,” says Adam Arkin, a bioengineer at UCB and the Lawrence Berkeley National Laboratory, a TR100 recipient in 1999, and one of the senior scientists on the project. “After the bacteria sense the cues, they turn on the rest of the apparatus to do the job.”\nThe last step, which the biologists haven’t yet started, will be to engineer the bacteria to produce an enzyme that converts a drug precursor into a cancer-killing compound. The patient would take a pill containing the precursor, which would be converted into the active drug only inside the bacterially infected cancer cells.\n“The bacteria are engineered to sense cells and then invade – that’s a unique demonstration of what you can do by programming cells,” says James Collins, a biomedical engineer at Boston University (who was not involved in the project).\nAnderson and colleagues still face many technical and safety hurdles before they have a viable bacterial cancer treatment. For example, the human immune system attacks bacteria when they’re injected into the bloodstream. So the researchers engineered the bacteria to produce a special lipid coating that makes the organism invisible to part of the immune system.\nThe team is also designing a series of safeguards into the system. Patients could develop sepsis, for example, if the bacteria reproduced too quickly in the blood. Previous research has shown that bacteria need iron from human blood to grow and reproduce. So the scientists deleted a certain gene that allows the organisms to extract iron from the blood, which should drastically limit bacterial growth inside the body.\nWhile the idea of bacterial therapies may sound highly experimental – and even frightening – it has existed for at least a century, and it’s already been put into practice. The Bacillus Calmette-Guerin (BCG) vaccine, a preventative treatment for tuberculosis made from a live but weakened version of the tuberculosis bacterium, is also effective in treating bladder cancer when injected directly into the bladder. Use of this therapy, however, is limited to cancers lining the surface of the bladder, because the bacteria can easily access these cancer cells.\nIn addition, some strains of bacteria have a natural affinity for tumor cells, a quality that scientists have tried to take advantage of when designing new therapies. Vion Pharmaceuticals, based in New Haven, CT, is developing a cancer treatment centered on the salmonella bacteria, which have this property. Anderson’s bacteria work differently, though: they actively target tumor cells through the invasin protein, rather than relying on the bacteria’s natural mechanism, and therefore may be less sensitive to differences in tumor types.\nStill, the latest work is likely to require a lot of scrutiny from the Food and Drug Administration. Anderson emphasizes that, at least at first, these bacteria would be given only to people who’ve failed to benefit from other treatments.\nBecome an Insider to get the story behind the story — and before anyone else.', 'The Trojan Bacillus: Transgenic Bacteria in Cancer Therapy\nKeywords:Synthetic biology, Oncology, Targeted therapy\nA classic conundrum in oncology is the identification of cancer-specific, druggable molecules which can be targeted with minimal systemic toxicity. A novel candidate for administering cancer therapeutics has emerged in bacteria, which may prove to be excellent delivery vehicles for biologics whose systemic delivery causes severe and unacceptable adverse effects. Bacteria are well-suited to this role due to their ability to colonise tumour microenvironments, synthesise drug molecules, and potentiate innate and adaptive immune responses. Genetic clockwork in the form of quorum sensing mechanisms allows these bacteria to lyse on demand, releasing therapeutic payloads into tumours. Recent in vivo evidence outlined here support this hypothesis, yet there is a great deal of research and refinement still to be done.\n2. Forbes NS. Engineering the perfect (bacterial) cancer therapy. Nat Rev Cancer [Internet]. 2010;10(11):785–94. Available from: http://dx.doi.org/10.1038/nrc2934\n3. Chowdhury S, Castro S, Coker C, Hinchliffe TE, Arpaia N, Danino T. Programmable bacteria induce durable tumor regression and systemic antitumor immunity. Nat Med [Internet]. 2019;25(7):1057–63. Available from: http://dx.doi.org/10.1038/s41591-019-0498-z\n4. Gurbatri CR, Lia I, Vincent R, Coker C, Castro S, Treuting PM, et al. Engineered probiotics for local tumor delivery of checkpoint blockade nanobodies. Sci Transl Med. 2020;12(530).\n5. Abisado RG, Benomar S, Klaus JR, Dandekar AA, Chandler JR. Bacterial quorum sensing and microbial community interactions. MBio. 2018;9(3):1–14.\n6. Mukherjee S, Bassler BL. Bacterial quorum sensing in complex and dynamically changing environments. Nat Rev Microbiol [Internet]. 2019;17(6):371–82. Available from: http://dx.doi.org/10.1038/s41579-019-0186-5\n7. Papenfort K, Bassler BL. Quorum sensing signal-response systems in Gram-negative bacteria. Nat Rev Microbiol [Internet]. 2016;14(9):576–88. Available from: http://dx.doi.org/10.1038/nrmicro.2016.89\n8. Danino T, Mondragón-Palomino O, Tsimring L, Hasty J. A synchronized quorum of genetic clocks. Nature. 2010;463(7279):326–30.\n9. Din MO, Danino T, Prindle A, Skalak M, Selimkhanov J, Allen K, et al. Synchronized cycles of bacterial lysis for in vivo delivery. Nature [Internet]. 2016;536(7614):81–5. Available from: http://dx.doi.org/10.1038/nature18930\n10. Lee YJ, Jeong KJ. Challenges to production of antibodies in bacteria and yeast. J Biosci Bioeng [Internet]. 2015;120(5):483–90. Available from: http://dx.doi.org/10.1016/j.jbiosc.2015.03.009\n11. Bannas P, Hambach J, Koch-Nolte F. Nanobodies and nanobodybased human heavy chain antibodies as antitumor therapeutics. Front Immunol. 2017;8(1603):1–13.\n12. Hu Y, Liu C, Muyldermans S. Nanobody-based delivery systems for diagnosis and targeted tumor therapy. Front Immunol. 2017;8(NOV):1–17.\n13. Darvin P, Toor SM, Sasidharan Nair V, Elkord E. Immune checkpoint inhibitors: recent progress and potential biomarkers. Exp Mol Med [Internet]. 2018;50(12):1–11. Available from: http://dx.doi.org/10.1038/s12276-018-0191-1\n14. Kwang Chae Y, Arya A, Lams W, Cruz M, Chandra S, Choi J. Current landscape and future of dual anti- CTLA4 and PD-1/PD-L1 blockade\nimmunotherapy in cancer; lessons learned from clinical trials with melanoma and non- small cell lung cancer (NSCLC). J Immunother Cancer. 2018;6(39):1–27.\n15. Postow MA, Chesney J, Pavlick AC, Robert C, Grossmann K, McDermott D, et al. Nivolumab and ipilimumab versus ipilimumab in untreated melanoma. N Engl J Med. 2015;372(21):2006–17.\n16. Toso JF, Gill VJ, Hwu P, Marincola FM, Restifo NP, Schwartzentruber DJ, et al. Phase I study of the intravenous administration of attenuated Salmonella typhimurium to patients with metastatic melanoma. J Clin Oncol. 2002;20(1):142–52.\n17. Duong MTQ, Qin Y, You SH, Min JJ. Bacteria-cancer interactions: bacteria based cancer therapy. Exp Mol Med [Internet]. 2019;51(152). Available from: http://dx.doi.org/10.1038/s12276-019-0297-0\nHow to Cite\nAuthors retain copyright and grant the journal the right of first publication with the work simultaneously licensed under a Creative Commons Attribution (CC-BY) 4.0 License that allows others to share the work with an acknowledgement of the work’s authorship and initial publication in this journal.\nProvided they are the owners of the copyright to their work, authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal’s published version of the work (e.g., post it to an institutional repository, in a journal or publish it in a book), with an acknowledgement of its initial publication in this journal.']	['<urn:uuid:6dac8eba-e3e6-43cf-9fd2-9f3431124a06>', '<urn:uuid:3b881b77-4708-4ca0-8ec6-eebbaf35b643>']	factoid	direct	long-search-query	distant-from-document	three-doc	novice	2025-05-12T22:13:20.996306	10	88	1568
12	As a construction engineer, what's a module's definition and its benefits in Egyptian housing?	A module is a self-contained component, often interchangeable, used in combination with other components with well-defined interfaces. In Egyptian affordable housing projects, modular systems like Light Gauge Steel (LGS) and Glass Reinforced Concrete (GRC) demonstrate significant environmental benefits, reducing solid resource usage by 66% and 46% respectively compared to conventional construction.	"[""Definitions for Moduleˈmɒdʒ ul\nThis page provides all possible meanings and translations of the word Module\nRandom House Webster's College Dictionary\na separable component, frequently one that is interchangeable with others, for assembly into units of differing size, complexity, or function.\nany of the self-contained segments of a spacecraft.\na standard or unit for measuring.\nCategory: Weights and Measures\na selected unit of measure used as a basis for the planning and standardization of building materials.\nCategory: Weights and Measures\nOrigin of module:\n1555–65; < L modulus\nfaculty, mental faculty, module(noun)\none of the inherent cognitive or perceptual powers of the mind\ndetachable compartment of a spacecraft\ncomputer circuit consisting of an assembly of electronic components (as of computer hardware)\na self-contained component (unit or item) that is used in combination with other components\nA self-contained component of a system, often interchangeable, which has a well-defined interface to the other components.\nA standard unit of measure used for determining the proportions of a building.\nA section of a program; a subroutine.\nA unit of education covering a single topic.\nA pre-prepared adventure scenario with related materials for a role-playing game.\nAn abelian group.\nK-module, module over K\nAn algebraic structure which behaves just like a vector space over a field F, except that F is replaced by K, a commutative ring with unit.\nAny module extends easily into a uE000133009uE001-module.\nA file containing a music sequence that can be played in a tracker (called also mod or music module).\n(hydraulics) A contrivance for regulating the supply of water from an irrigation channel.\nOrigin: From module, from modulus, diminutive of modus; see mode.\na model or measure\nthe size of some one part, as the diameter of semi-diameter of the base of a shaft, taken as a unit of measure by which the proportions of the other parts of the composition are regulated. Generally, for columns, the semi-diameter is taken, and divided into a certain number of parts, called minutes (see Minute), though often the diameter is taken, and any dimension is said to be so many modules and minutes in height, breadth, or projection\nto model; also, to modulate\nIn abstract algebra, the concept of a module over a ring is a generalization of the notion of vector space over a field, wherein the corresponding scalars are the elements of an arbitrary ring. Modules also generalize the notion of abelian groups, which are modules over the ring of integers. Thus, a module, like a vector space, is an additive abelian group; a product is defined between elements of the ring and elements of the module that is distributive over both parameters and is compatible with the ring multiplication. Modules are very closely related to the representation theory of groups. They are also one of the central notions of commutative algebra and homological algebra, and are used widely in algebraic geometry and algebraic topology.\nBritish National Corpus\nSpoken Corpus Frequency\nRank popularity for the word 'Module' in Spoken Corpus Frequency: #3024\nRank popularity for the word 'Module' in Nouns Frequency: #785\nTranslations for Module\nKernerman English Multilingual Dictionary\na self-contained unit forming eg part of a building, spacecraft etc\na lunar module.\n- وِحْدَه، مَرْكَبَه قَمَرِيَّهArabic\n- móduloPortuguese (BR)\n- die KapselGerman\n- modul; -modulDanish\n- نمونه پیش ساختهFarsi\n- egység, modulHungarian\n- unit tersendiri, modulIndonesian\n- autonoms nodalījumsLatvian\n- enhet, modulNorwegian\n- نمونه پیش ساختهPersian\n- مخكى جوړه شوى نمونهPashto\n- modul, [maskin]enhetSwedish\n- bölüm, modülTurkish\n- (建築物的)模組，(太空船的)船艙Chinese (Trad.)\n- کسی عمارت یا خلائی جہاز کا علیحدہ حصہUrdu\n- bộ phận tháo rờiVietnamese\n- 模块，（宇宙飞船上各个独立的）舱Chinese (Simp.)\nGet even more translations for Module »\nFind a translation for the Module definition in other languages:\nSelect another language:"", 'Lifecycle Assessment LCA comparison for different prefabricated modular construction systems with the conventional construction system for affordable houses in Egypt\nThe British University in Dubai (BUiD)\nEgypt is considered as the most populated country in the Arab world with its high population that exceeds 94 million (CIA, 2017), 62.6% of them are in working age and the young generation forms 19.24% of the overall population. This overpopulation puts a huge pressure on many sectors including affordable housing which suffers from severe supply shortage. All conventional housing solutions that the different governments have put to solve this issue along the last 70 years, they are still fare away from being affordable to the majority of population in many cities in Egypt. This research focuses on studying the environmental viability of introducing prefabricated modular systems as an alternative sustainable solution to the current conventional construction system in affordable housing sector. To achieve the goals of this research, a comparison was conducted between four prefabricated and modular construction systems to the conventional construction system and studied their different impacts on the environment. The conventional construction system that is widely used in Egypt is cast in situ and masonry work. In this research, this conventional construction has been compared to the following four prefabricated construction systems: 1) Pre-cast Concrete PCC, 2) Glass Reinforced Concrete GRC, 3) Light Gauge Steel LGS, 4) Adapted Shipping Containers. Life Cycle Assessment (LCA) has been used as the research methodology to compare the environmental impact of the four construction system to the conventional system which was used as the base case for this comparison. This research aims to assess these prefabricated systems that exist in Egypt and evaluate their impacts on environment from the perspective of the used resources and land emissions over the life-cycle of each system, total primary energy consumption and global warming potentials over the life cycle of the building. Five building models, one for each of the previous mentioned construction systems, have been designed and used in the LCA analysis. All the 5 models share the same physical dimensions and the same spatial components for the building under study but vary in their construction materials. The building is composed of a 4-storey building that holds 3 residential units per floor. Athena Impact Estimator was the software used for the LCA assessment tool and eQUEST was the energy analysis software used. After running the LCA analysis we concluded that among the four construction systems and over the 50 years of their life cycle, both GRC and LGS options proved a lot of reduction in their impacts on environment. During the production and the construction stages as compared to the convention construction system, LGS and GRC reduced 66% and 46% of the used solid resources respectively. These reductions are due to the reduction in the materials mass used in these 2 construction systems. Accordingly and for the same reasons, the land emissions for the same systems (amount of waste generated along the life cycle of the building), have been reduced as well. LGS saved 60% while GRC saved 54% of the amount of waste that the conventional system produced. For the total primary energy consumption, LGS saved 40% of the total embodied energy compared to the conventional construction system while GRC saved 18%. For the Green-House Gas GHG emissions, LGS saved 64% while GRC saved 25% of the embodied emissions that the conventional system produced. With their reduced environmental impacts, these two prefabrication systems are recommended to be used for the affordable housing projects in Egypt. Besides all the environmental benefits of these two systems, their prefabrication process in factories improves quality, reduces construction time and saves cost due to the economy of scale of the large housing projects especially those which are supported by the government.\nLifecycle Assessment (LCA), construction systems, affordable houses, Egypt, energy consumption']"	['<urn:uuid:2e4305c4-fc7c-4902-a3a2-72b7b48527c2>', '<urn:uuid:2a50db62-6ee0-42f0-9955-f7b6e8595d1f>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T22:13:20.996306	14	51	1256
13	what are explosive vs on schedule route combinations in football plays	Explosive route combinations are aggressive downfield shot plays like Verticals, Dagger and Mills that don't succeed as often but generate big gains when they do. On Schedule plays like Stick, Outs, and Curl/Flat are more frequently successful but generate smaller gains, and are typically used to start drives, get quarterbacks in rhythm, or convert short third downs.	['Last week the Sports Info Solutions Data Challenge came to its conclusion, as the judging portion of the competition took place on Wednesday night. The challenge tasked teams with taking a dataset of play-by-play data provided by SIS and answering two questions. First, what was the most popular route combination in the NFL last season? Second, what was the most successful route combination in the NFL last season against each type of coverage?\nI was lucky enough to pair up with a brilliant team. Before going any further, I strongly recommend that you follow all three: Dr. Bud Davis, Keegan Abdoo and Joey Ferraiola. They’re brilliant, and approximately 98% of what follows is due to their work.\nFor those interested, you can view the judging portion of the finals here:\nBut I thought it might be interesting to look at the findings with respect to the second prompt: Which route combinations work best against each coverage. What follows is a discussion of the methodology our group used to answer this question, our findings for each coverage and film examples to illustrate the concepts.\nBefore diving into the data a few words on the methodology. To begin, we built a “route concept dictionary.” This was based on playbook study and analysis, and eventually included 36 different route concepts, some of which you can see above. This dictionary enabled us to perform a first-pass identification of each of the route concepts on the plays provided.\nOn the first-pass, we were able to identify route concepts for 11,580 plays of the data set. However, that left 4,768 plays, or approximately 30% of the snaps, unidentified.\nThis is where the fun began.\nDr. Davis then constructed a Convolution Neural Network. This model used machine learning to identify route concepts within the plays provided:\nThe way this worked was by looking at each route concept, building a graphical representation of the route, and then comparing it to other route combinations and offering anywhere from one to three possibilities for the route concept identification. Here are some examples of how the CNN model identified routes that the route dictionary failed to uncover:\nThis method got us much closer to the full identification, although there were some errors. Most errors were either false positives — wherein a route concept identified was not labeled by the route concept dictionary — or false negatives — wherein the CNN failed to identify the route concept. Still, we were much closer to identifying all the plays, so it was time for some analysis.\nThat led us to this grid of each concept versus the charted coverage data, in terms of Expected Points Added per play:\nBefore diving in further, a word on sample sizes. There was not a ton of data to really go with here. Some plays were executed against a given concept just a handful of times, as such we excluded instances with less than 25 occurrences for sample-size issues. So some of the pairings you will see happened just 25 times last season, which is interesting, but likely not enough data to form concrete, definitive conclusions. But in the next few seasons I will be paying close attention to these pairings.\nCover 0: Y-Cross\nAs you can see from the above diagram, the most successful concept against Cover 0 was Y-Cross.\nNow, this also needs further context. In our sample size Y-Cross was defined as an inside receiver running the crossing route over the middle. This was our only single-receiver concept, which in part led to this design being the most frequently observed concept out of all the plays examined.\nGiven that context, however, the Y-Cross came up as the most successful route combination. This example, tagged in our analysis as Y-Cross and charted as a Cover 0 play by SIS, provides some reasoning:\nAs you can see, Cover 0 is often a blitz package, with the defense sending pressure and playing man across the board in the secondary with no safety help. That is exactly what plays out here, with the New York Giants sending pressure after Ben Roethlisberger and playing without safety help in the secondary, and using man coverage across the board.\nJuJu Smith- Schuster runs the crossing route — leading to the Y-Cross tag in our analysis — and without safety help in the middle of the field, he simply crosses the face of the man defender. Then you see a scenario play out: “If you play Cover 0 and you don’t get home, the other band is going to play.” Rod Gilmore said that in the booth once, and that has stuck with me. The Giants do not get to the QB, and Roethlisberger finds Smith-Schuster for the TD.\nCover 1: Drive\nNext we look at Cover 1, which combines man coverage across the board with a free safety in the middle of the field. Our analysis found that against Cover 1, the most successful route combination was the Drive concept, which by our definition pairs a dig route from an inside receiver with a shallow crossing route from the outside receiver.\nHere is an example of Drive run against Cover 1, thanks to the Seattle Seahawks:\nOn this play the Seahawks use pre-snap motion to help identify the coverage, as Tyler Lockett motions in towards the middle of the field, and then back to the outside. The cornerback trails him the whole way, so Russell Wilson knows man coverage is in play. Lockett then runs the shallow while tight end Greg Olsen runs the dig. Because Wilson knows it is man coverage, and the release of the tight end helps create some traffic, Lockett is open on the shallow route for a gain of five.\nCover 2: Diagonal Nine\nNext up is Cover 2. Before diving into Diagonal Nine, and why that might have been an easy choice before seeing the data, another word on methodology. As touched upon in an earlier image, we grouped vertical routes such as “out and up,” “fade,” and “chip-seam” all under the bigger umbrella of “vertical routes.” The reason being that all routes that ended up attacking vertically stressed the same areas of the field, and that would give us a wider look at concepts rather than narrowing things down.\nBut we found that Diagonal Nine, which pairs a go route from an outside receiver with a route to the flat from an inside receiver, was the most successful against Cover 2. Here is an example from the Seahawks against the New York Giants, and keep that methodology in mind as you watch this clip, a play identified as both Diagonal Nine and Cover 2 in the dataset:\nYou can probably see why I put the caveat about the methodology at the start.\nOn this play Lockett runs an out-and-up, which gets tagged as a vertical route in our database. Running back Chris Carson chips and then releases to the flat, and that combination led the CNN model to identify this play as a Diagonal Nine, with Lockett on the vertical and Carson into the flat.\nStill, the reason why Diagonal Nine (or Go/Flat in other systems) is a stress for Cover 2 is on display here. The go route attacks the soft spot of Cover 2, the “turkey hole” deep along the sideline. The flat route works to hold the cornerback in place, expanding the window. The safety has to race to the sideline to make a play on the football. Complicating matters here is the fact that Seattle runs this otu of a three-receiver set, and D.K. Metcalf is running a post route from the outside alignment. That puts the safety to that side in a bind: Buzz outside for Lockett and you expose the post. Stay home and stick on the post, and you expose the vertical.\nHere, the safety tries to read Wilson and react. He gets there too late to prevent the completion.\nAnother reason why this is the most dangerous combination against Cover 2 is this: Think of all the different ways defenses try and stop Diagonal Nine/Go Flat from Cover 2 looks. Teams can run Palms or 2-Trap, where the cornerback reads the break of the inside receiver and if he sees that player break to the flat, he will peel off the vertical release and “trap” the flat route. Another viscous coverage is something termed 5 Cougar, where the cornerback actually executes a man coverage turn, showing the quarterback his back as he breaks with the vertical. But the cornerback is still reading the break of the slot receiver, and if he breaks out, he’ll bail off the vertical and trap the flat route. (For a deep dive into this I wrote about that coverage two summers ago):\nIf defenses are tweaking a coverage because of what a specific route combination can do against it, you’re probably right in thinking that combination works against the base coverage.\nCover 2 Man Underneath: Verticals\nSpeaking of Cover 2 variations, we come to Cover 2 Man Under, which retains the two-deep safety structure over the top of the defense, but utilizes man coverage underneath. In terms of context it is important to remember, as coach Rick Neuheisel pointed out as one of the judges, that this coverage is often used in third-and-long situations. Downs and distances where the deck is already stacked against the defense. That illustrates in part why Cover 2 Man Under was the most successful coverage out of any in the data set.\nBut the play that works against it the best, in our study, was Verticals. The reasoning is similar to the previous analysis of Cover 2, in that the outside vertical route has t he chance to threaten the weak spot of the coverage, that “turkey hole.” In addition, Verticals against Cover 2 Man Under also finds an inside vertical route attacking the hole in the\nmiddle of the field between the safeties, with a receiver working against a man coverage defender. If that receiver can get a step on that defender, he should find space to work in the middle of the field.\nThat is exactly what happens here, as Gardner Minshew hits his tight end for a touchdown against Cover 2 Man Under:\nTampa 2: Portland\nNext up is Tampa 2, another member of the Cover 2 family that tasks a defender, often a linebacker, to be the “middle field runner.” He will drop deep to help the safeties by dropping into the middle, helping protect the soft spot between them.\nThe route concept that we found works best against Tampa 2 is what we termed Portland, which has a post route from one side of the field with a crossing route from the other. This could also be referred to as the Yankee concept. Whether Portland or Yankee, this is usually a play called out of a heavy formation and off of play-action, looking to hit the defense deep.\nOn this example the Las Vegas Raiders call Portland on a 2nd-and-7 situation against the Tampa Bay Buccaneers out of 21 offensive personnel. The Buccaneers have a base 3-4 defense in the game. You will see linebacker Devin White drop deep with the post route from Henry Ruggs III, and that creates space — along with the run fake — for the crosser to Nelson Agholor:\nWhether Portland or Yankee, this design is often thought of as a single-high beater, as it puts the post safety in a box. He either drops to take away the post, and crashes down on the crosser, opening up an option for the QB. That has led to a defensive adjustment where the safety is free to “cut” or “nail down” on the crosser and the cornerback peels off the man running the crosser and replaces him in the middle of the field.\nBut as the data showed up, as well as this clip, when run off play-action and in favorable situations, you can execute this design against Tampa 2.\nCover 3: Spacing\nBoring, I know.\nOne of the more intriguing findings in our study was the idea of “Explosive” versus what we termed “On Schedule” plays. We found that there was a tradeoff between how frequently a route generates a positive EPA, and the magnitude of that EPA. That led us to two different buckets of route combinations: “Explosive” and “On Schedule.”\nExplosive route combinations were not successful as often, but when they hit, they hit big. Think aggressive, downfield shot plays like Verticals, Dagger and Mills.\nThen there are the On Schedule plays, that might not hit for huge yardage, but they were successful far more frequently. Those are your typical West Coast designs like Stick, Outs, Curl/Flat and the like. Those are plays you might call to start a drive, to get your QB in rhythm, or to move the chains on 3rd-and-4 or so.\nOne of the more interesting findings, at least in my mind, was that Spacing was a more explosive play than you might expect. Below is our graphic showing “Explosiveness” versus “Consistency” by concept, and the upper left quadrant shows the more explosive plays. Amidst the aggressive shot plays like Dagger and Mills and Verticals, you see Spacing, a concept with a cluster of short routes designed for easy reads and throws:\nAdditionally, we found that this design was the most successful against Cover 3. And it might make a bit of sense. After all, Spacing tasks recivers with sitting down in soft spots against zone coverage, and Cover 3 does offer a lot of those underneath. If you make a quick read and throw, that can offer the potential for yardage after the catch, turning a quick throw into an explosive play.\nHere is a play identified in our study as a Spacing concept against a coverage charted by SIS as Cover 3:\nYou can see how this design works against Cover 3, and has the potential to turn into a big play. The quick throw puts the receiver in position for YAC, and the route design works to find holes against the underneath coverage early in the down. Usually good things for the offense.\nCover 4: Cab\nThe next coverage and route combination to discuss is Cover 4, and the Cab concept. We defined Cab as a route with an outside receiver running a dig, and the inside receiver running a shallow crosser. You can find the graphic in the upper right of the earlier image showing all the route combinations.\nHere is an example of this concept in action against a play charted as Cover 4 in the dataset by SIS:\nNow, a few words on this pairing. One of the potential limitations to our study is that we did not isolate the EPA added on a play by the route targeted. That leads to situations where, as is often the case, you might have two different route concepts on a given play, but both concepts get credit for the EPA added. So, for example, say you have an aggressive vertical concept like Diagonal Nine called to one side of the field, but Curls or Spacing to the other. If the defense drops into Cover 3 the QB might look to the Curls or Spacing concept, but both concepts get credit for the resulting EPA. On a good number of the Cab against Cover 4 examples, the QB ended up throwing elsewhere. Something to keep in mind for future study.\nOn this particular play, however, Teddy Bridgewater has both the crossers open, due in part to the structure of the coverage, as well as the vertical release from the inside receiver which draws the attention of the underneath defenders for a moment. That opens up Robby Anderson on his shallow crosser.\nCover 6: Shallow Cross\nOur last pairing involves Cover 6, and the route combination we found most successful against this design was Shallow Cross, which pairs a dig route from one side of the field with a crossing route underneath it, just past the line of scrimmage.\nHere is an example of that pairing, with the Los Angeles Rams running the Shallow Cross concept against the San Francisco 49ers:\nOn this play Josh Reynolds runs the shallow crosser underneath while Cooper Kupp runs the deeper dig from the opposite side of the field. San Francisco drops into the Cover 6 look, and Jared Goff takes the easy read and throw to Reynolds on the shallow, in front of the linebackers.\nYou can see how conceptually this design might work against this coverage, as it puts the underneath defenders in a bind between dropping under the dig or collapsing on the shallow. But that is a situation encountered with most zone coverages, so I would caution readers not to take this as gospel truth. Something tells me there might be a bit of a sample-size issue with this coverage and route combination pairing.']	['<urn:uuid:90589a2a-003c-48e6-93ae-9bbb837e856f>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T22:13:20.996306	11	57	2825
14	concerned about symptoms bone metastases what signs should watch out for	Common symptoms of bone metastases include fragile bones, high levels of calcium in the blood causing nausea and confusion, loss of urinary or bowel control, weakness in the legs, and a low blood cell count and anemia due to the loss of bone marrow.	['What are bone metastases?\nBone metastases are tumors that occur when cancer cells break away from the\nplace where they started growing and move into bone tissue. Bone metastases are\nconsidered a form of advanced cancer. These secondary cancers within the bone\nare difficult to cure, but treatments are available to lessen the symptoms and\nWhat are the symptoms of\nBone metastases are common in many people with cancer. Bone metastases can sometimes\nresult in severe pain and neurological impairment due to changes in your bone\nstructure. Other symptoms of bone metastases can include:\n- fragile bones\n- high levels of calcium in the blood, which may cause\nnausea and confusion\n- a loss of urinary or bowel control\n- weakness in the legs\n- a low blood cell count and anemia due to the loss of bone marrow\nMetastatic cancer can severely damage your bones. Metastatic tumors can\ndestroy your surrounding bone tissue, causing osteolytic bone destruction.\nOsteolytic damage occurs most often from tumors that originate in the:\nOther damage can result when new bone is formed due to chemicals released by\nthe tumor. This new bone may be weak and deformed. When this occurs it’s known\nas osteoblastic, or bone formation, damage. This occurs in cancers that begin\nas prostate, bladder, or stomach cells. Some cancers, like breast cancer, can\ncreate both osteolytic and osteoblastic damage.\nBoth osteoblastic and osteolytic damage can cause pathological bone\nfractures. A pathological bone fracture is a fracture caused by a disease, as\nopposed to a traumatic fracture caused by external damage to your bone. Bones\naffected by this kind of damage break not from a fall or pressure, but during\neveryday activities. Damage to the bones of the spine can also affect the\nnerves of the spinal cord, causing neurological problems.\nWhat causes bone metastases?\nBone metastases aren’t the same as bone cancer. Bone metastases are formed\nfrom cancerous cells that start elsewhere in your body. So, bone metastases\ncould, for instance, be cancerous breast tissue, or another type of tissue somewhere\nin your body, that has started growing inside the bone tissue.\nCancer is a disease in which abnormal cells start dividing without control.\nSome tumors cells can break off and move around your body. Most of these shed\ncells die off, but some live on in new tissue and create a metastatic tumor.\nThese metastases can remain hidden, even when the original tumor is gone and your\ndoctor determines that you’re free of cancer.\nIt’s unclear why certain tumors become metastatic, and others don’t. In\ncertain types of cancer, such as advanced breast cancer or advanced prostate\ncancer, up to 70\npercent of patients develop bone metastases.\nThe most common cancers that result in bone metastases include:\nThe most common locations for bone metastases include the:\n- long bones of the leg\n- upper arms\nare bone metastases diagnosed?\nYour doctor will perform a full medical history and physical exam, including\na discussion of any past incidence of cancer. They can then order several tests,\n- X-rays of the affected bone\n- bone scans to see if other bones are affected\n- CT scans\n- MRI scans\n- blood tests\nIf your doctor needs to determine whether the affected bone is the result of\na bone metastasis or a primary bone cancer, they may perform a biopsy. During a biopsy, they’ll\nremove a small amount of the tumor and send it to a pathologist for a thorough\nHow are bone\nTreatment of metastases often depends on the location and the source tumor\ncells. Treatments can include radiation, medication, and surgery.\nRadiation therapy is often used to slow the growth of a bone metastasis. The\ntypes of radiation therapy include the following:\n- Local field radiation\ninvolves your doctor directing radiation at the tumor and nearby\ntissue. It can completely relieve pain in 50-60 percent of cases.\n- Hemi-body radiation\ninvolves your doctor directing radiation at a large part of your body.\nYour doctor can do this if you have multiple bone metastases.\n- Radioisotope therapy involves your doctor\ninjecting radioactive medication through your vein.\nMedications are a key part of therapy for treating bone metastases. They may\ninclude one or more of the following:\nmedications, such as bisphosphonates, to help reduce bone damage\n- chemotherapy to kill tumor cells and\nreduce tumor size\n- hormone therapy to slow certain hormones\nfor cancers like breast cancer and prostate cancer\n- pain medications\nSurgery may be necessary when your bones have fractured or will soon\nfracture. Your docotor may remove tumors surgically. They may attach fixation devices\ndirectly to surrounding bone. They can use bone cement for reinforcing your bone\nHeating or freezing cancer cells with a probe, called radiofrequency\nablation or cryoablation, can also reduce tumor size.\nAll of these treatment methods have risks. You and your doctor will arrive at\na unique treatment for your specific cancer. You may work with a variety of\ndoctors to tailor your care.\nWhat is the outlook\nfor people with bone metastases?\nBone metastases are a type of advanced cancer. It’s often not possible for\ndoctors to remove all cancer cells. A wide variety of treatments are available to\nreduce the size of metastases and slow their growth. This can reduce pain and\nother symptoms, and it can improve quality of life and longevity.']	['<urn:uuid:b3ea6831-e91d-4a12-864e-75535f77bfb1>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T22:13:20.996306	11	44	890
15	What are the warning signs that might indicate an elderly person should stop driving, and how can family members gather evidence about potential driving problems?	There are several warning signs to watch for. Family members can collect information by taking rides with the elderly driver and checking their vehicle for dents and scrapes. Warning signs include getting lost while driving, having 'close calls' on the road, difficulty turning to check over their shoulder while backing up or changing lanes, and receiving multiple traffic tickets or warnings from law enforcement officers. One important consideration is whether you would feel comfortable letting a 5-year-old grandchild ride in the car with them. A professional driving evaluation can also test cognition, reflexes, vision, flexibility, and visual attention, though this costs $300-$500.	['Family Conversations with Older Drivers\nOur nearby senior community, Somerby of Alpharetta, invited me to a seminar they were giving titled “We need to talk… Family Conversations with Older Drivers” one of several excellent seminars they were giving. This is a good service they are doing and the meeting room was full. Didn’t hurt they included dinner, but the seminar was very professional and included speakers Thomas Corrigan who is a Geriatric Consultant, Mr. Jim Swain an elder care attorney, an Alpharetta police officer, and Ms Deann Young the Somerby representative.\nThe stop driving seminar was excellent and here are some of the highlights from my notes.\nMs. Young pointed out age alone is not sufficient to make the decision to stop driving. Their is a post man in Birmingham Al who is over age 90 and still drives his postal route and has never had an accident. Many elderly drives are fine despite the jokes about old drivers.\nThere was a resident of Somerby who got up to talk about his decision to stop driving. He and his wife had been living up at Big Canoe when he started having mini seizes, just going blank a couple of seconds. He got better, then off on on he might have one. He said he always had a practice throughout his life of arriving early for an appointment and to do that, he had to leave early. So in his mind he certainly didn’t want to hurt or injury anyone, and although he might could continue to drive until something happen, he would stop now. It was not about him, but about other people and their children. Just like leaving early to keep his appointments, he would stop driving now, maybe early but the safe route.\nMr. Swain pointed out most people have been driving longer than they have been married, or longer than their career. It is difficult decision for them to make, to stop driving.\nSome facts were presented. You can’t say when a safe time of time to drive is, accidents happen all the time. An elderly driver has a 66% chance of dying in a vehicle crash. When you are young you can bounce back, but vehicle crashes are more serious to older people.\nAge related conditions like macular degeneration and cataracts can interfere with driving. Your field of vision really goes down hill fast after age 75. Medical factors which impact driving include diminished eyesight and hearing, prescription medications, decreased reaction time, physical flexibility, impaired judgment, dementia and memory loss, increase use of alcohol or OTC medications.\nSo how do you being a conversation with an older person about whether it is time to stop driving? Begin collecting information. Take a ride with the driver. Ask yourself would you let your 5 year old grandchild be in the car with them? Get a driving evaluation (cost $300-$500)that will test cognition, reflexes, vision, flexibility, and visual attention. Has the Senior been in “close calls” or got lost while driving. Take a walk around their vehicle and look for dents and scrapes. Seniors have may difficulty turning to check over their shoulder while backing up or changing lanes. Has the Senior received multiple traffic tickets or warnings from law enforcement offers? All of these could be warning sign tells about driving problems.\nIt was pointed out that Seniors are a tough generation and don’t know quit, stop or give up. Later Officer Howard offers that you should talk with Seniors about Retiring from driving -not quitting.\nDiscussion About Driving\nIt is good to have the whole family on board before having conversations with the Senior about not driving. Discuss in a private setting. Then maybe the least confrontational relative or friend should begin the conversation about it maybe time to stop driving. Try to be non-confrontational. Encourage the Senior to express their concerns about their driving. Realize but remove the emotion from conversation. Then the whole family could reinforce the conversation.\nMaybe give the Senior socially acceptable reasons that they could tell their friends why they stopped driving, like a hip replacement or medications for example. They need to explain why they stopped driving.\nOffer up solutions\nOther options for getting around could include take a taxi, using senior center vans, home care agency, senior service agencies, public transportation, and family members or neighbors and friends.\nSome new routines for the Senior that do not include driving may be gardening, spending time with grandchildren, travel, senior centers, family activities.\nPositive steps Loved Ones Can Take\nSchedule an eye exam with an Ophthalmologist, ask physician if there are assistive devices which could help with driving, have senior undergo a complete physical examination, consider neuropsychological testing if dementia is a concern, review all medications and side effects which could affect driving, arrange for the Senior to take a driver education refresher course through AAA or AARP, offer to pay for a comprehensive driving evaluation.\nDoctors can help but it was pointed out they are not going to tell the Senior they can’t drive anymore. Geriatric Care Manager can assisted as well as counseling professionals, Senior Realtors and certified driving instructor.\nHope this points help if you are concerned about a Senior continuing to drive.']	['<urn:uuid:5a2abed3-dd93-43d2-85e3-ee24ccc5561d>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T22:13:20.996306	25	102	874
16	What documents are needed for inheriting property in Turkey?	Beneficiaries need to obtain a beneficiary certificate from a Turkish court by providing legalised copies of birth, marriage and death certificates, plus sworn declarations (affidavits) proving they are entitled to inherit. This certificate allows changing the property ownership from the deceased's name to the beneficiaries' names.	['Trying to understand Turkish Inheritance laws or dealing with an estate administration where there is a property or other assets in Turkey can be a minefield. Especially when also grieving the loss of a loved one.\nIt’s hard to know where to start!\nIn case it helps we have provided some answers to our 5 most frequently asked questions about what to do when an English national dies owning property or assets in Turkey.\n1. How do I sort out the estate of a person who has died owning assets and property in Turkey?\nWhen someone dies owning property in Turkey, the beneficiaries need to obtain a beneficiary certificate in Turkey which will allows them to change the ownership of the property or assets in Turkey out of the name of the deceased and into the names of the beneficiaries.\nIn order to obtain this certificate, the beneficiaries to the Turkish estate will need to apply to a local court in Turkey. The court will need to establish that they are the people entitled to inherit the Turkish estate before issuing the certificate.\nThis may be done by presenting legalised copies of documentation to the Turkish court such as birth, marriage and death certificates and providing sworn declarations (affidavits) that there are not other people who are more entitled to the Turkish assets.\nThe easiest way to obtain a beneficiary certificate is to instruct a Turkish lawyer who has experience in assisting clients with estate administrations and inheritance of assets in Turkey.\nThe Turkish lawyer will be able to:\n- Guide you through the process;\n- Make the application to the Turkish court\n- Obtain all the necessary documentation to prove who is entitled to benefit from the Turkish estate and\n- Arrange for the transfer of the assets and property title into the name of the beneficiaries.\nIf you require assistance from an English-speaking Turkish inheritance lawyer, contact Worldwide Lawyers on 01244 470 339 or at email@example.com.\n2. Who will be the beneficiaries of the Turkish property?\nWho will inherit the Turkish property will depend on several factors.\nMovable assets (such as bank accounts, personal possessions, stocks and shares etc) are subject to the laws of the deceased’s nationality. Therefore the beneficiaries of these assets will be determined in accordance with the laws of England and Wales. Immovable assets (such as land and properties) located in Turkey will be subject to Turkish laws.\nIf the deceased had a Turkish will or an English will that includes the property in Turkey, the beneficiaries will usually be those named in the will as long as the will is recognised as valid in Turkey.\nIf the deceased did not have a will, Turkish law will determine the people who will inherit the property. Who inherits depends on who the surviving relatives are. The order of inheritance is as follows:\n- If the deceased had children, the children will be beneficiaries. If the deceased had a husband/wife they will also inherit a set share.\n- If the deceased has no children, the deceased’s parents will inherit. If the deceased had a husband/wife they will also inherit a set share.\n- If the deceased’s parents are not alive, the deceased’s siblings will be beneficiaries. If the deceased had a husband/wife they will also inherit a set share.\n- The last of a deceased person’s heirs among his or her close relatives are the grandparents and their children. If the deceased had a husband/wife they will also inherit a set share.\n- If none of the deceased’s next of kin survives, the entire estate goes to the surviving spouse (husband/wife).\n- If the deceased leaves no surviving heirs, the estate becomes the property of the Turkish State.\nThe amount any surviving husband/wife receives differs depending on which other relatives are living at the date of death.\n3. Is there inheritance tax to pay in Turkey?\nIf someone dies with assets in Turkey there will usually be inheritance tax to pay in Turkey. However Inheritance tax in Turkey is relatively low compared with the UK. The rate varies depending on how much the assets in Turkey are worth, with a lower percentage of tax applied to lower value estates.\nIf the deceased was domiciled in the UK, the estate may also have to pay inheritance tax in the UK in relation to the Turkish assets. Inheritance tax will be calculated on the value of the deceased’s worldwide assets so the assets in Turkey will be taken into account when calculating whether there is inheritance tax payable in the UK. For more information about this click here: Wills and Probate in Turkey.\n4. Once the Turkish assets are in the beneficiaries name, what then?\nYou will be able to deal with the assets as your own property. Therefore, if you choose to keep any immovable assets, such as land or property, you should ensure that your will reflects that you own assets in more than one jurisdiction. It can be helpful, if you have assets in more than one country, to have separate wills drawn up to cover the assets accordingly (i.e. a will to cover your estate in the UK and a different will to cover your assets in Turkey). This can make it much less complicated for your heirs to deal with the probate process.\nIf you decide to sell your assets in Turkey, your lawyer can assist with the property sale. You should also consider using a currency specialist when transferring the proceeds of the sale back to the UK, as you save up to 5% of the amount transferred , compared to using a high street bank. Find out more about how a currency specialist can save you money: When you should consider using a currency specialist.\n5. How Do I Find an English-Speaking Turkish Inheritance Lawyer?\nIf you are dealing with an estate administration where there are assets in Turkey or you think you are a beneficiary of assets located in Turkey, it is highly advisable to instruct a Turkish lawyer to assist you.\nYou should instruct an English-speaking Turkish lawyer who has experience dealing with inheritance matters for clients who have property and assets in Turkey.\nTo get in touch with an English-speaking Turkish lawyer who has experience assisting clients from the UK with regard to estate administrations with property or other assets located in Turkey, contact Worldwide Lawyers on 01244 470 339 at firstname.lastname@example.org or via our Contact Form.\nIf you are due to receive an inheritance from Turkey, read our article How To Get More From Your Overseas Money Transfers to avoid loosing money on the currency conversion when you receive your inheritance.']	['<urn:uuid:eacd54d6-6e64-4068-ba72-0b0702bbef90>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T22:13:20.996306	9	46	1114
17	What happened in Antioch that was important for Christianity?	Antioch was where the early followers of Jesus Christ were first called 'Christians' by their pagan Roman masters. It was also the base from which Peter, Paul and Barnabas started their missionary journeys into the outside world, and St. Peter preached for the first time and founded the Christian community from the Grotto of St. Peter.	['Tour Style : Private Tour\nDuration : 15 Days – 14 Nights\nHighlights : Istanbul-Adana-Antioch-Tarsus-Cappadocia-Konya-Antalya-Pamukkale-Izmir-Selcuk-Ephesus-Pergamon-Troy-Bursa-Istanbul\nDAY 1 : ARRIVE ISTANBUL\nWelcome to Turkey ! We meet you at airport and transfer you to your hotel for overnight.\nDAY 2 : ISTANBUL – ADANA – ANTAKYA – ADANA\nDepart to the airport for the flight to Adana. Full day tour of Antioch; the city was the centre of artistic, scientific and commercial life in Roman times. Antioch was much more than one of the great cities of Hellenistic culture, for it is here that the early followers of Jesus Christ were first called “Christians” by their pagan Roman masters. The church was also the base from which Peter, Paul and Barnabas started their missionary journeys into the outside world. Outside of town is located the Grotto of St. Peter, the rupestrian church from which St. Peter preached for the first time and founded Christian community. Lunch will be served at a local restaurant. Time will be available to visit the Mosaic museum. These mosaics were fastidiously removed from Roman villas in the Harbiye neighborhood, and date mainly from the second and third centuries, They represent one of the most most extensive and impressive collections of art.\nDAY 3 : ADANA – CAPPADOCIA\nDepart to visit Tarsus, the birthplace of St. Paul. Continue to Cappadocia area after. Visit Eski Gümüs, the rock church in Nigde. It was restored by British archaeologists in the 1960s, has a completely preserved courtyard (the only one to survive in its entirety) and solemn frescoes in its interior which deserve to be better known. From a room upstairs springs a surprise: a smoky wall covered with non-religious pictures, mostly from Aesop’s fables. Continue to Cappadocia and visit the Underground city of Kaymakli, one of the biggest underground settlements in the area where early Christians hid from their persecutors in fear and faith.\nDAY 4 : CAPPADOCIA\nThe area offers an unique landscape and some specialties like wine, made of local grapes (Urgüp), handmade pottery from a special clay of the Red River crossing the region (Avanos), onyx stone and handmade carpets with regional motifs. Visit the Goreme Open Air Museum with the marvelous churches placed in the rocks. Many secret rock churches are found graved in the valley with beautifully preserved frescoes and wall painting. While touring the region, there will be many photograph stops with the possibility to take a photograph of a ‘Fairy Chimney’ Peri bacalari in Turkish. After the visit to the Güvercinlik valley the land of stone dwellings, rock chapels and monasteries. Lunch will be served in the area. Visit Ortahisar and Avanos.\nDAY 5 : CAPPADOCIA – KONYA\nDepart for Konya. While driving to Konya general information about Turkey will be given to the guests while in the bus. Visit The Sultan Han Caravanserai on the way. It dates from the time of the Seljuk sultan Alaeddin Kaykobad I., who had it built in 1229. It is considered to be one of the most beautiful inns in Turkey. Continue to Konya which was known as Iconium during Roman times. The area where Konya is located is one of the most fertile regions of Turkey, even in the 4th millennium BC, the Chalcolithic period, it attracted cattle breeders and arable farmers. Besides its old history, Konya is a city that reflects Seljuk architecture. Lunch will be served at a local restaurant. One can see the Seljuks influance in the city on buildings like the ruin of Sultan palace and Ince minare medresesi (Thin minaret medrese). A visit to the Aleaddin mosque will give an opportunity to see an unique example of the development of early-Seljuk mosque architecture, as it took 100 years to complete. The Karatay medrese was an exclusive institution of advanced specialist education, in contrast with the mosque, in which elementary instruction was also given. The ‘University’ of Philosophy, astronomy, medicine, and mathematics was founded in 1251 by Vizier Karatay later became a famous law school and ‘administrative academy’ for civil servants. Also some other important locations in this region of Anatolia are Derbe and Lystra…\nDAY 6 : KONYA – YALVAÇ – ANTALYA\nVisit the Mevlana Monastery that is the major symbol of the city. It is a venerated national shrine and one of the holiest places in Islam. Although only a few have read the profound writings of the poet, philosopher and mystic Celaleddin Rumi (Known as Mevlana), the fascination in which Mevlana kept his pupils and followers is still alive today. The Selimiye mosque can be seen near the monestary and it houses people on praying times. Departure from Konya and visit the Esrefoglu Mosque near the Beysehir Lake. The mosque is famous for its wooden construction that dates back to 1296. Lunch will be served at a local restaurant near the Beysehir lake. Continue to Antalya via Yalvaç (Antioch in Pisidia). Antiochia was notorious in its early history for its excessive cult of the pisidian God men, Askaenos, for which a large priestly shrine was built. Also St Paul and St Barnabas conducted a mission here, but were driven from the town by the Jews. On the way to Antalya drive through the Taurus Mountains…\nDAY 7 : ANTALYA\nDepart to the west coast. Visit Side, another old, leading Hellenistic city situated on the Pamphylian Coast. The Side Peninsula was populated as early as 1000 BC. The remnants of the city are witness to its grandeur and splendor during the Helenistic Period. Visit Aspendos: According to tradition, Aspendos was one of the original Pamphylian cities founded by Mopsus and his mixed Multitudes. Those who visit Aspendos almost invariably do so because of its magnificent theatre, the finest of those which have survived from the ancient Greco-Roman world. Lunch will be served in the area. Visit Perge : The ruins of Perge, although not on the scale of Ephesus or Pergamon, are nevertheless vast. Most of today’s Perge, however, belongs to the period of Pax Romana; the Theatre, Stadium, baths and the colonnaded street. The Stadium is one of the best preserved in the ancient world…\nDAY 8 : ANTALYA – PAMUKKALE\nDepart to Denizli. On the way, the wonderful site of Aphrodisias will be visited. Aphrodisias is named after the goddess Aphrodite, whose cult-like status became synonymous with the celebration of sensual love and exquisite femininity. The ancient city of Aphrodisias itself became renown throughout Asia Minor as a center of medicine and philosophy but above all, of sculpture and the arts. Aphrodisias underwent a series of earthquakes in late Antiquity and was abondoned by the survivors after attacks by the Arabs. On arrival to Denizli, Laodicea, ancient city, where one of the Seven Churches of Revelation is located will be visited near Collosae. Continue to Pamukkale (Cotton Castle in Turkish) which is a shimmering white cascade, formed by limestone-laden hot springs, which have formed stalactites, potholes and magical fairy-tables. Bathing in this warm water is an exhilarating experience. Visit the Hierapolis. In this ancient city, the ruins spread over a mile from the city founded by Eumenes II of Pergamon. It was leveled by an earthquake in AD 17 but was rapidly rebuilt and enjoyed prosperity between the second and third centuries…\nDAY 9 : PAMUKKALE – IZMIR (SMYRNA)\nDeparture to visit Philadelphia, which is another of the Seven Churches mentioned in the Bible. Visit Sardes, which was the capital of the Lydian empire and another of the seven churches mentioned in the Bible. One of the cultural achievements of the Lydians was the invention of coins. The coins were first made of electrum, an alloy of silver and gold, during the reign of Croesus, and after this there were gold and silver coins. The city offers good preserved ruins and a wonderfull smell of history. Visit Izmir (Smyrna), the third biggest city in Turkey and one of the Seven Churches mentioned in the Bible…\nDAY 10 : MILETOS – SELÇUK – EPHESUS – KUSADASI\nDrive to visit Miletos : Although Ephesus is more famous, Miletos was by far the greatest of the Ionian cities, as measured by its enormous contribution of the development of Greek civilization. It was the principal port and the richest emporium on the Aegean coast. During the 7th and 8th centuries BC, Miletos established nearly one hundred colonies. Continue to Selçuk. See the ruins of the Artemis temple which is one of the Seven Wonders of the Ancient World and find the possibility to take a photograph of it with in the background a wonderful view of the Isabey mosque, the Basilica of St. John, a ruin of an Ottoman bath and the Citadel of the city. Visit the Basilica of St. John near Selcuk. It is believed that St.John lived in that area during the last years of his life and is buried beneath the Basilica. Lunch will be served in Selçuk. Unmatched by any archaeological site anywhere in terms of sheer magnitude, Ephesus appeals to every visitor, whether serious or casual. The roots of this impressive site goes back to the 13th century B.C. The surviving ruins of Ephesus belong to the Roman Imperial Period. There is an impressive amphitheatre big enough to hold 24,000 people, and a well preserved and restored bibliotheque called the Library of Celsus. Ephesus is known as one of the seven churches mentioned in the Bible. Departure to visit the House of Virgin Mary, the small house where believed Mary lived from AD 37 to 48, which has been visited by two Popes and millions of pilgrims…\nDAY 11 : KUSADASI – BERGAMA – AYVALIK/ASSOS\nDeparture to Bergama (Ancient Pergamon) via Thyatira. Visit Thyatira which is one of the seven churches mentioned in the Bible. Towering 1000 feet above the city, the ruins at Pergamon command an extraordinary view in nearly all directions. This great Hellenistic city was a great centre of culture. Pergamon which is one of the seven churches mentioned in the Bible, was the rival of Ephesus in the field of commerce and of Alexandria and Antioch in that of letters and the arts. Visit the Asclepion, the sanctuary that was dedicated to the God of Medicine and which was the first complete health spa in history. Visit Acrapolis where the ruins of a library and sanctuary Athena are located, as well as an impressive theatre of 80 rows, allowing seating of nearly 10,000…\nDAY 12 : AYVALIK/ ASSOS – CANAKKALE – BURSA\nDeparture to visit Assos through Edremit and Adramyttium. Assos is even more interesting for its situation than its ancient remains. It is in the northwest of Asia Minor, opposite Lesbos on the South coast of the Troad and at the entrance to the Gulf of Edremit on a steeply terraced trachyte cone (767 ft). Drive through Alexandria Troas to visit Troy, which is home to excavations that revealed nine principal levels of occupation dating from 3000 BC to 400 AD. Here, one can see the ruins of the city and the famous wooden horse of Troy. This is the place the mythological war of Troy took place, which Homer immortalized in his Iliad…\nDAY 13 : BURSA – ISTANBUL\nEarly departure to visit the Green Tomb and the Green Mosque of Bursa. Cross the Izmit bay with the ferry boat to reach Istanbul. Drive to Istanbul via the south coast road of the Marmara sea. The road offeres nice scienary and different types of aggriculture. On arrival to Istanbul , visit Chora Museum; which dates back from 11th century, and is famous for its fascinating mosaics and frescoes, most of which date back to 13th and 14th centuries, and are by far the most important works of Byzantine pictorial art known today. Visit the Hippodrome; where the four-horsed chariots raced around beneath the gaze of the Byzantine emperors and a quarter-million spectators. The Hippodrome dating back to 203 BC. under the Roman Rule, this was the largest track in the ancient times. The monuments which survived until our days are the Egyptian Obelisk, the Serpentine Column and Constantine’s Column…..\nDAY 14 : ISTANBUL\nDeparture to visit the Orthadox Pathricate or St Antuan Catholic Church or any other church On request. Continue to the old city districy. Visit St. Sophia Museum; originally a church,converted into a mosque and now a museum, this superb building wins admiration for the splendor of its Byzantine artwork. It was built by the Emperor Constantine, then re-built by Emperor Justinianus in the 6th century, this magnificent example of architecture rises with dignity in the Old City. Besides the dome which wins admiration by its height and size, the mosaics and frescoes decorating the walls are unique examples of Byzantine art. Visit Topkapi Palace. The Palace had been built between 1465-1478. The Ottoman sultans lived here and administered the empire for more than 400 years. Each sultan added pavilions, thus spreading the palace to a total area of 70.000 sq. meters. The palace also houses one of the greatest treasuries of the world. Visit the Blue Mosque; which is the only mosque in the world with 6 minarets and was constructed between the dates 1609-1617 by the order of Sultan Ahmet. Inside the mosque, everything seems to float in the azure light reflected from the 21,043 blue ceramic tiles from Iznik. Continue to the Grand Bazaar with its 4400 shops, which was built by the order of Sultan Mehmet the Conqueror, after the conquest of Istanbul in 1460’s …\nDAY 15 : END OF TOUR\nOur guests will be transfered to the airport by their guide.\nWhat is included\n- English Speaking Expert Tour Guide\n- Transportation by A/C vehicle\n- Transfers to/from Airport\n- Buffet Lunches daily (Except Day 1 and Day 15),\n- Accomodation in four Stars hotels on Half Board basis (Breakfast and Dinner included),\n- All Taxes.\nNot included :\nDrinks, tips to guide and driver, personal expenses and optional tours:']	['<urn:uuid:fc53ef42-1008-4fe2-a51a-c9156f654852>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T22:13:20.996306	9	56	2320
18	I'm researching timber decay identification methods and would like to understand the key visual indicators that distinguish wet rot from dry rot in terms of their appearance and structural effects on wood?	Wet rot and dry rot show distinct visual indicators. In wet rot, the timber becomes spongy and soft to touch, turns darker (though sometimes lighter), and shows cracks mainly along the grain. The paint finish can remain intact as a veneer while rot is active inside. For dry rot, the wood develops characteristic 'cuboidal' cracking both along and across the grain, and shows distinctive growth features including a silky grey to mushroom-colored skin with lilac and yellow patches that peels like a mushroom. Dry rot also produces white, fluffy 'cottonwool' mycelium with teardrops under humid conditions, and distinctive orange-ochre fruiting bodies with wide pores that produce rust-red spore dust. Both types cause structural weakening, but dry rot can cause more widespread structural damage as it attacks wood even far from the moisture source.	"['What Is Wet Rot?\nIf you think you have identified decay in your property it is important that the issue is inspected, and repair work is carried out as soon as possible. Regardless of whether your property is a new build or an older property it can still suffer from dry or wet rot.\nWood becoming wet provides the platform which allows fungal decay to develop. Timbers suffering from decay will normally have a minimum moisture content of 20%. Locating the cause of the water ingress and rectification is critical to allow the timber to dry.\nTired of reading? Listen to this article!\nWet and dry rot begin their life cycle in a similar manner when spores of the fungus are germinated on damp wood. When wood destroying fungi begins to attack a piece of timber and create a fruiting body, they release huge amounts of microscopic spores which are widely spread by air currents. Wood which is damp and has not been treated by a fungicide protective treatment may be susceptible to decay. Mycelium will develop within the timber and will start to break down the wood. The timber shrinks and can show a cracked and dark appearance.\nWet Rot Timber Decay\nWet rot is a fungus which attacks unprotected timbers in a property when they have become wet. If the environmental conditions are suitable for wet rot, the rot can develop and result in the breakdown of the structural integrity of the affected timbers. A water penetration issue or defective internal plumbing can provide the perfect conditions for timber decaying fungi to thrive.\nThere are numerous different species of wet rot though, which can prove problematic to the homeowner when trying to diagnose what type of rot problem is affecting their property.\nWhat Causes Wet Rot?\nWet rot tends to thrive in timber which has a moisture content between 25% and 50%. Rot can occur from a variety of different causes. However, the most likely causes of the damp are:\n- Defective rainwater goods, defective roof coverings and masonry.\n- Defective seals on showers or baths.\n- Defective plumbing to sinks, baths, washing machines, dishwashers, etc.\n- Rising Damp.\nWet rot can occur in any area of a property where damp is present, and the damp has been in contact with the wood for a prolonged period.\nThe source of the moisture must be identified and rectified to ensure the new timbers are maintained in a dry condition and the property fabric can dry to normal moisture levels.\nWhere Can Wet Rot Occur?\nMoisture is the source of all rot attacks. By keeping your property in a good state of repair and wind and watertight you are reducing the risk of decay developing. Wet rot can affect timber at any level of your property. Some of the most common external defects noted are;\n- Defective roof coverings and flashings\n- Defective gutters and downpipes\n- Defective masonry pointing or render\n- Path levels which are raised can provide water access above your property’s damp proof course which can result in damp penetrating your home. You can rectify this by having the damp proof course checked by a professional and having the paths lowered.\n- Timbers on the ground floor are more susceptible to decay if air bricks are blocked or missing.\n- Damaged or blocked drains caused by debris such as leaves is a common cause of water penetration.\nThere are many potential causes, but the ones outlined above are the most obvious and easiest to resolve. If you identify these issues around a property which you are about to buy or sell, you should consider a specialist survey be undertaken.\nDecay in Upper Flooring and Roofs\nWet rot occurring in the upper flooring and roof timbers is less common than in ground floor timbers, due to ground floor timbers potentially being in closer contact with damp soil areas and rising damp and less likely to be seen. The most common causes of water ingress to upper areas of a property are listed below;\n- Defective flashing around chimneys can result in long term water penetration into your property. In some instances, this may not appear as water staining on the ceilings and therefore not visually apparent internally. This water ingress can result in the timbers surrounding the chimney becoming decayed. It is prudent to have your roof coverings inspected by a roofing contractor on an annual basis.\n- Slipped tiles can allow water ingress which may provide suitable conditions for dry rot or wet rot. If dealt with quickly damp issues to your property will not arise, but if left untreated decay may develop.\n- Wet stains or algae growth below gutters should be a red flag for property owners. Ensure that these are repaired whenever necessary and are kept clear.\n- Upstairs bathroom floors and central heating systems may have overflow pipes which protrude out from the walls. It is important that these are not dripping. Also, defective plumbing may be hidden behind walls and under floors. If issues are noted to these pipes a plumber should be called to rectify the issue and the property dried out.\nHow to Identify Wet Rot?\nWet rot has a specific life cycle. At its inception, this type of rot will begin to develop strands which feed from the timber cells causing the timber to shrink and then will create a fruiting body which releases spores. The presence of wet rot within timber will cause the timber to begin to change colour. The timber will usually turn darker however it can turn considerably lighter but this is down to environmental factors. The timber will become spongy and soft to touch.\nThe decay caused by wet rot tends to be localised to one specific area (The area of the water ingress / leak). Construction timbers within properties such as windowsills, floorboards and skirting boards will normally show clear signs of a wet rot problem through the paintwork flaking and becoming generally damaged.\nWet rot is usually accompanied by a damp and musty smell which is very obvious around the timber which is infected. However, one of the biggest issues which property owners face surrounding wet rot is that it tends to affect areas of the property which aren’t usually visible. These areas of the property tend to be underneath flooring, behind walls or built into the external masonry wall which are unlikely to be inspected by the occupants.\nWhat are the Differences between Wet Rot and Dry Rot?\nThere are numerous differences between wet rot and a dry rot wood destroying fungi, some of which are listed below.\nDry Rot Signs\n- Wood affected by dry rot will lose its structural strength and even wood far from the damp source can be attacked.\n- Dry rot will cause obvious cracks to appear along and across the grain of the wood which may show mycelium growth being evident on the wood.\n- The fungus mycelium appears white like cotton wool when fresh and goes to a grey colour when older. It can grow through the mortar beds of masonry walls to seek more timber to colonise.\n- Dry rot fruiting bodies give of a red spore dust.\n- Dry rot germinates on damp timber which has over 20% moisture content and wet rot germinates at higher levels of moisture content.\nWet Rot Signs\n- Paint finished on timber can stay intact as a veneer with the rot active in the heart of the timber\n- Wood likely to feel spongy and weak, showing that the structure of the wood is affected.\n- If the mycelium is apparent it tends to stay in the damp area with the fungal attack and not travel far from the source of moisture.\n- Advanced decay will result in the wood shrinking and showing cracks mainly along the grain.\n- Moisture content needs to be around 35 to 50% for wet rot to begin development.\nWhat to Do Next?\nTreating a wet rot issue as soon as possible once you suspect your property may be suffering from one is crucial. As previously stated, the most important part of the treatment is to identify and rectify the source of moisture causing the fungal attack to the timber.\nArranging a timber and damp survey if you suspect you have a rot problem is essential. Our specialists have the necessary experience, resources and needed certifications to resolve your wet rot problem efficiently. Our specialists will specify and complete the required remedial treatments to eradicate the rot attack affecting your home. Richardson & Starling have over 85 years’ experience in dealing with timber decay problems and offer a 30-year guarantee you can trust on the wet rot remedial treatment.', 'Dry Rot and its Control\nDry rot (Serpula lacrymans) is a wood-destroying fungus that is found in most parts of the world. Although it affects forest timbers, dry rot is best known for its ability to destroy timbers in ships and buildings.\nIdentification of Dry Rot\nIt is important to identify whether timber decay has been caused by dry rot or another wood-destroying fungus such as one of the wet rots. This is because dry rot has the ability to travel through building materials other than timber, giving outbreaks the potential to spread quickly through a building. For this reason additional measures (e.g. masonry sterilisation) often have to be taken when treating dry rot outbreaks over and above those necessary when dealing with outbreaks of other wood-rotting fungi.\nTypical indications of dry rot include:\n- Wood shrinks, darkens and cracks in a \'cuboidal\' manner (see picture)\n- A silky grey to mushroom coloured skin frequently tinged with patches of lilac and yellow often develops under less humid conditions. This \'skin\' can be peeled like a mushroom.\n- White, fluffy \'cottonwool\' mycelium develops under humid conditions. \'Teardrops\' may develop on the growth.\n- Strands develop in the mycelium; these are brittle and when dry and crack when bent.\n- Fruiting bodies are a soft, fleshy pancake or bracket with an orange-ochre surface. The surface has wide pores.\n- Rust red coloured spore dust frequentky seen around fruiting bodies.\n- Active decay produces a musty, damp odour.\nImportant Note: Dry rot can cause widespread structural damage. We recommend that a professional timber treatment company is called in to carry out a survey if dry rot is suspected. If you suspect dry rot contact our technical department and we will be happy to arrange for an experienced timber treatment company to contact you.\nDry Rot Control and Treatment\nDry rot will only affect timber that is damp, typically affecting timber with a moisture content in excess of 20%. For this reason, removing the source of moisture should form the core of any dry rot eradication strategy.\nTimber can become damp for a number of reasons. Among the most common causes are leaking washing machines, shower trays, baths, condensation etc... The dampness can also come from outside the building, for example, leaking roofs, rising dampness, or dampness penetrating through walls. Whatever, the source of the dampness, if it is rectified and the timber allowed to properly dry out, the dry rot will eventually be controlled.\nHowever, it is not always possible or practical to be sure that the timbers will remain dry in the long term. Therefore, it is important that secondary measures are taken to defend against re-infection. Any affected timbers should be removed and replaced with pre-treated timber. Any remaining timbers at risk of being affected by the dry rot should be treated with an effective fungicide. Where the dry rot has passed through the masonry, it should be isolated using physical containment and/or masonry sterilisation.\nSafeguard\'s ProBor range of wood preservatives are particularly suitable fungicides for the treatment of dry rot, as they are able to spread much more deeply into the timber than conventional preservatives. This gives them an extensive performance advantage, as no wood preservative can start working until it comes into contact with the fungi that it is designed to defend against.\nFree Dry Rot Book\nA more comprehensive 19-page guide to ""Dry Rot and its Control"" can be downloaded free of charge from the ""Download Datasheets"" section of this page in Adobe Acrobat (pdf) format. This publication gives more detailed guidence on the identification and treatment of dry rot.']"	['<urn:uuid:5c740c6b-0ed0-4aa5-afe5-1b97eb5b9259>', '<urn:uuid:0b4523cb-5125-469e-bbe2-114b7b069f12>']	open-ended	with-premise	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-12T22:13:20.996306	32	133	2061
19	What are the key differences between hydrogen's auto-ignition process and traditional hydrocarbon fuels when stored in contained environments like tanks and silos?	Hydrogen stored in tanks presents minimal explosion risk - it is kept in secure airtight tanks and, if leaked, would quickly dissipate into the atmosphere since it is 14.4 times lighter than air. Hydrogen is also four times more diffuse than natural gas and 12 times more diffuse than gasoline fumes. In contrast, when hydrocarbons are stored in silos, bins and fuel deposits, their minimal ignition temperature drops rapidly, increasing fire hazard. Materials with lower auto-ignition temperatures present greater fire hazards than those with higher auto-ignition temperatures in storage conditions. The automotive industry safely uses hydrogen at 700 bars pressure in storage systems that are mechanically controllable.	"['Hydrogen is a chemical element with symbol H and atomic number 1. It is the simplest, most abundant and lightest element in the whole universe. Hydrogen constitutes about 75% of the universe mass (while some argue that hydrogen constitutes 90% of the visible universe). At earth temperature and pressure (say when found in our living environment temperature and pressure), hydrogen is most commonly identified as a colourless, tasteless, non-toxic, flammable and odourless diatomic gas (H2). Hydrogen gas is rarely found alone in nature because it is usually bonded with other elements. For instance, hydrogen can be found in water (H2O).\nHydrogen is an extremely safe fuel. The gas is stored in secure airtight tanks, and in the extremely unlikely event of a leak there wouldn’t be any build-up. Instead, as hydrogen is lighter than air, it would escape quickly and harmlessly into the atmosphere.\nWhen hydrogen is in a tank, there is no danger of explosion. Hydrogen is an energy source that is indeed, as is the case with every other fuel, flammable in contact with air. Risk assessments show however, that hydrogen is less dangerous than petrol or natural gas for example. In addition, hydrogen has also been used commercially for more than 100 years in large quantities safely, including in the chemical industry. The energy content of the hydrogen tanks in the vehicles is typically less than that of regular petrol or diesel vehicles. The automotive industry is geared towards 700 bars hydrogen storage (the pressure of hydrogen storage systems is mechanically controllable). The hydrogen vehicles driven today in demonstration programmes have undergone the automotive manufacturers’ complete development cycle, including crash tests, and are cleared for road transport. The vehicles are therefore just as safe as conventional vehicles.\nNo, modern fusion weapons consist essentially of two main components: a nuclear fission primary stage (fueled by uranium-235 or plutonium-239) and a separate nuclear fusion secondary stage containing thermonuclear fuel: the heavy hydrogen isotopes deuterium and tritium, or in modern weapons lithium deuteride. For this reason, thermonuclear weapons are often colloquially called hydrogen bombs or H-bombs but it is an improper definition. Hence the destructive power of a hydrogen bomb is based on a nuclear reaction that occurs just at very high temperature (millions of Celsius grades); in the use as fuel there is no possibility to trigger nuclear reactions.\nOne of the common myths that has questioned the security of the H2 is the infamous Hindenburg disaster, which occurred in New Jersey in 1937.After decades of debate and research, it is now considered that while the German passenger airship was docking during an electrical storm on the fateful day of the explosion, an electric discharge from the clouds ignited the airship\'s skin surface covering. This resulted in the ignition of the ship\'s hydrogen bags. However, what caused the great and deadly fire was not hydrogen, which burned quickly and safely over the occupants of the ship, but the dark iron oxide and the reflective aluminum paint that covered the surface of the ship. Those components were highly flammable and toxic and burned at a high energy rate once they caught were on fire.\nThe short answer is: yes, hydrogen is safe. Of course, this does not mean that it cannot be dangerous, but any fuel can be dangerous under the right circumstances, which is why all fuels must be handled with care, including hydrogen. That being said, the dangers of hydrogen are different and generally more tractable than the dangers arising from hydrocarbon fuels, such as natural gas and gasoline. According to the 2003 White Paper entitled ""Twenty Hydrogen Myths"", of the American physicist and president / head scientist of the Rocky Mountain Institute, Amory B. Lovins, hydrogen is exceptionally floating and is 14.4 times lighter than air. Lovins notes that hydrogen ""is also four times more diffuse than natural gas or 12 times more than gasoline fumes, so the leaking hydrogen quickly disperses up and away from its source"". Furthermore, he says that if the hydrogen was turned on, it burns quickly with a non-luminous flame. This flame cannot readily burn a person at a distance as it emits only a tenth of the radiant heat of a hydrocarbon fire and burns 7% colder than gasoline. In fact, to further talk about the safety of hydrogen compared to the safety of petrol, in general, victims of hydrogen fires are not generally burned unless they are actually in contact with the flame. Furthermore, they are not suffocated by smoking. Lovins also refers to a filmed demonstration comparing a hydrogen car fire with a gasoline car fire. What shows the proof is that the hydrogen fire caused a plume of vertical flame. This flame has increased the internal temperature of the vehicle by a maximum of 1 - 2 degrees Fahrenheit, while the external temperature closest to the flame was not hotter than that experienced by a car sitting directly in the sun. Furthermore, the passenger compartment was unharmed. However, in the second gas test, the gasoline fire gutted the car and killed anyone trapped inside.\nHydrogen can be produced in many different ways. However, the vast majority of Hydrogen currently produced worldwide uses steam reformation of natural gas and electrolysis of water. Water electrolysis is simply a means to split up water into its two gases constituents that are hydrogen and oxygen (this last is present in the breathable air for 21%) using electrical current. To split water, we use a device called electrolyzer.\nYes, and the process is known from over 150 years. When hydrogen is produced using electrical current from renewable sources through electrolysis process, it is an environmentally friendly fuel. Renewable energy, for instance, is the power generated from a photovoltaic panel or wind turbine. If an electrolyzer is powered with this green power, green hydrogen would be produced. Furthermore the electrolysis process has only as “waste product” the oxygen (the same that we breathe), that could be stored and used for medical reasons for example without CO2 emissions, while in the steam reforming (process by which hydrogen is made from methane), 11 grams of CO2 are obtained for each gram of hydrogen produced.\nBecause hydrogen can be produced and used with no harmful emissions. Also, hydrogen can be used everywhere we use energy (transport, cooling, heating, cooking, electricity generation, etc). In addition, hydrogen can be used in a fuel cell (a device that can convert hydrogen directly in electricity with only water as waste product) which is more efficient than any conventional energy devices, considering the heat recover coming from split (electrolyzer) and recombination (fuel cell) processes.\nYes. The 71% of Earth surface is covered from water that can be used. In fact, the full cycle is water neutral as per the below description:\nWe can use electricity from renewable sources directly to satisfy our immediate needs in electricity and heat. However, there are many times when there is wind/solar/hydro electrical power being generated, but there is no demand or there are times in which the demand is larger than production (for solar panel during the night for example). In other words, we are not needed to use this green electricity at the time of it being produced. Hence hydrogen production system using electrolysis processes can be used when we have surplus renewable energy, and give back the energy when we need it. The energy power in form of hydrogen gas remains unchanged even after years in contrast with other storage systems (for example batteries).\nYes. Wind and solar systems, on their own, do not address our need for continuous power and fuel due to their discontinuous character. Green hydrogen energy systems can do that through the storage and reuse on demand.', 'You can often find it in the section 9 of safety data sheets.. Regulatory Implications of Auto-ignition Temperature Table 1: Typical minimum ignition temperatures of various dusts. Minimum Auto-ignition Temperature of Hydrocarbons  Little Pro on 2016-01-13 Views: Update:2019-11-16. Autoignition Temperatures and Flash Points for Hydrocarbons - Autoignition temperature and flash point ( °C and °F) of different types of hydrocarbons with varying carbon number up to C12 ; Biogas - Carbon Nitrogen Ratios - Carbon - Nitrogen ratios for biogas produced from various raw materials Ignition Temperatures of Materials Auto-ignition temperature - the minimum temperature required to ignite a dry material in air without a spark or flame being present: Fuel or Chemical A material with a low auto-ignition temperature is a greater fire hazard than a material with a high auto-ignition temperature. Ignition temperature definition is - the lowest temperature at which a combustible substance when heated (as in a bath of molten metal) takes fire in air and continues to burn —called also autogenous ignition temperature. ★ HOPE THIS HELPS U BUDDY (^_^) !! Minimum ignition temperature (MIT) Minimum ignition temperature of a dust cloud The minimum ignition temperature (MIT) is the lowest temperature of a hot surface that will cause a dust cloud, rather than a dust layer, to ignite and propagate flame. » BEST EXAMPLE : Candle flame & Fire. ★ While autoignition temperature is measured in a laboratory, the actual temperature of ignition in a vehicle is significantly affected by many factors. » IGNITION TEMPERATURE : It means that a substance is the lowest temperature at which it spontaneously ignites in normal atmosphere without an external source of ignition. The auto-ignition temperature or kindling point of a substance is the lowest temperature at which it will spontaneously ignite in a normal atmosphere without an external source of ignition, such as a flame or spark. Auto-ignition Temperature. For example, the minimum ignition temperature of dust clouds and layer deposites are given in the following table. minimum auto-ignition temperature is the lowest temperature at which the fuel vapours spontaneously ignite. The test follows EN ISO/IEC 80079-20-2:2016 (Explosive atmospheres Part 20-2: Material characteristics - Combustible dusts test methods). This temperature is required to supply the activation energy needed for combustion. Auto-ignition temperature is the lowest temperature at which a chemical will spontaneously ignite in a normal atmosphere without an external source of ignition, such as a flame or spark. When material is stored in silos, bins and fuel deposits, the minimal ignition temperature drops rapidly. Experiments have found, for example, that the auto-ignition temperature for newspaper is about four degrees lower than that of the filter paper used in chemistry laboratories. Hydrocarbons that have been heated can ignite if they are exposed to air. The Auto-Ignition Temperature (AIT) is the minimum temperature at which a chemical (e. g., vapor) will spontaneously ignite without an external ignition source such as a spark or pilot flame – under specific conditions – in an oxidant, which is usually air. Hot surface ignition is governed by the temperature of surfaces in the vehicle, the autoignition properties of the flammable materials, and factors that influence these parameters. Fuels and their flash points for some common substances at atmospheric pressure are indicated below: Fuel Type: Temp (Deg C) Temp (Deg F) Acetaldehyde -37.78-36: Acetone -17-78: 0: Benzene -11.11 Autoignition / Hot Surface Ignition. The figure below shows the auto-ignition temperatures of hydrocarbons at atmospheric pressure.\nApex Legends Canvas,\n80/20 Portfolio Historical Returns,\nDance Zumba Definition,\nDog Breeds That Can Walk Off-leash,\nBloem Self Watering Planter,\nBaked Potato Topping Combinations,']"	['<urn:uuid:d0485f62-a837-49fe-aa60-39cadb3dc5d9>', '<urn:uuid:0f30d162-db05-4030-a85c-068788b58179>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T22:13:20.996306	22	107	1882
20	What are the essential benefits of incorporating heavy bag training into a boxer's workout program, and how does it help improve fighting skills?	Heavy bag training plays a pivotal role in boxing training and helps boxers improve their skills without needing a partner. It provides multiple benefits including improved coordination, punching power, aerobic endurance, footwork, technique, and overall physical condition.	['Getting in shape for boxing involves not only the use of quality boxing shoes, gloves, and training equipment but also a specific program that, although personalized depending on the fighter’s physical condition, includes certain exercises. Since we’ve detailed the importance of the boxing gear you use in our recent article, we’ll dedicate today’s post to boxing workout tips.\nWhile boxing might seem to be a lot about shoulder strength and forearm power, this type of fighting actually requires a full-body workout. The exercises should be combined in such a way to help you increase your strength, endurance, speed, agility, and power.\nWhether you’re a beginner or a more seasoned fighter, the tips and exercises mentioned below are usually part of any boxer’s workout program as they cover all the muscles of the body. Boxing requires a well-balanced body because muscle imbalances can limit your performance.\nEvolving from the kid’s playground to many athletes’ workout routines, jumping rope is now almost a trademark of boxing. This popular cardio exercise will help you reduce body fat and train your muscles intensely. Moreover, it will help you improve your agility, endurance, coordination, and footwork.\nDouble-unders, high-knees, running in place, and criss-crosses are preferred by many fighters for the benefits they trigger. Therefore, include them in your rope workout to get one step closer to the body you need to fight like a pro.\nBurpees and sit-ups\nConsidered to be one of the most important exercises for increasing endurance and strength, burpees should also be part of your boxing workouts since you will get up and down in the ring. Pair your workout routine with sit-ups as well since they will help you build up core strength which is needed to throw punches effectively.\nSince you’re training for boxing, it is self-explanatory that shadow boxing should be included in your training. Usually performed to warm up your body before getting to more challenging exercises like heavy-bag hitting, this type of exercise will help you practice your footwork and movements. Moreover, it will further help you build your endurance.\nUse the reflection in the mirror or your shadow as your opponent and punch in the air. It may seem easy but it actually is quite a challenging exercise.\nHeavy bag training\nOnce you’ve warmed up by doing shadow boxing, you can get to more difficult exercises such as heavy bag punching. This plays a pivotal role in boxing training and has been used as one of the main ways to get in shape for boxing for quite some time now.\nHitting the heavy bag helps boxers improve their skills without training with a partner. Its importance lies in the many benefits it triggers. Training with a heavy bag will help you improve coordination, punching power, aerobic endurance, footwork, technique, and your overall shape.\nSparring and speed bag hitting\nWhile the heavy bag will help you enjoy the above-mentioned benefits, a speed bag, just as the name suggests, will help you enjoy better punching speed. As a beginner, you might have a bit of a problem at first but practice makes perfect and training with a speed bag will help you enjoy the needed rhythm and coordination.\nSparring with a real person is also part of a boxer’s usual training since it sharpens the fighter’s reflexes and ability to anticipate the opponent’s movements.\nShoulder presses, pull-ups, and walking lunges\nAs we’ve said before, getting in shape for fighting means working your entire body. Therefore, adding a few more exercises to your routine is mandatory. Without a well-balanced body, your performance will be affected.\nAs a fighter, you need strong shoulders to keep punching the opponent. Shoulder presses will help you improve your shoulder endurance and strength. Add pull-ups to your training to further build shoulder, arm, chest, back, and core strength. Don’t forget about building your leg strength and try walking lunges. They won’t just help you with that but will also work your core strength and glutes.\nYes, running should be part of any boxer’s workout because fighting requires aerobic and anaerobic endurance and this type of exercise will help you improve such capacities and burn calories.\nInclude running in your regular training. Get your body ready for running with a few minutes of warming up and stretching. Also, make sure you cool down and stretch once you’re done running.\nImprove your flexibility\nIt’s not enough to build strong muscles and endurance to get ready for the ring. Flexibility is also required, given the nature of this type of fighting. By stretching regularly, you will improve your flexibility and this will further help you prevent injuries caused by a less trained body or reduced flexibility.\nGo for the usual stretching exercises such as neck/shoulder/arm/wrist/ankle/knee/elbow circles, standing toe touches, and so on.\nOther tips and considerations\nThe exercises included above are part of many boxers’ training program yet you will need a personal trainer to create a personalized workout for you depending on your physical condition, boxing experience, and other personal factors.\nAn experienced trainer will help you train your body fully and get you balanced. Not to mention that by being supervised, you will be able to improve the way you work your body and correct your mistakes (if any).\nThen, make sure to use the proper equipment. Protect the bones in your hands by using hand wraps and gloves. Complete your gear with a quality headgear and a mouthpiece to protect your head and cups to protect your groin area.\nTo enjoy a safe and complete full-body workout, you also need to pay attention to your diet and how many hours of sleep you get. Boxing is not just about learning the right punching and defense techniques. It requires your whole body, focus, and mental clarity, therefore, it is a complex process that takes time and involves various changes. Again, a personal coach and a visit to your nutritionist are mandatory.']	['<urn:uuid:87368f67-1064-4f39-85a2-6ccc00f64572>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T22:13:20.996306	23	37	987
21	How do experts work together to fix both horse feet and ocean problems?	For horse foot issues, it's critical that farriers and veterinarians collaborate closely - the farrier should be present during radiographs or receive them via email to determine the best treatment approach together. Similarly for ocean restoration, IntelliReefs partners with multiple experts and organizations including iSENSYS, Absolute Vorticity, and Isla Mar Research Expeditions to conduct surveys and gather data. They also work with the Reef Life Foundation and other partners for restoration implementation.	"[""Why does your horse have no heels?\nUpdated: Mar 22, 2018\nLast week, we defined several terms to describe issues with the horse’s feet. These included sole depth, heel height, negative coffin bone angle and breakover. So now let’s talk about how this applies to specific foot-related problems. Before we do that, we must recognize a critical member of the team. Your farrier. It’s all well and good to come up with suggestions for improving your horse’s foot, but the farrier will also have an opinion about this. Your farrier needs to be involved in this discussion and will play a key role when it comes to trimming the foot and applying the correct shoe.\nBefore proceeding with hoof radiographs, it is important to know how your farrier wants to be involved in the process. Ideally, he or she is present when the radiographs are taken. Alternatively, the radiographs can be emailed to the farrier or even printed off and left at the farm. Either way, it is important that the farrier and veterinarian talk to each other to determine the best course of action before any final decisions are made.\nLet’s concentrate on our most common problem, and the root of much evil, the horse with low heels, low hoof angles and long toes. Let’s look at three causes of the case of low heels/negative coffin bone angle:\nSome horses have a low hoof angle based on genetics and events that occur early in the horse’s life. These causes are not well understood but most horses will have one foot with a slightly higher angle than the other. And the higher angled foot tends to be the right front foot (in the northern hemisphere).\nThe lower angled hoof wall will wear away the heel portion of the foot faster than the toe because the horny tubules are not as perpendicular to the bearing surface (shoe) as the more upright foot. Those horny tubules, which grow down from the coronet at an even rate from the toe to the heel, will wear minimally when perpendicular to the ground or shoe surface but will wear rapidly if they approach the ground surface at an angle. Consequently, the toe wears minimally, the heel wears excessively.\nWhen shod, the toe of the foot and the shoe are in constant contact with little or no motion between them. The heel is another story. The heels tend to expand and contract with each step, more so at speed or when jumping. This movement creates friction which wears the heel down more quickly, regardless of the angle of the foot.\nConventional horse shoes work well for most horses but if a horse is prone to excessive wear in the heels, the conventional horse shoe will exacerbate the issue. To prove this, look at your horse’s front shoes the next time your farrier pulls them. You will notice that the toe area of the shoe is unchanged from when it was applied and maybe even a bit rusty. The heels, on the other hand, will be shiny and there will even be a small divot where the heel contacts the shoe, more so on the medial heel.\nBecause horses tend to be base-wide and toe-out while moving, the medial heel will undergo more wear than the lateral heel. The tendency to wear the medial heel more than the lateral heel results in uneven heels with the medial heel being lower than the lateral. As if it wasn’t enough to have the heels crushed, uneven heels wreak havoc on the coffin joint and its associated structures.\nSo why do we worry about low hoof angles? Negative coffin bone angles create added strain on the deep digital flexor tendon. This added strain also takes a toll on the navicular bone and its associated structures. Negative coffin bone angles also put compressible forces on the front of the coffin joint. Although we tend to regard issues of the coffin joint and navicular bone as manageable or inevitable, we can go a long way preventing and treating these conditions with close attention to hoof angles, sole depth, heel height and breakover distance.\nLow heels are also hard on the heel structure itself. Low heels are prone to bruising and chronically low heels will ultimately damage the important digital cushion. As proof of chronic inflammation in the heels, we often see new bone growth in the heel region subsequent to chronic heel bruising.\nLet’s look at some solutions for under-run heels. Our first challenge trimming and shoeing the horse with low heels is that some solutions (for instance, wedge pads) tend to be good for changing the hoof angle but will place excessive pressure on the heel structures. In general, we make recommendations that provide a larger platform for the heels that go beyond the conventional horse shoe. This could include bar shoes, heart-bar shoes, frog-pressure pads and flip-flop shoes.\nThe most important consideration for helping the heels is to bring the breakover of the toe as far back as possible. This is accomplished with rolled toes, rockered toes, squared toes or simply setting the shoe back as far as possible. Whichever method is chosen, the key to bringing the breakover as far back as possible is by using radiographs to determine how far is far enough.\nThis solution below worked well for this horse. As you can see, this shoe provides a large platform for the heels and frog to land on and as the radiograph shows, provides a very early breakover at the toe:\nIt is important to remember that every horse with low heels will have a different solution based on what the horse does for a living, the experience and past experience of the farrier and veterinarian and the unique characteristics of the foot and the underlying structures. There is no cookie-cutter recipe for shoeing any horse with hoof-related problems!\nNext week we'll tackle the issue of horses pulling their shoes off. Stay tuned."", 'January 2021 Press Release: The Decade of Ocean Science\nUpdated: Aug 23, 2021\nUPDATED MEDIA ADVISORY: FOR IMMEDIATE RELEASE\nIntelliReefs is the world\'s first science-driven, nanoengineering marine restoration company. We are working to put the world’s best technology underwater to build additional coral reefs that are more resilient in the face of climate change, disease, and ocean degradation. We manufacture from the nano to industrial scale, customizing our designs to help our clients reach their sustainability goals.\nTORONTO, ON —\nThe IntelliReefs research team is elated to report that our scientific premise with regards to coral reef pathogenic disease remediation has been accepted by the Program Director of the National Science Foundation (NSF) in Arlington, VA. NSF requested that IntelliReefs bring a novel, highly technical upgrade to our original submittal, which was an intensive study of wild coral recruitment and growth rates on our biomimicking substrates. Our research team is currently designing an extension of this research program that will implement the testing of our coral disease remediation prototype.\nOur current IntelliReefs project in Sint Maarten was just recognized with a Greening the Islands 2020 award on November 26, 2020. You can read our UN Ocean Action Update for SDG14 here. IntelliReefs will be speaking at the US Ocean Decade Launch Meeting held by the National Academies of Science, Engineering and Medicine from February 3-4, 2021 about our proposed ""Ocean Shot"" project examining coral recruitment and survival on pre-seeded IntelliReefs coral habitats with Zac Forsman from the Hawai\'i Institute of Marine Biology (HIMB).\nThe United Nations has declared 2021 through the end of 2030 the “Decade of Ocean Science for Sustainable Development” to address SDG14. This means that governing bodies around the world are going to double down on efforts to reverse the serious and long-lasting damage we have caused. This next decade will focus on adaptive management of vital ocean resources, data-driven scientific responses to climate change and degradation, and strategic use of global funding for restoration. IntelliReefs is dedicated to advancing not only our overall understanding of ecosystem protection and conservation, but our site and region-specific responses to careful restoration.\nTo increase our capacity for data-driven analysis and further customize our targeted restoration approach for site, species and function, IntelliReefs has partnered with innovative data-collection and technology companies in 2021. We are proud to announce that we will be working with iSENSYS, Absolute Vorticity, and Isla Mar Research Expeditions this year to conduct high resolution surveys of current and future deployment sites, gather much needed oceanographic data on and around current IntelliReefs, and advance our understanding of fish community dynamics at our sites.\nOur science and outreach partner, Reef Life Foundation, is joining us as we forge ahead with our current restoration efforts in Sint Maarten for 2021. They recently partnered with VR and immersive media company, Cyan Planet, to create the world\'s first coral restoration video game. They are raising funds to create a web-based game and app that allows for you the player to direct restoration efforts and deploy IntelliReefs restoration structures in the real world. Cyan Planet will develop the web-based game and app and Reef Life will manufacture and deploy the real world replica of the underwater coral city created in the game from IntelliReefs restoration habitats. Reef Life Foundation has also partnered with the polish animation company Studio Pigeon to produce an educational coral reef cartoon series for kids and accompanying classroom curriculum for grades 3-9. They are currently crowdfunding for the pre-production and development.\nIntelliReefs: Intelligently collaborating with the ocean to create habitat, food, tourism, and economic prosperity.\nIn response to continued coral reef degradation around the world, enhanced man-made habitats are being deployed in an attempt to build back underwater habitats that have been lost or severely degraded. Of the current advancements in coral reef restoration technology, IntelliReefs bio-enhancing nanotechnology substrates may be the most timely, natural, and cost-efficient way to build back physical structure, biodiversity, and resilience on a global scale.\nIntelliReefs is sustainably manufacturing intelligent marine habitats to save coral reefs and other vulnerable ocean ecosystems. We are a research-driven company that is physically and chemically engineering smart substrates to mitigate biodiversity loss, marine pathogens, effects of coral bleaching, and ocean acidification. IntelliReefs has created artificial coral reefs to increase biological and economic resilience, biodiversity, and available habitat, and has developed and proven a series of nanotechnology substrates called “Oceanite” for underwater construction and artificial reef manufacturing for restoration.\nIntelliReefs before and after 14 months of deployment in Philipsburg, Sint Maarten. In just over a year, the Oceanite substrates were home to a biodiverse array of reef organisms.\nOceanite mineral mixtures address declining marine biodiversity, ocean acidification, and environmental deterioration. IntelliReefs deployed their first pilot structures in Philipsburg, Sint Maarten in November 2018 that were engineered to mimic established coral reefs and build oceanic infrastructure that improves resistance to climate stressors and diseases. After conducting fieldwork to assess the efficacy of this pilot project in January 2020, IntelliReefs scientists found that Oceanite can increase local biodiversity, foster wild coral settlement, and accumulate a healthy, early stage coral community in just 14 months. Read more about the benefits of Oceanite here.\nIntelliReefs’ Co-Founder and concept designer, Guyon Brenna, describes the innovative philosophy behind their restoration approach: “IntelliReefs are both science and art; it is about finding the most beautiful way to communicate relationships between the objectives and the environment, the client, and the wildlife. Thus, when we design IntelliReefs, we think far beyond the material. We overlay the broader regional context, considering the objectives that you want to produce within, and design correspondingly.”\nIntelliReefs has 20 years of experience in advanced materials and manufacturing. Manufacturing Oceanite requires precision and the use of highly-skilled experts in nanomaterials and casting. Each Oceanite mineral mixture can be fine-tuned down to the nanoscale for site, species, and function requirements for a specific location, increasing the likelihood that its use will bring about high-impact conservation and economic results.\nSquirrelfish seeking shelter and food on IntelliReefs pilot project in Sint Maarten.\nBrenna explains the exclusive underwater experience created by IntelliReefs: “Our clients are drawn to the sea, by calm or by storm, sharing our passion for the intersection between ocean and land. As a result, we have designed a dramatic organic dive performance space, an elegant swimming path nestled within an existing reef, and many other restoration products building windows between the land and sea. While each Intellireefs project is as unique as its user, our day is always guided by a reverence for the environment, an understanding that we can only improve a small part within a greater whole, and our commitment for beautiful data driven design”.\nWith coral reefs are dying at unprecedented rates, marine species and coastal communities are left without food or storm protection. Reefs continue to degrade because we continue to take too much fish, and leave behind pollution and excess nutrients. The effects are made worse by rising carbon emissions, making our oceans warmer and more acidic every year.\nReefs in American Samoa before, during, and after a bleaching event (The Ocean Agency / XL Catlin Seaview Survey).\nConservation organizations and scientists around the world are engaged in a race against climate change to try to preserve and restore the world’s coral reefs. Sea surface temperatures on the Great Barrier Reef hit a record high last year as the world was self-isolating during the early stages of the COVID-19 pandemic. It was the most widespread mass bleaching event on record, and coral reef scientists estimate that we have lost about 50% of the Great Barrier Reef to date.\nCurrently, IntelliReefs is the only marine habitat building company on the market that has the ability to customize restoration substrates to enhance select biological variables by site and conservation goal. IntelliReefs is currently researching and developing additional Oceanite products for coral restoration that address disease mitigation, pre-seeding full artificial coral reefs with small colonies for accelerated restoration, and growth enhancing substrates for coral gardening projects that are more resilient to storms, bleaching, and acidification.\nReef conservation organizations need to make the conscious choice globally to only add artificial habitats and man-made structures onto reefs if they do not harm the existing ecosystem, address conservation goals on a large, ecologically relevant scale, and allow nature to build-up it\'s resilience to environmental and anthropogenic stressors. Otherwise, unsustainable or sub-par artificial habitats are simply fouling an already violated ecosystem.\nFor more information or to arrange an interview, please contact:\nFollow IntelliReefs online:\nAdditional Links for Media:\nVideo: “Why Corals Love IntelliReefs”\nHigh resolution photos are available upon request.\nIntelliReefs are innovative reef restoration systems; the result of breakthroughs in science and nanotechnology, these engineered structures mimic established coral reefs to build an oceanic infrastructure that improves resistance to climate stressors and diseases.\nIntelliReefs is a division of Reef Life Restoration, LLC with 20 years of earned experience in advanced materials, manufacturing methods and delivering custom projects, using scientific research, and architectural innovation to offer high performance marine restoration systems designed from nano to industrial scale.']"	['<urn:uuid:67696856-3baa-4c9f-8a6f-6f2344f275a3>', '<urn:uuid:7675fd54-bcdf-4197-a007-78fee0f4ca58>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T22:13:20.996306	13	72	2506
22	explain microscopic macroscopic physics principles relationship difference	The relationship between microscopic and macroscopic physics reveals a fascinating dichotomy. At the macroscopic level, Newtonian physics effectively describes visible objects and continues to be valid for force, motion, and gravitation calculations at low speeds. However, at the microscopic quantum level (nanometer scale or less), particles behave very differently - they lack well-defined positions and are subject to violent quantum fluctuations. These quantum fluctuations become more intense at shorter distances and seem to threaten the very existence of macroscopic order. This apparent contradiction is resolved through mechanisms like supersymmetry, which helps explain how the universe can maintain macroscopic stability despite microscopic quantum chaos.	['SW: Can you explain to us some of the places where supersymmetry shows up in these various theories, and what it does for you when it does show up?\nLet me back up for one second here: Supersymmetry is an extension of the symmetries of space-time, and it has this really interesting character. On the one hand, supersymmetric theories are examples of ordinary quantum field theories. They’re not radically outside the framework of the rubric handed down to us by our ancestors by the 1930s. But on the other hand, while being ordinary quantum field theories, they have extraordinary properties; they extend the symmetries of space-time. And so they fit at a nexus between two worlds. Considering this deep, central idea, it’s not surprising that it’s going to show up in a host of places.\nOne of the places it shows up is in attempts to extend, very pragmatically, the standard model of particle physics and to solve a variety of its problems. So there are these famous fine-tuning problems and other difficulties we have, which can be summarized as attempts to understand the following major puzzle: Because of quantum fluctuations—violent vacuum fluctuations that get more and more violent as you go to shorter and shorter distances—it seems to be impossible to have any macroscopic order in the universe at all. The universe is big, gravity is weak; there is a very big macroscopic universe, but that seems almost impossible given that there are these gigantic quantum fluctuations.\nSupersymmetry is one attempt to solve these problems by coming up with an explanation for why the quantum fluctuations disappear at short distances. This isn’t a small problem, a details thing. If you’re going to fix it, it’s going to need a big fix. The way supersymmetry does it is by extending the idea of space-time, and it does it in a way that you can’t fluctuate at all in these quantum dimensions. There’s a perfect symmetry between the quantum dimensions and the ordinary dimensions, and so the gigantic quantum fluctuations have to cancel out. That’s why it showed up and people care about it a lot in particle physics and in finding extensions of the standard model.\nIt also shows up all over the place in string theory, because if you’re going to have a quantum mechanical theory of gravity, which is what string theory is about, one of the first things it should do is give you a nice big macroscopic universe to play with—even a toy universe. Any other attempt to talk about quantum gravity just fails at this starting point, because of exactly the same violent quantum fluctuation problem. So supersymmetry shows up because it allows us to get going and even talk about it. It also shows up for other reasons.\nIt turns out that just the structure of quantum field theories—how to calculate with them, and see what the consequences are—is very rich, very complicated, and difficult to calculate with. When the couplings between quarks and gluons get strong, it’s impossible to calculate anything analytically, and for a long time people had no idea how to make progress. Supersymmetric theories have so many theoretical properties that you can really make wonderfully significant progress studying the dynamics of quantum field theories. And you do it by studying them in their most supersymmetric aspect first.\nSee:Nima Arkani-Hamed on Maximally Supersymmetric Theories– ScienceWatch.com correspondent Gary Taubes.\n- An interview with Arkani-Hamed on SUSY by Lubos Motl', 'What is the difference between Newtonian physics and quantum physics?\n1. Classical Newtonian mechanics deals with things that are larger – generally large enough to see, and quantum mechanics deals with things that are tiny – a nanometer or less, which is the size of atoms.\nDoes Newton’s laws apply in quantum physics?\nNewton was obliged to give his laws of motion as fundamental axioms. But today we know that the quantum world is fundamental, and Newton’s laws can be seen as consequences of fundamental quantum laws. This article traces this transition from fundamental quantum mechanics to derived classical mechanics.\nWhat is meant by quantum transition?\nIn physics, a quantum phase transition (QPT) is a phase transition between different quantum phases (phases of matter at zero temperature). The transition describes an abrupt change in the ground state of a many-body system due to its quantum fluctuations.\nCan classical physics be derived from quantum physics?\nClassical physics can be derived from quantum physics in the limit that the quantum properties are hidden. That fact is called the “correspondence principle.” Page 2 2. Quantum physics is the revolution that overthrew classical physics.\nIs Newtonian physics still valid?\nNewtonian physics continues to be applied in every area of science and technology where force, motion, and gravitation must be reckoned with. However, today’s physicists, unlike Newton, know that his laws do not work in all circumstances.\nIs Newtonian physics wrong?\nFor applications of mechanics at low speeds, Newtonian ideas are almost equal to reality. That is the reason we use Newtonian mechanics in practice at low speeds. On a conceptual level, Einstein did prove Newtonian ideas quite wrong in some cases, e.g. the relativity of simultaneity.\nDoes quantum physics disprove Newtonian physics?\nFirst of all, Quantum mechanics does NOT disprove Newtonian determinism. Newtonian determinism works wonderfully in the applications for which it was developed. However, it does not work for certain systems. That’s where you need Quantum mechanics to take over.\nDoes quantum physics contradict Newtonian physics?\nQuantum mechanics (QM) clearly violates Newton’s First Law of Motion (NFLM) in the quantum domain. In the process, a general argument is made that such a disparity may be found commonly for a wide variety of quantum predictions in the classical limit. The meaning of the classical limit is examined.\nIs the quantum realm?\nThe Quantum Realm is a dimension in the Multiverse only accessible through magical energy, mystical transportation using a Sling Ring, by tremendous subatomic shrinking caused by the Pym Particles, or a quantum bridge.\nHow are quantum mechanics and Newtonian mechanics related?\nThere was no explanation using using classical electromagnetism and newtonian mechanics 4) Interference effects seen in particles, like electrons, as if they were waves: individual electrons passing through slits showed an intensity pattern appropriate to waves not to newtonian particles\nWhat does it mean to talk about quantum phase transitions?\nTalking about quantum phase transitions means talking about transitions at T = 0: by tuning a non-temperature parameter like pressure, chemical composition or magnetic field, one could suppress e.g. some transition temperature like the Curie or Néel temperature to 0 K.\nWhere does the QPT occur in a quantum system?\nThe QPT occurs at the quantum critical point (QCP), where quantum fluctuations driving the transition diverge and become scale invariant in space and time. Although absolute zero is not physically realizable, characteristics of the transition can be detected in the system’s low-temperature behavior near the critical point.\nHow does the position of a particle affect Newtonian mechanics?\nIt’s not that the position of the particle won’t change the way that Newtonian mechanics predicts. It’s that particles don’t have well-defined positions in the first place. The uncertainty in position times the uncertainty in momentum must always be greater than a constant.']	['<urn:uuid:e6ec9bcb-f635-41ea-9701-d746d5819777>', '<urn:uuid:c0de551d-d8ff-468c-85df-a45a71255f07>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-12T22:13:20.996306	7	103	1205
23	magnet fishing boat dock safety requirements	For magnet fishing at boat docks, safety requirements include wearing non-slip shoes for slippery surfaces, using gloves to protect hands, wearing life jackets, and maintaining proper distance from water edges. Additionally, it's crucial to check weather conditions beforehand and ensure all fishing gear is in stable condition.	['Magnet fishing has become an increasingly popular outdoor activity for adventurers and curiosity seekers alike. The thrill of pulling up a piece of history from the depths of a body of water is unlike any other, especially when it’s something unexpected or unique. As enthusiasts embark on this treasure-hunting journey, they often wonder which locations yield the best results for magnet fishing.\nSea, harbor, and creek environments all offer unique opportunities for magnet fishing enthusiasts. Along the coast, seas and harbors are home to a rich history of maritime activities, giving magnet fishers the chance to uncover relics from shipwrecks, submerged docks, and other sunken structures. In contrast, creeks weave through a tapestry of rural and urban landscapes, littered with remnants of historic settlements and industrial activity – presenting an entirely different set of possibilities for discoveries.\nConsidering various factors such as accessibility, historical background, and local regulations, magnet fishers can maximize their chances of finding a hidden gem by choosing the right environment for their needs. Regardless of whether one prefers coastal or inland exploration, every location holds the potential for a magnetic surprise waiting to be discovered.\nMagnet Fishing Basics\nEquipment: Magnet, Rope, and Accessories\nMagnet fishing is an exciting hobby that involves using a strong neodymium magnet attached to a rope to find metallic objects submerged in various bodies of water, such as seas, harbors, and creeks. The primary components for this activity include a powerful magnet, a durable rope, and essential accessories.\n- Magnet: Choose a magnet that has a high pulling force. Neodymium magnets are ideal for this purpose, as they provide strong magnetic attraction, ensuring a successful search for underwater treasure.\n- Rope: A sturdy and long-lasting rope is crucial for magnet fishing. Look for ropes that have high tensile strength and are resistant to wear and tear. Often, a minimum length of 50 feet is recommended.\n- Accessories: Additional fishing gear such as gloves, grappling hooks, and a metal detector may enhance the overall experience and ensure safety while treasure hunting.\nSafety and Etiquette\nPracticing safety and following proper etiquette are essential considerations when engaging in magnet fishing. Here are some key points to keep in mind:\n- Be mindful of local regulations: Before starting your magnet fishing adventure, check the local laws and guidelines to ensure that the activity is permitted in the chosen area.\n- Wear appropriate attire: Protect your hands with gloves and wear non-slip shoes to avoid accidents on slippery surfaces like coastline rocks.\n- Avoid littering: Dispose of any trash found during magnet fishing responsibly. Keep a container or a bag to collect waste and discard it properly once you are done.\n- Inspect your equipment: Regularly examine your magnet, rope, and other accessories for damages or wear, and replace them as needed to ensure optimal performance and safety.\n- Be cautious by the water: Always remain vigilant near the water, especially when fishing in fast-flowing creeks or at the edge of a harbor. Keep a safe distance and look out for potential hazards.\nUnderstanding the essentials of magnet fishing and maintaining proper safety practices will create a more enjoyable experience for those who appreciate this unique hobby.\nIdeal Locations for Magnet Fishing\nRivers, Streams, and Canals\nMagnet fishing in rivers, streams, and canals can be rewarding due to the water flow constantly moving items around, increasing the chances of finding something interesting. These water bodies often have a history of industrial activity, boating, or fishing, meaning there could be a variety of metal objects to uncover. It’s important to choose locations with easy access, low water levels, and slow currents for safety reasons.\nPonds, Lakes, and Dams\nPonds, lakes, and dams are also excellent spots for magnet fishing enthusiasts. Since these bodies of water are usually receiving less human traffic, the chances of discovering older, more interesting items may be higher. Popular areas for magnet fishing in ponds, lakes, and dams include fishing docks, piers, and spillways, as these tend to accumulate metal debris over time.\nSea, Beaches, and Coastline\nMagnet fishing in sea, beaches, and coastline regions provides a unique challenge due to the influence of tides and waves. The best areas for these locations include rocky coastlines and tidal pools, where metallic objects can become trapped. It’s crucial to check tide tables and monitor wave heights to ensure safe magnet fishing conditions.\nHarbours, Marinas, and Boat Ramps\nHarbours, marinas, and boat ramps are prime locations for magnet fishing due to the high volume of human activity concentrated in these areas. Look for spots near popular tourist destinations, marina breakwaters, and fishing piers to increase the chance of finding valuable or fascinating relics. Always make sure to follow local regulations and guidelines when magnet fishing in these locations.\nPoints of Interest and Historical Significance\nBridges, Piers, and Boat Docks\nBridges, piers, and boat docks are common locations for magnet fishing due to the high amounts of foot traffic and activities that take place around them. Popular spots include the Ontario Bridge in Canada and the Ohio River bridges in the United States. These locations often have items of historical significance as they have been central to transportation and trade for centuries.\nTourist Attractions and Historic Sites\nTourist attractions and historic sites provide a unique opportunity for magnet fishers to explore areas rich in history. Many historic locations, such as the Tower of London or Statue of Liberty, are situated near bodies of water, allowing for a chance to find artifacts, coins, or other fascinating items. Additionally, tourist attractions tend to have an increased number of lost items due to the high volume of visitors.\nCities, Villages, and Waterways\nCities, villages, and waterways, whether located along a bustling waterfront or a quiet canal, hold intriguing potential for magnet fishing. These locations may have seen a multitude of activities throughout history, from commerce and transportation to fishing and leisure. Some interesting finds can be uncovered where waterways and human habitation intersect, revealing stories of times gone by.\nTypes of Treasures and Finds\nMagnet fishing offers a variety of interesting treasures and finds that enthusiasts can uncover below the water’s surface. This exciting pastime allows people to discover hidden items that range from historical artifacts to everyday objects. In this section, we will explore some common types of treasures one can find while magnet fishing, such as coins, jewelry, relics, cameras, trash, and other ferromagnetic items.\nMagnet fishing enthusiasts often find coins that have been lost or discarded in various bodies of water. Coins can be found near beaches, harbors, and creeks, and sometimes hold significant value. Some of these coins may have historical significance, dating back to ancient civilizations or important time periods. Collectors and history buffs alike can appreciate the thrill of discovering such treasures hidden beneath the water.\nJewelry and Relics\nAnother common find while magnet fishing is jewelry. Rings, bracelets, necklaces, and other accessories made of magnetic materials can be attracted to the magnet and pulled from their watery resting places. Some finds might be modern, while others may have historical or cultural significance. Relics from shipwrecks or significant events can be discovered as well, such as old tools, weapons, or even keys. These artifacts offer a glimpse into the past and serve as valuable conversation pieces.\nUnderwater cameras can sometimes be found during magnet fishing expeditions, especially near popular diving spots or high-traffic waterways. These cameras may have been accidentally dropped by their owners, or perhaps were lost during a mishap on a boat or watercraft. While the chances of finding a functioning camera might be slim, such a discovery can still make for a fascinating story.\nNot all magnet fishing finds are treasures, and it is not uncommon to retrieve trash from the water. Pulling trash from the water, such as cans, bottles, or discarded fishing gear, can help improve the environment by reducing pollution. Even if the find is not valuable, it still contributes to a cleaner ecosystem.\nAs the name suggests, magnet fishing involves using a strong magnet to attract and retrieve metallic items submerged in the water. Naturally, ferromagnetic objects such as nails, screws, fishing hooks, and other metal items are the most commonly found objects during magnet fishing excursions. These items may not be the most valuable or exciting finds, but they can often provide a starting point for conversations and speculation about how they ended up in the water.\nLegal and Permission Considerations\nWhen it comes to magnet fishing, understanding the legal aspects and gaining permission from the appropriate authorities is crucial. In this section, we will discuss the various local regulations, as well as the process of obtaining permission from landowners and authorities.\nNooks and Crannies of Local Regulations\nIt is essential for magnet fishers to become familiar with the local laws and regulations governing their chosen fishing spots. These rules may vary from one location to another, and can encompass restrictions on:\n- Access to specific waterways or types of bodies of water (sea, harbor, creek)\n- The types of magnets or equipment allowed\n- The maximum size and weight of finds that can be legally kept\nTo avoid problems, it is advisable to research these regulations thoroughly before venturing out to a particular area. This may involve consulting local government, law enforcement, or angler associations for up-to-date information.\nGaining Permission from Landowners and Authorities\nIn some cases, the land surrounding a magnet fishing spot may be privately owned or subject to certain restrictions. It is important to seek permission from landowners, harbormasters, or local authorities before conducting any magnet fishing activities in these areas. This process may require:\n- Identifying the appropriate person or organization to contact\n- Explaining the purpose and scope of your magnet fishing activities\n- Addressing any concerns or questions they may have\nBy taking the time to understand the legal and permission aspects of magnet fishing, you can help ensure a smooth and enjoyable experience for all involved.\nResources and Tools for Magnet Fishers\nMagnet fishing has been growing in popularity, and knowing the best places to search is essential for a successful experience. This section will cover various resources and tools for magnet fishers, focusing on mapping the best places, boat docks, grappling hooks, and useful add-ons.\nMapping the Best Places and Boat Docks\nFinding the perfect location for magnet fishing requires some research. Online maps and satellite images can help identify ideal spots based on geographical features and accessibility. Some popular options include:\n- Sea: Generally, calmer waters with low waves are preferred for magnet fishing. Sheltered bays or inlets are great choices as they often have less sand movement, increasing the chance of finding underwater treasures.\n- Harbour: Harbours often contain a high concentration of discarded items due to their historic and ongoing use. Well-used boat docks can be fruitful as items can accidentally fall into the water during loading and unloading.\n- Creek: Creeks and rivers often have items washed down from upstream, making them another option for magnet fishing enthusiasts. Keep an eye out for popular fishing spots as these locations have a higher chance of containing fishing equipment.\nGrappling Hooks and Useful Add-ons\nIn addition to powerful magnets, many magnet fishers rely on grappling hooks and other useful add-ons to improve their experience and increase their chances of finding valuable items. Some of these essential tools are:\n- Grappling Hook: A grappling hook can help retrieve items that are too large or awkwardly-shaped for a magnet alone. Having one on hand is beneficial when facing an unexpected discovery.\n- Gloves: It’s essential to protect your hands from sharp objects while handling hooks and underwater finds. A durable pair of gloves is necessary for any magnet fisher.\n- Rope: Choosing a strong, high-quality rope in your preferred length will make it easier to manipulate your magnet and grappling hook while ensuring a secure connection.\n- Container: A sturdy container or bucket for collecting and transporting finds is crucial for keeping the area tidy and protecting your discoveries.\nBy equipping yourself with these useful tools and resources, you can find success in your magnet fishing adventures in various locations, including the sea, harbours, and creeks.', 'Safety Tips for Family Fishing\nWe all love to spend some quality time with family and loved ones. Especially with the arrival of holidays or a weekend, many plans for the picnic. Fishing is one of the most trending family excursions. Family members of all the age groups love fishing. Fishing is a healthy option, from grandpa to the grandson. But one must have to prepare and gear up well when kids are around. Each year we heard of some tragic mishaps happened while fishing. Stop going for family fishing trips is not the answer. It’s highly recommended to follow the basic fishing safety tips to avoid such occurrence. Here we have narrowed down the basic ones to make your fishing expedition safe and sound.\nFollow the Weather!\nKnow the weather conditions prior to planning the trip. An open sunny day is perfect for fishing. The temperature should be moderate. There are certain fishing areas with unpredictable weather conditions. If you plan to head out there, check the weather forecast.\nCondition of the water Vehicle\nMake sure, your boat is in stable condition. Is it fulfilling all the laws of the lough? Check all these points earlier. Knowing the water conditions like depth or shallowness is highly beneficial in fishing.\nChoose an Active Fishing Area\nInstead of looking for a big one, we recommend you to look for an area with lots of small panfish. It would be more fun.\nProper Fishing Attire\nFishing areas are quite slippery and muddy. The surface of the boat and deck, all are dangerously slippery. You have to be extra cautious when little ones are around. The most important thing is the stable footing. Wear the shoes with strong grip over such slippery areas. Must not forget hats, life jackets and sunglasses. Polarized sunglasses are the most suitable ones from the fishing point of view.\nThere are certain things that you must have, to face any emergency. It may include a flashlight, first aid kit, map of the area, radio, cell phone, water, insect repellent and sunscreen. SPF sunscreen application protects you and your family from sunburn. For young anglers, we advise covering the sensitive areas like face, neck, ears well, with a sunblock.\nFeeling nausea in the water is quite a common problem. Especially for kids. If you have any family member with any such symptoms, it’s good to consult a doctor earlier. He would surely prescribe the patient with some anti-nausea or anti-seasickness medicines.\nSome areas have the most unpredictable weather conditions. For families living in such locations, keeping your rain gear like umbrellas or rain ponchos is highly recommended. In case, you hear the sound of an approaching thunderstorm, it’s highly recommended to leave the fishing spot as soon as possible. Try to find some safe shelter for you and family.\nRenew your Fishing Gear\nFishing equipment is worn out easily. Each time you plan a fishing trip, check closely all the fishing gear. No need to take any broken fishing tackle.\nEspecially with the kids, you must carry the barbless hooks. These are easily removable without any safety risks.\nPersonal Floatation Device\nThis is something, all should wear. But especially for kids boarding in boats, kayaks, and canoes.\nKids Fishing Gear\nKids also love to have some fishing experience. Do not let them, with your fishing gear. It’s highly recommended to buy the fishing gear especially made for kids. Its light weight makes fishing easier for them.\nFishing knives are a must have. These must be extra sharp. But with kids and family around, the blades must be covered when you are not using them.\nOff Limits Area\nThere are certain fishing areas that are prohibited for fishing. Before you hit the water, inquire well whether it’s an open fishing spot or an off limit area. The reasons may vary. It could be your safety, vegetation’s, or wildlife safety.\nSupervise the young Anglers\nOnce you have hit the water, do not leave the kids with their fishing gear. Always keep an eye on them. They must be told about certain rules for fishing in the water. That also includes no running on the boat.']	['<urn:uuid:86508340-bc28-44e3-be6f-80d29238ee36>', '<urn:uuid:210116c8-85cc-4803-8e6e-8ff06d8020e6>']	factoid	direct	short-search-query	similar-to-document	comparison	novice	2025-05-12T22:13:20.996306	6	47	2723
24	What makes zero unique compared to other numbers, and what are its applications?	Zero is unique as it represents nothingness or void - unlike other numbers, you can't physically count or touch it. It is fundamentally important in two ways: as a placeholder in our number system (like in 2106) and as a number itself (0). Zero is universal, transcending culture, space, and time, becoming part of our global language. It is essential in calculus, physics, engineering, computers, and financial theory. Ancient peoples initially didn't have an intuitive grasp of zero since mathematical problems started as 'real' problems rather than abstract ones - answers like 0 or negative numbers weren't considered possible solutions when dealing with concrete situations like counting horses.	"['How many elephants are in the same room as you right now? Most people would answer zero to that question (if you answered something else, we should be friends). The concept of zero is familiar to us. Earlier today, my two-year-old cousin told me that his baby sister is zero years old. I filed sales taxes for my business and typed up countless zeros. Today, zero is part of daily life. Even a two year old understands the concept of zero.\nZero is nothingness — a void. If you think deeper, it’s fairly amazing that we throw around such a profound term. I can see, touch and count the number of teabags left in a box, but I can’t see, touch or count the number of elephants in my bedroom. There are also zero storm troopers, zero cookies and zero dinosaurs in my bedroom. In my bedroom, there are an infinite number of zeros. Our number zero, symbolized by “0,” enables us to do calculus, and it’s even half of the reason my computer works right now. In the early days of math, zero didn’t exist — there wasn’t even a word for it, which made even simple arithmetic a bit complicated. Thankfully, ancient Babylonian, Mayan and Indian mathematicians developed the concept of zero and paved the road for truckloads of discovery and innovation.\nJust like ours, the Babylonian number system (2000 BC) was positional. In our base 10 system, having a positional number system simply means you have a position for ones, tens, hundreds, etc. Babylonians used the same concept except their ones position included the numbers 1-59 instead of 1-9. Regardless of base, the problem with having no zero is the numbers ‘11’ and ‘101’ suddenly both look like ‘11’. Most people can’t read minds, so that makes understanding other people’s writings a bit difficult. The Babylonians developed a place holding symbol to solve this dilemma. For example, if we used a period as a placeholder, those numbers would look like ‘11’ and ‘1.1’. It dispersed some confusion, but the placeholder could only be used between numbers, so ‘1’ and ‘100’ both looked like ‘1’. Without a zero, modern mathematics had no chance of developing.\nSimilarly to the Babylonians, the Mayans developed a placeholder symbol that stood for zero. They developed the notion completely independently of the Babylonians — after all, they were half way around the world and didn’t have texting. Their symbol for zero supposedly looks like a shell. To me, it looks more like a spaceship, but I digress. They had the concept of a placeholder, but like the Babylonians, they didn’t use the symbol on its own. Again, its a start, but you can’t add, subtract or multiply using a placeholder.\nThe hero of this story is a Hindu astronomer by the name of Brahmagupta. Around 628 AD, Brahmagupta wrote down rules for getting to zero using addition and subtraction and the results of using zero in equations. There are earlier traces of zeros in Cambodia and various parts of India, but Brahmagupta’s account is primary because it gave the rules behind using zeros. Brahmagupta called zero ‘sunya’ or ‘kha’ which mean ‘empty’ and ‘place’ respectively. His rules included things like ‘the sum of two zeros is zero’, ‘the product of a zero and any other number is zero’, and ‘zero divided by a zero is zero’. These rules were revolutionary. As simple as they seem, this one list of rules effectively changed the entire human world. You may have noticed something wrong with one of those rules — our modern mathematics don’t allow you to divide by zero. Brahmagupta’s rules about dividing by zero may have been flawed, but that just means he left something for G.W. Leibniz and Isaac Newton to work on later!\nAfter zero became a fully formed number, it spread like wildfire. Along with spices and other tradable goods, Arabian voyagers brought zero back from India. A hundred years after Brahmagupta discovered zero, it reached Baghdad. In the 9th century, a man named Mohammed ibn-Musa al-Khwarizmi started to develop algebra by working on equations that equaled zero. He called zero ‘sifr’ which turned directly into our word ‘cipher’ and eventually developed into our word ‘zero’. Come 879 AD, people wrote zero almost exactly like we do today; the only difference between our zero and theirs was size. They used an oval that was smaller than the other numbers — it became ‘1’, ‘1o’ and ‘1oo’. Finally, when the Moors invaded Spain they brought zero to Europe, and by the mid-1900s, Al-Khowarizmi’s work reached England at last.\nZero is universal; it transcends culture, space and time. It is part of our global language and is one of the most fundamental ideas in calculus, physics, engineering, computers, and a lot of financial and economic theory. Our lives are full of zeros. Plus, after traveling around the entire world and changing the course of human history, zero inspired this brilliant little video. Enjoy!', 'A history of Zero\n|Ancient Indian Mathematics index||History Topics Index|\nThe first thing to say about zero is that there are two uses of zero which are both extremely important but are somewhat different. One use is as an empty place indicator in our place-value number system. Hence in a number like 2106 the zero is used so that the positions of the 2 and 1 are correct. Clearly 216 means something quite different. The second use of zero is as a number itself in the form we use it as 0. There are also different aspects of zero within these two uses, namely the concept, the notation, and the name. (Our name ""zero"" derives ultimately from the Arabic sifr which also gives us the word ""cipher"".)\nNeither of the above uses has an easily described history. It just did not happen that someone invented the ideas, and then everyone started to use them. Also it is fair to say that the number zero is far from an intuitive concept. Mathematical problems started as \'real\' problems rather than abstract problems. Numbers in early historical times were thought of much more concretely than the abstract concepts which are our numbers today. There are giant mental leaps from 5 horses to 5 ""things"" and then to the abstract idea of ""five"". If ancient peoples solved a problem about how many horses a farmer needed then the problem was not going to have 0 or -23 as an answer.\nOne might think that once a place-value number system came into existence then the 0 as an empty place indicator is a necessary idea, yet the Babylonians had a place-value number system without this feature for over 1000 years. Moreover there is absolutely no evidence that the Babylonians felt that there was any problem with the ambiguity which existed. Remarkably, original texts survive from the era of Babylonian mathematics. The Babylonians wrote on tablets of unbaked clay, using cuneiform writing. The symbols were pressed into soft clay tablets with the slanted edge of a stylus and so had a wedge-shaped appearance (and hence the name cuneiform). Many tablets from around 1700 BC survive and we can read the original texts. Of course their notation for numbers was quite different from ours (and not based on 10 but on 60) but to translate into our notation they would not distinguish between 2106 and 216 (the context would have to show which was intended). It was not until around 400 BC that the Babylonians put two wedge symbols into the place where we would put zero to indicate which was meant, 216 or 21 \'\' 6.\nThe two wedges were not the only notation used, however, and on a tablet found at Kish, an ancient Mesopotamian city located east of Babylon in what is today south-central Iraq, a different notation is used. This tablet, thought to date from around 700 BC, uses three hooks to denote an empty place in the positional notation. Other tablets dated from around the same time use a single hook for an empty place. There is one common feature to this use of different marks to denote an empty position. This is the fact that it never occured at the end of the digits but always between two digits. So although we find 21 \'\' 6 we never find 216 \'\'. One has to assume that the older feeling that the context was sufficient to indicate which was intended still applied in these cases.\nIf this reference to context appears silly then it is worth noting that we still use context to interpret numbers today. If I take a bus to a nearby town and ask what the fare is then I know that the answer ""It\'s three fifty"" means three pounds fifty pence. Yet if the same answer is given to the question about the cost of a flight from Edinburgh to New York then I know that three hundred and fifty pounds is what is intended.\nWe can see from this that the early use of zero to denote an empty place is not really the use of zero as a number at all, merely the use of some type of punctuation mark so that the numbers had the correct interpretation.\nNow the ancient Greeks began their contributions to mathematics around the time that zero as an empty place indicator was coming into use in Babylonian mathematics. The Greeks however did not adopt a positional number system. It is worth thinking just how significant this fact is. How could the brilliant mathematical advances of the Greeks not see them adopt a number system with all the advantages that the Babylonian place-value system possessed? The real answer to this question is more subtle than the simple answer that we are about to give, but basically the Greek mathematical achievements were based on geometry. Although Euclid\'s Elements contains a book on number theory, it is based on geometry. In other words Greek mathematicians did not need to name their numbers since they worked with numbers as lengths of lines. Numbers which required to be named for records were used by merchants, not mathematicians, and hence no clever notation was needed.\nNow there were exceptions to what we have just stated. The exceptions were the mathematicians who were involved in recording astronomical data. Here we find the first use of the symbol which we recognise today as the notation for zero, for Greek astronomers began to use the symbol O. There are many theories why this particular notation was used. Some historians favour the explanation that it is omicron, the first letter of the Greek word for nothing namely ""ouden"". Neugebauer, however, dismisses this explanation since the Greeks already used omicron as a number - it represented 70 (the Greek number system was based on their alphabet). Other explanations offered include the fact that it stands for ""obol"", a coin of almost no value, and that it arises when counters were used for counting on a sand board. The suggestion here is that when a counter was removed to leave an empty column it left a depression in the sand which looked like O.\nPtolemy in the Almagest written around 130 AD uses the Babylonian sexagesimal system together with the empty place holder O. By this time Ptolemy is using the symbol both between digits and at the end of a number and one might be tempted to believe that at least zero as an empty place holder had firmly arrived. This, however, is far from what happened. Only a few exceptional astronomers used the notation and it would fall out of use several more times before finally establishing itself. The idea of the zero place (certainly not thought of as a number by Ptolemy who still considered it as a sort of punctuation mark) makes its next appearance in Indian mathematics.\nThe scene now moves to India where it is fair to say the numerals and number system was born which have evolved into the highly sophisticated ones we use today. Of course that is not to say that the Indian system did not owe something to earlier systems and many historians of mathematics believe that the Indian use of zero evolved from its use by Greek astronomers. As well as some historians who seem to want to play down the contribution of the Indians in a most unreasonable way, there are also those who make claims about the Indian invention of zero which seem to go far too far. For example Mukherjee in  claims:-\n... the mathematical conception of zero ... was also present in the spiritual form from 17 000 years back in India.What is certain is that by around 650AD the use of zero as a number came into Indian mathematics. The Indians also used a place-value system and zero was used to denote an empty place. In fact there is evidence of an empty place holder in positional numbers from as early as 200AD in India but some historians dismiss these as later forgeries. Let us examine this latter use first since it continues the development described above.\nIn around 500AD Aryabhata devised a number system which has no zero yet was a positional system. He used the word ""kha"" for position and it would be used later as the name for zero. There is evidence that a dot had been used in earlier Indian manuscripts to denote an empty place in positional notation. It is interesting that the same documents sometimes also used a dot to denote an unknown where we might use x. Later Indian mathematicians had names for zero in positional numbers yet had no symbol for it. The first record of the Indian use of zero which is dated and agreed by all to be genuine was written in 876.\nWe have an inscription on a stone tablet which contains a date which translates to 876. The inscription concerns the town of Gwalior, 400 km south of Delhi, where they planted a garden 187 by 270 hastas which would produce enough flowers to allow 50 garlands per day to be given to the local temple. Both of the numbers 270 and 50 are denoted almost as they appear today although the 0 is smaller and slightly raised.\nWe now come to considering the first appearance of zero as a number. Let us first note that it is not in any sense a natural candidate for a number. From early times numbers are words which refer to collections of objects. Certainly the idea of number became more and more abstract and this abstraction then makes possible the consideration of zero and negative numbers which do not arise as properties of collections of objects. Of course the problem which arises when one tries to consider zero and negatives as numbers is how they interact in regard to the operations of arithmetic, addition, subtraction, multiplication and division. In three important books the Indian mathematicians Brahmagupta, Mahavira and Bhaskara tried to answer these questions.\nBrahmagupta attempted to give the rules for arithmetic involving zero and negative numbers in the seventh century. He explained that given a number then if you subtract it from itself you obtain zero. He gave the following rules for addition which involve zero:-\nThe sum of zero and a negative number is negative, the sum of a positive number and zero is positive, the sum of zero and zero is zero.Subtraction is a little harder:-\nA negative number subtracted from zero is positive, a positive number subtracted from zero is negative, zero subtracted from a negative number is negative, zero subtracted from a positive number is positive, zero subtracted from zero is zero.Brahmagupta then says that any number when multiplied by zero is zero but struggles when it comes to division:-\nA positive or negative number when divided by zero is a fraction with the zero as denominator. Zero divided by a negative or positive number is either zero or is expressed as a fraction with zero as numerator and the finite quantity as denominator. Zero divided by zero is zero.Really Brahmagupta is saying very little when he suggests that n divided by zero is n/0. Clearly he is struggling here. He is certainly wrong when he then claims that zero divided by zero is zero. However it is a brilliant attempt from the first person that we know who tried to extend arithmetic to negative numbers and zero.\nIn 830, around 200 years after Brahmagupta wrote his masterpiece, Mahavira wrote Ganita Sara Samgraha which was designed as an updating of Brahmagupta\'s book. He correctly states that:-\n... a number multiplied by zero is zero, and a number remains the same when zero is subtracted from it.However his attempts to improve on Brahmagupta\'s statements on dividing by zero seem to lead him into error. He writes:-\nA number remains unchanged when divided by zero.Since this is clearly incorrect my use of the words ""seem to lead him into error"" might be seen as confusing. The reason for this phrase is that some commentators on Mahavira have tried to find excuses for his incorrect statement.\nBhaskara wrote over 500 years after Brahmagupta. Despite the passage of time he is still struggling to explain division by zero. He writes:-\nA quantity divided by zero becomes a fraction the denominator of which is zero. This fraction is termed an infinite quantity. In this quantity consisting of that which has zero for its divisor, there is no alteration, though many may be inserted or extracted; as no change takes place in the infinite and immutable God when worlds are created or destroyed, though numerous orders of beings are absorbed or put forth.So Bhaskara tried to solve the problem by writing n/0 = ∞. At first sight we might be tempted to believe that Bhaskara has it correct, but of course he does not. If this were true then 0 times ∞ must be equal to every number n, so all numbers are equal. The Indian mathematicians could not bring themselves to the point of admitting that one could not divide by zero. Bhaskara did correctly state other properties of zero, however, such as 02 = 0, and √0 = 0.\nPerhaps we should note at this point that there was another civilisation which developed a place-value number system with a zero. This was the Maya people who lived in central America, occupying the area which today is southern Mexico, Guatemala, and northern Belize. This was an old civilisation but flourished particularly between 250 and 900. We know that by 665 they used a place-value number system to base 20 with a symbol for zero. However their use of zero goes back further than this and was in use before they introduced the place-valued number system. This is a remarkable achievement but sadly did not influence other peoples.\nYou can see a separate article about Mayan mathematics.\nThe brilliant work of the Indian mathematicians was transmitted to the Islamic and Arabic mathematicians further west. It came at an early stage for al-Khwarizmi wrote Al\'Khwarizmi on the Hindu Art of Reckoning which describes the Indian place-value system of numerals based on 1, 2, 3, 4, 5, 6, 7, 8, 9, and 0. This work was the first in what is now Iraq to use zero as a place holder in positional base notation. Ibn Ezra, in the 12th century, wrote three treatises on numbers which helped to bring the Indian symbols and ideas of decimal fractions to the attention of some of the learned people in Europe. The Book of the Number describes the decimal system for integers with place values from left to right. In this work ibn Ezra uses zero which he calls galgal (meaning wheel or circle). Slightly later in the 12th century al-Samawal was writing:-\nIf we subtract a positive number from zero the same negative number remains. ... if we subtract a negative number from zero the same positive number remains.The Indian ideas spread east to China as well as west to the Islamic countries. In 1247 the Chinese mathematician Ch\'in Chiu-Shao wrote Mathematical treatise in nine sections which uses the symbol O for zero. A little later, in 1303, Zhu Shijie wrote Jade mirror of the four elements which again uses the symbol O for zero.\nFibonacci was one of the main people to bring these new ideas about the number system to Europe. As the authors of  write:-\nAn important link between the Hindu-Arabic number system and the European mathematics is the Italian mathematician Fibonacci.In Liber Abaci he described the nine Indian symbols together with the sign 0 for Europeans in around 1200 but it was not widely used for a long time after that. It is significant that Fibonacci is not bold enough to treat 0 in the same way as the other numbers 1, 2, 3, 4, 5, 6, 7, 8, 9 since he speaks of the ""sign"" zero while the other symbols he speaks of as numbers. Although clearly bringing the Indian numerals to Europe was of major importance we can see that in his treatment of zero he did not reach the sophistication of the Indians Brahmagupta, Mahavira and Bhaskara nor of the Arabic and Islamic mathematicians such as al-Samawal.\nOne might have thought that the progress of the number systems in general, and zero in particular, would have been steady from this time on. However, this was far from the case. Cardan solved cubic and quartic equations without using zero. He would have found his work in the 1500\'s so much easier if he had had a zero but it was not part of his mathematics. By the 1600\'s zero began to come into widespread use but still only after encountering a lot of resistance.\nOf course there are still signs of the problems caused by zero. Recently many people throughout the world celebrated the new millennium on 1 January 2000. Of course they celebrated the passing of only 1999 years since when the calendar was set up no year zero was specified. Although one might forgive the original error, it is a little surprising that most people seemed unable to understand why the third millennium and the 21st century begin on 1 January 2001. Zero is still causing problems!\nReferences (14 books/articles)\nOther Web sites:\nAstroseti (A Spanish translation of this article)\nArticle by: J J O\'Connor and E F Robertson']"	['<urn:uuid:bfc02041-9236-4d9c-8180-67d585fdd8bd>', '<urn:uuid:36642d73-15f0-4d00-b82d-f3e3fb3b1a60>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T22:13:20.996306	13	108	3746
25	How frequently are the oil pressure measurements collected in the production line monitoring system?	The oil pressure values are collected once an hour and stored in Azure SQL.	['In this article, we will learn Key Performance Indicators usage in Power BI and solve a business case problem through Power BI.\nKey Performance Indicators also known as KPI are very efficient indicators that compare any organization target and real performance. Through the KPIs, organizations can evaluate their success at reaching the target. In fact, the main idea of KPI is very simple that is “If you don’t measure it, you can’t manage it”. According to this idea, KPI are crucial metrics for organizations, at the same time we need to track KPI for effective management capability. Such as, if you manage a sales team you have to track monthly sales growth. This KPI measures the sales revenue according to the sales target or compares with the same prior month. With help of these monthly sales Key Performance Indicators, you can get an idea about your sales trend so that you can decide the product prices or campaigns.\nOn the other hand, business intelligence solutions play key-role for organizations to make decisions about business so we need to virtualize KPIs in business intelligence reports. If we place KPIs in our BI reports or dashboards that offer to track KPIs regularly and assists to take proper decision.\nAfter this main description of Key Performance Indicators and getting through the importance of interaction with business intelligence solutions, we can start to talk about how to use KPIs in Power BI. In this article, we will create the demonstration over a production case scenario.\nBusiness Case: Assume that you are report writer of a production company and there is a production line which is the vital part of the factory. This production line oil pressure value has to be 80 and this value indicates the optimum performance of the production line. Also, this value is collected one an hour and store into Azure SQL. Production engineers’ wants to track this production line oil pressure in their mobile phones and they also want to observe the last value of selected month and weekly Key Performance Indicators of this production line oil pressure values. After a little meeting with production engineers, you deal a draft design which looks like the below image.\nDemonstration: At first, we need to generate synthetic data in the Azure SQL. The table structure is very simple, it stores oil pressure values and measurement of date. At first, we will create the oil pressure log table through the following T-SQL script.\nCREATE TABLE OilpressureLog\n(Id Int IDENTITY(1,1) PRIMARY KEY,\nValOilPre INT ,\nAnd then populate it with test data using a tool like ApexSQL Generate\nAfter this test data preparation step, we will launch the Power BI and connect to Azure SQL database.\n- Click Get Data and then select More… option\nClick Azure tab and then select Azure SQL database\n- Click Connect\n- Set the connection credentials of Azure SQL database\n- Select the Database tab and then set the connection credentials of the database\nAfter all these steps, Power BI will launch Navigator screen. In this screen, we will choose the OilpressureLog table in order to retrieve data.\nAccording to our design draft, we will add slicers and this slicer help us to filter month number of year but in our dataset we don’t have any column which indicates this column. At the same time, we need to week of year column to calculate weekly oil pressure Key Performance Indicators. So, we will create two new columns which help to figure out the week of year and month. Select the Modeling tab and add a new column. Write the following DAX (Data Analysis Expressions) expression and this expression will calculate the week of the year.\nLogDateWeek = WEEKNUM(OilpressureLog[Dt])\nWe will repeat the previous step but will only change the DAX expression to calculate month number of year.\nAnd then, we will add the target value of oil pressure column. This value is constant therefore we don t need to calculate any think about this value.\nNow, we will add the month filter slicer visual.\n- Drag and drop slicer visual to Power BI design panel\nDrag and drop LogDateMonth column to Field\n- Hover over the mouse in the right corner of the slicer and then click the small down arrow and then chose the list option\nNow we will add our custom Key Performance Indicators visual from the marketplace.\n- Click Import from marketplace\nAdd custom KPI Indicator from Power BI Visuals marketplace\n- Drag and drop KPI Indicators visual to design panel\n- Set Actual value field to ValOilPre\n- Set Target value field to TargetVal\nSet Trend axis field to LogDateWeek\nChange the ValOilPre and TargetVal calculation to Average\nChange the Key Performance Indicators visual Banding Type to closer be better because optimum oil pressure value must be 80 according to our report documentation\nClick the right three point (…) in the right corner of the KPI visual and change the sort of data\nThe last need of this Power BI report is the last value of the oil pressure in the selected dataset. In this calculation, we will use the following DAX expression. With help of this DAX, we will figure out the last value of oil pressure. Select the Modelling tab and then we will add LastValOfOilPressure measure.\nMeasureLastValOfOilPressure = CALCULATE(AVERAGE(OilpressureLog[ValOilPre]);FILTER(OilpressureLog;OilpressureLog[Id]=MAX(OilpressureLog[Id])))\nDrag and drop card visual and then set Fields to MeasureLastValOfOilPressure\nEventually, all visuals are ready for phone design. We will select the View tab and then select Phone Layout.\nWe will design all visuals which are like the below image and then return to desktop layout.\nFinally, we will publish our report to Power BI portal.\nWhen we download mobile app of Power BI and connect to the Power BI web portal, the report Phone design will look like the below image.\nIn this article, we mentioned about Key Performance Indicators and how to handle a business case with help of Power BI. Actually the organization more sophisticated than this one but we can find several alternative visual in Power BI Visual Market Place so that we can handle this problem very easily. In addition, we can increase our flexibility through Power BI mobile support.']	['<urn:uuid:978d4930-e797-44ca-bf6c-5f1fa7a66b94>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T22:13:20.996306	14	14	1034
26	what complications can happen during delivery with shoulder dystocia in babies	The complications during delivery with shoulder dystocia include brachial plexus injury (neurological damages), generalized asphyxia, hematoma, clavicle fracture, and humerus fracture.	"['New Prophylactic Maneuver: the ""Pushing"" Maneuver, Aiming to Reduce the Risk for Shoulder Dystocia (CONTRADYS)\n|Shoulder Dystocia, Brachial Plexus Injury Asphyxia, Hematoma, Clavicle Fracture, Humerus Fracture.||Procedure: suctioning of fetal nose and mouth during delivery Procedure: Pushing maneuver|\n|Study Design:||Allocation: Randomized\nIntervention Model: Parallel Assignment\nMasking: Single (Participant)\nPrimary Purpose: Prevention\n|Official Title:||CONTRADYS : A Randomized Controlled Trial of a New Prophylactic Maneuver, the ""Pushing"" Maneuver, Aiming to Reduce the Risk for Shoulder Dystocia|\n- occurrence of shoulder dystocia [ Time Frame: during delivery ]shoulder dystocia is defined as a necessity of requiring a specific obstetrical maneuver (McRoberts\' maneuver).\n- complications [ Time Frame: 5 days after delivery ]neonatal complications including neurological damages (brachial plexus injury), generalized asphyxia, hematoma, clavicle and humerus fractures.\n|Study Start Date:||March 2011|\n|Study Completion Date:||March 2014|\n|Primary Completion Date:||November 2013 (Final data collection date for primary outcome measure)|\nNormal delivery without ""pushing"" maneuver suctioning of fetal nose and mouth during delivery\nProcedure: suctioning of fetal nose and mouth during delivery\nNormal delivery without ""pushing"" maneuver: either an expectative attitude or a suctioning of fetal nose and mouth during delivery, since the crowning of the head (appearance of the fetal scalp at the introitus between pushes).\nOther Name: M:Mouchage meaning suctioning of fetal nose and mouth\nExperimental: group C\n""Pushing"" maneuver on the fetal head\nProcedure: Pushing maneuver\nThe ""pushing"" maneuver is performed gently on the fetal head since the crowning of the head (appearance of the fetal scalp at the introitus between pushes), during one uterine contraction, aiming to facilitate the anterior shoulder to slip off behind the symphysis pubis, reducing thus the risk of shoulder dystocia.\nOther Name: C ;""contre pulsion"" in French, meaning ""Pushing maneuver""\nHypothesis: the ""pushing"" maneuver reduces of 50% the risk of shoulder dystocia in comparison with either an expectative attitude or a suctioning of fetal nose and mouth.\nMain objective: to assess whether prophylactic use of the ""pushing on the fetal head"" maneuver decreases the prevalence of shoulder dystocia, in comparison with an expectative attitude or a suctioning of fetal nose and mouth.\nSecondary objective: to compare the occurrence of neonatal complications including brachial plexus injury, clavicle and humerus fracture, hematoma and generalized asphyxia.\nMain criterion: occurrence of shoulder dystocia, defined by a necessity of requiring a specific obstetrical maneuver (McRoberts\' maneuver).\nSecondary criterion: neonatal complications including neurological damages (brachial plexus injury), generalized asphyxia, hematoma, clavicle and humerus fractures.\nMethods: prospective, randomized, multicenter blind study with a modified intention-to-treat analysis. Patients are included during the last obstetrical consultation and randomized in the delivery room.\nNumber of patients (α error, β error): a sample size of 1126 patients was calculated to allow detection of a 50% reduction of shoulder dystocia, with a prevalence of dystocia reaching 4.3% in usual deliveries (expectative attitude or suctioning of fetal nose and mouth), with a 65% dystocia risk decrease in the group C (α error of 0.05, β error of 0.20).\nInclusion and exclusion criteria. Inclusion: women having completed 37 or more gestational weeks with singleton vertex fetus, delivering vaginally. Exclusion: patients with caesarean section are excluded.\nPlace of the study: department of gynecology and obstetrics, BEAUJON hospital, Clichy, France and department of gynecology and obstetrics, BICHAT hospital, Paris, France.\nDuration of inclusion: two years and 6 months Duration of patients\' participation: two months maximum Duration of the study: two years and 9 months. Mean number of inclusion each month: 30 Number of investigation centre: 2 (BEAUJON hospital, BICHAT hospital).\nPlease refer to this study by its ClinicalTrials.gov identifier: NCT01297439\n|Clichy, France, 92110|\n|Principal Investigator:||Olivier Poujade, MD||Assistance Publique - Hôpitaux de Paris|']"	['<urn:uuid:78a948fb-5b1c-4a3b-8e81-d130fa595d67>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T22:13:20.996306	11	21	597
27	What drives cold storage demand and fruit consumption patterns?	Cold storage demand and fruit consumption patterns are influenced by multiple factors. Tree fruit per capita consumption has declined over 20 years because consumers stop buying fruit that looks good but tastes bad, leading growers to focus on new varieties with better flavor. Meanwhile, cold storage demand is driven by growing middle-class populations in Asia Pacific, with grocery imports valued at $373 billion in 2018. Online grocery shopping is expanding rapidly, with Asia Pacific online grocery sales projected to grow from $80.7 billion in 2018 to $260 billion in 2023. The market is particularly strong for high-value items, with imports of berries showing a 7.5% CAGR between 2013 and 2018.	"['Growing up in the produce industry and working summers on the wholesale produce market in downtown Los Angeles, my fondest memories are of the wonderful smell of tree ripened summer fruit.\nYou know what I mean. Peaches, nectarines and plums… These fruits were at their peak of season while I was working summers in high school. If I was lucky, our next door neighbor on the produce market, Giumarra Brothers, would occasionally give us a box of fruit to enjoy.\nWell, it’s almost that time of year again.\nOver the past few weeks, Frieda’s has been receiving bulletins from many of our fruit grower friends, updating us on their crop projections. I know it’s hard to believe, but some of the biggest grower/shippers (a shipper is a company that packs and sells fruit on behalf of growers) start working with their big retail customers (Kroger, Wal-Mart, Safeway, Albertson’s and all the regional chain stores) at least 6 months in advance to plan their promotional activities, so the fruit is pre-sold before it is harvested.\nUnfortunately, over the last 20 years, the tree fruit industry has seen a decline in tree fruit per capita consumption. I’m pretty sure most of that is because consumers vote with their dollars. If you go to the store and buy fruit that looks good but does not taste good, you just don’t buy it again.\nYears ago, too many growers thought that the appearance of the fruit was the most important quality – a red blush on the outside skin, or “high color” as we call it in the trade. Many growers planted beautiful-looking fruit that did not taste good when they ripened. Or, they harvested fruit before it was mature, and it never got that juicy, mouth watering ripeness and flavor. So, consumers stopped buying them.\nOur industry also found out something incredibly important. Tree fruit must either be kept REALLY cold (35 degrees or colder), or it must be warm (over 58 degrees). If the fruit is held at the temperature zone in between before it ripens (36 – 57 degrees, like in storage or at the supermarket warehouse), it is the “killing zone,” and that temperature range basically kills the flavor of the fruit.\nSo what does this mean to you, the home shopper?\n- Buy only fruit that you can consume in a few days.\n- When you get it home, put the tree fruit on the counter out of the sun and let it ripen up (which it will do quickly). Or if you want to hasten the ripening, put the fruit in a brown paper bag to create a mini ripening atmosphere.\n- Only after the fruit is soft and smells good can you put it in the refrigerator. But really, you should just eat it!\n- Don’t refrigerate unripened tree fruit, because your home refrigerator is not usually under 35 degrees… It’s probably around 37 degrees, aka “the killing zone” for tree fruit flavor.\nThe good news is that growers are aware of the poor practices in the past and there has been a wave of new fruit varieties with fantastic flavor. Many of the old varieties have been “torn out” and replaced with newer varieties. And our industry has spent a lot of research and education dollars letting retailers know best practices for handling tree fruit.\nI am looking forward to this year’s harvest of peaches and nectarines. (Nectarines are my personal favorite!) I think we will see some wonderful tasting fruit. Plus, the newest varieties of Pluots (a cross between a plum and apricot) also have some amazing taste profiles.\nI would love to hear about your experiences with this year’s fruit crop. Feel free to post your comments on our company Facebook Page: http://www.facebook.com/FriedasSpecialtyProduce', ""Growing investor interest in cold storage real estate has been brewing for some time. This appetite is part of a larger push into less competitive areas of industrial real estate. The shift in demographic and consumer behaviour is also playing a major role in fanning the interest.\nThere is a robust demand for high quality industrial and logistics assets in Asia Pacific in recent years. Irrespective of asset type, the driver has frequently come from strong domestic consumption.\nCoupled with the juggernaut of e-commerce, the associated industry expansion and the ongoing development of modern logistics facilities into an institutional investment product, cold storage has carved out a substantial niche with the promise of sustainable growth for the foreseeable future.\nIt is not a new concept in Asia Pacific, but the investment opportunity cold storage offers hasn't been fully realised. Until recently, that is. As a result, the pickup in interest has led to investors seeking investment opportunities at all stages of the supply chain, with temperature-controlled warehouses, or cold storage, rapidly emerging as an area of focus.\nPart of this pickup can also be explained by investors' deeper understanding of cold storage facilities. Increasingly, the latter have been decoupled from other warehouse facilities in industrial and logistical portfolios.\nThe reason? Cold storage facilities are far more complex than conventional dry warehouses. Irrespective of location, they must be equipped with various structural and mechanical installations to maintain temperature and humidity within a pre-defined range.\nInvestors also better understand the operators of cold storage facilities. Based on their operational model and scope of services, cold storage players can be divided into four main types.\n- Cold storage and logistics specialists handle agricultural produce in cold storage facilities that often come equipped with food processing capabilities. For example, kiwi fruit in New Zealand are collected and shipped from orchards to specialised cold storage companies, where they are then sorted, washed, packaged, labelled and boxed for export. This category also includes cold chain providers for restaurant chains.\n- End-users and owner-occupiers include leading grocery, FMCG and pharmaceutical companies. These firms frequently opt to manage their cold chain in-house as they have special requirements in terms of layout and equipment.\n- Third-party logistics (3PLs) firms have set up specialised business units catering to rising demand for distribution and delivery service along the cold chain.\n- Developers and investors are the primary owners of cold storage facilities. Recently, a number of new players have entered the cold storage business, including China Vanke, which purchased Swire Cold Chain Logistics with a portfolio of seven cold storage warehouses in Chengdu, Guangzhou, Langfeng, Ningbo, Shanghai and Xiamen last year.\nDemand for cold storage facilities in Asia Pacific is being powered by several factors.\nA mix of consumption and omnichannel distribution\nParticularly in Asia, a growing middle-class population is driving the fortunes of cold storage. This is especially true when considering robust demand for high quality groceries sourced from home and abroad continues to swell.\nAsia Pacific grocery imports were valued at US$373 billion in 2018, with imports of high-value items growing especially rapidly. Imports of berry fruits including strawberries and blackberries to Asia Pacific recorded a CAGR of 7.5 per cent between 2013 and 2018.\nAs a result, growing consumption is being facilitated by the expansion of omnichannel distribution. However, online grocery shopping is still at a nascent stage, with less than 5 per cent of grocery spending in most Asia Pacific markets occurring online in 2018. This ratio is expected to increase on the back of rapid improvement in delivery speed, especially in Korea and China.\nAlso, working in the market's favour, is online grocery retailing. Aggregate online grocery sales in Asia Pacific are forecast to surge from US$80.7 billion in 2018 to US$260 billion in 2023.\nFood loss and quality concerns\nInvestors also see the potential of cold storage facilities in maintaining the integrity of food. Cold storage can significantly reduce food loss and wastage during transportation, storage and handling by extending the shelf-life of temperature-sensitive products. For example, post-harvest tomatoes stay fresh for just three days at a temperature of 35 degrees Celsius, compared to 14 days at a temperature of 15 degrees Celsius.\nAdded to the mix, reducing food loss and wastage can play a key role in improving retailers' bottom line. Food losses in China amounted to US$15 billion in 2017, primarily due to inadequate cold chain facilities. The total volume of food losses was around 20 per cent of groceries distributed, a ratio three times higher than that in advanced economies. Proper handling and storage of food products can also reduce the risk of cross-contamination.""]"	['<urn:uuid:a7329179-dbf6-4b5f-a3db-3b5601b70769>', '<urn:uuid:46b9b32f-0c2b-4ec3-960b-5d5528b20d09>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T22:13:20.996306	9	110	1401
28	ct mri radiation safety cancer risk	CT scans and MRI differ significantly in their safety profiles regarding radiation exposure. MRI scans use strong magnets and radio waves without any ionizing radiation, making them safer from a radiation perspective. CT scans, however, use X-rays which expose patients to ionizing radiation. While CT is valuable for medical imaging, there's growing evidence about potential cancer risks. According to recent research, even a few CT scans may pose a very low but real cancer risk, particularly concerning for organ doses exceeding 100 mGy. The exact cancer risk from a single CT scan remains debated, with risk estimates based primarily on data from Hiroshima and Nagasaki survivors. Professional organizations recommend tracking radiological examinations, especially for children, to monitor cumulative radiation exposure.	['Doctors routinely request diagnostic imaging tests to monitor what is happening inside your body. The various imaging tests help the doctor make a clear diagnosis and decide on the best treatment option.\nYour doctor can identify specific medical conditions with the use of images produced by these imaging tests.\nWhat is a CT Scan?\nCT, computerized axial tomography, produces images of the body, including bone, using rotating x-rays. The patient lying on the table is rotated around by the x-ray tube. The x-ray detector is located on the patient’s opposite side. This detector picks up the beam that passes through the patient.\nThe best uses for a CT scan (also called a CAT scan) are evaluating bone damage, identifying lung and chest conditions, and finding cancer.\n- Bone structure imaging using CT is effective.\n- Some people cannot have an MRI but can have a CT scan if they have specific surgical clips, cardiac monitors, metallic fragments, or pacemakers.\n- Compared to MRI, the total testing time is less with CT.\n- For claustrophobic people, CT may be more comfortable.\nWhat is an MRI?\nA strong magnet and pulsed radio waves are used in MRI (Radio Frequency or RF). Any axis of the body is used to reconstruct the acquired data into a two-dimensional image.\nSince bone is devoid of water, it does not produce any picture data. As a result, the photos have a black area. Therefore, the imaging of soft tissue is best suited for MRI scanners.\nDoctors can use MRI technology to see soft tissues like muscles, ligaments, tendons, bones, and organs. The spinal cord and nerves can also be seen.\nMRI is also used:\n- To identify sports injuries such as strained or sprained muscles, torn anterior cruciate ligaments, and ruptured Achilles tendons\n- Discover malignant and benign tumors\n- Recognize issues with the circulatory, digestive, circulatory, and respiratory systems.\n- Check for any abnormalities in the brain, such as aneurysms and tumors\n- Identify bone and cartilage diseases\n- See how much your joints are inflamed.\nThe noninvasive diagnostic imaging process known as computed tomography (CT scan or CAT scan) creates horizontal or axial images of the body (commonly referred to as slices) using a mix of X-rays and computer technology. The CT scan is a noninvasive, pain-free, and relatively safe process that doesn’t require recovery time.\nThe noninvasive medical imaging procedure known as magnetic resonance imaging, or MRI, creates precise images of every internal bodily structure, including the organs, bones, muscles, and blood arteries. MRI scanners create images of the body by using a powerful magnet and radio waves.\nSoft tissue, bone, and blood vessel details can all be seen in great detail in CT scans. MRI scans are better and more accurate at visualizing the ligaments, soft tissue, or organs.\nSoft tissue injury, ligament damage, and herniated discs may be simpler to detect as issues after an MRI scan. Medical professionals can use a CT scan to obtain images of organs, fractured heads, or physical components.\nIn a CT scan and an MRI scan, the patient is on a bed that gently rotates across the gantry as a narrow beam of x-rays is shot into the body by the x-ray tube in case of a CT scan. For MRI, the magnetic field momentarily realigns your body’s water molecules. Then, radio waves induce these aligned atoms to emit weak signals to make cross-sectional MRI pictures.\nYou must lie still and motionless throughout the scan. If you move, the MRI or CT scan images might need to be clarified. If you are claustrophobic (afraid of enclosed places), have trouble staying still, or suffer chronic pain, your primary care doctor might prescribe you a light sedative.\nAre CT and MRI contrast the same?\nMRI and CT use the same contrast materials, except iodine, which is only present in CT.\nWhat are the differences between MRI and FMRI?\nMRI and fMRI scan both employ the same fundamental atomic physics concepts. However, MRI scans show anatomical image structure, and fMRI scans show metabolic function. As a result, MRI scan results resemble three-dimensional photographs of anatomical structures.\nWhat can an MRI show that a CT scan cannot?\nA CT scan cannot detect some disorders, which is where MRI excels. Some tumors, including liver cancers, uterine cancers, and prostate cancers, are virtually undetectable or extremely difficult to find on a CT scan. An MRI is also better at showing bone and brain metastases.\nWhich is safer, MRI or CT scan?\nA tiny dosage of ionizing radiation is used in CT scans to create the images. However, an MRI scan doesn’t operate in this manner. Instead of ionizing radiation, it creates images using strong magnets and radio waves. So, in contrast to a CT scan or x-ray, you are not exposed to radiation when you undergo an MRI scan although it does take longer.\nWhy would a doctor order a CT scan instead of an MRI?\nThe doctor might order a CT scan instead of an MRI if a patient cannot have an MRI. Due to the strong magnet inside the machine, those with metal implants, pacemakers, or other implanted devices shouldn’t undergo an MRI.\nIs MRI more expensive than CT?\nMRIs are substantially more expensive than CT scans and other imaging procedures due to the higher equipment expenses. Additionally, reading these intricate images by radiologists requires additional time.\nWhy do they inject dye for an MRI?\nDye is injected because MRI scans sometimes require it as a contrast agent. Certain tissues and blood arteries become more distinct and detailed as a result.\nCan you do CT and MRI at the same time?\nCT and MRI data are spatially and temporally registered in a perfect CT-MRI scanner. Therefore, CT and MRI scans that were collected separately could be combined to mimic a simultaneous acquisition.', 'Safety of CT scan\nIn light of a recent paper by Mark S Pearce et al. in The Lancet (Radiation exposure from CT scans in childhood and subsequent risk of leukaemia and brain tumours: a retrospective cohort study) early online publication, 7 June 2012, doi:10.1016/S0140-6736(12)60815-0, the following information is provided:\n- CT is one of the most valuable medical imaging techniques when used justifiably.\n- A number of professional organizations have provided appropriateness criteria or referral guidelines to achieve appropriate use. This website provides a link to some of these guidelines.\n- X rays used in CT, while having tremendous benefits, can have side effects such as a potentially increased risk of cancer in later years.\n- The risks of cancer from radiation dose imparted by a single CT scan is debated and international consensus is currently not available.\n- The risk figures for cancer available in literature are projected, that is, “estimated cancer risks” based on risk coefficients derived from data from survivors of Hiroshima and Nagasaki primarily, with some other study groups also supporting the data.\n- Despite a common view among international organizations that risk coefficients are also applicable at the level of radiation doses encountered in CT scanning, some groups have expressed the view that the risks are hypothetical as they are extrapolated from higher exposure down to lower exposures. Also, there have been objections that the risks from X rays may not be the same as with gamma radiation.\n- With current knowledge, there is no controversy on carcinogenic effects for organ doses in excess of 100 mGy, which can accrue in five to 15 CT scans (such a wide range is needed as it depends upon techniques utilized and organs scanned).\n- The controversy remains (despite common view among most international organizations) about cancer risk from a single or a few CT scans and this is where the new research published in the Lancet has importance.\n- The new research reinforces the belief of major international organizations about cancer risks from a few CT scans. The risk is very low but appears to be real, not hypothetical.\n- Image Gently, an alliance for radiation safety in paediatric imaging, has posted a message to parents on its website.\n- The IAEA recommends tracking of radiological examinations a patient undergoes, in particular a child, to enable easy reference to availability of desired information from previous scans. This also helps to compare radiation doses with previous examinations and help in the detection of a scope for optimization. Recently, a Joint Position Statement on the IAEA Patient Radiation Exposure Tracking has been issued jointly with ESR, FDA, IAEA, IOMP, ISRRT, WHO and also CRCPD.\n- There is information available on this website for patients and, in particular, for safety in computed tomography (CT).\nCT scans are an important diagnostic tool when used appropriately (American Association of Physicists in Medicine), direct link to AAPM web page\nPaediatric CT Scans Save Lives When Used Appropriately (American College of Radiology)\nImproving Radiation Protection of Children Worldwide (IAEA)\nTraining material for free download in different areas\nCopyright © 2013 International Atomic Energy Agency, Vienna International Centre, PO Box 100, 1400 Vienna, Austria']	['<urn:uuid:6d8ca417-2ebb-431c-9184-233b2a5424fd>', '<urn:uuid:d69b9ded-e78b-4db4-9ed7-e2fe449f0545>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T22:13:20.996306	6	120	1503
29	Who should determine the roof tile attachment specifications?	The attachment of roof tiles should be incorporated into the design of the building by the project designer of record, not just left to the installer of a roof system. The calculation of aerodynamic uplift moment should only be performed by a qualified designer.	['This week, I’d like to introduce Sam Hensen as the first guest blogger for the Structural Engineering Blog. Sam is the engineering manager for Simpson Strong-Tie in the Southeastern United States. Sam will be posting occasionally on topics that are relevant to our work. We will continue to post on a regular weekly schedule (usually Wednesdays) to the blog. In the future, we hope to expand the voices on the blog to include more Simpson Strong-Tie engineers, along with other industry colleagues and associates.\nI tend to think of designers as dealing with either wind or seismic design, yet the Southeast region contains everything that Mother Nature can throw at a building. This includes high seismic areas along the New Madrid and Charleston faults, hurricanes along the Gulf and Eastern coast, and tornado prone areas throughout the South and Midwest. Sam participated in the investigation and was a co-author of the Damage Study and Future Direction for Structural Design Following the Tuscaloosa Tornado of 2011, which gives him some very recent experience with tornado damage. This week, Sam will be discussing a topic not often thought about by structural engineers – the importance of proper roof tile attachments. Here is Sam’s post:\nAccording to recent studies by the Insurance Institute for Business Home and Safety (IBHS), roof coverings are a major problem area in wind-related events and account for 95% plus of home claims after the event.\nPreventing roof tiles from becoming wind-borne debris in high wind regions is essential for several reasons, and may also have an effect on insurance premiums. In this post, I’d like to discuss two reasons that roof tiles can pose a significant threat to life safety:\n- Many designers rely on the weight of roof tiles to reduce the overall wind uplift effects on the structure. If the roof tiles do not remain in place during the wind event, then an under-designed wind uplift restraint system can exist.\n- Roof tiles can essentially become missiles in a wind event, damaging nearby property and creating a significant and direct risk to public safety.\nA Complete Load Path\nSection 1604.4 in the International Building Code (IBC) requires a complete load path from point of origin to the load-resisting elements. For wind conditions, where does the wind uplift load path begin?\nThe roof tiles to roof decking attachment!\nDesigners often rely on the benefit of the weight of the roof tile to reduce uplift loads on the structure, but rarely (if ever) design or specify these connections. The code allows a portion of the roof dead load to be used to reduce uplift loads on the structure (60% of the typical 25 psf of dead load for tile roofs), so keeping it in place during a wind event is critical. Consequently, the attachment of the tile should also be included in the design by the project designer of record, or it should not be used to reduce uplift loads. This consideration is clarified in section 1605.3.2, “…only two-thirds of the dead load likely to be in place during a wind event shall be used…”.\nThe design and construction of roof systems is addressed in section 1609.5 of the 2012 IBC. Section 1609.5.3 provides a method for determining wind loads on rigid tiles, as well as the minimum detailing and design requirements. In this section, equation 16-33 provides a means for calculating the minimum aerodynamic uplift moment, Ma (ft-lbf), acting to raise the tail of the tile.\nMa = qhCLbLLa[1.0 – GCp]\nThe use of this challenging equation should only be performed by a qualified designer and it should not just be left to the installer of a roof system to ensure the roof tiles are properly attached to the roof deck. Fortunately, the Florida Roofing, Sheet Metal and Air Conditioning Contractors Association (FRSA) teamed up with the Tile Roofing Institute to create the Concrete and Clay Roof Tile Installation Manual. The appendix of this document includes simple tables that assist the designer in determining the proper attachment of clay or concrete roof tiles to resist the aerodynamic uplift moment.\nImproper attachment of roof tile can also create powerful missiles in wind events. These missiles can pose a significant risk to life safety as they become wind-borne, either directly to the public, or through damage to nearby occupied structures, thereby reducing the ability of that structure to remain in place during the event. Both of these reasons make it essential for proper design and detailing of the connection of roof tiles to be incorporated into the design of the building by the project designer of record.\nWhat are your thoughts? Let us know in the comments.\nWhat are your thoughts? Visit the blog and leave a comment!']	['<urn:uuid:5501a9a7-059b-4e79-a5bf-fc1ea1b3461d>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T22:13:20.996306	8	44	789
30	artistic style comparison ranjit fernando traditional religious art	Ranjit Fernando's artistic style focused on landscapes painted with symmetry and vibrant colors aimed at creating emotional tranquility and audience gratification. This contrasts with traditional religious art forms discussed in liturgical traditions, which focused on specific theological purposes - such as baroque art's use of shadow and light to represent good versus evil, or iconographic art's representation of divine light and elimination of spatial illusion. While Fernando's art emphasized calmness for the viewer's soul, traditional religious art forms were specifically structured to communicate different aspects of humanity's relationship with God.	['Ranjit Fernando was an eminent figure related to the ‘43 Group, a collective of artists from 1940s Sri Lanka, who contributed to the visual art landscape of the country through the interrogation of politics, society and culture rooted in a larger commitment to promoting a modernism informed by their specific sociocultural location.\nFernando’s artistic peer Neville Weeraratne refers to him as a connoisseur who was always drawn to “assiduous study of such topics as the philosophy of art and religion, seemingly esoteric subjects”. The trajectory along which his relationship with art progressed throughout the years manifests Weeraratne’s claim with much dynamism. Although handicapped due to an early childhood illness, Fernando did not let this affect his passion for the arts. His initial exhibitions held in the UK during the early 1950s were mainly landscapes painted in symmetry characterized by vibrant colour bringing about calmness for the soul of the beholder. He was a believer in the ideology that art should result in emotional tranquility and had an innate aim for audience gratification embedded in his work.\nFernando, although not a core member of the ‘43 Group, often exhibited his art with them, and played a key role in facilitating international exhibitions of their work. He successfully organised exhibitions in Venice (at the Venice Biennale), London, Cambridge, Paris and the Sao Paolo Biennale in Brazil, which drew attention from renowned art critics of the time such as John Berger, Maurice Collis and Myfanwy Piper. One such exhibition was a solo exhibition showcasing the works of Justin Deraniyagala at the Beaux Arts Gallery in 1954. Another exhibition of the ‘43 Group held at the Petit Palais culminated in the acquiring of two paintings by Ivan Peries and Richard Gabriel for the permanent collection of the gallery. Fernando thus provided critical side support for the core members of the ‘43 Group to receive recognition globally for their locally acclaimed work.\nFernando’s interests in the 1980s and 1990s were drawn to religious unity in Sri Lanka, and the role played by Ananda Coomaraswamy in relation to the subject. Keeping the growing ethnic conflict in the country in mind, Fernando edited two publications on the subject in the late 1990s, namely Traditionalism: Religion in the Light of The Perennial Philosophy and The Unanimous Tradition: Essays on the Essential Unity of All Religions. These volumes of collected essays entered the discourse of religion and spirituality, emphasizing a necessary unity, a “unanimous tradition” between all religions and types of traditional societies with unique characteristics that define them. Through his writing and continued philosophical engagements with art, Fernando contributed to the growth of the traditionalist philosophy and its application to Sri Lanka as introduced by Coomaraswamy and Rene Guenon, highlighting the significance of religion, tradition and convention in relation to reconciliation and peace-building strategies\n|Name of Exhibition||Year||Place|\n|-||1952||Heffer Gallery, Cambridge, UK|\n|-||1952||Robin Nance’s Gallery, London, UK|\n|-||1951||Montage Gallery, London, UK|\n|Name of Book||Year of Publication||Author||Printer|\n|Traditionalism: Religion in the Light of The Perennial Philosophy (Ed.)||2000||Sri Lanka Institute of Traditional Studies|\n|The Unanimous Tradition: Essays on the Essential Unity of All Religions (Ed.)||1991||Sri Lanka Institute of Traditional Studies|\nOther Publicaitons the Artist has been mentioned in', 'Abstract: When considering the suitability of art for its purpose, style – ‘form’ – is as important as the content. It is through the characteristic style the invisible truths relating to what is painted are conveyed. This is also how we distinguish one liturgical artistic tradition from another, for example, the iconographic from the baroque. This article describes in broad terms what characterises each of the Church’s liturgical artistic traditions. It is my intention to write a series of articles that describe in much greater detail how each tradition does this starting with sacred geometry, and how much modern art deviates from these norms.\nEvery time I paint I have to ask myself two questions: what will I paint? And how will I paint it? The answers to these questions govern the content and the form, respectively, of my finished painting and in turn their conformity to what is good, true and beautiful.\nMost discussions of Christian art that I read focus on the significance of the content. This is appropriate, of course, but in focusing on content, one must not forget that form is a vital component. I must seek to ensure that the form conforms, so to speak, to these transcendentals. This is not only the responsibility of the artist. Patrons must be aware of this too. When as I am selecting art done by other artists, no matter what the purpose, I must have some criteria to guide me, so that what I pick is good and appropriate to the purpose.\nMy approach is first to disregard personal taste. Rather than ask, do I like it? I ask first, is this true? If the content and form in combination seem to be conveying a message that is contravention of truth, then I will reject it. I must acknowledge the possibility that my judgment as to what is beautiful is flawed, so regardless of how attractive I might find the painting, I try to judge first based upon truth. I know that beauty and truth cannot be in opposition, so if reason tells me that something does not conform to truth, then I will disregard my sense of its beauty, which is more intuitive and less easy to rationalize, in this case. However, personal taste should not be ignored altogether. Once I have done my best to make a judgment in regard to truth, then I ask myself the question, do I like it? (or if it is to be seen others, will it be liked?)\nIt is easier to see how one can relate truth to content. If what is shown is contrary to the message of the gospels, for example, then it is false. But what is truth in relation to form? This is a more difficult question to answer.\nPerhaps the most important factor that governs my ability to do this is an understanding of the nature of the human person and his relationship with God and the rest of creation.\nAll of creation is made by God so that we might know Him through it. Therefore an image of any aspect of creation must do this also – so a landscape, for example, must portray the beauty of the scenery depicted in such a way that the image draws the person who sees it to God, its Creator. Mankind has a privileged place in the hierarchy of Creation and so when painting the human figure, there is a special responsibility of the artist to reflect the truth and beauty of the human person. Because the human figure consists of a profound unity of body and soul, the artist must reveal both. So the figure must have a recognizable human body, but in addition must be portrayed as a thinking feeling person. When we meet someone in the flesh, we know the spiritual aspects of a person most obviously through observation of their actions and words over a period of time. The artist who paints (or sculpts) is forced to create a snapshot, frozen in time. Nevertheless he must somehow reveal the spiritual through the material. To this end, the good Christian artist will introduce controlled deviations from a strict photographic representation. This partial abstraction when done well reveals more, not less, of the reality of what is portrayed.\nTo abstract means literally to draw out and so in this context the artist is drawing out the truth. It is this process of partial abstraction that gives an artist or artists work its characteristic style. When that style reveals truth the product is a beautiful idealization. When it hides truth, as much modern art does, the result is an ugly distortion. The work of the Christian artist, in the context of figurative art, must always contain this balance of naturalism and idealization. To do so in accordance with the teachings of the Church requires the artist to be theologian, philosopher, liturgist and craftsman all rolled into one. Fortunately, the Church offers guidance here in the form of tradition.\nThe identifiable traditions of authentically Catholic art are distinguishable from one another stylistically because they seek to reveal different aspects of humanity in relation to God and creation. Those who have read John Paul II’s Theology of the Body will be aware that there are different stages of human existence. First, there is man before the Fall, called Original Man, when Adam and Eve were ‘naked without shame’ and enjoyed innocence that comes from dependence upon God. Second there is Historical Man, mankind after the Fall, experiencing the fear and resentment that results from a dislocation in the relationships with each other and with God. Though not as good as man ought to be, Historical man is still good and has the potential for sanctity. As historical men and women, we are all too familiar with this aspect of the human condition. Third there is Eschatological Man: in this stage we fulfill our human purpose, partaking of the divine nature in heaven in communion with the Trinity in a perfect exchange of love and in perfect and perpetual bliss.\nThe iconographic tradition reveals Eschatological Man (right is a modern example by a Russian icon painter based in Belgium). Drawing on biblical episodes such as the Transfiguration, the style shows for example the divine light shining from the saints and eliminates the illusion of space to show that the heavenly dimension is outside time and space.\nThe baroque reveals Historical Man. The painting of the Virgin at the top, by Sir Anthony van Dyck, is an example of the baroque style. In contrast with the iconographic style, the baroque sets out to create an illusion of space using devices such as perspective, and shows deep cast shadow from external light sources. Shadow represents presence of evil and suffering, which is contrasted with brightly lit areas representing the Light that overcomes the darkness.\nThe gothic is the third Catholic figurative tradition that is cited in his book, The Spirit of the Liturgy, by Pope Benedict XVI as an authentic Catholic liturgical tradition. This appears to oscillate between the styles of Eschatological and Historical Man. Like the spires of the gothic churches, they reach up to heaven, but they are firmly planted on earth. It might be argued that this reflects the fact that although we can never fully make that transformation to Eschatological Man in this world, there is nevertheless a continuum between the two states along which we can make progress through the transforming process of participation in the sacramental life of the Church. The late-gothic artist Fra Angelico, for example, used elements of both the visual vocabulary of the increased naturalism that was developing around him, such as perspective and shadow and the iconographic prototype of light and ‘flatness’. His selection depended upon the theological point he wanted to communicate.\nI am not aware of a tradition that has a distinctive form that has emerged from a focus on the theology of Original Man, although Fra Angelico’s late gothic/early Renaissance form could be the appropriate form. Certainly, if I were set the task, this would be my starting point. The example shown left is Fra Anglico’s Resurrection. This is just early speculation based upon passages written by the Church Fathers that I have read, which describe the appearance of Original Man as a slightly more naturalistic Eschatological Man, clothed in splendor.\nWhen I paint I aim to let tradition guide me in my work as far as possible. If I want to paint the human figure, the first question I should ask myself is, ‘What form of man?’ If I decide that I will to portray Eschatological Man, then rather than trying to develop my own individual style, the sensible thing is to go to the iconographic tradition and let the principles of that tradition guide my hand. This will not lead to pastiche, for every truly living tradition is defined by principles, rather than strict rules, which are reapplied in every age. So, for example, someone who knows icons can recognize within that tradition geographical variations and place it to with, perhaps, 50 years. Only if I am seeking to communicate something previously uncommunicated should I look to create something original; and then I should do as artists did in the past and seek out the guidance of the theologians, philosophers and liturgists of the Church.\nDoes this rule out any new forms? Hasn’t it all been done already? This will be clear as time progresses. But it is conceivable to me that discussion on how to create the image of Original Man could create something that previously unimagined in art. Certainly in his Letter to Artists, John Paul II called on artists to find new ways to represent human sexuality as gift. Maybe this will be the art of the next age? But if it is to be so, it will be a dialogue between the artists and the theologians, philosophers and liturgists of the Church that will shape the new form, just as similar dialogues shaped the established ones; and what will drive the development of new forms will be the need of the Church to articulate something new, or in a new way – not the whim of an artist who wants to stand out from the crowd.\nThe principles of being open to new forms, with the impetus for change coming from the Church rather than the artist; and the need for a balance of naturalism and idealism were expressed with great clarity Pope Pius XII in Mediator Dei, writing in 1947: ‘Recent works of art which lend themselves to the materials of modern composition, should not be universally despised and rejected through prejudice. Modern art should be given free scope in the due and reverent service of the church and the sacred rites, provided that they preserve the correct balance between styles tending neither to extreme realism or to excessive “symbolism” and that the needs of the Christian community are taken into consideration rather than the particular taste or talent of the individual artist.’\nI have only discussed liturgical form so far, which would placed inside the church, but what I say applies just as much to art that is not intended to be seen in a church, or even to devotional art. This is because all good art should be rooted in the liturgy. It is the cult that is the foundation of Catholic culture. So even if the content is not obviously religious, such as with a landscape, the form can be liturgical and so the art then becomes a profane signpost, as it were, to the sacred liturgy. The baroque of the 17th century, for example, began is a liturgical art form intended for church, but it came to dominate the whole culture of the period so that all art, architecture and music was in conformity with it. Pius XII put it as follows: ‘The fine arts are really in conformity with religion when “as noblest handmaids they are at the service of divine worship”.’\nAnd non-figurative art? Much abstract art produced since the turn of the 20th century is based upon a secular understanding of the human person that is in opposition to the Catholic teaching. So, for example, some abstract expressionists sought to portray human emotion without any reference to the body of the human person. This is, in effect, an abstraction that goes beyond the bounds of truth. It seeks to remove the soul from the body altogether reflecting the error of dualism. For the Christian, emotion, though an aspect of the soul is revealed through the body. So we cannot portray human emotion fully in art without the portrayal of a body.\nThat is not to say that there is no legitimate Catholic form of non-figurative, ‘abstract’ art. The traditional quadrivium, which are the ‘four ways’ that were the higher part of the seven liberal arts, sought to represent the divine order mathematically. The fact that mathematics can be conceived in the abstract and represented visually, allows for a patterned, geometric art form that is conformity with truth.\nThe Cosmati pavements of the 13th century are examples of this art form. Shown below is the church San Clemente in Rome.\nThe mathematical description of the divine order also allows for the structuring and organization of time and space, and the design of just about anything, that is the culture in the fullest sense of the word – the family, society, business, education – to be ordered liturgically by conformity to the cosmic order that points to the mind of the Creator. Sacred geometry and its applications will be discussed first in the next series of articles.\n It should be noted that this does not rule out the possibility of the portrayal of the imagination provided that it directs the imagination of the viewer to something that is true, as in good children’s’ stories (and their illustrations) for example.\n This includes all the local variations that are consistent with the iconographic prototype, for example, the Romanesque in the West.\n Mediator Dei, 195\n Ibid, 196, quoting Pius XI, Constitution Divini cultus.']	['<urn:uuid:08f604aa-fdfe-4950-9366-96093d890d2b>', '<urn:uuid:a034e245-3465-4a21-9f09-bf523af6d764>']	open-ended	direct	long-search-query	distant-from-document	comparison	novice	2025-05-12T22:13:20.996306	8	90	2882
31	For married couples filing bankruptcy together, how does the selection of exemption laws work - can each spouse choose different exemption systems?	Joint debtors (husband and wife) must choose the same exemptions if they have a choice. In states that allow choosing between federal and state exemptions, one debtor cannot choose federal exemptions while the other chooses state exemptions. If joint debtors cannot agree on the exemption schedule, they will default to the federal exemption schedule. Additionally, a debtor cannot mix federal and state exemptions.	['When you file bankruptcy, your property and debts are subject to the jurisdiction of the bankruptcy court. These assets and debts are referred to as the “debtor’s estate.”\nThe estate includes the debtor’s (your) interest in all property, including the debtor’s interest in community property that is under the debtor’s management and control. Also included is property the debtor acquired within 180 days after filing, such as an inheritance, marital settlement property, or proceeds from a life insurance policy.\nThe bankruptcy court also has jurisdiction over any property the estate acquires after filing bankruptcy.\nThere are two categories of assets:\nThere are two categories of debts:\nThe assets of an individual debtor that are under the jurisdiction of the bankruptcy court include:\nThe court recognizes that the debtor (you) needs certain items to live and start a new financial life. Therefore, the court allows the debtor to keep certain assets, which are referred to as exempt assets. Both federal and state laws provide a list of assets determined to be exempt.\nThe following types of property are excluded from debtor’s estates in bankruptcy:\nBoth Federal and State law lists assets that are exempt from the jurisdiction of the bankruptcy court and may be retained by an individual debtor in Chapters 7 and 13 cases.\nIn Chapter 7, all property must be surrendered to the trustee unless allowed to be retained by a bankruptcy exemption. In Chapter 13, as a condition of plan confirmation, creditors must receive value at least equal to or greater than would be distributed if the case had been liquidated in a Chapter 7 proceeding. Plan confirmation depends on individual bankruptcy exemptions to pass this test.\nUsing federal or state exemptions depends on the state in which the debtor lived for the two-year period before filing.\nDebtors who are husband and wife (joint debtors) may double the exemptions listed. However, joint debtors must choose the same exemptions if they have a choice. For example, in states that allow a choice between federal and state exemptions, one debtor cannot choose the federal exemptions while the other debtor chooses the state exemptions. If the joint debtors cannot agree on the exemption schedule to use, they will be deemed to choose the federal exemption schedule. Additionally, a debtor may not choose some federal exemptions and some state exemptions.\nSome retirement funds are exempt regardless of whether state or federal exemptions are elected.\nAll assets must be included on the debtor’s schedule of assets and indicate their exempt status when appropriate. If an exempt asset erroneously is not listed as such, the asset may be determined to be nonexempt and thereby subject to the trustee’s sale or disposition. If the debtor fails to claim the exemptions within the time specified, a dependent of the debtor may file the exemptions within 30 days thereafter.\nFederal law allows exemption for the debtor’s interest in:\nEvery state has its own set of bankruptcy exemptions. Items covered and the values of items vary greatly from state to state. In some states, the debtor has a choice of state or federal exemptions, and in others the debtor must use the state exemptions.\nIf a husband and wife (or two or more persons) are filing jointly, they must choose the same exemption schedule and may not alternate between state and federal schedules.\nThe bankruptcy process is full of pitfalls and complexities for the unprepared. By hiring the right attorney, you can help yourself get through a difficult process as easily as possible.']	['<urn:uuid:e162c643-10f4-40a0-97d4-485f71fd745b>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T22:13:20.996306	22	63	585
32	my house feels cold main heat pump components and possible reasons heat not working	The main components of a heat pump include: the air handler that moves heated air through ductwork, a compressor that pumps refrigerant, condenser and evaporator coils for heat transfer, and a thermostat that controls temperature settings. When a heat pump isn't heating properly, there are several possible reasons: thermostat malfunction, frozen outdoor unit (due to snow accumulation or refrigerant leaks), failed reversing valve that prevents switching between heating and cooling modes, clogged air ducts restricting warm air flow, or simply extremely cold outdoor temperatures that make it difficult for the heat pump to extract sufficient heat from outside air. In extremely cold conditions, a backup heating system like propane may be needed.	['Understanding the different components that make up HVAC systems can make your life as a facility manager or property manager much easier.\nHaving the ability to quickly and correctly identify the component of your HVAC equipment that is causing a problem can positively affect the speed and efficiency in which your HVAC provider can get to work solving your problem.\nThere are two obvious instances in which knowing your HVAC system components will benefit you:\nFirst, sending the correct person or team of people out to your location.\nWhen your HVAC provider receives your service request the more information they have, the better. By providing your HVAC contractor with a detailed description of what the problem is and where you see the issue, they can better decide who to send out to your location based on the different skill sets of their technical team members and staff.\nSecond, arriving to your location with the proper tools and equipment to solve your problem.\nBeing prepared with the tools or in some situations replacement system components that are necessary to fix your HVAC equipment is a guaranteed way to make sure that your HVAC provider is using both your time and their time wisely.\nAn air handler is the indoor part of an air conditioner or heat pump that moves cooled or heated air throughout the ductwork.\nAn air handler is usually a furnace or a blower coil.\nA compressor is the outdoor part of the air conditioner or heat pump that compresses and pumps refrigerant to meet cooling requirements.\nThe condenser coil is the outfoot portion of an air conditioner or heat pump that either releases or collects heat, depending on the time of year.\nThe evaporator coil is the part of the air conditioner or heat pump that is located inside the air handler.\nThe primary function of the evaporator coil is to absorb heat from the air.\nTXV stands for Thermo Expansion Device. The TXV meters the refrigerant from high pressure to low pressure based on the suction pressure and temperature, controlling the amount of refrigerant that goes into the evaporator coil.\nRefrigerant lines are two copper lines that connect the outdoor AC or heat pump to the indoor evaporator coil.\nThe discharge line is the line between the compressor and the condensing coil. The discharge line carries the vapor.\nThe liquid line is usually the smaller line coming off the condenser. The liquid line carries refrigerant to the metering device and flows towards the air handler.\nThe suction line is usually the larger of the two lines going into the condenser. The suction line carries vapor from the evaporator coil to the compressor.\nA contactor is a relay that is normally open, and when voltage is applied to the coil it closes, completing the circuit.\nThe contactor is normally found on the outside condensing units. The contactor supplies the power to the compressor, fan motor and electric heating systems.\nThe capacitor is a device that stores an electric charge and helps give a boost of power to motors and compressors.\nIn the HVAC field there are generally two types of capacitors, run capacitors and start capacitors.\nThe condensate drain is usually made of PVC, CPVC or copper.\nThe condensate drain leads the condensation out or into another main drain.\nThe drain pan is attached to the evaporator coil. This pan collects the condensation and then drains out through the condensate drain line.\nThe blower assembly is located inside of the air handler/furnace.\nThe blower assembly moves the blower wheel to move a volume of air in HVAC applications.\nThe blower wheel is attached to the blower motor.\nIf the blower wheel is dirty or has debris build up it can potentially restrict air flow or rotate out of balance.\nCondenser Fan Motor\nThe condenser fan motor that has blades attached to move a volume of air.\nIn HVAC terms, the fan motor refers to the condensing unit while the blower motor refers to the air handlers.\nThe heat exchanger transfers heat from one fluid to another without the fluids coming into direct contact with one another.\nThe heat pump is an HVAC unit that heats or cools by moving heat.\nIn the Winter, a heat pump draws heat from outdoor air and circulates it through the air ducts.\nIn the Summer, it removes heat from the space and releases it outdoors.\nThe furnace is the place in the HVAC system where the heat transfer takes place when in heating mode.\nFurnaces are typically in oil or gas systems.\nA fan coil unit is a unit that includes a cooling and/or heating coil and a fan to move air through the ductwork to a specific room.\nAir Filters and accessories used to introduce outside ventilation air may also be included in the fan coil.\nThe thermostat is usually found on an inside wall. This device controls heating and cooling equipment, allowing simple adjustments of temperature and other comfort controls factors such as humidity and air flow.\nThere are two types of thermostats: programmable and non-programable.\nA programmable thermostat is a thermostat with the ability to record different temperature and time settings for your heating and cooling equipment.\nA non-programable thermostat is the simplest type of thermostat to operate and set up. This type of thermostat cannot be programmed to turn on and off at set days and times.\nDuctwork is described as a square channel or round tube by which air is distributed from the air handler to the room.\nDuctwork is used with Rooftop Units, Furnaces, Fan Coils and VAV Boxes.\nA damper is a movable plate, located in the ductwork, that regulates airflow.\nDampers are used to direct air to the areas that need it most. They are typically used in zoning application. You can have manual dampers for air balancing.', 'Help! My Heat Pump Is Not Heating Enough\nOf the many different methods for heating your home, perhaps the most versatile, is a heat pump. You’ll be able to use a heat pump throughout the year: During the summer, it pumps cold air through the home and acts as your home’s air conditioning. During the winter, your heat pump draws heat in from the outside and leaves your home nice and toasty.\nThis versatility comes from the heat pump’s method of operation. It makes use of a refrigerant to either draw heat from the outside (to heat the home) or to disperse the heat that’s inside (thus cooling the home.) This method is particularly energy efficient, saving you a fortune on your energy bill over the years.\nOf course, as with any other home heating system, there will occasionally be problems. This can be particularly frustrating if it’s a cold winter and you can’t seem to get your home to the right temperature. Is your heat pump not heating enough? How do you diagnose and solve the problem? The answer, of course, depends on what exactly the problem is. Below are a few of the most common problems found with air-to-air heat pumps and what you can do about them.\nPotential Problem #1: The Thermostat\nThe first potential problem may be the fact that there’s actually not a problem with the heat pump itself. All of the equipment may be in perfect working order, but you still can’t seem to get your home to the temperature you want. This can mean there’s an issue with the thermostat, which is the device responsible for programming your heat pump to your preferred temperature.\nThere may be issues with the software. If this is the case, you’ll need to get an HVAC professional to take a look. If the temperature doesn’t feel like it’s quite matching what the thermostat says, you can try a temporary fix by simply turning the thermostat up a few extra degrees.\nPotential Problem #2: The Pump Is Frozen\nAir-to-air heat pumps work by drawing heat from the outside and to the inside. That air is then distributed throughout your home. If your home is still chilly—regardless of how high you crank up your thermostat—then the heat pump may simply be struggling to draw heat from the outside. If that’s the case, go out and take a look at your outdoor equipment. It’s possible the pump itself has frozen over. With ice all over your heat pump, no wonder it’s not able to heat up.\nThere are actually several reasons a heat pump might freeze over. They usually won’t do so on their own. Some of them are easily fixed, while others will require immediate help from a professional.\nThe easiest problem to fix is a pile of snow that has accumulated all around your heat pump. Well, we say “easy,” but we really mean simple; shoveling all that snow may not be anyone’s idea of easy!\nYou also might notice your heat pump is leaking refrigerant. Because it’s the refrigerant that absorbs the heat, low levels of it will cause the pump to struggle to do its job. For safety purposes, handling refrigerant is best left to professionals, so this will require a technician to fix. A company like Entek can do the job for you.\nPotential Problem #3: Your Reversing Valve Has Failed\nOne of the reasons many homeowners are choosing to use air-to-air heat pumps is because they’re so versatile. That is, they can function as both a heating system and an air conditioning unit. To be able to do this effectively, however, they have to be able to switch between heating and cooling seamlessly. They do this through the use of a mechanism called the reversing valve. This is the piece of equipment that switches your pump from drawing heat from inside to cool the home to drawing it from outside to heat the home. But if this valve isn’t working as it should, your home may never heat up.\nUnless you’re a professional HVAC technician, you’ll need to get outside to fix the reversing valve in your heat pump. Contact a professional at Entek to begin the repair process.\nPotential Problem #4: Your Air Ducts Are Clogged\nThe heat pump not heating enough isn’t always due to equipment failure. Sometimes, every part of the heat pump is working just fine but your home still isn’t getting as warm as you would like it to. In this case, the problem might be as simple as a clogged air duct. If there’s any sort of blockage anywhere in the ducts, the warm air won’t be able to circulate properly, and your home will remain cold.\nSometimes, a simple problem has a simple solution. A trained HVAC technician can perform a cleaning that meets EPA standards. This means your air won’t only be warmer; it’ll be healthier too.\nIt’s important to regularly replace the filters in your heat pump as well. And even after professional duct cleaning, you still periodically need to have a full duct inspection and tune-up. And consider having your ducts sealed to prevent the leakage of any air.\nPotential Problem #5: It’s Just Too Cold\nWhile air-to-air heat pumps have plenty of advantages over other methods of heating, they’re simply not as powerful as some others. If you live in an area that gets extremely cold, your heat pump may just plain struggle to keep up. After all, the air-to-air heat pump works by drawing heat from outside. If there just isn’t enough of that heat, your pump simply won’t be able to do the job.\nYou can’t control the weather so your only real option in this case is to have a backup heating system installed. While this doesn’t mean that people living in cold climates can’t make use of an air-to-air heat pump, it does mean you’ll need another heat source on hand if it gets well below freezing outside. Propane is a good option: it burns cleanly and gets very, very hot. Contact the HVAC technicians at Entek for further information about a backup heating system.']	['<urn:uuid:3ccb881f-ac47-4987-8776-bf6e7fdff289>', '<urn:uuid:02560a61-b84e-4e65-9d29-a933c8d8648e>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T22:13:20.996306	14	112	2000
33	How do prevention measures for food poisoning compare between CDC and Canadian guidelines?	Both CDC and Canadian guidelines share core prevention strategies but have some distinct approaches. They both emphasize the importance of proper hand washing with soap for at least 20 seconds, separating raw foods from ready-to-eat items to prevent cross-contamination, and proper temperature control. The CDC focuses on a 'Cook, Separate, Chill, and Clean' approach, while Canadian guidelines add specific details about sanitization, including a precise recipe for sanitizing solution (15 mL bleach per 4 liters of water). Both stress the importance of cooking foods to safe temperatures, though Canadian guidelines specifically recommend 74°C (165°F) as a general safe temperature, and both emphasize prompt refrigeration, with Canadian guidelines specifying a 2-hour window for chilling food.	"['by Dr. Lenny Powell\nThe evening of July 31, 2011 was lovely – my wife and I went to an Italian restaurant for dinner. She ordered pasta with marinara sauce, and I had the fettuccini Alfredo. We were sitting outside and the temperature was a balmy 95 degrees. We took our time eating our meals, picking at them while we gabbed and laughed about our day.\nThen, it hit me – a grumbling and gurgling in my stomach about an hour after I finished my meal. “Hmm, that’s odd.” I thought. My stomach didn’t really hurt but it felt kind of annoying. I excused myself to the bathroom and came back a few minutes later, feeling somewhat better. Then it happened again- not cramps, but stomach pangs. I thought to myself, “this isn’t going to be a fun night.” By the time I got home I realized that I had food poisoning!\nFoodborne illnesses strike about 1 in 6 Americans (48 million) every year; 128,000 people are hospitalized, and 3,000 people die from complications. Food poisoning can be caused by bacteria, viruses, toxins, or chemicals. Foods most commonly associated with foodborne illnesses include raw foods such as meat, poultry, milk, shellfish, eggs, and raw fruits and vegetables (1).\nFood poisoning is caused by eating foods that are contaminated with bacteria or other pathogens. The good news is that most of these infections are largely preventable, and there are simple steps that can be taken to minimize exposure to these pathogens.\nSafe food preparation is a key to avoiding foodborne illness. According to the CDC (1), it’s important to “Cook, Separate, Chill, and Clean” foods to prevent food poisoning:\n• Cook meat, eggs, and poultry thoroughly. Use a food thermometer to make sure you meat is cooked properly and use this chart as a guide: http://www.foodsafety.gov/keep/charts/mintemp.html. Make sure your eggs are cooked until the yolk is firm.\n• Separate foods so that they do not cross-contaminate one another. Keep produce separate from meat, poultry, seafood and eggs. Use separate cutting boards and plates and keep these foods separate in the refrigerator as well.\n• Chill promptly fruits, vegetables, milk, eggs, and meats. Bacteria can start to grow as quickly as after one hour unless foods are refrigerated. A cooler in the car with ice may be a temporary fix but the best thing to do is to get your food home, especially on a hot day, and into the refrigerator. It is also important to thaw or marinate foods in the refrigerator and never on the counter or in the sink.\n• Clean your hands, your surfaces, your utensils, and your fruits and vegetables. Clean your hands for at least 20 seconds with soap and warm water, your utensils and small cutting boards with soap and hot water, and surfaces as well as larger cutting boards with a bleach/water mixture. Wash fruits and vegetables before peeling to prevent bacteria from creeping inside and causing contamination.\nThose who are most at risk from foodborne illnesses include pregnant women, the elderly, and those with weakened immune systems. Additionally, bottle-fed infants should be provided with clean and disinfected bottles with each feeding.\nSymptoms of food poisoning include abdominal pain, nausea and vomiting, diarrhea, and fever. You should see your doctor if these symptoms progress to any of the following (1):\n• Diarrhea, particularly if you notice blood, that lasts longer than 3 days\n• Persistent abdominal cramping or pain\n• Nausea and vomiting that is prolonged and such that you cannot keep liquids down\n• Fever (greater than 100.4 ºF, measured orally)\n• Decreased urination\nSymptoms can range depending on the cause of the food poisoning. The goal of treatment is to replace fluids and, if warranted, provide antibiotic therapy. Not all foodborne illnesses require antibiotic therapy, as many diarrheal illnesses are viral in nature and will not be affected by antibiotic therapy. In some cases antibiotics may be more harmful than helpful.\n“Don’t eat the Alfredo, sweetheart,” my mother warned me sternly at an outdoor wedding we attended shortly after this episode, “you don’t want to go through all that again!” I nodded in agreement, realizing that food safety and handling is of paramount importance and that with appropriate measures, illness can effectively be prevented.\nDr. Lenny Powell is a third-year Family Medicine resident at Rowan University School of Osteopathic Medicine. He presently has the pleasure of rotating with Dr. Jennifer Caudle (@drjencaudle) on a Medicine in Media rotation exploring the media communicates health information to the public. Follow Dr. Powell on Twitter @drlennypowell.\nReference 1: www.cdc.gov\nInformation in this article is for informational and educational purposes only and is not intended to provide medical consultation or serve as a substitute for medical advice provided by a physician or qualified medical professional.', ""Foodborne illness causes over 4 million illnesses every year in Canada. The symptoms of foodborne illness include:\n- Stomach cramps\nSymptoms usually go away after 1 to 3 days, but in some cases, foodborne illness can be life-threatening. Children under 5 years, pregnant women, seniors and people with a compromised immune system are more likely to get sick. Foodborne illness during pregnancy can make the baby sick before and after birth.\nGerms such as bacteria and viruses make us sick. Germs can be picked up in many ways, for example, by petting your dog, handling your pet turtle, changing diapers or preparing raw foods like meat and poultry. Even healthy people can carry germs.\nAlways follow these food safety rules to help prevent food-borne illness:\nWash Your Hands\nWash your hands after you use the washroom and before you eat or handle any food. Scrub all parts of your hands with soap for at least 20 seconds and rinsing them under warm water. Dry your hands with a clean cloth or paper towel.\nWash and Sanitize Surfaces\nWash and sanitize surfaces where you prepare and place foods. Foodborne illness is often caused by spreading germs from raw meat to other foods. This is called cross-contamination.\nTo reduce the risk for cross-contamination, use separate cutting boards, plates and utensils for raw meat and cooked or ready-to-eat foods. Foods like potato salad, fresh salads, vegetables or fruit should not be prepared on a cutting board used for raw meat (including fish and poultry). After using cutting boards, plates and utensils for raw meat, wash them in hot soapy water and sanitize them before reusing them for cooked or ready-to-eat foods. Here’s how to make your own no-rinse sanitizing solution:\nHow to make a 200 ppm no rinse sanitizing solution:\n- Mix 15 mL (1 tablespoon) of household bleach into 4 litres (1 gallon) of water; or mix 5 mL (1 teaspoon) of household bleach into 1 litre (4 cups) of water.\n- Allow the sanitizer to contact the surface or utensil for at least 1 minute before wiping off with a clean paper towel or allowing it to air dry.\nUse B.C.’s FOODSAFE Chlorine Dilution Calculator tool to make up the proper sanitizer strength based on the concentration of your bleach product www.foodsafe.ca/dilution-calculator.html.\nCaution: Mixing bleach with products that contain acid or ammonia makes a toxic gas that causes serious breathing problems, choking and potentially death.\nDishcloths are ideal breeding grounds for germs. Use a clean dishcloth every day and wash dishcloths frequently in the washing machine. To quickly sanitize your dishcloth, wet it completely and place in the microwave on high for 1 minute. Do not microwave a dry dishcloth.\nDon't Prepare Foods When You Are Sick\nPreparing foods while sick can spread germs, such as Salmonella, hepatitis A and E. coli, to the food. People who eat the food can become sick. If you have diarrhea, vomiting, or infected cuts or sores, don’t prepare or serve food for others for 48 hours after your symptoms have resolved. If you must prepare food for your child or another family member when you are sick, wash your hands well and often, and consider wearing gloves that are safe for food handling.\nCook Foods to a Safe Internal Temperature—Use a Food Thermometer\nMost germs can be killed by cooking foods to an internal temperature of 74°C (165°F) or hotter before you eat them. For specific cooking temperatures for meat, seafood, poultry and eggs, refer to Health Canada’s Safe Internal Cooking Temperatures Chart www.canada.ca/en/health-canada/services/general-food-safety-tips/safe-internal-cooking-temperatures.html.\nAlways use an instant-read (digital) probe-tip thermometer to check the temperature of the food. Insert the probe tip of the thermometer into the deepest part of the food, without touching the bone if there is one, to make sure the proper internal temperature has been reached. Clean and sanitize your thermometer between temperature readings to avoid cross-contamination.\nCool Foods Within 2 Hours\nImproper cooling is one of the leading causes of foodborne illness. Large items such as roasts or soups are difficult to cool quickly. Separate them into smaller portions no more than 7 centimeters (3 inches) deep, and place them in the refrigerator or freezer to cool. Chill food in the fridge within 2 hours after buying or preparing it. Keep the fridge set at 4°C (40°F).\nThaw Foods in the Refrigerator\nThe safest way to thaw foods is in the refrigerator. Place the food in a drip-proof container and store it on a shelf below uncooked and ready-to-eat foods. To thaw faster, place the food under cold, running water or in the microwave. Foods thawed in running water or in the microwave should be cooked immediately.\nReheat Foods to 74°C (165°F)\nIf reheating leftovers, reheat to at least 74°C (165°F) to kill germs that may have been introduced into the leftovers during cooling and storage. Plan to reheat only what you need for that meal. Leftovers should only be reheated once. Use refrigerated leftovers within 2 to 4 days.\nMicrowave Cooking to a Safe Internal Temperature\nCook or reheat foods in a microwave until all parts of the food reach an internal temperature of 74°C (165°F) or the temperature stated on the cooking instructions. Use a microwave-safe container and cover foods with a microwave-safe lid to trap steam. Stir or rotate food halfway through cooking for even heating. Let the food stand for 2 minutes at the end of heating before checking the temperature with a probe tip food thermometer.\nKeep Foods Out of the Danger Zone\nGerms grow rapidly between 4°C (40°F) and 60°C (140°F). Foods such as meat, poultry, milk products, eggs and prepared meals must be kept hotter than 60°C (140°F) or colder than 4°C (40°F).\nProtect Your Food from Germs\nTransport and refrigerate perishable food as quickly as possible. This helps prevent the growth of germs. Cover or wrap ready-to-eat foods and store them in the fridge on a shelf above uncooked foods. Always read the label for storage instructions.\nIf in Doubt, Throw it Out\nDo not take chances with your food. Remember, contaminated foods may not look or smell bad, so if in doubt, throw it out. Write the date on the food package or container before you put it in the fridge. This will help you remember how long it has been there.\nFor More Information\nFor more information, see the following:\n- HealthLinkBC File #59b Food Safety for Fresh Fruits and Vegetables\n- HealthLinkBC File #59c Food Safety: Instructions on Food Labels\n- HealthLinkBC File #76 Foods to Avoid for People at Higher Risk of Food-borne Illness\n- HealthLinkBC File #85 Hand Washing: Help Stop the Spread of Germs\n- HealthLinkBC Food Safety in Children Older than 1 Year Preventing Foodborne Illness\n- Food Safety at http://www.bccdc.ca/health-info/prevention-public-health/food-safety""]"	['<urn:uuid:50c3a8dc-e0e2-4fac-bbbb-5646d2a7ea8c>', '<urn:uuid:61ddee45-a8aa-496e-8c27-ffc3de366f42>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T22:13:20.996306	13	114	1926
34	What makes calculating the business value of security testing complicated, and how can organizations demonstrate its benefits despite these challenges?	Calculating ROI for security assessments is challenging because most benefits are intangible and difficult to measure, while costs are clearly visible. However, organizations can demonstrate value through: 1) penetration tests' ability to identify high-risk vulnerabilities that automated scans cannot detect, 2) assessment of defenders' ability to detect and respond to attacks, showing the real value of defensive tools like antivirus and firewalls, and 3) meeting compliance requirements for frameworks like PCI-DSS, Sarbanes-Oxley, HIPPA, and 201 CMR 17.00.	['Why Penetration Testing is Critical to Improving Cybersecurity Defense\nPenetration Testing is Critical to Improving Cybersecurity Defenses\nPenetration testing has long been a primary method for organizations to test their defenses against cyberattacks. By hiring an outside company to pose as an attacker, organizations are able to identify weaknesses in their systems to prevent future breaches. During a penetration test, a CEH or certified ethical hacker simulates the techniques a criminal attacker might use during an attempt to gain access to IT systems, potentially including password cracking, malware, and even social engineering.\nPenetration testing or ethical hacking, has been around since at least the 1970s – when the U.S. military and RAND Corporation began using tiger teams to test the ability of computer networks to resist attack. Today’s penetration tests are increasingly a standardized service – a packaged bundle of discovery scans, vulnerability scans, and limited attempts to exploit any discovered vulnerabilities.\nWhile traditional techniques still dominate marketplace offerings, penetration tests in 2018 are increasingly adopting new and improved methods of testing defenses, including new attack techniques, red teaming, capture the flag and bug bounty programs.\nPhases of a Penetration Test\nPenetration tests remain a primary method of simulating a cyber-attack and testing defenses. A penetration test does not stop with simply discovering vulnerabilities as a vulnerability scan would – it takes the next step of actively exploiting vulnerabilities to simulate a real-world attack.\nPenetration tests usually include the following phases:\n- Reconnaissance – testers use internet searches, social engineering, DNS inquiries, and non-intrusive network scanning to map out organization systems and networks, potentially identifying targets for further exploration.\n- Vulnerability Detection – testers use automated vulnerability scans and manual testing to identify open ports and services, vulnerable applications and operating systems, and weak network configurations that may be vulnerable to exploitation.\n- Exploitation Attempt – testers attempt to gain access by exploiting discovered vulnerabilities. This might include exploitation of remote code exploitation vulnerabilities like MS17, which allow attackers to gain administrative access to vulnerable systems.\n- Reporting and Remediation – testers deliver a report on their progress during the test, identifying vulnerabilities discovered, successful breaches and the results of those breaches including sensitive data that was accessed so that the organization may make the proper updates and patches.\nAttackers progress through these phases over a period of days to weeks in order to simulate an attack and produce meaningful discovery data to the target company.\nTop Five Benefits of a Penetration Test\nPenetration test benefits include the following\n- Penetration tests help organizations to identify high-risk vulnerabilities that are often difficult or impossible to detect with an automated network or application vulnerability scan. Penetration testing is one of the only types of tests that allows a realistic method to gauge the actual risk to their systems. Vulnerability scanning can help to find some weaknesses, but an ethical hacker has access to networks and systems that might not be compatible with scanning, and can use a manual, methodical process to verify actual exploitability of weaknesses.\n- Penetrations tests are a way to assess the ability of defenders to successfully detect and respond to attacks. A frequent element of a test is to gauge the ability of defensive tools and personnel to respond to attacks. The real value of tools like antivirus, intrusion detection systems, and firewalls becomes clear when organizations see them stop malware and attackers – or fail to do so. The ability of defenders to analyze alerts and logs to detect the underway attack also provides a gauge of the defensive personnel in place.\n- Tests provide evidence to organization leadership to support increased investment in security program initiatives, personnel, and technology. Many organizations use penetration tests to assess the effectiveness of their security investments and cost effectiveness as an IT security organization. They either assess after an initiative is complete and evaluate the defensive strength of a new system, or to test before a new project as a way to justify the budgetary spend.\n- Tests help organizations prevent potential future incidents – by identifying vulnerabilities before they are exploited by attackers, tests can help organizations prevent potential breaches. As we mentioned earlier, vulnerability scanners are not intended to uncover weaknesses beyond software vulnerabilities. By responding to the penetration test findings, organizations can improve their overall cybersecurity posture. Usually the low-hanging fruit is found early during a penetration test, vulnerabilities that are easy to remediate but allow attackers easy access into the environment.\n- Penetration tests help organizations meet their compliance requirements, including Payment Card Industry Data Security Standard (PCI-DSS), Sarbanes-Oxley, HIPPA, 201 CMR 17.00. Some compliance frameworks – like PCI – explicitly require an annual penetration test as a mandatory activity.\nNew Tools and Techniques\nWhile traditional penetration tests usually involve similar tools that have been in use for many years, tools are regularly updated to target new vulnerabilities and system misconfigurations. Some of today’s most popular tools include the following:\n- Web application scanning tools – Testers frequently use tools like Netsparker, OWASP Zed Attack Proxy, and Burp Suite Scanner to scan web applications and discover vulnerabilities. These tools are able to test for injection attacks and other frequently found web application vulnerabilities that open organizations to attack from the Internet.\n- Discovery scanning – Penetration tests frequently begin their reconnaissance with stealthy port scans using tools like nmap and Advanced Port Scanner to identify ports and protocols that are open and potentially vulnerable to attack.\n- Network Traffic Analysis – Testers often use traffic analysis tools to intercept credentials or exploit potential network weaknesses. Wireshark is one popular tool to intercept and analyze network packets. Some attackers target wireless network vulnerabilities with tools like Aircrack-NG in attempts to gain access. Other tools like Impacket can help to target and exploit vulnerabilities exposed by weak network traffic protocols.\n- Exploitation – testers often leverage Metasploit, Empire, and other tools as ways to craft malicious payloads and exploit the vulnerabilities discovered during earlier phases of the attack.\nOrganizations with more advanced defenses are increasingly turning to red teaming to simulate attacks on their cyber systems. A red teaming exercise is more in-depth and wide ranging than a penetration test. Red teams are tasked to simulate cyber-attacks at a greater depth than a penetration test, without the scope or time-limits of penetration tests. Defensive actors are typically not notified of the red team exercise. Red teams can include reconnaissance and physical breach specialists, phishing experts, and traditional penetration testers skilled in communications and IT.\nCapture the Flag\nSome organizations choose to turn their penetration test into a type of competition – placing a ‘flag’ (usually a sensitive file) in a secure location on their network. The attacking penetration testers are given the task of accessing this file or “capturing the flag” by any means possible. A defending ‘Blue Team’ – usually the incident response staff at the organization – is evaluated during the simulated attack, testing their ability to detect and respond to the attacker. This style of capture the flag penetration test allows companies to test their defensive capability in a more realistic way, placing the focus on protecting sensitive data rather than their entire network.\nBug Bounty programs are another increasingly popular way for organizations to test their cyber defenses. Sites like Bugcrowd and HackerOne offer ways for potential attackers to turn in discovered vulnerabilities in exchange for a reward. Organizations use these programs to offer compensation and recognition to white hat hackers that report bugs, exploits, or vulnerabilities on their systems, allowing the organization to patch them before they are exploited by a malicious attacker. Large organizations like General Motors, Microsoft, and HP are offering $10,000 to $100,000 per verifiable discovery reported. Larger organizations are able to start their own programs, and smaller companies can leverage bug bounty-as-a-service providers like Bugcrowd or Synack to run a program for their website or applications.\nPenetration tests remain a primary way for organizations to test their cyber defenses. While the traditional penetration test remains important, some companies are now leveraging new tools and techniques, more advanced red teaming exercises, capture the flag competitions and continuous testing via bug bounty programs as a way to test their defensive capability. Regardless, having an ethical hacker, a genuine cybersecurity professional hack into your system instead of a legitimate attack, the defense weaknesses can be bolstered before a malicious hacker targets your organization.', 'The ROI of Security Assessments\nIn the business world, Return on Investment (ROI) is used to evaluate an expense and is calculated by dividing the return (benefit) of an investment by the cost of the investment. This means a higher ROI represents a better investment. In a situation where the return and cost are tangible and easily measured, calculating ROI is not difficult. Unfortunately, calculating ROI for a security assessment is not easy because most of the benefits are intangible and are not easily measured.\nThe ROI of security assessments lies in the overall reduction of risk, which is accomplished by identifying and mitigating vulnerabilities, implementing or improving controls to match threat agents and attack vectors, and reducing the impact of an incident by improving incident response procedures. These benefits are difficult to measure at best, while the cost of a security assessment is glaringly obvious. Combine these two issues, and it is easy to see why companies have trouble justifying the cost of a security assessment.\nVulnerability Identification and Mitigation\nA comprehensive security assessment will identify not only network vulnerabilities but also vulnerabilities in policies and procedures. The security assessment will typically confirm the vulnerabilities identified within a risk assessment and, in some cases, may identify new ones. In either case, the security consulting firm performing the assessment should provide strategies for mitigating the identified vulnerabilities. These strategies, if followed, should reduce the number of vulnerabilities, resulting in an overall reduction of risk.\nIt is important to understand that most vulnerabilities are the result of non-existent or poor policies, procedures, and system administration. When a security assessment is limited to only network vulnerability scanning, it may not result in an overall reduction of risk because it will not identify the vulnerabilities resulting from poor policies, procedures, and system administration.\nImproved Understanding of Threats and Attack Vectors\nA comprehensive security assessment should also include penetration testing. A well documented penetration test is an excellent way to gain insight into the current threat agents and attack vectors. In addition, the penetration test can identify new threats not included in the risk assessment. An improved understanding of the threats, threat agents, and attack vectors should improve the controls used to protect against those threats and threat agents resulting in an overall reduction of risk.\nPreparation for Future Incidents\nAnother benefit of including the penetration testing with a comprehensive security assessment is the opportunity to test incident response policies, procedures and personnel. The results of the security assessment should encourage improvements in incident response procedures that will reduce the time to identify and respond to incidents. Reducing the response time during an incident may also reduce the number of affected systems and the amount of pilfered data. According to research by Symantec and the Ponemon Institute, the average cost of a data breach is 5.5 million dollars. A two percent reduction in the average cost of a data breach will significantly improve the ROI of security assessments.\nThe primary benefit of a comprehensive security assessment should be the overall reduction of risk. This reduction in risk can only happen when the security assessment:\n- Identifies both network vulnerabilities and vulnerabilities in the policies and procedures of the organization,\n- Is used to improve the controls meant to protect against threats to the organization, and\n- Is used to improve incident response procedures and response times.\nA comprehensive security assessment that accomplishes these three things will provide the highest return on investment.']	['<urn:uuid:c043ad49-8339-4f96-bdf7-e55bb5e12e3d>', '<urn:uuid:ff17f0d8-5790-4858-816c-6ed11142f677>']	factoid	direct	verbose-and-natural	distant-from-document	three-doc	expert	2025-05-12T22:13:20.996306	20	77	1980
35	head protection specs required astm cert compared horse riding longboarding	For horse riding, ASTM certified helmets are required for head protection. For longboarding, ASTM F1492 certification is needed for half shell helmets used in freestyle and cruising, while ASTM F1952 is required for full face helmets used in downhill riding. Both sports' helmets must use EPS foam liners for impact absorption, and proper certification ensures protection against serious head injuries and death.	"[""The height above the ground, the frequency of falls and the variables introduced by the horse itself, create an environment where head impacts are relatively common in horse-related activities. Despite the agility of a rider, it is often impossible to adjust one’s position during a fall to avoid a high energy, head impact with the ground, a rock or other objects. Facts like these are what make wearing a helmet so important. A rider's experience nor a horse's training make a difference in the risk factors involved.\nTo help riders better understand the risks involved with equestrian riding activities and the importance of helmet safety, Troxel founder and chairman, Dr. Richard Timms, compiled a list of medical facts from a range of equestrian studies. If you're trying to convince a friend or loved one to wear a helmet, we encourage you to pass these facts along. And remember, buckle up every ride, every time.\n- 70,000 people are treated in emergency rooms because of equestrian related injuries accounting for an estimated 2,300 admissions annually. Head injuries account for 18% of emergency room injuries.\n- Lifetime riders who report riding six or more times per year reported a 13% lifetime rate of being hospitalized because of a riding injury.\n- Head injuries account for the majority of hospitalizations and deaths.\n- The risk of head injuries appears to be similar in English riding as compared to western riding. Serious head injuries and deaths are more common with aggressive riding such as jockey’s and eventer’s. Deaths are rare when wearing an ASTM certified helmet during non-aggressive riding activities.\n- The mechanism of the majority of equestrian related head injuries relates to your distance above the ground. In more aggressive forms of riding such as jumping and racing, the potential for a crush type injury is increased. The majority of head injuries occur from falls caused by unpredictable events. However, about 20 percent occur while performing non-riding activities or as a bystander.\n- Risk of injury relates more to your cumulative time riding and less to level of expertise. Concussions accounted for 8% of horse riding related emergency room data, a rate that is higher than many other major sporting activities. The majority of riders with a onetime concussion resolve completely without residual problems. This high rate of complete recovery diminishes with subsequent concussions.\n- Public health studies indicate that there are scattered riding related fatalities each year throughout the USA. Deaths are significantly reduced by wearing an ASTM certified helmet. For each fatality, there are many times more head injuries.\n- Recurrent head traumas (or TBI’s) and their consequences are uncommon in equestrian riding in contrast to contact sports such as football. They do occur to riders who frequently ride in high risk activities.\n- Certified helmets provide a highly effective means of absorbing much of the impact with the ground or objects such as a rock or the horse’s hoof. Helmets are certified for their ability to absorb energy, cover your skull and stay properly in place. Equestrian riders are wise to wear certified helmets. Uncertified helmets are for look and vanity. They should not be worn as a means of head protection. Riders who wear ASTM certified helmets rarely suffer serious head injuries or death from a head impact but there are exceptions especially during aggressive riding activities.\n- April 15, 2016"", 'Longboarding is all fun and games until you crack your head open like a watermelon on hard asphalt. Many have learned this the hard way but there’s no need for you to join the club.\nWhether you’re going to only cruise on a longboard or if you’re planning to skate downhill, you have to consider that sooner or later you will fall and slam your head. The right longboard helmet will help you walk away without having serious head injuries or even save your life.\nIn this post, you will learn what kind of helmet is the best for longboarding, which ones aren’t good and how to choose the best longboard helmet for your needs.\nFor a summarised version, check out the table below:\n|Half Shell||Freestyle and cruising, entry level helmet||Weighs 0.8 lbs\nCertified Multi-Impact (ASTM) and High Impact (CPSC)\n14 sizes available\n|Half Shell||Freestyle and cruising, entry level helmet||Sweat saver liner\nUnder 1 lbs\n|Full Face||Downhill and Freeride- Advanced riders||Certified Multi-Impact (ASTM) and High Impact (CPSC)\nHand-laid fiberglass shell & Shatter-resistant flip-up visor\nEPS foam liner with velvet lining\nFox Head Rampage Comp Imperial Helmet\n|Full Face||Downhill and Freeride- Advanced riders||Channeled EPS allows air to pass through\nMade of fiberglass\nPoured PU Chinbar construction for added protection\nThere are two main types of longboard helmets you should consider depending on what skateboarding discipline you (plan to) practice:\n- FULL FACE and\n- HALF SHELL helmets.\nHalf shell helmets are best for cruising, dancing, slalom, and freestyle. They generally offer less protection compared to full face helmets. Full face helmets provide more protection and are best for fast downhill skateboarding and freeride.\nFull face helmets are more expensive than half shell helmets, so it’s very common practice for beginners to start with a half shell helmet and then buy an additional full face helmet later when they start skating down hills.\n1. HALF SHELL longboard skateboard helmet\nAs a standard helmet in skateboarding, a half shell helmet (also known as “bucket”) offers basic protection suitable for cruising, longboard dancing, freestyle, slalom, street skateboarding and similar activities like roller skating or cycling.\nHowever, a half shell helmet is not the best option for downhill skateboarding or longboarding. The classic bucket shape protects the top and back of your head, as well as the forehead, but it doesn’t protect your face, eyes, ears or jaw.\nHard-shell vs Soft-shell helmets\nMost half shell longboard skateboard helmets have a “hard-shell”, which means that they are made of an outer ABS molded shell that cradles the inner lightweight EPS foam liner designed to disperse and minimize the impact. Definitely, avoid “soft-shell” helmets as they have a more flexible external layer and a softer multi-density foam liner which doesn’t provide the same level of protection as hard shell helmets with EPS do.\nAnother somewhat less popular shape of half shell helmet is a retro full cut which covers your ears as well. In addition to the safety aspect of it, a full cut helmet will also keep your ears warm when skating during wintertime. What doesn’t fall into the full cut category are half-shell helmets with removable ear pads you most often see in snowboarding or skiing.\nTwo types of Hard-shell helmet construction\nThere are two construction types of hard-shell helmets if we look at how the EPS liner is bonded with the outer shell: these are called a hardshell construction and an in-mold construction.\nA Hardshell Construction means that the outer shell and the inside EPS shell are not directly bonded together but rather glued on various contact points. If you’ve ever had a skateboard helmet wherein both pieces came apart due to longterm wear, this is the reason why.\nAn In-Mold construction means that the EPS is foamed directly into the outer shell. This results in an extremely stable bond between the components that are not dependent on the glue strength otherwise used in the hardshell construction.\nMulti-impact vs Single-impact helmets\nWhen a helmet receives an impact, the EPS foam liner gets deformed or cracks in order to disperse the energy of the blow but with this, it becomes structurally compromised. Some helmets can take more than others…\nHelmets can be multi or single-impact which indicates how many impacts they can withstand before they should be replaced with a new one. As the name suggests, the single-impact helmets should be replaced after they’ve received a single hit while the multi-impact helmets can withstand multiple impacts.\nHowever, it’s also important to understand that multi-impact helmets are made to withstand a single high impact hit and/or multiple low impact hits before they need to be replaced. This means that if you’ve used your helmet for a longer time period, it’s more likely it received multiple small impacts due to handling and transport, potentially resulting in loss of protective properties.\nCompared to the half shell helmets, the full face longboard helmets provide more protection as they cover the entire head and protect your eyes, face, jaw, and ears.\nAlso read: Best Full Face helmets for longboarding\nMost of the full face helmets come with a removable visor in different tints that protects your eyes from dust and bugs, as well as sun rays blinding you while you skate. They are built pretty much the same way as all other hard shell helmets; with an outer ABS molded shell, an inner EPS foam liner and additional foam padding for a better fit.\nAlso, a full face helmet is required at almost all downhill skateboarding/longboard events, so if you want to participate at racing or freeride events, you will have to have it.\nDesigned to be as aerodynamic as possible, full-face helmets don’t have many vents but it all comes down to what’s the safest for you.\nUnfortunately, full face helmet manufacturers share very little information about how big of an impact or the number of impacts their helmets can withstand. To make sure that you’re getting the best level of protection with your helmet, you should also check for certification marks the helmet might have (more information below).\nLongboard helmet certification marks\nWhen buying a longboard skateboard helmet it is extremely important to choose one that obtained the right certification marks. This ensures that the helmet was properly tested and that it met the required safety standards for this sport.\nAn example are the Icaro helmets used in DH some years ago…well the brand had to warn customers that that’s not really what they were made for…\nWarning: There are shops out there that are selling paragliding helmets like Icaro SkyRunner and claiming that these are “ideal for downhill skateboarding”. That is not correct. Do not use Icaro helmets for downhill skateboarding or longboarding because they have EN 966 certification for free flight and microlight sports, not for skateboarding. You’ve been warned and the folks at Icaro are saying the same thing:\nAnyone selling a helmet has to ensure that it is certified for the sport in which it will be used. Selling an uncertified helmet is an offence liable to criminal prosecution! Source\nHere are the certification marks that your longboard skateboard helmet should have…\nASTM – American Society for Testing and Materials (USA)\nASTM is an international standards organization that develops technical standards for different kinds of materials, products, systems, and services.\nWhen you want to buy a half shell helmet for cruising, dancing, and freestyle riding, you should look for an ASTM F1492 certified helmet. This is a standard specification for helmets used in skateboarding and trick roller skating. Helmets that comply with this standard are designed to protect your head for more than one moderate impact, but for a limited number of hits.\nAnother mark you may find is the ASTM 1447 for use by recreational bicyclists or roller skaters. These helmets, however, provide less protection than those with an ASTM F1492 mark.\nIf you want to buy a full face helmet for downhill and freeriding look for ASTM F1952; this is a standard for helmets used in downhill mountain bicycle racing, which also provides performance criteria for chin bars on full-face helmets.\nCSPS U.S. Consumer Product Safety Commission (USA)\nAs an American government standard, a CSPS certification means a helmet is safe for bicycle use. Currently, the CSPS does not have a standard specific for skateboarding helmets, but they are tested and marked with the CPSC 1203 certification for bicycle helmets.\nCE certification mark (EU)\nIf you’re buying a helmet in Europe, it should have a CE certification, which means it meets the health, safety and environmental protection requirements of the European Economic Area (EEA). For longboard or skateboard helmets the most common is the CE EN 1078 certification.\nThis standard specifies the requirements to withstand a single high impact and a few puncture impacts for bicycling, skateboarding or in-line skating.\nWarning: Pay attention to the CE mark if you’re buying a cheap skateboard helmet from China. Chinese manufacturers apply a logo similar to the CE mark, which actually means China Export and has nothing to do with the CE certificate.\nAS/NZS 2063:2008 (Australia)\nLongboard skateboard helmets which have this certification mark meet the current Australian Standards certification to withstand a single high impact in non-motorised recreational bicycling.\nSo that’s it…\nWhen buying a longboard skateboard helmet make sure that it’s a hard shell helmet with an EPS liner and that it’s certified. If you hit it hard, replace it to keep skating safely 🙂\nHow often should you change your longboard helmet?\nDepending on what kind of certification your helmet has, a single or multiple impact, you should replace it if you went through a hard slam or a serious crash.\nEven multi-impact certified helmets that went through numerous hits don’t offer the same amount of protection as when they did when they were new. With each slam, even these helmets provide less and less protection. If you don’t know if the helmet still offers good protection, you should replace it.\nIf you’ve been wearing your helmet for a few years now and never experienced a crash, you might also want to consider replacing it. The materials of the helmet can age and lose their protective properties more rapidly when exposed to sweat and heat, meaning they will not offer the same protection. Because of this, it is advised to replace a helmet after 3 years, even if you never exposed it to a fall or a crash.\nAre other helmets like cycling or motorcycling helmets also good for longboarding?\nAny helmet is better than no helmet but to maximize your safety it’s best to wear a helmet that is certified for the sport you’ll be practicing.\nIf you can’t get your hands on a skateboarding helmet, use whatever you have but be aware of the following downsides; Cycling helmets don’t offer enough protection, motorcycling helmets are too heavy and paragliding helmets (like Icaro SkyRunner) are not really made to be smashed against the asphalt and do not have certificates required for skating.\nThe best alternative to a longboard full-face helmet would be a mountain biking helmet. An example is the POC Coron Air Carbon SPIN mountain bike helmet on the photo which includes all of the certification mentioned above.\nHowever, these are bulkier, feature lots of vents which means that they aren’t as aerodynamic as you want them to be. The downside is also that they don’t come with a visor, so you would probably have to make one yourself.\nDo you really need a longboard helmet?\nYou may have heard about skateboarders not being fans of helmets and that they take away the freedom. In case you need convincing, especially after seeing many street skateboarders hatin’, you should know that the stats are not on your side.\nCheck out these results from the research done in 2014:\nLongboarders suffered twice as many injuries to their heads and necks (23.3% vs. 13.1%, p < 0.000) and twice as many severe neurological traumas (8.6 vs. 3.7%, p < 0.000) while skateboarders suffered more injuries to their lower extremities (33.7% vs. 24.7%, p < 0.002). Source\nKeep it safe out there!']"	['<urn:uuid:17d8b1a3-0cb3-4cfd-a84c-4697e90b43bd>', '<urn:uuid:5c22e7d4-3dc1-447d-bcd1-08b2c3f443bf>']	factoid	with-premise	long-search-query	distant-from-document	comparison	expert	2025-05-12T22:13:20.996306	10	62	2580
36	I'm doing research on modern regional cooperation - can you tell me who organized a workshop about regionalism and neighborhoods in Central Asia and where it was held?	The workshop was co-organized by the World International Studies Committee (WISC) in cooperation with the Johan Skytte Institute of Political Studies at University of Tartu. It was held at the Hall Nilufar, Dushanbe Serena Hotel in Dushanbe, Tajikistan.	"['Neighborhoods and Regionalisms: Shifting Borderlines of Inclusion and Exclusion in Central Asia\nDate: 3-4 April 2019\nDeadline for proposals: February 15, 2019\nPlace: Dushanbe, Tajikistan\nThis workshop will be co-organized by the World International Studies Committee (WISC) in cooperation with the Johan Skytte Institute of Political Studies at University of Tartu. It intends to give an analytical perspective on the dynamics of different types, forms and modalities of regionalism unfolding in Central Asia where local agency intersects with policies of major power holders. These cooperative and conflictual encounters can be conceptualized as a series of neighborhoods that differently link and relate major actors – China, Russia, Turkey, and Iran - to each other and thus shape these interconnections as areas of inclusion and exclusion, bordering and debordering, securitization and desecuritization.\nThe workshop plot draws upon a rich legacy of studies in region-(un)making that became one of the buzz words after the end of the Cold War, with the EU being widely considered as a pioneer and a locomotive of regional integration in those spaces where EU-centered normative order could be projected. However, in recent years a new set of questions is being raised: how feasible might be alternative regional projects beyond the West? How important regional dimensions could be for a “post-liberal” and apparently “multi-order” world? Can illiberal political regimes develop their own visions and conceptualizations of regionalism and policies of neighborhood?\nAgainst the backdrop of these main questions, the organizing team wishes to focus on the following landmarks:\n- to discuss the variety of regionalisms and neighborhoods in the light of the most recent trends in – and factors of - major actors’ international policies (authoritarian devolution in Turkey and Russia, new economic projects launched by China, different conceptualizations of Eurasianism, etc.);\n- to unpack the logic behind the ‘old’ and ‘new’ (emerging) models /forms of regionalism developing at the intersections / meeting points of major actors’ zones of interest and influence;\n- to look at these regionalisms and neighborhoods as phenomena of their own, capable of fostering political changes beyond the West, and in particular in the “global South”.\nThus, the structure of the workshop will reflect the basic points we seek to elucidate and understand:\n- What Central Asia can tell us about the post-liberal international order and its prospects?\n- What are the major factors of change in the region in course of the post-Cold\n- War decades, how predictable / unpredictable they were?\n- How Central Asia might fit in a new type of international order viewed through post-liberal lens?\n- Are regional actors to adapt to the dominant structures of power, or to generate / amplify the post-liberal momentum?\n- How the concepts of regionalism and regionalization can be applied toward China’s economic policies and projects in this region?\n- Can Central Asia be considered as a meeting point for regionalist strategies developed by external powers?\n- And, in a wider sense, how can we look at some other regions (in particular, the Mediterranean, North Africa, the Middle East and the Black Sea area) from a post-liberal perspective, and how the securitization momentum can be related to post-liberalism?\nSince these questions are explicitly multi-disciplinary in their conceptual underpinnings, we are eager to bring together a team of contributors coming from various academic backgrounds – comparative politics, foreign policy analysis, international political economy, and cultural studies and sociology. This combination of variegated approaches might be instrumental in uncovering different sides and angles of regionalism and spatiality in Central Asia and beyond, and thus giving more multidimensional vistas to all participants. The workshop will serve as a key point in the process of publishing a special issue in a well reputed peer-reviewed international journal, or an edited volume with one of the leading global publishers.\nProf. Andrey Makarychev, Tartu Ülikool\nVenue: Hall Nilufar, Dushanbe Serena Hotel, 14 Rudaki Avenue, Dushanbe, Tajikistan 734013\nDay 1 (April 3)\n10:00 – Registration, morning coffee and opening of the workshop\n10:30 – Session 1: “Theories, Concepts and Vocabularies of Central Asian Regionalism”\n- Dr. Filippo Costa Buranelli (University of St Andrews, UK): “International Society and Central Asia – The Brotherly Republics?”\n- Dr. Diana T. Kudaibergenova (University of Cambridge, UK): “Routes, Connections and Hubs in the Inter-Asian Context”\n- Thomas Linsenmaier (University of Tartu, Estonia): ""Russia\'s Revolt against the West: the Central Asian Dimension""\n- Prof Andrey Makarychev (University of Tartu, Estonia): “Central Asia: between the post-Soviet and the post-colonial”\n13:00 – 14:00 Lunch\n14:00 – Session 2: «Integration and Institutions”\n- Prof Tran Viet Dung (Ho Chi Minh City University, Vietnam): “Development of ASEAN Economic Community: Experiences for the Central Asia’s regional cooperation”\n- Dr. Shiva Alizadeh (University of Tehran, Iran): «Developmental Motivations and Central Asian Integration”\n- Dr. Gulnara Dadabayeva (KIMEP University, Kazakhstan): “Eurasian Union: Is It an Attempt to Restore a Soviet Union?”\n- Dr. Karolina Kluczewska (University of Paris 13, France): “Development aid in Tajikistan 1991-2018 and local perceptions of regional organizations”\n- Dr. Volkan Ipek (Yeditepe University, Turkey): “Kazakhstan’s foreign policy towards Sub-Saharan African states”\n17:00 Tea time and socializing\nDay 2 (April 4) for invited speakers\n10:00 – 11:30 Round table: “International Relations and the Home-Grown Knowledge in the Global South: Towards a Comparative Framework for Analysis”\n12.00 – 13:00 Publication plans and closing discussion']"	['<urn:uuid:c3f91820-8179-4f59-a965-9640417ac2dc>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T22:13:20.996306	28	38	881
37	father english literature chaucer and japanese zen monks death poems writing style differences compare	While Japanese Zen monks wrote death poems incorporating nature imagery and Buddhist philosophy of transience in a 17-syllable structure, Chaucer's writing style was marked by lucid expression, joyous originality, and rhyming couplets, free of ambiguities and using direct philosophical maxims in colloquial language.	['Yoel Hoffmann traces the tradition of writing a death poem by Japanese Zen monks and Haiku poets to the early part of the 9th century when Buddhism, already established in Japan, began to become much more influential. The Japanese already had their own indigenous beliefs, related to animism, which continued to develop side by side with Buddhism and which has its modern-day expression in Shinto. The two traditions influenced one another and Buddhism – which originated in India – owes its distinct flavour of Zen to the history and culture of Japan and the impact of its journey through China.\nIn his introduction Hoffmann writes: “The Japanese love for nature… precluded any escape into abstraction; that which is formless and colourless has no solace for the heart. The idea of transience, expressed in the Buddhist literature of India by the sight of putrefying corpses and rotten food, is conveyed by the Japanese through images of the changing seasons.”\nWoven in to this is the Japanese attitude to death, which according to Hoffman “takes place in an atmosphere of serenity, with almost pleasurable expectation of the voyage to the next world”. Japanese culture has given us the concepts of hara-kiri and of course the Kamikaze of the Second World War who flew their planes, packed with explosives, into US war ships in the Pacific Ocean. Suicide in Japan, Hoffmann tells us, does not have the same stigma as in the West.\nHoffmann also traces the development of the Haiku, the 17-syllabel three-line nature poem expressing a poet’s experience that brings about an imaginative leap forward. The Haiku has its origins in Zen but has been adopted by secular poets throughout the world – there is even an Irish Haiku society which publishes the web-based journal Shamrock.\nThis the landscape in which the ‘death poems’ came to be written, incorporating the Japanese love of nature, Zen Buddhist philosophy of transience and the Japanese stoicism in the face of death. They were mostly written by a poet or monk who realised they were about to die and served as their last commentary on the world they were about to leave.\nJapanese Death Poems has the subheading – Written by Zen Monks and Haiku Poets on the Verge of Death. Not all the poems by the monks are Haiku but they do all incorporate elements of nature, its transience and a revelatory insight. Many of the poems are accompanied by a brief biography of the poet/monk and the circumstances that led them to writing their poem.\n‘Kozan Ichikyo, died February 12, 1360, at 77\nEmpty-handed I entered the world\nBarefoot I leave it.\nMy coming, my going --\nTwo simple happenings\nThat got entangled.\nA few days before his death, Kozan called his pupils together, ordered them to bury him without ceremony, and forbade them to hold services in his memory. He wrote this poem on the morning of his death, laid down his brush and died sitting upright.’\nThe Haiku are given in their ‘phonetic’ Japanese version to allow the reader to see the tight syllabic structures but the translations are kept simple, without trying to recreate the 17-syllabel structure (which many Haikuists do not believe is appropriate for the English language).\nHoffmann renders the Haiku death poem of a poet called Senryu, who died June 2, 1827 as:\nLike dew drops\non a lotus leaf\nThere are hundreds of Haiku and other ‘death poems’ in this compact little book and while the introduction is worth reading through to get the context and background the poems and the biographies of the poets and monks can simply be dipped in and out of.\nSometimes you are simply left with a poetic image but on others seem to be written in the true belief that their author felt they were really moving from one state of existence to another.\nThe Haiku written by Mitoju, who ‘Died on the eighteenth day of the seventh month, 1669 at the age of 82’ was:\nThe foam on the last water\nmy mind is clear\nor Sodo who ‘died on the fifteenth day of the eighth month, 1716 at the age of 75’ wrote:\nFull autumn moon:\nmy shadow takes me with him', 'He is known to have been proficient in English, French, and Latin, and may…\nDid Chaucer write in French?\nChaucer was of the gentle classes and he clearly spoke French from an early age and probably first wrote poems in French, the language of the courts in which he served first as a page in the court of the Countess of Ulster and then as squire in the courts of Prince Lionel and Kings Edward III and Richard II.\nWhat is Geoffrey Chaucer’s writing style?\nBy Geoffrey Chaucer\nThe style of The Canterbury Tales is characterized by rhyming couplets. That means that every two lines rhyme with each other.\nWhat are three features of Chaucer’s language and writing style?\nFirstly, Chaucer’s style is marked by lucidity of expression, joyous originality and easiness free of ambiguities and direct philosophical maxims. In describing nearly all his characters, he uses colloquial language easy to understand for a common man.\nWho is father of English?\nWho is known as the father of the English language? Geoffrey Chaucer. He was born in London sometime between 1340 and 1344. He was an English author, poet, philosopher, bureaucrat (courtier), and diplomat.\nIs Chaucer written in Old English?\nWritten in Middle English, the story follows a group of pilgrims who are travelling the long journey from London to Canterbury Cathedral. … The Canterbury Tales was one of the first major works in literature written in English. Chaucer began the tales in 1387 and continued until his death in 1400.\nWho is known as the father of poetry?\nGeoffrey Chaucer was born in the 1340s in London, and though he is long gone, he is by no means forgotten. … Ever since the end of the 14th century, Chaucer has been known as the “father of English poetry,” a model of writing to be imitated by English poets.\nWho is the father of literature?\nGeoffrey Chaucer, the father of English literature, was born in circa 1340 in London. He is most famous for writing his unfinished work, The Canterbury Tales, which is considered as one of the greatest poetic works in English.\nCan Chaucer read Latin?\nStories from the classics were well known and loved in the medieval period, even though their overt paganism often resulted in Christianised retellings by ‘mythographers’. Chaucer evidently knew the Latin writings of Ovid, Cicero, Virgil, Macrobius, and Boethius, and he probably learned Latin at school.\nWhich is the best Canterbury Tale?\nThe Miller’s Tale.\nAnd Nicholas amydde the ers he smoot … Perhaps the most famous – and best-loved – of all of the tales in Chaucer’s Canterbury Tales, ‘The Miller’s Tale’ is told as a comic corrective following the sonorous seriousness of the Knight’s tale.\nWhat is the language of the Canterbury Tales like?\nLanguage in The Canterbury Tales\nThe Canterbury Tales is written in Middle English, which bears a close visual resemblance to the English written and spoken today.\nWhat is the criteria for the winning tale?\nThe criteria includes that each pilgrim would tell two tales each–one on the way there and one on the way back. The tales will be judged by the Host on two criteria: entertainment value and moral lesson. The winner of the contest will enjoy a meal paid for by the remaining pilgrims at the Host’s Taberd Inn.\nWho was the most romantic poet?\nThe best known English Romantic poets include Blake, Coleridge, Wordsworth, Keats, Byron and Shelley. In America, the most famous Romantic poet was Edgar Allan Poe; while in France, Victor Marie Hugo was the leading figure of the movement.\nWhat was the first poem?\nTHE BEGINNING of the world’s first truly great work of literature – the 4,000-year-old Mesopotamian Epic of Gilgamesh, the poem on which the story of Noah and the Flood was probably based – has been discovered in a British Museum storeroom.\nWho is the father of Old English poetry?\nGeoffrey Chaucer (/ˈtʃɔːsər/; c. 1340s – 25 October 1400) was an English poet and author. Widely considered the greatest English poet of the Middle Ages, he is best known for The Canterbury Tales. He has been called the “father of English literature”, or, alternatively, the “father of English poetry”.\nDid Shakespeare write in Middle English?\nTo begin with, though: no, Shakespeare is not Middle English. He actually wrote in Elizabethan English, which is still classified within the confines of Modern English. … This can be traced back to what is called Old English, a language spoken by the Anglo-Saxons.\nWhat is an example of Old English?\nOld English was the language used by Anglo-Saxtons from around 450 to 1150 that used many German words, was formal and was very different from modern English. The language that the book Beowulf is written in is an example of Old English.\nWhy Geoffrey Chaucer is called the father of English poetry?\nGeoffrey Chaucer is called the father of English literature because he was the first to write what became generally well-known and recognized poems and stories in the language of the common people of his time – medieval English.\nWho is the mother of English language?\nEnglish is a West Germanic language that originated from Anglo-Frisian dialects brought to Britain in the mid 5th to 7th centuries AD by Anglo-Saxon migrants from what is now northwest Germany, southern Denmark and the Netherlands.\nWho invented English?\nEnglish has developed over the course of more than 1,400 years. The earliest forms of English, a group of West Germanic (Ingvaeonic) dialects brought to Great Britain by Anglo-Saxon settlers in the 5th century, are collectively called Old English.']	['<urn:uuid:ff2dd46f-d40f-42fc-8aca-c3e2c03277e4>', '<urn:uuid:756a04ce-b760-48e5-a881-d8392ce58770>']	factoid	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-12T22:13:20.996306	14	43	1636
38	What defines Supreme Court jurisdiction and when will it hear appeals?	The Supreme Court's jurisdiction extends to all cases arising under the Constitution, federal laws, treaties, cases affecting ambassadors, maritime cases, controversies involving the United States, disputes between states, and cases between citizens of different states. However, unlike circuit court appeals, the Supreme Court is not required to hear most appeals. Parties must file a writ of certiorari, and the Court grants less than 1% of appeals. The Court typically only takes cases when there are conflicting decisions across the country on an issue or when there is an egregious error.	['The District court system has three main levels: District courts (the trial court), Circuit courts which are the first level of appeal, and the Supreme Court of the United States, the final level of appeal in the federal system. There are 94 district courts, 13 circuit courts, and one Supreme Court throughout the country.\nCourts in the US District Court system work differently in many ways than state courts. The primary difference for civil cases (as opposed to criminal cases) is the types of cases that can be heard in the federal system. Federal courts are courts of limited jurisdiction, meaning they can only hear cases authorized by the United States Constitution or federal statutes. The federal district court is the starting point for any case arising under federal statutes, the Constitution, or treaties. This type of jurisdiction is called “original jurisdiction.” Sometimes, the jurisdiction of state courts will overlap with that of federal courts, meaning that some cases can be brought in both courts.\nCases that are entirely based on state law may be brought in federal court under the court’s “diversity jurisdiction.” Diversity jurisdiction allows an Attorney of one state to serve a witness in a U.S.D.C Subpoena when the witness is located in a different state.\nThe district courts are the general trial courts of the US court system. Each district court has at least one United States District Judge, appointed by the President and confirmed by the Senate for a life term. District courts handle trials within the federal court system. The districts are the same as those for the U.S. Attorneys, and the U.S. Attorney is the primary prosecutor for the federal government in his or her respective area.\nDistrict court judges are responsible for managing the court and supervising the court’s employees. They are able to continue to serve so long as they maintain “good behavior,” and they can be impeached and removed by Congress. There are over 670 district court judges nationwide.\nSome tasks of the district court are given to federal magistrate judges. Magistrates are appointed by the district court by a majority vote of the judges and serve for a term of eight years if full-time and four years if part-time, but they can be reappointed after completion of their term. In criminal matters, magistrate judges may oversee certain cases, issue search warrants and arrest warrants, conduct initial hearings, set bail, decide certain motions (such as a motion to suppress evidence), and other similar actions. In civil cases, magistrates often handle a variety of issues such as pre-trial motions and discovery.\nDistrict trial courts have also been established for a few subject-specific areas. Each federal district also has a bankruptcy court for those proceedings. Additionally, some courts have nationwide jurisdiction for issues such as tax (United States Tax Court), claims against the federal government (United States Court of Federal Claims), and international trade (United States Court of International Trade).\nOnce the federal district court has decided a case, the case can be appealed to a United States court of appeal. There are twelve federal circuits that divide the country into different regions. The Fifth Circuit, for example, includes the states of Texas, Louisiana, and Mississippi. Cases from the district courts of those states are appealed to the United States Court of Appeals for the Fifth Circuit, which is headquartered in New Orleans, Louisiana. Additionally, the Federal Circuit Court of Appeals has a nationwide jurisdiction over very specific issues such as patents.\nEach circuit court has multiple judges, ranging from six on the First Circuit to twenty-nine on the Ninth Circuit. Circuit court judges are appointed for life by the president and confirmed by the Senate.\nAny case may be appealed to the circuit court once the district court has finalized a decision (some issues can be appealed before a final decision by making an “interlocutory appeal”). Appeals to circuit courts are first heard by a panel, consisting of three circuit court judges. Parties file “briefs” to the court, arguing why the trial court’s decision should be “affirmed” or “reversed.” After the briefs are filed, the court will schedule “oral argument” in which the lawyers come before the court to make their arguments and answer the judges’ questions.\nThough it is rare, the entire circuit court may consider certain appeals in a process called an “en banc hearing.” (The Ninth Circuit has a different process for en banc than the rest of the circuits.) En banc opinions tend to carry more weight and are usually decided only after a panel has first heard the case. Once a panel has ruled on an issue and “published” the opinion, no future panel can overrule the previous decision. The panel can, however, suggest that the circuit take up the case en banc to reconsider the first panel’s decision.\nBeyond the Federal Circuit, a few courts have been established to deal with appeals on specific subjects such as veterans claims (United States Court of Appeals for Veterans Claims) and military matters (United States Court of Appeals for the Armed Forces).\nAfter the circuit court or state supreme court has ruled on a case, either party may choose to appeal to the Supreme Court. Unlike circuit court appeals, however, the Supreme Court is usually not required to hear the appeal. Parties may file a “writ of certiorari” to the court, asking it to hear the case. If the writ is granted, the Supreme Court will take briefs and conduct oral argument. If the writ is not granted, the lower court’s opinion stands. Certiorari is not often granted; less than 1% of appeals to the high court are actually heard by it. The Court typically hears cases when there are conflicting decisions across the country on a particular issue or when there is an egregious error in a case.\nThe members of the Court are referred to as “justices” and, like other federal judges, they are appointed by the President and confirmed by the Senate for a life term. There are nine justices on the court – eight associate justices and one chief justice. The Constitution sets no requirements for Supreme Court justices, though all current members of the court are lawyers and most have served as circuit court judges. Justices are also often former law professors. The chief justice acts as the administrator of the court and is chosen by the President and approved by the Congress when the position is vacant.\nThe Central District of California is the largest federal district by population. it includes all five counties that make up the Greater Los Angeles Area. By contrast, New York City and the surrounding metropolitan area are divided between the Southern District of New York (which includes Manhattan and The Bronx) and the Eastern District of New York (which includes Brooklyn, Queens, Staten Island, Nassau County and Suffolk County). New York suburbs in Connecticut and New Jersey are covered by the District of Connecticut and District of New Jersey, respectively.\nThe Southern District of New York and the Central District of California are the largest federal districts by number of judges, with 28 and 27, respectively.', 'In a case or controversy, there must be an injured plaintiff. The injured plaintiff requirement to prove that there is an existing injury or damage is a key requirement of the standing doctrine of the court. The reason why issues that were not raised by both parties should be resolved is that standing doctrine is necessary to establish the jurisdiction of the court. The law requires that a plaintiff must prove the injury-in-fact, that the injury under controversy is justifiably traceable to the challenged action of the defendant, and that the injury or damage could be decided by a favorable decision.\nA caveat is that if a decision of a state court follows on the independent and adequate state grounds, the result of the case would remain unchanged even if the federal court decided otherwise. Justiceability means the capability of a text to take effect as a legitimate rule or to make legal effects. In many jurisdictions, justifiability of a case depends on the way a plaintiff proves an existing injury or damage.\nIf the injury or damage cannot be proved by the plaintiff and there is no showing that there is injury-in-fact, then justifiability of an action is not accepted. Advisory opinions are also released by the court but do not have the effect of deciding a particular court case or controversy, but purely advises on the constitutionality or interpretation of a legitimate rule. Advisory opinions have no bearing in a given lawsuit as it only explains a particular issue of such case.\nThe law of the United States substantially provides that if a matter in question is a political question it means that there is a need of coordination with a branch of the federal government as committed by the constitution, the standard is not adequate for the court of law to apply and that it is not prudent for the court to encroach judgments. A lawsuit is not considered ripe for litigation if it rests upon contingent future events that may not happen as expected or in truth may not occur at all. Ripeness then is defined as a readiness of a lawsuit for trial or hearing.\nThe United States law provides that a lawsuit is considered moot or academic if future lawful processes with respect to the same can have no effect, or events have placed it beyond the reach of the law. Hence, any matter that is considered moot has been deprived of significance in any setting. Standing is otherwise known as locus standi which means the capability of a party to prove to the courts of justice adequate link to and harm from the law or suit challenged to maintain that party’s participation in the case.\nA person can only question the constitutionality of a law if he can prove that he could be affected of the law being questioned. Congressional Power over Federal Court Jurisdiction was designed to strike a proper balance between the need for an independent federal judiciary to enforce supreme federal law and the competing need to ensure political control over the judiciary. Supreme Court Jurisdiction\nThe jurisdiction of the highest judicial body is stated in Article III of the Constitution which states as “The judicial Power shall extend to all Cases, in Law and Equity, arising under this Constitution, the Laws of the United States, and Treaties made, or which shall be made, under their Authority; to all Cases affecting Ambassadors, other public Ministers and Consuls; to all Cases of admiralty and maritime Jurisdiction; to Controversies to which the United States shall be a Party; to Controversies between two or more States; between a State and Citizens of another State; between Citizens of different States; between Citizens of the same State claiming Lands under Grants of different States, and between a State, or the Citizens thereof, and foreign States, Citizens or Subjects” (Cornell Education, 2008, p. 1). Jurisdiction is a power of the court to hear or decide a case. Subject matter jurisdiction basically provides that whether a federal court can rule on the subject matter of a case. An example of this principle is a probate of will case wherein it can only be decided by the court a quo.\nA federal question jurisdiction means that federal courts will process trial of cases that involve questions pointing on the constitution or other federal rules or laws. Diversity jurisdiction means that federal law also authorizes federal courts to try and decide lawsuits wherein the opposing parties belong to different states. Pendent and Ancillary Jurisdiction Pendent jurisdiction is a joint cause of action arises out of the common nucleus of operative facts as a cause of action with an independent source of federal jurisdiction while ancillary jurisdiction means that it is a joint cause of action which arises out of the common nucleus of operative facts as a cause of action with an independent source of federal jurisdiction.']	['<urn:uuid:408239a7-643d-4cee-b7cb-d5c5964aec96>', '<urn:uuid:1ec26e53-cae4-4bf8-8945-46b0ccd8d1a1>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T22:13:20.996306	11	90	2008
39	cells adapt change environment function process explain methods	Cells can adapt to environmental changes through various mechanisms. In some cases, cells undergo metaplasia, where one cell type changes into another more suitable type, such as when squamous epithelium changes to columnar cells in response to corrosive gastric acids. Cells can also adapt through hypertrophy, where they increase in size to better perform their functions. Additionally, early prokaryotic cells demonstrated adaptation by developing different methods of obtaining nutrition - some became chemoautotrophs when organic nutrients became scarce, while others evolved to perform photosynthesis using bacteria-chlorophyll and later true chlorophyll.	"['Phases of Biological Evolution of Life:\nAfter the formation of complex macromolecules and nucleoproteins which are actually supposed to be the building molecules of a living system, further events in relation to the origin of life constitute the biological evolution of life which includes the following phases-\nOrigin of Primordial Life:\nIt is believed that a number of complex organic and inorganic compounds were present in the extremely hot sea-water in the initial stages. These compounds got aggregated in various combinations and got settled down at the bottom of the sea as large colloidal masses. The fluid sediments, which were rich in colloidal substances formed small globules called coacervates (Pre-cells). The coacervates thus formed were highly organized colloidal system having different types of proteins, nucleoproteins, organic and inorganic molecules suspended in water. They had special organisations due to a definite pattern of arrangement of colloidal particles inside them. Their outer limiting membranes were formed of certain types of fatty acids like lecithin and cephalin which show a great affinity for water. The formation of limiting or plasma membrane might have resulted in differential accumulation of substances inside the coacervates and there might have occurred certain internal reactions. Some of the proteins of coacervates started working like organic catalysts, i.e., enzymes and the coacervates started absorbing organic substances from oceanic soup and became anaerobic heterotrophs. They grew in size and became variable in chemical composition. Finally, they multiplied by breaking down into smaller droplets and led to the formation of protocells.\nIt is presumed that proteins aggregated around nucleic acids resulting into the forms which resembled the present-day viruses and were probably the first most primitive or primordial life. Thus, the first living cell which appeared had clumps of nucleoproteins i.e. prokaryotic type without a definite nucleus. These cells were similar to present Monera i.e. like bacteria. The second cell type, probably descended from the first cells, was represented by protistans, i.e., they had nucleoprotein clumps condensed into a central mass surrounded by a fine membrane. Thus, the major achievements during this phase were the origin of early viruses, early prokaryotic cells and early eukaryotic cells.\nOrigin of Autotrophs:\nWith the gradual increase in the number of heterotrophs, organic nutrients were consumed considerably and this caused a reduction in the natural sources of food in the ocean. Therefore, early living organisms resorted to different methods of obtaining nutrition. During this course, some of the early prokaryotes acquired enzymes which would catalyse the synthesis of simple carbohydrates molecules from inorganic substances of the oceanic water. This was supposed to be the start of autotrophism and energy utilized in this was only obtained from anaerobic breakdown and not from sunlight. Therefore, earth autotrophism was chemoautotrophism which is also shown by sulfur bacteria.\nIn the meantime, certain autotrophic bacteria synthesized a new substance from a metallo-porphyrin, i.e., magnesium porphyrin of the seawater. This substance was called bacteria-chlorophyll which could absorb sunlight and help these prokaryotes in trapping solar energy and caused the fixation of CO2 into carbohydrates molecules. Such photosynthetic autotrophism is shown by certain marine planktonic sulfur bacteria.\nSubsequently, bacteria chlorophyll underwent certain molecular changes which resulted in the development of true chlorophyll which enabled the prokaryotes to synthesize carbohydrates by using water in place of H2S. Thus, the prokaryotic cells which were chemo-autotrophic became converted into true autotrophic which were comparable with the blue-green algae of today.\nOrigin of Eukaryotic Cells:\nWith the release of free oxygen into the atmosphere, conditions suitable for aerobic respiration were established upon the earth about 2700 million years ago. The aerobic respiration produces about 20 times more energy in a biological system than anaerobic respiration. Therefore, the prokaryotes gradually modified to become adapted for aerobic mode of respiration. They developed a true nucleus, mitochondria and other cellular organelles. In this way, free living eukaryotes originated in ocean probably about 1500 million years ago. These eukaryotes were like unicellular organisms of today.\n- Constituents Of Air And Their Uses\n- Air Pollution- Causes, Effects, and Control Measures\n- Water Pollution- Causes, Effects, and Control Measures\n- Solid Waste Management\n- Applications Of Laser And Maser\n- Uses of Washing Soda, Bleaching Powder, etc.\n- Plant and Animal Cell\n- Mitosis: Process and Significance\n- Meiosis: Process and Significance\n- Modes of Nutrition\n- Process of digestion in human beings\n- Components of Blood\n- Human Heart Important Facts\n- Nervous System\n- Chemical Coordination and Integration', 'Mechanisms of organelle division and inheritance and their implications regarding the origin of eukaryotic cells, Phenotypic analysis of bacterial virulence using electric cell-substrate impedance sensing, Spirochete attachment ultrastructure: implications for the origin and evolution of cilia, A units-of-evolution perspective on the endosymbiont theory of the origin of the mitochondrion, How Did Life Evolve? The nucleus (plural = nuclei) houses the cell’s DNA. Apoptosis, hyperplasia, hypertrophy, metaplasia, dysplasia. Credit: modification of work by Matthew Britton; scale-bar data from Matt Russell. (b) This image shows paired chromosomes. As a consequence, afflicted individuals have an impaired ability to absorb nutrients. A eukaryote is defined as any organism that is chiefly characterized by a cell with one or more nuclei at least once in its lifetime as opposed to a prokaryote that has a cell lacking a well-defined nucleus and with a nucleoid only. Hypertrophy on the other hand refers to an increase in the size of the organ following an increase in the size of the constituent cells. The cell wall is a rigid covering that protects the cell, provides structural support, and gives shape to the cell. Therefore, the pH within lysosomes is more acidic than the pH of the cytoplasm. This fixed form is facilitated by the rigid cellulose-based cell wall that surrounds the plasma membrane of the plant cell; which is also absent in animal cells. In the lectures and readings on bacterial cell structure, we discovered some morphological features of large bacteria that allow them to effectively overcome diffusion-limited size barriers (e.g., filling the cytoplasm with a large storage vacuole maintains a small volume for metabolic activity that remains compatible with diffusion-driven transport). Eukaryotic Cells Definition. Their genetic material is organized in chromosomes. Like plant cells, photosynthetic protists also have chloroplasts. In addition to the membrane bound ATP synthase proteins that facilitate ATP production, mitochondria are the only organelles that contain their own DNA material and is therefore capable of replication. Each centriole is a cylinder of nine triplets of microtubules. DNA that takes the euchromatic form is usually more frequently transcribed and expressed by the cell. Figure 1. Some of the phospholipid structures are bonded to cholesterol molecules. In some areas of the envelope the inner and outer layers merge, forming openings known as nuclear pores. The boundary of the nucleus is called the nuclear envelope. The light harvesting reactions take place in the thylakoid membranes, and the synthesis of sugar takes place in the fluid inside the inner membrane, which is called the stroma. We also acknowledge previous National Science Foundation support under grant numbers 1246120, 1525057, and 1413739. Examples of eukaryotic cells are plants, animals, protists, fungi. These enzymes are active at a much lower pH than that of the cytoplasm. Like bacteria and archaea, eukaryotic cells have a plasma membrane, a phospholipid bilayer with embedded proteins that separates the internal contents of the cell from its surrounding environment. Some chromosomes have sections of DNA that encode ribosomal RNA. To be completely honest, however, in many cases, we don\'t actually know all of the selective pressures that led to the creation or maintenance of certain cellular structures, and the likelihood that one explanation will fit all cases is slim in biology. It is the basic unit of life. They also help to detoxify many toxins that may enter the body. In previous sections, we began to consider the Design Challenge of making cells larger than a small bacterium—more precisely, growing cells to sizes at which, in the eyes of natural selection, relying on diffusion of substances for transport through a highly viscous cytosol comes with inherent functional trade-offs that offset most selective benefits of getting larger. Nontubulin proteins (indicated by the green lines) hold the microtubule triplets together. Colloquially, the lysosomes are sometimes called the cell’s “garbage disposal”. Have you wondered why? For example, alcohol is detoxified by peroxisomes in liver cells. Some bacteria perform photosynthesis, but their chlorophyll is not relegated to an organelle. In this case, chronic exposure of the squamous type epithelium to the corrosive gastric acids promote cellular change to the columnar type of cells, which are more robust. The plasma membrane controls the passage of organic molecules, ions, water, and oxygen into and out of the cell. The centrosome (the organelle where all microtubules originate in animal and yeast) replicates itself before a cell divides, and the centrioles appear to have some role in pulling the duplicated chromosomes to opposite ends of the dividing cell. Many of these redox reactions release hydrogen peroxide, H2O2, which would be damaging to cells; however, when these reactions are confined to peroxisomes, enzymes safely break down the H2O2 into oxygen and water. Francesca Salvador MSc (Every benefit has some cost; can you list both?) In either case, these enzymes are stored in the lysosomes for subsequent release. The type of cell composing multicellular, as well as a number of unicellular, organisms. ATP synthesis takes place on the inner membrane. Unlike prokaryotic cells, eukaryotic cells have many of their intracellular functions organized within structures called organelles. This is not considered as a direct relative to malignant lesions. There is another form of cell death that is unplanned and may result in more injury to adjacent cells that is known as cellular necrosis. lung parenchyma). Answer Could Be Inside Out, Dining in: how the right amount of cellular self-cannibalism can keep you healthy, Benitec Limited (ASX:BLT) Updates On Progress Of The US Graham Patent 10/646,070 Notice of Allowance, Pyotr Alekseyevich Kropotkin: ""the struggle for existence"" and ""mutual aid"", Mechanically Active\' DNA Material Created, How cancer cells \'hijack\' a mechanism to grow, Eukaryotic initiation factor 4A-like NUK-34, Eukaryotic Polypeptide Chain Release Factor 3, eukaryotic translation initiation factor 1A domain containing, Eukaryotic translation initiation factor 1A domain-containing protein, eukaryotic translation initiation factor 1A, X chromosome1. They produced a theory to account for the evolution of the, Their DNA gel, at only 10 microns in width, is roughly the size of a, Chromatin is the combination of DNA and proteins that makes up the contents of the nucleus of a, Dictionary, Encyclopedia and Thesaurus - The Free Dictionary, the webmaster\'s page for free fun content, atypical glandular cells of undetermined significance. The primary function is to make and assimilate r-RNA that will be exported to the cytoplasm to translate m-RNA. By definition, eukaryotic cells are cells that contain a membrane-bound nucleus, a structural feature that is not present in bacterial or archaeal cells. In contrast, the smooth endoplasmic reticulum synthesizes phospholipids, steroids, and lipids, which are subsequently used in steroid based hormone synthesis. Other than the fact that vacuoles are somewhat larger than vesicles, there is a very subtle distinction between them: the membranes of vesicles can fuse with either the plasma membrane or other membrane systems within the cell. human) to survive. It is a double layered structure that encircles the nucleolus and the chromatin within the nuclear matrix. The region within the cristae is called the mitochondrial matrix and contains—among other things—enzymes of the TCA cycle. fats) are repelled from the outer surface. The matrix is the site at which the citric acid cycle (Krebs cycle - process in ATP formation) occurs. There are some terms that are specifically associated with changes at the cellular level that are common place in the medical field. The Golgi apparatus (also referred to as the Golgi complex or Golgi body) is structurally subdivided into cis and trans components. When viewed through an electron microscope, ribosomes appear either as clusters (polyribosomes) or single, tiny dots that float freely in the cytoplasm. Here, the death of the cell follows an external agent (i.e. There are some striking differences between animal and plant cells worth noting. We want you to understand these hypotheses and to be able to discuss the ideas presented in class, but we also want you to indulge your own curiosity and to begin thinking critically about these ideas yourself. This is an area where DNA that codes for ribosomal RNA (or tandem repeats) is found. The space between the two membranes is called the intermembrane space, and the space inside the inner membrane is called the mitochondrial matrix. Plant cells have a cell wall, chloroplasts and other specialized plastids, and a large central vacuole, whereas animal cells do not. It is crowded in there. In addition to the nucleus, eukaryotic cells are characterized by numerous membrane-bound organelles such as the endoplasmic reticulum, Golgi apparatus, chloroplasts, mitochondria, and others. The cytoplasm refers to the entire region of a cell between the plasma membrane and the nuclear envelope. As previously stated, the fundamental components of a cell are its organelles. The controlled movement of substance is done by protein channels and carrier proteins anchored in the plasma membrane that selectively or generally allow particular particles to enter and leave the cell. In humans, for example, the chromosome number is 23, while in fruit flies, it is 4. https://medical-dictionary.thefreedictionary.com/eukaryotic+cell. Figure 6. Such cells can be found lining the small intestine, the organ that absorbs nutrients from digested food.\nSoftware Engineering Tutorial Ppt, Small Black Spider With Pincers In Front, M1 Finance Aggressive Portfolio, Healthy Oatmeal Blueberry Muffins, Best Summary Generator, Sizes Of Alcohol Bottles, Blue Horizon Corporation, Equinox Meaning In Malayalam, Special K Chocolate Almond Nutrition Facts, Is King Arthur Evil In Once Upon A Time, Htc 5g Hub Ee, Imine Hydrolysis Ph, Kalderetang Baka Kapampangan, John 3:16 Meaning, Benefits Of Strategic Management With Examples, Three Threes Apple Sauce, Saint Mary\'s University Jobs, Ctia Exam Blueprint, Interpretation Of Ir Spectra Pdf, What Ethnicity Is Thompson, John Byrne Art For Sale, Sicilian Ricotta Pie, Kenstar Wonder Cool 40-litre Air Cooler, White Jade Necklace Meaning, Show Not Tell Sentences, How To Pronounce Exultation, Lysol Power Toilet Bowl Cleaner Ingredients, Online Arts And Crafts, Zaxby\'s Menu Nutrition, Mgk Crossfire Label, How To Make Spaghetti From Scratch, Sog Twitch Ii Black, Netgear Wn3000rp Uk, Dark Coral Color, Land O Lakes Whipped Cream Ingredients, Applying Walnut Oil To Wood, Is Ghana Airport Open For International Flights, Celtic Helmet For Sale, Crude Crossword Clue, Carpenter Jobs Near Me, James 4:6-8 Esv, 2pac Sad Song Lyrics, Silk Almond Creamer Caramel Review,']"	['<urn:uuid:4b1628ab-f68b-406c-99b6-f48189a94b53>', '<urn:uuid:0af2b3d9-efa2-4225-99c5-8658dfa07023>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T22:13:20.996306	8	90	2430
40	How has wealth concentration evolved in America's top households?	According to Federal Reserve economists, the richest 10 percent of households have consistently held over 50 percent of all wealth, with their share growing steadily over the past two decades. Currently, the top 1 percent of Americans own 31 percent of the country's wealth, while the top 10 percent own 70 percent. In contrast, the bottom half of Americans have very little wealth, owning only 1.2 percent of all wealth.	['What Is Income Inequality In America – The world #inequalityreport 2022 presents the most up to date & complete data on inequality worldwide: 💵 global wealth🌍 ecological inequality💰 income inequality since 1820♀ gender inequality, Income inequality: u.s. redistribution of income through the tax code, Inequality is brazil’s achilles heel, Policy implications from rising economic inequality, Wealth gap widens as pandemic deepens economic inequality, Ne chart that shows how much worse income inequality is in america than europe\nIncreased economic inequality over the past 40 years has redrawn the landscape of U.S. wealth and income, shifting many of its wealth gains into the hands of ever smaller groups of people and marginalizing members of vulnerable communities. This change, in turn, reduces income mobility and reveals differences in educational attainment and health outcomes between different income levels. The eight diagrams in the three sections below illustrate these results.\nThe first chart follows the share of total revenue that has fallen by the highest 1 percent of income recipients, followed by the highest 9 percent, the highest 40 percent (from 90 percent to 90 percent), and the lowest 50 percent. The share of income in the top 10 per cent peaked in the 1970s, but reached a new record – now 10 per cent of all income earners control about 38 per cent of national income. (See Figure 1.)\nWhat Is Income Inequality In America\nThe concentration of wealth grew faster than ever. According to a new study by Federal Reserve economists, the richest 10 percent of households have long held more than 50 percent of all wealth, but the share has grown steadily over the past two decades. Today, only one in 100 Americans owns 31 percent of the country’s wealth and the top 10 percent own 70 percent of all wealth. Meanwhile, half of the richest Americans have small fortunes: only 1.2 percent of all. (See Figure 2.)\nTrends In U.s. Income And Wealth Inequality\nTo some extent, these patterns are evident in other countries, suggesting that global effects may explain some of the growing inequality. But growth in the United States has been much stronger than in Europe. (See Figure 3.)\nUnderlying this widespread income gap in the United States is long-standing racial inequality, which leads to lower wages for people of color than white and male workers with similar levels of education, especially women of color. Not all of these gaps are related to discrimination, but a significant portion of it remains unclear and is generally discriminatory. (See Figure 4.)\nEconomic growth models that increase income inequality also make it difficult for people to rise to the income ladder. A study by Raj Chetty of Harvard University and its authors shows that absolute inequality between generations has decreased in the United States. Those born in 1940 were about 90 percent more likely to earn money than their parents when they were 30 years old. But the chance for people born in 1980 was only 50 percent. Chetty’s research shows that much of this decline is due to income disparities, not recent low growth. (See Figure 5.)\nIn fact, economic inequality and low economic mobility often go hand in hand. The following chart was first created by Miles Korak, an economist at the University of New York City, and has since been named the “Great Gatsby Curve.” This shows that there is a correlation between inequality and poor mobility between countries. (See Figure 6.)\nWealth Inequality In America: Key Facts & Figures\nAs economic inequality grows, the lives of the rich and the poor grow. This applies to many indicators, but gives two examples. First, the rich in the United States are more likely to graduate from college, and this gap has widened with inequality. A child in the upper quartile family is 45 percent more likely to graduate from college than a child in the lower quartile family, exacerbating the income mobility issues discussed above. (See Figure 7.)\nWealth also buys longevity. Studies by Raj Chetty and others show that the life expectancy of the poorest and richest Americans is 15 years for men and 10 years for women. It should be noted that the gap between men and women has widened slightly in just 13 years. (See Figure 8.)\nAt the heart of Equitable Growth’s work is the question of whether and how this increase in inequality will affect economic growth and stability. Therefore, we look at how economic inequality affects individuals and families in many ways and what policies can address these challenges. The share of the poorest side of Americans is declining, while the richest earn more. This is not happening in Europe.\nIncome inequality is a growing problem in the United States. The richest Americans have amassed a disproportionate amount of economic growth, and workers ’wages have not kept pace. The $ 1.5 trillion tax cut approved by Republicans since December is exacerbating the situation.\nU.s. Income Inequality: What Is It And How Is It Measured?\nOne chart in the 2018 Global Inequality Report shows the uniqueness of income disparities in the United States compared to other developed regions, particularly Western Europe. And the contradiction is very clear.\nFrom 1980 to 2016, the poorest half of the income of the U.S. population declined steadily, while the top percentage earned even more. The same trend is not observed in Europe.\nIn 1980, the highest 1 percent of revenue was about 10 percent in both Western Europe and the United States, but since then the two have grown sharply apart. In 2016, the highest 1 percent of revenue in Western Europe was about 12 percent, compared to 20 percent in the United States. In the United States, the lower 50 percent share of revenue fell from 20 percent in 1980 to 13 percent in 2016.\nThe economists behind the report, Facundo Alvaredo, Lucas Chansel, Thomas Picketti, Emmanuel Saez and Gabriel Zukman, explained what was happening:\nEight Graphs That Tell The Story Of U.s. Economic Inequality\nThe development of income inequality in the United States is largely due to inequality in mass education combined with a non-progressive tax system, despite high wage growth in the 1980s and a peak in capital income in the 21st century. In continental Europe, its tax development has declined less and the pay gap has been addressed through relatively favorable education and wage formation policies for low- and middle-income earners. Income inequalities between men and women have narrowed in both regions, but remain particularly strong at the forefront of the distribution.\nWealth inequality is, of course, a global problem, and among all countries, the United States is better than dozens. But it’s amazing to imagine how the richest and poorest Americans have been replaced in the last two decades.\nIn 2017, Dylan Matthews took note of a study by Pickett, Saez and Zuckman – three researchers in the Global Inequality Report – and also looked at how much the very rich have earned in recent decades instead of the middle and lower classes. From 1946 to 1980, the growth of the wages of the middle class and the poor exceeded that of the rich. But since then, the trend has changed dramatically in the opposite direction.\nThe economists behind the Global Inequality Report explain the gap between the United States and Europe, but there are a number of factors that have a broader impact on income disparities in the United States. However, the GOP tax law is unlikely to help. A law passed in December lowered the corporate tax rate from 35 percent to 21 percent, bringing disproportionate benefits to businesses and the wealthy.\nWealth Inequality In America Over Time: Key Statistics\nThe Center for Budgetary and Policy Priorities estimates that one-fifth of income recipients receive 70% of statutory benefits and up to 1% 34%. The new tax system for “transition” organizations organized into sole proprietorships, corporations, LLCs, or S companies means about $ 17 billion in tax savings for millionaires in 2018, in part due to tax savings, and generated about $ 700 billion for investors this year.\nMillions of people turn to the news to understand what’s going on. Our mission has never been more important than it is today: empowerment through understanding. The financial contribution of readers is an important part of supporting our resource-intensive work and freeing up all of our journalism. Consider participating to this day. Have asset differences in America changed over time to provide an updated analysis of U.S. asset differences? Here are the main statistics released on 2.12.2020.\nThe St. Louis FRS Center for Household Financial Stability looks at the relationship between wealth and different demographic characteristics: race or ethnic origin, education, age, or year of birth. We believe that this demographic target is more informative than looking at wealth across income limits because income can change (and change often) from year to year and Demographics are more stable.\nWe find that growing families are white, have a college degree, and / or are older. We find that families in difficulty have one or more of these characteristics: black or Hispanic; lack of a four-year college degree; and / or a minor.\nThe Distribution Of Wealth In The United States And Implications For A Net Worth Tax\nWe use data from the Federal Reserve Consumer Finance Survey and discuss trends in a series of charts and consider ways to build this security. All text values and dollar values are regulated by the Consumer Price Index (CPI-U) for all urban consumers. ) Until 2016\nNe Chart That Shows How Much Worse Income Inequality Is In America Than Europe, Inequality, Income Inequality In America, Inequality Is Killing The American Dream, The New Gilded Age: Income Inequality In The U.S. By State, Metropolitan Area, And County, The Distribution Of Wealth In The United States And Implications For A Net Worth Tax, Comparing Income Inequality In The United States And France, Infographic Of The Day: 15 Facts About America’s Income Inequality, Chart: The Top 10 Percent Own 70 Percent Of U.S. Wealth']	['<urn:uuid:d0cbd220-d3aa-492a-a168-9ea6567729bb>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T22:13:20.996306	9	70	1673
41	How does Fidus Law Chambers support employee mental health?	Fidus Law Chambers provides services of a coach for confidential consultation at the firm's expense, offers weekly yoga classes, and has established a gym to help employees release stress and work in a relaxed environment.	['How to promote diversity and inclusion in the IP workplace\nNext month, WTR will be publishing its annual Global Leaders supplement, in which a select number of the trademark elite reflect on their professional journeys and offer insights and guidance into career development, practice management and trademark industry trends.\nIn the 10th instalment of our series presenting key takeaways from these interviews, we take an in-depth look at the steps that two firms are taking to improve diversity in the workplace at a time when consumers are placing more value than ever in brand ethics and brand owners themselves are demanding greater representation from their counsel.\nAndrew Price, partner at Venable LLP, is a commercial and dynamic lawyer who understands the business of brands and how to take his clients to the next level in terms of international trademark protection. Having been a member of Venable’s Diversity Committee, we asked him what role diversity and inclusion initiatives play in the law firm environment and how firms can better support these. “Diversity and inclusion are linked to the success of not just a trademark practice but the whole law firm,” he says. “When professionals interact in an open and equal way, diversity and inclusion are advanced and a greater sense of belonging is created.”\nFor Price, trademark practitioners are particularly well poised to promote this level of interaction. “Because of the global character of the trademark profession and the symbiotic nature of foreign relationships in this field, trademark attorneys are natural ambassadors for diversity and inclusion within firms,” he says. What is more, the Venable IP team is a shining example of this. “The trademark group has led by example, developing diverse and inclusive leadership and working teams; fostering an extra-collaborative working environment; publicising team successes to the firm; playing an active part in social events that showcase the varied interests and personalities of team members; and demonstrating commitment to key issues such as unconscious bias, by attending firm presentations and openly discussing the issues raised,” Price cheers.\nRome was not built in a day though, and for many decision makers eager to make a difference, there can be a number of barriers to progress. There is no disputing the fact that greater diversity leads to greater creativity, and people will work best in an inclusive environment where they are free to be themselves – and seek support if and when they need it. But fear of hitting the wrong note or taking a misguided step can hold entire firms back from introducing genuine change.\nShwetasree Majumder, managing partner at Fidus Law Chambers, has not only made a huge difference for her clients as a compellingly dynamic litigator, she has positively impacted all stakeholders in the Indian IP market through her mentorship, training and IP association activities. “Diversity and inclusion are the cornerstones of the policies at our firm,” she says. “Although the Indian law against sexual harassment in the workplace is aimed at protecting women, our firm’s internal policy is gender neutral. [In addition,] All members of our firm, no matter whether they are part time, full time or temporary, are treated fairly and with respect.”\nTo highlight the concrete steps that firms can take to achieve this, we asked Majumder what specific initiatives Fidus has undertaken that set it apart. One way that the firm goes about creating an inclusive workplace is by having a mature and transparent approach to mental health. “We are committed to issues of mental health and use the services of a coach who is available for consultation by individual members of the team under a strict obligation of confidentiality, at the firm’s expense,” she states. “We place a premium on the emotional intelligence and the happiness quotient of our team. Our weekly yoga classes and newly established gym are all geared towards helping colleagues release stress and work in a relaxed and happy environment.”\nAnother way that law firms can achieve this is through formal and informal mentorship programmes. “We recently put in place a buddy system, where each member of the team looks out for the wellbeing of their designated buddy in the firm,” says Majumder.\nAs well as supporting employees on an individual level, Fidus also promotes inclusivity through public events and team-building away days. “We observed Pride Day on 29 June 2020 to reinforce the message that our workplace is a safe place for people of all sexual orientations,” Majumder notes. “We [also] attend a yearly retreat with an artiste, who uses tools from the performing arts to help us break down barriers and connect as a team.”\nAll of this comes at a time when companies around the world are being held to a higher standard than ever before – and the need for corporate social responsibility (CSR) has never been greater. “The covid-19 pandemic has made CSR more important than ever,” says Ayala Deutsch, 2020 INTA president, executive vice president and deputy general counsel at NBA Properties Inc, and another of this year’s WTR Global Leaders. As well as firms supporting employees through times of severe stress and upheaval, consumers are also placing increasing pressure on brands – and by extension, their representatives – to be seen to be doing ‘the right thing’. “This global crisis will have a long-lasting impact on brands and their role in society, as well as consumer expectations,” says Deutsch. “Brands need to assume a higher degree of social responsibility now and post-pandemic. The bar has been raised for ethical standards.”\nEchoing Price’s point, the very nature of intellectual property places IP practitioners at the forefront of this effort. “Intellectual property, and trademarks specifically, are the foundation for trust between brands and consumers,” continues Deutsch. “According to the ‘2019 Edelman Trust Barometer – In Brands We Trust’, 69% of consumers say that a brand’s impact on society is a key reason for trusting that brand. IP professionals protect that trust. In our recent Brands and CSR Survey Report, INTA members overwhelmingly agreed that treating CSR as an operational priority ‘constitutes good economic practice and will benefit a company and/or brand’.”\nUltimately, IP practices have an important role to play when it comes to establishing politically aware, ethically accountable brands – and do this, they need to lead by example. “To sum up,” says Price, “we all share the responsibility to further diversity and inclusion – and we all share the rewards”.\nFor further information on Venable’s diversity initiatives, see “Stronger together: building diversity and inclusion through partnerships”, co-authored by Andrew Price and published in WTR 81.\nWTR Global Leaders 2020 will be published in September. Other articles in this insight series include:\n- “Become business partners” and “prepare for the long game”: six tips for stakeholder buy-in and resource management? Read more here.\n- The six signs of an accomplished leader. Read more here.\n- “It is part salesmanship, part showmanship”: how to build a sturdy practice. Read more here.\n- Tips for obtaining well-known status in China. Read more here.\n- From classroom to boardroom: why practitioners must do more to raise IP awareness. Read more here.\n- “Never relax your guard” and “tailor your strategy”: the biggest challenges facing brands and how to overcome them. Read more here.\n- Five ways to beat the odds and protect your brand in Asia. Read more here.\n- Stakeholder buy-in, new AI tools and improved compensation: how to make change happen. Read more here.\n- “Brands must adapt or be eaten”: WTR best in class on fighting domain name abuse and online infringement. Read more here.']	['<urn:uuid:7d69719d-f0ca-4472-8fcc-a884d8e8a063>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T22:13:20.996306	9	35	1261
42	Does cracking knuckles cause arthritis and what helps prevent joint problems as we age?	Knuckle cracking does not cause arthritis - scientific studies comparing X-rays of people who crack their knuckles versus those who don't show no difference in arthritis rates. However, as we age, we need to protect our joints through strength training. After age 30, we start losing muscle mass, which forces joints to do more work and risks damage. Regular strength training strengthens muscles around joints, protects them from damage, and reduces arthritic pain. It's never too late to start - even 90-year-olds have shown significant improvements in strength and mobility through weight training.	['Do you wince when people crack their knuckles? Or — gasp — are *you* the knuckle cracker in the room everyone stares at?\nIf you are a knuckle cracker, has anyone (say, your mom) ever tried to get you to stop by warning you that your annoying hand habit will lead to osteoarthritis (OA)?\nWell — sorry, mom — there’s no evidence to back up the claim that joint cracking can lead to arthritis.\nHere, we address the finer points of cracking joints:\nWhat Causes the Sound When You Crack Your Knuckles?\nYour joints are surrounded by a capsule, or sac. The capsule contains a liquid called synovial fluid, which acts as a lubricant and prevents friction so the joints can move around smoothly. Synovial fluid contains gas bubbles.\nWhen you crack your knuckles, you stretch the space between your finger joints, which causes the bubbles to burst and create that distinctive popping sound.\nThe reason you can’t crack the same knuckle or joint twice in a row is because it takes some time for the gas bubbles to accumulate again in the joint. (If you want an in-depth explanation of the physiology of knuckle cracking, check out this journal article in Scientific Reports.)\nSo, Can Cracking Your Knuckles Cause Arthritis?\nNo — and there’s ample scientific research to back up why not. A 2011 study published in the Journal of the American Board of Family Medicine looked at 215 people between ages 50 and 89 who had had an X-ray of their right hand during the previous five years. The researchers found a similar incidence of knuckle OA in any one joint among people who said they cracked their knuckles and those who didn’t, irrespective of how frequently or for how many years the person cracked.\nAnother study, published in a 1998 issue of the journal Arthritis and Rheumatology, was done by a single doctor who experimented on his own hands. (Talk about dedicating your life to science.) Over his lifetime, he cracked the knuckles of one hand, but not the other. After decades of this behavior, he took X-rays and found that both hands had the exact same incidence of arthritis.\nAre There Any Risks of Joint Cracking?\nThere has been some research indicating that longtime knuckle crackers may experience hand swelling and a reduced hand grip over time, but there is still no evidence that cracking your knuckles causes arthritis.\nHowever, if there’s pain that accompanies your knuckle cracking, it could mean that there are other problems with the structure of the joint, according to WebMD, such as loose cartilage or injured ligaments. And some people with arthritis, bursitis, or tendinitis might notice cracking sounds because of the snapping of their swollen tissues.\nIf you feel pain when your knuckles crack, you should see a doctor to figure out why.\nWhat Does Cause Hand Arthritis?\nOsteoarthritis occurs when cartilage — a tough slippery substance that coats the ends of the joints — breaks down. Over time, as the cartilage continues to wear away, the bones rub together, which makes movement difficult and painful. Causes of arthritis include wear-and-tear that occurs with age, repetitive hand motions (like typing), prior injuries, and possibly a genetic predisposition. There is not much you can do to prevent arthritis in the hands.\nSo, if you want to crack your knuckles, crack away — and tell your mom that she has to come up with a better reason than arthritis to get you to stop.', '“She girds herself with strength, and strengthens her arms“. Proverbs 31:17\nAs we have seen in a previous post, a sedentary lifestyle is a health threat that worsens many medical conditions and leads to premature death in both men and women. Hopefully, this fact motivates you to start moving a little more. I have only become a regular exerciser in the last few years. I never really did it before, maybe off and on, because I just don’t like to exercise.😰 I now realize I was foolish in that respect. According to Vonda Wright, (an orthopedic surgeon and director of a program for senior athletes) it is a myth that we are destined to become frail as we age. However, being inactive will make it more likely to happen. Walking and doing cardio activities are great to get our hearts pumping and our bodies moving, but that’s not enough! We need to be strength training as well. Miriam Nelson states that adding strength to your cardio should be non-negotiable, especially if you are over 40.\nAccording to Marilyn Diamond, in her book Young for Life, after age 30, we start to lose muscle mass. This begins even earlier if we have a sedentary lifestyle. This is called sarcopenia. Dr. Wright states that we lose 15% of muscle mass each decade between 50-70 and 30% after age 70.\nThe loss of muscle is one factor that leads to premature aging. We feel weaker, our muscles stiffen, and we don’t have the endurance we once had. When our daily tasks become more difficult, we do less. Our bodies adjust to the lower activity and we become weaker.\nReduced muscle mass causes the joints to do most of the work risking damage to the joint.\nOur metabolism slows down, giving us less energy and more weight. We replace muscle with fat.\nOur muscles also become infiltrated with fat between our muscle fibers, like the marbling on beef. Nice on a steak, but not on us.\nOur posture becomes poor, because of weak neck and trunk muscles.\nWeaker leg muscles cause poorer balance, leading to falls.\nAs we age we also lose bone density, leading to osteoporosis. After age 40 women lose bone twice as fast as men. The most critical time is the first five years after menopause. Oh, by the way, men lose bone density too. Just not as fast as menopausal women.\nThe biggest influence on weight gain in menopause in not hormonal, it is the loss of muscle mass. According to Pamela Peeke, for every pound of muscle lost through disuse, a woman loses the ability to burn 50 calories per day. She state that a 1992 study showed that women who do not strength train lose 7 pounds of muscle every ten years and a reduced metabolism equal to 350 calories per day. Another study showed that this is NOT observed in women who exercise regularly.\nCardio and walking won’t make us strong. We need more.\nBENEFITS OF STRENGTH TRAINING\n- become stronger\n- improve balance\n- more energy\n- lose weight\n- boost metabolism\n- increased mobility\n- beneficial for high blood pressure and cardiovascular disease\n- increase bone density. Not many participating in weight bearing sports get osteoporosis\n- aerobic and strengthening exercises are effective for chronic arthritis\n- easier to move\n- lose inches not pounds\n- protects your joints- by strengthening muscles around them\n- make aerobic exercise more enjoyable, because stronger muscles make it easier to do\n- lift depression\n- sleep better\n- reduced risk of bone fractures\n- reduction of symptom severity in fibromyalgia\n- reduces arthritic knee pain\n- improved glucose control\n- increased self confidence\n- feel younger\nBUT I”M TOO OLD OR SICK TO START…\nNo you’re not.\nIt’s never too late to start exercising. Age doesn’t matter, you can improve at any age. Consider this- physical therapist, Marilyn Moffat, states in one study, men and woman over the age 90 achieved a 200% increase in strength after only 6 weeks on an exercise plan. The weaker you are the more you need it. You are not doomed to be weak and dependent as you age.\nA study by William Evans and Maria Fitatarone took 10 nursing home patients between the ages of 86-96 with a least two serious chronic diseases such as heart disease and diabetes. Most of them needed walkers or canes and some couldn’t get up from a chair without help. They were taken three times a week to lift weights (at an appropriate starting point and slowly increasing) for 8 weeks. After 8 weeks they increased their strength by 175%. Their walking speed improved by almost 50%. Two didn’t even need their canes anymore. Cool.😎\nRead that paragraph again. I don’t know who you are, but I would venture to say you are healthier than a 90 year old in a nursing home, no?\nResearch has shown that men and women with health conditions are often helped the most by exercise programs that include strength training. It is beneficial even if you are not in great health.\nAt age 95, Jack LaLanne, who was considered a crackpot for his views on fitness, stated that while he wasn’t as strong or nimble as his younger days, he didn’t feel any different than he did at 25. He was still working out everyday at 95. He was mocked for the idea that people would pay money to go exercise. His ideas aren’t so weird anymore. 😕\nYou don’t need to spend lots of money or even join a gym. Of course, you can if you want to. This is all my equipment above, just an assortment of dumbbells. You can do so much with them. I also have some DVDs which give me some variety to keep it interesting. As always, there are fun activities that build muscle as well, such as rock climbing, heavy gardening, certain yoga moves, anything that engages your muscles in a strenuous way.\nThe amount of weight to start with is a trial and error process. Use weights that are challenging. You might even have to start with just your body weight. Lift 70-80% of your maximum weight. Your maximum is the weight you can only do 8 repetitions of. Lifting more than that risks injury and less than that is not effective. If you are over 50, start with a weight that allows 20 reps. If it is not challenging, you will not receive any benefit from it. You need to graduate beyond soup cans.\nMuscles work in pairs, you must work both parts of the set, to avoid injury. Don’t work your biceps without working your triceps.\nLower your weights slowly for most development. We have fast and slow muscle fibers, for strength and endurance. For slow-twitch muscle fibers do lower weight with more repetitions. Abs are slow-twitch. For fast-twitch muscles use higher weight with less reps. Thighs are fast-twitch.\nUse safety measures to avoid injury. I discuss safety here.\nBe sure to check with your doctor before you start any exercise program. If you have an injury or pain, don’t avoid exercising, find out from a physical therapist how to exercise differently. Also keep in mind that soreness and pain aren’t same. You may experience soreness, but you shouldn’t experience pain.']	['<urn:uuid:91537e86-4582-4a4a-a6b5-f983d27a2208>', '<urn:uuid:261e7c2e-b928-4c88-be2a-3eefa8ec6b81>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T22:13:20.996306	14	93	1804
43	trying to understand quantum computers future and limitations what makes them break down and what solutions exist	Quantum computers face significant limitations due to noise and interference, which cause decoherence - the degradation of information in qubits. While typical transistors in classical computers can operate for billions of years without faults, quantum bits become randomized in about one-thousandth of a second. This severely limits the number of operations that can be performed before fatal errors occur. To address this, two main solutions are being developed: Quantum Error Correction (QEC) and quantum firmware. QEC distributes quantum information across multiple supporting qubits to protect it, but requires about 1000 physical qubits for each logical qubit. Quantum firmware complements QEC by stabilizing qubits against noise without extra qubits, through dynamic stabilization techniques. When combined, these approaches offer a pathway to building large-scale quantum computers, though we're still far from having useful devices. Current expectations suggest it may take more than a decade before quantum computers can influence practical applications like drug design or battery development.	"['People of ACM - Scott Aaronson\nJune 1, 2021\nWhat has surprised you most about the development of quantum computing since you began working in the field?\nWhat’s surprised me most is the extent to which, even before we have useful devices, quantum computing has shifted from an academic basic research enterprise to a field that’s awash in venture capital and large investments from governments and corporations. On the positive side, the new funding gives the field a much better shot at building truly scalable devices, which won’t be cheap if it can be done at all! On the negative side, people who make such big investments do so in the hope of a return, and the new environment has put enormous pressure on researchers to paint a picture of the near-term applications of quantum computers that (in my view) is often wildly overoptimistic or false. I, too, hope I‘ll live to see quantum computers influence the design of drugs, batteries, solar cells, and so forth, but I’d be surprised to see any of that in the next decade!\nWill you tell us a little about the concept of quantum supremacy, and why this concept has been important to both the theoretical underpinnings of the field, as well as the ongoing efforts to build a quantum device?\n“Quantum supremacy” just means the use of a quantum computer to perform some benchmark task that we have reason to believe is extremely hard for any existing classical computer. Much like the Wright brothers’ airplane, the task doesn’t need to be useful; it just needs to prove a point. We do, however, insist that the task have a clear mathematical definition (thus, simply pointing to some physical system and challenging people to predict its behavior doesn’t count). In some sense, the concept of quantum supremacy goes back to the very beginning of the field, to Feynman and Deutsch in the 1980s: beating a classical computer is a quantum computer’s entire reason for existence! Since 1994, the gold standard for quantum supremacy has been to use Shor’s algorithm, say to factor some thousand-digit number. That would be almost impossible to argue with (note that alleged prime factors are easy to check). But alas, we’re still far from building any quantum computer that can do such things; and as a separate issue, there’s no clear consensus that factoring really is hard for classical computers.\nThe modern push for quantum supremacy started a decade ago, when theoretical work by me and others indicated that, if you just wanted to demonstrate a clear quantum speedup, then there were huge benefits to switching attention from problems with a single right answer (like factoring) to so-called ""sampling problems,"" where the goal is just to sample from some prescribed probability distribution. One benefit is that you can then base your belief in the hardness of classical spoofing on some of the most solid conjectures in computer science (such as the “non-collapse of the polynomial hierarchy”). A second benefit is that, to achieve quantum supremacy for these sampling tasks, you don’t necessarily need a full, error-corrected quantum computer with millions of physical qubits and gates—something we’re still quite far from. Rather, just 50-100 noisy qubits and a few hundred noisy operations on them could be enough. Around 2013, that realization jump-started an experimental quest actually to demonstrate quantum supremacy based on sampling problems—and of course, the quest finally started to bear fruit in 2019.\nYou should think of the existing supremacy experiments as proofs-of-principle: even though they fall short of scalability and don’t yet solve useful problems, they’ve successfully illustrated how a programmable system can harness the enormous dimension of quantum states for computational advantage. These are triumphs for the engineering groups that achieved them, and I’m proud to have played some role in causing them to happen.\nTeams from Google and China have recently claimed to have developed devices that achieve quantum supremacy; how would the computer science community verify these claims?\nThe computer science community has been heavily involved in the effort to verify these claims since the claims came out! The central question is how hard it is to spoof the behavior of these devices using a classical computer—which is clearly a question for classical CS at least as much as for physics. My students and I have given complexity-theoretic evidence that quantum supremacy experiments are sampling probability distributions that, in least in principle, should take exponential time to sample classically. But not only is this evidence less airtight than we’d like it to be, it’s asymptotic (rather than specialized to specific numbers of qubits like 50 or 60), and doesn’t adequately account for noise and many other practical issues.\nOver the past couple of years, there were partial rebuttals of Google’s quantum supremacy claim, by IBM, Alibaba, and others, which showed how to pass Google’s test for supremacy (what they called the “Linear Cross-Entropy Benchmark”), with a classical computer, orders of magnitude more quickly than Google had thought possible, although the quantum computer still retains some advantage. With the quantum supremacy claim from USTC in China, people are working on classical spoofing strategies as we speak—especially, strategies that would take advantage of imperfections in the experiment, such as the high rate of photon loss. Having said that, one can sometimes evade these spoofing attempts just by making small tweaks to the test being applied to the quantum computer’s outputs—to say nothing of improving the quantum computer, say by adding more qubits or increasing their fidelity! So I expect there to be cat-and-mouse claims and counterclaims for several more years. Just like in applied cryptography, we’ll only be truly confident about quantum supremacy once some particular claim has survived serious attempts by skeptics to shoot it down for a long time.\nWhat is the primary focus of your research right now?\nSadly, I haven’t been getting much research done during Covid! In general, though, I continue to be interested in the fundamental capabilities and limitations of quantum computers—so, the relationships among quantum complexity classes, the power of quantum states as proofs or advice or unclonable currency, and much more.\nI’m also interested in quantum computing questions that come from the study of quantum gravity and black hole information problem, which has been an exciting direction for the past decade, spearheaded by physicists like Lenny Susskind. But perhaps my most urgent focus for the next few years will be to understand better the capabilities of noisy, non-error-corrected quantum computers with a few hundred qubits. For example, can they beat classical computers at some task for which it’s easy for classical computers to verify the answer? What about generating certified random bits, or other tasks with possible real-world applications?\nWhat advice would you offer a college student who is interested in working in the quantum computing field?\nMajor in CS, Physics, Math, EE, or some combination thereof. Learn as much math as you can—especially linear algebra and probability; finite groups and fields and representations are also helpful. Study classical algorithms and complexity theory too. Surprisingly, quantum mechanics (as taught in most physics departments) is not essential if you’re coming at the field from a Math/CS perspective, though of course it is if you’re coming from Physics, Engineering, or Chemistry.\nIn any case, if your university offers a course in quantum computing and information, then take it, obviously; then, if you do well, talk to the instructor or others afterward about research opportunities. To go further in the subject, you’ll probably want a PhD. In the meantime, there are also lots of great resources for self-study: dozens of textbooks (of which the most famous remains Quantum Computing and Quantum Information by Michael Nielsen and Isaac Chuang); MOOCs; free online lecture notes by Umesh Vazirani, John Preskill, myself, and many others; and virtually the entire literature of the field just a few clicks away on arXiv.\nScott Aaronson is a Professor at the University of Texas at Austin. His research interests include theoretical computer science, the capabilities and limits of quantum computers, and computational complexity theory. Through several influential papers, Aaronson showed how results from computational complexity theory can provide new insights into the laws of quantum physics, and brought clarity to what quantum computers will, and will not, be able to do. He is also credited (with John Preskill and others) in helping develop the concept of quantum supremacy, which denotes the milestone that is achieved when a quantum device can solve a problem that no classical computer can solve in a reasonable amount of time.\nAaronson is also recognized as an effective spokesperson and educator on quantum computing. His publications include the respected textbook, Quantum Computing Since Democritus. He was named the recipient of the 2020 ACM Prize in Computing for groundbreaking contributions to the field of quantum computing.', 'What is Quantum Error Correction?\nThe journey to realizing functional quantum computers will be long and it\'s a path that Q-CTRL is committed to making as easy as possible for you. And by easy, we mean less difficult. Building a universal quantum computer with millions of entangled, coherent quantum bits running complex algorithms is not going to be simple or straightforward.\nFrom noise to error in quantum computing\nHere we’ll get to the heart of why quantum computing is really hard: noise and error.\n“Noise” describes all of the things that cause interference in a quantum computer. Just like a mobile phone call can suffer interference leading it to break up, a quantum computer is susceptible to interference from all sorts of sources, like electromagnetic signals coming from WiFi or disturbances in the Earth’s magnetic field. When qubits in a quantum computer are exposed to this kind of noise, the information in them gets degraded just the way sound quality is degraded by interference on a call. This is known as decoherence.\nCompared with standard computers, quantum computers are extremely susceptible to noise. A typical transistor in a microprocessor can run for about a billion years at a billion operations per second, without ever suffering a hardware fault due to any form of interference. By contrast, typical quantum bits become randomized in about one one-thousandth of a second. That’s a huge difference.\nNow consider a quantum algorithm, executing many operations across a large number of qubits. Noise causes the information in the qubits to become randomized - and this leads to errors in our algorithm. The greater the influence of noise, the shorter the algorithm that can be run before it suffers an error and outputs an incorrect or even useless result. Right now, instead of the trillions of operations that might be needed to run a full-fledged quantum algorithm, we can typically only perform dozens before noise causes a fatal error.\nQuantum Error Correction\nSo what do we do about this?\nCompanies building quantum computers like IBM and Google have highlighted that their roadmaps include the use of “Quantum Error Correction” as they scale to machines with 1000 or more qubits.\nQuantum Error Correction - or QEC for short - is an algorithm known to identify and fix errors in quantum computers. It’s able to draw from validated mathematical approaches used to engineer special “radiation hardened” classical microprocessors deployed in space or other extreme environments where errors are much more likely to occur. QEC is the source of much of the great promise supporting our community\'s aspirations for quantum computing at-scale.\nIn QEC quantum information stored in a single qubit is distributed across other supporting qubits; we say that this information is ""encoded"" in a logical quantum bit. This procedure protects the integrity of the original quantum information even while the quantum processor runs - but at a cost in terms of how many qubits are required. Overall, the worse your noise is, the more qubits you need.\nDepending on the nature of the hardware and the type of algorithm you choose to run, the ratio between the number of physical qubits you need to support a single logical qubit varies - but current estimates put it at about 1000 to one. That\'s huge. Today’s machines are nowhere near capable of getting benefits from this kind of Quantum Error Correction.\nQEC has seen many partial demonstrations in laboratories around the world - first steps making clear it’s a viable approach. But in general the enormous resource overhead leads to things getting worse when we try to implement QEC. Right now there is a global research effort underway trying to cross the “break even” point where it’s actually advantageous to use QEC relative to the many resources it consumes.\nHow do we get there?\nQuantum firmware and Quantum Error Correction\nThis is where Q-CTRL comes in. We add something extra - quantum firmware - which can stabilize the qubits against noise and decoherence without the need for extra qubits. Quantum firmware serves as a complement to QEC, such that in combination we can accelerate the pathway to useful quantum computers.\nOne kind of quantum firmware works by something called dynamic stabilization - if you constantly rotate your qubits in just the right way you can make them effectively immune to the noise which would normally randomize them. It sounds a bit like magic, but believe it or not, similar techniques are already used to stabilize the memory in your computer.\nThe techniques are easy to implement and the benefits can be huge - our own experiments have demonstrated more than 10X improvements in cloud quantum computers!\nIn the context of QEC, quantum firmware actually reduces the number of qubits required to perform error correction. Exactly how is a complex story, but in short, quantum firmware reduces the likelihood of error during each operation on a quantum computer. Better yet, quantum firmware easily eliminates certain kinds of errors that are really difficult for QEC, and actually transforms the characteristics of the remaining errors to make them more compatible with QEC. Win-win!\nLooking to the future we see that a holistic approach to dealing with noise and errors in quantum computers is essential. Quantum Error Correction is a core part of the story, and combined with performance-boosting quantum firmware we see a clear pathway to the future of large-scale quantum computers.\nFor a deeper dive into the intersection of QEC and Quantum Firmware.\nDiscover how you can use Fire Opal, our effective error suppression technology for real quantum computers, to improve the success of quantum algorithms without any user intervention, hardware knowledge, or configuration.\nLearn how Quantum Error Correction can enable the quantum computing revolution and the vital role it plays in the future of large-scale quantum computers.\nQuantum firmware bridges the gap between mathematical abstractions of quantum algorithms and practical physical manipulation of imperfect quantum hardware.\nLearn how about the current ""noisy"" era of quantum computing and what it stands to deliver. Uncover what quantum computing in the NISQ era looks like today.\nLearn the fundamentals of how to build quantum algorithms for quantum computing - starting with quantum circuits and logic through to how they\'re measured.\nLearn the fundamentals of quantum physics for developing quantum computers and explore insights into the basic principles driving quantum innovation.\nLearn how the fragility of quantum hardware lets us detect the undetectable and discover the principles and potential applications for quantum sensing.\nLearn about the foundations of quantum computing and discover the principles for how this technology will power a transformative information age.\nTake the next step on your journey with short articles to help you understand how quantum computing and sensing will transform the world']"	['<urn:uuid:4925f4c7-2bfe-482c-996b-56b98de7ecf3>', '<urn:uuid:c0355044-ecac-4637-a513-85de5dd13ef8>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T22:13:20.996306	17	155	2591
44	how many screws hold canon 35mm filter barrel	The filter barrel is held by 6 screws, which are long, strong, and deeply threaded. Each screw passes through a brass spacer with a spring around it that maintains tension.	['Canon 200-400 Pre-Production Sample Lens Review from Joshua Holko on Vimeo. Joshua Holko got his…\nWe got a little time this week, so we used it to take apart the new Canon 35mm Mk II f/1.4. Those of you who like to follow along from home with your own lens go get your #2 JIS screwdriver and a spanner wrench and let’s get to work!\nFirst the makeup decal on the front of the lens has to be peeled off.\nNext we use a spanner wrench to take off the front plastic ring.\nIn most lenses rings like this hold the front element in place, but in this case, it seems the entire purpose of this ring is to improve weather resistance. (I refuse to use the term weather seal, and always will. No lens is weather sealed). It fits tightly around the glass and into the front barrel of the lens, and it has a rubber gasket for further sealing internally.\nWith the weather resisting ring removed, we can looking into the barrel front and see two sets of 6 screws. The brass ones that are open to view hold on the filter barrel. The partially covered set appear to be holding the front group in place.\nWe were impressed with 6 screws to hold onto the filter barrel; most lenses use 3. We were even more impressed when we removed them. Each of the 6 screws is long, strong, and deeply threaded. Not the usual tiny screw assigned to a task like this.\nAnd when the screw was removed, we found that each hole contained a brass reinforcing spacer with a spring around it. So basically each of the 6 screws passes through the brass spacer and screws into the front barrel, with a spring maintaining tension. This is an expensive way to do things and obviously serves a purpose. It may be to maintain even tension on the focusing ring (which is right below the filter barrel), to provide a more even stress distribution, or probably is for something else entirely. But just because I don’t know what it’s for doesn’t mean I can’t appreciate the careful attention to detail taken here.\nWith the screws taken out the filter ring barrel comes right off. This will be a quick, simple replacement when the inevitable broken filter ring occurs.\nWith the filter barrel off, we can see that the front group is attached with 6 screws through 6 thick plastic tabs. It’s not visible in the picture, but there’s also a copious amount of loctite around the screws and silicone glue under the tabs.\nAt this point, Aaron decided that the amount of glue on that element meant he might be doing some prying to get it out. Rather than do that blind, he decided to do some back-end disassembly first so we flipped the lens over and removed the bayonet mount.\nThere is the usual thick Canon rear weather gasket around the bayonet.\nUnderneath the bayonet is a black plastic spacing ring. The fact that it’s numbered suggests it comes in different thicknesses to adjust infinity focusing distance. Canon tends to use various-thickness bayonets or spacers like this rather than shims.\nA fairly standard Canon PCB with lots of flex connections is underneath the spacing ring.\nThe rear glass does make a nice Selfie-Cam for Aaron.\nWith the PCB off, the screws holding the rear barrel in place are evident – again, multiple robust screws are used, not the minimal 3 screws we usually see.\nRemoving those let us slide the rear barrel off and reveals the more interesting stuff; primarily at this point the rear group and focusing motor.\nA closer look shows a couple of nice details. First there are two more layers of rubber felt sealing gaskets (red lines) between the rear of the lens and the AF Motor (green brackets). There’s also a very, very robust eccentric collar set (blue arrow) used to optically adjust the rear group. We consider thick nylon collars robust, brass collars very robust, but these massive heavy collars with a center locking screw are beyond anything we’ve seen outside of super telephoto lenses and the 70-200 f/2.8 IS II.\nYou can also see that once adjusted into position, Canon has sealed the adjustment collars in plastic cement to prevent them from moving. For you that means they’re not likely to get moved during the life of the lens. For us that means we’ll have to pick the plastic cement out when we do adjustments. That’s OK, the pink cement isn’t too hard to get out.\nRather than take out those adjustment collars we went back to the front of the lens and removed the focusing rubber. Underneath that the ring is completely sealed in tough tape, covering all the holes (you’d be amazed how many supposedly ‘weather resistant’ lenses have huge gaping holes under the rubber).\nWith the tape off, the three plastic keys (white and round in the picture below) that hold the focus ring in place are exposed.\nThese are large and require a special tool to remove properly, but once they’re out the focus ring slides off the front of the lens. Notice the part of the lens in Aaron’s left hand still contains all of the optical elements. Basically, all we’ve done so far is remove the mount and coverings.\nWith the focus ring off, we can see another set of robust optical adjustment collars that adjust group 2. (Later experimentation hinted that these were primarily centering collars, with the rear collars primarily adjusting tilt.)\nTurning the focus mechanism exposed the stays that the inner barrel rotates on. They aren’t the usual nylon bushings over screws, they are very large brass posts held in place by oversize screws. Again it’s something we’d expect to find in a large telephoto zoom, not a standard range prime.\nSince we discovered that group 2 and not group 1 was an adjustable group, we felt much more comfortable with prying up the glue and removing Group 1. At this point we weren’t at all surprised to find that not only was group 1 held in place by 6 screws (3 is the norm) but that they were some big, long screws at that.\nWith the screws removed and a little heat applied to the glue, we were able to pry group one up and remove it. You can see that the top of the front group has a spanner ring holding the first element in place, so a scratched front element can be replaced separately without having to replace the entire front group. That’s a good thing. While I don’t know the cost of a front element yet, front groups like this are expensive, costing as much as $700. Being able to replace just the front element is a good thing, it will be less expensive and the front group doesn’t have to be removed to do it.\nHere’s a view inside the barrel with the front group removed. There are two points to make here. The first is minor, and hard to see from the photo, but the front of group 2 is really different and unusual. It’s a concave element with a large band of ground glass around the edge. It’s not important; just different, looks cool, and when one of these comes back dead from water damage I’m making a Christmas ornament out of it.\nThe more important part the thick gray barrel of this lens. That’s heavy gauge metal. All the pieces of lenses we’ve removed attach to this heavy metal center barrel. This is unusual, but it’s so logical I want to weep with joy just for having seen it. I have rolled my eyes for years when people say a lens is “Built like a tank” because it has a heavy metal shell. Then we open it up and see the insides are tiny little screws and weak nylon collars set in thin sheet metal helicoids. That kind of ‘built like a tank’ is probably useful if you want your lens to stop a bullet, but doesn’t make the lens reliable.\nThis is my kind of built like a tank. There is a flexible polycarbonate shell over a very solid metal core with really heavy-duty rollers, screws, and bearings. That’s a logical way to build things; make the core the strongest part, not the shell. It sounds so simple, but like I said, this is the first time we’ve ever seen this kind of construction in a prime lens of standard focal length. We take apart A LOT of lenses (we passed 20,000 in-house repairs some time ago) and this is the most impressively built prime I’ve seen. This is an engineer’s lens.\nAt this point we had to decide whether we wanted to take apart the optical core of the lens. Doing so meant removing the large adjustment collars for the front group, and having to readjust the lens optically during reassembly. I spoke at length about why we should just stop here and put things back together. Aaron took out the collars and large brass posts we showed you earlier while I was talking, so I guess he didn’t hear me. After that the front half of the optics came right out of the central barrel (it’s being held in a rubber cone which is not part of the lens in the picture below).\nAnd rotating it to line things up, the rear group could slide out from the focusing motor and barrel as well.\nWith the USM motor and barrel off we can see the focusing helicoids and the heavy metal rollers that move the focusing elements within the helicoid. In almost every lens, these would be small nylon washers over a screw, not the relatively huge metal rollers we see in this lens.\nWhen you look close up you see these aren’t just sliding posts, there are actually tiny ball bearings inside them. There’s also a spring tensioning system around one of the rollers. I keep repeating myself, but by this point I was really rather awestruck by the amount of careful over-engineering that went into making this lens. Nobody, and I do mean nobody, else is engineering lens mechanics like the newer Canon lenses.\nOf course, we took one the rollers out to get a better look at it. This may look small to you, but the nylon collars used in other lenses are at most 1/4 of this size, aren’t metal, and don’t roll, they just slide.\nI’ll close with another view of the internal barrels in extended position, this time also showing the aperture control motor.\nOK, this concludes the disassembly portion of our program. Those of you who have been disassembling your own lenses at home, just reverse the steps and it will go right back together, probably. Since someone always freaks out when we do a teardown and thinks we’ve made a sacrifice to the lens gods, I will mention that the lens was fully reassembled, Aaron optically adjusted it back to perfection, it’s been completely retested by our techs, and is working fine. Yes, it’s back in the rental fleet and no, it’s not going to have problems. This is what we do all day long.\nI’m sure you can tell we’re impressed with the Canon 35mm f/1.4 Mk II. The weather resistance appears better than most weather resistant lenses. (As always, I’ll add that weather resistance still means water damage voids the warranty.) The mechanical construction is beyond impressive. This lens is massively over-engineered compared to any other prime we’ve ever disassembled. It’s built like a tank where it counts; on the inside. Moving parts are huge and robust. Six big screws are used in locations where 3 smalls screws are common in other lenses. Heavy roller bearings move the focusing group, it doesn’t slide on little nylon collars.\nIt’s also designed thoughtfully and logically. Things that will inevitably get damaged on any lens, like the front element and filter ring, are designed to be replaced easily. There are some things inside, particularly with the tensioning screws and springs, that I’m not certain I understand the purpose of, but I am certain there is a purpose. If I had to summarize the mechanical design of this lens, I would say simply that no expense was spared, no corner was cut.\nSometimes things are expensive because they’re worth it. Sometimes they’re heavy because they’re so solidly constructed. This is one of those times.\nRoger Cicala and Aaron Closz\nOriginally posted on Lensrentals.com, and posted here with Roger’s permission.\nPlease rate this story, and share it with your friends?']	['<urn:uuid:41620022-363f-4f58-add5-715c2cf4b721>']	factoid	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T22:13:20.996306	8	30	2116
45	As a clinical immunologist, I'm curious about comprehensive anaphylaxis management in schools. How should schools prepare?	Schools need to develop specific anaphylaxis management plans covering both prevention and response. Prevention involves implementing school-wide strategies like peanut-free canteens, maintaining lists of allergic students with photos, and ensuring proper documentation from parents including unexpired EpiPens. For response, schools must provide regular EpiPen training for staff in small groups, ensure mobile phones and landlines are available in classrooms, designate staff responsibilities during emergencies (including who guides paramedics and contacts parents), and require teachers on yard duty to carry backup EpiPens. The response must consider various situations like excursions and before/after school care. Additionally, schools should raise community awareness through parent education sessions and communication about affected students. This comprehensive approach helps ensure proper management of Type 1-4 allergic reactions and anaphylactic emergencies which can be fatal if not handled properly.	['Allergy: What is it?\nAllergy is a hypersensitivity (hyperreactivity) of the body’s immune system against a xenobiotic substance that can act as an allergen or antigen. This type of reaction by our immune system is undesirable since it predisposes the affected tissue to damage.\nThe allergic response is mediated by different mechanisms that may be due to an antigen-antibody reaction, or cell-mediated. The predisposition of an individual to different allergens is not predictable; hence, antiallergic drugs become essential. The drugs are used to decrease the reactivity of the cells to allergens.\nIf not taken proper care, the allergic response may become fatal.\n- How do they work?\nThe reaction of cells to an allergen can be classified into four broad categories –\n- Type 1 reaction – this allergic reaction is the IgE antibody-mediated. The response is immediate (within seconds – minutes). IgE sensitizes basophils in the peripheral blood or mast cells and causes them to produce anaphylactic mediators like histamine, serotonin, intestinal peptides, prostaglandins, leukotrienes upon degranulation. These mediators get released in high amounts that show the pharmacological effect.\n- Type 2 reaction – this reaction is cytotoxic, i.e., causes cell death. The result of such a response is tissue damage due to the binding of antibody (IgM, IgG) to the antigen. The damage is caused by cell membrane lysis, phagocytic action of macrophages on the target cell, and direct damage to the affected cell.\n- Type 3 reaction – this reaction results from the antigen-antibody complex, causing an immune complex formation. The complement system gets activated, and complex-mediated tissue injury by exogenous antigens (microbes), or endogenous antigens (blood components).\n- Type 4 reaction – it is a cell-mediated reaction, involving T cells, antigens, and macrophages. The macrophages present an antigen to T cells, by which cell-mediated immune memory develops. Upon subsequent exposure, T cells cause direct cytotoxicity, lymphokine, and inflammatory mediator recruitment.\nThe drugs, therefore, are used to inhibit these types of responses. The treatment initially begins with allergen elimination or avoidance, followed by drug interventions to prevent mediators’ exaggerated release.\n- Different classes of these drugs\nThe major types of antiallergic drugs are –\n- Antihistaminic – dimedrol, diprazine, loratadine, cetirizine, levocetirizine, fexofenadine\n- Mast cell stabilizers – cromolyn, nedocromil, lodoxamide, pemirolast\n- NSAIDs – ketorolac, flurbiprofen, tromethamine\n- Corticosteroids – fluoromethanol, betamethasone, hydrocortisone, prednisolone\nDrugs used to relieve generalized symptoms of immediate allergic response –\n- Adrenomimetics – adrenaline\n- Methylxanthines – euphylline, theophylline\n- Ca+2 preparations – calcium chloride, calcium gluconate\n- Indications in which they are preferred\n- Antihistaminic used in histamine-mediated allergic reactions – allergic rhinitis, allergic conjunctivitis\n- Mast cell stabilizers used in vernal keratoconjunctivitis, keratitis, allergic keratoconjunctivitis, and giant papillary conjunctivitis, itching\n- NSAIDs are used to inhibit inflammatory response mediated by prostaglandins and in seasonal allergic reactions\n- Corticosteroids are used in extreme allergic reactions, to suppress the overall immune response and inflammatory response throughout the body.\n- Combinations of drugs used to treat drug-mediated allergic response (e.g., penicillin)\n- Side effects\n- Antihistaminic – sedation, peripheral anti-secretory effects, CNS dysregulation, palpitations\n- Mast cell stabilizers – burning and stinging pain at the site of administration, watery, puffy eyes (ophthalmic preparations)\n- NSAID – transient stinging and burning sensation, allergic reaction & keratitis in rare conditions\n- Corticosteroids – overall decrease in immune response, secondary infection, anti-prostaglandin effects\n- Adrenomimetics – increased sympathetic effects\n- Methylxanthines – hypotension, arrhythmias, tremors, insomnia, GI disturbances\n- Ca+2 preparations – bradycardia, vasodilation, GI disturbance, hypercalcemia, arrhythmias\nv Antihistaminic – Sensitivity to specific antiallergic drugs, pregnancy, and lactation\nv Mast cell stabilizers – sensitivity of the patient to drug, and formulation\nv NSAIDs – sensitivity to NSAID, corneal epithelial defect (ophthalmic preparation), pregnancy\nv Corticosteroids – pregnancy, immunodeficient patients\nv Adrenomimetics – pheochromocytoma, patients on sympathomimetic therapy\nv Methylxanthines – sensitivity to specific drugs\nv Ca+2 preparations – patients with the risk of cardiovascular events\nDrugs used in anaphylaxis\n- What are they\nAnaphylaxis can be explained as a severe allergic reaction that has a rapid onset and may be fatal. The hyperactivation of mast cells and basophils in the blood involved crosslinking IgE and aggregation of other mediators. Activation of mast cells or basophils rapidly respond and release preformed mediators like histamine, peptides, and proteoglycans. This is further accompanied by phospholipase A2 activation, followed by prostaglandins, leukotrienes, and PAF (platelet-activating factor).\nA release of inflammatory cytokine and chemokines is also mediated that include tumor necrosis factor (TNF), interleukins. Many of these mediators are thus believed to be responsible for the pathophysiology of anaphylaxis. Anaphylaxis can thus be a response to multiple mechanisms of allergic responses. The drugs used to suppress such an exaggerated body response are known as anaphylactic drugs.\n- How do they work\nSeveral mediators have variable effects on different organs that precipitate serious implications. The table below comprises the key mediators and their multiple effects on organs.\nTable 1 – Mediators and their physiological effect\n|Chemical mediator||Physiological effect|\n|Histamine||Vasodilation, increase in vascular permeability, tachycardia, increased cardiac contractility, hypersecretion|\n|Prostaglandin D2||Bronchial, pulmonary, and coronary vasoconstriction, peripheral vasodilator|\n|Leukotrienes||Bronchoconstriction, increased vascular permeability, airway remodeling|\n|Platelet-activating factors||Potent bronchoconstriction and increased vascular permeability|\n|Tumor necrosis factor (TNF)||Neutrophils activation, chemokine synthesis, recruitment of other immune cells|\nThe drugs that are used for management, either act on the chemical mediator release, or they are used to relieve the physiological effects elicited by these mediators.\n- Different classes of these drugs\nSince anaphylaxis is a life-threatening condition, the management is focused on emergency stabilization, followed by a combination of drug therapy.\nv Emergency management – Airway, breathing, and circulation resuscitation to ensure the survival\nv Vital stats monitoring – oxygen administration with cardiac monitoring and oxygen saturation.\nv Airway management: in case of severe respiratory distress, tracheal intubation should be immediately performed. Investigation for angioedema and following fluid removal should be done immediately to reduce the chance of respiratory failure.\nv Decontamination: the patient should be physically examined for any causative agent, and removed if found, followed by removal of the endogenous agents by chelation or mechanical removal.\nDrug of choice – Epinephrine – 0.3 to 0.5 mg (0.3 to 0.5 mL of the 1:1000 dilutions) I.M. repeated every 5 to 10 minutes as per response/relapse.\nAfter controlling the emergency crisis successfully, a drug regimen specific for anti anaphylaxis action is used.\nSecond-line therapy –\nv Corticosteroids – methylprednisolone, hydrocortisone, prednisolone\nv Antihistaminic – cimetidine, ranitidine, diphenhydramine\nv Anti-asthmatic medications –albuterol, ipratropium bromide, magnesium sulfate\nv Glucagon – for patients on β blockers, to manage refractory hypotension\n- Indications in which they are preferred?\nBefore the management therapy is initiated, a patient needs to be examined for proper diagnostic criteria. The investigation for clinical criteria is essential since the treatment protocol includes heavy doses of drugs, which may precipitate further complications.\nClinical criteria for anaphylaxis include –\n- Acute onset of illness lasting minutes to several hours, affecting the skin and/or mucosal tissue (pruritus hives/urticaria, flushing, edema). The onset shows associations with one or more than one of the conditions – i) Respiratory insufficiency – breathlessness, difficulty breathing, wheezing ii) Hypotension\n- Symptoms of organ dysfunction – loss of body control, pain, hypotonia\n- If a patient shows signs after exposure to an antigen (minutes-hours) as – skin, mucosal tissue reaction, respiratory compromise, fall in blood pressure with associated symptoms and GI symptoms of abdominal cramps, and vomiting.\n- Patients get exposed to a known allergen and develop hypotension rapidly.\nThe use of treatment is indicated as soon as these symptoms develop. The early symptoms for anaphylaxis include –\n- cutaneous flushing\n- chest tightness\n- life-threatening laryngeal edema (symptom – hoarseness in voice)\n- Side Effects\nThe side effects of these medications are as same as when used as an antiallergic drug. The side effect of epinephrine includes tachycardia, arrhythmias, and tremors.\nSince anaphylaxis is a life-threatening condition, the treatment protocol cannot be contraindicated as it may result in morbidity.\nHowever, caution should be taken with the patients using adrenergic blockers since epinephrine administration may precipitate a severe hypertensive crisis, upon high adrenergic stimulation.', 'When my eldest son started pre-school, the highly regulated environment provided me with confidence in its ability to reduce risk and respond to an instance of food-related anaphylaxis. The pre-school teachers knew exactly what role they had to play in the response plan. The less regulated primary school environment highlighted, to me, the importance of preventing, planning and responding to anaphylaxis in the school setting.\nSevere food allergies are a part of everyday life and severe peanut allergies affect between 0.25% and 3% of children. Peanuts and tree nuts are the most common foods associated with food related anaphylaxis and death during childhood, with one EpiPen being given to every 544 children under the age of 10 years. The number of children affected, and the potential severity of anaphylaxis, emphasises the importance of having a well-planned prevention strategy for food related anaphylaxis at school.\nThere are a number of guidelines that are currently available (listed in Further Reading) which describe fundamental principles such as communication, information sharing and responsibilities of school staff and parents/carers of the students. These generic documents alone however, are insufficient for schools.\nInstead, schools need to develop their own specific anaphylaxis management plan (in the form of protocols/detailed plans) which takes into account the size of the school, the staff and the available facilities. School specific management plans should detail step-by-step processes for all school staff responding to a case of anaphylaxis.\nManagement plans that cover prevention and responding to food related anaphylaxis provide the opportunity to ensure that all necessary support is in place throughout the school. Staff and school communities can feel increasingly confident in the school’s ability to deal with incidents involving anaphylaxis. Also, such procedures will put in place safer working conditions that are consistent with relevant state occupational health and safety legislation.\nPreventing an event generally involves targeting it from a number of angles. Preventing anaphylaxis is no different. This involves identifying and targeting the groups of individuals who may be responsible for preventing and responding to anaphylaxis: that is, parents, the school itself and the wider school community.\nParents play a vital role in preventing anaphylaxis by providing the school with all the necessary documentation and medical information about their child’s condition. It is the responsibility of parents to provide medically-approved instructions to the school once their child has been diagnosed with a severe food allergy. Parents are also responsible for providing an unexpired EpiPen while their child is at school as well as educating their child(ren) about general avoidance measures such as never sharing food with other children.\nOne of the major components of any prevention strategy is awareness by the wider community in which a child exists. Within the school setting, the community can be represented by the other parents and students at the school. Anaphylaxis should not be an issue about privacy. Rather, the more people who are aware of the child’s allergic condition, the safer the child’s environment.\nInitiatives that will increase community awareness can be taken by the school or parents. These may include some or all of the following:\n• Education about anaphylaxis should routinely take place in situations where parents attend general information sessions about the school;\n• Distributing letters to parents identifying students in the class who have been diagnosed with food-related anaphylaxis. They also present ideal opportunities in which to request parents of other students to refrain from taking, for example, peanut products to school (my experience is that almost all parents are happy to comply);\n• Providing safe catering for birthday parties by including food alternatives for children with food-related anaphylaxis. Also, parents of the affected child could provide safe alternative suggestions for food;\n• Placing a photograph of each student who is diagnosed with a severe allergy near the medication in the classroom. Medication should be centrally located in a bag/cupboard. Students should always have their personal EpiPen close by;\n• Ensuring that any replacement teacher is aware of the bag/cupboard, its contents, children with severe allergies and relevant procedures to be implemented, if necessary; and\n• Schools providing parents of children with severe allergies, information about products such as the ‘Medic Alert’ bracelet or disc. Vinyl name stickers (for books and accessories taken to school) are now available with a ‘no peanut’ symbol (see Further Reading).\nSchool responsibilities: reducing risk\nThe school’s responsibility in preventing anaphylaxis can be best described as a two tier system. The first tier involves implementing school-wide strategies that minimise a child’s risk of coming into contact with peanuts at school. This can be achieved in a number of ways i.e. increasing community awareness (discussed above) and minimising the risk of exposure to peanuts.\nSome schools have decided to become ‘peanut free zones’, which means that foods containing peanuts cannot be brought from home or sold in the school canteen. This approach could minimise, but does not eliminate, the risk of peanut related anaphylaxis occurring at school. Notably, peanut free schools are not endorsed by organisations such as the Australian Society of Clinical Immunology and Allergy (ASCIA) (perhaps because of the practical difficulty of monitoring what children bring to school), whereas peanut free canteens are recommended, as the products sold can be easily managed.\nCanteens are an important part of a school and should be considered as an important part of the prevention process by contributing to the overall risk reduction strategy. Suggestions for achieving this might include:\n• Placing a list (including photographs) of all children who are known to have severe food-related allergies in the canteen;\n• Clear labelling of food related allergies on the lunch-order bag; and\n• Flagging products with known allergens on the canteen list.\nSchool responsibilities: responding to anaphylaxis\nThe second tier involves the responsibility that lies with the school in responding to a case of anaphylaxis. This can best be determined by assessing how well a school, as a whole, is prepared for such a situation.\nA detailed management plan, which is specific to the school, will assist to instil confidence in staff and increase the chances of a best possible outcome. A last minute search for a mobile phone, an EpiPen that is not easily accessible, a phone that has not been recharged or expired medication can contribute to delays, furthering the panic and increasing the risk of a fatality. Teachers, who are generally not medically trained, should be provided with as many clear guidelines and given the confidence of a plan that maximises a positive outcome for the student.\nResponses must therefore be carefully considered and individually tailored for each school. They may include some or all of the following:\n• EpiPen training for staff represents a fundamental component to the response process, and for this reason, it is imperative that staff undertake regular training in small groups;\n• Implementing a school response which involves a number of staff members and the sharing of responsibility (including administration staff). For example, response plans should allocate responsibility to staff guiding the paramedics to the student, to call the parents and (if applicable) contact the first aid officer to offer additional support to the student and teacher;\n• Mobile phones and/or landline telephones must always be available for a teacher’s use in the classroom;\n• Teachers who are on yard duty should carry a ‘back up’ EpiPen (a generic EpiPen that is not prescribed to any student) and a charged mobile phone. Once the student is identified, the teacher can administer the (back-up) EpiPen. The staff member should also make the 000 call (advising that a student has experienced anaphylaxis/severe allergic reaction) and advise reception of the situation;\n• A ‘unique’ telephone number could be used to call reception, avoiding being placed on hold (as is the case with some schools); and\n• Any medically qualified staff member/first aid officer should always be accessible via mobile phone to other staff members including reception staff.\nOther situations the school should consider include children attending before and after school care. As the school is ultimately responsible, it must ensure it is satisfied that staff has been adequately trained, even if this responsibility has been contracted to an external organisation.\nAdditionally, children attending day excursions present another situation where planning is important. Medication and instructions must remain with the students at all times. A precautionary measure such as taking a minimum of two EpiPens on an excursion is suggested. If one EpiPen is misfired, a back-up EpiPen is available. Also, food taken on excursions must be clearly marked with the student’s name and kept separately from other food taken on the excursion. Clearly, there must be no sharing of food.\nA ‘feedback loop’ can be implemented for the purposes of learning from experience. After such an event, the staff members involved should, even with successful outcomes, assess procedures and identify areas for improvement.\nThis article is designed to make a number of suggestions to assist schools in preventing and responding to anaphylaxis. It is up to the individual schools to take the initiative to develop their own management plan. It is also important not to forget that two of the three deaths in the past five years have occurred within the school camp setting. It is therefore also important to develop formal management/response plans extending to camps and, most notably, those held in remote areas. In the short term, anaphylaxis management protocols in the school setting must not be overlooked as the first step in a vital process which helps maintain a high standard of child safety.\nAnaphylaxis guidelines: A resource for managing severe allergies in Victorian government schools (Victorian based document): http://www.eduweb.vic.gov.au/edulibrary/public/stuman/wellbeing/Anaphylaxis_guidelines-v1.01b.pdf\nAnaphylaxis:Guidelines for schools (NSW based document): http://www.schools.nsw.edu.au/media/downloads/schoolsweb/studentsupport/studenthealth/aguidelines_v2.pdf\nAustralasian Society of Clinical Immunology and Allergy (ASCIA): http://www.allergy.org.au/aer/infobulletins/posters/Anaphylaxis_plan_(child)_Au.pdf\nFood Allergy and Anaphylaxis Network\nhttp://www.foodallergy.org/school/childdescribe.pdf (anaphylaxis in a child’s language)\nGuidelines for managing food allergies at camps: http://www.foodallergy.org/downloads/CampGuidelines.pdf\nMedic Alert bracelet: http://www.medicalert.com.au/\nLillian De Bortoli is a mother of three children with her eldest son being diagnosed with a severe peanut allergy and asthma. She works at Monash University and has completed a Master of Social Work research degree analysing the public health framework as a basis for preventing child abuse.']	['<urn:uuid:0cbe35c6-0112-4f63-99f2-fa872191987b>', '<urn:uuid:4ef045f0-e5af-400f-a926-e0d6cd8cf9f6>']	open-ended	with-premise	concise-and-natural	similar-to-document	three-doc	expert	2025-05-12T22:13:20.996306	16	131	3038
46	i play guitar whats difference fame studios rick hall vs leo fender	Rick Hall was a music producer who founded FAME Studios and created hit records, while Leo Fender was an inventor who created guitars but never learned to play them - he was primarily a radio repairman who tinkered with instruments. Hall focused on producing music and discovering talent, while Fender focused on designing and manufacturing guitars.	['MUSCLE SHOALS, Ala. – The world lost a music pioneer on Tuesday. Legendary music writer, producer, and publisher Rick Hall passed away in Colbert County. Hall created FAME Studios which put the “Muscle Shoals Sound” on the international stage.\nThe studio stands out just as much as the music he produced. Rick Hall’s dreams were realized inside the four walls of FAME Studio’s. He made Muscle Shoals “The Hit Recording Capital of the World”.\n“To this day, that sound. Everybody still talks about that sound and wants to be a part of that,” stated Dixie Griffin with the Alabama Music Hall of Fame.\nRick Hall was 85-years-old. Hall was raised in Franklin County the son of a share-cropper. He got his first taste of music when he moved to Illinois as a teen and got hooked.\nAfter serving in the Korean War, hall returned to the Shoals and a factory job.\nBut after some the loss of his father and new bride within weeks, Hall turned to music. His first songwriting success came in the late 50’s with a hit by George Jones.\n“Music comes from the soul, and it came from Rick’s soul,” explained Debbie Wilson with Muscle Shoals Sound Studios. “He had the ear and the heart for what would make something successful and he was very driven to do that, and to do that with others to help them be successful.”\nAfter a failed music studio partnership, Hall founded Florence Alabama Music Enterprises.\nRecording artists from around the world started hearing Hall’s soul and wanted a piece of it.\nEtta James, Wilson Pickett, Clarence Carter, and the Osmond’s all recorded in Studio “A” at Fame. That’s just a tasting of the big names who wanted Rick Hall behind their music.\nAnd his legacy has been passed down to today’s generation of recording artists.\nMusicians from around the world passed on their condolences Tuesday as word spread of Hall’s passing. Names such as Mac McAnally, Jason Isbell, and John Paul White; Shoals artists who learned from the “Godfather of Muscle Shoals Music”.\nEarly in his music career, Hall didn’t mind breaking down barriers. Churning out hit after hit in the 60’s, he didn’t shy away from integrating his studio. He saw the potential in people, not the color of their skin.\nFAME Studios, according to its website, has been involved in the recording or publishing of records that have sold more than 350 million copies over the past 50 years.\n“The songs that they created are classics. They have lasted over the years and they are the best music ever, and all ages listen to that music,” said Dixie Griffin.\nHis friends said Hall always remained humble about his music success.\nHe also leaves behind a huge legacy filled with determination and pride in his work.\n“He had a rough time. He had a lot of adversity in his life, and he really was an inspiration for turning that adversity around into something positive. And he was really an integrating force in the music industry,” Debbie Wilson said.\n“It’s that Muscle Shoals sound,” said Griffin. “It’s just a very humble man that did more than he ever really realized he did I think, yeah he was a great guy.”\nFAME Recording Studios posted a social media note early Tuesday saying “We hope the band in heaven is ready!!! If not, there’s going to be a problem!”\nThe funeral will be held Friday at Highland Park Baptist Church on South Wilson Dam Road. Visitation will begin at noon with services starting at 2:00 p.m.', 'I love music. I feel its beats in my bones. Even human voices are acapella treats – except those whose mission is spite and hate. Their sour voices sow discord.\nHere’s a discussion of some of my favorite musical instruments: Piano, tuba/sousaphone, flute, and guitar.\nWhen a piano is played, each key controls a hammer that strikes a taut-and-tuned string inside, under its elongated top. The sound resonates from strings, making it a stringed instrument like a harp or a guitar — but the action that produces the sound is a strike, putting it in the percussion category, along with other melodic instruments like the xylophone or steel drum. Thus, pianos are considered to be both a percussion and stringed instrument. Both describe how the piano works. Some consider the piano to be a form of hammered dulcimer, another hammered-string instrument that’s hard to pin down. It’s also a keyboard instrument, which are never just keyboard instruments alone — that would also include the pipe organ (wind), harpsichord (string), glockenspiel (percussion), and synthesizer (electronic).\nThe latter are fundamental to rock ‘n roll bands.\nPianos are large instruments, so they stand like a table on the floor. Another large instrument is a tuba, held, hug-like, in a seated player’s lap.\nIn a Harvard University basement, the King Tuba, possibly commissioned by John Philip Sousa, is about 7 feet tall and 100 pounds. It was restored in 2019 and occasionally makes appearances at performances. John Philip Sousa created the Sousaphone (duh) because he wrote march music and needed the bass instrument to be carried by a member of a marching band. The instrument has been popular for a century, featured high school and college football half-time entertainment. My grandpa played one, while my dad played trombone.\nAt the opposite end of the tonal spectrum of a band is the flute.\nFlutes were among the earliest musical instruments, present in ancient cultures throughout the world. The oldest one that modern archaeologists have discovered so far is 35,000 years old, uncovered in the Hohle Fels Cave near Ulm, Germany. It was created in the middle of the last Ice Age.\nFlutes have also been components of modern rock ‘n roll bands: Jethro Tull, the Moody Blue, and certain incarnations of Jefferson Starship.\nLeo Fender Didn’t Play Guitar\nFender is one of the most popular guitar and amplifier brands, perhaps best known for its enduring classic the Stratocaster electric guitar. The founder’s interest and expertise, however, was solidly in creation, not performance. Before inventing the first mass-produced solid-body electric guitar, Leo Fender was a radio repairman who sometimes tinkered with his musician friends’ instruments. He left the music to Les Paul.\nWhile he briefly played piano and saxophone as a youth, even after decades in the guitar business, he never actually learned how to play the instrument. Legend has it that he couldn’t even tune one. He was too busy tinkering with them: Country music guitarist Bill Carson (who has been dubbed the “test pilot of the Stratocaster”) told Reverb that Fender would show up to his gigs to swap out equipment.\nHere’s one of the most famous Stratocaster creations: “Layla” by Derek and the Dominos, aka Eric Clapton and misc. crew:']	['<urn:uuid:d910877a-4f73-46ce-a6d1-8d59c2f795de>', '<urn:uuid:8cd76394-7a13-4418-a584-83fbfe11d824>']	factoid	with-premise	short-search-query	distant-from-document	comparison	novice	2025-05-12T22:13:20.996306	12	56	1132
47	My grandma has extra muscle in her wrist. Is this common?	Yes, accessory muscles in the hand and wrist are relatively common. For example, the accessory abductor digiti minimi is present in as many as 24% of wrists, and is bilateral in 50% of cases.	['Clinical history: A 29 year-old female presents with increasing swelling of the volar 1st web space for several years. She has numbness in the thumb and pain with gripping. Proton density-weighted (1a) and STIR (1b) axial images and T1-weighted sagittal (1c) and coronal (1d) images are provided. What are the findings? What is your diagnosis?\nAccessory flexor digitorum superficialis indicis muscle, most likely of the digastric variety.\nMany accessory muscles have been described in the hand. Most are asymptomatic, but they can be a cause of compressive neuropathy or a palpable soft tissue mass1,4. Detection can be difficult because the muscles have the same signal intensity and overall appearance as the adjacent normal muscles and therefore do not attract the eye in the fashion of a typical mass3. The key to diagnosis is knowledge of normal muscle anatomy, in particular being aware of spaces where muscles should not exist normally.\nUlnar-sided Accessory Muscles\nThe accessory abductor digiti minimi is the most common of the accessory muscles, present in as many as 24% of wrists, and bilateral in 50%1,2,3,4,5. It is seen radial and volar to the pisiform and inserts with the abductor digiti minimi (ADM) at the ulnar base of the 5th proximal phalanx, projecting into Guyon’s canal1,2,3,4. Normally there should be no muscle tissue in Guyon’s canal at the level of the pisiform3.\nA normal muscle that can be mistaken for a variant is the palmaris brevis, which lies in the subcutaneous tissues volar to the neurovascular structures of Guyon’s canal1. The palmaris brevis is distal to the pisiform and inserts into the skin2.\nMidline Accessory Muscles\nPalmaris longus variants are midline and superficial to the flexor retinaculum. The normal palmaris longus has the muscular component in the proximal to mid forearm and is tendinous in the wrist. The variants that cause muscle tissue to be present at the wrist include an inverted configuration with the tendon proximal and the muscle distal, a digastric configuration with muscle at each end separated by tendon and a non-tendinous variant with muscle along the entire width. The last variant can be solid or bifid in the wrist1.\nIn most people, the lumbrical muscles begin distal to the carpal tunnel. Proximal origin of the lumbricals in the carpal tunnel can occur in as many as 22% of people and can cause carpal tunnel syndrome1,4.\nRadial-sided Accessory Muscles\nThe accessory flexor digitorum superficialis indicis is a replacement of the FDS tendon by muscle at the carpals or second metacarpal. Like the palmaris longus, there are several variants including complete replacement, a digastric muscle belly and distal extension of the muscle into the carpal tunnel1,2.\nThe flexor carpi radialis brevis vel profundus extends from the distal radius to the capitate and base of the third and fourth metacarpals. It passes between the pronator quadratus and FCR muscles2.\nDorsal Accessory Muscles\nThe extensor digitorum brevis manus is found ulnar to the extensor tendon of the index finger. Diagnosis is easier to make if you keep in mind that the muscle bellies of the extensor tendons should not extend to the carpal bones1,3.\nAccessory muscles of the hand and wrist are usually incidental findings but can be a cause of compressive neuropathy or present as a palpable mass. Diagnosis depends on familiarity with the normal muscular anatomy of the wrist and being aware of places in the wrist where muscles should not exist.\n1 Timins M. Muscular anatomic variants of the wrist and hand: Findings on MR imaging. AJR. 1999; 172: 1397-1401.\n2 Sookur PA, Naraghi AM, Bleakney RR, Jalan R, Chan O, White LM. Accessory Muscles: Anatomy, symptoms and radiologic evaluation. Radiographics. 2008;28(2).\n3 Chung C, Steinbach L. MRI of the Upper Extremity: Shoulder, Elbow, Wrist and Hand. 2009. Lippincott Williams & Wilkins.\n4 Pfirrmann CWA, Zanetti M. Variants, pitfalls and asymptomatic findings in wrist and hand imaging. European Journal of Radiology. 2005; 56: 286-295.\n5 Zeiss J, Jakab E, Khimji T, Imbriglia J. The ulnar tunnel at the wrist (Guyon’s canal): Normal MR anatomy and variants. AJR. May 1992; 158: 1081-1085.']	['<urn:uuid:ea72ff77-9426-4721-9160-84249b1d1128>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T22:13:20.996306	11	34	676
48	When floods hit Rio de Janeiro in 2011, how many died?	In January 2011, flooding and mudslides killed more than 900 people and left thousands throughout the state homeless.	"['German police, prosecutors and forensics experts are facing a mystery after a 10-year-old boy found a human mummy in a sarcophagus in a corner of his grandparents\' attic. A CT scan has revealed a well-preserved human skull, with an arrow sticking out of the left eye socket, and large parts of a skeleton with the arms crossed over the chest, the local newspaper Kreiszeitung has reported. Adding to the riddle is a death mask also found in the box, and the fact that X-rays show a metal layer covering the bones of the 1.49-metre-long (4 feet 8 inches) human remains of unknown gender.\nThe boy\'s father, Lutz-Wolfgang Kettler, said his own father, who died 12 years ago, had in the 1950s travelled to North Africa and may have brought back the mummy as a grisly souvenir.\nThe bandages used for the mummy -- which has not been unwrapped for fear of damaging the remains -- date from the 20th century and are machine-woven, said Kettler, a dentist who attended the CT scan.\nPathologist Andreas Nerlich of Munich\'s Bogenhausen hospital told news website Spiegel Online that, while the skull and the bones are real, the mummy is ""a fake, made from one or several human bodies."" ""What we have are questions upon questions"" since the boy, Alexander, found the mummy about a month ago, said Kettler.\nPolice and prosecutors have taken note of the case in the town of Diepholz, Lower Saxony state, and are waiting for more information on where the body came from before looking into the possibility of modern-age foul play.\n""We\'ll wait until we know how old the bones are,"" police spokesman Frank Bavendiek told German news agency DPA. ""If they are a few hundred years old, then it\'s a mummy and we won\'t investigate."" - AFP\nRecommended for You\nBERLIN - Northwestern Europe is bracing for a severe storm that forecasters say likely will bring gale-force winds and flooding in coastal areas.Associated Press\nDENVER - The jet stream hunkered to the south Wednesday, promising to bring nearly a week of temperatures that could dip to -29 Celsius or worse in the northern midsection of the country, and forcing much of the rest of the nation to deal with unexpectedly cool temperatures.Associated Press\nDENVER (AP) — The jet stream hunkered to the south Wednesday, promising to bring nearly a week of temperatures that could dip to minus 20 or worse in the northern midsection of the country, and forcing much of the rest of the nation to deal with unexpectedly cool temperatures.Associated Press\n""Gale-force winds and large waves along the east coast of England are forecast during Thursday and Friday, coinciding with high tides and a significant coastal surge,"" explained Paul Leinster, chief executive of the Environment Agency. ""Flooding of some coastal communities is expected and some…AFP\nBy Keith Coffman DENVER (Reuters) - A powerful Arctic cold front blanketed the western and central United States on Wednesday, dropping heavy snow on the Colorado Rockies across to the Great Lakes and bringing frigid temperatures across the region. Some mountain locations west of the Continental…Reuters\nJust one week after a Thanksgiving winter blast upended some oil and gas production operations in Texas, winter storm Cleon made its debut in the Upper Plains on Tuesday and has beBusiness Wire\nHELENA, Mont. (AP) — A wintry storm pushing through the Rockies and Midwest is bringing bitterly cold temperatures and treacherous driving conditions blamed in at least six deaths as it threatens crops as far south as California.Associated Press\nBEAVER CREEK, Colo. (AP) -- The second men\'s World Cup downhill training session was canceled Wednesday due to heavy overnight snowfall.The Associated Press\nItaly coach Cesare Prandelli has called for the introduction of drink breaks during next year\'s World Cup finals in Brazil to counter the debilitating effects of heat and humidity at venues in the tropical northeast. ""You can\'t even give players water because FIFA want the bottles near the…Reuters\nResources needed to meet consumer demand for electricity are expected to be adequate during the 2013/2014 winter season in New England, according to ISO New England Inc., the operaBusiness Wire\nRio de Janeiro Prepares for Severe Floods Using Early Warning Technology from Earth Networks for Advanced Weather Tracking\nRio de Janeiro’s subtropical climate and coastal location make it vulnerable to intense storms with lightning, heavy rainfall and extreme flooding. In January 2011 alone, flooding and mudslides killed more than 900 and left thousands throughout the state homeless because timely weather alerts are…Business Wire\nHELENA, Mont. (AP) — A wintry storm pushing through the western half of the country is bringing bitterly cold temperatures that prompted safety warnings for residents in the Rockies and threatened crops as far south as California.Associated Press\nRescuers were searching a rugged and snow-covered swath of central Idaho for a single-engine airplane carrying five people, authorities said on Tuesday, two days after the pilot complained of engine trouble and then lost contact with air traffic controllers. The pilot of the missing 1980s-model…Reuters\nBy Nina Chestney LONDON (Reuters) - An internationally agreed target to limit rises in global average temperatures to within 2 degrees Celsius is around double the threshold that would avoid catastrophic climate change, a study by 18 eminent scientists said. Governments decided in 2009 that such…Reuters\nHELENA, Mont. (AP) — Temperatures across the western half of the nation are plunging as much as 40 degrees below normal as an Arctic cold-air mass moves south.Associated Press\nAlbertans are digging out after a massive burst of winter weather buried the southern part of the province in up to 40 centimetres of wind-whipped snow.Associated Press\nBut picking the right airport will also boost your odds of getting where you’re going on schedule. A combination of weather and airport traffic largely determines airports’ performance in the cold-weather months. Boston’s Logan Airport, for example, has won awards two of the last three years for…The Fiscal Times\nUsually the Grand Canyon offers stunning views stretching for miles, deep into valleys etched by the Colorado River. But that vista has changed over the past few days, as a rare weather event has filled the canyon with fog, offering an even more stunning view than is typical. The weather event…LiveScience.com\n(Reuters) - The U.S. Federal Aviation Administration on Monday proposed fining Southwest Airlines The U.S. agency alleged in a statement that the incident, from August 2011, involved the faulty installation of a switch on a Boeing 717 that allows crews to test the plane\'s windshield heating…Reuters\nNEW YORK (AP) — ""Good Morning America"" weather anchor Sam Champion is leaving ABC News to join The Weather Channel.Associated Press']"	['<urn:uuid:54aff793-af3c-4831-a5a8-d693da6bbd05>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T22:13:20.996306	11	18	1103
49	Can you tell me about the USS Wharton's involvement in World War II battles and what role Lieutenant Carleton Thayer Fogg played in the Pacific war effort during the same period?	The USS Wharton was a troop transport that earned three battle stars during World War II, participating in major operations like the Marshall Islands invasion, Guam landings, and Okinawa campaign. The ship served both as a transport and hospital ship, notably assisting with casualty evacuation at Guam where it processed 723 patients. As for Lieutenant Carleton Thayer Fogg, he was a Naval Air Service officer who was killed in action in the Pacific Theatre in February 1942. Before his death, he had pursued his passion for aviation, training at Pensacola and being commissioned as an Ensign. He was the first North Yarmouth Academy alumnus to enlist in the Naval Air Service and the first to give his life in service of the nation during World War II.	"['Memorial Flagpole Erected in Honor of Carleton T. Fogg at\nNorth Yarmouth Academy in Yarmouth, ME.\nReplica of the plane in which he and Otis Dennis gave\ntheir lives, in WWII.\nPhotos and Speech courtesy of\nFEBRUARY 22, 1942 4PM\nSAFFORD AUDITORIUM - North Yarmouth Academy\nTen years ago North Yarmouth Academy welcomed to this\nbuilding its initial group of Yarmouth students. Sophomores of\nthe Class of 1934, entering in September 1931, were the first to\nspend three years here before graduation. That class was not\nmerely a leader in point of time, but may fairly be described as\noutstanding in other respects, for, although their enthusiasm,\ncooperative effort, and general morale may have been equaled, it\nhas never been exceeded by any subsequent group.\nIt was a member of that class who first won a scholarship\nin competitive open examination at the State University and thus\nnailed high the guidon for the stimulation of others who were to\nbe equally successful in the years to follow.\nIt was that class which furnished more winners in County\nPrize Speaking than any other, and it was the boys of that class\nwho, by their own hard effort, graded the unfinished lawn in\nfront of this auditorium and thus became (as one of the number\nsitting in the audience this afternoon hoped they might), ""the\nfirst class to graduate with a completed school campus.""\nAnd from that class went the first graduate to enlist in\nthe Naval Air Service, the first to win his wings, and as we\nhave sadly learned within the week, the first alumnus to give\nhis life in the service of the Nation.....\nLieutenant Carleton Thayer Fogg, United States Naval Air\nService, killed in action in the Pacific Theatre in February,\nWe all remember ""Foggy"" as it was inevitable he should be\nnicknamed by his classmates.... remember him for his cheerful\nmanner, his unfailing spirit, and his active participation in\nthe affairs of school, church and community. A member of the\nYarmouth orchestra and band, he was also a member of the\nYarmouth Town Band, and found time as well to take part in the\ndramatic activities of the school.\nHis interest in aviation began before he entered the\nAcademy and was continually manifest in his study of the related\nsciences. Despite mild efforts on the part of parents, teachers\nand friends to steer him toward less hazardous engineering\npursuits, his heart and mind fixed on the one objective, ... a\ncareer in aviation. He was one of those fortunate lads who knew\nearly in life what he wanted and stuck steadfastly to his\nobjective. And so it came about that, after three years spent at\nthe University of Maine, he entered the Naval Air Service\nTraining at Pensacola and, in due course, was commissioned an\nEnsign in the service.\nAlmost exactly one year ago he was here in Yarmouth for a\nvisit and on March 7, 1941 sat here on this platform and, much\nto the delight of the student body, answered their questions\nregarding the Naval Air Service and the training of an air\npilot. It was altogether characteristic of him that he appeared\non that occasion, not in uniform, but in ""civvies"", or, as our\nBritish allies would say, ""in mufti"".\nIndeed, his parents never saw him in uniform. To him a\nuniform was not a device for display, but merely one of the\nnecessary accoutrements of an officer of the service to be worn\nonly when on duty. His whole life was like that, in school and\nout, for it was his ambition to silently serve rather than to\nThat we honor his memory by this gathering today is wholly\nfitting; but, should we honor him ONLY by such an assembly,\nshould we feel our duty now done to him and to inestimable\nthousands like him who have fallen and who will continue to fall\nin these days ahead, we would indeed be relaxed in our\nEvery man who has served on the field of battle in time of\nwar knows that, though he may have undergone his baptism of\nfire, though he may have stood long hours of sentry duty under\nadverse conditions; though he may have observed long lines of\nambulances carrying wounded to the rear; yet the grim reality of\nwar does not sink into his consciousness, permeate his whole\nbeing, and shock his very soul until some ""buddy"", some\nschoolmate, some dear friend is struck down.\nSo, in a lesser degree, it is with the community. We read\ndispatches of military disaster, the air waves carry black news\nand dire warnings, we are depressed, ""war-weary"" (as we think)\nand often annoyed by such inconveniences as rationing of\nconsumer goods until, one day, there comes news of a man who was\nour friend and our neighbor caught in the pattern of war.\nTHEN WAR STRIKES HOME.\nWhat man, whatever his need, can grouse and grumble at the\nrationing of rubber when his friend has made the supreme\ncontribution; what civilian club-room admiral or barbershop\ngeneral can revise and rearrange with his pitiful\nHALF-information battle lines, worldwide in their extent,\nentered by his neighbor who, like a brigade famous in history,\nknew and acted upon the military axiom ""Theirs not to reason\nwhy, theirs but to DO, - and die!""?\nWhat man can sit in idle revolt, refusing to labor pending\nthe adjustment of some petty difference between labor and\nmanagement, with the example of sacrifice of the soldier and\nsailor before him!\nWhat man, faced with the news of a neighbor lost in action\nand living daily under the expectation of similar news from\nothers, does not need to reexamine the semantics of the words\n""Hardship"" ""Sacrifice"" and ""i n c o n v e n i e n c e"" in his\nNo greater honor could be done the memory of him whom we\nwould salute today than for each and every one of us here to\nresolve and NOW to erect an insurmountable wall in our mentality\nand consciousness to which it may truly apply.\nIn the difficult, dark and dire days that lie immediately\nbefore us and in the days to follow when luxuries and\nconveniences we once knew may not return, let us, in abject\nhumbleness, be ever mindful of the youth of our own nation who\nare or who have been carrying ""somewhere out there"" where the\nmeanest comforts of home would represent the peak of luxury.\nToday, from the pulpits of churches throughout our\ncountry, tribute is being paid to an American patriot, the\nchosen leader of men whose steadfast courage in the face of\nhardship never since known in this land made the establishment\nof this nation possible. What sacrifice imposed by rationing,\nrestriction, and regulation today can compare with the suffering\nof those ill-clad, ill-fed, and barefoot men of Washington at\nValley Forge? And yet in THAT day there were TORIES who held the\nDeclaration of Independence a mistake, ridiculed the nondescript\nvolunteer army, and made the lives of those soldiers and their\nleaders the more miserable by withholding full financial and\nWithin the fortnight we have honored the memory of another\ngreat American, Abraham Lincoln, who, long before he rose to the\npresidency, repeatedly expressed his ""personal wish that all men\neverywhere might be free."" Viewed with the perspective of three\nquarters of a century how Christ-like, how God-like, how sublime\nthe career and acts of that man. For Abraham Lincoln\'s character\nand personality were such that, the more one studies his life\nand acts, the more familiar one becomes with his philosophy and\nhis career, the more one\'s very soul is gripped by the innate\nhumanness and humaneness of the man. As the poet, Carl Sandburg,\nhis most recent and perhaps his greatest biographer has said,\n""There is something about that man, Lincoln, that ""gets"" you!""\nAnd yet, by a very substantial minority of his\ncontemporaries in the NORTH as well as in the South, in Yarmouth\nMaine as bitterly as in Birmingham, Alabama, he, Abraham\nLincoln, was reviled, hated, detested, and openly cursed. The\nslave-holders of the South, the cotton brokers of Boston, the\nmill owners of Manchester, England and of Manchester, New\nHampshire, -- men whose material fortunes were threatened by the\nthreat to slavery -- rose almost as one against him.\nBefore the Emancipation Proclamation abolitionists abused\nhim for not going far enough, after it substantial and\ninfluential men in the north as well as in the south found\nexcuse for greater condemnation than ever.\nBut, through all his trial, through the hard years when he\nlikened himself to __________negotiating the Niagara Gorge on a\ntight-rope, the lovers of TRUE and UNIVERSAL freedom stood by\nhis side. While financial interests of ""the City"" in London\nplaced their bets on the South, the working men of Manchester,\nEngland stood solidly behind his principals of freedom.\nAnd so today, with a greater slavery threatening to\nspread, not through half a nation, but through the entire world,\na slavery which freely flaunts overseers more cruel, punishment\nmore savage, and intolerance extending beyond race and color to\ncreed and party... what, too often, do we find?\nUnity of division?\nTolerance or intolerance?\nSacrifice or selfishness?\nWhile leaders more avaricious, unprincipled, cruel, and\nsadistic than any the world has every known struggle to impose a\n""new order"" (which is in deed NO order) upon the entire sphere,\nsome sit mentally supine, calm and complacent, while sentries\nsound the alert.\nSome, wearied with the trouble and toil, the friction\nbetween warp and weft that preserves the very fabric of\ndemocracy and liberty, led by laziness , dare to think that\nmaybe, -- perhaps, -- democracy is too much trouble after all.\nYOU know the answer.\nIf not, ask the starving Poles, the Danes, the\nQuisling-ruled Norwegians, the inhabitants of UN-occupied\nFrance, the Greeks, the tea-dancers of Singapore!\nWAR is neither a tea-party nor the hullaballoo of a\nmoment, but a long, hard, wearying grind which can and WILL be\nmet with characteristic American courage.\nDemocracy IS worth saving, it IS worth the ""blood, and\nsweat, and toil, and tears"" that it has demanded of its\nadherents down through the ages.\nReveille sounded in September 1939, ASSEMBLY on December\n7, 1941; let us make sure that RETREAT will never be sounded by\nAmerica though TAPS sound a million times or more in the\nLet us here dedicate ourselves each and every one, to the\nunfinished task remaining before us that, in the words of\nAbraham Lincoln, we may ""nobly win"" and not ""meanly lose the\nlast best hope of Earth"".\nTo the names of illustrious military alumni, Generals\nOliver C. Howard, Charles W. Roberts, Frederick D. Sewall,\nSamuel J. Anderson, and Isaac W. Starbird, NORTH YARMOUTH\nACADEMY, in sorrowful pride, adds yet another,\nLIEUTENANT CARLETON THAYER FOGG.\nIn his spirit let us carry on!\nStanley W. Hyde\nNORTH YARMOUTH ACADEMY', '|USS Wharton (AP-7)|\n|Laid down:||8 October 1918|\n|Launched:||20 July 1919|\n|Completed:||24 September 1921|\n|Acquired:||8 November 1939|\n|Commissioned:||7 December 1940|\n|Decommissioned:||26 March 1947|\n|Struck:||4 April 1947|\n|3 battle stars (World War II)|\n|Fate:||Sold for scrapping, 21 March 1952|\n12,250 long tons (12,447 t) light|\n21,900 long tons (22,251 t) full\n|Length:||636 ft 2 in (193.90 m)|\n|Beam:||72 ft (22 m)|\n|Draft:||31 ft 3 in (9.53 m)|\n|Speed:||16.6 knots (30.7 km/h; 19.1 mph)|\n|Complement:||666 officers and enlisted|\n• 4 × single 5""/38 caliber guns|\n• 8 × .50 cal. machine guns\nUSS Wharton (AP-7) was a troop transport and hospital ship in the service of the United States Navy during World War II. Originally built for the Munson Steamship Line as the cargo liner SS Southern Cross in 1921, she was acquired by the Navy as World War II approached and, once the United States became a combatant, she served as a troop transport and hospital ship in the Pacific Theatre. At war\'s end, she returned Stateside proudly with three battle stars. SS Southern Cross operated in the South American trade from 1921 until acquired by the Navy from the Maritime Commission on 8 November 1939. Two days later, the ship was renamed Wharton and designated AP-7. She was converted to a troop transport by the Todd Shipbuilding Corp., in the Robbins Drydock in Erie Basin at Brooklyn, New York. The transport was commissioned at the New York Navy Yard on 7 December 1940, Capt. Ernest L. Vanderkloot in command.\n- 1 World War II Pacific Theatre operations\n- 2 First wartime operations\n- 3 Invasion of the Marshall Islands\n- 4 Temporary duty as a hospital ship\n- 5 Running aground at Manus\n- 6 Assisting the wounded at Guam\n- 7 Supporting Invasion of the Philippines\n- 8 Supporting the Okinawa invasion\n- 9 End-of-War activities\n- 10 Bikini Atoll A-Bomb testing\n- 11 Post-War deactivation and decommissioning\n- 12 Awards\n- 13 References\n- 14 External links\nWorld War II Pacific Theatre operations[edit | edit source]\nWharton departed Brooklyn on 7 January 1941, bound for Guantanamo Bay, Cuba, where she conducted shakedown before proceeding on through the Panama Canal to her home port, Mare Island, California. Assigned to the Naval Transportation Service, Wharton transported service personnel and their families, as well as cargo, on triangular runs from San Francisco, San Diego, and Pearl Harbor. She also made one trip to Midway Island.\nFirst wartime operations[edit | edit source]\nWhen the Japanese struck Pearl Harbor, Hawaii, on 7 December 1941, Wharton was undergoing overhaul at the Mare Island Navy Yard, Vallejo, California. On 6 January 1942, the transport sailed from the west coast for her first wartime voyage to the Hawaiian Islands. A series of runs followed in which Wharton transported service families and dependents home to the west coast on her eastbound passages and troops and cargo to Hawaii on her westbound trips.\nFrom June through September, Wharton made three voyages to the Southwest Pacific theater — loading and unloading at such ports as Pago Pago, Samoa; Auckland, New Zealand; Espiritu Santo, New Hebrides; Nouméa, New Caledonia; Canton Island, and Suva, Fiji Islands, before returning to the west coast for an overhaul which lasted into October. The troop transport then began a series of trips to the Aleutians which lasted from December 1942 to February 1943, carrying troops from Seattle, Washington, to Kodiak and Dutch Harbor and returning with civilians, troops, and patients. For the remainder of the year, Wharton made five more trips to the Southwest Pacific, during which she revisited Pago Pago, Nouméa, Suva, Espiritu Santo, and Wellington, while adding Apia, British Samoa; Guadalcanal, Solomons; and Efate, New Hebrides; to her itinerary.\nInvasion of the Marshall Islands[edit | edit source]\nIn January 1944, Wharton joined Transport Division 30 for the Marshall Islands operation. Equipped with seven manned LCVP\'s, Wharton sortied from Pearl Harbor in Task Group 51.1 on 23 January 1944, bound for Kwajalein and Eniwetok, with 526 Army Headquarters troops embarked. The group operated off the island of Bigej in Kwajalein Atoll from 31 January to 2 February, during the shore bombardment phase of the operation and the initial landings, before moving into the lagoon and anchoring there on 2 February.\nTemporary duty as a hospital ship[edit | edit source]\nWharton remained in the lagoon until she headed for Eniwetok on the 15th. Following her arrival there two days later, the troop transport, while disembarking her troops and unloading her cargo, took on additional duty as a hospital ship. She received on board 85 patients for treatment and subsequently transferred them all to other facilities prior to sailing for Kwajalein on 25 February.\nRunning aground at Manus[edit | edit source]\nOn 29 February 1944, Wharton got underway for the Ellice Islands to embark the 11th and 58th Construction Battalions for transportation to the Admiralties. At 1700 on 17 April, while entering Seeadler Harbor at Manus, she ran aground due to an inaccurate chart and poor placement of buoys marking the channel. After the ship had been refloated at 0100 on the 18th, a quick check revealed no damage to her hull or machinery.\nWharton later transported 1,782 men of the Royal New Zealand Army from Green Island to Nouméa before sailing for Espiritu Santo and Guadalcanal. At the latter island — the scene of bitter struggles from August 1942 to February 1943 — the ship participated in training exercises with Transport Division 8. After two weeks of practice landings, Wharton sailed for Kwajalein with 1,587 troops of the 2nd Battalion of the 12th Marines and the 1st Battalion of the 3rd Marines embarked. At Kwajalein, she transferred the latter unit to LST\'s for the impending operations against the Japanese-held Marianas.\nShe got underway for Guam on 12 June and spent 17 days at sea before returning to Kwajalein, because fierce Japanese resistance on Saipan had forced Admiral Nimitz to postpone American landings on Guam. Underway again on 17 July, the transport made landfall off Guam four days later and soon disembarked her assault troops. That night, she retired to sea until midnight, when she reversed course to return to the beachhead for her role as casualty evacuation ship.\nAssisting the wounded at Guam[edit | edit source]\nOn the day that followed, she continued this pattern of operations. Although not designed for such work, Wharton performed yeoman service off the beaches. Two of the ship\'s lifeboats were kept ready in their davits for instant deployment, and litters containing casualties were brought alongside in landing craft and transferred to these boats which were then hoisted up to the promenade deck level to be rushed to emergency dressing stations in the passenger officers\' wardroom spaces. During the landing operations, some 723 patients were logged into Wharton\'s sick bay, most of them coming on board by way of this improvised ""lifeboat elevator."" Operating in company with USS Rixey (APH-3), Wharton returned to the transport area each morning for eight successive days to receive casualties and send an occasional beach party ashore. These latter groups worked on the off-shore reef, unloading supplies and ammunition from LCM\'s — which could not cross the coral to waiting amphibious tractors which carried the cargo to the beachhead. Working often in 24-hour stretches, these men on occasion came under enemy mortar fire. On 29 July, her part in the Guam operation completed, Wharton headed for Eniwetok with 519 patients embarked.\nFollowing the Marianas operation, Wharton returned to the United States, reaching San Francisco on 25 August. After two months of repairs, the ship resumed her transport duties and made a voyage to Guadalcanal, Espiritu Santo, and Nouméa before returning to the United States late in the year.\nSupporting Invasion of the Philippines[edit | edit source]\nOn 7 January 1945, Wharton got underway for the Philippine Islands, carrying troops and cargo in support of the operations to wrest the islands from the Japanese. She disembarked 1,386 troops and 131 tons of cargo at Samar on 14 February and, two days later, unloaded 134 tons of cargo and 869 more troops at Leyte Island. Underway for home on the 17th, the transport stopped at Ulithi before pressing on eastward and arriving at San Francisco on 12 March.\nSupporting the Okinawa invasion[edit | edit source]\nWharton next participated in the operations against Okinawa, arriving offshore on 19 May. The transport soon disembarked 2,118 troops (including 30 Army nurses) in LCM\'s sent from shore, as Wharton ordinarily carried no landing craft of her own. Several times, the ship went to general quarters and was screened by smoke, but she emerged from the campaign unscathed by kamikazes that had taken such a dreadful toll from American ships. On 22 May, the transport departed for the Caroline Islands, with 273 troops and 29 casualties embarked, and arrived at Ulithi on the 28th.\nWharton took part in no further combat operations and returned home — via Seeadler Harbor, Guadalcanal, Espiritu Santo, Nouméa, and Suva — to San Francisco on 25 June. The ship remained there until 3 August, when she moved to Seattle, Washington, before returning to Pearl Harbor.\nEnd-of-War activities[edit | edit source]\nHostilities had then ended, but the gigantic job of returning troops from the far-flung bases and islands nonetheless remained. Wharton conducted three voyages to the western Pacific — calling at Eniwetok, Guam, Saipan, Samar, Tacloban, and Puerto Princessa through the end of 1945 to pick up Army, Navy, and Marine Corps veterans and return them to the United States in ""Operation Magic Carpet"". Wharton made one more trip under ""Magic Carpet"" to Yokosuka, Japan, in February 1946.\nBikini Atoll A-Bomb testing[edit | edit source]\nIn the spring of 1946, Wharton participated in ""Operation Crossroads"" — transporting observers to Bikini Atoll for the atomic bomb tests which were to be conducted there in July. She remained there until the completion of her duties on 27 August. She made one round-trip cruise from San Francisco to Guam and one from San Francisco to the Far East, adding Yokohama and Sasebo, Japan; and Shanghai, China; to her list of ports of call.\nPost-War deactivation and decommissioning[edit | edit source]\nThe transport returned to the United States on 28 January, when she made port at San Francisco prior to heading north to Seattle, Washington, and arrived there on 9 February 1947. On 11 March, the Secretary of the Navy declared Wharton ""surplus to Navy needs"" and accordingly authorized her disposal. Decommissioned on 26 March 1947, Wharton was struck from the Navy List on 4 April 1947.\nAwards[edit | edit source]\nWharton was awarded three battle stars for her World War II service\nReferences[edit | edit source]\n- This article incorporates text from the public domain Dictionary of American Naval Fighting Ships. The entry can be found here.\n[edit | edit source]\n- Photo gallery of USS Wharton at NavSource Naval History\n|This page uses Creative Commons Licensed content from Wikipedia (view authors).|']"	['<urn:uuid:1fef1df0-d0ba-4098-a542-204c559c73d5>', '<urn:uuid:f62725cf-223d-4bb3-8947-d88905ef46d3>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T22:13:20.996306	31	127	3645
50	What evaluation metrics and visualization techniques help assess K-means clustering performance?	K-means clustering performance can be evaluated using Sum of Squared Error (SSE), which measures the distance between points and their nearest cluster centers, and the dissimilarity ratio, which compares inter-cluster to intra-cluster distances. For visualization, clusters can be plotted in 2D/3D space to identify well-separated, center-based, contiguous, or density-based patterns. The elbow method, which plots cluster numbers against inertia values, helps determine the optimal number of clusters where adding more clusters provides diminishing returns in reducing inertia.	"['Unsupervised Learning using KMeans Clustering\nCan you group similar customers in the churn data? Hmm... maybe, but similar in what way?\nWhen choosing a telecommunication service provider, customers usually have many choices. They can choose any service provider and may move away from the current provider. The percentage of customers moving out and disconnecting the service is known as “churn”. It is very important to reduce churn for business growth and customer retention. If the churn is high, the business will continually be in search of new customers without a stable customer base. The performance of the business will be very unpredictable. Businesses try to keep the customers satisfied, to retain them as long as possible. However, in the real world, the customer churn can be as high as 25% annually in the telecommunication industry. Also, the cost of acquiring a new customer is 10 times more than the cost to retain an existing customer. This poses a serious challenge to business owners.\nWe will use customer churn data to apply unsupervised learning methods and will group them or clustering them into groups. Machine learning algorithm with determined these clusters and which customer lands in which group.\nData used for this analysis was obtained from Kaggle. The data ranges from demographic information to types of services being provided. Using this data clusters or groups of customers can be identified. We will then try to understand if those groups can be tied to some real-world characteristics and figure out in what way are those customers in a particular group similar, which leads the algorithm to group them together.\nNote: Data for this analysis is available at https://github.com/microbhai/CustomerChurnAnalysis/blob/master/telecommunication_customer_churn.csv\nUnsupervised learning models are usually used on data when data scientists don’t have prior knowledge about the categories in the data. Any information pertaining to how the data can be grouped is not present. It is just data with several rows and columns with numerical values, but no information about what rows belong to what group, if any.\nBased on the description of the customer Churn data, we know that our data is perfectly labeled. And we have prior knowledge about different categories in the data. For example, we know the customers who churned out, we know the customers who have fiber-optic internet, we know the customers who opted for streaming services for TV and Movies, customers who have tablets, customers who pay their bills by checks, etc. Our data is as labeled as it can be. Although the data provided to us is labeled, in this exercise we will be exploring an unsupervised learning mechanism to figure out clusters in the data.\nUsing unsupervised learning, solely based on how data points align themselves by the virtue of their values for different fields, we will determine clusters or groups in the data, if any. We may not know the reason behind the clusters and any properties associated with them until we do further analysis. However, the unsupervised learning model will give us multiple clusters and how our data points are labeled into those clusters.\nThe overall scope of the analysis can be summarized as:\n1. Creation of an unsupervised learning model using the available customer churn data to label the data into multiple clusters\n2. Analyze the clusters to determine if they align with any customer churn behavior\nBased on the problem summary, we need to implement an unsupervised learning algorithm and use it to label unlabeled customer churn data. One of the very popular clustering techniques is “KMeans” and is frequently used to solve clustering problems. In the section below, we will know how the KMeans algorithm works. Let’s start with a visual example to understand the basics. If the data is 2 dimensional, it is easy to plot it on charts and visually identify if certain points can be grouped or labeled in clusters based on the distance between the points. In this scenario, there could be a few points clustered together (being near to each other) with some space around it and the next cluster located nearby. So, based on the proximity we can visually label the points.\nThe images below show the visual grouping of data points in a 2-dimensional space.\nKMeans is a popular unsupervised clustering algorithm designed to group data into clusters and label data points. It is widely used in applications such as market segmentation, document clustering, image segmentation, and image compression, etc. The algorithm is based on the distance between the points in N-dimensional space and relies on finding centroids for each of the numbers of clusters, we want to identify in the data. It shuffles and randomly chooses as many points as the number of clusters required to be identified and assign them as cluster centers. Afterward, iteratively, it calculates the distance between these cluster centers (or centroids) and data points to group the data points into clusters.\nWith each iteration, it reassigns the centroids by taking the mean of the points assigned to clusters and progressively calculates a better representation of cluster centers until no further improvement can be made in choosing cluster centers. Once the model is created, and labels are assigned to the existing points, the unsupervised learning model can then be used to label new data points, based on their distance from the centroids of the identified clusters.\nFor explanation and demonstration, I will generate data for clustering and apply the KMeans algorithm to label clusters and their centroids, in 2D and 3D space. We will generate 50 data points, such that the points inherently belong to 3 clusters. Then we will use KMeans to identify 4 clusters in the data and find their centroids. We will plot for in 2D and 3D space for visual presentation. In the charts below, the red points represent the cluster centers.\nAs can be seen that we started with a bunch of random points in 2D/3D space as input data. The data was biased to have 3 clusters inherently. I did this for demonstration. After running the KMeans, we ended up getting our random points labeled into 4 different clusters. Each of those 4 clusters has its center. And those were marked with red dots on the plots. We could visually see the centers being identified and data points being plotted with different colors to mark the cluster labels. However, in higher-dimensional space, such visualization is not possible. The concept remains the same, the computation logic and algorithm remain the same, but it works over many more dimensions than can be plotted on a chart.\nData preparation starts with an understanding of the available data for analysis. Customer churn data has 40 fields. One of the important tasks is to determine which fields can be used for KMeans analysis. There are categorical data fields like Martial, Gender, etc., and continuous numeric data fields like Tenure, Age, etc. Some of these fields may not be important for analysis such as customer ID, interaction, and UID (which are related to customer service interactions). Some other fields which are not important for analysis are the Latitude and Longitude of the customer, case order (used as a serial number). We will also check data for null values and if found they will need to be handled appropriately.\nIn our Customer Churn data, a large number of columns are categorical such as gender, internet service, phone service, etc. don’t provide any values when it comes to the KMeans algorithm. As we can see that the KMeans algorithm is based on the distance between points, categorical data doesn’t fit the model well. We can convert the categorical data into 0 and ones, but it won’t add any meaningful value to an algorithm with works on the distance between points. Hence, we will drop all the categorical data fields from our analysis.\nKMeans algorithm is based on distance, and we need to normalize the values of data fields before these distances are computed. Normalization will bring the field values within a similar range for different dimensions. For example, if monthly payment for telecom service is between 100 and 400, age of the customers is between 10 and 90, bandwidth consumed per month is between 100–700 and income is 300 to 260000, all these fields are widely at different ranges of values and an attempt to compute the distance between such points will give distorted results. If we normalize the data and bring it to a similar range of values, computation of distance would make a lot more sense in this high dimensional space, and corresponding KMeans clustering will be more reliable and usable. The accuracy of prediction increases when these points are plotted using similarly scaled dimensions, instead of dimensions that vary greatly (orders of magnitude).\nWe have the following variables in customer churn data after the initial data cleanup. All categorical variables have been removed.\nContinuous Numeric variables:\n‘Population’, ‘Children’, ‘Age’, ‘Income’, ‘Outage_sec_perweek’, ‘Email’, ‘Contacts’, ‘Yearly_equip_failure’, ‘Tenure’, ‘MonthlyCharge’, ‘Bandwidth_GB_Year’\nIn supervised learning models, the usual practice is to take the available data and split it into training and testing sets. The model is trained on the training set, then predictions are made on the test set. Afterward, to check the accuracy of the prediction, the predicted values and actual values on the testing set are compared. If possible, tuning of the model is executed to increase the accuracy of prediction.\nContrary to supervised learning models, in unsupervised clustering models, there are usually no labels present in the data. It is the algorithm that will label the data with cluster labels. So, there is nothing to compare against for determining the accuracy. So, the step of splitting data into training and testing sets and comparison to assess the accuracy doesn’t hold good. It makes the task of clustering relatively simple. However, the basis of clustering is the distance between the points in N-dimensional space, and it may be hard to characterize these clusters in the real-world sense. For example, if you feed the customer data to the KMeans algorithm, and we get the clusters labeled, the real-world significance of those customers clustered together may be hard to determine. It will be hard to take actions based on these clustered customers without understanding the characteristics associated with them.\nLuckily, our customer Churn data is not unlabeled. Even though we have dropped the data columns for analysis, we can use them to understand how customers in the created clusters may be related and if these clusters mean something with respect to the real-world categorization.\nTo cluster this data using a KMeans classification algorithm:\n1. Choose a range of number of clusters (k)\n2. Create a model for each choice of the number of clusters\n3. Computer inertia for each of the models\n4. Create a plot of inertia values and number of clusters\n5. Determine the right number of clusters using the Elbow method\n6. For the chosen number of clusters create the KMeans model and label the data\n7. Additionally, run exploratory analysis to check if the cluster labels assigned by KMeans correspond to the existing label in the data (the labels which existed by were dropped for the purpose of KMeans analysis, the categorical variables)\nAnalysis of the Model\nWe chose a range of 1 to 10 for the number of clusters. For each of these, we created a KMeans model. And for each of models, we computed the inertia of the model. The inertia of the model is the sum-of-squares within the clusters. The inertia is at its maximum when the number of clusters is one (all the points grouped under a single cluster, not clustering of data into separate groups). And the inertia is 0 when we have as many clusters as the number of data points, such that each data point gets to be in its cluster and there is 0 sum of squared distance between the points with the cluster.\nBetween the maximum and minimum value of inertia, we can compute inertia for each of the intermediate number of clusters and plot them on a chart. This chart shows how inertia drops when the number of clusters increases. At a point somewhere on the chart, the increase in the number of clusters starts having a progressively lower impact on the drop of the value of inertia. At this point, we see an elbow-like shape of the curve being formed. This is what determines an optimum number of clusters that KMeans can find with the data points provide for fitting the model.\nThe k vs inertia plot for our customer churn data is provided below.\nAs can be seen from the plot, the elbow-like shape occurs at k=2. This means that KMeans is optimally able to find 2 clusters in the data. We can find more clusters but the drop in the inertia is most significant from k=1 to k=2, and if goes down gradually afterward.\nModel Summary and Implications\nThe elbow method showed us that the KMeans algorithm suggests 2 distinctive clusters in the data. We can find the cluster centers and inertial values using the model. We can also find the cluster labels for various points in the data. Considering that we are dealing with unsupervised machine learning here, we don’t know what these 2 clusters correspond to. They have been found by the algorithm based on the proximity of the points. And an number of clusters is determined by the drop in inertia with respect to the number of clusters. How do they relate to the customers in the real world?\nThis is where domain knowledge of the data can shine some light. Knowing the data and the background, we understand that the data belongs to customer churn records. If KMeans tell us that there are 2 distinct clusters in the data, it is intuitive to try and correlate these 2 clusters with the customer churn, a cluster of customers who churned out, and another cluster of customers who stayed with the telecom company. To verify, if these clusters correlate with the customer churn, we will have to use the “Churn” data field which was previously dropped from the data. Taking the labels as predicted by KMeans, we can add them to the original data set, and run some queries to check how the correlation of label values with Churn field value presents itself.\nThe KMeans labels have values of 0 and 1. A quick query on the data shows that there are 2650 customers in the data who have a churn value of Yes. These are the customer who chose to change the telecom provider and moved out of the company. Another query tells us that there 2366 customers which have a Churn value of Yes along with a KMeans cluster label of 1. This shows that majority of the customers who churned out of the company were labeled by the KMeans clustering algorithm under cluster 1. This shows a strong correspondence of cluster 1 customers and their churn behavior. With this correspondence established, we can consider that the labels are predictions of customer churn behavior. Label value of 0 is the prediction for the customer to stay with the company and label 1 is the prediction to move out. Churn column values of Yes or No are the actual values to compare against and come up with prediction accuracy.\nUsing sklearn.metrics functions, we find that KMeans is accurately able to predict 70.81% of the churn behavior. If the focus of the analysis is the customers who are move out of the current service provider, KMeans predicted 2366 out of 2650. This gives us a churn prediction accuracy of 89.28%.\nThe KMeans clustering analysis of customer churn data shows, that this unsupervised learning method with no prior knowledge about the data was able to identify 2 distinct clusters in the data. One of the clusters closely corresponded with the customers who left the company changed their service provider. Another cluster corresponded largely to those customers who chose to stay with the company. Even without any churn-related information provided, the data told its own story when passed through this unsupervised machine learning algorithm. As such, this algorithm can be used for predictive analysis on customer churn data.\nLimitations of KMeans\nWhile KMeans is intuitive, easier to implement, and computationally faster compared to some other clustering mechanisms, it comes with some limitations. The KMeans clustering technique is an unsupervised learning mechanism (no prior labeling of the data). It identifies the clusters in the data based on the distance of points from each other. One of the important limitations of this method is that it doesn’t make use of categorical variables. We know from previous analysis on the customer churn data that some of the categorical variables have been very important in predicting churn behavior, such as StreamingTV, StreamingMovies, Contract, etc. All that information is lost when using Kmeans as the categorical variable plotted as 1s and 0s in N-dimensional space provides no valuable information. Understandably enough, Kmeans never promised to created clusters based on customer churn behavior.\nAs the number of dimensions in the data increase, the ratio of standard deviation to the mean of distance decreases. This makes Kmeans less effective at distinguishing between data points and consequently the assignment of cluster labels.\nAlso, centroids can experience have a high impact on the determination of their values, due to a few high deviation outliers. Additionally, KMeans makes assumptions about the data:\n1. Clusters are spatially grouped — or once cluster can be confined within a sphere\n2. Clusters are of a similar size\nIf any of the assumptions are violated, KMeans fails to create the right clusters.\nThe 2 images above show incorrect clustering (shown by color) by KMeans because the points are not spatially grouped.', 'Presentation on theme: ""What is Cluster Analysis?""— Presentation transcript:\n1 What is Cluster Analysis? Finding groups of objects such that the objects in a group will be similar (or related) to one another and different from (or unrelated to) the objects in other groupsInter-cluster distances are maximizedIntra-cluster distances are minimized\n2 Applications of Cluster Analysis UnderstandingGroup related documents for browsing, group genes and proteins that have similar functionality, or group stocks with similar price fluctuationsSummarizationReduce the size of large data setsClustering precipitation in Australia\n3 What is not Cluster Analysis? Supervised classificationHave class label informationSimple segmentationDividing students into different registration groups alphabetically, by last nameResults of a queryGroupings are a result of an external specification\n4 Notion of a cluster can be ambiguous How many clusters?Six ClustersTwo ClustersFour Clusters\n5 Types of Clusterings A clustering is a set of clusters Important distinction between hierarchical and partitional sets of clustersPartitional ClusteringA division data objects into non-overlapping subsets (clusters) such that each data object is in exactly one subsetHierarchical clusteringA set of nested clusters organized as a hierarchical tree\n6 Partitional Clustering A Partitional ClusteringOriginal Points\n8 Other Distinctions Between Sets of Clusters Exclusive versus non-exclusiveIn non-exclusive clusterings, points may belong to multiple clusters.Can represent multiple classes or ‘border’ pointsFuzzy versus non-fuzzyIn fuzzy clustering, a point belongs to every cluster with some weight between 0 and 1Weights must sum to 1Probabilistic clustering has similar characteristicsPartial versus completeIn some cases, we only want to cluster some of the dataHeterogeneous versus homogeneousCluster of widely different sizes, shapes, and densities\n9 Types of Clusters Well-separated clusters Center-based clusters Contiguous clustersDensity-based clustersDescribed by an Objective Function\n10 Types of Clusters: Well-Separated Well-Separated Clusters:A cluster is a set of points such that any point in a cluster is closer (or more similar) to every other point in the cluster than to any point not in the cluster.3 well-separated clusters\n11 Types of Clusters: Center-Based A cluster is a set of objects such that an object in a cluster is closer (more similar) to the “center” of a cluster, than to the center of any other clusterThe center of a cluster is often a centroid, the average of all the points in the cluster, or a medoid, the most “representative” point of a cluster4 center-based clusters\n12 Types of Clusters: Contiguity-Based Contiguous Cluster (Nearest neighbor or Transitive)A cluster is a set of points such that a point in a cluster is closer (or more similar) to one or more other points in the cluster than to any point not in the cluster.8 contiguous clusters\n13 Types of Clusters: Density-Based A cluster is a dense region of points, which is separated by low-density regions, from other regions of high density.Used when the clusters are irregular or intertwined, and when noise and outliers are present.6 density-based clusters\n14 Types of Clusters: Objective Function Clusters Defined by an Objective FunctionFinds clusters that minimize or maximize an objective function.\n15 Characteristics of the Input Data Are Important Type of proximity or density measureThis is a derived measure, but central to clusteringSparsenessDictates type of similarityAdds to efficiencyAttribute typeType of DataDimensionalityNoise and OutliersType of Distribution\n16 Clustering Algorithms K-means and its variantsHierarchical clusteringDensity-based clustering\n17 K-means Clustering Partitional clustering approach Each cluster is associated with a centroid (center point)Each point is assigned to the cluster with the closest centroidNumber of clusters, K, must be specifiedThe basic algorithm is very simple\n18 K-means Clustering – Details Initial centroids are often chosen randomly.Clusters produced vary from one run to another.The centroid is (typically) the mean of the points in the cluster.‘Closeness’ is measured by Euclidean distance, cosine similarity, correlation, etc.K-means will converge for common similarity measures mentioned above.Most of the convergence happens in the first few iterations.Often the stopping condition is changed to ‘Until relatively few points change clusters’Complexity is O( n * K * I * d )n = number of points, K = number of clusters, I = number of iterations, d = number of attributes\n24 Evaluating K-means Clusters Most common measure is Sum of Squared Error (SSE)For each point, the error is the distance to the nearest clusterTo get SSE, we square these errors and sum them.x is a data point in cluster Ci and mi is the representative point for cluster Cican show that mi corresponds to the center (mean) of the clusterGiven two clusterings, we can choose the one with the smallest errorOne easy way to reduce SSE is to increase K, the number of clusters\n25 Evaluating K-means Clusters Need to incorporate measure of separation between clusters, not only similarity inside each clusterDissimilarity ratio = (inter-cluster distance / intra-cluster distance)Computing dissimilarity ratio from JMP output\n26 Computing dissimilarity ratio from JMP output: Centroid coordinates are given in the output. However each value has to be normalized.Obtain mean and standard deviation of each dimension to compute normalized coordinates of centroids. These can be obtained from Histograms.Distances between centroids X and Y are given byThe inter-cluster distance will be the smallest of the distances between centroids.The intra-cluster distance is the mean distance (obtained from JMP histogram)Compute ratioDissimilarity ratio = (inter-cluster distance / intra-cluster distance)\n27 Two different K-means Clusterings Original PointsOptimal ClusteringSub-optimal Clustering\n32 Problems with Selecting Initial Points If there are K ‘real’ clusters then the chance of selecting one centroid from each cluster is small.Chance is relatively small when K is largeIf clusters are the same size, n, thenFor example, if K = 10, then probability = 10!/1010 =Sometimes the initial centroids will readjust themselves in ‘right’ way, and sometimes they don’tConsider an example of five pairs of clusters\n33 10 Clusters ExampleStarting with two initial centroids in one cluster of each pair of clusters\n34 10 Clusters ExampleStarting with two initial centroids in one cluster of each pair of clusters\n35 10 Clusters ExampleStarting with some pairs of clusters having three initial centroids, while other have only one.\n36 10 Clusters ExampleStarting with some pairs of clusters having three initial centroids, while other have only one.\n37 Solutions to Initial Centroids Problem Multiple runsHelps, but probability is not on your sideSample and use hierarchical clustering to determine initial centroidsSelect more than k initial centroids and then select among these initial centroidsSelect most widely separatedPostprocessing\n38 Pre-processing and Post-processing Normalize the dataEliminate outliersPost-processingEliminate small clusters that may represent outliersSplit ‘loose’ clusters, i.e., clusters with relatively high SSEMerge clusters that are ‘close’ and that have relatively low SSE\n39 Limitations of K-means K-means has problems when clusters are of differingSizesDensitiesNon-globular shapesK-means has problems when the data contains outliers.\n40 Limitations of K-means: Differing Sizes Original PointsK-means (3 Clusters)\n41 Limitations of K-means: Differing Density Original PointsK-means (3 Clusters)\n42 Limitations of K-means: Non-globular Shapes Original PointsK-means (2 Clusters)\n43 Overcoming K-means Limitations Original Points K-means ClustersOne solution is to use many clusters.Find parts of clusters, but need to put together.\n44 Overcoming K-means Limitations Original Points K-means Clusters\n45 Overcoming K-means Limitations Original Points K-means Clusters']"	['<urn:uuid:91b1a86f-c683-40a8-8fd3-f4873d3e1643>', '<urn:uuid:349fe84e-08a8-4330-bad3-d037e510c940>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T22:13:20.996306	11	77	4117
51	How do different types of climate records show the relationship between temperature changes and atmospheric composition throughout Earth's history?	Ice core analysis reveals that carbon dioxide and methane concentrations in the atmosphere closely follow temperature patterns - when temperatures were cold, these gases were present in low concentrations, and when temperatures were warm, their levels were high. This is evidenced in Antarctic ice cores, where trapped air bubbles from 2.7 million years ago showed CO2 levels of 300 ppm, much lower than current levels. Scientists can track these historical relationships through various methods including oxygen isotope ratios in marine sediments and analyzing gas compositions in ice cores.	"[""Climate Change · Part One\nClimate Change · Part Two\nClimate Change 2 Syllabus\n1.0 - The Ice Ages: An Introduction\n2.0 - Discovery of the Ice Ages\n3.0 Ice Age Climate Cycles\n· 3.1 - Milankovitch Theory Supported\n· 3.2 - Ice Core Science\n· 3.3 - The Speed of Deglaciation\n· 3.4 - Lessons from the Ice Ages?\n4.0 - Climate Through the Last 1000 Years\n5.0 - Determining Past Climates\n6.0 - Causes of Millennial-Scale Change\n7.0 - Climate and CO2 in the Atmosphere\n8.0 - Recent Global Warming\n9.0 - Climate Change in the Political Realm\n10.0 - The Link to the Ozone Problem\n11.0 - Future Energy Use\n12.0 - Outlook for the Future\nIntroduction to Astronomy\nLife in the Universe\nGlossary: Climate Change\nGlossary: Life in Universe\nIce Core Science\nand Fluctuating Temperatures\nThe best way to get an impression of what Canada and Scandinavia looked like during the last ice age is to fly over Greenland. From high up, the frozen white endless desert in the interior seems utterly hostile to the observer; the eye wanders in search of markers to fasten on but fails to penetrate the cold haze. The margins, on the other hand, are spectacular, with dramatic crags and steep-walled ice-filled valleys where the glacier tongues leave the main body and seek their way to the ocean, feeding it with icebergs. A geologist can readily sense the relentless motion of these ice streams running down into the fjords, at an imperceptibly slow speed, but powerful in purpose, removing what is in their way and carving ever deeper ravines into the hard host rock.\nCartoon of water molecules\n, with and without deuterium\n. Remember that the oxygen atom\nin each of these can also vary its isotope value as well, either as O-16 or O-18. Since the coldest snow is the lightest (containing the least D and O-18), analysis of these isotopes\nin polar ice allows for the reconstruction of past temperatures\nThe icy wastelands of Greenland and Antarctica have something extremely precious to offer, which makes some scientists want to spend months at a time there, even while enduring hard labor: the memories of the ice. As was described in Lesson 4, year after year the layers of snow turn to ice, as new layers are added on top. The frozen water contains a record of the conditions under which the snow fell, in the form of the isotopic composition of the ice. The isotopes of interest are hydrogen (H) and its heavy sibling deuterium (D), as well as oxygen-16 and oxygen-18, which have been described previously in connection with the deep-sea record in foraminifers. Water vapor turns to precipitation over the polar ice sheet more readily when it has the composition HOD and H18OH than if it is normal water, H16OH. As air cools upon climbing up an ice shield, water changes phase from vapor to liquid, thus losing D and 18O preferentially. This means that the coldest snow has the least D and 18O in it.\nWith this basic information (and some statistics and isotope chemistry) we can extract a temperature record from the ice on Greenland for the last 100,000 years. For Antarctica, a record going back 400,000 years has been reconstructed. To this end, scientists employ heated drills, which penetrate the ice layers one after one from the surface on down, and recover cylindrical cores of ice one after another until the bottom of the ice is reached. The ice cores are then sampled, and the composition of the water is measured in the laboratory using mass spectrometers. Then, when plotting the ratio of D/H or 18O/16O, a record of temperature change emerges. (Actually, as described in Section 7.0, what is plotted is the percent deviation of the isotope ratio from the value of a standard, which simplifies the relationship to temperature.)\nAncient Glacial Ice Composition\nThe founding event of ice-core science may be taken as the publication of several articles in 1969 by the Danish chemist Willi Dansgaard, professor in Kopenhagen, Denmark, and his collaborators. Their first important result was the determination of the composition of Greenland ice for the last 100,000 years. Assuming that glacial-age ice had a similar ratio of O-18 to O-16, this is a much better guess than Emiliani's Chicago snowfall model, and the composition of the glacial ocean could now be guessed with much greater confidence. It turned out that the deviation from present-day conditions was really twice greater during the glacial period than had been postulated by Emiliani, so that much of the variation of the oxygen isotope signal detected by Emiliani had to be ascribed to the effects of ice-buildup and decay rather than to temperature.Just as important was the second finding that the oxygen isotope composition of glacial ice varies with time, and that it faithfully reflects glacial and interglacial conditions. To bring out this pattern, Dansgaard and his co-workers had to date the raw ice record with simple flow model based on the movement of large glaciers. As the ice sheet maintains the balance between thickening on top, and moving horizontally toward the sea, the patterns of flow are such as to maintain thick layers in the upper half of the glacier (which gets carried along by the flow below) and greatly thinning the layers in the more mobile lower half, and especially on approaching bedrock.\nThe third important result is that the temperature variations are much larger for glacial time than for interglacial time. This suggests that the climate was much more fickle when large ice sheets covered Canada and Scandinavia, compared with periods when these areas were free of ice. The presence of ice sheets did not stabilize climate (as one might expect) but introduced instability. Many geologists believe that this instability came from the ice itself. From time to time small changes in climate led to sudden surging of large glaciers which covered much of the North Atlantic with icebergs. Such events, presumably, came suddenly and unannounced. They produced severe winters and poor summers, which was bad for growing things, and resulted in starving mammoths and other large mammals. If this is so, the herds of the large mammals in North America and in Eurasia were in worse shape than ever at the end of the last ice age, around 15,000 years ago.\nA record of temperature change in Greenland, based on isotopes\nOther Information from Ice Cores\nBesides temperature, many other types of information can be extracted from ice cores by analyzing wind-blown dust, volcanic matter, and trapped air. The dust content of glacial ice in Greenland is more than ten times greater than that in ice made during warm intervals. This suggests that glacial periods tended to have dry weather and strong winds, in the regions south of the ice shields. (Cold air holds much less water vapor than warm, so this is not surprising.) During a cold dust storm in the plains, the thick matted hair of mammoth must have been a real life-saver.\nA record of volcanic activity is preserved in the amount of acid found in the ice. Some layers are very rich in acid, presumably from the sulfuric matter entrained by the snow at the time, in the atmosphere, after an eruption. These intervals also tend to show low temperature! This relationship strongly suggests that volcanism influences climate. More volcanism means cooler summers and more severe winters. The resulting catastrophic weather, following such eruptions, must have greatly stressed the populations of mammoth and other large mammals that need much food and cannot hide in burrows and caves. (In light of this data it cannot be entirely excluded that Icelandic sagas refer to such events in a legend about the end of the world, which includes a terrible winter lasting three years. If they do, their tribal tradition would have to reach back 11,000 years! However, the legend could easily refer to more recent events such as the eruption of Santorini, 3500 years ago.)\nIn addition, the air trapped in the ice can be analyzed for trace gases such as carbon dioxide and methane. This was done in laboratories in France (in Grenoble, by the physicist Claude Lorius and his co-workers) and in Switzerland (in Bern, by the physicist Hans Oeschger and his team). Results show that the carbon dioxide content of the atmosphere closely follows the ups and downs of temperature. Whenever it was cold, carbon dioxide and methane were low in concentration, whenever it was warm, they were both high."", 'All little children are taught that a tree’s age can be estimated by the number of rings in its trunk. But to modern science, a closer examination can read humidity, rainfall, and temperature, flood and fire events, even the direction of the jet stream. In this light, says the New York Times, trees can be viewed as “organic recording devices that contain information about past climate, civilizations, ecosystems,” and more.\nNotes the Times, “At this particular time, the most essential role for tree rings is probably their use in reconstructing past climate and providing much greater context.”\nThat is critical in order to understand how climate’s change, and to use that information to postulate how we can best manage the climate of today – and the even more dynamic one perhaps still to come.\nBristlecone pine tree in California, U.S.A.\nSo far, so good. But the oldest recorded trees, the bristlecone pines of the western U.S., go back about 5,000 years, with data from dead trees that goes back thousands more. That’s old for organisms, but information that may help scientists effectively model and make informed conjectures about climate requires much older data. The most recent ice age, as we discussed in our article The Cries of the Cryosphere, ended 7,000 years before the oldest extant tree. No rings, no data.\nYet climatologists teasing out the extraordinarily complex systems and conditions – encompassing gas levels, air currents, saline conditions, flora and fauna types, volcanic conditions, temperatures, meteor strikes, and more – have more than trees at their disposal.\nNational Ice Core Lab (NICL) storage freezer. Image by NICL. Source: Wikipedia\nOne of these, ice cores, can be incredibly revealing, for they too have layers that reveal age and climate conditions, but of far, far greater lineage. Just a few years ago an Antarctic ‘blue ice’ core was taken that was found to be more than 2.7 million years old. Bubbles in the ice held intact gasses, making it a trove of data on “greenhouse gasses from Earth’s atmosphere at a time when the planet’s cycles of glacial advance and retreat were just beginning.” As one researcher explained in Science, “This is the only sample of ancient Earth’s atmosphere that we have access to,” and the CO2 data it contains – at 300 ppm, much less than today’s load – is helping understand how such levels correspond with various other conditions at the time.\nOne-meter marine sediment cores taken by the research vessel Polarstern.\nThe sediments date back about a million years. Photos by Hannes Grobe.\nImpressive, right? But for scientists who take the long view, marine sediment cores are the way to go. Some of these sediments, via analysis of their oxygen isotope ratios among other things, can paint a vivid picture of historical climate conditions. Two isotopes in particular (16O and 18O) occur in various ratios in water depending on climate, so a comparison of these isotopes in samples suggests the climatic conditions extant at the time.\nOne such core, from seabed 5,700 meters (18,700 feet) below sea level, rendered living organisms – eight different groups of known bacteria were represented – that had lain dormant, it is believed, for more than a hundred million years. When fed, these microbes began to divide, just like it was any other Tuesday during the cretaceous period.\nPhoto by Kohei Tanaka / Unsplash\nRock strata, too, inform almost all natural sciences, and reveal terrestrial conditions billions of years into the past, almost from the earth’s formation.\nTrigonium species, a marine diatom from Bay of Villefranche, France.\nWhether drawn by tearing the secrets from crushed 100-million-year-old diatomaceous sediments in the ocean depths, or the gaseous bubbles in cores of Antarctic ice, scientists are working diligently, applying painstaking research and forensic evidence to protect the planet, and the living things on it, into the future.']"	['<urn:uuid:78eec99d-48a3-4dd4-8d66-4d139f9062d5>', '<urn:uuid:05e5ddff-53b7-46e4-a915-9c807868302a>']	factoid	direct	verbose-and-natural	distant-from-document	three-doc	expert	2025-05-12T22:13:20.996306	19	88	2066
52	What skills do I need for a computer engineering career?	For a computer engineering career, you need good knowledge of C/C++ programming languages, database management, and computer networking skills. The field requires understanding both theoretical concepts and practical applications, including digital electronics, embedded systems, and networking protocols. Specific technical skills include circuit analysis, semiconductor devices, digital systems, programmable logic controllers, and networking. Additionally, you should be able to work in multidisciplinary teams, communicate effectively, and solve technical problems. These skills prepare you for roles such as engineering consultant, computer engineering technologist, or project manager.	"['Tárgy hivatalos adatlapja:\nGood knowledge of the C / C ++ programming language.\nRequired/suggested previous courses\nThe course can be attended by students who have completed the course in Basics of Programming II.\nSynopsis and Objectives\nThis course is a basic BSc course based on previous terms. The main goal is to provide a comprehensive approach to the following areas: computer networking, database management, data processing, programming theory, and formal languages. Both theoretical and practical approaches are presented in order to help students in their research activities as electrical engineers. As a result, students become able to detect, analyze and manage problems that arise in network systems, database engines, and simple human-computer and computer-to-computer interfaces.\nThe curriculum of laboratories continuously follows the lectures.\nDetailed Syllabus of the Course\n1. Database Management\n- General concepts and components of a database system. Data independence, architectural layers.\n- An entity, relational descriptions, binary and many-valued relationships. Data storage, representation.\n- Database operations (insert, delete, update). Relation, domain, attribute, key concepts. Integrity rules, operations on relations. Relational database.\n- The concept of functional dependency. 0, 1, 2, 3, Boyce-Codd normal forms, many-valued dependencies.\n- Introduction to SQL. Creating a table, and inserting, modifying, deleting data. Database search. Static and dynamic SQL. Data types, null. Queries in SQL; filters, expressions, operators. Grouping, sorting. Complex queries, embedded queries (join). DML and DDL statements.\n- Transactions and basics of transaction management. Data consistency.\n2. Computer Networks\n- Classification of computer networks; network topologies, structures, architectures.\n- Basic concepts. The OSI model and TCP/IP model layers.\n- Communication channel. Ideal and real transmission line.\n- Error detection and correction. Error detecting and error-correcting code. The CRC code.\n- Wired and wireless communication. Point-to-point connections.\n- Channel coding. Some well-known codes (e.g., Manchester). Fundamental digital modulation methods.\n- Addressing modes. Connection lifecycle. Connection-oriented and connectionless communication.\n- Virtual networks (VPN). Forwarding nodes, routing, congestion problem.\n- Media protocols. CSMA/CD, IEEE 802.3 Ethernet, Token Bus, Token Ring, etc.\n- Analog and digital communications. Modems.\n- TCP/IP protocols stack. Addressing. Basic concepts of routing. Static and dynamic routing algorithms.\n3. Formal Languages and State Machines\n- Basic concepts of formal languages. BNF. Languages generated by the grammar. Classes, Chomsky classes.\n- Finite state machine (FSM) and linear language acceptor. FSM and regular languages. FSM transformations.\n- Context-free grammars. Derivation. The concept of LL(k) languages, LL(k) parser.\n- General concepts and functions of a compiler. Lexical and syntactic parsers. High-level language elements (case study).\nLaboratories and dates in 2022:\n- Feb. 23: Database design: entity-relationship diagrams\n- Mar. 09: SQL language: DDL and DML\n- Mar. 23: SQL language: advanced queries to multiple tables\n- Apr. 06: Transaction management, problems of concurrent access\n- May 04: Introduction to Wireshark and network monitoring\n- May 18: Practical approach to formal languages. Regular expressions\nPlace and time: IL206 (bldg. I, 2nd floor), 10:15-12:00 a.m. Wednesday\nEducation methods of the course\nLectures, laboratory exercises.\nIn lecture term:\n- The continuity of learning is checked by Placement Tests at laboratories, which is a pass/fail check. If a student fails the Placement Test, he cannot take part in the computer laboratory (in other words his attendance is null).\n- In one week after each laboratory exercise, the student should submit a Laboratory Report that requires the documentation of tasks and exercises, including system and algorithm designs, test cases, and theoretical background if needed. It should be submitted electronically on the course webpage. The student must always follow the directions of the Laboratory Instructor (if any), or their Report will not be accepted. Please note that Laboratory Reports may not be submitted by e-mail and/or after the deadline for any reason!\n- The quality of learning is checked by a Midterm Test that will take place during the semester.\n- The Final Exam takes place at the end of the semester.\nThe course is finished successfully, if\n- The student is a regular attendant at the lectures and laboratories; the absence cannot exceed 30% as it is defined in the Code of Studies and Exams (CSE), Article 14, Point 3. Absence at lectures and laboratories is taken separately.\n- The student has completed all laboratory exercises, and all corresponding Laboratory Reports are accepted by the laboratory instructor.\n- The Midterm Test grade is at least a ""pass"".\n- The Final Exam grade is at least a ""pass"".\nAll of the above conditions must be fulfilled; otherwise, the term grade is unsatisfactory.\nIf all these conditions are fulfilled, the grade is calculated as follows:\n- The final grade is a weighted average of a Midterm Test grade (30%), and a Final Exam grade (70%).\nThe grade can be modified by the instructor with +/-0.5. The instructor can make this decision based on a student\'s performance at the lectures and the quality of the submitted Laboratory Reports. Rounding grade to the nearest integer number will be done using usual algebraic rules.\n- It is the responsibility of each student to complete all assignments according to the requirements set by the Instructor and to submit only work that is his or her own. Submitting others\' assignments is a big academic sin; by doing that you are committing fraud. All assignments will be automatically and manually compared against each other.\n- All parties involved in plagiarism and cheating will be prosecuted no matter who copied from whom. It is your duty to keep your code secure. The incident of this kind will be reported to the respective academic authority for measures to be taken.\n- The attendance register is circulated regularly at classes, and it is your responsibility to ensure that you sign it. You can sign a register in the first 5 minutes after the start of classes. It is an offense to sign a register on behalf of someone else, and anyone found doing so may be investigated for fraud.\n- A student who is unsure whether an action constitutes an offense, or who needs help in learning how to avoid offenses (e.g., plagiarism, cheating) or about rules for group work/collaboration should seek guidance from the course instructor or academic advisor.\n- Code of Studies and Exams of BME can be found here.\nIn this course, we want to encourage collaboration and the free interchange of ideas among students, and in particular the discussion of laboratory exercises, assignments, etc. However, we do not allow plagiarism, which, as commonly defined, consists of passing off as one\'s own ideas, words, writings, etc., which belong to another. In accordance with this definition, you are committing plagiarism if you copy the work of another person and turn it in as your own, even if you should have the permission of that person.\nMissed Assignments Information\nThroughout the semester there are 6 Laboratory Exercises, but only 5 of them are required for getting the signature. Therefore, the Labs cannot be retaken, because not all of them are required. The unsatisfactory Midterm Test can be retaken once. The unsatisfactory Final Exam can be also retaken once.\nTextbooks and Weblinks\n- Andrew S. Tanenbaum ""Computer Networks"".\n- Jeff D. Ullman, Jennifer Widom ""A first course in Database Systems"".\n- Hector Garcia-Molina, Jeff Ullman, Jennifer Widom ""Database Systems: The Complete Book"".\n- Web: SQL for Nerds by Philip Greenspun, link.\n- Alfred V. Aho, Ravi Sethi, et. al. ""Compilers: principles, techniques, and tools"", Chapters 1-4.\n- The Visual Automata Simulator is a tool for simulating various automata. It also makes nice drawings of them. The JFLAP applet is a similar kind of tool with more sophisticated capabilities.\n- Introduction to Theory of Computation by Michael Sipser. This textbook covers automata theory, computability theory, and complexity theory. It also contains exercises, problems, and solutions.\nNincsenek aktuális hírek a tárgyhoz. Ha a később megjelenő hírekről azonnal értesülni szeretne, iratkozzon fel az\nRégebbi hírek »\nA tárgyhoz nincs feltöltve jegyzet.', ""Mechatronics (ESET) Program Overview\nESET graduates function in multidisciplinary teams to design, install, maintain, and repair systems, components, or processes meeting specific needs for engineering applications. They serve as a link between engineers and technicians in the workplace where they play a key role from the conception of electronic systems until their implementation. They are involved in the development, testing, production, and quality assurance of electronic and mechanical components and/or systems. Additionally, if you'd like to learn about the manufacturing gap creating career opportunities in Mechatronics view our infographic, here.\nThe curriculum provides ESET graduates with the education and foundation needed for employment in a variety of industries in the private and public sector, including the computer industry, homeland security, automation and manufacturing, and education.\nMechatronics concentration graduates are employed in a wide spectrum of areas in positions such as engineering consultant, electrical engineering or computer engineering technologist, product engineer, or project manager.\nThrough ECPI University's year-round schedule, you could earn a Bachelor of Science Degree in Electronic Systems Engineering Technology with a concentration in Mechatronics in as little as 2.5 years.\n52 semester credit hours\nCourse ID Course Name Credits ELECTRICITY EET110 Electric Circuits I 3 ESET111 Electric Circuits II 3 ESET111L Electric Circuits LAB 1 EET310 Circuit Analysis 1 ANALOG ELECTRONICS EET120 Semiconductor Devices 3 EET121 Electronic Systems Applications 3 EET220 Industrial Applications 3 EET221L Instrumentation and Measurement LAB 1 DIGITAL ELECTRONICS EET130 Digital Systems I 3 EET230 Digital Systems II 3 EET230L Digital Systems LAB 1 NETWORKING CIS150 Introduction to Computer Networking 3 PROGRAMMING CIS126 Introduction to Programming 3 EET207 Applied Engineering Programming 3 CONTROL SYSTEMS EET231 Introduction to Programmable Logic Controllers 3 EET231L Introduction to Programmable Logic Controllers LAB 1 EET331 Programmable Logic Controllers and Robotics 3 EET331L Programmable Logic Controllers and Robotics LAB 1 SENIOR PROJECT MET400 Senior Project 3 MET400L Senior Project LAB 1 EMBEDDED & DRIVE SYSTEMS ONE LECTURE COURSE AND CORRESPONDING LAB FROM THE FOLLOWING: EET390 Motor Drives 3 EET390L Motor Drives LAB 1 EET430 Microcontrollers 3 EET430L Microcontrollers LAB 1\nArts and Sciences\n37 semester credit hours\nCourse ID Course Name Credits CAP480 Arts and Sciences Capstone 3 COM115 Principles of Communication 3 ENG110 College Composition 3 ENG120 Advanced Composition 3 HUM205 Culture and Diversity: Exploring the Humanities 3 MTH131 College Algebra 3 MTH200 Pre-calculus 3 MTH220 Applied Calculus I 3 MTH320 Applied Calculus II 3 PHY120 Physics 3 PHY120L Physics LAB 1 CHOOSE TWO COURSES: ECO201 Macroeconomics* 3 ECO202 Microeconomics* 3 PSY105 Introduction to Psychology* 3 PSY220 Positive Psychology* 3\n*For allowable substitutions of arts and sciences courses, see the Arts and Sciences Department page\nSelf Integration and Computer Literacy\n9 semester credit hours\nCourse ID Course Name Credits CIS101 Computer Configuration I 3 COR090 Career Orientation Seminar 0 EET102 Engineering Math & Software Applications 3 FOR110 Essentials for Success 3\nElectronic Systems Engineering Technology Concentration Requirements\n16 semester credit hours\nCourse ID Course Name Credits EET251 Computer Configuration II 3 EET251L Computer Configuration II LAB 1 EET320 Semiconductor Processing 3 EET380 Digital Communications I 3 ESET280 Introduction to Communications Systems 3 CHOOSE TWO COURSES: EET252 Data Communications and Networking 3 CIS225 Networking Protocols and Services 3\nMechatronics Concentration Requirements\n16 semester credit hours\nCourse ID Course Name Credits EET191 Materials Science 3 EET192 Engineering Graphics Communications 3 EET192L Introduction to 3-D Modeling LAB 1 MET211 Statics 3 MET230 Hydraulics and Pneumatics Systems 3 MET410 Dynamics 3\n10 semester credit hours\nCourse ID Course Name Credits BUS102 Fundamentals of Customer Service 3 BUS121 Introduction to Business 3 BUS242 Technology Optimization 3 BUS328 Business Process Improvement 3 BUS328L Business Process Improvement LAB 3 BUS472 Applied Project Management 3 BUS472L Applied Project Management LAB 1 CIS106 Introduction to Operating Systems 3 CIS115 Computer Applications 3 CIS121 Logic and Design 3 CIS123 Introduction to Scripting 3 CIS126L Introduction to Programming LAB 3 CIS202 Introduction to Routing and Switching 3 CIS204 Intermediate Routing and Switching 1 CIS206 Linux Administration 3 CIS207L Network Routing and Switching LAB 1 CIS214 Object-Oriented Programming Using C# 3 CIS215 Object-Oriented Programming with C++ 3 CIS218 Object-Oriented Programming Using Java 3 CIS226 Introduction to Object Oriented Programming 3 CIS228 Service Desk Fundamentals 3 CIS245 Windows Client and Server 3 CIS245L Windows Client and Server LAB 1 CIS282 Web Interface Design 3 EET272 Fiber Optics Communication 3 EET272L Fiber Optics Communication LAB 1 EET301 Special Topics in Engineering Technology 3 EET302 Externship-EET Sr. III 3 EET306 Externship-EET Sr. I-a 1 EET307 Externship-EET Sr. I-b 1 EET308 Externship-EET Sr. I-c 1 EET309 Externship-EET Sr. II 2 EET350 Overview of Electronics Security Devices 3 EET352 Engineering Economics 3 MET114 Introduction to Geometric Dimensioning and Tolerancing (GD&T) 3 MET213 Advanced 3-D Modeling 3 MET221 Manufacturing Processes 3 MET313 Applied Strength of Materials 3 MET230L Hydraulics and Pneumatics Systems LAB 1 MET330 Applied Fluid Mechanics 3 MET330L Applied Fluid Mechanics LAB 1\n- Program Description\nThe ESET - Mechatronics concentration - program focuses on real-world applications of engineering principles. The program provides a career pathway for individuals who like working with their hands, putting things together, and combining different systems to come up with a solution for a real-world problem. The Electronic Systems and Mechatronics concentrations offer a broad exposure to analog and digital electronics, engineering programming, instrumentation and measurement systems, as well as embedded and drive systems. Learn by doing while grasping a firm theoretical foundation in electronics. Put to practice your acquired knowledge through a culminating capstone experience.\nWith the new emerging technologies, a skilled workforce in the electronic systems field could continue to be in demand for the design and implementation of new innovative solutions and products.\n- Possible Career Track\nMechatronics graduates are typically employed in areas such as:\n- Engineering Consultant\n- Electrical/Computer Engineering Technologist\n- Industrial Engineer\n- Product Engineer\n- Project Manager\n- Project Coordinator\n- Industrial Technician\n- Field Service Technician\n- Computer Engineering Technician\n- Electronic Engineering Technician\nGraduates of the B.S. Electronic Systems Engineering Technology program are expected to attain the following objectives within a few years of graduation:\n- Apply acquired technical and analytical skills as it relates to their professional positions in electrical, electronic, and related industries.\n- Apply relative mathematical, science, and engineering methods to solve technical problems.\n- Analyze and implement complex systems including both hardware and software.\n- Pursue lifelong learning and successful professional careers.\n- Perform as effective team members through adequate oral and written communication skills.\n- Relate and exercise an educated judgment in regards to their professional and ethical responsibilities.\nStudents in the B.S. Electronic Systems Engineering Technology, ESET program learn to design and integrate electronic systems through a strong foundation in analog and digital electronics. They are able to apply the acquired engineering and mathematical principles to implement and improve systems and/or processes for engineering applications.\nUpon completion of the Bachelor of Science in Electronic Systems Engineering Technology, ESET, graduates are expected to have:\n- An ability to apply knowledge, techniques, skills and modern tools of mathematics, science, engineering, and technology to solve broadly defined engineering problems appropriate to the discipline;\n- An ability to design systems, components, or processes meeting specified needs for broadly-defined engineering problems appropriate to the discipline;\n- An ability to apply written, oral, and graphical communication in both defined technical and non-technical environments; and an ability to identify and use appropriate technical literature;\n- An ability to conduct standard tests, measurements, and experiments and to analyze and interpret the results to improve processes; and\n- An ability to function effectively as a member or leader on a technical team.\n- Create 2D and 3D designs for engineering parts using CAD software\n- Implement hydraulic and pneumatic systems\n- Analyze forces and their effects on systems\n- Program Guide\n- Download Program Guide (667.68 KB)""]"	['<urn:uuid:59fd16ec-a877-4f88-a976-7d23be7ad2fa>', '<urn:uuid:b06fe91a-3537-4db2-a76b-155459ec13f9>']	open-ended	with-premise	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T22:13:20.996306	10	84	2602
53	i am new to online marketing what is performance marketing and when do advertisers pay for it	Performance marketing is online marketing and advertising where advertisers pay only when a specific action occurs. These actions can include a generated lead, a sale, a click, and more.	['What is performance marketing?\nPerformance marketing is a comprehensive term for online marketing and advertising programs where advertisers pay only when a specific action occurs. These actions can include a generated lead, a sale, a click, and more.\nPerformance marketing professionals—in agencies, with media companies, publishers, or otherwise—rely heavily on paid marketing channels including:\n- Native advertising\n- Sponsored advertising\n- Affiliate marketing\n- Social media advertising\n- Search engine marketing\nIn this post, we’ll unravel the essentials of performance marketing:\n- How performance marketing works\n- Terms you need to know\n- An overview of the top platforms and channels available\nReady to Promote Your Content? Click Here to Get Started\nHow do performance marketing platforms work?\nLet’s briefly explore how most platforms operate.\nAs is the case with all media (often purchased by media buyers), each channel has a specific audience and offers different types of advertising platforms to reach them. For instance:\n- Facebook, the leading social media channel, offers a variety of options to show your ads to people visiting the Facebook or Instagram platform.\n- Taboola, the leading content discovery network, offers advertisers the ability to reach the readers of tens of thousands of leading online publications.\n- Google displays your ads in search results pages (and across the Google Ads network).\nObviously, no channel shows all the ads available, all the time, to everyone. So how do the different platforms choose what to show? A combination of the following factors come into play based on:\n- Target audience and segmentation—Each ad platform offers ways for you to target your audience in the form of audience segments.\n- Bid—The modern advertising landscape calls upon programmatic capabilities that factor in the amount you’ve agree to pay to show your ad in a specific place and time, to your chosen target audience.\n- Quality and relevance—Trust is the biggest issue people have with advertising of any kind. As such, ad performance is factored. If your ad doesn’t work—that is, it earns low quality ratings—the network will reduce its exposure.\n- Conversion—The economics of performance marketing is based on consumers taking action. When the required action doesn’t take place the network doesn’t get paid. So your ad gets displayed more when it works.\nFacebook advertising (and the associated fees), for example, focuses heavily on the advertiser’s bid, ad quality and relevance , and estimated amount of actions. Most platforms work in a somewhat similar manner.\nPerformance Marketing Strategy\nIf performance marketing is about paying for action, it pays to have a strong grasp on the various strategies most commonly used, and the actions most commonly measured.\n- Cost Per Impression (CPM): The amount an advertiser pays a publisher per one thousand times that their advertisement is shown\n- Cost Per Click (CPC): The amount an advertiser pays only when their advertisement is clicked on\n- Cost Per Sales (CPS): The amount an advertiser pays only when a sale is directly generated by an advertisement\n- Cost Per Leads (CPL): The amount an advertiser pays when they receive a sign-up from an interested consumer as a direct result of their advertisement\n- Cost Per Acquisition (CPA): The amount an advertiser pays when a specific action, such as a sale, a form completion, or a click, occurs\nIt’s worth noting that each of the above actions is an advertising objective and therefore a measure of performance marketing effectiveness. To evaluate your choices, and continue refining them, you’ll need to consider your campaign goals, choice of platforms, costs, and of course, results.\nTop performance marketing channels\nIf performance marketing demands accountability, it follows that performance marketing channels are those that empower you to measure specific actions.\nYou already know how it works, and the basic terms, let’s look at the leading contenders now.\n1. Native advertising\nIt’s a tough pill to swallow, but swallow it you must: most people in your target market are oblivious to digital advertising. They either have filters in place—such as ad blockers—or they filter ads subconsciously, or both.\nNative advertising opens the door to creating click-through opportunities on the websites where your target market consumes content. Native advertising gets 10x more clicks than other forms of online advertising.\nThe linchpins of native advertising are:\n- Offering quality content that provides value.\n- Leveraging smart distribution via the right publishers.\nThe good news regarding the second requirement is content discovery platforms such as Taboola offer advanced predictive engines that ingest remarkable volumes of data to match viewers with content they’re likely to take interest in.\nThe nuances of launching successful content discovery campaigns via native advertising are nicely presented in this article and summarized in the following five steps:\n- Set a goal—Determine a specific goal and define essential metrics to gauge progress.\n- Right-fit the content—Aim to build campaigns around content with the potential to inspire action.\n- Prepare your launch—Launch preparation involves selecting content, audience filters and budget parameters.\n- Optimize—Fine-tune campaign performance by analyzing the data, identifying the publishers driving the best results, and adjusting your budget accordingly.\n- Re-align and revise—Review how campaign performance compares to your original goal and look for ways to make it even more efficient with granular variables like time of day, site, and device type. Consider how content discovery might support additional efforts throughout the marketing funnel.\n2. Sponsored content\nMuch like content discovery, sponsored articles are another form of native advertising considered to be content performance marketing plays. Sponsored articles can drive qualified traffic and conversions and increase overall online visibility in front of a highly targeted audience.\nNative advertising expert Chad Pollitt writes:\n“Sponsored articles have opened the door for brands to become part of natural conversations with consumers within the realm of their trusted environments—the publications they go to for news, education, and entertainment on a regular basis.”\nExecuted properly, sponsored articles have proven to be a financial win for both advertisers and publishers. In the Media Buyer’s Guide to Sponsored Editorial Content, Pollitt explains:\n- Sponsored content means the publisher is a media outlet.\n- The long-form editorial nature of sponsored articles distinguishes them from their short-form sponsored social counterparts.\n- Association with a publication and exposure to its audience increases brand awareness, web traffic, conversions, and the likelihood of profitable consumer buying behavior.\nA great example will be a Hear.com campaign.\nHear.com reached out to Taboola to partner on a performance marketing campaign utilizing sponsored content to create awareness around hearing loss and possible solutions.\nThrough educational-focused sponsored content, Hear.com was able to target specific geographical regions and increase traffic tenfold over two years.\n38 Case Studies From Brands That Have Succeeded With Taboola\n3. Social media advertising\nIt’s no secret that it has become far more difficult to achieve measurable results with organic reach via social media networks.\nConsequently, agencies and media buying professionals are investing in social media advertising to play a role in their performance marketing programs.\nSocial networks provide you clear metrics to measure KPIs such as clickthrough rate (CTR), cost per click (CPC), and your overall ROI. As is the case with all performance marketing efforts, your starting point is to establish your objectives.\nReasonable objectives for social advertising include increasing:\n- Lead generation\nThe practical next step is to select the social platforms you’ll begin with. Consider the following:\n- How the demographics of the network aligns with the brand.\n- The traction you’ve gained on each network with your organic efforts.\n- How your competition approaches the various social channels.\n- Ad formats that can be used on the network.\nOf course, the list of social media channels you can advertise on is long and includes Facebook, Instagram, Twitter, LinkedIn, YouTube, Snapchat, and Pinterest.\nWe won’t get into specific performance marketing campaign structures for each individually, however, consider the following best practices:\n- Create specific target audiences by diving into the targeting options provided. You’ll twist the dials going forward as you hone in on your audience.\n- Strive to create ads that blend well with the organic content found on each network. Blatantly promotional efforts tend to backfire on social.\n- Take time to understand the pros and cons of the various formats offered.\n- Experiment with your bidding strategy and optimize them over time to increase ROI.\n- Refresh your ads and landing pages often.\n4. Search engine marketing\nTechnically, the practice of search engine marketing (SEM) includes efforts to increase visibility and clicks via organic and paid advertising. In this post, we’ll address only paid, which is another viable performance marketing channel.\nThanks to the Google AdWords program, SEM has been the most popular performance marketing channel for decades. In 2017, Google’s ad revenue amounted to more than 95 billion US dollars.\nAdvocates of SEM will tell you its strength is it enables advertisers to put their ads in front of customers who are ready to buy.\nKeywords are the foundation of SEM and choosing them wisely is a bit of a science. You’ll benefit from conducting comprehensive keyword research as part of your strategy aiming to identify keywords that are relevant to your brand, which prospective customers are likely to use when searching.\nFor obvious reasons, keywords with high commercial intent that include terms such as buy, discount, deal, coupon, and free shipping, are more competitive and thus, pricier.\nOn that note, it’s important to understand SEM is based on a real-time auction system. That is, an auction process takes place every time someone enters a search query. To be entered into the auction, advertisers identify keywords they’ll bid on and determine how much they are willing to spend for a click.\nThe process is a complex one, but understanding it is critical to achieving high ROI. WordStream is a leading vendor in the SEM software space and offers deep resources for mastering the bidding process at its PPC University site.\n5. Affiliate marketing\nAffiliate marketing is an established form of performance marketing. In this popular model, a publisher becomes an affiliate by establishing a relationship with an advertiser to produce traffic and sales (or an agree action).\nAffiliates send traffic to advertisers and receive a commission in exchange for an agreed upon action (a sale, in most cases). Affiliate publishers essentially act as an extension of your brand using their site to sell your goods and services to users.\nAny website operator can be an affiliate. Any brand operating online can be an advertiser though in most instances, the advertiser has an ecommerce brand or sells a solution via the web.\nAccording to a recent report on the state of affiliate marketing from Business Insider:\n- Affiliate marketing is growing and accounts for approximately 15% of the digital media industry’s revenue.\n- Affiliate marketing now drives as many e-commerce orders in the U.S. as email.\n- Publishers often take a native approach to affiliate marketing by embedding product links within organic content.\nTo be successful in performance marketing it’d be convenient to join an affiliate network that meets all expectations when it comes to a big base of users and cooperation perspectives. We’d recommend MyLead because of its multilingual and technical support, well-developed education system, advanced tools and interesting bonuses and promotional codes, thanks to which your brand will be promoted even more willingly.\nAffiliate Marketing Software Platforms\nAffiliate marketing software platforms act as a middleman for advertisers and marketers. Affiliates and an online merchant agree upon a specific action(s) that will result in a payout for the affiliate, and the platform ensures that these transactions are safe, fair, and not left to manual labor.\n11 Best Affiliate Marketing Software Platforms\n- Everflow: A cost effective platform with wide-ranging capabilities and a foolproof UI\n- Post Affiliate Pro: A platform with large scalability, and a simple way for both merchants and affiliates to track success in real-time\n- LinkTrust: A leader in attribution accuracy with a customizable UI\n- CAKE: A cloud-based platform with robust and real-time management tools\n- AffTrack: A mobile-friendly platform with unlimited clicks, easy-to-understand analytics, and 24/7 customer support\n- Trackier: A platform with customizable reports, quick fraud detection, and a wide variety of integration capabilities\n- Voluum: An end-to-end, easy to operate system with advanced tools to track and optimize campaigns\n- Hitpath: A platform with reliable data and one of the simplest-to-use interfaces\n- TUNE: An intuitive platform with robust features to operate and maintain successful affiliate marketing programs\n- Click Inc: A platform with an approachable interface and a hands-on approach to maximizing affiliate campaigns\n- Affise: A performance marketing platform to manage, track, analyze and optimize online advertising campaigns in real-time\nPerformance Marketing Example\nFor example, French jewelry manufacturer and designer PANDORA worked with Taboola on a recent native advertising campaign to boost branding and conversions via content discovery and display advertising methods.\nThese efforts led to shoppers spending more time on the PANDORA website, and over time, increasing the conversion rate up to 130%.\nContent Performance Marketing\nContent performance marketing is the intersection of content marketing and performance marketing, where advertisers create, publish, and distribute content they feel is most relevant to their desired audience while simultaneously working to meet demand, optimize, and measure the success of the content. The three steps involved in content performance marketing are:\n- Target Demand: In this step, advertisers determine where their audience is most likely to be looking for their product or service. It’s also gauging an understanding of the types of content a particular audience likes to consume and creating bespoke content that fits those parameters.\n- Optimize Content: Once content is created to meet a demand, data must be analyzed in real-time to determine how content can become more successful. An example of this is keeping a close eye on SEO.\n- Measure Results: To inform future pieces of content, it’s crucial for advertisers to use learnings from past content performance campaigns. Setting strong KPIs allows advertisers to spend wisely and eliminate guesswork.\nA Quick Recap about the Types of Performance Marketing\nHere’s a concise checklist indicating when to pursue specific performance marketing channels.\n- Native advertising—Content discovery programs enable you to dramatically scale the reach of your content and achieve click-through to specific landing via the websites of publishers. This form of native advertising can be particularly fruitful for brands that are not achieving their goals via search and social media channels.\n- Sponsored content—Another form of native advertising, sponsored content does not immediately drive traffic offsite, but instead to a page on the publisher’s site. Sponsored content is a worthy investment for augmenting a brand’s owned media and can help reach those who tune-out traditional ads.\n- Social media advertising—Although social media advertising can run-up a sizable tab, ad programs offered by Facebook, Instagram, Twitter, LinkedIn and other channels enable marketers to target prospects with great precision and overcome the challenge of gaining exposure via organic posts.\n- Search engine marketing—Pay-per-click programs offered by Google and Bing accelerate a brand’s exposure via search engine results pages for keywords in ways organic SEO cannot. SEM ad programs are reputed to deliver high commercial intent and therefore impressive ROI for brands that have finely tuned their programs.\n- Affiliate marketing—Many brands pursue affiliate marketing and realize a strong return on investment (ROI). It’s essentially free advertising.\nPerformance marketing is a great strategy when you want to be able to measure results, and get them fast.\nTo really hit your business goals, it’s important you build your campaigns strategically—choose the right platform, ad format, optimization goals, focus on the right audience and of course—create an ad that will resonate with your target audience.\nHow to Start a Discovery Campaign From Scratch\nHow to Start a Discovery Campaign From Scratch']	['<urn:uuid:7d3129c0-a6f0-40c1-9769-7cf504699fbe>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T22:13:20.996306	17	29	2625
54	I need to perform some advanced structural calculations on beams for a construction project - is there a tool that can help me analyze things like stress and deflection?	MITCalc includes a straight beams calculation module that can analyze static indefinite beams with constant axis-symmetrical profiles. The software allows for simple beam type definition, supports 20 types of cross sections, and provides graphic illustrations of moment, stress, deflection, and bending angle.	['MITCalc is a multi-language calculation package includes solutions for gearing, belt, springs, beam, shaft, tolerances and many others. MITCalc support 2D and 3D CAD systems, Imperial and Metric units and many international standards ANSI, ISO, EN...\nitem has less then 1 vote\n|OS Support||Windows Install and Uninstall|\n|File Size||48.99 MB ( 50,168 KB )|\n|Published||Nov 07, 2003 (19 years ago)|\n|Updated||Jan 11, 2017 (5 years ago)|\nMultipage TIFF Viewer 3 hours ago\nSEO SpyGlass Enterprise 3 hours ago\nQXPMarkz 3 hours ago\nCSharp Create Excel File Tutorial 3 hours ago\nBuilderSYS 3 hours ago\nSOS Click for Microsoft Office 3 hours ago\nWMI Tools 3 hours ago\nLuxea Video Editor 3 hours ago\nThunderbird Import Mailbox File 4 hours ago\nnanoCAD Plus 174 times\nAntamedia HotSpot Software 171 times\nACDSee 168 times\nElcomsoft Phone Breaker 167 times\nElcomsoft eXplorer for WhatsApp 164 times\nPhotovoltaic System 162 times\nDatabase Workbench Pro 162 times\nTeraByte Drive Image Backup and Restore 158 times\nSAM Broadcaster Cloud 155 times\nMITCalc contains both design and check calculations of many common tasks, such as: spur and bevel gear, worm gear, planetary gear, gear rack, belt and chain gear, bearings, beam, pins, buckling, shafts, springs, bolt connection, shaft connection, welding, tolerance analysis, tolerances, plates, shells, aerodynamics, technical formulas, aerodynamics, ballistics and many others.\nThere are also many material, comparison, and decision tables, including a system for the administration of resolved tasks. The calculations support both Imperial and Metric units and are processed according to ANSI, ISO, DIN, EN, BS, CSN and Japanese standards. It is an open system designed in Microsoft Excel which allows not only easy user-defined modifications and user extensions without any programming skills, but also mutual interconnection of the calculations, which is unique in the development of tailor-made complex calculations.\nThe sophisticated interaction with many 2D CAD systems (AutoCAD, AutoCAD LT, IntelliCAD, Ashlar Graphite, TurboCAD, BricsCAD, ZWCAD, ProgeCAD) allows the relevant drawing to be developed in a few seconds. The support of 3D CAD systems covers Autodesk Inventor, SolidWorks, Solid Edge and Creo.\n|Requirements:||Microsoft Excel (97, 2000, XP, 2010, 2013, 2016...)|\n|Release Info:||Major Update on Jul 11, 2017|\n|Release Notes:||New calculations: Gear rack, Involute splines, Vehicle aerodynamics, External ballistics, Viscosity Improved calculations: Spur gear external, internal, planetary, Bolted connection, Roller bearings, Pins and clevis pins couplings, Beams|\n|Related Tags:||calculation calculations mechanical calculations technical calculations engineer|\nReviews of MITCalc 1.73You can be the first to write a review of MITCalc!\nGeometric design and strength check of bevel gear with straight and helical toothing. Application supports Imperial and Metric units, is based on ANSI/AGMA and ISO/DIN standards and support many 2D and 3D CAD systemsMITCalc Rolling Bearings Calculation I\nSelection, calculation and check of rolling bearings of the company SKF. Application supports Imperial and Metric units, is based on SKF, ISO, ANSI, SAE standards and support many 2D and 3D CAD systems.MITCalc Shafts Calculation\nThe calculation is designed for geometrical design and complex strength check of shafts. Application supports Imperial and Metric units, is based on ANSI, ISO, DIN standards and support many 2D and 3D CAD systemsMITCalc Spur Gear Calculation\nGeometric design and strength check of spur gear with straight and helical toothing. Application supports Imperial and Metric units, is based on ANSI/AGMA and ISO/DIN standards and support many 2D and 3D CAD systems.MITCalc Straight beams calculation\nCalculations of straight, maximum three times static indefinite beams with constant axis-symmetrical profiles. Simple definition of the beam type, 20 types of cross sections, graphic illustration of the moment, stress, deflection and bending angle.']	['<urn:uuid:07b6a092-b27f-4407-9f6f-5f554e0b2fbc>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T22:13:20.996306	29	42	586
55	I'm interested in community learning spaces. How do the educational activities offered at the Adult multi-lingual library compare to the recommended reading approaches for Buddhist texts in terms of supporting learners?	The Adult multi-lingual library offers structured community learning through specific scheduled activities, including Hebrew and English classes as well as guitar classes on Fridays and Saturdays between 16:00-19:00. The Buddhist texts, meanwhile, support learning through various recommended reading approaches, such as using checklists to track progress, following different time commitments (from 10-30 minutes per day), and choosing texts based on experience level. They also suggest using printed books for practice and provide introductions and notes to help readers approach the texts directly, even without much prior knowledge.	['The Adult multi-lingual library\n“The library is a small and parallel world: the books wander between the shelves as their readers wander the world, carrying with them their emotional history.”\n– Marit Benisrael, ARTEAM member\nThe Adult multi-lingual library provides residents with books in their mother tongue, and a safe and welcoming place to read, meet and study. The library contains more than 3,500 books in sixteen different languages – from Hebrew and Arabic to Tagalong, Nepali, Chinese, Hindu, Tigrinya, etc., servicing the different needs of the various migrant communities. There is also a section for children books in all the mother-tongues of the diverse communities in order to support parent’s efforts to maintain their culture from their home countries.\nThe Library was founded based on the belief that reading is a basic right. The library seeks to create for migrants and asylum seekers an “exterritorial space” that enables to raise above the harsh daily reality- to imagine and be inspired.\nThe Library was originally planned as a temporary structure – that can be dismantled – somewhat like a “Magic Box”. The architecture of the library consists of two main structures that resemble a book shelf, and in between them is the heart of the library- a space to read and study, to meet people and to hold spontaneous social gatherings.\nThere is a special way the books are organized in the library;\nSorting by Emotions\nDuring the first two years of the library run, the artist collective that established the Garden Library implemented a unique, participatory, indexing and cataloguing system based on readers’ emotional responses to the books they read.\nWhen returning a book, readers were asked to choose one of seven emotion categories that best described how they experienced it: amusing, boring, bizarre, depressing, exciting, inspiring, or sentimental. A volunteer librarian would then enter the color-coded judgment into the lending database, adding it to the past history of responses represented on the spine of each book. The book was then placed back on the library shelves according to its latest classification.\nIn other words, each book’s “identity,” its categorization and placement, was never fixed, and could change with each new reader. The placement of each book was not decided by popular vote, but by the most recent reader, using a system that everyone could impact and in which every participating reader’s input counted.\nThe cataloguing system perpetually restructured the layout of the book collection, creating at any given point in time a transient “wandering map” that visualized the current composition of readers’ opinions and preferences.\nThe instability and transience of the book arrangement, with each book marked by the record of its emotional history and the history of its wandering between categories and shelves, was intended as an echo of the spirit of the library and the destinies of its founders. In a country where refugees and migrant workers are not granted full recognition or rights, the cataloguing system was designed to be a symbolically empowering gesture that would highlight the readers’ individuality.\nIn addition to exchanging books you can also join other recreational activities at the library;\n• Hebrew and English classes: Fridays and Saturdays between 16:00-19:00.\n• Guitar classes: Fridays and Saturdays between 17:00-19:00.', 'If you are going to use the ebook edition for a daily sutta reading practice, you may want to use a checklist to keep track and have a physical reminder of what you have read. You can download checklists here.\nThis is a quick guide to the Sutta Pitaka as it relates to daily sutta reading practice. You may also want to consult the articles on choosing a text based on your current experience level and time commitment. The list below follows the traditional organization of the canon. See the sources page for information on how to obtain these books, as well as the page Building a Sutta Library. Because it is recommended that we use a printed book for sutta practice, only print books, downloadable PDFs, and some Kindle documents are mentioned. This is not meant to be a comprehensive bibliography. You may want to consult the glossary for unfamiliar terms.\nAll of the books below contain introductions and/or notes that will allow you to approach the text directly even without much knowledge of Buddhism.\nDīgha Nikāya, Long Discourses (D or DN): Contains 34 suttas that range in length from 5 to 47 pages. Many suttas are readily accessible to a newcomer and many are quite deep and detailed. In terms of a daily sutta practice, this text may be best suited to someone who is already familiar with one of the other nikāyas. Published books:\nThe Long Discourses of the Buddha: A Translation of the Dīgha Nikāya by Maurice Walsh, Wisdom Publications. Complete text.\nFound in Handful of Leaves Volume 1, translated by Ajahn Thanissaro. This anthology contains complete translations of eight suttas, and partial translations of two. Print copies from Metta Forest Monastery. Download E-books from DhammaTalks.org.\nMajjhima Nikāya, Middle Length Discourses(M or MN): Contains 152 suttas, most from 5 to 10 pages. This is an excellent text for a newcomer or an experienced practitioner. It is perfectly suited for a one-sutta-per-day practice, about 15-25 minutes each day. For more details, see Majjhima Nikaya as a Daily Practice. Published books:\nThe Middle Length Discourses of the Buddha: a Translation of the Majjhima Nikaya, translated by Bhikkhu Ñāṇamoli, edited by Bhikkhu Bodhi, Wisdom Publications. Complete text.\nFound in Handful of Leaves Volume 2, translated by Ajahn Ṭhanissaro. Contains 76 suttas from the Majjhima Nikāya. Print copies from Metta Forest Monastery. Download E-books from DhammaTalks.org.\nSaṁyutta Nikaya, Connected Discourses(S or SN): Contains thousands of short suttas grouped by topic. There is a wide variety of genres in this collection: verse, prose, questions and answers, stories, doctrinal analysis, similes, etc. Because most of the suttas are short, if one reads one sutta a day, it may require several years to complete this collection. Instead, a fixed reading time may be more appropriate, say from 10-30 minutes per day. If you have the patience and background to move through long series of analytical suttas, this text would work for a beginner, but it may be better suited to someone already familiar with one of the other nikāyas. If you are using this as your first text for practice, you may want to consider using the Handful of Leaves edition. Published books:\nThe Connected Discourses of the Buddha: A Translation of the Saṁyutta Nikāya, by Bhikkhu Bodhi, Wisdom Publications. Complete text.\nHandful of Leaves Volume 3, translated by Ajahn Ṭhanissaro. Contains 370 suttas from the Saṁyutta Nikāya. Print copies from Metta Forest Monastery. Download E-books from DhammaTalks.org.\nAṅguttara Nikāya, Numerical Discourses(A or AN): Contains thousands of suttas mostly one or two pages long. The suttas are grouped by the number of items around which the exposition revolves. For instance, suttas that cover three items are grouped in the Book of Threes; suttas that cover four items are grouped in the Book of Fours, etc. This collection contains lots of rich advice for practice in daily life. The suttas are generally well suited for a newcomer, especially if you use an anthology. If your time to read is limited, this collection would be well suited for a one-sutta-per-day practice. Otherwise you can read from it for a set amount of time each day. Published books:\nThe Numerical Discourses of the Buddha: A Translation of the Aṅguttara Nikāya, by Bhikkhu Bodhi, Wisdom Publications. Complete text. Download an index of the English titles here.\nHandful of Leaves Volume 3, translated by Ajahn Ṭhanissaro. Contains a collection of 333 suttas from the Aṅguttara Nikāya. Print copies from Metta Forest Monastery. Download E-books from DhammaTalks.org.\nKhudhaka Nikāya, Short Books: This nikāya is a group of smaller autonomous books, explained individually below. These texts are all good to use for daily practice. You may want to choose one to use as a backup text if you are doing a more involved practice with one of the Nikayas listed above.\nKhuddakapāṭha (Khp): This is a collection of 10 suttas. Important to read but perhaps not long enough on which to base a daily practice. Published books:\nThe Short Readings (Khuddakapāṭha, Khuddakanikāya 1), Translated by Ānandajoti Bhikkhu. Download from ancient-buddhist-texts.net in English (65kb) or Pāḷi and English (146kb). Look for the download link. Complete text. Complete audio recording available.\nKhuddakapatha: Short Passages, translated by Thanissaro Bhikkhu. Complete Collection. Available from the Metta Forest Monastery as part of the Sutta Nipata publication. Download e-book from DhammaTalks.org.\nDhammapada (Dhp): This is a collection of 423 short verses, grouped into 26 chapters. This is an excellent text for newcomers and experienced practitioners alike. It takes about 4 minutes to read one chapter so it is well suited to someone with a short amount of time available. Even just reading a single verse each day will instill your life with the Blessed One’s wisdom. It is also a good secondary/backup practice text. Be sure to find a translation that is made in line with the tradition that you are practicing. Recommendations for Theravada practitioners are found below. All are complete texts.\nItivuttaka (Itv): This collection contains 112 suttas of prose followed by verse. Most suttas are two pages or less. This is an excellent text for newcomers and experienced practitioners alike. Good for a short one-sutta-per-day practice. It is also a good secondary practice text. If you are new to the sutta, you may want to start with chapter two, read to the end, and then read chapter one. Published Books\nThe Udāna and the Itivuttaka, Two Classics from the Pali Canon, translated by John D. Ireland, Buddhist Publication Society (BPS) Complete text.\nItivuttaka: This was said by the Buddha, translated by Thanissaro Bhikkhu. (revised Jan. 17, 2017) Complete text. Print copies from Metta Forest Monastery. Download E-books from DhammaTalks.org.\nSutta Nipāta (Sn or Snp): Seventy one sets of verses, sometimes preceded by a prose story. Many of these suttas will be easily accessible to the newcomer; many of them are deep and profound. To use as a daily practice this collection may be better suited to someone with a background in the concepts of Theravada Buddhism. Good for a one-sutta-per-day practice. With this text especially, expect to spend some time in contemplation. Published Books:\nThe Suttanipata: An Ancient Collection of the Buddha’s Discourses Together with Its Commentaries, by Bhikkhu Bodhi, Wisdom Publications. Complete text. This edition also contains a translation of the ancient commentary. (This actually take up the majority of the book.)\nThe Rhinoceros Horn and Other Early Buddhist Poems (Sutta Nipāta), translated by K. R. Norman, with alternative translations by I. B. Horner and Ven. Walapola Rahula, Pali Text Society. Paperback edition available. Complete text.\nVimānavatthu (Vv) and Petavatthu (Pv): Stories of devata mansions and ghosts. Would be a good text for practice by someone with knowledge of Theravada concepts. Published Books:\nStories of Heavenly Mansions from the Vimanavatthu. Mahamegha. This is a new translation in very simple modern language. Available in print and Kindle. Complete Translation.\nStories of Ghosts from the Petavatthu. Mahamegha. This is a new translation in very simple modern language. Available in print and Kindle. Complete Translation.\nMinor Anthologies Vol. IV : Vimānavatthu (Stories of the Mansions) and Petavatthu (Stories of the Departed). This is a single volume of both books. ISBN 13: 978-086013073-4 Published by the Pali Text Society. The translation is quite readable. Complete translation including excerpts from the commentary. This translation is closer to the Pali than the two listed above.\nTheragāthā (Thag) and Therīgāthā (Thīg): Verses of Arahant Bhikkhus and Bhikkhuṇis. Two excellent collections for practice. The ultimate source for inspiration and reminder of the goal of the practice. Just reading a few verses a day can be beneficial. Consider reading a few verses each day as a supplement to any practice. Published Books:\nPoems of the Elders: An Anthology from the Theragatha & Therigatha, translated by Thanissaro Bhikkhu. (revised Jan. 17, 2017) Anthology. Print version vailable from Metta Forest Monastery. E-book from Dhammatalks.org.\nJataka (J): The canonical part of this collection are only verses. What are commonly known as the Jataka stories are actually the commentary stories behind them.\nThe Jataka or Stories of the Buddha’s Former Births, edited by E. W. Cowell. This is the only complete translation into English. You can download e-book versions here.\nJataka Tales of the Buddha: An Anthology, by Ken & Visakha Kawasaki. Although this is just an anthology, it contains all of the major stories and most of the others.']	['<urn:uuid:46c2c681-5b5d-4e05-80eb-3f9cc68c4c89>', '<urn:uuid:a970d600-5e8b-4f39-adbc-e9d2953a3b8d>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T22:13:20.996306	31	87	2093
56	website chunking information accessibility screen readers benefits challenges	Chunking information on websites has both benefits and challenges when it comes to accessibility and screen readers. While chunking can help with short-term memory retention by organizing content into 3-5 manageable groups, it can actually make content harder to scan and process for screen reader users if implemented incorrectly. However, when chunking is combined with other accessibility principles like proper heading hierarchy, descriptive link text, and skip navigation options, it can help all users better process and navigate content. The key is to ensure chunked content maintains a clear organizational structure that works both visually and for assistive technologies, using techniques like proper heading tags and descriptive labels to make the content hierarchy clear to screen reader users.	['The point of visual design is to communicate. You have a message you want to convey to your audience and your design should take part in that communication. Not only do you want your visitors to receive your message, you want them to understand and remember it. Fortunately we have some principles at our disposal to help our readers comprehend our message and recall it later.\nWe’ll talk about 3 of those principles here and how they can help you better communicate with your audience.\n- depth of processing\n- von Restorff effect\nDepth of Processing\nImagine two groups of people are given a list of words. The first group is asked to read the words while the second is asked to read the words, look up the definition of each, and use them all in a sentence. A few hours later both groups are asked to recall each of the words on the list. Which group do you think will be able to remember more of the words?\nYou can probably guess it would be the second group with the greater recall and the reason is they processed the information on a deeper level than the first group.\nDepth of processing is the idea that information that is analyzed deeply is better understood and recalled than information that is analyzed superficially.\nHow We Process Information\nThere are two ways in which we process information known as maintenance rehearsal and elaborative rehearsal.\n- Maintenance rehearsal repeats the same kind of analysis that has previously been carried out. It’s repeating a phone number back to yourself a few times in order to remember it later. No additional analysis is performed.\n- Elaborative rehearsal is a deeper and more meaningful analysis of the information. It’s reading a passage of text and then being asked to answer questions about what you just read. In order to answer the questions you must take a deeper look at the information read and that deeper look leads to greater comprehension and recall.\nTypically elaborative rehearsal shows 2 – 3 times improvement in the recall of information.\nThe above should be fairly obvious. The real question for us as designers is how to do we get our audience to more deeply analyze our message. How do we get them to put in the time and effort to understand and remember what we want to communicate.\nKeys to Deeper Processing\nThere are three keys to depth of processing:\n- Distinctiveness of the information – The more unique information is in relation to the surrounding information and the past experience of the reader, the more likely the information will be processed deeply\n- Relevance of the information – The more relevant or important the information is to the reader the more likely they will spend time processing the information\n- Degree to which the information is elaborated – The more thought that is required to interpret and understand the information the more deeply it will be analyzed\nWhat the above tells us is we should strive to engage our readers by giving them unique presentations and offering interesting activities. Give them incentives to think about the information you’re giving them. Don’t leave your audience in a passive mode. If your aim is to teach something new, provide a quiz at the end of the lesson. Ask questions of your readers so they contemplate your message after they finishing reading.\nMake the information more relevant to your readers. Talk about them more and you less. Let your audience know how the information applies to their lives and better how it will improve their lives. Use case studies to show how your ideas have helped others in similar situations.\nCommunicate your core message early on the page and progressively add more details further down the page. Make it clear to your visitors what your content is about right away to spur interest.\nAlso use a variety of media types. Add video or audio to reinforce your copy. Use images. Create a slideshow presentation. The more ways you can communicate your ideas, the more ways your visitors will think about those ideas.\nAs a side note keep the above ideas in mind when you’re attempting to learn something yourself. A quick scan of an article isn’t going to help you understand something as well as working your way through a book. Instead of copying and pasting someone’s code, spend some time understanding why it works.\nvon Restorff Effect\nThe von Restorff effect tells us that things that stand out and are noticeably different are remembered more. It’s an effect of the increased attention paid to something noticeably unfamiliar. Consider the short list of words below.\nThe above is an example of difference in context. By being styled in red the word “four” is set apart and likely gets noticed more and consequently remembered more from the list of words. Granted in this case the words would all be easy enough to recall later, but imagine a longer list without such a simple pattern.\nWith lists people often remember the first few and last few items more so you might want to style items in the middle somewhat differently to make them more memorable as well.\nYou have to be careful though as the von Restorff effect can work too well de-emphasizing attention and recall to other elements.\nDifference in experience is another way to make things noticeably different. Anything atypical from previous experience stands out. It’s why we often remember our first day at a new school or job, but not the second or third day. It’s why we remember significant events in our lives, but forget what we had for dinner earlier in the week.\nDo keep in mind the idea that noticeably different gets more attention and more recall. If every brand in your niche uses a blue dominant color scheme, then making your color scheme red dominant will stand out. You’ll likely be remembered more than were you to also use a blue dominant scheme.\nIn writing try an unusual word or sentence construction. Don’t force words and sentences or make them needless complex, but try something different enough to attract attention at times. Offer a different presentation than is typical. Use images to stand apart from your text. If everyone in your niche is offering text only, try adding a video. If video is commonplace, try audio or an interactive game.\nIf we expand the idea it may be argued that custom design work is better than themes or templates. At the very least customize your theme and template so as not to look the same as everyone else.\nUse contrast and emphasis for more important design elements. Bold key words and phrases, use a different color for important information, make a important paragraphs bigger. Do something noticeably different so the most important elements in your design grab attention and are remembered.\nWhich set of 9 numbers below do you think would be easier to remember?\n698 — 356 — 280\nThe second group should help you remember the numbers better. It’s an example of chunking, a principle where many items are combined into limited groups or chunks of less items each in order to make the items easier to process and remember. Three to five chunks usually work best.\nIt’s important to keep in mind that this principle is limited to tasks involving memory, particularly short-term memory. There’s no need to chunk reference-related information such as what’s found in a dictionary. If anything chunking could make it more difficult to process reference information as it makes it more difficult and time consuming to scan for any single item.\nChunking is sometimes misapplied in visual design. It can make things harder to scan since the focus is on the group of items and not the individual items themselves. However if you apply the concept in combination with other principles it can be used effectively to make documents easier to process.\nUsing the principle of proximity to group related items won’t make the individual items easier to scan, but if you combine that with a contrasting heading for each group and organize the information into a hierarchy, then your readers can quickly scan the headings to find the group they want and then quickly scan the items in that group.\nIn writing use several shorter paragraphs (chunks) instead of one longer paragraph. Provide transitions between chunks and organize them into a clearly labeled hierarchy. Link chunks together to form meaningful units of information.\nDon’t use chunking alone to help with scanning and searching information, but do use it where it can be an aid to short-term recall, and combine chunking with other ideas such as organizational hierarchies, proximity, and contrast to make information more understandable and easier to process and recall.\nIt’s not enough to get someone to visit your site or page if they don’t understand what you’re trying to tell them or can’t remember it or you later. As web designer’s it’s our job to communicate. If a reader doesn’t get your message, the fault is yours and not theirs.\nIdeally your audience will take the time and put in the effort to deeply process your prose or notice the important design elements on your page. However, don’t place the burden for doing so on them. Make it easy for your readers to absorb and recall your message.\nUse the von Restorff effect to call attention to important design elements and concepts you want to communicate. Chunk information that might be hard to recall and combine chunking with other design principles to better organize your design. Take steps to give incentives for your visitors to process your information more deeply. Get them to analyze your message beyond a quick glance.\nThe more you can bring attention to important ideas and concepts, the more easily you can make it for people to process those concepts, and the more you get your audience to analyze those concepts at a deeper level, the more you will communicate to them and the more they’ll remember your message later.\nDownload a free sample from my book, Design Fundamentals.', 'As the number of internet users keeps rising, web accessibility is becoming a more pressing concern. Due to the way some websites are designed, they can be difficult for people with special requirements to access, such as those who use screen readers. Not considering accessibility can lose you a lot of traffic.\nFor this reason, it’s vitally important that you design your affiliate program with accessibility in mind. This ensures that as many people as possible can use it without problems. Not only will this improve your affiliates’ user experience, it can even help to make your program more successful.\nIn this article, we’ll discuss web accessibility and why it’s important. We’ll also show you how to take your first steps towards making your program more accessible for everyone. Let’s get started!\nA Brief Introduction to Web Accessibility\nWeb accessibility is the practice of designing websites that can be used by anyone, regardless of whether they have a disability or other special needs. This is an important topic, especially as the number of internet users increases.\nIf you design any type of site and don’t make it accessible, you’re effectively shutting out a lot of your potential audience. This can be especially harmful for an affiliate program, as you’re limiting the number of potential affiliates who can sign up. As such, it’s important to consider how you can improve the accessibility of your affiliate website.\n3 Ways You Can Make Your Affiliate Program More Accessible\nNow, let’s discuss some of the ways you can improve accessibility for your affiliate program. Bear in mind that this is by no means a comprehensive list, so we recommend that you refer to sources like WebAIM and W3C for more information on designing with web accessibility in mind. However, the following three techniques are a perfect way to get started right now.\n1. Add Image Descriptions to Your Banners\nIncluding alt text for your site’s images is a crucial aspect of web accessibility. Alt text displays when the image itself cannot be loaded for whatever reason. However, it’s also used by screen readers to ‘describe’ the image to vision-impaired users.\nAs such, you’ll want to take care to add alt text to all the images on your site. While you aren’t able to do this for the banner ads you upload to your affiliate program, you can still achieve the same result by adding an image description. You can do this when creating a banner in Affiliate Royale > Links & Banners:\nBy adding a detailed description to the Info field, you can describe the image so users with assistive technology will understand what it contains. This can even be helpful for other affiliates, such as those who aren’t fluent in English and are using automatic translation software.\n2. Implement ‘Skip Links’\nOne of the most important aspects of creating an accessible site is making sure it’s keyboard-friendly. This means that a user should be able to easily navigate your site using only a keyboard (since some people have difficulty using a mouse). In practice, most sites achive this by enabling users to navigate using the tab key to jump between objects on the page.\nOne issue that can result from this setup is that users are required to tab through all your navigational elements just to get to a page’s content. To avoid this hassle, you can implement ‘skip links’, which let people jump ahead to the actual content. This is especially important for your affiliate dashboard, as affiliates may otherwise be stuck in your menus when they just want to access their personal information and resources.\nThere are a number of ways you can add skip links, but one of the easiest is to use a plugin. WP Accessibility is an excellent option, and makes it easy to add skip links to your site’s theme (among other features).\n3. Design Your Program’s Site With Accessibility in Mind\nFinally, in order to create a truly accessible affiliate program, you’ll need to keep the main elements of web accessibility in mind throughout the entire website design process. Let’s quickly run through some of the most important things you’ll want to consider:\n- Use headings correctly. Make sure that you implement headings and use them correctly, as they help assistive software to better understand the layout and content of your site.\n- Consider color contrast. If you use colors that are too similar or clash too much, your site could become difficult to read. This can cause problems for users with colorblindness or photosensitive epilepsy.\n- Use descriptive links. Your links’ anchor text should always clearly describe their purpose. You can do this both when designing your site and your affiliate dashboard, but also when adding descriptions to your affiliate links.\nWhile this can seem like a lot of work, it will help you avoid creating a site that’s difficult for a considerable number of potential affiliates to use. Building an accessible site will benefit both your affiliates and your program in the long run. Plus, most of these techniques can be used to improve an existing site as well as a brand-new one.\nWeb accessibility is becoming a more important consideration by the day, and it’s important to keep it in mind when designing the interface for your affiliate program. Fortunately, WordPress and Affiliate Royale can help you make your site more accessible, which makes the experience of using your program more enjoyable and increases its long-term potential for success.\nIn this article, we’ve shown you a few ways you can improve your affiliate program’s accessibility. These include:\n- Add image descriptions to your banners.\n- Implement skip links.\n- Design your program’s site with accessibility in mind.\nDo you have any questions about how to make your affiliate program as accessible as possible? Let us know in the comments section below!']	['<urn:uuid:28985899-f184-45bf-bf6e-84419e9a1a4a>', '<urn:uuid:4c095307-e893-4d71-8413-66317d8ea493>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T22:13:20.996306	8	118	2674
57	As an architectural historian studying papal influences, I'm curious which historical figure had a more complex relationship with the popes - John Henry Parker who documented Rome's religious buildings or Michelangelo who worked on papal commissions?	Michelangelo had a more complex relationship with the popes. While Parker simply received a medal from Pope Pius IX, Michelangelo worked for 7 different popes throughout his life and was known for his foul temper, frequently quarreling with them despite his commissions. However, he still maintained lengthy employment with the papacy, working on major projects like the Medici chapel commissioned by Pope Leo X.	['Place Your Bid!Click on the highest price you’d pay. If the final price is lower, that’s what you’ll pay.\n- Projected price\nExplore Rome’s history, culture, and greatest architectural achievements, with distinguished British archaeologist John Henry Parker. You’ll learn about Rome’s great buildings—the Colosseum, the Forum, and the aqueducts—as well as the mysterious catacombs. You’ll discover the city’s rich religious and artistic history by examining unearthed churches, pagan temples, and other important artifacts. Whether you want to explore above the city or below, Parker will lead you through Rome and demonstrate how it’s changed and why it remains one of the world’s greatest treasures.\nThe Logos editions of the multivolume The Archaeology of Rome equips you for better study with cutting-edge functionality and features. Citations link directly to English translations and original-language texts, and important terms link to dictionaries, encyclopedias, and a wealth of other resources in your digital library. The Topic Guide lets you perform powerful searches to instantly gather relevant biblical texts and resources. Tablet and mobile apps let you take the discussion with you. With Logos Bible Software, the most efficient and comprehensive research tools are in one place, so you get the most out of your study.\nKeep exploring the ancient world with the Near East Archaeology Collection (3 vols.).\n- Describes Rome’s most famous structures, artwork, artifacts, and tombs\n- Provides expert commentary on Roman archaeological discoveries from a distinguished expert\n- Covers the entire history of Rome from its earliest fortifications to the rise of Christianity\n- The Archaeology of Rome: The Primitive Fortifications, the Walls and Gates of Rome, and the Historical Construction of Walls\n- The Archaeology of Rome: Illustrations to The Primitive Fortifications, the Walls and Gates of Rome, and the Historical Construction of Walls\n- The Archaeology of Rome: The Egyptian Obelisks\n- The Archaeology of Rome: The Forum Romanium and the Via Sacra\n- Title: The Archaeology of Rome\n- Author: John Henry Parker\n- Publisher: John Murray\n- Volumes: 9\n- Pages: 2,868\nAbout John Henry Parker\nJohn Henry Parker (1806–1884) was an archaeologist, architect, and publisher. He had a deep interest in the history of architecture—he wrote and published many volumes on the subject. He was a chief advocate for the restoration of church buildings, an interest that eventually led him to study the archaeology of Rome. In his study of Rome, Parker commissioned an extensive series of photographs of Rome’s greatest buildings and monuments. The photographers also recorded the many excavations in Rome during the nineteenth century. In 1893, Parker’s entire collection was destroyed in a catastrophic fire at the Palazzo Della Porta Negroni Caffarelli, depriving later generations of a valuable resource. Parker wrote Glossary of Terms Used in Grecian, Roman, Italian, and Gothic Architecture, A Handbook for Visitors to Oxford, and A Concise Glossary of Architectural Terms.\nThough his scholarship was controversial, Parker was decorated by King Victor Emmanuel II of Italy and received a medal from Pope Pius IX. In England, he was named the first keeper of the Ashmolean Museum, and was nominated as a CB (companion) in the Order of the Bath.', 'On March 6, 1475, in a tiny Tuscan village of Caprese Michelangelo, the most famous artist of all times Michelangelo Buonarroti was born. Michelangelo was a universal renaissance artist – painter, sculptor, architect, poet, engineer, recognised as an artist while he was still alive and called “il divino” (English: the divine). True, he was also known for his foul temper – he was constantly quarrelling with other artists and clients, even the Popes of Rome. On the occasion of genius’ birthday, UnseenTuscany.com gives you a brief synopsis of his life and works that you can see during a visit to Florence.\nThe Buonarroti family, who were occupied in banking and marketing in Florence, were known since the 13th century. In the 15th century, the family was going through a difficult phase, Michelangelo’s father was appointed as the elder of a tiny village Caprese. After the birth of the artist, the family moved to the suburbs of Florence. Michelangelo’s father sent his son to Florence to study grammar with a teacher Francesco da Urbino. However, Michelangelo was more interested in art; he was painting and tried to get acquainted with the Florentine artists. In order to deter Michelangelo from arts, his father even beat him. Nevertheless, the stubborn Michelangelo had his will, got acquainted with Domenico Girlandaio and became his apprentice. Later Lorenzo il Magnifico personally took care of a young genius.\nAfter several long trips to Rome, in the 1501s Michelangelo returned to Florence, where he was already known as the author of ‘Pieta’. In Florence he got involved in the process of creating ‘David’ and at the same time was busy with the sculptures of Siena cathedral. The ‘David’ was completed in about two years and now this unique sculpture can be seen in the Galleria dell’Accademia in Florence.\nIn the 1518s Michelangelo returned to Florence and began the decoration works of the San Lorenzo church facade. He went to Carrara marble quarry to look for proper stones, but the project was ended until it was begun – Pope Leo X withdrew it due to the lack of funds. The Pope offered to take another project – to create a mortuary chapel for members of the Medici family and the Laurentian Library (Italian: Biblioteca Medicea Laurenziana) for the collection of the manuscripts that belonged to the Medici, located in the Basilica of St. Lawrence (which lacks a façade to this day).\nIn 1529 Michelangelo was appointed as one of nine republican officials, responsible for the design and implementation of defensive bastions and other fortifications. The main task for Michelangelo was the fortifications of the most important Florentine fortress on San Miniato hill.\nMichelangelo was invited to paint a huge fresco in the restored Palazzo Vecchio Palace, but this project was never realized. True, you can now see his sculpture ‘Genio della Vittoria’ in the main hall of the palace.\nMichelangelo died on February 18, 1564, just before his 89th birthday. During his long life Michelangelo worked for 7 popes, survived the great geographical discoveries, the Reformation, the seizure of Rome in 1527, the collapse of the ideals of the High Renaissance and the establishment of Mannerism in art, the announcement of Copernicus’ Heliocentric theory. He was never married and did not have any children. Church authorities hoped to bury him in St. Peter’s Basilica in Rome, but Michelangelo’s family almost secretly transported his body to Florence in order to satisfy his lasts will: ‘take me back to Florence, when I am dead, because I cannot go back there while I am alive’. Michelangelo was buried in the Basilica of Santa Croce in Florence, where his tomb remains till today.']	['<urn:uuid:56694530-f8ff-4bad-9ccb-4f1ec260147d>', '<urn:uuid:aa09fff6-b4fc-4e6a-af31-c41b3411b404>']	factoid	with-premise	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T22:13:20.996306	36	64	1130
58	taq dna polymerase versus proofreading polymerases adenine overhang addition pcr products cloning	Taq DNA polymerase and proofreading polymerases like Pfu and Tli differ in their ability to add adenine overhangs. Taq adds a single nucleotide base extension (usually adenine) to the 3′ end of amplified DNA fragments, though the efficiency of A-addition varies (4-75%) depending on the 3'-end nucleotide composition. In contrast, proofreading polymerases do not add A overhangs at all, producing blunt-ended PCR products. For cloning purposes, products from proofreading polymerases require an additional A-tailing reaction to enable T-vector cloning.	"['Some thermostable DNA polymerases, including Taq, add a single nucleotide base extension to the 3′ end of amplified DNA fragments. These polymerases usually add an adenine, leaving an “A” overhang. There are several approaches to overcome the cloning difficulties presented by the presence of A overhangs on PCR products. One method involves treating the product with Klenow to create a blunt-ended fragment for subcloning. Another choice is to add restriction sites to the ends of your PCR fragments. You can do this by incorporating the desired restriction sites into the PCR primers. After amplification, the PCR product is digested and subcloned into the cloning vector. Take care when using this method, as not all restriction enzymes efficiently cleave at the ends of DNA fragments, and you may not be able to use every restriction enzyme you desire. There is some useful information about cutting with restriction sites close to the end of linear fragments in the Restriction Enzyme Resource Guide. Also, some restriction enzymes require extra bases outside the recognition site, adding further expense to the PCR primers as well as risk of priming to unrelated sequences in the genome.\nThe easiest method, and the method of choice for cloning PCR products, is T-vector cloning. This method takes advantage of the “A” overhangs on the PCR product. T vectors are linearized plasmids that have been treated to add 3′ T overhangs to match the A overhangs of the amplicon. The PCR fragment is directly ligated to the T-tailed plasmid vector with no need for further enzymatic treatment other than the action of T4 DNA ligase. If desired, the insert can then be easily transferred from the T vector to other plasmids using the restriction sites present in the T vector.\nProofreading polymerases like Pfu and Tli do not add “A” overhangs, and PCR products generated with these polymerases are blunt-ended. Here is a simple method for adding an A-tail to DNA fragments generated by these polymerases to enable T-vector cloning.\nA-Tailing Reaction for Blunt-Ended Products\nAn A residue can is added by incubating the PCR fragment with dATP and a nonproofreading DNA polymerase, which will add a single 3′ A residue. Blunt ended restriction digest fragments can also be A-tailed using this method. The method below uses GoTaq Flexi DNA Polymerase (comes with a Mg-free reaction buffer), but any Taq DNA polymerase can be used.\nSet up the following reaction in a thin-walled PCR tube:\n1–4µl purified blunt-ended DNA fragment (from PCR or restriction enzyme digestion)\n2µl of 5X GoTaq Reaction Buffer (Colorless or Green)\n2µl of 1mM dATP (0.2mM final concentration)\n1µl GoTaq Flexi DNA Polymerase (5u/µl)\n0.6µl of 25mM MgCl2 (1.5mM final concentration)\nNuclease-free water to a final volume of 10µl\nIncubate at 70°C for 15–30 minutes in a water bath or thermal cycler. After the tailing reaction is finished, 1–2µl can be used without further cleanup for ligation into the pGEM-T or pGEM-T Easy Vector Systems.\nTips for T-Vector Cloning Experiments\nAlthough T-vector cloning experiments are simple, there are a few things to be careful of to ensure best results:\n- Avoid introduction of nucleases, which may degrade the T overhangs on the vector. Use sterile, nuclease-free water in your ligation reactions.\n- Use high-efficiency competent cells (≥1 × 108cfu/μg DNA) for transformations. The ligation of fragments with a single-base overhang can be inefficient, so it is essential to use cells with a transformation efficiency of at least 1 × 108cfu/μg DNA to obtain a reasonable number of colonies.\n- Limit exposure of your PCR product to shortwave UV light to avoid formation of pyrimidine dimers. Use a glass plate between the gel and UV source. If possible, only visualize the PCR product with a long-wave UV source.\nMore details on PCR cloning are available in the following resources:\n- Subcloning Notebook\n- T Vectors Technical Manual\n- Properties of Thermostable Polymerases\n- Protocols & Applications Guide, Cloning Chapter', 'Discussion of all aspects of biological molecules, biochemical processes and laboratory procedures in the field.\n7 posts • Page 1 of 1\nI want to ligate PCR product with T-vector, also I need high efficiency because the PCR product is random PCR. So, I only need a ""A"" at the PCR products ends to make sure the efficiency. How to do that.\nI worried Taq would add poly A at the ends.\nThanks for the reply.\nActually, I am not sure it adds one or more A at the ends.\nI just found in the manual of TOPO vector, ""Another approach was to utilize the terminal transferase activity of Taq DNA polymerase which adds a single 3´-A overhang to each end of a PCR product. "" It looks TAQ ONLY add one A.\nTaq adds one A at the end. If you are using Pfu or another polymerase you might have some problem but If I remembre correctly by just making a mix of buffer (1x final), DNA and a few units of Taq and incubating a short time at 72C you can add the extra A at the end of a PCR product that did not had it in the first place. ALternatively some highfidelity mix are a mix of Taq and Pfu and have the A overhang too.\nScience has proof without any certainty. Creationists have certainty without\nany proof. (Ashley Montague)\nI find useful information but cannot post the link.\nTaq does not always add an A to the 3\' end of the PCR product.\n(Hu, 1993 DNA and Cell Biology 12 (8) 763-770.,\nD. Denney, Jr. and I. Weissman 1990 Amplifications 4: 25-26.,\nV.L. Magnuson, D.S. Ally, S.J. Nylund, Z.E. Karanjawala, A.L. Lowe, S. Gough and F.S. Collins, 1996 Substrate nucleotide-determined non-templated addition of adenine by Taq DNA polymerase: implications afor PCR-based genotyping and cloning. Nucleic Acids Res. 21:700-709.(see for a more detailed version of the chart below)\nHite, J., Eckert, K. A., Cheng K. C., 1996. Factors affecting fidelity of DNA synthesis during PCR amplification of d(C-A)nd(G-T)n microsatelite repeats. Nucleic Acid Res. 24: 2429-2434.,\nCosta and Weiner, 1994 NAR 22 (12) 2423.)\nIf the 3’ nucleotide of the PCR product strand is a T, then Taq modifies the 3’ end to - T, + A. (nucleotides shown in order of occurence)\nIf the 3’ nucleotide of the PCR product strand is a G, then Taq modifies the 3’ end to +G > + A > +C.\nIf the 3’ nucleotide of the PCR product strand is a C, then Taq modifies the 3’ end to + A > +C.\nIf the 3’ nucleotide of the PCR product strand is an A, then Taq modifies the 3’ end to + A.\nThis means that you need to design the 5\' end of the primers to end in T in order to get an A added to the 3\' end of the PCR product.\nThe number of PCR cycles affects the cloning efficiency when using TA-vectors (vectors that have a single TTP or UTP overhang designed to exploit the addition of a single 3’-dA to PCR products by Taq DNA polymerase. Performing excessive cycles that result in more template than enzyme may result in ragged ends due to incomplete elongation. Amplified PCR products (particularly from excess template) exist as a heterogeneous mixture of ragged ends, blunt ends and 3’-overhangs (extendase activity) and incompletely extended fragments that can generate PCR monsters during subsequent PCR cycles. The degree of heterogeneity is strongly influenced by the nucleotides immediately flanking the 3’-end.\nThe efficiency of a 3’-dA addition ranged from 4-75% depending on the nucleotide composition of the 3’-end of the PCR product.\n( J. M. Brownstein, J. D. Carptena and J. R. Smith 1996 BioTechniques 20: 1004-1010.,\nV. L. Magnuson, P. S. Ally, S. J. Nyland, Z. E. Karanjuwala, J. B. Rayman, J. I. Knapp, A. L. Lowe, S. Ghosh and F. S> Collins 1996 BioTechniques 21: 700-709.)\nTo promote “3’-dA” addition, an extended post-PCR incubation (10 to 90 minutes at 72C) is required and the base composition of the 3’-ends is restricted.\nIf you have used a proofreading polymerase (with 3′- 5′ exonuclease activity), then you will not have A overhangs capable of ligating to the T overhangs of your vector.\nPCR reactions contain unincorporated dATP that is a competitive inhibitor for the ATP in the ligation reaction. 66 µM dATP in the ligation reaction will inhibit ligation 60%. (1992 BRL Technical Bulletin 5224-1. )\nSOC gives 2x more transformants than LB. SOB/SOC contain twice the Bacto tryptone of LB and provide protein precursors for rapid repair of cell walls damaged by generation of competent cells and the electroporation process, maintain isotonicity to prevent cell death by osmotic rupture and SOC contains glucose for easy energy conversion. (R. Hopper 1995 Genetic Engineering News 15 (9) 18.)\nThere is now a listing on the OpenWetWare Wiki about adding 3\' A overhangs using Taq polymerase.\nThe site listing can be found here: http://openwetware.org/wiki/Addition_of ... R_products\nIn order to overcome the fact that Taq will add non-A bases in some cases to the ends in a sequence dependent manner, you need to first purify your PCR product by using a PCR purification kit (I use the Qiagen column kit for PCR purification).\nAfter purification, you can add ONLY buffer, template, dATP, and Taq. Then it\'s absolutely impossible for Taq to add non-A bases on the ends. Even if your efficiency is low, incorporation of the A will occur at a MUCH higher rate than if you used raw PCR product containing all 4 trinucleotides.\n7 posts • Page 1 of 1\nWho is online\nUsers browsing this forum: No registered users and 0 guests']"	['<urn:uuid:862bc84b-7c7c-4af4-b19d-b2a2ae0c2f5e>', '<urn:uuid:12c28305-a5b0-4f15-8d34-548e753a3782>']	open-ended	direct	long-search-query	similar-to-document	comparison	expert	2025-05-12T22:13:20.996306	12	79	1604
59	What advances have improved breast cancer screening and biomarker analysis?	Recent advances in breast cancer detection combine improved imaging technology with molecular analysis. In imaging, three key developments are digital mammography, computer-aided detection (CAD), and breast tomosynthesis (3D mammography), which provide better image quality, automated abnormality detection, and improved visualization of dense breast tissue. On the biomarker front, prognostic tools like the 21-gene expression assay help predict recurrence risk, while novel biomarkers such as circulating tumor cells provide additional prognostic information for late recurrence in ER-positive breast cancer, complementing traditional screening methods.	"[""Dr. Sparano is Professor of Medicine & Obstetrics, Gynecology, and Women's Health at the Albert Einstein College of Medicine, Associate Chairman for Clinical Research in the Department of Oncology at Montefiore Medical Center, and Associate Director for Clinical Research at the Albert Einstein Cancer Center. He also serves as Vice Chair of the ECOG-ACRIN Research Group and Vice Chair of the AIDS Malignancy Consortium. He is former director of the Hematology-Oncology Fellowship Program at Einstein/Montefiore, co-directs the ECOG-ACRIN Young Investigator Program, and is a faculty member of the Calabresi K12 Oncology Training Program. He is co-principal investigator of the Montefiore-Einstein Minority/Underserved National Community Oncology Research Program (NCORP) grant (in conjunction with Dr. Bruce Rapkin), which funds multicenter, NCI-sponsored clinical trials in cancer therapeutics, cancer prevention/control, and cancer care delivery research. He is also the recipient of funding from the Breast Cancer Research Foundation that is supporting creation of a biospecimen bank designed to identify determinants of late relapse.\nDr. Sparano is a practicing clinician who specializes in medical oncology and clinical and translational cancer research. His research has focused on developmental therapeutic approaches for breast cancer, lymphoma, and HIV-associated cancers, and therapeutic application of molecular profiling in cancer. His research interests are described in greater detail below.\nBiomarkers of Recurrence in Early Stage Breast Cancer\nDr. Sparano serves as the study chair for the TAILORx clinical trial sponsored by National Cancer Institute, the first clinical trial integrating a mulitparameter gene expression assay in clinical decision making for adjuvant therapy with early stage ER-positive, HER2 negative breast cancer. This study completed accrual of approximately 10,000 patients in approximately 900 sites in 4 continents in 2010. Approximately 6700 atients with a mid-range Oncotype DX Recurrence Score (RS 11-25) were randomized to chemoendocrine therapy vs. endocrine therapy alone, whereas the remainder were assigned to endocrine therapy alone if the RS was low (<11), or chemoendocrine therapy if the RS was high (>25). Results from the low risk arm indicate a recurrence rate of only 1% at 5 years with endocrine therapy alone, indicating that these patients may be effectively spared chemotherapy. Results from the group with a mid-range RS randomized endocrine therapy +/- chemotherapy indicated non-inferiority for endocrine therapy alone, and also that integration of clinical features with the 21-gene RS added additional prognostic information to RS but not prediction of chemotherapy benefit. With funding from the Breast Cancer Research Foundation (BCRF) and Susan G.Komen for the Cure Founation, he established a late relapse biospecimen bank in collaboration with ECOG-ACRIN, and showed the circulating tumor cells are prognostic for late recurrence for ER+ breast cancer.\n- Sparano JA. TAILORx: trial assigning individualized options for treatment (Rx). Clin Breast Cancer. 2006 Oct;7(4):347-50. PubMed PMID: 17092406.\n- Sparano JA, Paik S. Development of the 21-gene assay and its application in clinical practice and clinical trials. J Clin Oncol. 2008 Feb 10;26(5):721-8. Review. PubMed PMID: 18258979.\n- Sparano JA, Gray RJ, Makower DF, Pritchard KI, Albain KS, Hayes DF, Geyer CEJr, Dees EC, Perez EA, Olson JA Jr, Zujewski J, Lively T, Badve SS, Saphner TJ, Wagner LI, Whelan TJ, Ellis MJ, Paik S, Wood WC, Ravdin P, Keane MM, Gomez Moreno HL, Reddy PS, Goggins TF, Mayer IA, Brufsky AM, Toppmeyer DL, Kaklamani VG, Atkins JN, Berenberg JL, Sledge GW. Prospective Validation of a 21-GeneExpression Assay in Breast Cancer. N Engl J Med. 2015 Nov 19;373(21):2005-14.doi: 10.1056/NEJMoa1510764. PubMed PMID: 26412349; PubMed Central PMCID:PMC4701034.\n- Sparano JA, Gray RJ, Makower DF, Pritchard KI, Albain KS, Hayes DF, Geyer CE Jr, Dees EC, Goetz MP, Olson JA Jr, Lively T, Badve SS, Saphner TJ, Wagner LI, Whelan TJ, Ellis MJ, Paik S, Wood WC, Ravdin PM, Keane MM, Gomez Moreno HL, Reddy PS, Goggins TF, Mayer IA, Brufsky AM, Toppmeyer DL, Kaklamani VG, Berenberg JL, Abrams J, Sledge GW Jr. Adjuvant Chemotherapy Guided by a 21-Gene Expression Assay in Breast Cancer. N Engl J Med. 2018 Jul 12;379(2):111-121. doi:10.1056/NEJMoa1804710. Epub 2018 Jun 3. PubMed PMID: 29860917.\nSparano JA, Gray RJ, Ravdin PM, Makower DF, Pritchard KI, Albain KS, Hayes DF, Geyer CE Jr, Dees EC, Goetz MP, Olson JA Jr, Lively T, Badve SS, Saphner TJ, Wagner LI, Whelan TJ, Ellis MJ, Paik S, Wood WC, Keane MM, Gomez Moreno HL, Reddy PS, Goggins TF, Mayer IA, Brufsky AM, Toppmeyer DL, Kaklamani VG, Berenberg JL, Abrams J, Sledge GW Jr. Clinical and Genomic Risk to Guide the Use of Adjuvant Therapy for Breast Cancer. N Engl J Med. 2019 Jun 20;380(25):2395-2405. doi: 10.1056/NEJMoa1904819. Epub 2019 Jun 3. PubMed PMID: 31157962.\n- Sparano J, O'Neill A, Alpaugh K, Wolff AC, Northfelt DW, Dang CT, Sledge GW Miller KD. Association of Circulating Tumor Cells With Late Recurrence of Estrogen Receptor-Positive Breast Cancer: A Secondary Analysis of a RandomizedClinical Trial. JAMA Oncol. 2018 Jul 26. doi: 10.1001/jamaoncol.2018.2574. [Epub ahead of print] PubMed PMID: 30054636.\nImproving Treatment Outcomes in Breast Cancer\nDr. Sparano is former vice-chair of the ECOG Breast Cancer Committee, and Chair of the ECOG-ACRIN Breast Cancer Committee. In those roles, he has led several phase III NCI-sponsored clinical trials in breast cancer, including a phase III which demonstrated that adjuvant weekly paclitaxel improves overall survival in stage II-III breast cancer (E1199) that has influenced clinical practice internationally. Other analyses of this data set have shown a strong association between obesity and black race with inferior outcomes in ER-positive breast cancer, which has been validated in other reports, and led to additional prospective trials and/or investigations.\n- Sparano JA, Wang M, Martino S, Jones V, Perez EA, Saphner T, Wolff AC, Sledge GW Jr, Wood WC, Davidson NE. Weekly paclitaxel in the adjuvant treatment of breast cancer. N Engl J Med. 2008 Apr 17;358(16):1663-71. N Engl J Med. 2008 Jul 3;359(1):106. PubMed PMID: 18420499; PubMed Central PMCID: PMC2743943.\n- Sparano JA, Wang M, Zhao F, Stearns V, Martino S, Ligibel JA, Perez EA, Saphner T, Wolff AC, Sledge GW Jr, Wood WC, Davidson NE. Race and Hormone Receptor-Positive Breast Cancer Outcomes in a Randomized Chemotherapy Trial. JNatl Cancer Inst. 2012 Jan 16. [Epub ahead of print] PubMed PMID: 22250182. PubMed Central PMCID: PMC3295746.\n- Sparano JA, Wang M, Zhao F, Stearns V, Martino S, Ligibel JA, Perez EA, Saphner T, Wolff AC, Sledge GW Jr, Wood WC, Fetting J, Davidson NE. Obesity at diagnosis is associated with inferior outcomes in hormone receptor-positive operable breast cancer. Cancer. 2012 Dec 1;118(23):5937-46. doi: 10.1002/cncr.27527. Epub 2012 Aug 27. PubMed PMID: 22926690; PubMed Central PMCID: PMC3586227.\nDeveloping and Validating Gene Expression Signatures in Breast Cancer\nDr. Sparano has collaborated with other investigators to validate and/or refine prognostic gene expression signatures in early stage breast cancer, including the Oncotype Recurrence Score for invasive cancer and DCIS Score for in situ cancer.\n- Goldstein LJ, Gray R, Badve S, Childs BH, Yoshizawa C, Rowley S, Shak S, Baehner FL, Ravdin PM, Davidson NE, Sledge GW Jr, Perez EA, Shulman LN, Martino S, Sparano JA. Prognostic utility of the 21-gene assay in hormone receptor-positive operable breast cancer compared with classical clinicopathologic features. J Clin Oncol. 2008 Sep 1;26(25):4063-71. Epub 2008 Aug 4. Erratum in: J Clin Oncol. 2009 Jul 20;27(21):3566. PubMed PMID: 18678838; PubMed Central PMCID: PMC2654377.\n- Sparano JA, Goldstein LJ, Davidson NE, Sledge GW Jr, Gray R. TOP2A RNA expression and recurrence in estrogen receptor-positive breast cancer. Breast Cancer Res Treat. 2012 Jul;134(2):751-7. doi: 10.1007/s10549-012-2112-7. Epub 2012 Jun 16. PubMed PMID: 22706628.PubMed PMID: 23877339; PubMed Central PMCID: PMC3732762.\n- Sparano JA, Goldstein LJ, Davidson NE, Sledge GW Jr, Gray R. TOP2A RNA expression and recurrence in estrogen receptor-positive breast cancer. Breast Cancer Res Treat. 2012 Jul;134(2):751-7. doi: 10.1007/s10549-012-2112-7. Epub 2012 Jun 16. PubMed PMID: 22706628.\n- Solin LJ, Gray R, Baehner FL, Butler SM, Hughes LL, Yoshizawa C, Cherbavaz DB, Shak S, Page DL, Sledge GW Jr, Davidson NE, Ingle JN, Perez EA, Wood WC, Sparano JA, Badve S. A multigene expression assay to predict local recurrence risk for ductal carcinoma in situ of the breast. J Natl Cancer Inst. 2013 May 15;105(10):701-10. doi: 10.1093/jnci/djt067. Epub 2013 May 2. PubMed PMID: 23641039; PubMed Central PMCID: PMC3653823.\nEvaluating Interaction between Breast Cancer and the Microenvironment\nAs co-program leader of the Einstein Cancer Center Tumor Micoenvironment Program along with Dr. John Condeelis, Dr. Sparano has collaborated with a multidisciplinary team at Einstein and Montefiore to validate a novel biomarker called ”TMEM” (Tumor Microenvironment of Metastasis), which provides complementary prognostic information to other markers (eg, IHC4) in ER-positive breast cancer. He is currently working with the team to evaluate novel agents which may prevent TMEM assembly and/or function, and hence prevent metatasis. He also collaborates with Dr. Xingxing Zang in evaluating the B7 family of proteins in breast cancer, and with Dr. Thomas Rohan (Chair, Dept. of Epidemiology & Populaton Health) to evaluate other prognostic and predictive biomarkers .\n- Rohan TE, Xue X, Lin HM, D'Alfonso TM, Ginter PS, Oktay MH, Robinson BD, Ginsberg M, Gertler FB, Glass AG, Sparano JA, Condeelis JS, Jones JG. Tumor microenvironment of metastasis and risk of distant metastasis of breast cancer. J Natl Cancer Inst. 2014 Jun 3;106(8). pii: dju136. doi: 10.1093/jnci/dju136. Print 2014 Aug. PubMed PMID: 24895374. PMCID Pending.\n- Jeon H, Vigdorovich V, Garrett-Thomson SC, Janakiram M, Ramagopal UA, Abadi YM, Lee JS, Scandiuzzi L, Ohaegbulam KC, Chinai JM, Zhao R, Yao Y, Mao Y, Sparano JA, Almo SC, Zang X. Structure and cancer immunotherapy of the B7 family member B7x. Cell Rep. 2014 Nov 6;9(3):1089-98. doi: 10.1016/j.celrep.2014.09.053. Epub 2014 Oct 30. PubMed PMID: 25437562; PubMed Central PMCID: PMC4250833.\n- Janakiram M, Chinai J, Fineberg S, Fiser A, Montagna C, Medaverepu R, Castano E, Jeon H, Ohaegbulam KC, Zhao R, Zhao A, Almo S, Sparano JA, Zang X. Expression, clinical significance, and receptor identification of the newest B7 family member HHLA2 protein. Clin Cancer Res. 2014 Dec 30. pii: clincanres.1495.2014. [Epub ahead of print] PubMed PMID: 25549724.\nIntegrating Novel Therapeutics with Standard Therapies in Localized and Advanced Stage Breast cancer\nDr. Sparano has led several clinical trials testing novel agents in combination with standard neoadjuvant therapy, including farnesyl transferase inhibitors, histone deacetylase inhibitors, and m-TOR inhibitors.\n- Sparano JA, Moulder S, Kazi A, Coppola D, Negassa A, Vahdat L, Li T, Pellegrino C, Fineberg S, Munster P, Malafa M, Lee D, Hoschander S, Hopkins U, Hershman D, Wright JJ, Kleer C, Merajver S, Sebti SM. Phase II trial of tipifarnib plus neoadjuvant doxorubicin-cyclophosphamide in patients with clinical stage IIB-IIIC breast cancer. Clin Cancer Res. 2009 Apr 15;15(8):2942-8. Epub 2009 Apr 7. PubMed PMID: 19351752; PubMed Central PMCID: PMC2785076.\n- Andreopoulou E, Vigoda IS, Valero V, Hershman DL, Raptis G, Vahdat LT, Han HS, Wright JJ, Pellegrino CM, Cristofanilli M, Alvarez RH, Fehn K, Fineberg S, Sparano JA. Phase I-II study of the farnesyl transferase inhibitor tipifarnib plus sequential weekly paclitaxel and doxorubicin-cyclophosphamide in HER2/neu-negative inflammatory carcinoma and non-inflammatory estrogen receptor-positive breast carcinoma. Breast Cancer Res Treat. 2013 Oct;141(3):429-35. doi: 10.1007/s10549-013-2704-x. Epub 2013 Sep 26. PubMed PMID: 24068539; PubMed Central PMCID: PMC3999640.\n- Tu Y, Hershman DL, Bhalla K, Fiskus W, Pellegrino CM, Andreopoulou E, Makower D, Kalinsky K, Fehn K, Fineberg S, Negassa A, Montgomery LL, Wiechmann LS, Alpaugh RK, Huang M, Sparano JA. A phase I-II study of the histone deacetylase inhibitor vorinostat plus sequential weekly paclitaxel and doxorubicin-cyclophosphamide in locally advanced breast cancer. Breast Cancer Res Treat. 2014 Jul;146(1):145-52. doi: 10.1007/s10549-014-3008-5. Epub 2014 Jun 6. PubMed PMID: 24903226.\nDeveloping More Effective Therapies for HIV-Associated Cancers\nDr. Sparano serves as the current vice-chair of the AIDS Malignancy Consortium (AMC), and prior chair of the AMC Lymphoma Working Group and Non-AIDS Defining Malignancy Working Group. In these roles, he has developed novel therapeutic approaches for the management of HIV-associated lymphoma (rituximab plus EPOCH) and anal cancer (cetuximab plus cisplatin/5-FU/RT), and organized an international team in collaboration with Dr. Stefan Barta to perform a metaanalysis of HIV-lymphoma trials, and a new prognostic score for HIV-associated lymphoma.\n- Sparano JA, Lee JY, Kaplan LD, Levine AM, Ramos JC, Ambinder RF, Wachsman W, Aboulafia D, Noy A, Henry DH, Von Roenn J, Dezube BJ, Remick SC, Shah MH, Leichman L, Ratner L, Cesarman E, Chadburn A, and Mitsuyasu R. ; Rituximab plus concurrent infusional EPOCH chemotherapy is highly effective in HIV-associated, B-cell non-Hodgkin's lymphoma. Blood, Dec 2009; doi:10.1182/blood-2009-08-231613 PMCID: PMC2636875\n- Barta SK, Xue X, Wang D, Tamari R, Lee JY, Mounier N, Kaplan LD, Ribera JM, Spina M, Tirelli U, Weiss R, Galicier L, Boue F, Wilson WH, Wyen C, Oriol A, Navarro JT, Dunleavy K, Little RF, Ratner L, Garcia O, Morgades M, Remick SC, Noy A, Sparano JA. Treatment factors affecting outcomes in HIV-associated non-Hodgkin lymphomas: a pooled analysis of 1546 patients. Blood. 2013 Nov 7;122(19):3251-62. doi: 10.1182/blood-2013-04-498964. Epub 2013 Sep 6. Review. PubMed PMID: 24014242; PubMed Central PMCID: PMC3821722.\n- Barta SK, Xue X, Wang D, Lee JY, Kaplan LD, Ribera JM, Oriol A, Spina M, Tirelli U, Boue F, Wilson WH, Wyen C, Dunleavy K, Noy A, Sparano JA. A new prognostic score for AIDS-related lymphomas in the Rituximab-era. Haematologica. 2014 Aug 22. pii: haematol.2014.111112. [Epub ahead of print] PubMed PMID:25150257.\n- Sparano JA, Lee JY, Palefsky J, Henry DH, Wachsman W, Rajdev L, Aboulafia D,Ratner L, Fitzgerald TJ, Kachnic L, Mitsuyasu R. Cetuximab Plus Chemoradiotherapy for HIV-Associated Anal Carcinoma: A Phase II AIDS Malignancy Consortium Trial. J Clin Oncol. 2016 Dec 12:JCO2016691642. [Epub ahead of print] PubMed PMID:27937092."", 'What is Mammography?\nMammography is specialized medical imaging that uses a low-dose x-ray system to see inside the breasts. A mammography exam, called a mammogram, aids in the early detection and diagnosis of breast diseases in women.\nAn x-ray (radiograph) is a noninvasive medical test that helps physicians diagnose and treat medical conditions. Imaging with x-rays involves exposing a part of the body to a small dose of ionizing radiation to produce pictures of the inside of the body. X-rays are the oldest and most frequently used form of medical imaging.\nThree recent advances in mammography include digital mammography, computer-aided detection and breast tomosynthesis.\nDigital mammography, also called full-field digital mammography (FFDM), is a mammography system in which the x-ray film is replaced by electronics that convert x-rays into mammographic pictures of the breast. These systems are similar to those found in digital cameras and their efficiency enables better pictures with a lower radiation dose. These images of the breast are transferred to a computer for review by the radiologist and for long term storage. The patient’s experience during a digital mammogram is similar to having a conventional film mammogram.\nComputer-aided detection (CAD) systems search digitized mammographic images for abnormal areas of density, mass, or calcification that may indicate the presence of cancer. The CAD system highlights these areas on the images, alerting the radiologist to carefully assess this area.\nBreast tomosynthesis, also called three-dimensional (3-D) mammography and digital breast tomosynthesis (DBT), is an advanced form of breast imaging where multiple images of the breast from different angles are captured and reconstructed (“synthesized”) into a three-dimensional image set. In this way, 3-D breast imaging is similar to computed tomography (CT) imaging in which a series of thin “slices” are assembled together to create a 3-D reconstruction of the body.\nAlthough the radiation dose for some breast tomosynthesis systems is slightly higher than the dosage used in standard mammography, it remains within the FDA-approved safe levels for radiation from mammograms. Some systems have doses very similar to conventional mammography.\nLarge population studies have shown that screening with breast tomosynthesis results in improved breast cancer detection rates and fewer “call-backs,” instances where women are called back from screening for additional testing because of a potentially abnormal finding.\nBreast tomosynthesis may also result in:\n- earlier detection of small breast cancers that may be hidden on a conventional mammogram\n- greater accuracy in pinpointing the size, shape and location of breast abnormalities\n- fewer unnecessary biopsies or additional tests\n- greater likelihood of detecting multiple breast tumors\n- clearer images of abnormalities within dense breast tissue\nWhat are some common uses of the procedure?\nMammograms are used as a screening tool to detect early breast cancer in women experiencing no symptoms. They can also be used to detect and diagnose breast disease in women experiencing symptoms such as a lump, pain, skin dimpling or nipple discharge.\nMammography plays a central part in early detection of breast cancers because it can show changes in the breast up to two years before a patient or physician can feel them. Current guidelines from the U.S. Department of Health and Human Services (HHS) and the American College of Radiology (ACR) recommend screening mammography every year for women, beginning at age 40. Research has shown that annual mammograms lead to early detection of breast cancers, when they are most curable and breast-conservation therapies are available.\nThe National Cancer Institute (NCI) adds that women who have had breast cancer, and those who are at increased risk due to a family history of breast or ovarian cancer, should seek expert medical advice about whether they should begin screening before age 40 and the need for other types of screening. If you are at high risk for breast cancer, you may need to obtain a breast MRI in addition to your annual mammogram.\nDiagnostic mammography is used to evaluate a patient with abnormal clinical findings—such as a breast lump or nipple discharge—that have been found by the woman or her doctor. Diagnostic mammography may also be done after an abnormal screening mammogram in order to evaluate the area of concern on the screening exam.\nHow should I prepare for the mammogram?\nBefore scheduling a mammogram, it is highly recommend that you discuss any new findings or problems in your breasts with your doctor. In addition, inform your doctor of any prior surgeries, hormone use, and family or personal history of breast cancer.\nDo not schedule your mammogram for the week before your menstrual period if your breasts are usually tender during this time. The best time for a mammogram is one week following your period. Always inform your doctor or x-ray technologist if there is any possibility that you are pregnant.\nIt is also recommended that you:\n- Do not wear deodorant, talcum powder or lotion under your arms or on your breasts on the day of the exam. These can appear on the mammogram as calcium spots.\n- Describe any breast symptoms or problems to the technologist performing the exam.\n- Obtain your prior mammograms and make them available to the radiologist if they were done at a different location. This is needed for comparison with your current exam and can often be obtained on a CD.\n- Ask when your results will be available; do not assume the results are normal if you do not hear from your doctor or the mammography facility.\nWhat does the mammography equipment look like?\nA mammography unit is a rectangular box that houses the tube in which x-rays are produced. The unit is used exclusively for x-ray exams of the breast, with special accessories that allow only the breast to be exposed to the x-rays. Attached to the unit is a device that holds and compresses the breast and positions it so images can be obtained at different angles.\nBreast tomosynthesis is performed using digital mammography units, but not all digital mammography machines are equipped to perform tomosynthesis imaging.\nHow does the procedure work?\nX-rays are a form of radiation like light or radio waves. X-rays pass through most objects, including the body. Once it is carefully aimed at the part of the body being examined, an x-ray machine produces a small burst of radiation that passes through the body, recording an image on photographic film or a special detector.\nDifferent parts of the body absorb the x-rays in varying degrees. Dense bone absorbs much of the radiation while soft tissue, such as muscle, fat and organs, allow more of the x-rays to pass through them. As a result, bones appear white on the x-ray, soft tissue shows up in shades of gray and air appears black.\nUntil recently, x-ray images were maintained on large film sheets (much like a large photographic negative). Today, most images are digital files that are stored electronically. These stored images are easily accessible for diagnosis and disease management.\nIn conventional film and digital mammography, a stationery x-ray tube captures an image from the side and an image from above the compressed breast. In breast tomosynthesis, the x-ray tube moves in an arc over the breast, capturing multiple images from different angles.\nHow is the procedure performed?\nMammography is performed on an outpatient basis.\nDuring mammography, a specially qualified radiologic technologist will position your breast in the mammography unit. Your breast will be placed on a special platform and compressed with a clear plastic paddle. The technologist will gradually compress your breast.\nBreast compression is necessary in order to:\n- Even out the breast thickness so that all of the tissue can be visualized.\n- Spread out the tissue so that small abnormalities are less likely to be hidden by overlying breast tissue.\n- Allow the use of a lower x-ray dose since a thinner amount of breast tissue is being imaged.\n- Hold the breast still in order to minimize blurring of the image caused by motion.\n- Reduce x-ray scatter to increase sharpness of picture.\nYou will be asked to change positions between images. The routine views are a top-to-bottom view and an angled side view. The process will be repeated for the other breast. Compression is still necessary for tomosynthesis imaging in order to minimize motion, which degrades the images. During screening breast tomosynthesis, two-dimensional images are also obtained or created from the synthesized 3-D images.\nYou must hold very still and may be asked to keep from breathing for a few seconds while the x-ray picture is taken to reduce the possibility of a blurred image. The technologist will walk behind a wall or into the next room to activate the x-ray machine.\nWhen the examination is complete, you may be asked to wait until the radiologist determines that all the necessary images have been obtained.\nThe examination process should take about 30 minutes.\nWhat will I experience during and after the procedure?\nYou will feel pressure on your breast as it is squeezed by the compression paddle. Some women with sensitive breasts may experience discomfort. If this is the case, schedule the procedure when your breasts are least tender. Be sure to inform the technologist if pain occurs as compression is increased. If discomfort is significant, less compression will be used. Always remember compression allows better quality mammograms.\nWho interprets the results and how do I get them?\nA radiologist, a physician specifically trained to supervise and interpret radiology examinations, will analyze the images and send a signed report to your primary care or referring physician, who will discuss the results with you.\nYou will also be notified of the results by the mammography facility.\nFollow-up examinations may be necessary. Your doctor will explain the exact reason why another exam is requested. Sometimes a follow-up exam is done because a potential abnormality needs further evaluation with additional views or a special imaging technique. A follow-up examination may also be necessary so that any change in a known abnormality can be monitored over time. Follow-up examinations are sometimes the best way to see if treatment is working or if a finding is stable or changed over time.\nWhat are the benefits vs. risks?\n- Imaging of the breast improves a physician’s ability to detect small tumors. When cancers are small, the woman has more treatment options.\n- The use of screening mammography increases the detection of small abnormal tissue growths confined to the milk ducts in the breast, called ductal carcinoma in situ (DCIS). These early tumors cannot harm patients if they are removed at this stage and mammography is an excellent way to detect these tumors. It is also useful for detecting all types of breast cancer, including invasive ductal and invasive lobular cancer.\n- No radiation remains in a patient’s body after an x-ray examination.\n- X-rays usually have no side effects in the typical diagnostic range for this exam.\n- There is always a slight chance of cancer from excessive exposure to radiation. However, the benefit of an accurate diagnosis far outweighs the risk.\n- The effective radiation dose for this procedure varies.\n- False Positive Mammograms. Five percent to 15 percent of screening mammograms require more testing such as additional mammograms or ultrasound. Most of these tests turn out to be normal. If there is an abnormal finding, a follow-up or biopsy may have to be performed. Most of the biopsies confirm that no cancer was present. It is estimated that a woman who has yearly mammograms between ages 40 and 49 has about a 30 percent chance of having a false-positive mammogram at some point in that decade and about a 7 percent to 8 percent chance of having a breast biopsy within the 10-year period.\n- Women should always inform their physician or x-ray technologist if there is any possibility that they are pregnant.\nA Word About Minimizing Radiation Exposure\nSpecial care is taken during x-ray examinations to use the lowest radiation dose possible while producing the best images for evaluation. National and international radiology protection organizations continually review and update the technique standards used by radiology professionals.\nModern x-ray systems have very controlled x-ray beams and dose control methods to minimize stray (scatter) radiation. This ensures that those parts of a patient’s body not being imaged receive minimal radiation exposure.\nWhat are the limitations of Mammography?\nInitial mammographic images themselves are not usually enough to determine the existence of a benign or malignant disease with certainty. If a finding or spot seems suspicious, your radiologist may recommend further diagnostic studies.\nInterpretations of mammograms can be difficult because a normal breast looks different for each woman. Also, the appearance of an image may be compromised if there is powder or salve on the breasts or if you have undergone breast surgery. Because some breast cancers are hard to visualize, a radiologist may want to compare the image to views from previous examinations. It is very important to realize that not all breast cancers can be seen on mammography.\nIncreased breast density has attracted attention from a number of state legislatures for multiple reasons, including:\n- Increased breast density makes it difficult to see a cancer on mammography.\n- Increased breast density may increase the risk of getting breast cancer.\nThe radiologist reading your mammogram determines your breast density and reports it to your doctor. Some states also require the facility to notify you if you have dense breasts.\nBreast implants can also impede accurate mammogram readings because both silicone and saline implants are not transparent on x-rays and can block a clear view of the tissues behind them, especially if the implant has been placed in front of, rather than beneath, the chest muscles. Experienced technologists and radiologists know how to carefully compress the breasts to improve the view without rupturing the implant.\nWhen making an appointment for a mammogram, women with implants should ask if the facility uses special techniques designed to accommodate them. Before the mammogram is taken, they should make sure the technologist is experienced in performing mammography on patients with breast implants.\nWhile mammography is the best screening tool for breast cancer available today, mammograms do not detect all breast cancers. This is called a false negative result. On the other hand, when a mammogram looks abnormal and no cancer is present, this is called a false-positive result.\nResearch is being done on a variety of breast imaging techniques that can contribute to the early detection of breast cancer and improve the accuracy in distinguishing non-cancerous breast conditions from breast cancers.']"	['<urn:uuid:91aa1ab8-0721-4b59-abad-5805ffd561a0>', '<urn:uuid:62d7712b-cbab-434d-be0e-b2af9ebcc334>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T22:13:20.996306	10	82	4640
60	mythology story behind andromeda perseus constellation placement together	According to Greek mythology, Perseus rescued Andromeda from a sea monster sent by Poseidon. This monster was sent because Andromeda's mother Cassiopeia bragged that she and Andromeda were more beautiful than the Nereids. Perseus saved Andromeda using Medusa's head to defeat the monster, and they flew away together on Pegasus. All these characters - Perseus, Andromeda, Cassiopeia, and the sea monster (Cetus) - were placed in the sky as constellations.	"['Look for the sky lovers\nBy Vernon Whetstone\nThere are many romantic couples in history. Romeo and Juliet, Pyramus and Thisbe, Tristan and Isolde, Wayne and Wanda (okay, you have to be a Muppets fan to know that).\nOne such romantic couple can be seen in the eastern evening sky about an hour and a half after local sunset.\nThe story of Perseus and Andromeda comes to us from ancient Greek mythology. Andromeda is rescued by Perseus from the evil sea monster and they fly off together on Pegasus, the Flying Horse.\nTo find this couple start with the horse, Pegasus.\nLook east about halfway up the sky for four stars forming a square. Actually, at this time of year in the early evening it looks like a baseball diamond standing up on one corner.\nGo to the star on the left corner (as you are looking at it). That is Alpheratz, and it is shared by Pegasus and Andromeda.\nStarting at Alpheratz, look leftward for two strings of stars. That is the constellation. Andromeda. I like to think of it as the two arms of Andromeda reaching out toward her hero, Perseus.\nIf you follow the two lines of stars they will lead you right to Perseus. He is the constellation at the end of those arms that is shaped like a capital letter “A” right below the big “W” shape of Cassiopeia, another of the stories characters.\nBoth constellations have interesting objects for us to look at. First, in Perseus, on the lower right side (as you are looking at it) is the second magnitude star Algol, or as some have called it “The Demon Star.”\nIt is supposed to represent the evil eye of Medusa’s head that has the power to turn people to stone. Recall that Perseus cut off the head of the Gorgon, Medusa and used it to slay the sea monster, Cetus.\nAlgol is an eclipsing binary star. It is actually a three-star system and every two days, 20 hours and 49 minutes the brightness of Algol will dim for a period of about 10 hours when one of the other stars in the system eclipses, or blocks, its light.\nYou can see for yourself. Find Algol and observe it from night to night over a period of a couple of weeks and watch how the star dims in brightness.\nArabic astronomers called it “Al Ghoul,” or, The Demon’s Head. With the approach of Halloween, this would be a good object to find.\nAndromeda’s jewel next time.\nSKY WATCH: First quarter moon, Sunday, Oct. 21. Tomorrow night, Thursday, Oct. 18, a very slender crescent moon will be to the upper left of the planet Mars. You may want to wait until about an hour after local sunset for an attempt to find the pair as the sky will be darker.\nWhile you are out there, take a gander to the lower left of Mars for the star Antares, the “Rival of Mars.” Both will just fit in the same binocular field of view. Mars at two o’clock and Antares at eight o’clock.\nThis upcoming Saturday, Oct. 20, is the date for the autumn version of Astronomy Day. So, go out and look at something.', 'The constellation Cepheus.\nClick on image for full size\nWindows to the Universe original image\nCepheus was king of a land called Ethiopia in Greek myth. He had a wife named Cassiopeia and a daughter, Andromeda. Cassiopeia liked to brag about her beauty so much, that she said she and Andromeda were more beautiful than the Nereids. Poseidon got very angry, and sent a sea monster to kill Cepheus and his family.\nAndromeda was offered as a sacrifice, and just when the sea monster was going to eat her, Perseus saved her. All four people, along with the monster, are in the sky as constellations.\nCepheus looks like a house. The point on top is a special star called a cepheid. These stars are used to measure long distances. Just below the constellation is another cepheid. This red star would be the North Star if we lived on Mars.\nThere are several galaxies, star clusters and nebulae within Cepheus. However, most are very dim and can only be seen with a telescope. If you have a telescope to use, browse this old constellation to find many celestial objects.\nShop Windows to the Universe Science Store!\nThe Summer 2010 issue of The Earth Scientist\n, available in our online store\n, includes articles on rivers and snow, classroom planetariums, satellites and oceanography, hands-on astronomy, and global warming.\nYou might also be interested in:\nHow did life evolve on Earth? The answer to this question can help us understand our past and prepare for our future. Although evolution provides credible and reliable answers, polls show that many people turn away from science, seeking other explanations with which they are more comfortable....more\nIn Greek mythology, Andromeda was the daughter of Cassiopeia and Cepheus, the king of Ethiopia. Andromeda\'s mother claimed that they were more beautiful than the sea nymphs, the Nereids. The Nereids felt...more\nThe constellation Cetus represents the Sea Monster. It is one of the largest constellations known. In classical civilizations, the figure was the giant sea monster that almost ate Andromeda. King Cepheus...more\nPerseus was an ancient Greek hero. His mother was Danae, the daughter of Acrisius, the king of Argos. When a prophecy revealed to Acrisius that his grandson would kill him, Acrisius imprisoned his daughter...more\nAndromeda is a ""V"" shaped constellation best viewed in the fall if you live in the Northern Hemisphere. Andromeda lies close to the north pole, so only a few in the Southern Hemisphere can see this strangely...more\nPerseus, the Hero, can be found in the sky during the winter in the Northern Hemisphere. With a little imagination, you can see the image of a man in the stars. He has a sort of triangular body, with...more\nUrsa Major is probably the most famous constellation, with the exception of Orion. Also known as the Great Bear, it has a companion called Ursa Minor, or Little Bear. The body and tail of the bear make...more\nWhat\'s in a Name: Star designated 40 in the constellation Eridanus. Claim to Fame: One of the first white dwarfs found. A white dwarf is the exposed extremely hot core of a star that has blown off its...more']"	['<urn:uuid:0e05ee1a-94bc-44b0-a33f-ecf491470a19>', '<urn:uuid:1f2b1e05-e0ae-444c-95b7-d1ffef5151e5>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T22:13:20.996306	8	70	1067
61	why do ears hurt more when flying with nasal congestion explain pressure equalization	Ear discomfort during flying can be particularly painful if you have nasal congestion or any condition that blocks the eustachian tubes. This is because the eustachian tube needs to be clear to equalize the air pressure between the middle ear and the outside. If the tube is blocked, the air pressure in the middle ear becomes different than the pressure outside the eardrum, causing discomfort. The condition is called ear barotrauma and is more likely to occur if you have a congested nose from allergies, colds, or upper respiratory infection.	"['Air pressure is lower in the mountains. The air inside your ears takes\na while to adjust to pressure changes, and unequal pressure can cause\ndiscomfort in the ears both when ascending and descending. This can be\nparticularly painful if you have nasal congestion or any condition\nthat blocks the eustachian tubes.\n""When we are on the ground, say, at sea level or below, a maximum\namount of air presses down on us -- the whole weight of the atmosphere\nabove. But higher up, air gets thinner and thinner. So air pressure is\nlower on top of a mountain or around a high-flying plane than in the\nmiddle of New York\'s Times Square.\nAlthough a passenger jet may fly 35,000 feet above the Earth, where\nair is very thin, cabin pressure is kept higher, so that passengers\ncan breathe easily. Even so, as a jet climbs into the sky, air\npressure inside changes from that at ground level to that found on a\n5,000-foot mountain. In a car climbing a real mountain, it\'s the same\nstory -- a dramatic change for the ears... When you drive up a\nmountain or ascend in a plane, the air pressure in your ear canal\ndrops. Meanwhile, the air caught in your middle ear expands, making\nyour eardrum push out into the canal...\nDescending to lower altitudes, it\'s the opposite story: The eardrum\ncaves in as outside pressure rises, with middle-ear pressure\ntemporarily stuck at \'cloud\' or \'mountaintop\' level. Now air rushes in\nthrough the eustachian tube to fix the too-low pressure.""\nMilwaukee Journal Sentinel: Ears feel the pressure of change\n""The outside of the eardrum is exposed to the pressure of the air\nwhere ever you may be located. That pressure is 14.7 pounds per square\ninch at sea level on a standard day.\nThe space inside of the eardrum is usually at the same pressure as the\noutside. This is facilitated by a tube, one end of which opens into\nthe space of the \'inner ear\' and the other end opens into the rear\npart of your throat. But, that end of the tube is normally lightly\nsqueezed closed. Each time you swallow or yawn it opens just enough to\nadmit air to the inner ear, thereby equalizing the pressure with the\noutside. Your ear drum then feels comfortable.\nWhen you climb higher as in an airplane or in a car in the mountains,\nyou are going into an area of lesser air pressure. The air from the\nlower altitude is \'trapped\' in the inner ear. If it can not escape you\nwill soon have an ear ache caused by the ear drum being balooned\noutward (stressed) by the higher pressure inside. So, it pushes out\nthrough the \'eustachian tube\' into your throat with bubble-popping\nPhyslink: Why your ears pop when you are riding on an airplane?\n""Normally, the eustachian tube, a passageway that leads from the\nmiddle ear to the back of the throat behind the nose, equalizes the\nair pressure in the middle ear to the outside air pressure by opening\nand letting air reach the middle ear. When your ears \'pop\' while\nyawning or swallowing, your eustachian tubes are adjusting the air\npressure in your middle ears...\nWhether you\'re flying, scuba diving, climbing a mountain, or even\nriding in an elevator, air pressure decreases as you go higher and\nincreases as you go lower. If the pressure isn\'t equalized, the higher\nair pressure pushes on one side of the eardrum and causes pain.""\nKids Health: Flying and Your Child\'s Ears\nA condition of discomfort in the ear caused by pressure differences\nbetween the inside and the outside of the eardrum... The air pressure\nin the middle ear is usually the same as the air pressure outside of\nthe body. The eustachian tube is a connection between the middle ear\nand the back of the nose and upper throat. Swallowing or yawning opens\nthe eustachian tube and allows air to flow into or out of the middle\near, equalizing the air pressure on both sides of the eardrum. If the\neustachian tube is blocked, the air pressure in the middle ear is\ndifferent than the pressure on the outside of the eardrum, causing\nMany people experience barotrauma at some time. Barotrauma commonly\noccurs with altitude changes, such as with flying, scuba diving, or\ndriving in the mountains. If you have a congested nose from allergies,\ncolds, or upper respiratory infection, barotrauma is more likely.""\nADAM Healthcare Center: Ear barotrauma\nMy Google search strategy:\nGoogle Web Search: pressure ears mountains\nI hope this is helpful. If anything is unclear or incomplete, please\nrequest clarification; I\'ll be glad to offer further assistance before\nyou rate my answer.']"	['<urn:uuid:a72ea8bd-dc6e-47a9-9feb-f4ae98771b63>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T22:13:20.996306	13	90	785
62	Is there a way to make a database work faster when lots of people are trying to look up and change information at the same time?	Yes, there is a feature called Read Committed Snapshot Isolation (RCSI) that stores older versions of committed data in tempdb. When someone tries to look up data that's being modified, the system shows them the last known committed version instead of making them wait. While this requires additional disk space and has a slight performance impact, the benefits typically outweigh the costs.	['Last week we looked at ACID compliance. This week we dive a little deeper into the Isolation portion, and what it all means.\nIsolation levels work to manage how my transactions may be isolated from your transactions, to allow concurrency. They work by locking sections of the database until a transaction is committed or rolled back, which results in blocking other access to those sections of the database.\nIn other words, locking and blocking is expected behaviour in a RDBMS.\nThere are four isolation levels in SQL Server (as quoted from SQL Server Books Online):\n- Read uncommitted (the lowest level where transactions are isolated only enough to ensure that physically corrupt data is not read)\n- Read committed (Database Engine default level)\n- Repeatable read\n- Serializable (the highest level, where transactions are completely isolated from one another)\nThe default on SQL Server (Read committed) means that readers block writers, and writers block readers. If I’m performing an\nINSERT on a table when you’re trying to run a\nSELECT, and they happen to be trying to access the same set of 8 KB data pages that make up that table, whichever transaction started first (the one with a lower LSN) will take precedence, and the database engine will lock all the portions of the table that are being affected. Any other operation that also tries to access those portions of the table will be blocked until the transaction is complete. Locks happen thousands of times a second on a busy system, and most of the time we barely notice.\nDeadlocks occur when two operations manage to lock each other out. SQL Server will kill a deadlock by terminating the process that was using fewer resources. This means that a long-running\nSELECT could easily take precedence over a short-running\nUPDATE. There’s no way of knowing who the deadlock victim will be, which is why our code should always have a retry mechanism built in. Always keep in mind that concurrency, by design, must assume locking, blocking, and deadlocks.\nWe may have used the query hint\nWITH (NOLOCK) in the past because we noticed that queries seemed to run a lot faster. That’s because the NOLOCK keyword forces the lowest possible isolation level.\nReferring back to the Books Online link above (or even the Wikipedia article), Read Uncommitted, or\nNOLOCK, permits the following bad behaviour:\n- dirty reads – a row that is uncommitted (either has not been committed yet, or may be rolled back) will end up in your query.\n- non-repeatable reads – a row might be read twice, with differing results, because the database engine might decide to delete and reinsert that row at the end of a table, for example.\n- phantom reads – a row that will eventually be deleted, can also show up in your results.\nConsider the reliability of these uncommitted reads. Consider the negative impact of making business or financial decisions based on these results.\nNOLOCK on data that is changing all the time.\nThere’s a fairly painless solution, though. We use\nNOLOCK in the first place because the default Read Committed isolation level causes readers and writers to block each other. A database-level setting, called Read Committed Snapshot Isolation, will use tempdb to keep track of older versions of committed data.\nWhen a data modification (\nDELETE) takes place, the database engine stores the last committed version of each affected row in an internal table on tempdb. When a\nSELECT comes along, the engine redirects it to see the last known committed version instead of having to wait for the data modification to complete.\nNothing is for free, though. There is a slight performance impact, relating to the disk requirements for tempdb, along with additional space. However, in all cases that I have implemented RCSI, the benefits far outweigh the costs.\nYou can read more about this feature in a post by Paul White for more details.\nFind me on Twitter at @bornsql if you have any dirty read stories to share.']	['<urn:uuid:5e7c6f36-ef57-44f0-bdf6-e4b75cc14bda>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T22:13:20.996306	26	62	670
63	In what ways do the visual and artistic approaches of German Expressionist cinema manifest in both The Golem and The Cabinet of Dr. Caligari, particularly in their set design and atmospheric elements?	Both films exemplify German Expressionist cinema but through different approaches to set design and atmosphere. The Golem features groundbreaking expressionistic pictorial design, with Hans Poelzig's constructions being revolutionary for cinematic set design. The Cabinet of Dr. Caligari employs deliberately distorted and disproportionate forms in its sets, created by designers Hermann Warm, Walter Reimann, and Walter Röhrig, who used painted decorations and even painted light effects, partly due to economic constraints. This created an unrealistic, fantastic atmosphere emphasizing subjective perception over objective reality. Both films' visual styles contributed to their horror elements, with The Golem being considered an inspiration for the horror genre, while Caligari's distorted reality and dark atmosphere influenced subsequent horror and noir films.	['6th APRIL | Cinelab – Cineporto in Lecce\nThe first pre-opening event will take place on Saturday 6th April at the Cinelab Giuseppe Bertolucci of the Cineporto in Lecce, with the screening of a restored version of the classic silent film The Golem – How He Came Into This World (Der Golem – Wie er in diese Welt kam) (1920, 76’) by Paul Wegener, a version by Friedrich-Wilhelm-Murnau-Stiftung and Cinémathèque Royale de Belgique (Cinematek). The Golem will be screened in a new digital copy made from the original negative, which had been believed to be lost, through a 4K restoration by Wiesbaden-based Friedrich-Wilhelm-Murnau-Stiftung and Brussels-based Cinémathèque Royale de Belgique (Cinematek). The digital restoration was done by the “L’ Immagine Ritrovata” lab in Bologna.\nThe screening of the film, which opened the last edition of the Venice International Film Festival, will be accompanied by the original music of composer Admir Shkurtaj, commissioned on an ad-hoc basis by the Biennale, performed live by Mesimèr Ensemble made up by:\n- Hersjana Matmuja (soprano)\n- Giorgio Distante (trumpet in Bb, trumpet Midi)\n- Pino Basile (cupafon – a set of drums, percussion instruments and ocarina)\n- Vanessa Sotgiù (synthesizer, piano)\n- Iacopo Conoci (cello)\n- Admir Shkurtaj (conductor, electronic music, accordion, piano)\nDER GOLEM, WIE ER IN DIE WELT KAM The Golem: How He Came Into the World\nIl Golem – Come venne al mondo\n1920 – DCP – b/n – 76’\nDirection: Paul Wegener\nScreenplay: Paul Wegener\nCinematography: Karl Freund\nSet design: Hans Poelzig\nCostumes: Rochus Gliese\nCast: Paul Wegener, Albert Steinrück, Lyda Salmonova, Ernst Deutsch, Hans Stürm, Max Kronert, Otto Gebühr, LotharMüthel\nFunded by: Die Beauftragte der Bundesregierung für Kultur und Medien, Verwertungsgesellschaft für Nutzungsrechte anFilmwerken mbh, Murnau Förderverein\n16th century Prague: The emperor wants to expel the Jewish population from the city. In order to avert the impending disaster fromthe inhabitants of the Jewish ghetto, Rabbi Löw creates the fabled clay figure Golem, animating it through black magic. In a turn ofevents the Golem saves the emperor’s life, making him draw back his orders out of gratitude. Due to a concatenation of unfortunatecircumstances the Golem turns on his creator and runs amok, with only a little girl being able to stop him.\nDer Golem, Wie er in Die Welt Kamwas Paul Wegener’s third cinematization of the Jewish legend. The film was one of the most successful German silent film productions, both artistically and financially, whose exceptional expressionistic pictorial design anddécor has lost nothing of its suggestive impact. The constructions conceived by Hans Poelzig were groundbreaking for cinematic setdesign. Until today the film is regarded as an inspiration and reference for the horror genre.\nThe 2018 digital 4K restoration was based on the A-negative, cut from the best takes and preserved in the Cinematek (Royal FilmArchive of Belgium). In 1921 the negative was heavily altered and cut for American distribution. The digitization intends to restorethe A-negative as close to the previously lost German version as possible. Missing parts were sourced from a duplicate of theAmerican distribution print from the George Eastman Museum and a copy from the Cinémathèque française made from the B-negative. Most of the original intertitles survived in the A-negative. Additional titles come from the Gosfilmofond of Russia, themain title from the Museum of Modern Art. Reference for the tinting was a historical Italian distribution copy from the Fondazione Cineteca Italiana.\nThe screening of the film will be accompanied by the original music of composer Admir Shkurtaj, commissioned on an ad-hoc basis by the Venice Biennale, performed live by Mesimèr Ensemble made up by: Hersjana Matmuja (soprano), Giorgio Distante (trumpetin Bb, trumpet MIDI), Pino Basile (cupafon – a set of drums, percussion instruments and ocarina), Vanessa Sotgiù (synthesizer,piano), Iacopo Conoci (cello), Admir Shkurtaj (conductor, electronic music, accordion, piano).', 'A century has passed since The Cabinet of Dr. Caligari premiered at the Marmorhaus theatre in Berlin, on 26 February 1920. The influence of this film—which is considered now as a classic of German Expressionism—can be traced through the history of cinema up to the present day.\nExpressionism, like any other movement in art, had its social and cultural preconditions which became even more acute after World War I. Crisis of the individual in the industrial era, a militaristic society and defeat of Germany in WWI with resulting isolation and economic crisis, and the destruction of the old value systems were among the contributing factors. In these adverse circumstances to the individual, turning inwards to the subjective world, as opposed to the outer reality, seems only natural.\nThe years preceding expressionist cinema was also the period of the origin of psychoanalysis. I can see it as both the result of repressive reality and the potential solution—hence a growing public interest in psychoanalysis. There was also an interest in spiritual practices against the declining role of traditional religions. Art, in general often acts as a mirror that reflects the processes going on in society. This can be said about Expressionism in our case. The distorted reality, the uncertainty of what is real and what is false, freedom and free will, power of authority, and power for influencing people, sanity vs. insanity—these are the topics reflected in The Cabinet of Dr. Caligari.\nThe idea of The Cabinet of Dr. Caligari was conceived by the writers Hans Janowitz and Carl Mayer and was inspired by several events from Janowitz’s real-life experience. The writers later sold the script to Erich Pommer who initially offered Fritz Lang to direct the film, but as the latter was busy with another film, Robert Wiene was finally chosen as a director.\nThe expressionist style of set design with distorted and disproportionate forms was a deliberate choice of designers (Hermann Warm, Walter Reimann, and Walter Röhrig). They believed it would be the best fit for the dark mystery story.\nThe visual style of the early expressionist films like The Cabinet of Dr. Caligari was to a large extent determined by the economic factors. They simply couldn’t spend much on the decorations and had to make it as simple and cheap as possible. The use of limited, unusually small space with painted decorations, even painted light effects, was, of course, a great solution for saving money. But luckily, it was also responsible for the unique style of the film, creating the film’s unrealistic, fantastic atmosphere that emphasizes the prevalence of the subjective perception of reality vs. objective reality.\nThe performance of the actors, especially, that of Werner Krauss as Dr. Caligari and Conrad Veidt as Cesare (both these actors had the experience of working in expressionist theatre) also helped to create that dark, exaggerated, unrealistic mood. These characters truly belong to the fantastic world of the film.\nWatching The Cabinet of Dr. Caligari once again, I constantly recognized the details, character types, or themes from the later films, be it noir or modern horrors or psychological thrillers. Just to mention a few; the theme of uncertainty about reality, the existence of false reality, when you can never be sure which narrative is real and which is the result of delusion (Martin Scorsese’s Shutter Island first came to mind). The twist in the narrative with swapped personalities reminded the finale of David Lynch’s Mulholland Drive. Hypnosis and controlled, manipulated behavior, a person compelled to do things under the influence—there are so many films about this. The double identity, the evil personality taking over the socially adapted one (the shadow personality of the mystic Dr. Caligari which takes over the character of the director of the asylum).\nThe film starts with the young man, Franzis (Friedrich Feher) talking with an older man on a bench. The man is telling him that there are spirits around us who were responsible for taking him away from home. This tells us already from the very beginning that the film will be dealing with something beyond ordinary. We see a ghost-like woman, but it immediately becomes clear that she is real. Franzis tells the man that it’s his fiancée, Jane (Lil Dagover) and that they had been through the even stranger and darker events.\nFranzis begins his story with a fair in his home town. The fair itself is an interesting place—various funny and unusual things can be seen there. If something weird should be happening in the story, it can perfectly fit in the context of the fair. This is the place where strange things belong because they can attract public attention. Dr. Caligari uses the show at the fair to mask his crimes. He brings with him a somnambulist—Cesare who, when awakened by Caligari, supposedly can tell the future. But it’s very easy to predict the future of a man who you are going to kill that night, isn’t it? That’s precisely what happens to Franzis’ friend, Alan.\nAlan’s murder scene is worth mentioning. The murder is shown as a shadow. Cesare’s shadow creeping up on Alan reminds of the scene in F.W. Murnau’s Nosferatu where Graf Orlok’s shadow is creeping up on Hutter. The knife Cesare is holding makes you think of Alfred Hitchcock’s famous shower scene from Psycho.\nAs I said, Cesare is a somnambulist; he has slept whole his life. He only awakens when Caligari tells him to and predicts the future. Then Caligari instructs him to kill people and Cesare obeys him, until he goes to kill Jane, but instead kidnaps her. Apparently, Jane’s beauty is stronger than Caligari’s suggestive force.\nEventually, Caligari’s crimes committed through Cesare are discovered and he flees. Franzis follows him to the asylum. There he discovers that Caligari is, in fact, the director of the asylum who is obsessed with the secrets of an Italian mystic Dr. Caligari. The story of this mystic is similar to what we saw. The director wants to know if it is really possible to control the sleepwalker so as to make him do the things he wouldn’t normally do, even commit murder. So he assumes the personality of Dr. Caligari and repeats his crimes. But the final twist reveals something completely different: Franzis turns out to be the one who is insane. He is the patient in the asylum and in his delusion believes that the director is Dr. Caligari.\nOn the subject of themes of the film, there is one more that goes beyond the typical horror tropes and may be interpreted in social rather than psychological context. It’s the theme of authorities abusing their power. (This takes my inner nerdy personality to Fritz Lang and his films Metropolis and M). Dr. Caligari can be seen as a metaphor for the abuse of authority. The film also contains the scenes with the actual representative of state authority town clerk who is sitting on an unusually high chair and not being in the mood that day, acts quite rudely toward Dr. Caligari. (Of course, he pays for it soon with his life.)\nThe frame story of Franzis as a patient of the asylum seems to be a reasonable explanation of the strange events of the film: Dr. Caligari doesn’t really exist and the whole story was the construct of Franzis’ delusional fantasy. All the characters from his story are the patients of the same asylum; he just changed their roles. The person believed to be Dr. Caligari is the director of the asylum. Everything turns out to reasonable, if not the last frame—the close-up on the director. The final frame raises suspicion again: what exactly is real and what is a delusion in this film? Is the director going to cure poor Franzis, or he aims to put him under his control, pushing into more madness?\nAs I already mentioned, The Cabinet of Dr. Caligari contains the seeds for so many themes or expression forms and moods that later became typical for film noir and horror films. These themes are still relevant, which means that problems of the society or individuals that defined the Expressionist movement are still present, regardless of the different socio-political situation. And it’s not surprising—the darkness of the human soul will exist in any epoch and any society, becoming more apparent in crises. Consequently, art forms reflecting and dealing with that darkness will always stay relevant.']	['<urn:uuid:4eefeee9-6a36-4621-bc91-cead72636e6b>', '<urn:uuid:bcfa01a8-6b50-4c63-9073-0d0cc45ce60b>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T22:13:20.996306	32	115	2029
64	How do storage batteries affect grid markets and what assumptions limit productivity theory?	Storage batteries are transforming grid markets by increasing from 1.5 gigawatts today to potentially hundreds of gigawatts by 2030, requiring new technical and economic systems for management. Meanwhile, productivity theory is limited by unrealistic assumptions including that all factor units are identical, factors can be perfectly substituted, and perfect competition exists - assumptions that don't reflect real-world conditions where resources like batteries have unique characteristics.	"['Theory (Neo-Classical Version):\nproductively theory is an attempt to explain the\ndetermination of the rewards of various\nfactors of production in a competitive market. The marginal\nproductivity theory of resource demand was the work of many\nwriters, it was widely discussed by many economists like J.B.\nClark, Walras, Barone, Ricardo, Marshall. It was improved,\namended and modified later on. The final version of the theory\nas stated by Neo Classical economists is given below.\nDefinition and Meaning:\nproductively theory of a factor is meant the value of\nthe marginal physical product of the factor. It is worked out by\nmultiplying the price of the output per unit by units of output.\nVMP = MP x P\nValue of Marginal Product (VMP) =\nMarginal Physical Product x Price\nproductivity theory contends that in a competitive market, the\nprice or reward of each factor of production tends to be equal\nto its marginal productivity.\nThe demand for\nvarious factors of production is a derived demand. The resources\ndo not usually directly satisfy consumer wants. They are\ndemanded because these help in producing goods and service\'s. An\nentrepreneur while hiring a factor of production calculates the\ncontribution which it makes to total production and the amount\nwhich has to be paid to it in a competitive market. An\nindividual firm cannot influence the price of the factor of\nproduction. It has to take the ruling price in the market as\ngiven. The firm can employ as many number of factors units as it\nwishes at the ruling price of the factor.\nIt has been observed\nthat as a firm hires increasing amounts of a variable factor to\na combination of fixed amounts of other factors, the marginal\nproductivity increases up to a certain stage of production and\nthen it begins to decline. The buyers of a factor of production\nwhile deciding whether one more unit of factor should be\nemployed or not, compares the net addition which it makes to\ntotal revenue and the cost which has to be incurred on engaging\nit. If the marginal revenue product of a factor is greater than\nits marginal cost, the entrepreneur will employ that unit\nbecause it earns more than what he has to spend on employing the\nAs he employs more and more units of factor of\nproduction, the marginal revenue productivity increases up to a\ncertain limit and then it begins to decrease. On the other hand,\nmarginal cost decreases as production is expanded. After a\ncertain point, when business becomes difficult to manage,\nmarginal cost begins to increase. When both marginal revenue\nproductivity of a factor and its marginal cost are equal, (MRP =\nMC) the entrepreneur stops giving further employment to a factor\nThe last variable\nunit which an employer just thinks it worthwhile employing is\ncalled the marginal unit and the addition made to the total\nproduction by the employment of the marginal unit is called\nmarginal productivity or marginal revenue productivity. The\nentrepreneur will pay the remuneration to each factor of\nproduction according to its marginal revenue productivity.\nSchedule and Example:\nproductivity theory is explained with the help of a schedule:\nDemand for a Factory or Resource (Daily):\nUnits of Resource (Labor)\nTotal Product Meters\nMarginal Productivity MP\nProduct Price ($)\nMarginal Revenue (Product)\nFor Employing a Factor of Production:\nAn entrepreneur is to\nmaximize profits. While hiring any resource, he compares the\nmarginal revenue product of a factor with the additional cost he\nhas to pay. So long as the marginal revenue product is greater\nthan the marginal cost of the factor, he will continue hiring\nit. When the MRP of the factor is equal to its MC, he will stop\nengaging more labor. The firm at this point will be in\nequilibrium and maximizing profit. In the table above, the\nentrepreneur adds more to its total revenue than to total cost\nup to the fifth unit. When he hires the sixth labor, the MRP =\nMC. The firm here is in equilibrium and maximizing profits, In\ncase, the 7th worker is hired, the MRP is then < MC. The firm\nsuffers loss and is not reaping the optimum profit.\nLeast-Cost Combination of Resources:\nThere are a number of\nresources which are required for the production of a commodity.\nThe entrepreneur can maximize his profit only if the least cost\ncombination can be arrived at by equalizing the ratios between\nthe marginal products and the prices of the different factors of\nproduction. If the ratios differ, then it is in the interest of\nthe employer to make necessary adjustment by employing more of\none factor and less of other till be ratio between the marginal\nproductivity and price of each factor becomes equal. The least\ncost combination will be achieved, when:\nMRP of Factor A = MRP of Factor\nB = MRP of Factor C = MRP of Factor N\nPrice of Factor A Price\nof Factor B Price of Factor C Price of\nIn the long run,\nunder conditions of\nperfect competition, the price of each factor of production\nis already determined in the market. An individual entrepreneur\ncannot affect the market price of various factors of production\nby his own individual action as his demand for a factor or\nfactors forms only a small part of the total demand. He is a\nprice taker. So, what he does is that he goes on employing each\nfactor of production up to a point which makes marginal revenue\nproductivity of the factor equals to its price.\nIn the figure 18.1,\nthe supply of labor is perfectly elastic. The wage (W) is equal\nto average wage (AW) and marginal wage, (MW) = W = AW = MW. At\npoint E, the MRP of labor is equal to marginal wage (MW). The\nproducer is-in equilibrium at point E. He will employ ON units\nof labor because when ON units of labor are employed, the\nmarginal revenue productivity of labor MRPL = Wage. To the left\nof E the MRP of labor is higher than wage (MRP > W), the\nproducer will increase the units of labor. To the right of the\nMRPL < wage, so the firm will curtail the units of labor. It is\nonly at point E, the firm is in equilibrium where MRPL = Wage.\nThe theory of\nmarginal productivity is based on the following assumptions:\nidentical: It assumes that all the units of a factor are\nexactly alike and so can be substituted to any extent.\n(ii) Factors can\nbe substituted: It is assumed that the various factors of\nproduction, which help in the production of particular commodity\ncan also be substituted for one another. We can use more of\nlabor or less of land or more of labor and less of capital.\nmobility of factors: It is assumed that the various factors\nof production can be moved from one use to another.\nof law of diminishing return: The theory rests en the\nassumption that the law of diminishing returns applies also to\nthe organization of a business.\ncompetition: It is based on the assumption that the reward\nof each factor of production is determined under conditions, of\nperfect competition and full employment.\nproductivity theory has been subjected to scathing criticism on\nthe following grounds.\n(i) Theory based\non unrealistic assumptions: The theory is based on a very\nwrong assumption, that all the units of a factor of production\nare homogeneous. The fact is that neither all land, nor all\nlabor, nor all capital, nor all organizations are alike. We know\nit very well that labor varies in efficiency; capital in form,\nland in fertility and entrepreneur in ability.\n(ii) Factors are\nnot perfect substitutes: It is also wrong to assume that the\nfactors of production are close substitutes for one another.\nLabor is not a perfect substitute for capital, and vice versa.\nSo is also the case with land in relation to other factors of\n(iii) Law of\nproportionate return: The theory rests on a very wrong\nassumption that the law of diminishing returns applies to a\nbusiness. Is this not a fact that when there is proportionate\nincrease in the factors of production, ""the law of diminishing\nreturn is held in, abeyance in all businesses.\n(iv) Wage cuts\ndoes not determine demand: According to this theory, if\nemployment is to be increased, the wages should be lowered. J.M.\nKeynes vehemently disagrees with this view and says that this\nmay be true in case of an individual firm or industry but it is\nwrong when it is applied to aggregate or effective demand.\n(v) Difficulty In\nthe measurement of MP: The other criticism levied on the\nmarginal productivity theory by Tausslng, Davenport and Ardiance\nis that production is the outcome of joint efforts of different\nfactors and so it is not possible to separate the contribution\nof each factor individually.\n(vi) Effect of\nwithdrawal of a factor: Hobson criticizes this theory on the\nground that if a factor of production which works in\nco-operation with other factors is withdrawn, it will\ndisorganize the whole business and it may result in the decrease\nof production which may be greater than the addition made by the\n(vii) Factor units\ncannot be raised: Another criticism levied by Hobson on the\nmarginal productivity theory is that there are many cases in\nwhich the variations in the use of factors is not possible. The\nproportion in which factors of production are to be employed is\nalready determined by the technical conditions prevailing in a\nbusiness. For instance, there are many machines for the working\nof which only one labor is required. If we engages two laborers,\nit will not be of much use. A variation in proportions in such\ncases are not possible, therefore, the marginal productivity of\nsuch a factor cannot be ascertained.\n(viii) One sided:\nThe marginal productivity theory is also criticized on the\nground that it assumes the supply of a factor or factors as\nfixed while in reality the remuneration paid to a factor does\ninfluence its supply. As the theory approaches the problem only\nfrom the side of demand and neglects the effect of supply,\ntherefore, it cannot be accepted as true.\ntheory: Marginal productivity theory neglects the problem of\ntechnical change altogether. It is therefore, static theory.\nFrom all that we have\nsaid above, It can be concluded that the Theory is true only\nunder the assumption of perfect competition and state of full\nemployment Fraser has commented on the theory of distribution as\n""No economist would\nclaim that theory is as yet complete, even as a purely academic\nstructure of framework. It has the defects of its quantities\nbeing simple and self-consistent; it is abstract and impersonal\nit is quantity of sins both of omission and commission; its\npostulates are unduly rigid and narrow"".\nIn the words of\nproductivity theory is not a theory that at explains\nwages, rent or interest; on the contrary it simply explains how\nfactors of production are hired by the firms, once their prices', ""How can U.S. transmission grids and wholesale energy markets adapt to the gigawatts of energy storage coming online over the next decade?\nIn the near future, the scale of the batteries serving U.S. power grids is set to explode, increasing from about 1.5 gigawatts today to tens or hundreds of gigawatts by 2030. These batteries will play a vital role in shifting intermittent wind and solar power from when it’s produced to when it’s needed and serving broader grid services needs on an increasingly decarbonizing grid.\nBut as a resource that can both absorb and discharge energy at a moment’s notice, batteries are very different from both dispatchable generators and intermittent wind and solar farms. That requires new technical and economic systems for managing and valuing them — and the grid operators that run wholesale electricity markets serving about two-thirds of the country are struggling to make those changes to keep up with the pace of growth.\nThat’s one of the key takeaways from last week’s Energy Storage Association policy forum, where representatives of the country’s regional transmission organizations (RTOs) and independent system operators (ISOs) joined storage industry groups and regulators to describe their work on energy storage integration.\nRichard Glick, the newly named chair of the Federal Energy Regulatory Commission, which regulates ISOs and RTOs, noted that storage is one of several new technologies facing barriers to full market participation that FERC is trying to eliminate.\n“I think the commission has done a pretty good job on that over the past half-decade or so,” the Democrat said. FERC Order 841 has led to major new opportunities for energy storage to participate in wholesale energy, capacity and ancillary services markets, albeit at different paces and in different ways across ISOs and RTOs. FERC Order 2222 sets a similar path for distributed energy resources, including aggregated batteries.\nAt the same time, “renewable energy generators see storage as a very important partner,” he said, as wind and solar projects add batteries to firm and shift their power output to meet grid demands. These hybrid resources now make up about two-thirds of all solar projects in the interconnection queue of California grid operator CAISO and constitute a rising share of clean energy projects across other markets. Developers including EDF Renewables, Enel, NextEra Energy, LS Power and many others are increasingly combining renewables and batteries in multiple states.\nGregory Cook, California ISO’s executive director of market and infrastructure, highlighted the work underway to integrate these novel combinations into its interconnection and market structures. CAISO expects its roughly 550 megawatts of energy storage will rise to 1,750 MW by this summer and to 3,300 MW in the next few years.\nCalifornia’s rolling blackouts last August has led state regulators to push to accelerate battery deployments, and “much of that new storage is going to be in the form of hybrid resources, where we’re seeing storage added to existing solar sites.” Beyond allowing multiple resources to connect at a single point, pairing batteries with solar allows them to access federal tax credits that are, as yet, unavailable to standalone storage.\nOther ISOs are seeing similar patterns, if at lower volumes. Manu Asthana, CEO of mid-Atlantic grid operator PJM, noted that of the 145 gigawatts of generation in its interconnection queue, 92 percent is solar, wind, batteries or hybrid resources. That includes about 15 GW of standalone storage and another 18 GW of hybrid storage.\nRenuka Chatterjee, executive director of system operations for the Midcontinent Independent System Operator, reported about 4 GW of standalone storage and 5 GW of hybrid resources, much of it linked to solar. Bruce Rew, senior vice president of operations for the Southwest Power Pool, cited about 6 GW of hybrid storage and 3 GW of standalone batteries.\nStorage and hybrid hangups, from interconnection to market valuation\nBut renewable and storage developers are running into problems with how grid operators’ interconnection and market rules manage hybrids. FERC’s technical conference on hybrid resources last summer raised “significant and substantial concerns,” he said, adding, “we’re going to take a serious and...long look at the responses we received.”\nThe root of these problems, according to a 2019 paper from Grid Strategies’ Rob Gramlich and Michael Goggin and Energy Storage Association interim CEO Jason Burwen, “is that present market rules will represent them as deviations from existing resource types.”\nThat approach ends up shoehorning hybrids into market rules that treat them as a “variable generator,” or in other words, a solar and wind farm “that is also dispatchable, or as a storage resource that has onsite fuel.” Either choice ends up misapplying interconnection and market concepts that would better be managed by creating a separate hybrid category to deal with them, they argue.\nFor example, some ISOs and RTOs require renewable projects seeking to add batteries to resubmit interconnection applications, which can push them behind in already long and difficult-to-navigate interconnection queues.\nOthers may presume that renewables and storage will simultaneously inject power at levels that would cause reliability problems at their interconnection site, even though project operators would have no economic incentive to do that and could easily control their systems to avoid it.\nOn the market disconnect front, hybrids and standalone storage systems can be subjected to rules that require them to submit to grid operator dispatches that may limit their ability to optimize their economic value. Examples include rules that force batteries to retain minimum states of charge, which are meant to assure they’ll be available at full capacity when dispatched but which developers say constrain their freedom to optimize their storage capacity.\nISOs are taking steps to resolve these disconnects. CAISO allows solar projects adding batteries to submit a “material modification” rather than resubmitting projects to its queue. Its Hybrid Resources Initiative is developing state-of-charge parameters that offer more flexibility, as well as an option for projects to manage the combined dispatch of renewables and storage, both expected to be finalized later this year, Cook said.\nThe Midcontinent Independent System Operator is developing a similar Surplus Interconnection Service to ease adding batteries to wind and solar projects, Chatterjee said. And New York state grid operator NYISO has developed rules for energy storage to participate in both wholesale energy markets and to provide services to utilities as part of its August implementation of Order 841, according to Michael DeSocio, director of market design.\n“Market power mitigation” rules, designed to prevent power plant owners from using their relatively high concentration of resources in an ISO's footprint to act in ways that shift prices in uncompetitive ways, can also be improperly applied to batteries that make up a fraction of today’s markets, developers argue.\nThese complications have been compounded by several orders passed by FERC’s Republican majority under the Trump administration that have applied these market mitigation structures on state-subsidized resources, including batteries, in ways that have limited their ability to compete in capacity markets in PJM’s 13-state territory and in downstate sections of NYISO’s grid.\nGlick, who voted against these decisions, repeated his previously stated view that they are likely to be overturned by legal challenges. But he also noted that ISO and RTO stakeholders are working on market reform proposals that will need to account not just for resolving conflicts between state and federal jurisdiction over energy policy but also for finding ways to accommodate the shifting mix of resources that supply them.\n“Our role is to make sure whatever market rules there are don’t act as barriers,” he said.""]"	['<urn:uuid:50fafacc-450a-4f53-895c-ca4a48580df7>', '<urn:uuid:5df24cc5-179a-4516-8798-49931fc798a2>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T22:13:20.996306	13	65	3082
65	mandatory research requirements architect ux designer comparison pros cons	For architects, research is legally mandated and required for regulatory approvals, with architects being obligated to conduct detailed site analysis and submit plans showing existing conditions. In contrast, UX design research is not legally enforced, which can lead to failures when designers skip research due to time or budget constraints. However, UX designers can use both qualitative and quantitative methods like usability tests and inferential statistics to validate their decisions, though this remains optional.	"[""Frank Lloyd Wright’s 1935 masterpiece, Fallingwater, is a perfect example of how minding a site’s properties can seamlessly cement relationships between a new architectural structure and its existing environment. In fact, when an architect receives a new project brief, he is obligated to conduct a detailed site analysis. According to The Architect’s Handbook of Professional Practice, the architect must possess the skills to do an initial assessment of the site, both for regulatory approvals and economic reasons, and also because it’s fundamental to good building design. Perhaps it’s time for those same requirements to apply to UX designers.\nEvery state has its own building code that architects and developers must comply with. In fact, in order to receive a permit for building construction, the architect has to submit a plan with the site’s existing conditions, along with a proposal for future development—not unlike a design proposal.\nHowever, unlike a design proposal, the architect is unable to complete his plan without in depth research. Given that research is not a legally enforced prerequisite for design, is it any surprise that lack of research is one of the main reasons why startups fail? Some UX designers struggle to justify spending time on user research to product owners. Others see it as a luxury they can’t afford. As a result, designers and stakeholders make inaccurate assumptions about their potential users, and create features that no one uses. Dictating solutions without first engaging in user research costs startups time and money, and will continue to do so until we consider user research as high a priority as architectural research.\nUX professionals can learn from architects in the early stages of designing a product. Let’s take a look at how successful architects design in context, study the competition, and consider the user journey. By the end of the piece, readers will be ready to design like an architect.\nDesigning in context\nIn both architecture and UX design, the context of the design can make or break the product. Consider Mombasa, Kenya, where new construction resulted in a large, concrete building standing between old colonial buildings. This was more than just a one-time problem; it happened so frequently that UNESCO created a World Heritage List in 1972 to protect heritage “in its environment.”\nThat’s not to say that good architecture must completely conform to its surroundings. In 1981, the new building of the School of Architecture at Rice University was completed. Its quietness and respect to its context were shocking. At first glance, the new addition appears to look just like its neighbors—an imitation of the other buildings. However, the new building was full of subtle variations, gradually moving architecture toward the future without disrupting the general look and feel of the area. This is no accident; the architect of the building, James Stirling, believes that his buildings can be completely different from their neighbors, but they should never defy their surroundings.\nThe American Institute of Architects’ Code of Ethics states that “architects should uphold human rights in all their professional endeavors.” The ethical challenges of architectural practices involve the responsibility to study the site so that the eventual building design is sustainable ecologically and financially.\nSimilarly, a good user experience involves designing for both the user’s context and the existing constraints of associated products or systems owned by the same brand. It’s important to develop a clear product style guide and to remain consistent within brand guidelines. This serves to ensure visual consistency and respect between new and existing elements throughout the product’s overall design, just like in the case of the Rice University extension. In addition, a product should be intended for a clear purpose, and for a specific group of users. Since many startups create solutions for problems the team hasn’t personally experienced, user research is valuable tool. The product owner can then serve as the user’s representative, and must therefore understand his constituency.\nOne recurring problem that causes ecommerce businesses to lose money is the failure to optimize their websites for mobile users. A lot of retailers have taken steps to improve their mobile shopping platforms. Together with greater connectivity, consumers are gaining confidence in purchasing products securely over mobile phones. While many businesses understand this trend towards mobile shopping, there are still ecommerce businesses lagging behind. Understanding the general mobile usage behavior of a store’s target customers’ would inform stakeholders on the usage of mobile platforms as shopping channels.\nDesigning for context also means having a clear understanding of what the product is, and why people will use it. If users come across the product thinking it will solve one problem, but then discover it is not intended to accomplish those goals, they are less likely to give the product a chance.\nCase in point, at myWebRoom, we build a visual bookmarking platform focused on content discovery and organization. Giving users the ability to personalize and design their web rooms is complimentary to our main feature. However, based on user feedback, we learned that our old landing page delivered the message that myWebRoom was a design tool before it was a bookmarking tool. Upon learning that, we tested new landing pages focused on explaining the true concept of our product. The result? Conversion rates increased from 10% to 18%.\nWhen an architect receives a brief to design a commercial retail space, it’s his responsibility to analyze competition so that his design will yield more profit for his client than nearby competitors. The same is true of UX design: it’s the designer’s responsibility to analyze the competition and identify opportunities.\nI worked on a commercial shopping complex, the Westgate Mall, located in Singapore, right by the train station and bus interchanges. There were two other major shopping malls in close proximity. In order to help my client boost traffic to his development, my team and I had to incorporate special design features to stay ahead of our competition. We designed the mall to be the only mall that has direct connections to the train station and bus interchanges. The mall was also awarded the GoldPlus Award under the Universal Design scheme which meant that it was designed beyond the minimum requirements of user-friendliness. These implementations have ensured success in the mall, clinching >75% committed occupancy with high rents.\nIn UX design, competitive research measures the experience of a product in comparison to its direct and indirect competitors. This work can help designers identify problem areas, develop new insights and inspirations, and plan for a vision that showcases best practices.\nFor example, Userzoom conducted a benchmark study between Priceline and Kayak to compare the mobile experience of obtaining hotel information. By having participants attempt to find a specific hotel in San Francisco, the team at Userzoom was able to determine the strengths and weaknesses of both products, and could then identify ways to provide what the others lacked.\nConsider the User Journey\nAn important aspect of a building’s success is easy access to the site. As I mentioned earlier, part of Westgate Mall’s success could be attributed to the fact that it’s the only mall with direct routes to major transportation ports. With respect to the competition, easier access to the retail space ensures that users get to Westgate Mall first.\nIn the larger scheme of things, designing delightful connectivity not only contributes to a business’ success, but also adds to the vitality in a neighborhood. An excellent example is the beautiful and functional linear High Line park in Manhattan. The park connects three neighborhoods on Manhattan’s West Side: the Meatpacking District, West Chelsea and Hell’s Kitchen. It’s essentially a walkway boasting views of the Hudson River and Midtown Manhattan, alongside spaces for art programs and gardens. It’s certainly changed its environment—employees of a pizza shop located by the park report that business has doubled twice in the past 2 years.\nIn UX design, designing delightful connectivity means considering the journey—sometimes by driving organic traffic to the site, other times by designing a delightful onboarding experience. Learning the user’s journey ensures the product teams better understand their users, which then allows for a more persuasive, attractive design.\nOnce users arrive to a product’s landing page, a good onboarding process could drive growth in a few ways. Onboarding is like approaching the entrance of a building. There are a few ways designers can make this walk as pleasant as possible and make a great first impression. One of the most common approaches is the “joyriding approach” where the user is walked through the features of a product. When executed beautifully, there is clarity from the get-go. Read about some other onboarding methods in MC Cook’s article, UX Flows: How to Turn Onboarding into an Amazing First Date with Your User.\nArchitects are also experience designers. The considerations and steps that architects take before going into designing ensure they are addressing the target users’ needs. Unlike in UX design, many of these steps are mandatory and regulated by authorities. But that’s no reason for UX designers to slack. Great design in both architecture and UX design boils down to having deep empathy and sensitivity for users.\nAn architect is an agent of the client and the public; he or she needs to incorporate delightful user experiences while achieving business goals. For new projects, UX designers should consider the following:\n- Relentlessly attempt to understand the product’s context. This could mean traveling to where the problem is and living it before committing on a design solution.\n- Simplify. Be clear on the product’s main intention. Reducing confusion is one of the first things UX designers can do to ensure good UX.\n- Conduct competitive analysis. Userzoom has an excellent article, Competitive UX Benchmarking, on how to conduct UX benchmarking.\n- Make it fun! Just like in real life, people like to engage in fun and positive experiences.\nUX research - or as it’s sometimes called, design research - informs our work, improves our understanding, and validates our decisions in the design process. In this Complete Beginner's Guide, readers will get a head start on how to use design research techniques in their work, and improve experiences for all users."", 'UX designers have a variety of methods for gathering feedback and iterating on their design, such as contextual interviews, persona creation, customer journey, storyboard, and etc. Some of the methods are prevailing and intuitive; they reveal a lot about user needs and stories.\nQualitative methods are driven by the urge to understand the users and empathize with them to create better design solutions. Nevertheless, qualitative methods aren’t always the right methods. Sometimes we need to step back and take a different perspective to understand the “who” and “what” of user behavior. That’s when quantitative data and statistics could help us with collecting “data” rather than just insight.\nRecording data about the number of user errors and unfinished tasks during a usability test, and then using graphs to present the frequency and severity of usability issues is quite useful. However, once you go beyond graphs, averages, and percentages, you will need to delve into analysis and estimations with the help of inferential statistics.\nAs the name suggests, inferential statistics are used to make conclusions and generalizations about the population by taking a representative sample from it.\nImplementing inferential statistics can help you find out how more confident you could be in your resulting UI design choice or prioritization.\nEven if you’re only doing qualitative usability testing (concern with what the usability problems are, not how frequent they are), understanding the basics and principles of inferential statistics is useful. For instance, it could help to clarify to stakeholders why even 1 among 5 users failing a task, is justification for changing the UI intended for many thousands of users.\nInferential statistics are useful for clarifying to stakeholders why even 1 among 5 users failing a task, is justification for changing the UI intended for many thousands of users\nStatistical analysis and inferential thinking are applied in a myriad of UX research methods, including usability tests, card sorting, surveys, and physiological testing like eye tracking and etc. Don’t panic though, in this article we will only discuss the use case scenarios of two frequently used statistical analyses and simplify their explanations as much as possible.\nWhat statistics to know?\nFor interpreting research results such as surveys and usability tests, some inferential statistical procedures can cover most quantitative research methodologies. Statistical tools that begin with basic correlation and T-test (this will be discussed later) are fairly easy to access. In fact, if you have Excel or access to an online calculator, you probably have most of these tools already at your fingertips!\nNowadays many UX professionals come to the profession without formal statistical training. As a matter of fact, usability studies, contextual inquiries, surveys, and other UX research methods are sometimes performed on an ad hoc basis by designers, information architects, product managers, and front-end coders who have had no formal training in these methods, let alone training in the statistical tools used in the analysis of the data collected through such methods.\nAs mentioned, in this article, we will go through two frequently used and easy-to-understand statistical methods employed by UX practitioners and researchers and their use case scenarios in UX.\nWhat type of data to look for?\nAs the nature of usability testing is qualitative, I would argue that you need to know very little statistical knowledge to understand and interpret usability measurements. However, some descriptive statistics such as the following could help you find the red flags:\n- Completion Rate: number of users completing a task)\n- Number of Errors: number of mistakes users make\n- Task duration: the time that it takes to complete a task\nDescriptive statistics are used to summarize data, for instance, measures of frequency. Quantifying the aforementioned three factors should tell you if a user is capable of successfully completing a task with ease.\nSevere issues are raised when they cannot complete a task, have many errors when completing a task, or when they take a long time to complete the task. These metrics help you understand if what was designed is indeed usable.\nQuantitative research and metrics obtained from studies help you understand if what was designed is indeed usable.\nIt is worth mentioning that I am only going to go through each statistical method with its usage case scenario rather than the technicality of performing the statistical methods – but all of these could be easily calculated by Excel, free online calculators, or SPSS which is a software for statistical analysis.\nNow let’s dive in and see what’s all the fuss about statistics in UX:\nIndependent sample T-tests\nAs the UXer on the team, you are probably frequently asked to determine which version of a design or feature is better, in essence, more usable. You may also be asked to determine which design is preferable on a variety of attributes, such as sophistication, trust, emotional connection, aesthetic appeal, and of course, its commercial potential.\n“Design” could be a web page, the ever-important shopping-cart check-out process, a prototype, or a single important feature. In fact, this kind of comparison test might be one of the most common types of jobs of UX professionals.\nSo what should you do when the final design needs to be chosen? And that isn’t always easy without further, and perhaps sophisticated analysis.\nHere is an example of such situations:\nConsider you are comparing the perceived aesthetic appeal of 2 images for the homepage of your bike e-commerce website. You’ve recruited 20 subjects, 10 view and rate image A, and another 10 viewing and rating image B, in other words, you are surveying the design with two independent samples of participants.\nPhotos by Murillo de Paula and Carl Nenzen Loven from Unsplash\nA sample is a small group of individuals chosen from the larger population.\nFor instance, if you have an e-commerce website for clothes and you have more than 10,000 customers, you are not going to survey all of the customers. Obviously, time and finances stop you from doing so, instead you choose 20 random people from the customer base. These 20 people are your sample.\nAt this point, you might ask why not have just a sample of 10 people rating both designs? Well, sometimes you have to eliminate any potential user bias originating from viewing one design after another. The first design affects the person, so they cannot give an objective evaluation of the second design.\nNow back to your design, you have already obtained the perceived aesthetic appeal of 10 subjects per design. What you have to do next is to compare the mean of your two independent samples. In statistics mean is another name for average. By comparing the means or averages you want to determine the one that has the higher perception of aesthetic appeal.\nNull and alternative hypothesis\nTo do so, we will create two hypotheses in an effort to decide between the two:\n- Null hypothesis and,\n- Alternative hypothesis\nThe null hypothesis is the default position. To put it simply, in our example the default position would be the assumption that there is no difference in the aesthetic appeal of the two images.\nStatistically speaking, the default position is called the null hypothesis and its symbol is H0. And the symbol for sample mean or average is μ (mu).\nSo in our design example, this is how we can show it:\nH0: μ1 = μ2\nThis is how we can read it: the null hypothesis (H0) expects equal sample means for design 1 (μ1) and design 2 (μ1). In other words, the two designs do not differ with respect to the aesthetic appeal of the two images.\nIn statistics, besides the null hypothesis, we also have the alternative hypothesis.\nThe alternative hypothesis is the one that we want to prove. Its symbol is H1 and it directly contradicts the null hypothesis, H0.\nIn our design case, the alternative hypothesis or H1 would indicate that there is a significant difference between the mean aesthetic appeal of the two images:\nH1: μ1 ≠ μ2 (The two designs do indeed differ with respect to mean aesthetic appeal of the two images)\nSignificance means that the result is unlikely due to chance.\nPhoto from xkcd.com, used with permission\nMean and significant differences could be easily calculated by T-test in Excel or SPSS. T-test is a type of inferential statistic used to determine if there is a significant difference between the means of two groups, which may be related to certain features.\nIn our case, the means we are comparing is the perceived aesthetic appeal of image A versus image B.\nIf the alternative hypothesis (H1) is determined to be true, then we can conclude that the means of aesthetic appeal of the two images are different. Therefore, the design with the higher aesthetic mean will be chosen.\nWhat we just covered is called an independent sample T-test. Having two different groups of people is the reason this approach is called “independent samples”. Basically, no one person is in both groups, and the mean score for one group of people is totally independent from the mean score for the other group.\nOf course, at times it is appropriate for the same person to evaluate both designs, this may be a better choice. This is what I am covering next.\nPaired sample T-tests\nWhat we just did in the previous scenario was launching a survey with two different designs, one home page with picture A, and another with picture B, to two different groups and just sat back to see which one would win.\nThe reality is that we often don’t have the luxury of obtaining even the moderately small sample sizes. Why?\nBecause (1) it’s been established that larger sample sizes do not reveal more problems, and (2) conducting studies with large populations is both time consuming and expensive.\nSometimes you have to compare 2 designs in less time and that’s when you opt to compare Design A and Design B, one after another and with one sample.\nOf course, you’ll counterbalance to ameliorate bias. With counterbalancing, the participant sample is divided in half, with one half seeing picture A and then picture B in one order, and the other half of the sample seeing picture B and then picture A in order. Counterbalancing is a technique used to deal with order effects when we have only one sample for evaluating two variations.\nSo this is one group of users and two designs: welcome to the world of paired-samples T-tests.\nAs opposed to the scenario with independent samples, the fundamental characteristic here is that each person evaluates both designs. So, in essence, the aesthetic appeal evaluations are “paired.”\nOne person provides two data points and we know which two came from a given person. As mentioned, you will be reducing the bias by having for instance 5 subjects viewing design A first and then B, and another 5 subjects viewing design B after design A.\nWhen we are comparing two means and the data has the same people providing measurements of both alternatives (the aesthetic appeal rating of two designs, or the time for the performance of some task for two designs), we have what is referred to as “paired data” and hence, the statistical method is called paired sample T-tests.\nThe hypotheses are the same as in the Independent sample T-tests:\nH0: μ1 = μ2 (The two designs do not differ with respect to mean aesthetic appeal of the two images)\nH1: μ1 ≠ μ2 (The two designs do, indeed, differ with respect to mean aesthetic appeal of the two images)\nBy and large, detailed exploration of the underlying behavior of users has been left to qualitative techniques and analysis. However, quantitative and qualitative both have their place in UX validation. Implementing tasks and acquiring quantifiable responses can help with testing the hypotheses and generalize a finding to a representative sample of a population. In comparison with qualitative research methods or data from analytics, implementing some statistics into our everyday design research could help us with decision-making and predicting.\nQuantitative techniques could be widely used in user experience research, especially taking into account the rich array of statistical techniques and available tools such as free online calculators and Excel.\nCreswell, J. W. (2009). Research designs: Qualitative, quantitative, and mixed methods approaches.\nFritz, M., & Berger, P. D. (2015). Improving the user experience through practical data analytics: gain meaningful insight and increase your bottom line. Morgan Kaufmann.\nLeech, N., Barrett, K., & Morgan, G. A. (2013). SPSS for intermediate statistics: Use and interpretation. Routledge.']"	['<urn:uuid:75ebd980-c25c-490c-9198-7ca18ab3c391>', '<urn:uuid:06e5722b-408c-4d2c-b887-bea338b50869>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T22:13:20.996306	9	74	3780
66	minimum age requirement tutor player tutoring football manager game	The tutor must be at least 24 years old, although the club captain can be a tutor at any age.	['Player tutoring involves a senior player (a tutor) and a young player (a tutee) being paired, with the tutee shadowing his tutor for a period of time (a tutoring session).\nPlayer tutoring can be used to:\n- Improve a young player’s Determination mental attribute, and Ambition and Professionalism personality attributes – improving the player’s ratings in these attributes directly benefits his training progression. Improving his Determination and Professionalism directly benefits his match performances (and, therefore, indirectly benefits his match progression). Improving his Professionalism makes him more likely to have a later natural decline and less likely to complain that his individual training workload is too heavy.\n- Improve a young player’s personality as a whole – improving the player’s personality directly benefits his match performances and makes him easier to man manage, both of which help his morale. It therefore indirectly benefits his training progression (as he is more likely to have good morale) and match progression (as he is more likely to give good match performances, due to both his positive personality and his good morale).\n- Teach a young player useful traits – tutoring is the most efficient method of teaching a player traits as, unlike trait training, it does not add to the player’s individual training workload and it can potentially enable him to learn more than one trait at a time, while it is also the only way to teach those player traits that cannot be trained.\n- Develop good relationships between players in your squad – improvements in player relationships can help player morale and therefore indirectly benefit match performances, training progression and match progression.\nTutoring is therefore a particularly effective method of developing young players and comes at no cost (unless experienced players are signed especially for their use as tutors).\nFurthermore, tutoring can be an ongoing cycle in which you develop the personalities and traits of young players who later pass these on to your club’s future young players when they become tutors themselves.\nHow Tutoring Works\nWhat Happens During Tutoring\nDuring a tutoring session the tutee’s personality attributes (Ambition, Controversy, Loyalty, Pressure, Professionalism, Sportsmanship and Temperament) and Determination mental attribute (and, therefore, his personality description and media handling style description) change to more closely match those of the tutor. Depending on the tutoring option chosen (as detailed below), the tutee may also learn some of the tutor’s player traits.\nIf the tutor and tutee get on well (and if the tutoring session is not cancelled) then the session lasts for 180 days (approximately six months) and the relationship between the two players improves. Relationships between players in a squad may also improve over time as an indirect result of players developing similar personalities through tutoring.\nHowever, the tutor and tutee may fall out with each other during the tutoring session due to a personality clash or personal differences, in which case the session ends early and their relationship worsens. Despite this, the tutee’s personality attributes and Determination still change over the period of the session and he may also learn player traits. Although the shorter time of the tutoring session means that the attribute changes are smaller and the tutee has less opportunity to learn any traits.\nTherefore, as long as tutees are paired with tutors who have good personality attributes and good Determination (relative to the tutee), tutoring can almost always be viewed as being beneficial.\nYou can start a tutoring session by selecting a prospective tutee from the Interaction > Request Tutoring drop-down of the prospective tutor and then selecting one of the two tutoring options on the Tutoring Private Chat screen.\nAlternatively, you can go to the Development > Tutoring screen of either player but these only list three pairings each.\nIf you are not able to find the pairing that you want after trying both of the above methods then you could try starting separate tutoring sessions for the other players on your intended tutor’s Interaction > Request Tutoring drop-down to see if your desired tutee then appears on the drop-down.\nAfter you select one of the tutoring options on the Tutoring Private Chat screen the tutor or tutee may refuse to be paired with the other player. If this happens then it may be partly because of the refusing player’s personality. However, he may still agree to be paired with a different player.\nOn the Tutoring Private Chat screen it is important that you select the appropriate option for the type of tutoring that you want to happen.\nThe options are:\n- I’d like you to tutor [the tutee] as I feel you can help improve his game – this allows the tutor’s traits to be potentially learned by the tutee.\n- I think it would be beneficial if you were to take [the tutee] under your wing and mentor him off the pitch – this prevents any of the tutor’s traits being learned by the tutee.\nBoth options result in the tutee’s personality attributes and Determination changing during the tutoring session to more closely match those of the tutor.\nYou can cancel a tutoring session manually from the Development drop-down of either the tutor or the tutee. You may want to do this so that you can pair the tutee with a different tutor sooner than if you waited for the session to finish naturally, perhaps after his attributes have developed enough to give him a positive personality description. This allows you to give the tutee more tutors in a shorter amount of time.\nA tutoring session is cancelled automatically if:\n- Either the tutor or the tutee is transferred away from the club – for example, the tutee may be loaned out to gain match experience.\n- There is a serious injury to either the tutor or the tutee.\nGiving Further Tutoring\nWhatever the result of a particular tutoring session, as soon as it has finished (either naturally or after being cancelled) the tutee can be paired with a different tutor.\nHowever, for the tutor there is a tutoring cooling-off period that lasts for up to seven months, during which he cannot be paired with any tutee.\nThere are certain conditions that must be met for two players to be paired as tutor and tutee. These include the following:\n- The tutor must be at least 24 years old and at least a few years older than the tutee, although the club captain can be a tutor at any age.\n- The tutee must be younger than 24 years old.\n- The tutor and tutee must not already be involved in a tutoring session.\n- The tutor and tutee must not have a serious injury.\n- The tutor and tutee must not be undertaking trait training.\n- The tutee must not be an established member of the senior squad (which occurs after he has made 75 senior appearances in his career – a player needs to play for approximately 15 to 20 minutes in a match to register an ‘appearance’, in which case he is given a match rating out of 10).\n- The tutor and tutee must share a good level of positional ability in a similar playing position (a ‘similar’ playing position may be an adjacent playing position).\n- The tutor must have a higher reputation than the tutee. An approximation of a player’s reputation is shown under Personal Information on his Overview > Information screen.\n- The tutor must not be in his cooling-off period following a previous tutoring session (explained under Giving Further Tutoring above).\n- The tutor must have a squad status of backup or higher and have a higher squad status than the tutee.\n- The tutee must have a squad status of rotation or lower.\n- The tutor and tutee must have more than six months left on their contracts.\nHow to Choose a Tutor\nA young player’s tutor should be chosen very carefully. The important aspects to consider are as follows:\n- Whether the tutor has a positive personality – the tutor should ideally have a positive personality description (and a good media handling style description) with Ambition, Professionalism and Determination attributes that are rated higher than those of the tutee.\n- This is likely to result in the tutee developing or maintaining a positive personality and his Ambition, Professionalism and Determination increasing. If these attributes are rated lower for the tutor than the tutee then they are likely to fall for the tutee.\n- However, improving a tutee’s Ambition can make him more likely to want to leave a lower reputation club if he has the chance.\n- Whether the tutor has a similar personality to the tutee – players with similar personalities are less likely to fall out during a tutoring session. As such, you may want to take a patient approach to improving a tutee’s personality by using multiple tutors.\n- For example, you could use a player with average Professionalism to tutor a player with poor Professionalism, and later use a tutor with higher Professionalism to increase this attribute further.\n- However, a patient approach is not essential, as even if the tutoring ends early it should still have led to some personality changes for the tutee, while a seemingly unsuitable pairing can arguably be worth the risk of the players falling out if the tutee’s personality needs to be greatly improved. Furthermore, it is still possible for players to fall out even if they have seemingly well matched personalities.\n- A patient approach can be particularly useful if you want the tutee to learn player traits during tutoring, as it increases the likelihood of a tutoring session lasting long enough for traits to be learned.\n- Whether the tutor has beneficial or unsuitable traits – if you want the tutee to learn player traits during tutoring then the tutor should have traits that you believe would be beneficial to the tutee and ideally should not have any traits that you believe would be unsuitable for the tutee.\n- Remember to select the tutoring option that allows the tutee to learn the tutor’s traits, as detailed under Tutoring Options above.\n- To help you to decide whether any of the tutor’s traits would be beneficial or unsuitable for the tutee see the Player Traits guide. Unsuitable traits include those that are not suitable for your tactics and those that make a player less tactically flexible.\n- Any unwanted player traits learned by the tutee can potentially be unlearned through trait training later (although some traits cannot be trained) but this may not be successful and would add to his individual training workload.\nIt is advisable to give preference to young players who have higher potential ability when allocating tutors. This is because it is more beneficial to help the progression of your best prospects and a senior player can only tutor one player at a time (while there is also a cooling-off period for tutors as explained under Giving Further Tutoring above).\nIf you have enough possible tutors then you can still benefit from giving your lesser prospects tutoring, even if you plan on selling them later, as better progression can make them more valuable. However, you should not allow this to prevent your best prospects from being tutored, including any you expect to sign in the near future.']	['<urn:uuid:f95217d5-2e1c-4df7-9a94-40708754d07a>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T22:13:20.996306	9	20	1886
67	Being a home modification consultant, I need to know what are the essential bathroom modifications needed for both basic safety and enhanced accessibility for wheelchair users?	For basic safety, grab bars and toilet risers are essential, with grab bars costing between $100 to $300 apiece and toilet risers under $50. For enhanced wheelchair accessibility, necessary modifications include installing a walk-in bathtub or roll-in shower, adding a hand-held shower with accessible controls, removing cabinetry for turn-around room, replacing vanity-top sinks with pedestal or wall-mounted sinks, and ensuring proper insulation of pipes. Additional equipment like transfer benches and bath lifts can help with transitioning between wheelchair and bathtub.	['In the realm of aging, you have got to catch the young at heart before they realize that running, jumping, and climbing is truly a temporary state of mind. Anticipating the slowing of mobility and the onset of disability demands to make accessibility changes in home and hearth. In the twilight years, and for the disabled, living well is all about ensuring independent free movement throughout your homestead. Before you consider building a new home with considerations built-in before you cross the threshold, consider the following tips to make your home handicap accessible.\n- Home Access: Ramps and Doorways\nHave a ramp built to make for an easy transition from outside to inside. This will help with wheelchair access and with anyone who has mobility issues. Depending on materials and size, the cost varies with low-threshold ramps to a custom-designed ramp. Do not forget to add the cost of a permit. Check with your local municipality for permit regulations and information to meet building codes.\nMost household doorways are too small for a wheelchair or walker to easily pass through. Before investing $500 to $1000 for widening, consider offsetting hinges that could help a door swing completely open. That shift could add several inches before committing to a costly job. Also, evaluate all of your devices before beginning changes to your homestead. Knowing width, length, and turning radius can make it easier to accurately widen spaces but also be conscious of the thoroughfares in-between.\n- Bathroom: Grab Bars, Risers, and Showers\nThese can increase reassurance, security, and stability around the shower, tub, and toilet. From professionals to handymen, the cost will run your coffers between $100 to $300 apiece.\nToilet risers make a smoother transition from standing up to sitting down by raising the height of the toilet seat. Risers are perfect for people who have difficulty bending over, sitting down, or standing up. They may be available at drug stores and some home improvement businesses. According to some of the literature, the cost will run less than $50.\nMost showers have high walls that are difficult to step over for the mobility limited. Try creating or replacing the tub with a wheelchair accessible or walk-in shower. If you are on a tight budget, try buying or installing a shower bench for increased stability.\n- ADA Sinks\nThe bathroom is one of the most critical spots for an accident to happen. For starters try installing an ADA compliant restroom beginning with an ADA sink. The design for these sinks assists users with disabilities and mobility problems. Calculating sink mountings results in space for wheelchairs and clearance under the sink. Keep floor space around and beneath clear of debris. Pipes also need to be insulated.\n- Around the House: Cabinets, Flooring, and Furniture\nFor a homeowner with mobility issues, cabinets are frequently too high to reach; an increased difficulty is in having access to what is inside. Move frequently used items such as bathroom towels, medication, and dishes to lower cabinets. Also, move appliances closer to the sink to make for easy task performance.\nConsider hardwood or vinyl flooring for smooth transitions across walking surfaces. Area rugs and carpeting are hazards for anyone with mobility problems; especially, with those who use wheelchairs or walkers.\nFor independent freedom of movement, construct pathways in the house that are at least 32 inches wide between furniture. To make the thoroughfare easy to walk may require downsizing effects and using storage.\n- Government Assistance\nTry not to feel that these changes may be outside of your budget. Administered by the Federal government, disability grants exist for your use. Check into The Assisted Living Conversion for Eligible Multifamily Housing Projects (ACLP) through the U.S. Office of Housing and the Housing and Urban Development Office. Medicaid programs are also offered in most states to cover changes to the homestead.', '3 Home Modifications for Better Wheelchair Accessibility\nCategory: Home Modifications\nMarch 27, 2015\nThere has been great advances in wheelchair accessibility of public places in the last few decades. From accessible curbs to push-button door openers and handicap-friendly public transport, it’s now easier for someone in a wheelchair to go out and enjoy life. In fact, it’s probably easier to navigate city streets in a wheelchair than your own home. Sadly, a home is often the least accessible place for a wheelchair user. In this post, our Maryland home modification experts will discuss how to modify your home to better suit your mobility needs.\nIf you have a good remodeling budget, widening the door frames is one of those small changes that will make the biggest impact. A wheelchair user needs at least a 32-inch clearance to comfortably pass through a door opening. Many older homes don’t have this much room, so passing through a door frame in a wheelchair is literally a tight squeeze. If door frame widening is not an option, or if you want to make your doorways even more accessible, here are a few other modifications you can try:\n- Unhinge the doors between the rooms—curtains can be hung for privacy if needed.\n- Install expandable offset door hinges on doors that have to remain in place. These hinges allow the door to swing fully open and out of the way, so you gain about 2” in doorway width.\n- Install automatic door openers on frequently-used doors.\n- Replace swing-open doors with sliding pocket doors for more convenience.\nAchieving accessibility in your own kitchen is not easy, but possible. Extensive remodeling is needed to ensure that all the cooking surfaces, switches and appliances are within your reach. Some or all of the following modifications may be necessary in order to make a kitchen wheelchair-friendly:\n- Lowering the counters to 30”\n- Removing lower base cabinets to provide leg room\n- Adding roll-out or pull-out food prep areas\n- Improving lighting\n- Moving switches and appliances to the front of the counter/cabinets\n- Adding accessible storage areas.\nAlong with the kitchen, the bathroom is one of the rooms that requires extensive modifications in order to be considered safe and comfortable for a wheelchair user. There are issues with wet, slippery surfaces, sharp corners, not enough room and the overall strain of using a bathroom for someone who doesn’t have full control of their body. Here are some of the modifications that help address these and other challenges of bathroom accessibility:\n- Installing a walk-in bathtub or a roll-in shower\n- Adding a hand-held shower with accessible controls\n- Installing grab bars and safety rails\n- Removing cabinetry if needed to gain more turn-around room\n- Replacing a vanity-top sink with a pedestal sink or a wall-mounted sink\n- Raising the toilet height or installing a raised toilet seat\nBesides the actual modifications of your bathroom space, you can also use bathroom safety equipment specifically designed for wheelchair users. For example, a transfer bench can help you comfortably transition between your wheelchair and the bathtub. A bath lift, on the other hand, will eliminate the need to lower yourself into the bathtub.\nIf you need help deciding on and performing any of the described modifications, contact Freedom Mobility today for wheelchair-friendly home remodeling in Maryland.\nClick Here to View Service Areas\nAnne Arundel County Mobility Solutions: Annapolis (21401, 21403, 21409), Arnold (21012), Crofton (21114), Crownsville (21032), Gambrills (21054), Glen Burnie (21060, 21061), Hanover (21076), Jessup (20794), Pasadena (21122), Severn (21144), Severna Park (21146).\nMontgomery County Mobility Solutions: Olney (20832), Damascus (20872), Silver Spring (20910), Clarksburg (20871), Germantown (20876), Gaithersburg (20878), Laytonsville (20882), Chevy Chase (20815), Bethesda (20816), and more.\nHoward County Mobility Solutions: Clarksville (21029), Cooksville (21723), Columbia (21044), Dorsey (21075), Elkridge (21075), Glenelg (21737), Ellicott City (21043), Fulton (20759), Glenwood (21738), Granite (21163), Hanover (21076), Jessup (20794), Lisbon (21765), Highland (20777), Marriottsville (21104), West Friendship (21794), North Laurel (20723), Woodbine (21797), Woodstock (21163), and more.\nCarroll County Mobility Solutions: Eldersburg (21784), Finksburg (21048), Union Bridge (21791), Hampstead (21074), Manchester (21102), Taneytown (21787), Westminster (21157, 21158), New Windsor (21776), Sykesville (21784), Marriottsville (21104), Woodbine (21797), Taneytown (21787), Mount Airy (21771), and more.\nBaltimore County Mobility Solutions: Arbutus (21227), Cockeysville (21030, 21031, 21065), Dundalk (21222), Essex (21221), Catonsville (21228, 21250), Garrison (21055), Lansdowne (21227), Lochearn (21207), Lutherville (21093), Edgemere (21219), Middle River (21220), Overlea (21236), Owings Mills (21117), Parkville (21234), Park Heights (21215), Pikesville (21208), Milford Mill (21244), Randallstown (21133), Rosedale (21237), Timonium (21093), Towson (21204), Reisterstown (21136), White Marsh (21162), Woodlawn (21207), and more.\nFrederick County Mobility Solutions: Frederick (21701, 20702, 21703, 21709), Mount Airy (21771), Urbana (21704), Libertytown (21762), Ijamsville (21754), Walkersville (21793), New Market (21774) , Damascus (20872), and more.\nWashington, DC and Northern Virginia']	['<urn:uuid:b14be32e-088d-464d-8b3a-14b926c635dd>', '<urn:uuid:0d95993b-7e5c-4704-bb4b-a761a469e6cc>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T22:13:20.996306	26	80	1441
68	Can you explain image quality loss in JPEG editing and focus stacking?	JPEG quality degrades each time an image is modified and saved, as compression reduces quality by small amounts that can accumulate. For techniques like focus stacking in macro photography, it's better to work in lossless formats like PNG or TIFF during editing, and only save the final version as JPEG. Focus stacking involves blending multiple images with different focus points to achieve sharp detail throughout.	"['RAW Myths & Facts\nAs the endless debate of RAW versus JPEG continues, more and more people appear to be confused as to why these two formats exist, what each is good for and when to use one or the other. First time DSLR users often wonder which format to choose and make a choice based on some random fact they heard. It does not help that a large number of incorrect information exists on the internet about the RAW and JPEG images. This feature dispels common myths encountered around the internet and in digital photography books.\nGet it right in the first place is the mantra of those who advocate JPEG and Shooting JPEG is like throwing away the negative is the motto of RAW advocates. As usual, both sides are right and it is therefore very important to understand the facts before choosing sides.\nRAW Myths & Facts\nMyth: RAW IS AN IMAGE FILE FORMAT\nFalse. RAW files contain data which includes some information for each pixel but not enough to represent a digital image. Even in cases that actually store full color information for each pixel, such as in cameras which use a Foveon sensor, the stored information is not in the same color-space or scale as normal computer images.\nMyth: RAW IS BETTER THAN JPEG\nFalse. RAW has advantages over JPEG but so does JPEG over RAW. There would not be so much debates on the subject if it was not! The same JPEG as the camera would produce can be obtained from a RAW file using the camera manufacturer\'s RAW development software or in-camera RAW conversion on select models. This means that a RAW can at least produce an image of equal image quality.\nMyth: IMAGES PRODUCED FROM RAW HAVE MORE DYNAMIC RANGE\nFalse. Once a RAW file has been converted into an image, either explicitly by someone or implicitly by an image browser, it is limited by current display technology which cannot display more dynamic range or colors than a JPEG can hold. Nearly every display in existence uses 8 bits per channel (24 bits per pixel) which is what JPEG files store. All common operating systems are also incapable of displaying more than 8-bits per channel, even for the two monitors that can support more.\nMyth: RAW FILES ARE DULL AND LOW CONTRAST\nFalse. With all the hype about RAW quality, a huge number of people\'s first observation about RAW is that they look bad when compared to JPEG images. They only do by accident. As mentioned earlier, a RAW file contains data not an image. Therefore when a preview is for a RAW file is shown, software must produce an image from the RAW data. If it does that poorly, which is commonly the default for many image browsers, what is shown is a poor quality image. Usually the image looks dull and has low local contrast because it shows the entire tone curve uniformly.\nMyth: RAW LOOKS EXACTLY LIKE JPEG\nFalse. Several RAW files actually include an embedded JPEG. This embedded data allows fast previews and obviously looks exactly like a JPEG. Additionally, using the default conversion from camera maker\'s software normally does the exact same RAW to JPEG conversion as the camera. So there are cases where the RAW is converted into the exact same JPEG. However, the benefit of RAW lies in its ability to produce different results. Shooting RAW and then using the default conversion defeats its purpose. Benefits of shooting RAW are only seen when there is time to apply different processing.\nMyth: JPEG IMAGES ARE SOFT AND LACK DETAILS\nFalse. All DSLR cameras have settings that influence sharpness and noise reduction. Manufacturers seem to believe people prefer soft images over noisy images because the defaults are often high noise-reduction and middle sharpness. Those are only defaults and most digital cameras can produce images with finely tuned characteristics. Take the time to set up your camera to produce the most pleasing images possible.\nMyth: JPEG FILES HAVE HORRIBLE ARTIFACTS\nFalse. JPEG uses something called lossy compression which means some information is removed to make the file smaller. In practice the highest quality setting affects less than 2% of pixels by less than 0.4%. The data removed is also chosen to be the least perceptible to the human visual system.\nMyth: EDITING JPEG DEGRADES IMAGE QUALITY\nTrue. Anytime a JPEG is modified and then saved, compression may reduce image quality by a tiny amount. If done repeatedly it can add up to a substantial loss of image quality. The trick used by film artists to work with lossy formats is to do all the work in a lossless format and then save back the final version in a lossy format once all the work is done. Therefore it is better to do repetitive edits in PNG, TIFF or PSD and then save a final JPEG for printing or online use.\nMyth: SHOOTING JPEG WITH A DSLR IS LIKE USING A POINT & SHOOT\nFalse. DSLR cameras have a huge number of advantages over point & shoot digital cameras, including higher image quality and speed. Even DSLRs tend to shoot longer continuously in JPEG than in RAW.\nMyth: REAL PHOTOGRAPHERS SHOOT RAW\nFalse. The professional industry is just as divided on the subject as everyone else. They are pros that exclusively shoot JPEG, ones that exclusively shoot RAW, ones that always shoot JPEG+RAW and ones that use each format part of the time.\nMyth: DNG IS A NON PROPRIETARY RAW FORMAT\nFalse. DNG belongs to Adobe. Adobe grants a license to use its patents to those who wish to use the DNG file format. While it is not owned by any camera manufacturer, it remains a file format that belongs to profit oriented corporation.\nOne of the reasons for DNG is to have a standard format that can be processed by future software long after the camera maker stops making a particular camera or goes out of business. While with is true in principal, the interpretation of the sensor data is specific to each camera and therefore that knowledge must be built in somewhere and kept up to date with new generations of software for legacy support. The one hope for longevity is to process RAW to a commonly understood DNG interpretation. Adobe covers part of what is needed for that in its 2009 DNG update which supports embedded profiles.\nMyth: DNG CAN STORE RAW DATA FROM ANY DIGITAL CAMERA\nTrue today. The DNG format was updated in 2009 to make sure it can hold the data produced by any current digital camera. It does contain special information to treat the information differently if it comes from a Fuji SuperCCD since the arrangement of pixels is different than conventional images. If other new technologies appear that does not fit the current DNG mold, Adobe has to change the DNG format to support those as well. Therefore more than one type of DNG files will exist and software will have to cope with the difference, just like they have to cope with different RAW formats.\nNeocamera Blog is a medium for expressing ideas related to digital cameras and photography. Read about digital cameras in the context of technology, media, art and the world. Latest posts links:\nDxO ViewPoint 3 Review\nReview of DxO ViewPoint 3. Perspective, distortion and horizon correction software.\nNikon D5 XQD Review\nNikon flagship professional DSLR with 20 megapixels Full-Frame CMOS sensor. All-new 153-point Phase-Detect AF sensitive to -4 EV. ISO 50 to unprecedented 3,276,800! 12 FPS Drive for 200 JPEGs or 180 RAW. First Nikon DSLR with 4K Ultra HD video.\nOlympus Professional Lens Roundup\nRoundup of Olympus Professional and Premium lenses: M.Zuiko 7-14mm F/2.8 PRO, M.Zuiko 12-40mm F/2.8 PRO, M.Zuiko 40-150mm F/2.8 PRO, M.Zuiko 12mm F/2, M.Zuiko 60mm F/2.8 Macro.\nOlympus OM-D E-M10 Mark II Review\nOlympus second generation base OM-D with an anti-alias-filter-free 16 MP Four-Thirds CMOS sensor mounted on a 5-axis in-body stabilization system. Speedy 8.5 FPS drive, full HD @ 60 FPS and a wealth of features in a compact and lightweight body. Offers a 2.4 MP 0.45"" EVF with 0.62X magnification and 100% coverage, plus dual control-dials and a highly customizable interface.\nFuji X-Pro2 Review\nFuji flagship XF-mount mirrorless with 24 MP APS-C X-Trans CMOS III sensor. 273-Point AF with 169 Phase-Detect points. 8 FPS Drive, 1080p video. Dual control-dials, direct dials and a hybrid viewfinder in a weather-sealed freezeproof body.\nPanasonic Lumix DMC-ZS100 Review\nThe only premium travel-zoom! 20 megapixels 1"" high-speed CMOS sensor paired with a stabilized 25-250mm F/2.8-5.9 optical zoom. 50 FPS Drive, 4K Ultra-HD video, 1/16000-60s Hybrid Shutter, Post-Shot Focus, 4K Live-Cropping, Time-Lapse Video and more. Dual control-dials plus a built-in EVF with Eye-Start sensor.\nCanon EOS Rebel T6s Review\nNewly designed Rebel with dual control-dials and top status LCD. 24 MP APS-C sensor, Hybrid AF III with 19 all-cross points and on-sensor Phase-Detect AF. 5 FPS Drive and full 1080p HD video capture.\nCanon Powershot G3 X Review\nUltra-zoom with a 25X optical zoom lens and large 20 MP 1"" CMOS sensor in a weather-sealed body with dual control-dials, a lens ring and efficient controls. Captures full 1080p HD video at 60 FPS with internal or external stereo sound.\nBest Digital Cameras of 2015\nThe best new digital cameras of 2015. Plus, find out which ones of 2014 still lead their category. Compact, Premium Cameras, Ultra-Zooms, Mirrorless and DSLR are all covered.\nPanasonic Lumix DMC-G7 Review\n16 megapixels Micro Four-Thirds mirrorless. 2.4 MP 0.5"" EVF with Eye-Start sensor plus dual control-dials. 4K Ultra-HD video, 8 FPS continuous-drive, hybrid shutter with 1/16000-60s shutter-speeds, ISO 100-25600 and Contrast-Detect DFD autofocus system sensitive to -4 EV.', 'As anyone who has experimented with macro photography will know, this style of photography, and more specifically the nature of the subjects and lenses used, makes achieving a suitable depth of field very challenging.\nSome photographers may like to produce the style of image where only one small part of the image is in sharp focus, with the rest receding into a smooth, surreal blur.\nThis is well and good, but sometimes one simply wants all of the details in a macro image to be rendered in sharp focus.\nSufficient depth of field is quite difficult to achieve in macro photography for two main reasons.\nFirstly, the subjects often shot are typically small, which means that one needs to get close to the subject, using a lens whose magnification is life-size (or greater); and secondly, a longer focal length is needed, especially if the subject is a critter which will run or fly away if a photographer gets too close.\nLonger focal lengths and very short subject distances both reduce depth of field, and do so more dramatically when combined.\nOne counter-measure available is to stop the lens’s aperture down to a very narrow aperture. This sounds good, but there’s a hidden danger: diffraction.\nWhen a lens’s aperture is stopped down to an extremely narrow aperture, light rays entering the lens at various oblique angles must bend dramatically to enter the narrow aperture, and then find their way towards the focal plane.\nAt very narrow apertures, diffraction causes a loss of fine detail in images, resulting in a softer appearance. In macro photography, where fine detail is often the very subject the photographer wants to capture, diffraction is more problematic than it would be in other forms of photography, such as landscape photography.\nSo, with a combination of a long focal length, short subject distance and a practical limitation to how far down one can set the lens’s aperure without degrading the quality of the image, how does one gain more depth of field?\nIt’s easier than one would think, and doesn’t require any special software if you already have Adobe Photoshop, and even works with Adobe Photoshop CS4.\nThe technique is called focus stacking, and it involves capturing multiple images of the subject shot multiple times with different focus distances, and digitally blending them.\nEach image captured must be framed identically, but what differs is the field of focus. After each image is captured, it is simply a matter of adjusting the lens’s focus ring by a small amount to render another part of the image in focus.\nPerhaps there might be equipment available which can adjust the focus ring in precise increments, but I’ve never looked into it, in my experience of having produced three macro images using this technique, I’ve found that the adjustment of the focus ring can be done manually, and still produce excellent results.\nAt the capture phase, I recommend using a tripod so that precise framing can be achieved throughout the numerous images that will be captured.\nUsing the ‘live view’ mode on the camera allows for a much better view of the lens’s plane of focus, and also reduces any potential bumps resulting from contact with the camera’s optical viewfinder.\nI also recommend the use of a remote shutter release to further avoid needing to make any contact with the camera.\nSome contact with the camera will be required, as without a device to rotate the lens’s focus ring, it will need to be touched. For that reason, care must be taken so that the camera is not bumped out of alingment.\nHow many images need to be captured? That depends on the subject, the subject distance, the lens’s focal length, and how much adjustment of the lens’s focus ring is required to capture a series of images in which the closest subject matter is in focus, right through to the furthest subject matter.\nYesterday I photographed the intricate details of a rose laden with water droplets, and rather than opting for the arty shallow depth of field prevalent in many macro images, I wanted rich details in sharp focus throughout the image.\nFor my macro photography, I use a Canon EF 180mm f/3.5L Macro USM lens, which is challenging to use at the best of times, but I also attached my Canon Extender EF 2x II, which doubles the focal length.\nI was therefore shooting a relatively small subject at a focal length of 360mm, with not a great deal of distance between the lens and the subject. The result is not much depth of field.\nOne might think that I’d need to shoot a dozen or more images to capture the full field of focus, but in fact I only needed seven images.\nTo produce a focus-stacked composite image in Photoshop, the technique is as follows:\n- After raw conversion, load all of the raw files into Photoshop.\n- One at a time, and in sequential order (this is important), copy each image to the clipboard, and then paste it as a new layer into the first image.\n- Rename the layers using a logical naming scheme (eg, IMG_0001, IMG_0002, etc.).\n- In the layers pallette, select all of the layers.\n- Click the Edit menu, and select ‘Auto-Align Layers…’.\n- Set the Projection set to ‘Auto’ and click OK. Photoshop will align the layers.\n- Click the Edit menu and select ‘Auto-Blend Layers…’.\n- Make sure that ‘Stack Images’ is selected, and click OK.\nDepending on the bit depth, dimensions and number of images to be stacked, it could take a while for Photoshop to complete the focus stacking.\nThe result will be a stack of layers with layer masks on each. Photoshop’s focus stacking algorithm selects the in-focus subject material in each image and masks out the out-of-focus areas such that the resulting image is sharp throughout.\nThe next step is to create a new layer for the composite of the stacked images.\nFrom the top layer, press Cmnd-Alt-Shift-E on a Mac, or Ctrl-Alt-Shift-E. This keyboard-only command will produce a new layer of all lower layers without discarding the layers. This is not the same as the ‘flatten image’ command, which I never recommend.\nOnce the composite image has been captured as its own layer, proceed to apply any other post-processing as desired.\nNow, let’s look at my results from yesterday’s rose image.\nFirstly, here is a montage of my seven source images, and the resulting composite image Photoshop produced using the automatic layer blending option.\nAs can be seen in the above screen capture, images 1 to 7 all have different planes of focus. The composite image in the lower right shows the raw result from Photoshop’s focus stacking algorithm.\nOne caveat of which to be aware is that the technique results in a softening around the edges of the frame, so it will be necessary to marginally crop the image. For that reason, I recommend composing so that no critical subject matter is positioned close to the edges of the frame, as it may be damaged by the ‘soft border’.\nNow, here’s the final image after I completed my post-processing, which included cloning distracting spots, detail enhancement using Calvin Hollywood’s ‘Freaky Detail’ technique, cloning out of distracting spots, and contrast and sharpening adjustments.\nHopefully this article has been helpful in illustrating how a macro photographer can overcome the challenge of very limited depth of field by carefully executing the capture of multiple frames of the same subject at different planes of focus, and using one of Photoshop’s in-built functions to digitally composite the in-focus subject matter in the multiple frames to produce a rich, detailed image with everything rendered in sharp focus.']"	['<urn:uuid:2408c529-dd3a-4516-99d5-cb980fda92df>', '<urn:uuid:70151312-b1ef-45db-8d1d-e2b6feb80917>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T22:13:20.996306	12	65	2903
69	What happens when machines are not properly cared for?	Poorly maintained resources can cause instability and partially or completely pause the production. Malfunctioning machines or complete breakdowns can become a costly process for most companies.	['- Why maintenance is so important?\n- What are the 2 types of maintenance?\n- What is failure based maintenance?\n- What is condition based maintenance?\n- What is a good maintenance strategy?\n- What is the mean of maintenance?\n- What is the purpose of maintenance testing?\n- What does maintenance person mean?\n- What is a maintenance checklist?\n- What means maintenance time?\n- What is routine maintenance?\n- What are the 4 types of maintenance?\n- What is standard maintenance procedure?\n- What are the benefits of maintenance?\n- What is maintenance and its types?\n- What is maintenance process?\n- What are the 3 types of maintenance?\n- Why is reliability centered maintenance?\n- What are maintenance skills?\n- How can you improve asset reliability?\n- What are the duties of maintenance?\n- What is an example of maintenance?\n- What does low maintenance mean?\n- What is on condition?\n- What are the classification of maintenance?\n- What is maintenance in simple words?\n- What is hard time maintenance?\nWhy maintenance is so important?\nMaintenance is an important factor in quality assurance and in some cases determines the long-term success of a company.\nPoorly maintained resources can cause instability and partially or completely pause the production.\nMalfunctioning machines or complete breakdowns can become a costly process for most companies..\nWhat are the 2 types of maintenance?\nThe basic types of maintenance falling under MRO include:Preventive maintenance, also known as PM.Corrective maintenance, where equipment is repaired or replaced after wear, malfunction or break down.More items…\nWhat is failure based maintenance?\nCorrective or Failure-based Maintenance (FBM) is unscheduled maintenance or repair to return the machine to a defined state. There are no interventions until a failure has occurred. … Since the failures are unplanned, they might result in a big loss to the organization in terms of cost and time.\nWhat is condition based maintenance?\nDefinition. Condition-based maintenance (CBM) is a maintenance strategy that monitors the actual condition of an asset to decide what maintenance needs to be done. … Condition-based maintenance can be applied to mission critical and non-mission critical assets.\nWhat is a good maintenance strategy?\nAn effective maintenance strategy is concerned with maximizing equipment uptime and facility performance while balancing the associated resources expended and ultimately the cost. We need to ensure that we are getting sufficient return on our investment.\nWhat is the mean of maintenance?\nnoun. the act of maintaining: the maintenance of proper oral hygiene. … care or upkeep, as of machinery or property: With proper maintenance the car will last for many years. means of upkeep, support, or subsistence; livelihood: to provide a comfortable maintenance.\nWhat is the purpose of maintenance testing?\nMaintenance testing is a test that is performed to either identify equipment problems, diagnose equipment problems or to confirm that repair measures have been effective.\nWhat does maintenance person mean?\nA maintenance person, also known as general. maintenance worker, repair worker, handyman or. repair technician, is responsible for keeping buildings, shops or equipment in good repair.\nWhat is a maintenance checklist?\nMaintenance Checklist. An itemized list of discrete maintenance tasks that have been prepared by the manufacturers of the asset and/or other subject matter experts such as consultants. Checklists are the basic building blocks of a maintenance program. Organization of the Checklist.\nWhat means maintenance time?\nScheduled Maintenance Time means the time that the System is not in service due to a scheduled activity. Scheduled maintenance time is planned downtime with the prior permission of the Department, during non-business hours.\nWhat is routine maintenance?\nRoutine maintenance refers to any maintenance task that is done on a planned and ongoing basis to identify and prevent problems before they result in equipment failure. Some common routine maintenance includes regular inspections or service work. … Routine maintenance tasks are usually fairly straightforward.\nWhat are the 4 types of maintenance?\nFour general types of maintenance philosophies can be identified, namely corrective, preventive, risk-based and condition-based maintenance.\nWhat is standard maintenance procedure?\nA standard maintenance procedure is a detailed list of steps that describes how to perform a maintenance task and is also a documented standard to which the job or task should be performed.\nWhat are the benefits of maintenance?\nAdvantages of preventive maintenanceLess equipment downtime.Fewer interruptions to critical operations.Longer asset life.Improved efficiency (assets in good repair tend to operate better)Increased workplace safety and improved compliance with OSHA.\nWhat is maintenance and its types?\nMaintenance, in general, can be defined as efforts taken to keep the condition and performance of a machine always like the condition and performance of the machine when it is still new. … The type of maintenance cannot be equated for each equipment, which depends on the method, cost and critical level.\nWhat is maintenance process?\nThe revision and qualification of a software product after delivery to correct faults, to improve performance or other attributes, is what Software Maintenance Process was defined. It is a vast activity that includes optimization, error correction, deletion of discarded features, and enhancement of existing features.\nWhat are the 3 types of maintenance?\nName the three main types of maintenance?Preventive Maintenance: It is to maintain a level of certain service on equipment,to remove intermediate fault. … Predictive Maintenance: It pursues for report the status and operational capacity of the installations by knowing the values of certain variables, which represent such state and operational ability.More items…•\nWhy is reliability centered maintenance?\nReliability centered maintenance is a maintenance strategy that focuses on key assets and the specific things that make them break. … Instead of burning brain power on organizing failure codes, you can spend your mental energy on determining which assets need the most help.\nWhat are maintenance skills?\nMaintenance and janitorial skills are those related to repairing and maintaining the function and/or cleanliness of equipment, buildings, and machinery. Such skills are in high demand across the country, ranging from plumbers to carpenters to custodians.\nHow can you improve asset reliability?\n8 steps to improve asset reliability: Part 1Align the organization for reliability.Determine your maintenance strategies.The MRO storeroom.Identifying and prioritizing the work.\nWhat are the duties of maintenance?\nResponsibilitiesPerform cleaning activities such as dusting, mopping etc.Perform minor fixes such as repairing broken locks, filling gaps on walls etc.Check control panels and electrical wiring to identify issues.Install appliances and equipment.Do garden/yard upkeep by mowing lawn, collecting trash etc.More items…\nWhat is an example of maintenance?\nThe definition of maintenance is providing support or upkeep to something. An example of maintenance is a janitor keeping a school clean. (1) Hardware maintenance is the testing and cleaning of equipment. Actions performed to keep some machine or system functioning or in service.\nWhat does low maintenance mean?\nalso low maintenance. adjective. If you describe something or someone as low-maintenance, you mean that they require very little time, money, or effort to look after them.\nWhat is on condition?\nphrase. When you agree to do something on condition that something else happens, you mean that you will only do it if this other thing also happens. He agreed to speak to reporters on condition that he was not identified.\nWhat are the classification of maintenance?\nWhat are the Different Types of Maintenance?Preventive Maintenance.Condition-Based Maintenance.Predictive Maintenance.Corrective Maintenance.Predetermined Maintenance.Gaining Maintenance Knowledge with Interplay Learning.\nWhat is maintenance in simple words?\n1 : the act of maintaining : the state of being maintained : support The building has suffered from years of poor maintenance. 2 : something that maintains at least half of them are living parasitically on the other half instead of producing maintenance for themselves— G. B. Shaw.\nWhat is hard time maintenance?\nHard-time: A primary maintenance process under which an item must be removed from service at or before a scheduled specified time. Airframe checks and Landing gear overhaul events are example of events that are expressed as hard-time events.']	['<urn:uuid:6840e82b-aede-418f-add5-36d91ed215ed>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T22:13:20.996306	9	26	1311
70	when europe begin execute magic women	The witch-burning period in Europe primarily occurred between 1480-1750, during the time of the Reformation and Thirty Years War. However, before the 1400s, witch-burning was actually banned by the Church under penalty of death. The shift began in the early 15th century when popular pressure from German-speaking central Europe led the Church to change its stance. England was relatively late to join this movement, not passing a law on witchcraft until 1542.	"['Aside from that judgment, I am not ready to weigh further on the book\'s quality; but he does make one claim that is quite wrong. He is asking for a reconciliation of sorts between the old ""paternalistic, Christian heritage"" and the new world. Along the way he defends the old faith with a historical reference: ""[A]s Rene Girard says it is not because we invented science that we stopped burning witches, but rather when, because of the Judeo-Christianity, we stopped burning witches that we invented science."" (p. 39-40)\nThis is wrong as a point of history. We invented science when we started burning witches.\nThe usual dates for the witch-burning craze are 1480-1750, around the time of the Reformation and the Thirty Years War. It was about the time that there was this deep questioning of tradition -- that would lead to the Renaissance, the Enlightenment, and modern science -- that there also coincidentally spread a new cultural distrust of the feminine.\nI was just reading a collection of essays called Friendship in Medieval Europe that shows how much things changed. Several of these essays note that -- contrary to what many people might expect if they were raised with the modern critique of Western history as ""paternalistic"" -- the early and High Middle Ages were a golden age for relations between the sexes. Of the Anglo-Saxon period, we learn:\n""What we can conclude on a formal level, however, is that \'friendship\' is not significantly limited, either in a hierarchical manner -- Boniface and Alcuin are friends with bishops as well as priests, abbots as well as monks -- or with regard to gender -- they are friends both with men and women. Every person (ominis homo -- and not: \'every male\') needs a friend, as the Anglo-Saxon abbess Eangyth writes to St. Boniface; and she chooses him to be hers. Alcuin counts several women among his friends."" (125)It turns out that there are vast examples of robust friendships between men and women throughout the early and High middle ages, and into the late middle ages, including whole collections of letters now being studied by scholars across Europe. There is particular importance placed on the exchange of poetry between men and women as tokens of friendship; in the last few years, we\'ve gained awareness of a huge amount of female-written Medieval poetry that is normally captured in letters between friends, including between monastic communities and nunneries. St. Thomas Aquinas wrote on the virtue of friendship, drawing on Aristotle\'s dictum that we can say to our friend, ""You are the other half of my soul,"" (in Aristotle this is more usually translated \'the friend is our other self\'); and the priest Richard Rolle, who died in 1349, wrote that in spite of the dangers of physical attraction between men and women, ""that sort of friendship is not improper, but rewarding, if it is practiced with a good intention.""\nHowever, in 1401 the chancellor of the University of Paris, Jean Gerson, wrote a sermon in which he charged that friendship with women put you at risk of ""diabolical deception"" and \'great evil unless God in his goodness averts it.\' Gerson nevertheless knew some women, including Christine de Pisan, whom he engaged with intellectually (and agreed with) in the famous debate of the morality of Roman de la Rose. Christine de Pisan, in the early 1400s, was challenging this new, rising misogyny and objected to the vulgar language that the book put into the mouths of noble women. But when Gerson wrote his own book about it, he ignored her contributions and did not mention her at all. Gerson lived at the same time as Bernardino of Siena, whose traveling sermons popularized the idea of witches.\nAs for witch-burning in the early and High Middle Ages, it was actually banned by the Church under penalty of death. The same decree, in 785, banned belief in witches in order to suppress violence against women. That held until the 1400s, when popular pressure in Switzerland and Germany began to force the Church to rethink its stance. As late as the 1390s, though, women confessed to practicing \'white magic\' to Inquisitors, but the Inquisitors had nothing in their guidelines about women using magic and had to write for advice.\nIt looks like the early 15th century is the turning point in which the Church (following popular movements from German-speaking central Europe) began to take the dark view of female sexuality that we have come to associate with witch-burnings.\nThe easy friendships between men and women that we observe in Chaucer, who lets the Wife of Bath have a merry wit and deliver a good long sermon on the virtues of women, did not quite die with him in 1400. Sir Thomas Malory, born in 1405, stands at the end of this golden age. He was in the English speaking world, which did not receive the witch-burning craze until after his death (England did not pass a law on witchcraft until 1542). Though it is commonplace to blame Malory for making Guinevere\'s sexuality the cause of the fall of Camelot, it is clear that Malory does not view Guinevere as a bad or wicked woman. In fact, one of the few times that Malory directly addresses his audience is to make the point that ""she was a true lover and therefore she had a good end."" Her love with Lancelot, though it had tragic practical consequences, is what redeems her for Malory, not what damns her.\nEven the wicked sorceress Morgan le Fay is not a witch, but a student of necromancy -- see the article on the meaning of this in the Early and High Middle Ages, which is different from the word\'s meaning today. Of course, Morgan was a necromancer for Malory, but earlier she had been something else, not a witch but a fairy. In this guise she is the heroine of Marie de France\'s Lanval, saving the knight from an unfair judgment.\nThere is a doctrine that the Middle Ages were a dark and miserable time, and that the story of the Reformation, the Renaissance, the Enlightenment is a story of ever-marching progress toward a more rational society. In truth, the story is more complicated. The same breaking of the old order that allowed for a new scientific world view also let loose a great deal darker. It was not when we became scientific that we stopped burning witches. Rather, that was just when we began.\nHistory Ireland has a counterexample, the earliest witch-burning case I\'ve heard of from 1324. It\'s worth reading their account in full. The only one who was burned was a minor figure, a servant of the family chiefly charged with heresy; and the secular law worked hard to defy the Church in the face of a very enthusiastic (and foreign) bishop. Of the principal defendants, the woman escaped by flight (she was extraordinarily wealthy), and her husband got off with a penance. The Church forced him to go on pilgrimage and to re-do a part of the local cathedral with lead. It turned out the lead was too heavy for the cathedral roof, which collapsed as a consequence of his fulfilling the penance the bishop had imposed. Divine judgment, possibly.\nIt\'s interesting to see how this early case played out, compared with the enthusiasm that would appear in later centuries.']"	['<urn:uuid:c34fbf46-3799-4251-b47e-7a019b9b5f9d>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T22:13:20.996306	6	72	1232
71	how long does tick disease transmission take to host after attachment	Most tick-borne diseases will take several hours to transmit to a host. The sooner a tick is located and removed, the lower the risk of disease.	"[""Eggs are laid on the pet and fall to the ground where they hatch into larvae. The larval stages feed on organic material in the environment. They live deep in the carpet or lawn.\nThe pupae develop when the larvae form cocoons. They quickly emerge as adult fleas when the ideal temperature (above 70 degrees) and humidity (above 70%) exist.\nThe adult flea has up to 30 days to find a host or it will die. After the female's first blood meal, the reproductive cycle begins, and within 48 hours she starts laying her eggs. From this point on, the flea does not leave the pet except by death from insecticide, preening by the pet, or old age.\nSummer months provide perfect breeding, feeding, and hatching conditions for fleas. When conditions are right, the insect can go through its entire life cycle (from egg to adult) in as few as 10 - 14 days. The adult's life span on the pet can extend several weeks. Immature fleas can exist up to 2 years in the environment. Even in winter, the fight against the flea may not end. Outside conditions may prevent or limit flea development, but inside the home, temperature conditions still permit growth.\nUntreated animals can carry and support a colony of 60 - 100 fleas per week. Each female in a colony lays an average of 20 eggs per day and can produce as many as 600 eggs each month. A pet carrying approximately 60 fleas may yield as many as 18,000 eggs in a month.\nThe adult flea spends its entire life on the pet. If it leaves the host, chances for survival are greatly reduced. Adult fleas do not live in carpets, bedding, or furniture. However, eggs, larvae, and pupae do. This is the reason that successful flea control programs involve both the pet and the environment. The ideal time to start or intensify the flea control effort is early in the season.\nControl and Treatment\n- Vacuum all surfaces of furniture, carpets and floors thoroughly. Dispose of vacuum cleaner bag.\n- Treat all areas the pet has access to, including basement, bedrooms, etc. with a premise insecticide. Foggers are available for large areas, trigger spray products for smaller rooms and under furniture. Insecticides with IGR (insect growth regulators) will provide the broadest eradication. Be as thorough as possible; if only a few fleas survive, the cycle will start again. Sometimes a second treatment is necessary several weeks later.\n- Treat all pets in the household. Dips, sprays, powders and topical products are available. Read directions carefully. Not all products are safe for cats, and some may not be used on young puppies or kittens.\n- Treat or discard any bedding the pet uses. Premise insecticide should be used in dog houses or kennels.\n- Retreat pets according to the label of the product you are using to prevent re-infestation.\nTicks can transmit diseases and even cause anemia or paralysis. Ticks feed on the blood of their hosts. They are attracted to warmth and motion, often seeking out mammals, including dogs. Ticks tend to hide out in tall grass or plants in wooded areas waiting for prospective hosts. Once a host is found, the tick climbs on and attaches its mouthparts into the skin, beginning the blood meal. Once locked in place, the tick will not detach until its meal is complete. It may continue to feed for several hours to days, depending on the type of tick. On dogs, ticks often attach themselves in crevices and/or areas with little to no hair – typically in and around the ears, the areas where the insides of the legs meet the body, between the toes, and within skin folds. Most species of ticks go through four life stages - eggs, larvae, nymphs, and adults. All stages beyond eggs will attach to a host for a blood meal (and must do so in order to mature). Depending on species, the life span of a tick can be several months to years, and female adults can lay hundreds to thousands of eggs at a time.\nThe following types of ticks are among the most common seen in North America:\nBrown Dog Tick\nDangers of Ticks\nThough they are known vectors of disease, not all ticks transmit disease – in fact, many ticks do not even carry diseases. However, the threat of disease is always present where ticks are concerned, and these risks should always be taken seriously.\nMost tick-borne diseases will take several hours to transmit to a host, so the sooner a tick is located and removed, the lower the risk of disease. The symptoms of most tick-borne diseases include fever and lethargy, though some can also cause weakness, lameness, joint swelling and/or anemia.\nSigns may take days, weeks or months to appear. Some ticks can cause a temporary condition called tick paralysis, which is manifested by a gradual onset of difficulty walking that may develop into paralysis. These signs typically begin to resolve after tick is removed. If you notice these or any other signs of illness in your dog, contact your veterinarian as soon as possible so that proper testing and necessary treatments can begin.\nThe following are some of the most common tick-borne diseases:\n- Lyme disease\n- Rocky Mountain Spotted Fever""]"	['<urn:uuid:b0c37726-6fd8-4c83-a7e9-a732ea4dbd1d>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T22:13:20.996306	11	26	887
72	duration memory training alzheimers patients	The memory training intervention for Alzheimer's disease patients lasted 8 weeks.	['News and Views - Imaging in Medicine (2014) Volume 6, Issue 1\nApplication of functional neuroimaging to evaluating the efficacy of cognitive rehabilitation in neurological populationsNancy D Chiaravalloti*\nKessler Foundation, 300 Executive Drive, West Orange, NJ 07052, USA and Department of Physical Medicine & Rehabilitation, Rutgers, New Jersey Medical School, Newark, NJ, USA\n- Corresponding Author:\n- Nancy D Chiaravalloti\nKessler Foundation, 300 Executive Drive\nWest Orange, NJ 07052, USA and Department of Physical Medicine & Rehabilitation, Rutgers\nNew Jersey Medical School, Newark, NJ, USA\nTel: +1 973 324 8440\nFax: +1 973 324 8373\nE-mail: [email protected]\nWith the increasing availability of various neuroimaging techniques, as well as the continuous advancements in technology, neuroimaging applications in rehabilitation have expanded greatly in recent years. Alterations on neurofunctional imaging have been shown to be related to disease pathology, as well as impairments in specific cognitive domains across multiple neurological populations [1–3]. Such neuroimaging technologies have thus been recently applied to the study of cognitive rehabilitation efficacy in neurological populations.\nResearchers have theorized that improvements in cognition, independent of the disease process, such as in response to cognitive rehabilitation, would result in measureable changes on functional scans. Functional techniques recently applied to evaluate cognitive rehabilitation effectiveness have included both functional MRI (fMRI) and functional and resting state connectivity [4– 6]. Early thought leaders in the application of functional neuroimaging to evaluate changes in cognition have theorized that functional imaging techniques might be applied to test the belief that the effectiveness of cognitive rehabilitation is the result of plasticity in the processing of the brain . One might thus hypothesize that improvement in cognitive deficits following cognitive rehabilitation would also lead to changes in patterns of brain activation.\nInitial research examining the brain’s response to cognitive training demonstrates both increases and decreases in cerebral activity following treatment . In healthy individuals, Olesen et al.  noted increased activation on fMRI in parietal and frontal cortices and decreased activation in the anterior cingulate after 5 weeks of working memory training. Work from our group  showed greater activation on fMRI following a ten-session memory intervention in persons with multiple sclerosis (MS) in the frontal, parietal, precuneus and parahippocampal regions. Similar results were noted by Fillipi and colleagues  in the dorsolateral prefrontal cortex and precuneus, also in persons with MS. Changes in cerebral activity were correlated with cognitive improvement in both Chiaravalloti et al.  and Filippi et al. . Similar findings have been noted in cognitive rehabilitation studies in both schizophrenia  and mild cognitive impairment  patients.\nIn addition to an evaluation of changes in brain function with cognitive rehabilitation, it is hoped that, in the future, functional neuroimaging will provide clinicians with information regarding who will and will not benefit from specific forms of cognitive rehabilitation. That is, neurofunctional markers prior to treatment may predict who may or may not show a benefit from treatment. Functional neuroimaging may further be used to maximize the efficacy of specific cognitive rehabilitation tools through the identification of changes in brain functioning associated with improvements in the treated domain (e.g., working memory). The identification of a targeted brain region associated with a particular outcome may lead to the identification of other means by which one can maximize such changes in those brain regions through cognitive rehabilitation or other types of intervention (e.g., transcranial direct current stimulation or medication) and examine the resultant cognitive performance.\nGiven the extensive rationale for applying functional neuroimaging to the field of cognitive rehabilitation, it is encouraging to note that a few recent studies have reported changes in functional neuroimaging following cognitive rehabilitation interventions. Two of the most recent articles are summarized below. For details, the reader is encouraged to read the full publications.\nEvaluation of: van Paasschen J, Clare L, Yuen KS et al. Review of cognitive rehabilitation changes memory-related brain activity in people with Alzheimer disease. Neurorehabil. Neural Repair 27(5), 448–459 (2013).\nThe current study examines the neurofunctional changes on fMRI from pre- to post-treatment with an 8-week behavioral intervention for cognitive deficits in persons with early stage Alzheimer’s disease (AD). A total of 19 participants were assigned to a treatment group (n = 7) or a control group (n = 12). Participants underwent scanning procedures during a nonfamiliar face-name encoding and recognition task both prior to and following treatment. There were no differences noted between the groups in behavioral performance. However, a differential response to treatment was noted on the recognition task, with the treatment group showing significant increases in activation in bilateral prefrontal areas and the bilateral insula, while the control group showed decreases in the same regions. No changes were noted from before to after treatment on the encoding task.\nThe authors discuss the findings in light of the limited work on neurofunctional changes from before to after cognitive rehabilitation in AD. They hypothesize that the increased activation during recognition performance was due to a partial restoration of recognition functioning from treatment in brain areas within the frontal lobes that are less impacted in early-stage AD. Thus, the application of neuroimaging in the current study afforded investigators with additional sensitivity in their ability to measure the brain’s response to treatment and draw some conclusions regarding response to treatment in distinct brain regions. This suggests that, in this case, neuroimaging could potentially be used as a biomarker of brain plasticity. Future work with larger sample sizes could potentially seek to identify persons that benefit from treatment from those that do not at a neuroanatomical and neurofunctional level. Such data could be used to inform clinical care and maximize the efficacy of the treatment paradigm for the targeted population.\nEvaluation of: Parisi L, Rocca MA, Mattioli F et al. Review of changes of brain resting state functional connectivity predict the persistence of cognitive rehabilitation effects in patients with multiple sclerosis. Mult. Scler. doi:10.1177/1352458513505692 (2013) (Epub ahead of print).\nThe currently reviewed study examined the long-term efficacy of a 12-week cognitive rehabilitation paradigm in persons with MS via an evaluation 6 months after treatment completion. Outcome measures included both neuropsychological performance data and functional neuroimaging data, namely resting state (RS) functional connectivity (FC). A total of 18 individuals with MS with cognitive deficits were randomly assigned to a treatment group (n = 9) or a control group (n = 9). All participants completed a neuropsychological evaluation pretreatment, after 12 weeks of treatment and at the 6-month follow-up evaluation. RS fMRI was obtained at baseline and follow-up, but not at 6 months post-treatment. The treatment paradigm consistent of 12 weeks of cross-domain interventions, addressing attention, information processing and executive functioning.\nResults indicated an improvement in attention, executive functioning, depression and quality of life from before to after treatment in the treatment group, but not the control group. Performance on tests in each of these domains was correlated with changes in RS FC in networks underlying cognitive functioning, as well as the anterior cingulum. While neuroimaging was not repeated at the long-term follow-up, neuropsychological assessment was repeated and changes in RS FC from baseline-to-immediate follow-up were utilized to predict the long-term maintenance of the treatment effect over this 6-month period. RS FC changes in the default mode network from before to after treatment predicted cognitive performance and less severe depression at the 6-month follow-up. By contrast, RS FC changes from baseline to immediately post-treatment in the executive network predicted better quality of life 6 months post-treatment. Findings from this study are interesting in that neuroimaging parameters, such as RS FC can clearly be engaged to help predict the long-term maintenance of cognitive changes over time. Depression and quality of life can also be potentially predicted by such imaging parameters. Future research should seek to replicate these results and examine the clinical utility of such data.\nFinancial & competing interests disclosure\nND Chiaravalloti is supported by an NIH grant 1R01HD045798. The author has no other relevant affiliations or financial involvement with any organization or entity with a financial interest in or financial conflict with the subject matter or materials discussed in the manuscript apart from those disclosed.\nNo writing assistance was utilized in the production of this manuscript.\n- Chiaravalloti ND, Hillary FG, DeLuca J, Ricker JH, Liu WC, Kalnin AJ. Cerebral activation patterns during working memory performance in multiple sclerosis using fMRI. J. Clin. Exper. Neuropsychol. 27, 33–54 (2005).\n- Hillary FG, Chiaravalloti ND, DeLuca J, Ricker JH, Liu WC, Kalnin AJ. An investigation of working memory rehearsal in multiple sclerosis using fMRI. J. Clin. Exper. Neuropsychol. 25(7), 965–978 (2003).\n- Mainero C, Caramia F, Pozzilli C et al. fMRI evidence of brain reorganization during attention and memory tasks in multiple sclerosis. Neuroimage 21(3), 858–867 (2004).\n- Chiaravalloti ND, Wylie G, Leavitt V, Deluca J. Increased cerebral activation after behavioral treatment for memory deficits in MS. J. Neurol. 259(7), 1337–1346 (2012).\n- Leavitt VM, Wylie G, Girgis P, DeLuca J, Chiaravalloti N. Increased functional connectivity within memory networks following memory rehabilitation in multiple sclerosis. Brain Imaging Rev. doi:10.1007/s11682-012-9183-2 (2013) (Epub ahead of print).\n- Filippi M, Riccitelli G, Mattioli F et al. Multiple sclerosis: effects of cognitive rehabilitation on structural and functional MD imaging measures – an explorative study. Radiology 262(3), 932–940 (2012).\n- Penner IK, Opwis K, Kappos L. Relation between functional brain imaging, cognitive impairment and cognitive rehabilitation in patients with multiple sclerosis. J. Neurol. 254(Suppl. 2), II53–II57 (2007).\n- Olesen PJ, Westerberg H, Klingberg T. Increased prefrontal and parietal activity after trianing of working memory. Nat. Neurosci. 7(1), 75–79 (2004).\n- Haut KM, Lim KO, MacDonald A 3rd. Prefrontal cortical changes follow cognitive training in patients with chronic schizophrenia: effects of practice, generalization, and specificity. Neuropsychopharmacology 35(9), 1850–1859 (2010).\n- Belleville S, Clement F, Mellah S et al. Training-related brain plasticity in subjects at risk of developing Alzhiemer’s disease. Brain 134(Pt 6), 1623–1634 (2011).']	['<urn:uuid:bdba762c-111d-4114-a414-9b84ade035d1>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T22:13:20.996306	5	11	1630
73	california drought water conservation methods saudi arabia alternative solutions comparison	In California, conservation methods include using greywater systems that can save 16-40% of household water consumption (about 56,780 liters yearly) and developing large-scale recycling plants. In Saudi Arabia, while desalination has received enormous investments with 30 plants producing 1.1 billion cubic meters yearly, wastewater reuse is considered a more cost-effective and environmentally sustainable alternative, with 33 treatment plants capable of processing 748 billion cubic meters annually.	"['The liquid in the trough below looks remarkably like iced tea. But trust me, you don\'t want to drink it.\n""This is the first step of our treatment process,"" says Denis Bilodeau, vice-president of the Orange County Water District, on a metal catwalk next to a canal of municipal sewage.\n- California drought: \'Water cops\' hope to curb water waste\n- Farmers hit with record $1.5M fine for allegedly stealing water\nWater is precious in the drought-plagued region, and this massive water recycling plant in Fountain Valley, an hour south of Los Angeles, was built to convert raw sewage into drinkable H20.\nThe brown liquid begins to bubble. Then it will be drawn into straw-like filters 1/300th the size of a human hair.\n""That will keep all of the bacteria and viruses out,"" Bilodeau says.\nThe sewage will go through reverse osmosis and then be treated with intense UV light from bulbs manufactured by Ontario company Trojan UV.\n"" It\'s actually clean down to the atomic level, where all we have left is the H20 molecules,"" Bilodeau says. ""And now this plant produces enough water to serve 800,000 people.""\nGot any more sewage?\nAccording to the Orange County Water District, the facility is the largest of its kind in the world. Built in 2008, it has already gone through one expansion, and now Bilodeau says they\'re preparing for a second.\n""Our only limiting factor now is that we need more sewage water to process,"" Bilodeau says. ""Here in Orange County we actually have less sewage than we did 20 years ago because of all of the water conservation that takes place with low-flow toilets and low-flow shower heads.""\nUrban Californians are using a third less water than they were two years ago. But letting lawns go brown and fountains go dry isn\'t enough. Many here realize that at home they have to not just use less, but re-use more.\nThat\'s why Laura Allen decided to modified some of her appliances. Half of California\'s urban water is used on landscaping. But the garden at her apartment building gets most of its water from its sinks and washing machines.\nAs she washes her hands, the dirty water is piped from the sink into small underground reservoirs covered with small round covers. Water trickles out of the pipe and into the reservoir through a layer of wood shavings.\n""The greywater flows through, it soaks through the wood chips. And they are actually the filter, so they catch all the lint or debris in the water, and the greywater soaks down into the soil,"" Allen says.\nThe washing machine, too, has been retrofitted with a simple valve.\n""One side of the valve sends the water back to the sewer or the septic, the other side goes into our irrigation system,"" Allen says. ""You have to make sure you\'re using plant-friendly products… if you use that, the water\'s great quality for irrigation.""\nAs founder of the group Greywater Action, she now teaches people how to install their own water recycling systems, which are becoming more popular as Californians become more aware of the drought and receive generous incentives from water utilities.\n""Generally speaking, you can save 16 to 40 per cent of your water consumption,"" Allen says.\nThat\'s about 56,780 litres a year for an average household. Which is helpful, but some cities are starting to think a lot bigger.\nThis week, the the Metropolitan Water District of Southern California presented a plan to develop a water recycling plant in the Los Angeles area that could provide enough water for 300,000 people for a year. It would be even bigger than the mega-plant in Orange County, which generates enough to cover a quarter of the needs of the district\'s more than two million residents.\n""Once we explain to people that the water we create here is actually cleaner than bottled water or what\'s in their tap,"" Bilodeau says, ""then they understand that this water is very safe to drink.""\nExcept no-one will actually drink this water — at least not right away.\nRegardless of how much the water is filtered, health officials here still don\'t allow it to go directly from toilet-to-tap.\n""We\'re very mindful that perhaps the public isn\'t quite ready for that step yet,"" Bilodeau says.\nInstead, it\'ll be piped underground to replenish California\'s dwindling groundwater, which may eventually get drawn back into the municipal water supply.\nBut he\'s certain that soon Californians will have no choice.\n""The future is here,"" Bilodeau says.', 'Fresh water is an indispensable resource for human livelihood, agricultural irrigation and economic development (Brooks, 2007). However, due to the rapid population growth and the limited reserves, increasing regions have faced serious scarcity of fresh water (Williamson, 2010). Saudi Arabia is one of the driest countries in the world (CIA, 2011). According to World Bank (2011), the world average fresh water consumption is nearly 7000m³/year/person, while the water resource per capita in Saudi Arabia is less than 1200m³/year/person. In order to satisfy the demand for water, Saudi Arabia currently supplies fresh water via deep drilling of fossil groundwater (UNESCO, 2009).\nNevertheless, society increasingly recognises that those water resources are non-renewable and are liable to be reduced by the overexploited boreholes and wells. Thus Saudi Arabia needs to find alternative and sustainable methods to solve these issues. Since there is abundant sea water around Saudi Arabia, large-scale desalination could be the ideal solution to water scarcity. However, the expensive cost and the detrimental influence on the environment might limit the scale and sustainability of this method. Due to the cheap cost and the minor environmental damage, wastewater reuse is regarded as another potential solution. However, it seems to have a low social acceptance.\nTherefore, this report will compare the feasibility of desalination and water reuse in terms of cost, social acceptance and environmental impacts, thereby exploring the most suitable method to deal with the scarcity of water in Saudi Arabia.\nSaudi Arabia is located in the Middle East, bordering the Persian Gulf and the Red Sea (CIA, 2011). It is famous for the abundant reserves of oil and gas. However, the fresh water resources in Saudi Arabia are very limited. According to World Bank (2011), there is no one river and lake with perennial water throughout this country. Furthermore, due to the influence of the subtropical climate, the annual precipitation is only about 100 mm and the climate is hot and dry (ibid.). Additionally, rapid population growth has caused higher increase of demand for water (Abderrahman, 2000). Shortages of water have constrained the development of agriculture and economy (Williamson, 2010).\nSince the underground water is estimated to be able to supply for 320 years, the underground water is still the principal source of water at present (UNESCO, 2009: 100). However, with the increasing awareness of defects of this method, the focus of the future development of water provision has shifted to other sustainable water technologies. In order to deal with fresh water shortages, desalination has received enormous investments. According to Abu-Arabi (2007), in 2004 the number of desalination industries reached 30 and they can supply 1.1 billion cubic metres of fresh water per year.\nWastewater reuse is regarded as another future means of water provision. According to Bashitialshaaer et al (2009), in 2009 there were 33 wastewater treatment plants with a capacity of 748 billion cubic metres per year.\nCost should be the principal consideration of water provision because an expensive cost might limit the scale of application of methods. This also includes the cost of energy consumption.\nSocial acceptance plays a significant role in the development of water supply technologies. If the water cannot be accepted by society, it will lead to very little consumption.\nEnvironment has a profound influence on human beings. In order to prevent water supply technology undermining the environment, its impacts on the environment should be considered.\n4. Presentation of options\nDesalination is “a specific treatment process to take minerals from saline water to purify for drinking water and irrigation” (Al-Sahlawi, 1999). Sometimes this process is used to take salinity and other pollutants from wastewater. The general method of desalination is reverse osmosis or multi-stage flash distillation (Lone Star Chapter of the Sierra Club, 2008).\n4.2 Water Reuse\nWater reuse means treating wastewater to a specific quality, and then using treated or reclaimed water from one application for another application (McKenzie, 2005; Asano, 2006). The resources for wastewater reuse are various; according to Asano (2006), they could be domestic wastewater, industrial sewage, municipal sewage or agricultural wastewater.\n5. Comparison of Options\nAlthough the cost of desalination has decreased dramatically in the past three decades, it is still expensive to use in large scale. Alghariani (2003) points out that the expenditure of desalination consists of initial investment for equipment, running costs (including staff and maintenance), as well as chemicals or specialised parts. At present, according to the Third World Water Assessment Report (UNESCO, 2009: 155), the average cost of desalination is between $0.60/m³ and $0.80/m³. Moreover, Owens and Brunsdale (2000, cited in Alghariani, 2003: 5) claim that the cost of desalination in Florida can even be less than $0.55/m³, which is one-tenth of the cost price in 1979.\nHowever, as Wright (2009) points out, this cost is still higher than other water supply technologies. Apart from expenditure, desalination is generally considered a high-cost process due to the enormous energy consumption (Abu-Arabi, 2007). Nevertheless, this does not seem to be a problem for Saudi Arabia. According to the CIA (2010), the proven oil and gas reserves in Saudi Arabia are respectively the first and fifth in the world. As Abu-Arabi (2007) points out, in Saudi Arabia the annual solar energy received by each square kilometre of land is equivalent to 1.5 million barrels of crude oil. Abundant energy may lead to a low price.\nNevertheless, oil is non-renewable and oil reserves are estimated to last less than one century (World Energy Council, 2010), so the consumption of energy should be taken into account as a considerable cost. Moreover, the treatment of the waste gas generated by desalination also increases the cost (Al-Sahlawi, 1999).\nThe cost of water reuse is influenced by various factors such as treatment level, intended reuse options, location of treatment, wastewater collection and transportation. According to Qadir et al (2009), the average cost of recycling water is approximately $1.79 per cubic metre. However, compared to desalination, wastewater reuse has the advantage of cost. Fryer (2010) demonstrates that the relative marginal cost of seawater desalination is higher than water recycling, and amounts to up to $2000 per acre-foot. The water recycling represented a general fluctuation pattern between approximately $300 and $1000 per acre-foot (Fryer, 2010). Even so, water recycling appears cheaper than desalination.\n5.2 Social acceptance\nWhile both options can generate safe water, desalination seems to have higher social acceptance. Sloane (2009) investigated the acceptance of desalination and water reuse at Nourieh Palms. As shown in Table 2, in all areas but particularly drinking water, the approval rate for desalination is higher than water reuse. This reflects that more people trust the quality of water which is generated by desalination.\nSource: Sloane (2009: 128)\nFor most uses, reclaimed water tends to have lower social acceptance than desalination. There are various reasons why people do not trust reclaimed water. First, most people do not understand the difference between treated and untreated water (McKenzie, 2005). Secondly, they are often concerned about the type of wastewater, treatment levels and the availability of information (Qadir, 2009). There are particular concerns with the wastewater produced by the petroleum industry, brought to the surface when drilling oil.\nThis kind of wastewater is difficult to treat due to the high content of oil (Asatekin and Mayes, 2009). Therefore, though reclaimed water undergoes a very thorough treatment process which makes it entirely safe to drink, the public are reluctant to drink treated sewage. However, it is not impossible that people will accept drinking such treated sewage. For example, Singapore has successfully used reclaimed water, a product named NEWater, to supply drinking water (Tortajada, 2006). This reflects that treated wastewater could become widely accepted through public education.\n5.3 Influence on environment\nThere are some environmental disadvantages of desalination. Since Saudi Arabia is rich in oil and gas, clean energy such as solar energy tends to be used less than fossil energy (Al-Sahlawi, 1999). The overuse of fossil energy may cause serious environmental pollution. For instance, oil might generate large quantities of carbon dioxide, which is the main factor leading to global warming (Al-Aza, 2005). Furthermore, the gas emissions from oil could undermine the ozone layer and cause acid rain (ibid.). In addition to environmental pollution caused by fossil energy, brine discharge is another serious problem. After desalination, the brines generally have a higher concentration of salt, nearly twice that of natural seawater (Tsiourtis, 2002). The brines are generally discharged back to the same place where the seawater comes from. This might lead to increased concentration of salt in the sea, which is a potential threat to aquatics.\nIn contrast with the desalination, wastewater reuse is regarded as an eco-friendly way to supply fresh water. Recycling water can maximise the use of rainfall and other current water resources so that the limited underground water resources can be conserved (Miller, 2005). In the meantime, decreased energy consumption could reduce the pollution caused by the use of fossil energy (Ghermandi et al, 2007). Therefore, recycled water is a sustainable and eco-friendly method to supply good quality fresh water.\nFrom the information given above, the following conclusions can be drawn:\n1) Both desalination and wastewater reuse are feasible water supply technologies.\n2) The cost of desalination has decreased dramatically, but is still far more than water reuse. Desalination requires more capital and energy.\n3) Reclaimed water has low public acceptance, especially for drinking water.\n4) Desalination could undermine the environment, while water reuse is eco-friendly.\nConsidering the cost and the impact on the environment, wastewater reuse is recommended to be used as the main water supply technology. Although the public acceptance of recycled water is lower than desalination, the example of Singapore has proven that reclaimed water could be accepted in daily life. Desalination is a costly water supply technology. Furthermore, it needs a vast amount of energy. Even if Saudi Arabia has abundant oil and gas reserves, as these resources are non-renewable, desalination is not suitable for sustainable water supply. Additionally, it has detrimental influences upon the environment. Therefore, Saudi Arabia should improve the ratio of wastewater reuse in the whole fresh water supply system.\nAbderrahman, W. (2000). Urban Water Management in Developing Arid Countries. Water Resources Development 16 (1) pp7-20.\nAbu-Arabi, M. (2007). Status and Prospects for Solar Desalination in the Mena Region. In Rizzuti, L., Ettouney, H., and Cipollina, A. (eds.) Solar Desalination for the 21st Century: A Review of Modern Technologies and Researches on Desalination Coupled to Renewable Energies (pp163-178). Dordrecht: Springer.\nAl-Aza, M. (2005). Oil Pollution and Its Environmental Impact in the Arabian Gulf Region. Boston: Elsevier.\nAlghariani, S. (2003). Water Transfer Versus Desalination in North Africa: Sustainability and Cost Comparison. London: School of Oriental and African Studies.\nAl-Sahlawi, M. (1999). Seawater Desalination in Saudi Arabia: Economic Review and Demand Projections. Desalination (123) pp143-147.\nAsano, T. (2006). Water Reuse: Issues, Technologies and Applications. New York: McGraw Hill.\nAsatekin, A. And Mayes, A. (2009). Oil Industry Wastewater Treatment with Fouling Resistant Membranes Containing Amphiphilic Comb Copolymers. Evrion. Sci. Technol. 43 (12) pp. 4487-4492.\nBashitialshaaer, R., Persson, K., and Larsson, M. (2009). Estimated Future Production of Desalinated Seawater in the MENA Countries and Consequences for the Recipients. Dubai: IDA World Congress.\nBrooks, D. (2007). Fresh Water in the Middle East and North Africa. In Lipchin, C., Pallant, E., Saranga, D. And Amster, A. (eds.) Water Resources Management and Security in the Middle East (pp. 33-64). Dordrecht: Springer.\nCIA (2011). Saudi Arabia. Retrieved 5 April 2011 from https://www.cia.gov/library/publications/the-world-factbook/geos/sa.html\nFryer J. (2010). An Investigation of the Marginal Cost of Seawater Desalination in California. Retrieved 5 April 2011 from http://r4rd.org/wp-content/uploads/2009/07/Cost_of_Seawater_Desalination__Final_3-18-09.pdf\nGhermandi, A., Bixio, D. And Thoeye, C. (2007). The Role of Free Water Constructed Wetlands As Polishing Step in Municipal Wastewater Reclamation and Reuse. Science of the Total Environment. 380 (1-3) pp. 247-258.\nLone Star Chapter of the Sierra Club (2008). Desalination: Is It Worth the Salt?. Retrieved 5 April 2011 from http://texas.sierraclub.org/press/Desalination.pdf\nMcKenzie, C. (2005). Wastewater Reuse Conserves Water and Protects Waterways. On Tap Winter 44 (4) pp46-51.\nMiller, G. (2005). Integrated Concepts in Water Reuse: Managing Global Water\nNeeds. Desalination 187 (1-3) pp. 65-75.\nTsiourtis, N. (2002). Desalination and the Environment. Desalination. 141 (3) pp. 223-236.\nUNESCO (2009). The United Nations World Water Development Report, 3: Water in a Changing World. Paris and London: Earthscan.\nQadir, M., Bahri, A., Sato, T., and Al-Karadsheh, E. (2009). Wastewater Production, Treatment and Irrigation in the Middle East and North Africa. Biomedical and Life Science 24 (1-2) pp37-51.\nSloane, T. (2009). Water Provision: A Comparative Analysis. London: Sage.\nTortajada, C. (2006). Water Management in Singapore. International Journal of Water Resources Development (22) pp. 227-240.\nWilliamson, F. (2010). Water Management: Traditional and Alternative Approaches. International Resource Management. 15(2) pp. 227-231.\nWorld Bank (2011). Saudi Arabia. Retrieved 5 April 2011 from http://data.worldbank.org/country/saudi-arabia\nWorld Energy Council (2010). Issues. Retrieved 5 April 2011 from http://worldenergy.org/Issues\nWright, G. (2009). The Economic Feasibility of Desalination for Water Supply to Arid Regions. Global Water Issues 13 (2) pp202-206.\nUniversity/College: University of California\nType of paper: Thesis/Dissertation Chapter\nDate: 5 January 2017\nWe will write a custom essay sample on Fresh Water\nfor only $16.38 $12.9/page']"	['<urn:uuid:78034183-2d3f-4cb9-bcc0-a9ec9df69ac2>', '<urn:uuid:ac1333bd-4f70-434f-b665-68ccf14752ab>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T22:13:20.996306	10	66	2927
74	How does this new CO2 absorbing material look like?	The material consists of many small crystals and looks like salt. It can be crushed into pellets for use in power plants.	"['In the wake of last week\'s UN global climate agreement, it\'s never been clearer that we\'re on the path to a zero carbon-emission future. Today, scientists announced the creation of a radical new carbon dioxide-absorbing material which may help us cut down our carbon emissions quicker than we expected.\n""Clearly, one of the long-term goals [of our species] is to entirely switch to renewable energy sources like solar and wind power,"" says Phil De Luna, a materials scientist at the University of Ottawa. ""But the reality is that, right now, renewables still form only a small fraction of our global power supply. In emerging counties, where coal power is still the cheapest form of energy by far, and likely will be for a while, we\'re asking ourselves: how can we can heavily mitigate those power plants\' CO2 emissions in the medium-term?""\nDe Luna is part of a research team headed by Shyamapada Nandi—a chemist at the Indian Institute of Science Education and Research in Pune, India—that has just created and tested the fascinating new carbon dioxide-absorbing material, which is outlined today in the journal Science Advances. ""You can think of it like a molecular sponge, one that has a lot of really tiny pores,"" De Luna says. It works by absorbing and releasing carbon dioxide when subjected to various pressures.\nA Material Breakthrough\nThe new material designed by Nandi, De Luna, and their colleagues is a breakthrough in a class of materials called metal organic frameworks, or MOFs. Basically, a MOF is a repeating latticework of metal atoms—in this case, nickel atoms—that has other simple molecules attached at the metal\'s joints. To oversimplify it even further: It\'s a crystal with lots of tiny holes.\nThe idea behind using these materials to soak up CO2 is that (with the right material makeup and under the right conditions) CO2 molecules could conceivably pack themselves tightly into those crevices, forming weak bonds, which could be broken later to exhale the absorbed gas.\nOver the last few decades scientists worldwide have discovered and crafted increasingly interesting MOFs with carbon storage abilities. But there have been major practical problems with each. Some MOFs degrade in the presence of water vapor, a real killer for use in humid environments like coal and natural gas plant flues. Some won\'t absorb much CO2, or require crazy high pressure to do so. Some are so difficult to make or are made from costly materials that are just too expensive for any real-world use. Incredibly, De Luna\'s new material has none of these problems.\n""It\'s really quite simple to synthesize in a one-pot process,"" says De Luna, who notes that the only two ingredients required to make the material (nickel metal and an organic material called pyridylcarboxylate) are cheap. As for the potential degrading effects of water vapor? It doesn\'t effect the new material at all. There\'s simply nowhere on this MOF for water vapor to grab on. ""To sum it up, our material is very stable, easy to synthesize, highly scalable, and has great CO2 capture properties at high pressures. All four of these qualities make it an ideal candidate for industrial use,"" says De Luna.\nThe new material has been specially designed to suck up carbon dioxide gas under the modest to high pressures, the kind you might find in the vents—or flues—of a coal, natural gas, or coal gas power plant. The material works the absolute best at higher pressures (5 to 10 bar), exactly the conditions at which you could scrub CO2 out of coal gas before it is burned.\nCoal gas is a hodgepodge gas mixture created when coal is heated under pressure in the presence of water vapor—and De Luna\'s material can absorb twice as much CO2 from this mix as the best commercially-used approaches. That, and it can release it later while using far, far less energy. Today\'s best carbon absorbers (such as zeolite 13X) require intense heating to release their trapped gasses, but this new material exhales CO2 under a nothing but a modest vacuum.\n""What\'s exciting is that this material could be retrofitted onto existing power plants,"" says De Luna. The material consists of many small crystals, and looks like salt. Crushed up into pellets, they would be deployed into a system that could, in many ways, look and act like your car\'s catalytic converter.\nTiny Pores, Big Absorption\nThe beauty behind the new materials\' CO2 soaking abilities basically boil down to its tiny, tiny pores. ""That seems counter-intuitive, right?"" says De Luna. ""How can something hold more CO2 because it has smaller, not larger, pores?""\nWell, according to De Luna—whose research focused on the 3D chemistry of why the MOF could inhale so much CO2—the new material\'s pores each act like fantastically dense little CO2 parking garages. ""There\'s an incredibly high density of binding sites for CO2 molecules in each pore, and the material also shows a property we call cooperative binding."" That means that when a CO2 molecule nestles into one of the material\'s many pores, it also encourages incoming CO2 molecules to move in with it. Each CO2 molecule reduces the energy cost of its neighbors moving in.\nAs with other, currently existing carbon capture technologies, the CO2 potentially plucked from fossil fuel plants by De Luna\'s new material could be either re-purposed for manufacturing use (for example, carbonating your beer), or squirreled away. Right now, most captured CO2 is hidden away in dry, sealed and hard-to-reach oil wells.\nDe Luna says the next step for this technology will be up to the fuel plants, who will need to find ways to retrofit old power plants or integrate the new material into new ones. ""I\'d honestly say the biggest barrier for this technology is simple industry enthusiasm,"" he says. Because the material works much better than any commercially available option at high (5-10 bar) pressures, like those found at coal gas plants, that\'s likely where it\'ll first be deployed. De Luna says he imagines that other, future incarnations of this technology could be tuned for even better uptake at lower pressure; potentially offering equally stunning properties for regular coal plants, or natural gas burning power plants.\n""Here\'s my takeaway. This material shows extremely high CO2 capture capacity while being stable, cheap, and scalable. It represents a great strategy for medium-term CO2 emissions reductions,"" says De Luna. ""It\'s not the end all of this technology, but it\'s a breakthrough.""']"	['<urn:uuid:a09e79f2-7fd1-4518-b8a1-ffcafe5abd3c>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T22:13:20.996306	9	22	1070
75	What emergency items are required for any boat?	For any boat whatever the size the minimum that should be on board are Lifejackets 1 per person, VHF Radio or a charged mobile phone as a minimum, Flares a small pack of orange smokes, Suncream & Lip Balm plus a supply of Water.	['There are a few essentials that you should not go on the water without, on this page we review what you should take as a minimum.\nFor any boat whatever the size the minimum that should be on board are Lifejackets 1 per person, VHF Radio or a charged mobile phone as a minimum, Flares a small pack of orange smokes, Suncream & Lip Balm plus a supply of Water.\nRNLI Equipment Essentials\nLifejackets, harnesses and secure attachment points for lifelines are essential for any pleasure vessel heading out to sea. Here, David Parker reports on what to look for when choosing and using these vital items of safety equipment.\nThe most important thing with any piece of safety equipment is that you know how to use it. A yacht or motor cruiser should carry sufficient lifejackets for all those on board, but it’s no good waiting until it is dark and conditions are deteriorating before you decide to start wearing them. Trying to don a tangled lifejacket at night and adjust it with umb, cold hands turns what should be a straightforward exercise into a needlessly complicated task.\nBefore leaving on any passage the skipper should ensure that all crew members have well fitting lifejackets and they know how to activate them. When fastened, they should be a tight but comfortable fit. They should be stowed conveniently and it is good practice to always wear them when on deck. They should always be worn at night and in poor conditions.\nThere are two types of lifejacket – air and foam, and air only. The latter is less bulky and more common for adults on leisure vessels. Air only lifejackets are available which will inflate automatically on contact with the water. They either contain a hydrostatic valve operated by water pressure or a capsule which dissolves quickly in water allowing firing of the inflation cylinder.\nIf the jacket is manually inflated, make sure crew members know which side the pull lanyard is located, because they won’t be able to see it once they’re in the water. Lifejackets can also be orally inflated. You should know where this oral inflation tube is, not only if you need to top up the jacket, but also if you need to deflate the lifejacket. Partial deflation may be required for reducing bulk when climbing over the tubes into a liferaft.\nLifejackets should all carry the CE Mark of approval and are designed to support the head and airways clear of the water. They should also turn an unconscious person from being face down in the water to on their back; however the performance of lifejackets in this respect can vary, particularly if heavy foul weather clothing is being worn.\nBuoyancy aids have a rating of 50 newtons and are only suitable for competent swimmers in sheltered waters. But a lifejacket should have a minimum rating of 150 newtons to self-right an unconscious wearer. For extreme conditions, larger 275 newton jackets are available. The newton rating is a guide to the buoyancy of the garment with 10 newtons equalling 1 kg of flotation\nAlways wear crotch straps which should be run between the legs and not around the hips. Without crotch straps, people have been tragically lost because the lifejacket has come off them once they are in the water or when being rescued.\nLifejacket hoods and lifejacket lights are also important. Even when wearing a lifejacket people can drown if they have no protection over their face from the waves. A fluorescent hood also makes a man overboard much easier to identify. A night light or strobe, like a hood, weighs little but can make a huge difference when trying to locate a casualty, particularly in a swell or poor visibility.\nRegularly check lifejacket inflation cylinders, which can corrode when left in lockers in a salt water environment. Periodically inflate the jacket orally to check for leaks. Also check the condition of the stole, straps and buckles and have the lifejacket serviced annually.\nMany lifejackets have a built-in harness, so you have a secure point to attach a safety line to the body. A lifejacket/harness combination reduces bulk and does two jobs in one. Separate harnesses are available and should be CE approved to EN 1095. Harnesses, like lifejackets, should be properly stowed because if there’s an opportunity for webbing to twist and tangle, it will. Harnesses with a back patch attached keep the straps in a much neater position.\nMake sure there’s a harness for everyone and that it is easy to adjust, particularly if you want to put on an extra layer of clothing. (Ideally you should be able to make these adjustments on your own without asking for someone’s assistance; thin webbing can twist so easily you sometimes need an extra pair of hands to help sort it out.)\nThere are two main types of harness design. One has shoulder straps running parallel down the chest to join the waist strap. The other has the straps meeting at a central point to form a Y-shape. Women often find the second type more comfortable. For children look for the type of harness that dons like a waistcoat as often they don’t like having things put over their head.\nThe location of the D-ring is also a consideration; when it is higher up the body at the front it makes for a more comfortable lifting point. As with a lifejacket, good harnesses have toggles so you can keep them attached to your oilskin jacket. They should also have reflective strips sewn on to them. Waterproof jackets are available which have an integral harness.\nSafety lines usually have safety hooks at either end, but some designs have a single end hook and a loop attachment to the body. Lines are available as two hook or three hook combinations; the latter allows the wearer to remain attached to the boat with always one hook clipped on. Elasticated webbing safety lines are designed so that there is less trailing line to get snagged up. Select a safety hook which needs a deliberate action to release it.\nYou don’t need rough conditions to become a MOB and, ironically, some eminent sailors have been lost in calm conditions. A man overboard situation immediately becomes an emergency, not least because the body loses heat 26 times quicker in water than in air. Hooking your safety line on to secure attachment points as you move around the boat should become a habit.\nAttachment points and jackstays\nHarness attachment points need to be placed so you can clip on as you step up from the companionway into the cockpit. These cockpit attachment points are usually fixed to the companionway bulkhead. They should be positioned so that, when hanging on a line, a MOB can’t be dragged into the propellers at the back of the boat. Cockpit safety eyes are available which fold float when not in use giving a flush profile.\nA jackstay is a length of wire, or preferably webbing, which is stretched along the deck so you can clip your lifeline to it. Be aware that guardrails are not built to take loads. Ideally, boat designers should allow jackstays to be recessed into the deck. Jackstays need to be positioned where they can be accessed easily when undertaking common tasks such as sail handling, reefing or working forward.\nOn a yacht, the sheeting arrangements should be looked at so you can move up to the bow without unclipping your lifeline. Before fitting a jackstay, position the anchoring D-bolts and check safety lines can run freely. Remember to fit backing pads under the deck to strengthen the anchor points.']	['<urn:uuid:b0d1f2d9-b1ff-43e2-9771-9b0a8f19b8aa>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T22:13:20.996306	8	44	1279
76	What was the Coast Brigade's relationship with the German Navy?	The Coast Brigade (Grenzbrigade Kuste) at Rostock was under the operational control of the Kriegsmarine (German Navy), while the Grenzkommando maintained administrative control and support. The Brigade's personnel wore navy-style uniforms.	"[""An Illustrated Anatomy Of The Fighters The Inside Story Of by GREEN WILLIAM & SWANBOROUGH GORDON\nBy GREEN WILLIAM & SWANBOROUGH GORDON\nRead or Download An Illustrated Anatomy Of The Fighters The Inside Story Of 100 Classics In The Evolution Of Fighter Aircraft PDF\nSimilar history books\nIt's and index of a variety of books (fiction and non fiction) that have been censored for political purposes throughout the twentieth century. The books are offered in alphabetical order by means of its name, the writer provides a precis of every e-book and a short historical past of the way and why was once it censored.\nHe came upon Rome made from clay and left it made up of marble. As Rome’s first emperor, Augustus reworked the unruly Republic into the best empire the area had ever noticeable. His consolidation and enlargement of Roman strength thousand years in the past laid the principles, for all of Western heritage to stick to.\nThis reference presents greater than 800 alphabetically prepared entries on vital authors, texts, genres, topics, and issues in Greek literature from the Byzantine interval to the current. short, readable entries offer uncomplicated details at the historical past and improvement of contemporary Greek literature and language.\nBetter half quantity to Lechte's best-selling 50 Key modern Thinkers. publications readers in knowing society and tradition within the twenty-first century. Encyclopedic layout covers such subject matters as cybernetics, quantum conception, ideology and aesthetics. For an individual attracted to the human sciences. Hardcover, softcover on hand.\n- Where Are the Great Pyramids?\n- 100.WWII Axis Boobytraps and Sabotage Tactics\n- The Decembrist Myth in Russian Culture\n- Last Tango in Paris (BFI Modern Classics)\n- George Washington's Secret Six:The Spy Ring that Saved the American Revolution\nAdditional info for An Illustrated Anatomy Of The Fighters The Inside Story Of 100 Classics In The Evolution Of Fighter Aircraft\nRubble buttresses had to be built to support some sections. Lookout and guard posts began to appear. Steel hedgehog anti-vehicle obstacles were emplaced on streets. Wooden towers began to appear every 1,000m with searchlights sweeping barbed wire, and broken glass was cemented atop the walls. Log and earth lookout bunkers were positioned between the towers. A car driven at speed could crash through these walls in many places, and some of them crumbled after heavy rain as there were no supporting foundations.\nWest Berliners who aided East Berliners to escape were arrested by the DDR forces when caught. Young men lending identity cards to East Berliners were charged and sentenced to five years' imprisonment for 'organizing the slave trade'. From 23 August 1961 West Berliners were restricted from visiting East Berlin, and the number of authorized crossing points was subsequently reduced to seven. Some East Berliners approved of the Wall, as they deeply resented the professional drain that was taking place.\nGrenzbrigade Kuste) at Rostock near the Baltic Sea, sometimes incorrectly known as 'Frontier Command Coast'. The Coast Brigade was under the operational control of the Kriegsmarine (German Navy) while the Grenzkommando had administrative control and support. Brigade personnel wore navy-style uniforms. Two frontier sectors manned by only a few hundred troops operated crossing stations on the western and southern borders: Grenzabschnitt Yolks Republik Polen 'Hermann Gartmann' at Frankfurt (Oder) and Grenzabschnitt Tschechoslowakei 'Walter Breidfeld' at Pirna.""]"	['<urn:uuid:e74538a4-ccb9-4f2c-90ed-11b78946f091>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T22:13:20.996306	10	31	548
77	nasa women science management weather studies	Women hold significant roles in NASA's weather-related research. Christy Hansen manages airborne science projects at NASA's Goddard Space Flight Center, overseeing aircraft missions that study Earth's ice sheets, atmospheric composition, and hurricanes. This connects to NASA's broader weather research, which discovered how Earth's thunderstorms influence space weather in the ionosphere, affecting radio transmissions and GPS reception.	"['National Board of Directors\nKaren Clark Cole :: Girls Can Do Founder & Executive Director\nCEO, Blink :: Seattle, WA\nKaren is the POSSIBILITY THINKER behind Girls Can Do. She believes that all girls have the potential to make a difference in their lives, the lives around them, and the world if they are given the opportunity, tools, and someone who believes in them. Karen’s goal for Girls Can Do is to plant the seed for big dreams in as many young minds as possible so they can start to see the potential that lives inside them. Karen wants us all to start looking for and telling the stories of the great women right next to us so we create a clear vision for girls to move towards and become. Read more about How this big idea got started in Karen’s blog post.\nKaren is also the CEO and co-founder of Blink UX, a Seattle-based User Experience Research and Design firm. Today Karen is primarily focused on Blink’s culture and employees, communicating the company vision, and setting the company’s growth strategy. Karen also wears the Chief Operations Officer (COO) hat and spends time speaking and writing. Karen began her career in 1986 studying Fine Arts in Vancouver, BC, at the Emily Carr College of Art and Design. She holds a degree in Art History & Visual Arts from the University of Victoria and is a graduate of Information Technology & Multimedia from Capilano College, Vancouver and started a Ph.D. program in Values Driven Leadership in 2015. Karen was born and raised in Victoria, Canada. She loves to ski, garden, run, kiteboard, and lives in Seattle with her daughter, two dogs, and cat. You can read her full bio on Blink’s website.\nGreg Beams :: Retired Audit Partner, EY Seattle :: Seattle, WA\nGreg is an audit partner with EY’s Seattle office and leads their technology and emerging high-growth practice, specializing in Internet, software and venture backed entrepreneurial companies. Greg began his career in Seattle and transferred to EY’s Washington DC practice in the late 1990’s to serve technology companies in Washington DC, Northern Virginia and Maryland before moving back to Seattle in 2005. Greg has an extensive understanding of complex public and private company reporting matters, SEC registration statements and International accounting and operational issues. Greg has experience advising companies in their migration from a private venture-backed company through the IPO process to a public registrant. Greg has served on a number of community and professional boards and is currently the Treasurer and Audit Committee Chair for the Washington Technology Industry Association. He also sits on the Finance Committee at 4Culture (a non-profit arts organization). Greg and his wife Noelle have a 13 year old daughter, Sophia, and the three of them routinely enjoy climbing (and falling) while practicing indoor bouldering. Greg is in his final year of completing a Masters of Fine Arts degree in Photography from the Academy of Art University\nNoelle Beams :: Chief Operations Officer, The Thought & Expression Co. :: Seattle, WA & Brooklyn, NY\nNoelle Beams is an executive with over 20 years of experience in corporate finance and strategy roles, primarily in the telecommunications and media industries. She is credited with providing strategic and financial leadership in start-up, high-growth, and turnaround organizations. Noelle has guided companies from their initial concept through to becoming multi-billion dollar organizations. Her business experience includes strategic transactions (buy side and sell side), capital market transactions from public and private sources totaling over $10 billion, recapitalization transactions, business development, financial planning and reporting (including SEC reporting) for companies in all stages of development, Wall Street and investor relations, and treasury and accounting functions. Noelle began her career in public accounting, and holds a BA degree with an accounting concentration from the University of Washington. University.\nChristy Hansen :: Airborne Science Manager, NASA Goddard Space Flight Center :: Washington, DC\nChristy currently works at NASA’s Goddard Space Flight Center (GSFC) within the Earth Sciences Division where she is responsible for managing the strategic aspects for all airborne science related projects and missions. This encompasses all aircraft related science missions, where sophisticated scientific instruments are installed onto multiple types of aircraft to collect data on Earth Science across the globe, including studies of Earth’s ice sheets and glaciers, atmospheric composition and processes, forestry biomass, carbon cycle, and hurricane studies. Regions of study and deployment locations include Greenland, Antarctica, Chile, USA, Canada, and Africa. Prior to working at GSFC, Christy spent 11 years working at NASA’s Johnson Space Center (JSC) as an Extra-vehicular Activity (EVA) Operations Engineer, EVA Astronaut Instructor, and EVA Task Flight Controller in the Mission Control Center (MCC). She developed spaceflight procedures that astronauts executed on-orbit for the Space Shuttle, Space Station, and Hubble Space Telescope Programs. She worked with the Canadian Space Agency, European Space Agency, Russian Space Agency, and Japanese Space Agency during her time at JSC. Christy has participated in several outreach and mentoring roles, including 3 years for the Scottish Space School, special events, and activities within the US, and was a Girls Can Do speaker. She enjoys hiking, cycling, volleyball, basketball, mountaineering, traveling, and outreach.\nZabrina Jenkins :: Director, Corporate Council, Starbucks :: Seattle, WA\nZabrina oversees commercial, products, discrimination and general liability litigation for retail locations in North America, and provide legal advice and counsel to Starbucks management and partners (employees) relating to potential and actual litigation, primarily in the areas of commercial, real estate and general business law. Serve as an advisor to the Policy Governance Council, and have served on the Diversity Committee for the Law & Corporate Affairs department and as the co-coordinator of the department’s summer intern program.\nDawn Smith :: Acting Director of the Saanich Adult Education Centre for the WSANEC School Board :: Brentwood Bay, BC, Canada\nBefore joining the Adult Education Centre, Dawn was an instructor of Indigenous Studies and the Indigenous Cultural Acumen Training Community Coordinator for the Office of Indigenous Affairs at the University of Victoria. Smith has worked as an Indigenous student advisor in the Faculty of Human and Social Development, and recently re-elected to the band council, she will continue to use her gentle but determined leadership qualities to support the women in her community to find their own voices. Dawn is in the Doctor of Education in Educational Leadership and Policy at the University of British Columbia. Her research area includes confronting colonialism, particularly racism, within education and society in general, through the use of Indigenous pedagogies, decolonizing methodologies and indigenizing strategies. Dawn’s hope is to make a positive contribution to the growing discourse on decolonization while giving back to community.\nRashelle Tanner :: General Counsel Microsoft Corporation :: Seattle, WA\nRashelle Manages legal and compliance for 2,000+ employee international organization doing business in diverse highly regulated industries including: overseas relief & development; long term care; radio broadcasting; pre-K-12 education and corporate resource operations including, board governance and oversight, human resources, facilities and construction management, public relations and marketing communications, accounting and finance, information technology, fundraising and donor development. Rashelle is a trusted adviser to the CEO and executive team; delivering advice and counseling with confidence and accuracy.\nLinda Wagener, Ph.D. :: Chief Culture Officer, Blink :: Seattle, WA\nLinda has over 25 years of experience in consultation, teaching, administration, and research in human development. A Minnesota native, she received her doctorate in psychology at Clark University. Linda is a founding partner of Marigold Associates. She is a consultant in the areas of leadership assessment and development, personal and executive coaching, family dynamics and business solutions. In this capacity she brings a focus on enhanced relational communication and strength based personality dynamics. Linda is committed to exploring the concept of human flourishing across the life span in both research and practice. Her understanding of human experience is informed by her expertise in life span development and organizational systems theory as well as her experience of raising four children. Linda’s work on well-being has been published in various scholarly journals. A licensed psychologist, Linda is a member of the American Psychological Association, the Societies for Research in Child Development and Adolescence, and the Family Firm Institute. Linda is executive producer of the documentary, “Once There Was a War,” which examines the capacity of young people to flourish in Bosnia, Central Asia, and Iraq. One of Linda’s commitments is to provide training and counseling on critical incidents and trauma to international aid workers in the Middle East. Linda enjoys travel, popular culture, movies, books, skiing and hiking.\nThank you for your interest in Girls Can Do and for being a POSSIBILITY THINKER.', 'First Global Connection Between Earth And Space Weather Found\nWeather on Earth has a surprising connection to space weather occurring high in the electrically-charged upper atmosphere, known as the ionosphere, according to new results from NASA satellites.\n""This discovery will help improve forecasts of turbulence in the ionosphere, which can disrupt radio transmissions and the reception of signals from the Global Positioning System,"" said Thomas Immel of the University of California, Berkeley, lead author of a paper on the research published August 11 in Geophysical Research Letters.\nResearchers discovered that tides of air generated by intense thunderstorm activity over South America, Africa and Southeast Asia were altering the structure of the ionosphere.\nImage right: This is a false-color image of ultraviolet light from two plasma bands in the ionosphere that encircle the Earth over the equator. Bright, blue-white areas are where the plasma is densest. Solid white lines outline the continents; Africa is on the left, and North and South America are on the right. Dotted white lines mark regions where rising tides of hot air indirectly create the bright, dense zones in the bands. The picture is a composite built up from 30 days of observations with NASA\'s IMAGE satellite (March 20 to April 20, 2002). Credit: NASA/University of California, Berkeley Print-resolution copy (1 meg tif image)\nThe ionosphere is formed by solar X-rays and ultraviolet light, which break apart atoms and molecules in the upper atmosphere, creating a layer of electrically-charged gas known as plasma. The densest part of the ionosphere forms two bands of plasma close to the equator at a height of almost 250 miles. From March 20 to April 20, 2002, sensors on board NASA\'s Imager for Magnetopause to Aurora Global Exploration (IMAGE) satellite recorded these bands, which glow in ultraviolet light.\nUsing pictures from IMAGE, the team discovered four pairs of bright regions where the ionosphere was almost twice as dense as the average. Three of the bright pairs were located over tropical rainforests with lots of thunderstorm activity -- the Amazon Basin in South America, the Congo Basin in Africa, and Indonesia. A fourth pair appeared over the Pacific Ocean. Researchers confirmed that the thunderstorms over the three tropical rainforest regions produce tides of air in our atmosphere using a computer simulation developed by the National Center for Atmospheric Research, Boulder, Colo., called the Global Scale Wave Model.\nThe connection to plasma bands in the ionosphere surprised scientists at first because these tides from the thunderstorms can not affect the ionosphere directly. The gas in the ionosphere is simply too thin. Earth\'s gravity keeps most of the atmosphere close to the surface. Thunderstorms develop in the lower atmosphere, or troposphere, which extends almost 10 miles above the equator. The gas in the plasma bands is about 10 billion times less dense than in the troposphere. The tide needs to collide with atoms in the atmosphere above to propagate, but the ionosphere where the plasma bands form is so thin, atoms rarely collide there.\nHowever, the researchers discovered the tides could affect the plasma bands indirectly by modifying a layer of the atmosphere below the bands that shapes them. Below the plasma bands, a layer of the ionosphere called the E-layer becomes partially electrified during the day. This region creates the plasma bands above it when high-altitude winds blow plasma in the E-layer across the Earth\'s magnetic field. Since plasma is electrically charged, its motion across the Earth\'s magnetic field acts like a generator, creating an electric field. This electric field shapes the plasma above into the two bands. Anything that would change the motion of the E-layer plasma would also change the electric fields they generate, which would then reshape the plasma bands above.\nThe Global Scale Wave Model indicated the tides should dump their energy about 62 to 75 miles above the Earth in the E-layer. This disrupts the plasma currents there, which alters the electric fields and creates dense, bright zones in the plasma bands above.\n""The single pair of bright zones over the Pacific Ocean that is not associated with strong thunderstorm activity shows the disruption is propagating around the Earth, making this the first global effect on space weather from surface weather that\'s been identified,"" said Immel. ""We now know that accurate predictions of ionospheric disturbances have to incorporate this effect from tropical weather.""\n""This discovery has immediate implications for space weather, identifying four sectors on the Earth where space storms may produce greater ionospheric disturbances. North America is in one of these sectors, which may help explain why the U.S. suffers uniquely extreme ionospheric conditions during space weather events,"" Immel said.\nMeasurements made by NASA\'s Thermosphere Ionosphere Mesosphere Energetics and Dynamics (TIMED) satellite from March 20 to April 20, 2002, have confirmed that the dense zones exist in the plasma bands. Researchers now want to understand whether the effect changes with seasons or large events, like hurricanes.\nThe research was funded by NASA. The National Center for Atmospheric Research is sponsored by the National Science Foundation, Arlington, Va.\nThe team includes Immel, Scott England, Stephen Mende, and Harald Frey of the University of California, Berkeley; Eiichi Sagawa of the National Institute of Information and Communications Technology, Tokyo, Japan; Sid Henderson and Charles Swenson of Utah State University, Logan, Utah; Maura Hagan of the National Center for Atmospheric Research High Altitude Observatory, Boulder, Colo.; and Larry Paxton of the Johns Hopkins University Applied Physics Laboratory, Laurel, Md.\nNASA Goddard Space Flight Center']"	['<urn:uuid:e9ec2866-1d7d-4fec-a4f3-08e85fc8a45d>', '<urn:uuid:d9dc3d50-100b-47e5-9a74-8482cb4e6001>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T22:13:20.996306	6	56	2352
78	mechanical automatic pad drilling rig vs conventional pipeline pig purpose compare	A mechanical automatic pad drilling rig is designed to move as an interconnected convoy across matting with rollers from one wellhead to another on the same pad for efficient drilling of multiple directional wells. In contrast, a conventional pipeline pig is designed to be propelled through pipelines under fluid pressure to clean internal surfaces by scraping and removing debris, or to apply coatings to pipe walls through specialized blade edges.	"['|Número de publicación||US6962030 B2|\n|Tipo de publicación||Concesión|\n|Número de solicitud||US 10/265,026|\n|Fecha de publicación||8 Nov 2005|\n|Fecha de presentación||4 Oct 2002|\n|Fecha de prioridad||4 Oct 2001|\n|También publicado como||US20030066686|\n|Número de publicación||10265026, 265026, US 6962030 B2, US 6962030B2, US-B2-6962030, US6962030 B2, US6962030B2|\n|Cesionario original||Pd International Services, Inc.|\n|Exportar cita||BiBTeX, EndNote, RefMan|\n|Citas de patentes (17), Otras citas (2), Citada por (28), Clasificaciones (15), Eventos legales (4)|\n|Enlaces externos: USPTO, Cesión de USPTO, Espacenet|\nThis application claims the benefit of U.S. provisional application No. 60/327,077 filed Oct. 4, 2001.\nDescription of the Related Art\nIn certain land drilling locations a number of wells are drilled from the same surface location, referred to as a “pad.” The wells are drilled directionally to access oil reservoirs that extend horizontally from the pad. In order to optimize the time used to move the rig to the next well on a pad, it is necessary to skid the drilling rig from one well on the pad to another well location on the same pad. The distance between the wells on the same pad varies but can, for example, be in the order of about ten meters.\nThe land drilling rig used for drilling wells on the pad is comprised of a number of support modules containing such equipment as engines, mud pumps, accumulator, etc. The associated support modules or machinery are typically mounted on steel skids.\nIn the past, the movement of the drilling rig was facilitated by splitting the module into two different parts. One part (consisting of the substructure, mast, doghouse, transfer mud tank, and catwalk) was the mobile unit that was dragged from well to well. The second part (consisting of mud pumps, generators, electrical, central mud system) was spotted permanently on the pad location. These two parts are linked with the use of suitcases (an umbilical-cord like connection comprised mainly of cables and hoses). Large lease locations, difficulties in transferring gasified mud, the need to construct a pad-specific rig, and unpolished move times suggested the need for improvements.\nA conventional land drilling rig is slightly modified to adapt for efficient pad drilling. The conventional land drilling rig is interconnected to form a somewhat rigid convoy. The interconnected drilling rig is placed on top of matting. The matting has rollers to facilitate movement of the drilling rig as a “convoy” across the matting from one wellhead to another wellhead in pad-type work. The tanks in the mud system may also be interconnected and placed on top of a matting having rollers for moving the mud tank system as a separate convoy but in tandem with the first convoy.\nFor comparison, a prior art land drilling rig 10 for pad work is shown in FIG. 3A. The prior art drilling rig 10 with wellheads 14 a, 14 b, etc. has a mobile complex 80 which is separate from a stationary complex 82. The mobile complex moves along the well line 16. The stationary complex 82 is set to the side of the drilling location and includes several oilfield buildings. Suitcases 84 a, 84 b (typically, more than two are utilized) carry electrical cables and mud hoses (both not shown) from the stationary complex 82 to the varying location of the mobile complex 80.\nReferring back to\nIn other land drilling rig arrangements, several other types of oilfield buildings (generally designated by reference number 28) may be incorporated including additional pump houses, water tanks, tool houses, boilers, fuel tanks, storage buildings, change house, accumulator and generators. The buildings 28 may be any of at least some of the preceding types of oilfield buildings 28 and only one representative oilfield building 28 arrangement is shown in the drawings.\nTwo reinforced arms 40 a, b are attached on one end to the rear of the substructure 26 and the other end to the front of the oilfield buildings 28 a and b. The arms 40 a and b may be made of steel and are pinned in place. Connectors such as short connector plates 42 a and 42 b may be made between the skid 36 pick up rolls of longitudinally adjacent buildings 28. For example, the rear of building 28 a may be attached to the front of building 28 c by connector plates 42 a. Connector plates 42 a and b may be made of steel and may be attached by pinning or welding.\nLateral connectors 48 extend to auxiliary buildings, for example to the side of building 28 e. The lateral connectors 48 may be steel bars or plates attached by pinning or welding between adjacent buildings.\nLateral connections made between buildings 28 a and 28 b and between the buildings 28 c and 28 d must be such that there is no interference with the existing line of wellheads 14 that pass between the buildings 28. A connection would be made between such buildings at a level above the top of the wellheads 14.\nThe mud system 27, with mud tanks 28 h & 28 g, may be made into a separate interconnected system or second convoy 72 for purposes of independently moving the mud system 27. A hydraulic arm 44 is attached to the front of mud tank 28 g. This hydraulic arm would then pull the mud tank convoy 72 by inserting the front of the hydraulic arm 44 into the available roller-pockets 56 in the matting 30. Another short connector 46 similar to short connectors to 42 a and b may be made between buildings 28 g and 28 h.\nReferring more specifically to\nThe rollers 58 (fourteen in number in the embodiment shown\nThe rollers 58 support the weight of the entire drilling rig 10 (i.e. the entire convoy 70 or 72) and eliminate most of the shear friction force created when the substructure 26 and buildings 28 are moved across the mat 30. This movement is in the nature of a rolling motion across the matting 30 as opposed to a skidding motion. Moreover, due to the interconnections between the substructure 26 and the buildings 28, the entire first convoy 70 can be moved in unison from one wellhead 14 a to the next wellhead 14 b as seen when comparing\nDue to the large weight of the mud system 27 relative to the rest of the drilling module and their unbalanced position relative to the line of wellheads 14, it is preferable to move the mud system 27 separate from the substructure 26 and trailing buildings 28 a through e. Movement of the mud tank 28 g. h, i.e. the second convoy 72, mimics the movement of the first convoy 70 through the hydraulic arm 44 which is mounted on the front end of mud tank 28 g. It was discovered that “driller-to-off-driller” side misalignment was minimized by separating the movement between the two convoys 70 and 72.\n|Patente citada||Fecha de presentación||Fecha de publicación||Solicitante||Título|\n|US1103505 *||23 Dic 1912||14 Jul 1914||Herbert M Grant||Roller-platform for scales.|\n|US2420803 *||29 Abr 1946||20 May 1947||William B Tobin||Movable combination derrick floor and engine base|\n|US4037662 *||30 Ago 1976||26 Jul 1977||Leslie Bowling||Automated bailing apparatus in flexible combination for bailing shallow wells|\n|US4103503||21 Dic 1976||1 Ago 1978||Dixilyn International, Inc.||Drilling substructure transfer system|\n|US4305237 *||13 Jun 1980||15 Dic 1981||Pre Corporation||Compact sectionalized drilling mast, power arrangement and support means therefor|\n|US4324077 *||26 Oct 1979||13 Abr 1982||Lee C. Moore Corporation||Method of moving a drilling rig long and short distances|\n|US4420916 *||20 Nov 1981||20 Dic 1983||Muskogee Environmental Conservation Co.||Method and apparatus for aligning and securing auxiliary equipment with respect to a well drilling platform|\n|US4616454||19 Sep 1985||14 Oct 1986||Suncor Inc.||Slant service rig|\n|US4757592 *||8 Sep 1986||19 Jul 1988||Parco Mast And Substructures, Inc.||Method of erecting a portable drilling rig|\n|US4821816 *||10 May 1988||18 Abr 1989||W-N Apache Corporation||Method of assembling a modular drilling machine|\n|US4823870 *||1 Ago 1988||25 Abr 1989||Sorokan Ronald S||Cantilever drilling structure|\n|US4899832 *||10 Dic 1987||13 Feb 1990||Bierscheid Jr Robert C||Modular well drilling apparatus and methods|\n|US5109934 *||13 Feb 1991||5 May 1992||Nabors Industries, Inc.||Mobile drilling rig for closely spaced well centers|\n|US5390775 *||21 Oct 1993||21 Feb 1995||Aar Corp.||Modular roller mat and roller assembly|\n|US5492436||14 Abr 1994||20 Feb 1996||Pool Company||Apparatus and method for moving rig structures|\n|US5533604 *||6 Oct 1995||9 Jul 1996||Brierton; Dennis M.||Ball transfer cube|\n|US6161358||27 Jul 1999||19 Dic 2000||Mochizuki; David A.||Modular mobile drilling system and method of use|\n|1||Anonymous, Nerac.com, website printout, Jun. 22, 2000, p. 1, U.S.A.|\n|2||No Author, Unique Rig Fulfills Unusual Mobility Requirements, World Oil, Oct., 1989, pp. 60-61, vol. 209, No. 4, U.S.|\n|Patente citante||Fecha de presentación||Fecha de publicación||Solicitante||Título|\n|US7306055 *||1 Mar 2005||11 Dic 2007||Barnes R Michael||Automatic method for installing mobile drilling rig at a drilling site|\n|US7308953 *||1 Mar 2005||18 Dic 2007||Barnes R Michael||Mobile drilling rig|\n|US7921758||12 Abr 2011||Schlumberger Technology Corporation||Impact panels|\n|US8047303||1 Nov 2011||National Oilwell Varco L.P.||Drilling rig drawworks installation|\n|US8250816||28 Ago 2012||National Oilwell Varco L.P.||Drilling rig structure installation and methods|\n|US8291974||23 Oct 2012||Vitruvian Exploration, Llc||Method and system for accessing subterranean deposits from the surface and tools therefor|\n|US8297350||31 Oct 2007||30 Oct 2012||Vitruvian Exploration, Llc||Method and system for accessing subterranean deposits from the surface|\n|US8316966||27 Nov 2012||Vitruvian Exploration, Llc||Method and system for accessing subterranean deposits from the surface and tools therefor|\n|US8371399||12 Feb 2013||Vitruvian Exploration, Llc||Method and system for accessing subterranean deposits from the surface and tools therefor|\n|US8376039||19 Feb 2013||Vitruvian Exploration, Llc||Method and system for accessing subterranean deposits from the surface and tools therefor|\n|US8434568||7 May 2013||Vitruvian Exploration, Llc||Method and system for circulating fluid in a well system|\n|US8464784||18 Jun 2013||Vitruvian Exploration, Llc||Method and system for accessing subterranean deposits from the surface and tools therefor|\n|US8468753||29 Feb 2008||25 Jun 2013||National Oilwell Varco L.P.||Drilling rigs and erection methods|\n|US8469119||31 Oct 2007||25 Jun 2013||Vitruvian Exploration, Llc||Method and system for accessing subterranean deposits from the surface and tools therefor|\n|US8479812||31 Oct 2007||9 Jul 2013||Vitruvian Exploration, Llc||Method and system for accessing subterranean deposits from the surface and tools therefor|\n|US8505620||31 Oct 2007||13 Ago 2013||Vitruvian Exploration, Llc||Method and system for accessing subterranean deposits from the surface and tools therefor|\n|US8511372||31 Oct 2007||20 Ago 2013||Vitruvian Exploration, Llc||Method and system for accessing subterranean deposits from the surface|\n|US8549815||29 Feb 2008||8 Oct 2013||National Oilwell Varco L.P.||Drilling rig masts and methods of assembly and erecting masts|\n|US8813436||31 Ago 2011||26 Ago 2014||National Oilwell Varco, L.P.||Pinned structural connection using a pin and plug arrangement|\n|US8813840||12 Ago 2013||26 Ago 2014||Efective Exploration, LLC||Method and system for accessing subterranean deposits from the surface and tools therefor|\n|US8875911 *||20 Abr 2010||4 Nov 2014||National Oilwell Varco, L.P.||Drilling rig mast lift systems and methods|\n|US9091125||9 Ene 2013||28 Jul 2015||National Oilwell Varco, L.P.||Collapsible substructure for a mobile drilling rig|\n|US20050193645 *||1 Mar 2005||8 Sep 2005||Barnes R. M.||Automatic method for installing mobile drilling rig at a drilling site|\n|US20090218137 *||29 Feb 2008||3 Sep 2009||Robert Benjamin Donnally||Drilling rig drawworks installation|\n|US20090218138 *||29 Feb 2008||3 Sep 2009||Robert Benjamin Donnally||Drilling rig structure installation and methods|\n|US20090218139 *||29 Feb 2008||3 Sep 2009||Robert Benjamin Donnally||Drilling rigs and erection methods|\n|US20090218144 *||29 Feb 2008||3 Sep 2009||Robert Benjamin Donnally||Drilling rig masts and methods of assembly and erecting masts|\n|US20120047820 *||20 Abr 2010||1 Mar 2012||Donnally Robert B||Drilling rig mast lift systems and methods|\n|Clasificación de EE.UU.||52/741.1, 175/162, 175/57, 52/143, 166/75.11, 193/37|\n|Clasificación internacional||E04H12/18, E02B17/00, E04H5/02, E21B7/02, E21B15/00|\n|Clasificación cooperativa||E21B15/00, E21B15/003|\n|Clasificación europea||E21B15/00, E21B15/00F|\n|24 Ago 2005||AS||Assignment|\nOwner name: PD INTERNATIONAL SERVICES, INC., CANADA\nFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:PRECISION DRILLING CORPORATION;REEL/FRAME:016445/0058\nEffective date: 20050822\n|8 Abr 2009||FPAY||Fee payment|\nYear of fee payment: 4\n|7 Mar 2013||FPAY||Fee payment|\nYear of fee payment: 8\n|7 Ene 2014||AS||Assignment|\nEffective date: 20131201\nOwner name: WEATHERFORD CANADA PARTNERSHIP, CANADA\nFree format text: AMALGAMATION-DISSOLUTION-GENERAL CONVEYANCE AND ASSUMPTION AGREEMENT;ASSIGNOR:PD INTERNATIONAL SERVICES INC.;REEL/FRAME:031925/0268', '|Publication number||US6792641 B1|\n|Application number||US 09/600,902|\n|Publication date||21 Sep 2004|\n|Filing date||24 Nov 1999|\n|Priority date||24 Nov 1998|\n|Also published as||WO2000030773A1|\n|Publication number||09600902, 600902, PCT/1999/3907, PCT/GB/1999/003907, PCT/GB/1999/03907, PCT/GB/99/003907, PCT/GB/99/03907, PCT/GB1999/003907, PCT/GB1999/03907, PCT/GB1999003907, PCT/GB199903907, PCT/GB99/003907, PCT/GB99/03907, PCT/GB99003907, PCT/GB9903907, US 6792641 B1, US 6792641B1, US-B1-6792641, US6792641 B1, US6792641B1|\n|Original Assignee||Hamdeen Incorporated Limited|\n|Export Citation||BiBTeX, EndNote, RefMan|\n|Patent Citations (15), Non-Patent Citations (1), Referenced by (26), Classifications (8), Legal Events (6)|\n|External Links: USPTO, USPTO Assignment, Espacenet|\n1. Field of the Invention\nThis invention generally relates to mechanical pigging devices, commonly referred to simply as “pigs”, for use in pipelines or other tubulars. Such pigs are known to find utility in a diverse range of applications, including for cleaning purposes and for conveying equipment in the case, for example, of pipeline monitoring. In the present invention there is described a unique pig suitable for use in the cleaning of internal surfaces of a pipeline or tubular and, in a variation thereof, suitable for applying coatings or other fluids to the aforesaid surfaces.\n2. Background of Related Art\nIt is known in the art to cause cleaning pigs to be propelled through a pipe or tubing under the influence of a pressurised fluid. Pigs, designed for this purpose, typically have a flexible cylindrical body made, for example, from a polyurethane foam. Other materials have also commonly been used, including rubber, metal, plastics and combinations and composites. The rear and front end walls of the cylindrical body may be covered with an impervious coating designed to form a moving seal with the inner wall of the pipe. With this design, the pig essentially acts as a piston as it is conveyed through the pipeline or tubular; the fluid on its rear side having a higher pressure head than the fluid at its front side.\nNotably, pigs propelled through pipelines or the like in the manner described above are intended to prevent propelling fluid from flowing through or around the pig. Implicated by this, pigs used for cleaning purposes have, in the past, been intended to physically push and forceout debris in advance of the pig as it travels through the pipe bore. At best, the pig itself provides an additional wiping function on the pipe walls.\nIn the present invention it is recognised that, on occasion, it would be advantageous to provide a more rigorous cleaning process to a pipeline bore than merely pushing loose debris and wiping the surfaces. In the art, those who have addressed this problem have contemplated the provision of scratching elements, such as wire bristles, on the circumferential walls of the pig. While related designs provide for a more aggressive cleaning process, such pigs usually do not allow for sufficient fluid flow past the bristles to allow for the bristles themselves to be cleaned. In use, debris, shavings, slivers and the like can become lodged between the bristles, serving to reduce the efficiency of the pig\'s travel and the cleaning process.\nAn object of the present invention is to obviate or at least mitigate these and other disadvantages associated with pipeline or tubular cleaning pigs. In one aspect, the invention seeks to achieve this by creating an alternative means for the propulsion of the pig through the pipeline or other tubular.\nA further object of the invention herein is to employ the novel propulsion features disclosed herein in relation to pigs for use in respect of other functions, including pigs intended to act as mechanical applicators,\nThe invention will be described in detail with reference to the following drawings in which like reference numerals refer to like elements wherein:\nFIG. 1 shows, in perspective view, a pig intended for the cleaning of the internal surfaces of coiled tubing as the pig is propelled along the tubular by a propulsion fluid; and\nFIG. 2 shows, in perspective view, a pig intended to provide stabilization and cleaning of the internal surfaces of tubing as the pig is conveyed mechanically along a tubular.\nAccording to a first aspect of the present Invention there is provided a pig for use in a tubular bore, wherein the pig is provided with one or more blades having a profile that encourages both forward movement and rotation about a longitudinal axis of the pig when acted upon by a propulsion fluid travelling through the tubular.\nIt is envisaged that the pig may comprise of a cylindrical elongate body having an outer diameter less than the internal diameter of the tubular, wherein the body supports a plurality of blades.\nPreferably, the pig is a cleaning pig and the peripheral edges of the blades are adapted to perform a cleaning function as the pig rotates and travels through the tubular. More particularly, the blades of the pig are designed to scrape the internal surfaces of the tubular bore upon the rotation and passage of the pig. An advantage may be obtained in the option of providing the edges in an abrasive material. Similarly, the edges of the blades may be provided in a material that is relatively hard and therefore resistant to wear.\nAlternatively, however, the peripheral edges may be adapted to perform alternative functions. For example, the pig may be provided as a means for the mechanical application of a coating or fluid to a tubular bore and, optionally, the blades or at least the edges thereof may support a suitable applicator material having a high capacity for carrying by absorption or other means the coating or fluid to be applied.\nAccording to a second aspect of the present invention there is provided a pig for use in a tubular, wherein the pig is comprised of a stabilizer body wherein the stabilizer body supports a plurality of blades and is conveyed mechanically through a tubular.\nPreferably the pig is a cleaning pig wherein the blades of the pig are designed to scrape the internal surfaces of the tubular bore whilst the stabiliser body provides centralisation.\nPreferably the blades may be adapted so that they do not exhibit any abrasive qualities thereby reducing the risk of damage if the pig is to be used in tubing which is plastic coated.\nPreferably the blade properties can be pre selected to be adapted to flex through a profiled restriction in the tubular bore thereby providing a means of confirming the position of a pig within the tubulars.\nAccording to a third aspect of the present invention there is provided a pig for use in a tubular, wherein the pig is adapted to rotate in its longitudinal axis under the influence of a propulsion fluid as it is displaced through the tubular.\nThe pig may be further adapted to rotate in orbit within the tubular bore.\nAccording to a fourth aspect of the present invention there is provided a pig for use in a tubular, the pig comprising reaction surfaces adapted for forward propulsion of the pig under the influence of a positive pressure applied by propulsion fluid travelling through the tubular, characterised in that the reaction surfaces are spaced and orientated so as to provide for a net positive velocity of the propulsion fluid relative to the pig in the direction of travel through the tubular.\nThe reaction surfaces may be provided on a plurality of respective blades, such as turbine blades. Preferably, the blades define a fluid by-pass path, the blades being separated by void areas which permit the relative flow of fluid through the pig in a forward direction.\nPreferably the reaction surfaces also encourage the rotation of the pig around its longitudinal axis when acted upon by the propulsion fluid.\nAccording to a fifth aspect of the present invention there is provided a pig for use in a tubular, wherein the pig is provided with one or more blades having a profile that precludes rotation of the pig while travelling through the tubular.\nThe blades are typically of varying diameter, the largest blade or blades potentially, having a diameter greater than the internal diameter of the tubular,\nPreferably the largest blade or blades are sufficiently flexible to allow entry and passage of the pig through the tubular yet sufficiently robust to carry out and withstand the rigours of the cleaning process.\nIt should be understood that references to tubulars herein, unless the context otherwise dictates, should be construed in the broadest possible sense, and interpreted to encompass any form of tubing, pipe or pipeline.\nIn order to provide a better understanding of the invention, example embodiments of a pig incorporating the invention will now be described with reference to the accompanying Figures.;\nReferring firstly to FIG. 1, a pig, generally depicted at 1, comprises substantially of a body 2 and a plurality of turbine blades 3. The body 2 is generally elongate and cylindrical. The pig body 2 is suitably made of a robust material in view of its need to withstand substantial impact loads, while also functioning in an aggressive cleaning manner.\nThe blades 3 are afforded a turbine or impeller like profile, having reaction surfaces 4 that react to the influence of a propulsion fluid pumped through the coiled tubing in which the pig 1 is intended to travel. Typical of turbine blades, the blades 3 can be provided on the body 2 such that the reaction surfaces 4 are presented at an acute angle to the linear direction of the fluid flow, thereby imparting a reaction torque to the body 2 in addition to a reaction force in the axial direction. In consequence, the pig 1, when acted upon by a propulsion fluid, is caused to travel through the coiled tubing in a generally axial direction, but to also rotate about its longitudinal axis while so doing.\nAdditionally, the pig 1 moves in a third dynamic path. The outside diameter of the pig 1 can be sized to have a degree of clearance within the internal bore of the tubular. That is to say, the maximum outside diameter of the pig is less than the internal diameter of the tubular, allowing for radial displacement of the pig 1 during its travel through the tubing. In fact, it is recognised in the present invention, that such dimensioning of the pig 1 relative to the tubing causes the longitudinal axis of the pig 1 to orbit or rotate about the substantially parallel longitudinal axis of the tubing.\nThis third dynamic path is associated with a number of advantages For instance, where it is intended that the peripheral edges 5 of the blades 3 contact the internal surfaces of the tubing, the radial displacement of the pig 1 as it orbits around the longitudinal axis of the tubing allows for such contact over a range of tubing diameters. This means that it is not essential that a respective pig, incorporating the invention hereto, need be provided to correspond to each size of coiled tubing or other tubular.\nIn an alternative application a pipeline can be cleaned using a combination of two pigs. The first pig, having an outer diameter less than the internal diameter of the tubular, passes through the pipeline removing major restrictions. The second pig removes additional debris and in effect polishes the internal surface of the pipeline.\nA further advantage, which applies also to the other rotational movement of the pig about its own axis, is that the relative velocity of the blade edges 5 is considerably higher relative to the tubing surfaces than that of a pig merely designed for linear movement. This is particularly advantageous where the pig is intended for use as a mechanical cleaning device.\nThe cleaning pig 1 is designed to clean the internal bore of coiled tubing. More particularly, the pig 1 is adapted to apply a rotational cleaning action suitable for removing scales and other deposits located on the internal surfaces of the tubing.\nIn one use, the pig may be used to prepare the surface of a tubular, by removing scale or rust not removed by other cleaning methods, prior to the application of a coating fluid or material.\nSecondary cleaning of the coiled tubing is achieved by the effects of disturbances in the flow of the propulsion fluid through the voids between the turbine blades 3.\nThe interaction of the turbine blades with the propulsion renders the propulsion fluid flow of a turbulent nature. It will be appreciated by those skilled in the art that this enhances the cleaning efficiency of the device.\nFurthermore, the presence of voids between the blades 3 results in the propulsion fluid having a positive velocity relative to the pig. As a consequence of this positive velocity the propulsion fluid also removes the debris created by the cleaning of the coiled tubing. The removal of this debris has the advantage of preventing the build up of potential blockages in the coiled tubing. Moreover, debris is also cleaned from the blades of the pig itself:\nIn an alternative embodiment the reaction surfaces 4 and most particularly peripheral edges 5 of the blades 3 are provided with a material suitable for the application of a coating of other fluid material.\nWith reference to FIG. 2, an alternative embodiment of the present invention generally depicted at 7 comprises a stabiliser body 8 which has a plurality of blades 9 mounted in a similar configuration to the embodiment shown in FIG. 1. The stabiliser body 8 has coupling means 10 which allow attachment to mechanical driving means (not shown) so that the stabiliser body 8 is propelled through a tubular. Where the tubular is casing or liner in a well-bore, the mechanical driving means may be a pipe string, for example. Furthermore the blades 9 are mounted on the stabiliser body 8 in a watermelon shaped configuration which assists entry into and retrieval out of profiled restrictions.\nIn this manner the embodiment shown in FIG. 2 allows simultaneous centralisation and cleaning for coiled tubing.\nFurther modifications and improvements may be incorporated without departing from the scope of the invention herein intended.\n|Cited Patent||Filing date||Publication date||Applicant||Title|\n|US3480984 *||17 Jun 1968||2 Dec 1969||Kidd Joseph V||Pig apparatus|\n|US3619844 *||3 Aug 1970||16 Nov 1971||Oil States Rubber Co||Disc type pipeline scraper and batch separator|\n|US3939519 *||16 Jan 1974||24 Feb 1976||Muirhead Walter B||Condenser tube cleaning plug|\n|US4081875||10 Nov 1976||4 Apr 1978||Eizo Nishino||Scale removal device|\n|US4122575||9 May 1977||31 Oct 1978||Nihon Pipeline Service Kabushiki Kiahsa||Tube cleaning material|\n|US4603449 *||3 May 1985||5 Aug 1986||Knapp Kenneth M||Unitized pig body for parafin removal|\n|US5068142 *||12 Apr 1990||26 Nov 1991||Teijin Limited||Fiber-reinforced polymeric resin composite material and process for producing same|\n|US5244505 *||24 Feb 1992||14 Sep 1993||Pipe Rehab International, Inc.||Method for cleaning pipe|\n|US5379475||5 Aug 1993||10 Jan 1995||Sivacoe; Orlande||Scraper for a pipe pig|\n|US5600863||29 Dec 1995||11 Feb 1997||Curran; Ed.||Pipe scraper assembly|\n|US5657820||14 Dec 1995||19 Aug 1997||Smith International, Inc.||Two trip window cutting system|\n|US5797993 *||16 Jul 1997||25 Aug 1998||Chevron U.S.A. Inc.||Expandable pipeline pig assembly|\n|US5819353||30 Aug 1996||13 Oct 1998||Oiltools International B.V.||Tool for cleaning or conditioning tubular structures such as well casings|\n|US5964004 *||24 Sep 1997||12 Oct 1999||Bean; Douglas Colin||Device for cleaning medical endoscopic tubes|\n|DE2944709A1||6 Nov 1979||14 May 1981||Ruhrkohle Ag||Rohrreinigungsgeraet|\n|1||Excerpt from the website http://www.dupont.com/kevlar/whatiskevlar.html.|\n|Citing Patent||Filing date||Publication date||Applicant||Title|\n|US7500834||27 Feb 2004||10 Mar 2009||Robert Bonthron Durward||Method and apparatus for enhancing fluid velocities in pipelines|\n|US7588644||30 Aug 2007||15 Sep 2009||Integris Rentals, L.L.C.||Method and apparatus for cleaning pipeline pigs|\n|US7644463||2 Sep 2008||12 Jan 2010||James Robert Crawford||Spear method for retrievable pig|\n|US7874435||25 Jan 2011||Integris Rentals, L.L.C.||Pipeline pig storage rack apparatus|\n|US8001858||23 Aug 2011||Cogen William||Pipeline inspection apparatus and method using radio frequency identification and inertial navigation|\n|US8215499||24 Jan 2011||10 Jul 2012||Integris Rentals, L.L.C.||Pipeline pig storage rack apparatus|\n|US8387805||10 Jul 2012||5 Mar 2013||Integris Rentals, L.L.C.||Pipeline pig storage rack apparatus|\n|US8650695||16 Jan 2009||18 Feb 2014||Tdw Delaware Inc.||Pipeline cleaning pig with self-energizing diagonally oriented scrapers|\n|US8668642||1 Sep 2011||11 Mar 2014||Covidien Lp||Port device including retractable endoscope cleaner|\n|US8689653||26 Jul 2011||8 Apr 2014||William COGEN||Pipeline inspection apparatus and method using radio frequency identification and inertial navigation|\n|US8894772 *||19 Apr 2013||25 Nov 2014||Cokebuster Ltd.||Relateing to pipeline pigs|\n|US8926507 *||20 Feb 2014||6 Jan 2015||Covidien Lp||Port device including retractable endoscope cleaner|\n|US9009905 *||31 Jul 2009||21 Apr 2015||On Stream Technologies Inc.||Pipe pig and method of cleaning a pipe|\n|US9113947||12 Dec 2014||25 Aug 2015||Covidien Lp||Port device including retractable endoscope cleaner|\n|US9113948||17 Dec 2014||25 Aug 2015||Covidien Lp||Port device including retractable endoscope cleaner|\n|US9375765||9 Oct 2015||28 Jun 2016||Crossford International, Llc||Tube scraper projectile|\n|US20040247451 *||27 Feb 2004||9 Dec 2004||Durward Robert Bonthron||Method and apparatus for enhancing fluid velocities in pipelines|\n|US20070045205 *||22 Aug 2006||1 Mar 2007||Olivier Pierre L||""Pipeline Pig Storage Rack Apparatus""|\n|US20080173109 *||22 Jan 2008||24 Jul 2008||Cogen William||Pipeline inspection apparatus and method using radio frequency identification and inertial navigation|\n|US20100180391 *||22 Jul 2010||Tdw Delaware, Inc.||Pipeline Cleaning Pig with Self-Energizing Diagonally Oriented Scrapers|\n|US20100205757 *||18 Feb 2009||19 Aug 2010||Munden Bruce A||Bypass pig|\n|US20100205822 *||19 Aug 2010||Munden Bruce A||Method of drying a tubular string to prevent bedwrap corrosion|\n|US20110203676 *||31 Jul 2009||25 Aug 2011||On Stream Technologies Inc.||Pipe pig and method of cleaning a pipe|\n|US20110206491 *||25 Aug 2011||Integris Rentals, L.L.C.||Pipeline Pig Storage Rack Apparatus|\n|US20130276828 *||19 Apr 2013||24 Oct 2013||Cokebusters Ltd.||Relateing to pipeline pigs|\n|WO2010096379A3 *||16 Feb 2010||2 Dec 2010||Misc B.V.||Bypass pig|\n|U.S. Classification||15/104.061, 118/105, 15/104.068, 15/104.15|\n|International Classification||B08B9/04, B08B9/055|\n|24 Jul 2000||AS||Assignment|\nOwner name: HAMDEEN LIMITED, UNITED KINGDOM\nFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:LAKER, KEITH;REEL/FRAME:010982/0216\nEffective date: 20000721\n|10 Jun 2003||AS||Assignment|\nOwner name: HAMDEEN (UK) LIMITED, UNITED KINGDOM\nFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:HAMDEEN LIMITED;REEL/FRAME:014153/0498\nEffective date: 20020324\nOwner name: HAMDEEN INCORPORATED LIMITED, UNITED KINGDOM\nFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:HAMDEEN (UK) LIMITED;REEL/FRAME:014153/0495\nEffective date: 20020403\n|26 Feb 2008||FPAY||Fee payment|\nYear of fee payment: 4\n|22 Sep 2011||FPAY||Fee payment|\nYear of fee payment: 8\n|26 Oct 2011||AS||Assignment|\nOwner name: WEATHERFORD SWITZERLAND TRADING AND DEVELOPMENT GM\nFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:HAMDEEN INCORPORATED LIMITED;REEL/FRAME:027120/0597\nEffective date: 20090812\n|9 Mar 2016||FPAY||Fee payment|\nYear of fee payment: 12']"	['<urn:uuid:c3d96bff-9969-4241-b2be-ce40578ebddc>', '<urn:uuid:66e0e804-14c7-4d8a-a64b-2e56c4ff65ec>']	factoid	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-12T22:13:20.996306	11	70	4870
79	civil engineering field inspection process improvements mobile technology real time updates	Civil engineers can now conduct inspections using mobile devices that provide near real-time access to key information, including original project calculations, drawings, scheduled maintenance, material tests, and previous inspection results. The information is stored in a digital repository that can be accessed through an app. Inspectors can immediately add new inspection details and photos through their mobile devices, making the information instantly available to the entire organization.	['ROME, Nov. 25, 2020 /PRNewswire/ — A new phase in the field of road network security and infrastructure monitoring is starting. ASPI Tech, IBM (NYSE: IBM) and Fincantieri NexTech announce that the new digital system for civil infrastructure monitoring, based on IBM Maximo, is now in operation. The system has been developed by Autostrade Tech, the technology-focused subsidiary of Autostrade per l’Italia Group, together with IBM and Fincantieri NexTech.\nThis is a very important stage for the digital strategic development of Autostrade per l’Italia as an integrated mobility manager. It represents a significant investment in the digitization of assets and procedures, sustainable mobility, management and the provisioning of innovative services for civil infrastructure, road networks and beyond.\nThe new monitoring platform will use IBM Artificial Intelligence, drones, IoT (Internet of Things) and Fincantieri NexTech 3D digital modeling to deliver innovation in the surveillance and monitoring activities of the more than 4,500 structures managed by ASPI including bridges, viaducts, flyovers and tunnels, obtaining improved efficiency and transparency in these processes.\nThe collaboration signed between Autostrade Tech, IBM and Fincantieri NexTech is expected to be made available to the market both as a service and through on premises models.\nThe new system is intended to improve the process of conducting civil infrastructure inspections in a number of core ways. Civil engineers will be able to carry out inspections on the condition of each structure and access key information in the field via a mobile device that is updated in near real time, including calculations and drawings of the original project and subsequent interventions; scheduled checks and maintenance; investigations and tests on materials; and the results and details of previous inspections. This information is maintained in a digital repository, which collects information classified by type and can be consulted through an app for adequate usability in the field. Through the same mobile device, the inspector can include details and photos from an inspection, making them immediately available to the rest of organization.\nThe new software also traces and manages the various steps necessary for the care of each structure, from conducting inspections, to the planning and implementation of maintenance or activities according to the priority criteria developed with the Ministry of Infrastructure and Transportation (MIT). The system also introduces Fincantieri NexTech advanced technologies never used before on Italian road networks, i.e. the ability to analyze a structure through three-dimensional «digital twins.» These digital twins reproduce a structure’s features through the use of drones equipped with topographic laser-scanners and high-resolution cameras, which can then be analyzed by AI to assist in the detection of imperfections. This visual defect detection model was specifically developed to support technicians in the recognition and classification of defects and in the planning of maintenance activities.\nThe use of Digital Twins will help enable the implementation of an innovative instrumental monitoring model, which utilizes Fincantieri NexTech Industrial IoT sensors and latest generation technological solutions and will allow analysis of the trends of the static and dynamic structural engineering parameters, as well as the development of algorithms to evaluate a structure in use and during maintenance activities. By 2021, a new dynamic weighing system is expected to be introduced to monitor the authorized weight limits for the road system as well as the near real time behavior of infrastructure being used by heavy vehicles.\nThe platform is in use today on 430 structures located on the two trunk roads of Cassino and Bari in Italy and is expected to be further extended by the end of the year to all 1,943 bridges and 2,000 overpasses across the Autostrade per l’Italia network. Over the course of 2021, these capabilities are expected to be extended to all bridges, overpasses, and to all 587 tunnels in the network throughout the country.\nThe technologies put in place by Autostrade Tech, IBM and Fincantieri NexTech will also allow experimentation at a scientific level, which will further the development of new models, algorithms and parameters for infrastructure security. To support this ongoing effort, Autostrade Tech has set up a Technical-Scientific Committee, with the participation of the Polytechnic Universities of Trento, Turin, Rome, Naples and Milan, with the task of coordinating these experimental activities, and defining new operational procedures that will be later agreed upon with the Ministry of Italian Transportation. The first research project will be dedicated to the use of modern sensors in monitoring of infrastructure’s behavior under certain conditions.\nThe total investment for the new system is more than 60 million euros, provided by Autostrade per l’Italia.\n«We are working hard and passionately to implement a radical transformation at Autostrade per l’Italia that includes technological innovation, infrastructure digitalization, the enhancement of environmental sustainability and mobility services. By connecting our networks territory-wide in compliance with local guidelines we continue to advance our vision of becoming an integrated mobility operator at the European level. The safety of the infrastructure we manage continues to be our first and fundamental objective. I would like to thank the Autostrade Tech team for their work and IBM and Fincantieri for the support they have given us. Together we expect to make the new platform available to European and international markets,» said Roberto Tomasi, CEO of Autostrade per l’Italia.\n«Technologies such as artificial intelligence, together with renowned civil engineering expertise can help us face most of the challenges we encounter during this transformation, assisting operators in more efficient management of our civil infrastructure. In particular, the project with Autostrade Tech and Fincantieri NexTech aims to support operators and professionals in making better, more effective and efficient decisions. This innovation, based on IBM Maximo Application Suite, will bring real benefits which the country needs today. We will continue to work with IBM Research as well as strategic partners who specialize in civil infrastructure engineering and assessment on advancements to our tools. These civil engineering tools will continue to become more and more powerful in the detection of faults and helping to assess risk. We expect to deliver additional advanced AI enhancements in the middle of 2021 and will continue to enhance them over time,» said Enrico Cereda, President and CEO of IBM Italy.\nFincantieri’s CEO Giuseppe Bono commented: «This agreement confirms Fincantieri’s strategy of expanding our competencies, a path that over time has allowed us to strengthen the Group, and to always work together with leading partners, like Autostrade and IBM. We have an extraordinary wealth of know-how in many areas, gained through the management of very complex processes and projects and we are therefore proud, through the high technological level we express, to contribute to the goal of increasingly secure networks.» Bono concluded, «In this action we acknowledge the great attention of the Government, specifically the Ministry of Infrastructure and Transport, which follows very closely our action and at the service of which we can achieve very important objectives for the growth of the country.»\nAbout IBM Maximo\nPowered by IBM’s investments in artificial intelligence, fueled by IoT data, and built for hybrid cloud, The IBM Maximo Application Suite is extending its leadership as one of the most trusted enterprise asset management systems on the planet. And with new investments in remote monitoring, computer vision and AI-powered anomaly detection, it is poised to remain a leading solution for tomorrow’s asset management challenges, empowering Operational Technology (OT) and Information Technology (IT) leaders with a comprehensive view into asset performance. For more information please visit: www.ibm.com/products/maximo.\nView original content to download multimedia:http://www.prnewswire.com/news-releases/autostrade-tech-ibm-and-fincantieri-nextech-deploy-a-new-artificial-intelligence-and-iot-based-system-for-monitoring-autostrade-per-litalia-civil-infrastructure-301180728.html']	['<urn:uuid:407cfe80-0eaf-4c2c-b40a-212b36f18ec7>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T22:13:20.996306	11	67	1238
80	heart disease death rate female usa	Heart disease kills 1 out of 4 women in the United States. Heart attacks are more deadly in women than in men, with more women dying within a year of having a heart attack. This higher mortality rate may be due to women being older when they have heart attacks, not recognizing their symptoms as a heart attack, delaying seeking care, and not responding as well to standard heart attack treatments.	['According to the National Heart, Lung, and Blood Institute, heart disease kills 1 out of 4 women in the U.S.\nHeart attacks and their aftermath tend to be more deadly in women. More women than men die within a year of having a heart attack. This may happen because women are generally older than men when they suffer heart attacks. Women also tend to think their symptoms are not those of a heart attack and they don’t seek immediate care. Further, women don’t respond as well as men to the treatments usually prescribed during or after a heart attack.\nFor many women, a heart attack may feel like a strange discomfort in the back or some other sign that is easy to ignore instead of crushing chest pain.\nWhen women do go to the hospital, healthcare providers may miss the diagnosis of heart attack because the symptoms are not clear. Without a definite diagnosis, a woman may be sent home thinking that her symptoms don’t mean anything serious.\nStudies confirm that heart disease may differ in women in ways that healthcare providers may not recognize. Heart disease in many women doesn’t occur from obvious blockages in arteries as it does in men.\nInstead, for women, plaque often spreads evenly along the artery wall or in the smaller arteries. This is called microvascular syndrome. Some experts suggest that changes in hormones connected to aging or inflammation may explain why women’s smaller blood vessels are affected. These areas don’t show up on an angiogram, the standard imaging test that measures blood flow in the big arteries.\nIn women with microvascular syndrome, blood flow to the heart falls dangerously low. But women don’t often feel the “elephant-on-the-chest” pain that takes place when large arteries shut down. Instead, they may have subtle symptoms. They may feel pressure or squeezing or shortness of breath. Symptoms may even pop up elsewhere in the body, such as the jaw. (This symptom of jaw pain can also appear in men having a heart attack.) Many women feel more tired than usual or dizzy or lightheaded. Many have pain in their stomach and think they have indigestion or heartburn.\nMany women with this disease may continue to have symptoms and become sicker. They may be at an increased risk for heart attack within 5 years.\nThe stage for heart disease is set before menopause by factors like high blood pressure, high cholesterol, extra weight (especially around the waist), and smoking. All of these factors play a part in plaque buildup.\nWomen are more likely to have “nonclassic” heart attack symptoms than men. Still, healthcare providers note, unexplained jaw, shoulder, back, or arm pain can also signal a man’s heart attack even when the classic crushing chest pain is absent.\nThese are the most common warning signals for heart attack:\n- Pain or discomfort in the center of the chest that lasts more than a few minutes, or goes away and comes back\n- Chest discomfort with sweating\n- Pain that spreads from the chest to the arm, neck, or jaw\n- Shortness of breath, tiredness, or upset stomach. These are particularly common in women.\nIf you are at risk for heart disease and have any of these symptoms, seek medical attention, up to and including calling 911, immediately. Time is a crucial factor in a heart attack because the longer the blockage remains untreated, the more heart muscle will die. Also, drugs that break down blockage in the arteries must be given within the first few hours.\nCheck your blood pressure\nExperts recommend that women have their blood pressure checked any time they go to a healthcare provider, including visits to an OB/GYN provider.\nHigh blood pressure is one of a cluster of symptoms called metabolic syndrome that also includes high cholesterol, high triglycerides (another type of blood fat), insulin resistance, and obesity.\nBefore menopause, women tend to have high levels of “good cholesterol” (HDL) which provides some protection to their arteries. After menopause, HDL levels go down and there is an increased risk of heart disease and heart attacks after menopause.\nAny woman found to have microvascular disease, metabolic syndrome, and other heart disease risk factors needs treatment.\nThis often means taking medicine, but with mildly elevated numbers, lifestyle changes like regular exercise and losing weight may help. Talk with your healthcare provider about your target LDL (“bad”) cholesterol, HDL (“good”) cholesterol, and triglycerides levels.\nFor women with high cholesterol, high blood pressure, or diabetes, taking medicine is key to help prevent heart disease. Exercise and diet are still important, but are secondary. Many women will need to make healthy lifestyle changes and take medicines to keep their risk factors under control.\nPreventing or controlling heart disease may mean making changes in the way you live. A healthy heart requires a personal action plan. But where do you start? A complete medical checkup is a sensible first step, especially if you have many risk factors. Your healthcare provider can tell if you have cardiovascular disease or its risk factors, and if so, help you with a practical treatment plan. Even if you don’t have any risk factors now, you can discuss ways to lessen your chances of developing them.\nOnline Medical Reviewer: Snyder, Mandy, APRN']	['<urn:uuid:bbb06f88-a045-4632-97f5-257f144e8bfb>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T22:13:20.996306	6	71	877
81	How do fishing conditions and battery charging methods vary by season?	During spring and early summer, the lake offers fantastic fly-fishing opportunities with trout visible in shallow waters. Golden perch are mainly a spring and summer target species. The lake can become turbid after high rainfall and is susceptible to blue-green algae in summer. Regarding battery charging, it should be done in well-ventilated areas year-round, as recharging produces explosive hydrogen and oxygen gases. The battery should charge for around 30 minutes while running the engine to allow sufficient charging by the alternator.	"[""Curran Reservoir - Malden / Newstead. Victoria\nCurran Reservoir was formed by the damming of the Loddon River\nand is located near the historical gold mining towns of Malden\nand Newstead in central Victoria.\nThere are caravan parks at Welshman's Reef (Newstead) and near\nthe dam wall. The boat ramps are located at the dam wall, Welshman's\nReef and Picnic Point and are suitable for boats to 5m for Picnic\nPoint and the wall and 4m for Welshman's Reef (depending on water\nThe lake can be turbid after periods of high rainfall and is susceptible\nto blooms of blue-green algae during summer but is clear most\ntimes of the year.\nThe lake holds good populations of trout (max. 3.5kg, av. 1kg),\nredfin (max.2.5kg av. 500g), golden perch (max. 5kg av. 1.2kg),\neels and carp.\nThere are extensive shallows around the lake's margins that offer\nsome fantastic fly-fishing around spring and early summer. Trout\nto 3kg can be seen in water as shallow as 20cm and can be very\nhard to tempt. Best flies are stick caddis, Mrs. Simpson, Tom\nJones (and damsel nymph imitations), Fuzzy Wuzzy and smelt patterns.\nThere are also many steep banks and rocky outcrops that give good\nland-base access to deep water.\nTrout can be caught on a variety of baits in this lake including;\nscrub worms, live minnow, small yabbies and mudeye suspended under\na bubble float.\nMoloney holds a solid Cairn Curran brown trout\nTrolling is very popular at Cairn Curran and trout can be taken\nin water from 2 to 20 meters with the use of a downrigger.\nBest lures include; Tassie devils with a bit of sparkle in them,\npink or mauve Tassie devils, minnow styled lures such as Attacks,\nMerlins, Rapalas and McGraths and wide action spoons.\nRedfin are another very popular target in Cairn Curran and on\nof the best method is to tie up to one of the dead trees in the\nlake and lower a bait or jig to the bottom. Give it about 20 minutes\nand move to the next tree. Continue this until a school of fish\nBest baits include; gudgeon or minnow, small yabbies, mudeye and\nworms. Best jigs include; ice jigs and Baltic bobbers.\nAnother very popular way of targeting the redfin population is\nto find a patch of water that is clear of trees and about 5 -\n8 meters deep, send down some heavy jigs and drift across.\nThinking anglers are also employing the use of downriggers in\nthe old Loddon River bed and near the wall to target the larger\nredfin during winter with good success.\nGolden perch are mainly a spring and summer target species and\ncan be taken on most baits, lures and some flies. The lakes 'GP'\npopulation are best targeted using live yabbies and large scrub\nworms around submerged timber or rocky points.\nThey will also take a large range of deep diving lures such as\nStumpjumpers, small codgers, wiggle warts and fat raps.\nFly-fishing for golden perch in Cairn Curran is still in its infancy,\nbut due to gain in popularity. Flies to use when targeting GP's\nneed to be bulky and lightly weighted to get down deep. Your fly\nline should be either intermediate or sinking and be around 7\n- 9 weight.\nCurran Reservoir Facts & Figures\nLevel Link Goulburn - Murray Water\nCurran Reservoir Map Goulburn - Murray Water\nCurran Reservoir Map\non map for printable version\nCurran Reservoir Map"", 'The practice of charging a car battery indoors is not entirely reliable or recommended. By charging your car battery at home or in a confined space, you might get the corrosive sulfuric acids stuck to your clothes and skin. Moreover, hydrogen build-up can cause the battery to explode.\nIs it safe to have a car battery indoors?\nIf the battery is simply being stored (not charged or discharged) in the room and it is in a proper protective case, then it is perfectly safe. If the battery is the type with removable caps, then care must be taken during charging due to the release of hydrogen gas during the charging process.\nHow do I charge a car battery with a wall outlet?\nHow to Charge a Car Battery From a Wall Outlet\n- Add distilled water to the battery until the internal plates are covered. …\n- Remove any jewelry to reduce risk of arcing and burns in the event of a short circuit.\n- Connect the red cable to the positive (+) pole on the battery.\nCan I charge a car battery in my basement?\nPlace the battery charger on a concrete, flat surface away from flammable materials and wood. Use the charger in a garage or basement where it is away from household furnishings.\nHow long should you leave a car running to charge the battery?\nRemember: After you’ve done a jump start, you’ll need to keep the vehicle’s engine running for around 30 minutes to allow the alternator time to charge the battery sufficiently.\nIs it OK to charge a car battery in a closed garage?\nCharging of the battery in the vehicle is simpler and is preferable for safety reasons, although this is not always possible. If no garage or an electricity connection is available, there is often no alternative to charging the battery outside of the vehicle. Ensure good ventilation when charging in enclosed spaces.\nDoes charging a car battery give off fumes?\nRecharging, moving, or shaking a lead-acid battery can produce an explosive mixture of hydrogen and oxygen gases that escape through the battery’s vents. These fumes, if allowed to accumulate in a small area, ignite easily and can cause a fire or explosion.\nDo sealed batteries need venting?\nSealed batteries do not need to be vented.\nAlthough most of the normal gasses (oxygen and hydrogen) produced in an SVR battery will be recombined and not escape, oxygen and hydrogen will escape from the battery in an overcharged condition (which is typical with any battery type).\nCan a car battery be used for home electricity?\nDue to advances in electronic technology, an automobile battery can also serve as a temporary power source for a small home. Since most household appliances require alternating current as an energy source, an electronic device called a power inverter must be connected to the automotive battery before using in the home.\nCan I charge my car battery without disconnecting it?\nAlthough charging a car battery while still connected or in situ is possible, it’s always a good idea to disconnect the battery before charging after a quick clean.\nIs there any way to charge a car battery without a charger?\nPlace the red clamps on the positive terminal (marked positive) of your car. Place the other end of the red clamps on the positive terminal of the good battery. The charging car operator should start her vehicle first and let it run for a minute or two. Allow your car battery to draw a charge for a few minutes.\nIs it OK to leave my car battery on charge overnight?\nIt is not safe to charge your car battery overnight as it damages the battery. Replacing a damaged battery will cost you significantly. To avoid all these extra costs, follow your battery manufacturer on the car battery’s usage and maintenance. Select the correct charger for your car battery.\nCan you charge a battery on the ground?\nSo, it’s OK to set or store your battery on concrete.” … Encased in steel, an Edison cell battery placed directly on a concrete floor would discharge more quickly than normal. A subsequent innovation, encasing the battery in hard rubber, also had its drawbacks, as rubber is both made of carbon and a bit porous.']"	['<urn:uuid:f8a739ae-a3e9-4028-8dba-7e725d3ba131>', '<urn:uuid:f26683b5-9cef-4bea-9356-bfa6d7db202a>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T22:13:20.996306	11	81	1289
82	In what situations does our body naturally produce unstable molecular compounds during everyday activities?	Free radicals are produced naturally as a byproduct of chemical reactions in our cells. They are formed when food is converted into energy, during exercise, and when we are exposed to things like sunlight, cigarette smoke, radiation, or pollution. This formation is normal, as plants, animals, and humans produce free radicals all of the time.	['© Caldix | Dreamstime.com\nYou may already be supplementing your diet with antioxidant-rich foods and supplements like berries, green leafy vegetables, and more. But do you actually know what an antioxidant is? What do antioxidants do? To give you a better understanding of how these disease-fighting compounds keep you healthy, let’s start with a discussion of free radicals and oxidative stress.\nWhat Are Free Radicals?\nFree radicals are highly unstable molecules produced naturally as a byproduct of chemical reactions in our cells and also when we are exposed to toxins. Free radicals are formed when the food we eat is converted into energy, when we exercise, and when we are exposed to things like sunlight, cigarette smoke, radiation, or pollution.\nElectrons typically come in pairs. Free radicals contain unpaired electrons, which is what makes them so unstable; they will wreak havoc to steal the electrons they need from other molecules. In doing so, they can cause significant damage to cells in the body and can set off chain reactions of what is termed oxidative damage and oxidative stress.\nOnce one free radical is formed, it creates a domino effect, creating many more free radicals as a result.\nThe Effects of Oxidative Damage\nFree radical formation is normal. Plants, animals, and humans produce free radicals all of the time. Our bodies have defenses against free radicals, but when the strength of these defenses are outweighed by the amount of free radicals themselves, they can cause lasting harm, and even cell death.\nOxidative stress on the body can damage DNA, leading to cancer; compromise cell walls, allowing substances into the cell that aren’t supposed to be there; and make LDL-cholesterol stick to blood vessels, causing atherosclerosis.\nOxidative damage has been linked to several diseases, including cardiovascular disease, Alzheimer’s disease, and cancer.[1,2] So how can you help your body to prevent these devastating effects of free radicals and oxidative damage? The answer lies in antioxidants.\nWhat Do Antioxidants Do?\nAntioxidants come in many different shapes and sizes, but the quality they all share is that they are able to neutralize free radicals, preventing them from damaging the body. They often act by donating electrons to free radicals, making them more stable. They also can degrade free radicals, rendering them harmless.\nWhatever the mechanism it uses, an antioxidant helps protect the body from damage by interrupting the deleterious effects of free radicals and the chain-effect of oxidative damage that they cause. In essence, the benefits of antioxidants are to properly detoxify the body of these harmful molecules.\nResearchers are continuously learning more about how oxidative stress plays a role in the development of many diseases and age-related conditions, and how antioxidants might help to prevent them. Antioxidant intake may be protective against dangerous diseases such as cancer, cardiovascular disease, and neurodegenerative diseases like Parkinson’s and Alzheimer’s.[1,2]\nSome antioxidants, such as the compound glutathione, are produced by the body to naturally defend against oxidative stress. Others we get through diet, such as vitamin C and vitamin E.\nThere are numerous foods with antioxidants that benefit our health, including polyphenols from berries, carotenoids from sweet potatoes, resveratrol from grapes, and more. Oftentimes, the most colorful foods have the most potent antioxidants.\nNow that you know why antioxidants are so important to your body, be sure to keep loading up your plate with foods high in antioxidants such as fresh berries, green leafy vegetables like kale, green tea, vitamin C rich bell peppers, vitamin E rich sunflower seeds, and other antioxidant powerhouses such as artichokes, dark chocolate, pomegranates, cinnamon, and nuts.\nShare Your Experience\nWhat are antioxidants that you like to incorporate into your diet? What are your favorite antioxidant foods? Do you have any experience with antioxidant supplements? Share your tips in the comments section below.\nOriginally published in 2015, this post has been updated.']	['<urn:uuid:374fc3fc-f4a2-4466-a961-c28fcba33889>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T22:13:20.996306	14	55	639
83	first humans farming agriculture arrived when central france	The first agricultural Neolithic people arrived in central France around 6,500 years ago, following the recession of the Ice Age. These early farmers developed plows which allowed them to produce more food and support a growing population.	['by Dean Alexander\nErosion has had a monumental impact on the character of the wines of Burgundy. It took several decades once the INAO began preventing exogenous soil additions (early 20th century), before growers slowly began to realize that they must change the way they work their fields. They could no longer hit reset, by bringing in new soil to fix what they had damaged through poor farming practices. The vineyards have since responded positively; with increasingly healthier soils, and far better soil retention. The region is now producing the finest wines in its long history. But without a doubt, the erosional damage of the past has been so immense and irreparable, that we will never really know what the terroir of Burgundy might have been.\nHow long ago this happened, will certainly surprise you.\nThe First Farmers\nWith the recession of the Ice Age, the Neolithic hunter-gatherers of the region were now free to venture northward, allowing the arrival of agricultural Neolithic man in central France, 6,500 years ago. Around that time, the first plows were developed, and with the economy of effort it provided, more food could be produced. This in turn allowed the population to grow, greatly increasing the need for arable land.\nTo meet that demand, they burned to clear forests for pasture and fields. This was an expedient means of what would otherwise take years of work. The unintended consequences of burns to facilitate clearing, were often massive, fast-moving wildfires that swept though forested and grassland areas.\nWithout the protection of trees and grasses upon the hillsides, the erosion that ensued was monumental. There may have been more erosion in the 700 years Neolithic man farmed the land of central Europe, than in the preceding 35 million years since the Côte d’Or was formed, and perhaps more than all of the time since. Although through intervening centuries have seen the reforestation of the hillsides, the damage done by Neolithic man permanently changed the landscape of France.\nWhat did Neolithic man look like? Click here.\nThe Middle Ages\nSince the Neolithic, two subsequent periods of deforestation occurred, each time followed by large-scale erosion. The least destructive of the two was the periods between the 12th and 15th century, which despite the black plague in the middle 1300s, saw a large population growth in France.\nThe king, or the Duke in Burgundy’s case (1), would grant large parcels of land from the royal demesne (domaine) to his nobility, who were considered the servants of the Duke. Known as seigneurs, the nobility, would then use the land to raise money to fund the Duchey. The seigneur granted strips of land to tenants (serfs) to farm in open fields. These fields where then were farmed communally by the inhabitants of the manor. Intermixed with the tenant parcels were the demesne of the seigneur, and the demesne of the church – all of the land which was worked by the surf communally as partial payment for their tenant rights.\nThe rights the tenants had to the land were very strong and generational. They could not be evicted from the land by the seigneur. Additionally, the tenants were able to accumulate rights to more than one strip of land, meant parcels could be scattered across the manor. A transfer of land rights typically happened when a tenant died and had no heirs. At that time another tenant would assume the right to work that parcel. This occurred on a massive scale in the wake of the black plague, which arrived in Lyon in 1348. Lyon, which was only 155 km, or 96 miles along the main highway, the Via Agrippa, from wine villages of the Cote d’Or. There is little doubt that the plague struck the Cote d’Or very hard.\nNewcomers to the manor who had no land rights worked for tenants that had more land than they could work themselves. It is estimated that half the of the agricultural community consisted of landless serfs.\nThe manor model, with its communal farming, required everyone to adhere to the norms of the region, and this discouraged innovation and adoptions of new techniques, causing production per hectare to lag behind farms in England, Holland and elsewhere in the world. The farmer’s dependence on the communal sharing of prohibitively expensive horses and plows needed to farm the heavy clay soils of central Europe only reinforced the status quo.\nThe inefficiencies of farming under this system meant that as the population grew, it required that the economy remained primarily both rural and agrarian. The existing estates could not supply enough food if population grew mainly in urban centers, so population tended to grow in rural areas. More mouths to feed, and more able hands to employ, meant economic opportunity for the Duchy if new arable land could be developed from the forests.\nEven though the open field system inherently discouraged innovation and suppressed productivity, the system proved to be so economically successful its existence eclipsed the time of feudalism. Right up to the revolution, the open field system to continue to fund well-heeled landowners in this very capitalist endeavor. But even then, to say the open field system was gone, might be an incomplete truth. The people may have then owned the land, but their situation had not greatly changed. In fact, until only recently, the wide-spread division of small parcels ensured the impoverishment of paysans across Bourgogne-Franche-Comté, with an obvious, strong parallel to the medieval tenant arrangement. Indeed, the old lord-tenant arrangement of métayage (sharecropping) would reemerge. post-1789 revolution, between those who owned the land, and laborers who would work it. In 1929 there were 200,000 Métayers in France, farming the same 11 percent of agricultural land. This was truly not so differently as had been the arrangement in 1729, or in 1529 for that matter.\nAs with a population that doubled in the 3 centuries after 1000 AD, the needs for timber and hardwood also increased. Wood was needed for construction, woodworking, iron smelting and metal working, not to mention fuel for heating. All of these needs multiplied the pressures on deforestation. Although forest management had to various degrees been practiced, it tended to be exercised on forests on properties owned by the aristocracy and the church. Elsewhere, woods fell to the ax and saw.\n18th century: The last major assault on terroir\nA devastatingly cold 17th century followed, slowing the population growth and economies. The end of that century saw the failed harvest of 1693, when the death toll, according to David Huddart, and Tim Stott of Europeans is thought to have numbered in the millions. This period of economic lull set the stage for a final epoch of deforestation and erosion of France.\nBy the mid 18th century, the average temperature had risen enough to achieve food security. Once again, with food in their bellies, populations rebounded, and focus on innovation brought healthy economies. Industrial development ensued, bringing expansion and colonialism. Massive fleets were built, from forests felled for the needed timber. As the population grew again, farming and pastureland expanded once again to support the needed food supplies. The open field system prevailed through this period, and given their inefficiencies, yet more land was needed to feed the population. To these pressure, the forests fell away, leading to erosion.\nThe protected hunting forests of the Aristocracy, and those belonging to the Church, alone stood untouched. While these forests were often noted as early forestry, it is somewhat disingenuous call this entitlement “forest husbandry”. Indeed, by the time of the French Revolution the royal forests had become a hated symbol of privilege.(2)\nUnlike the medieval period that saw erosion primarily because of deforestation, this dawn of industrialization created many new erosional sources. Iron works and foundries required mines and open pits to be dug to excavate ore, while limestone, prized for its hardness, was quarried across the country, including within the vineyard land of the Cote d’Or.\nIt was the wealth of the times that created a demand for Burgundy’s limestone. Thousands of large building projects: for the Church, wealthy private citizens, the aristocracy, for government buildings and public works, all of which required vast amounts of building materials. The high demand created such soar value for the “marble”. I had originally concluded when first writing this article, that the value of the limestone below, outsized the value of the grape production of that location, but I have since come to what I believe to be a more valid conclusion. I submit that the quarries dug in locations in which the limestone remained unfractured, examples of which can be seen in the climates of Meursault Perrières, Clos de Beze, Bonnes-Mares, and some submit, even Romanee-Conti, made those particular locations unsuitable for quality vine cultivation, unlike the superb plots which surrounded them.\nIt was used in its solid slab form for wall paneling and floors, but the rubble was also burned in special kilns to produce Quick lime (calcium oxide) which is the primary ingredient of both mortar and plaster. Softer limestones were often sought for the production of quicklime, as it was far easier to excavate the softer stone than the harder, unfractured stone which was required for floors and wall paneling.\nThe excavation of the limestone not only changed the substratum and topography of these vineyards, but greatly affected vineyard lands to either side of these projects, and with substantial impact to the vineyards below. This is where the overburden (the topsoil and useless rubble) was cast, in the most expeditious manner, downhill.\n175 years later, the disruption of such a quarry site to the terroir of the region is easily seen in the two vineyards of Les Perrières in Meursault, and Les Charmes, which lies just below. A large quarry was cut out of the hillside of Meursault–Perrières Dessous. The location of bulk of the excavation appears to now have been declassified from Les Perrières, as well as a wide strip above the exposed limestone wall. The sub-plot of Clos des Perrières which is owned by Albert Grivault vineyard is just below the main area of excavation, but it was certainly was part of the quarry itself. The area directly behind the removal site would certainly have been utilized for temporary buildings, for staging or even storage of limestone before transport, a loading area for horse carts, and space for any other logistical needs a quarry would require. The slope of this entire area was more or less leveled from it previous gradient. Clos des Perrières begins that the overburden would have been spread, although. The dirt roads of the regions were also impacted, by the transit of thousands of heavily loaded wagons, itself causing extensive erosion. And then it would rain.\nThe sections of Les Charmes-Dessus, lying just below this quarry received the discharge of overburden, deepening the soil along this half mile of roadway. That this discharge and erosion onto Les Charmes Dessus, and no doubt Les Charmes Dessous, lying just below that, is without question. The soil depth was increased by the alluvial soils eroded from the quarry site, in addition to any normal erosional deposits that would have occurred, giving the vines more depth than they require, mimicking vineyards that are actually lower on the slope. The wines from Meursault Charmes, are fairly commonly described as fat, without the vibrancy and minerality of Les Perrières, and often given the faint praise of being rather hedonistic.\nExcavations by Thierry Matrot in 1990 in his parcel of Meursault–Perrières (parcel 15 in the map to the right) found roughly one foot of topsoil before striking the limestone base. Whereas, digging into his plot of Meursault-Charmes however proved to be far more work. Here a pit of 6 feet was dug before hitting the limestone substrata.(3) This indicates, a significant amount of limestone colluvium had developed in Charmes, that has mixed with transported clay to attain this six-foot depth of marl dominated soil.. I have not been able to determine the location of the Matrot’s plot (or plots) in Les Charmes. It is a large vineyard and without the dig location, this information doesn’t have nearly as much meaning as it would otherwise. It does illustrate the dramatic effect erosion has had on the vineyards of Burgundy and the character of the wines from each location.\n(1) The Burgundians were an Eastern Germanic tribe which likely crossed the Rhine in 406 AD, in a combined force with the Vandals, Alans and Suebi tribes. The Roman forces there had largely departed four years earlier to deal with Visigoth king, and sometimes Roman ally, Alaric, who would ultimately be an actor in the fall of Rome. But the crossing signaled the end of Roman rule Central Europe.\nThe Kingdom of the Burgundies, ruled the lands east of Paris, down to the Mediterranean with various boundaries. A series of smaller Duchy, including the Duchy of Burgundy, succeeded the Kingdom of Burgundies in 1032. The Duchy was relatively sovereign, but owed its allegiance to the French crown. The influence and power of the Duchy expanded greatly in 1384 with a union with the Hapsburgs. The house of Valois – Burgundy, the ruling family of the Duchy of Burgundy at the time, ultimately expanded its control of fiefs in Holland and the Netherlands, parts of northern France and Luxembourg. In a bid to gain independence from France, 1477 Charles the Bold was killed in battle by a combined force of the Duke of Lorraine and a Swiss Confederacy. With no heir to Charles, and a weak hold on their power, the Valois were unable to prevent the Duchy from eventually being absorbed into France.\n(2) Empire Forestry and the Origins of Environmentalism, Gregory Allen Barton (p.11) Cambridge University Press\nClass and State in Ancien Regime France: The Road to Modernity?\nBy David Parker']	['<urn:uuid:bf7b1fdc-580d-482a-90ec-6cf42a04fafe>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T22:13:20.996306	8	37	2309
84	pro volleyball player looking for guidance on ball movement during serving give advice	The floater serve is effective for creating unpredictable ball movement. Start with left foot in front of right, shoulder width apart. Hold the ball at eye-level, left hand bottom and right hand top. Toss should only be high enough to contact the ball on its downward swing. When serving, stop the motion of your right arm upon contact with the ball, as if punching it, and don't snap your wrist. This technique makes the ball float and wiggle in the air, making it difficult for opponents to read its positioning and return the serve.	"[""The server holds the ball in\nthe left hand and contacts\nwith the right to serve underhand.\nEach play starts off with a serve. The server steps behind the line at the very back of the court, called the end line, and has freedom to serve from wherever he or she pleases as long as the foot does not touch or cross the line. If the server’s foot crosses the end line, it is considered a foot fault, and results in a side-out—a change in possession—of the ball.\nThe Underhand Serve\nThe underhand serve is simple—the player holds the ball in the hand opposite from the hitting hand, i.e. a right-handed player would hold the ball in the left hand. Hold the ball below the waist and above the knee so that with bent knees, the server is in good position to get the ball over the net.\nPlace contact between\nwrist and knuckles.\nFor a right-handed player, the ball should be held still in the left hand, and the right hand should make a fist with the thumb on top of the folded-in fingers. The right fist should make contact with the underside of the ball, and the point of contact is on the palm side of the right hand, between the folded-in fingers and the wrist. Bring the right arm back and swing through to make contact with the ball and send it over the net. The left hand should hold the ball still; do not throw the ball up or move it. The underhand serve is mainly used for lower skill levels.\nOverhand Serves: The Floater\nStart with your feet. Put your left foot in front of your right, about shoulder width apart. Hold the ball at about eye-level with your left hand on the bottom and your right hand on top. The toss should only be high enough so that when you pull back your right arm, the hand contacts the ball on its downward swing. Toss the ball with your left hand; pull back your right arm as far as possible and swing—but not all the way through.\nStop forward arm swing\nand 'punch' the ball.\nStop the motion of your right arm when your hand contacts the ball, as if you’re punching the ball to the other side of the net, and don’t snap your wrist. This serve allows the ball to float and wiggle in the air. This makes its positioning on the opposite side of the court very hard to read, and it confuses people, making it difficult for opponents to return float serves.\nThe beginning steps for the topspin serve are just like the floater, but the toss for the topspin must be higher. Toss the ball high enough so that the right arm can come down on the ball in a snapping motion on the underside of the ball.\nIt's all in the wrist.\nDo not stop the motion of the right arm when it contacts the ball; rather, turn your shoulder away from the ball, swing all the way through and DO snap your wrist. Snapping your wrist will deliver a hard and fast serve to your opponent. This is very similar to a tennis serve.\nThis is a very advanced type of serve, and it should only be used at high skill levels. Instead of keeping the feet stationary, the player tosses the ball high in the air and takes a multistep approach to hitting the ball. Although the player may land in the court after the jump serve, the feet must leave the ground behind the end line to be legal. The motions are similar to the topspin serve, but they are done in the air. Jump serves are very effective, but they are often inaccurate because of the inconsistency of tosses.""]"	['<urn:uuid:2d4d2d6e-3d35-4455-ace2-e68ab9211cad>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T22:13:20.996306	13	94	635
85	military expert here how did non israeli pilots assist the iaf in their fight for independence	Veterans from World War II – primarily from the U.S., Canada, England and South Africa – were crucial for Israel's air defense, as there were only a few Palestinian Jews with military pilot training. These veterans manned various aircraft including German, American and British planes to counter Arab attacks. According to Eddy Kaplansky, an expert on the IAF during 1948-49, the participation of these foreign volunteers (Machal) in the IAF was likely decisive in Israel's War of Independence, as without them, the IAF would not have been able to operate heavy bombers and transport planes or gain control of the skies.	"[""Aliyah Bet & Machal Virtual Museum\nNorth American Volunteers In Israel's War of Independence\nPictorial History: Air Force Volunteers\nOnly a handful of Palestinian\nJews had military pilot training. Veterans from World War II – primarily\nfrom the U.S., Canada, England and South Africa – manned the\nhodgepodge of German, American and British aircraft to stop the Arab\nair and ground attacks.\nSince England would not admit Palestinian Jews into the Royal Air Force for aircrew training until 1943, pilots, navigators and other flying personnel with military experience were in short supply. Israel turned to the Diaspora – primarily to the U.S., Canada, England, and South Africa – for veteran flying crews who could help stop the Arab invasion on the ground and in the air. More than half of all American and Canadian volunteers served in the air force.\nEddy Kaplansky, an authority on the IAF during the 1948-49 war, concluded: “It is generally accepted that without Machal the nascent IAF would not have been able to fly its heavy bombers and transport planes, nor could it have wrested control of the skies from enemy air forces. It may well be, therefore, that the participation of Machal in the IAF is what tipped the scales in Israel’s favor in the War of Independence, whose outcome hung precariously in the balance.”\nGroup of men in front of B-17\nA Heavy Bomber for Israel\nThis group of men, almost all Machal, are shown in front of their B-17 shortly before it took off for Israel from Zatec airport in Czechoslovakia in July 1948. It was one of three B-17s that had come from Miami. They remained the only heavy bombers in the Israeli Air Force for the duration of the war.\nTop Canadian Ace\nGeorge F. “Buzz” Beurling, shown painting a “win” on his Royal Canadian Air Force spitfire, was the top Canadian air ace in World War II, with 31 Nazi planes to his credit. He was one of 34 Christian pilots from the U.S. and Canada to volunteer for the IAF. Beurling, 26, and his Jewish co-pilot, Leonard Cohen of England, were killed while attempting to ferry a plane from Italy to Israel on May 20, 1948.\nGerman Fighter Plane for Jewish Air Force\nIronically, Israel’s first fighter plane was the famed German fighter Messerschmitt ME-109. The Czech’s sold 25 of its own version of the fighter to Israel. It had a bomber motor, making it very unwieldy to fly and land. Many of the planes crashed shortly before or after arriving in Israel. The Czechs then sold Israel Spitfires they had obtained from England. This was a much better fighter.\nRudy Augarten, 26, a Harvard student when he volunteered for the IAF, stands in front of a crashed Egyptian spitfire. Augarten had downed two German ME-109s while flying a P-47 during World War II. He shot down four Egyptian aircraft while flying a Czech ME-109, a British Spitfire and an American P-51. He was one of only six Americans awarded “ace” status by combining victories in two wars. Augarten returned to Israel in 1950-52 to become commanding officer of Ramat David Air Force Base and IDF Flying Courses.\nThis page of a flight log kept by flight engineer Bill Lichtman, 30, of New York City shows that the B-17 took off from Miami on June 19, 1948. After refueling stops in Puerto Rico and the Azores, it landed in Zatec, Czechoslovakia, on June 20 to be refitted with guns and bomb racks. It took off from Zatec on July 15 with a full bomb load, and on its way to Israel detoured for a bombing mission over Cairo. This was the first and time an Israeli plane bombed Cairo in 1948. This raid essentially put a stop to the daily bombing of Tel Aviv by the Egyptians. Pilot of the B-17 was Ray Kurtz, a former New York City fireman. Both Kurtz and Bill Lichtman were World War II veterans.\nRow of Norseman aircraft\nIsraeli Parcel Service\nSome 16 of these Canadian-designed Noorduyn Norsemans were purchased as war surplus in Germany, and refitted with long-range tanks by KLM airlines in the Netherlands. Machal pilots flew 10 of them non-stop from Rome to Israel in the first weeks of the war. It was a grueling 11½-hour journey. The large, single-engine planes supplied Dead Sea and Negev outposts, and even flew as improvised bombers.\nThree Americans Who Flew A German ME-109\nIsrael’s only fighter planes in the first few months of the war were 10 S-199 Aviahs (the Czech version of the famed German Messerschmitt ME-109). Shown with this Avia are, left to right, Gideon Lichtman, 24, Newark, New Jersey, Bill Pomerance, 26, also of Newark, and Leon Frankel, 24, St. Paul, Minnesota. Gideon Lichtman was the first Israeli Air Force pilot to shoot down an Arab fighter plane. Pomerance was one of the few U.S. pilots in World War II to down four enemy planes in one day. Frankel won the Navy Cross, the U.S. Navy’s second-highest medal, for sinking a Japanese cruiser while flying a torpedo bomber.\nPiper Cubs go to War\nPiper Cubs provided a host of different services. They served as observation planes in battle, ferried medical supplies to isolated kibbutzim, and dropped mail and newspapers to front-line troops. Machal instructors also used them as trainers for Israeli student pilots. A line of Piper Cubs is shown at the St. Jean student pilot airfield near Acco. (Note the bomb racks on the belly of the Piper.)\nNew Generation of Pilots\nOne of the first Israeli graduates of the flying course, Shaikey Gazit, receives his wings from Lt. Gen. Yaacov Dori, first chief of staff of Israel’s armed forces.\nMachal Ground Forces\nVolunteers brought important specialized skills to the fledgling Israeli Air Force – among them aerial photography, radar operations, and aircraft maintenance. Shown here are Machal volunteers exercising control room skills learned during World War II service.\nFreight to Win the War\nCurtis-Wright C-46 Commando\nThe Curtiss-Wright C-46 Commando could be called the plane that saved Israel. Ten of the huge planes were bought as war surplus in California. They were sent to Panama as part of the dummy “Lineas Aereas de Panama” (Panamanian Airlines) fleet, and then flown to Czechoslovakia. With Machal pilots serving in Israel’s Air Transport Command, they became the freight trains that shipped Czech arms, ammunition, and even disassembled aircraft to beleaguered Israel.\nHal Auerbach, 33, a Navy pilot in World War II, flew heavy transports in 1948 on their various journeys to Panama, Czechoslovakia, and Israel. A native of Escanaba, Michigan, he is shown with a DC-5. The crew drew a Star of David with wings on the nose and named the plane “Yankee Pasha, Bagel Lancer.”\n1,800 very, very tough miles, Zatec, Czechoslovakia, to Israel.\nThe Czech Connection\nA little-known story of the Arab-Israeli war of 1948 involved American planes, mostly-American pilots and the decision of one small country to help another.\nAfter approval of the Partition Plan in November 1947, England continued to supply weapons and ammunition to its client-states Egypt and Trans-Jordan. But virtually the entire world imposed an embargo on sale and shipment of military equipment to Israel—except Czechoslovakia, which agreed to sell a substantial amount of German-designed rifles, machineguns, ammunition, and even fighter planes to the Jewish state\nBut Czechoslovakia was land-locked, surrounded by countries that would not allow transshipment of arms in violation of the embargo, and more than 1,800 miles from Israel. With a chartered C-54 and seven of the C-46s originally purchased as war surplus in California, Machal pilots and crews in the Israeli Air Force Air Transport Command began an airlift that would become one of the miracles of the War of Independence.\nFor more than three months—May 1948 to August 1948—Czechoslovakia loaned Israel its Zatec air field. It become virtually an Israeli installation and was used for refitting\nplanes, training pilots, and shipping point for guns and ammunition. Pilots and crews of planes utilizing the airfield stayed at the Stalingrad Hotel in Zatec. (Shown at left of\nphoto is Bill Lichtman, a crew member of a B-17 being refitted at Zatec).\nC-46 over Haifa Bay\nCzech-made ME-109 fighters were taken apart, loaded into C-46s (it took two C-46s to carry the pieces of one ME-109) and flown to Israel, with one refueling stop in Corsica. The first ME-109 arrived in Israel on May 20, 1948. It was immediately re-assembled, and within one day attacked Egyptian troops nearing Tel Aviv. By the time the Czechs closed Zatec to the Israeli armed forces, Machal volunteers in the Air Transport Command had completed 95 flights, carrying 25 Me-109s to Israel and an incredible 35 tons of arms and ammunition, including more than 15,000 rifles, 4,000 machineguns and 3 million rounds of ammunition.\nEddy Kaplansky. A native of Montreal and a veteran of the Royal Canadian Air Force in World War II, Eddy was both an Aliyah Bet crewman (Northland\\Jewish State) and a Machal pilot in the Israeli Air Force during the War of Independence. A resident of Haifa, Eddy provided research and advice on most of the panels in this display. He wrote the entire article in this section on the history of the Air Force volunteers, and compiled the photos and text for the section on the 40 men who died in the service of Israel. Eddy wrote the definitive book on the early days of the Israel Air Force, “The First Fliers” (Israel Air Force Historical Branch, 1993), and meticulously named every Canadian who served in Machal. Eddy died in Haifa on March 6, 2005, while corresponding daily with the Gainesville production crew of the Museum of American and Canadian Volunteers in Israel’s War of Independence, located in the main hallway of the University of Florida’s Hillel building.""]"	['<urn:uuid:5e8c4767-f0b3-4eea-ae21-8a1078655576>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T22:13:20.996306	16	101	1636
86	I'm curious about how temperature affects different materials - can all changes caused by heating or cooling be reversed back to their original state?	No, not all changes caused by heating or cooling can be reversed. While some changes are reversible, others are not. For example, changes to water and butter at different temperatures are reversible. However, changes like cooking an egg, freezing a plant leaf, or heating paper are irreversible - they cannot be returned to their original state.	"['Disciplinary Core Ideas\nPS1.A: Structure and Properties of Matter\n• Different kinds of matter exist and many of them can be either solid or liquid, depending on temperature. Matter can be described and classified by its observable properties. (2-PS1-1)\n• Different properties are suited to different purposes. (2-PS1-2),(2-PS1-3)\n• A great variety of objects can be built up from a small set of pieces. (2-PS1-3)\nPS1.B: Chemical Reactions\n• Heating or cooling a substance may cause changes that can be observed. Sometimes these changes are reversible, and sometimes they are not. (2-PS1-4)\nPerformance Expectations: Students who demonstrate understanding can:\n2-PS1-1. Plan and conduct an investigation to describe and classify different kinds of materials by their observable properties. [Clarification Statement: Observations could include color, texture, hardness, and flexibility. Patterns could include the similar properties that different materials share.]\n2-PS1-2. Analyze data obtained from testing different materials to determine which materials have the properties that are best suited for an intended purpose.* [Clarification Statement: Examples of properties could include, strength, flexibility, hardness, texture, and absorbency.] [Assessment Boundary: Assessment of quantitative measurements is limited to length.]\n2-PS1-3. Make observations to construct an evidence-based account of how an object made of a small set of pieces can be disassembled and made into a new object. [Clarification Statement: Examples of pieces could include blocks, building bricks, or other assorted small objects.]\n2-PS1-4. Construct an argument with evidence that some changes caused by heating or cooling can be reversed and some cannot. [Clarification Statement: Examples of reversible changes could include materials such as water and butter at different temperatures. Examples of irreversible changes could include cooking an egg, freezing a plant leaf, and heating paper.]*\nUse the Template and Resource Links to Fulfill NGSS\nll. Introductory Activities to Assess Prior Knowledge\nA. Simple Activities - that assess students’ understanding of the phases of matter.\nB. Analyzing Matter: Give students 5 objects to ""analyze,"" i.e. sponge, block of wood, wax candle, ball of yarn, square of cheese. Ask them to:\nAnalyzing Matter Activity\nC. Brainstorming Session\nQuestion: Thinking about materials and their properties.\n1. Break students down into groups of 3-4.\n2. Ask students to generate a list of the different materials that exist is at least two phases (liquid, solid, gas).\nInquiry related to state of matter.\nV. Summarize Knowledge - Enduring Understandings\nWhen you research information you must cite the reference. Citing for websites is different from citing from books, magazines and periodicals. The style of citing shown here is from the MLA Style Citations (Modern Language Association).\nWhen citing a WEBSITE the general format is as follows.\nAuthor Last Name, First Name(s). ""Title: Subtitle of Part of Web Page, if appropriate."" Title: Subtitle: Section of Page if appropriate. Sponsoring/Publishing Agency, If Given. Additional significant descriptive information. Date of Electronic Publication or other Date, such as Last Updated. Day Month Year of access < URL >.\nAmsel, Sheri. ""Grade 2 - 2-PS1 Matter and Its Interactions"" Exploring Nature Educational Resource ©2005-2019. July 21, 2019\n< http://www.exploringnature.org/db/view/Grade-2-2-PS1-Matter-and-Its-Interactions >']"	['<urn:uuid:a36f0d59-191e-40a5-a0d1-6301b3f74551>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T22:13:20.996306	24	56	502
87	What materials can store heat in fireplaces versus wood stoves?	Traditional fireplaces use metal firebacks to capture and re-radiate heat, while wood-burning stoves can use soapstone panels or specialized heat storage modules with massive storage blocks to absorb and slowly emit heat over many hours.	"[""A open fireplace is a form\nA fireplace is a structure made of brick, metallic designed to contain a fire.|metal or stone designed to contain a fire.} utilized for the relaxing ambiance they create and for heating system a room.|Fireplaces are used for the relaxing ambiance they create and for heating system a available room.} Modern fireplaces vary in high temperature efficiency, based on the design.\nHistorically they were used for heating a dwelling,|These were used for heating a dwelling historically,} cooking, and heating water for laundry and domestic uses. is normally within a firebox or firepit;|A fire is within a firepit or firebox;} a chimney or various other flue enables exhaust to flee. A fireplace may have the following: a base, a hearth, a firebox, a mantelpiece; a chimney crane (found in kitchen and laundry fireplaces), a grate, a lintel, a lintel bar, overmantel, a damper, a smoke chamber, a throat, and a flue.\nOn the exterior there is often a corbeled brick crown,|On the exterior there exists a corbeled brick crown often,} where the projecting courses of brick become a drip course to keep rainwater from working down the surface walls. A cap, hood, or shroud acts to maintain rainwater out of the external of the chimney; rainfall in the chimney is certainly a much greater issue in chimneys lined with impervious flue tiles or steel liners than with the original masonry chimney, which soaks up all however the most violent rainfall. possess a spark arrestor included in to the crown or cap.|Some chimneys have a spark arrestor incorporated in to the cap or crown.}\nChoices of fireplace\nWooden Fireplace Mantel Masonry and prefabricated fireplaces could be fueled by timber, natural gas, gas sources.|propane and biomass fuel sources.} Ventless Fireplaces (duct free/room-venting fireplaces) are fueled by either gel, liquid propane, bottled gas or natural gas. In the United States, some states and local counties have laws restricting these types of fireplaces.|some continuing states and local counties have laws restricting these types of fireplaces.} correctly sized to the region to be heated.|They need to be sized to the region to be heated properly.} Additionally, there are air quality control issues because of the amount of moisture they launch into the space air, basic safety essentials.} Direct vent fireplaces are fueled by either liquid propane or natural gas. totally sealed from the region that is heated,|They are sealed from the region that's heated completely,} and vent all exhaust gasses to the surface of the structure.\nA wide variety of accessories are used with fireplaces, which range between countries, areas, and historical intervals. For the inside, common in recent Western cultures include grates, fireguards, log boxes, andirons, pellet baskets, and fire canines, all of which cradle fuel and accelerate burning.|which cradle accelerate and fuel burning.} A grate (or fire grate) is a framework, usually of iron bars, to retain fuel for a fire. Rock firebacks are sometimes used to fully capture and re-radiate heat,|Heavy metal firebacks are used to catch and re-radiate heat sometimes,} to protect the back of the fireplace,|to safeguard the relative back again of the fireplace,} and as decoration. Fenders are low steel frames set in entrance of the fireplace to contain embers, ash and soot. For fireplace tending, equipment include pokers, bellows, tongs, shovels, tool and brushes stands. Other wider add-ons can include log baskets, companion models, coal buckets, cabinet add-ons and more."", ""CO2 neutral heating with wood logs\nWood-burning stove with integrated water heat exchanger and very high efficiency (over 85%). 70% of the thermal energy is fed through the buffer storage system (min. 500l) into the heating cycle. Stored energy can be released from the buffer at any time and as needed. Only 30% of the nominal thermal output is radiated directly into the room. This means the stove can also be used in low-energy or passive houses.\nThermal discharge safety device, thermowell, air vent, fill and discharge valves are integrated into the stove.\nThis wood-burning stove has been granted a General Construction Authority Certification for room-air-independent operation by the DIBt (German Institute for Building Technology).\nFor room-air-independent operation, the combustion air supply must use a duct leading outdoors or be connected to an exhaust air flue.\nThis stove is ideally suited for installation in highly insulated buildings such as low-energy or passive houses in combination with a controlled home ventilation system.\nOf course, this wood-burning stove can also be operated using room air for combustion, which does not require air supply from outdoors.\nwodtke HiClean-Filter technology works without any complex additional components. With innovative filter materials and a well-thought-out air supply, emissions and fuel consumption are effectively reduced, while the stove's efficiency and heat exchange are increased significantly.\nThe depth filter made of ceramic foam and positioned above the combustion chamber retains the particles generated primarily during the initial phase of combustion. After the ignition temperature is exceeded, these are burnt, leaving the filter clear after each combustion phase. The filter also retains combustion gases for a longer period of time at a significantly higher temperature in the combustion chamber, leading to noticeably better combustion.\nModern wood-burning stoves need to be fast, creating thermal energy as quickly as possible and providing the cosy home atmosphere users expect. This requires a well-thought-out combustion chamber geometry and the right stove materials to ensure optimal heat yield and output. Intelligently located convection openings ensure energy is used efficiently and that radiant thermal energy reaches your entire living area.\nIn addition, many users prefer a system that stores heat, then emits it over a long period of time. Soapstone of different weights is a classic choice for heat storages of this type; soapstone can be used to cover large sections of the side of a stove, or as a small decorative panel.\nThe wodtke heat storage module is a new innovation.\nA massive storage block absorbs heat from the combustion chamber, then slowly emits it into the room as radiant heat. In this way, the stove ensures that you will enjoy the joy of heat and warmth for many hours.\nwodtke Air Control regulates the combustion air supply for an optimal combustion process. The system requires no valves or gates. This significantly increases operating comfort and eliminates user error. All users need to do is regularly refill the fuel supply.\nAir Control thermal regulation is based on a hydraulically activated control valve connected to a temperature sensor through a capillary system. Thanks to Air Control, the right quantity of combustion air is delivered to the right area of the combustion chamber. At the beginning, when the system is still cold, the fire needs a large quantity of primary air in order to quickly start a clean combustion process. This combustion air must be fed through the grate from below. As the system heats up, the thermal oil in the temperature sensor expands, slowly closing the control valve. This process automatically closes the primary air duct. Afterwards, the combustion air is fed downwards to the flame as secondary air and as an 'air curtain' for the window. This AWS air wash system effectively delays soiling of the viewing window.""]"	['<urn:uuid:62b3a2bc-157d-465f-aa61-0e37968489a7>', '<urn:uuid:7bd467d0-1474-4d04-b928-8cff7c0203bc>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T22:13:20.996306	10	35	1190
88	How do insurance limits and coverage differ between theft from a college dorm room and theft of valuable personal items like jewelry or artwork from a home?	For theft from a college dorm room, home insurance extends coverage to personal belongings, but only if the student is fulltime and a dependent. However, this doesn't apply to off-campus apartments. For valuable items like jewelry or art in a home, there are sub-limits within the personal property coverage - for example, jewelry might have a limit of $2,500 and art a limit of $5,000 within a $100,000 personal property limit. To get better coverage for valuable possessions, a scheduled personal property rider can be added to give each high-value item its own specific limit on the policy.	['If you have insurance, it is possible that it had some coverage you were not well aware of.\nTo get the most value out of your policy, always take not of the things that are already incorporated in the premiums you pay.\nThere is a 10% chance any person can get sued in a given year with 33% chance that any person will get sued in his lifetime. If you have home and/or auto insurance, should you get sued by an incident that your policy covers, the insurance company will provide you with legal defense. For example, if someone gets injured within your property and sues you, your home insurance company can provide you with a lawyer in your defense.\nShould the court find you negligent and thus award damages to the person suing you, then your insurance company will pay for the liabilities within the limits covered by your insurance policy. If there are certain assets which you may want to protect e.g. house, savings, or investments, then you should consider getting an umbrella policy. This type of coverage will provide you with liability coverage that extends beyond your home or car insurance.\nIf you have a very docile pooch that ends up biting a neighbor’s child and receive and injury that necessitates hospitalization and thus get sued by the event for reimbursement on medical bills, depending on the breed of dog you have, your insurance company, and your residential location, your home insurance may have you covered. There are certain breeds that home insurers will not insure. There are others on the other hand that will consider such issues on case-to-case basis. There are also those who will charge more on your policy if you have any of the listed biting breeds. Always check your home insurance policies to see if they have your particular pet covered.\nVehicular accidents are dangerous to both humans and pets alike. Since pets do not have the necessary seat restraints in cars, a simple and minor collision can result in their injury and potentially costly veterinary bills. Depending on that auto insurance policy you have, your insurer may cover the bill.\nIf any terrorist group bombs your neighborhood, your property is under the coverage of your home and car insurance policies. The standard home insurance policy provides damage to property as well as personal belongings that result from any act of terrorism. If you have comprehensive car insurance, your car insurer will cover the damages should your car get damaged or destroyed by the terrorist attack.\nInsurance covers terrorism but not acts of war. The government needs to declare if an attack is war or terrorism. This will enable your insurance company to act and respond accordingly. The insurance however do not cover biological and nuclear attacks.\nIf your kids have friends over at your home only to find out later that your gun is missing, if you file a theft report to the authorities, then your liabilityinsurancequotes.ca – professional liability insurance will provide coverage over the theft. Most home insurance policies cover firearm theft. If you have an extensive collection though, consider buying additional coverage.\nHosting a party at home for friends can be lots of fun. However, should one of them leave your premise drunk and be the cause of an accident, the blame may at times be pointed out to you. I many states, the host can be held responsible for the actions of the guests under these circumstances. If you have been filed with a civil claim, your home insurance will provide and cover your legal representation as well as any damages that the court may award – all these depends on the extent of your home insurance policy. Your liability extends to the ‘social host’ liability laws. This is why it pays to have sufficient liability coverage.\nLending your car to a friend is not uncommon. If he crashes it, your auto insurance policy will provide coverage whether your friend is insured or not. The auto insurance policy insures both the vehicle and you, along with any friends or relatives using the car provided it is used with your permission. Basically, your car insurance will pay for any damages caused to others. If you have collision insurance, this will cover the damages on your car. You will however need to pay any deductible with the collision claim.\nIf you have personal properties stolen from your home, you can file a police report and claim insurance. If you have purchased gifts and the receipts are stolen along with them, you will be required to document the purchases you have made, or possibly obtain duplicate receipts from the stores where you have made your purchase.\nIf your luggage from a trip does not make its way back to you, many home insurance policies will cover and reimburse lost or stolen luggage. This is basically ‘off-premise coverage’ in home insurance policies. Some insurance companies provide this automatically, whereas others charge additional fees for it.\nIf you have made a lot of purchases on your vacation that have gotten lost along with your luggage, these however will not be reimbursed. Generally, only up to $500 can be reimbursed in lost personal properties regarding luggage. If you are going to make an expensive purchase, be sure to insure this separately.\nShould a thief commits any robbery in your kid’s dorm, your home insurance policy will have this covered. A lot of home insurance extends coverage on theft to personal belongings at your child’s dorm. This coverage, however, does not extend to off-campus apartments being rented. You will need a renter’s insurance for this. Also, for the coverage to apply, your kid must be a student fulltime and is your dependent.\nThere are many things to consider going wrong with your car. If you hit a pothole on your way somewhere and get a flat, ran out of gas because your fuel gauge has malfunctioned, or you accidentally locked the keys in your car, if you have roadside assistance coverage with your auto insurance policy, then you got yourself covered for these. Progressive Emergency Roadside Assistance coverage provides you with the following assistance: towing, battery jumpstart, locksmith service, flat-tire changes, and emergency fuel delivery. This type of coverage is optional in auto insurance and is very affordable.\nIf your credit card gets stolen, receive a fake check, or receive fake money, you are covered by your home insurance. The policy for this however is very low with up to $500 only of coverage.\nIf your family grave or something similar have been desecrated by vandals and gotten damaged, depending on your insurance company, this type of issue may be covered. A lot of companies provide you with up to $5,000 over damages. This includes monuments, urns, and gravestones. Damages from catastrophes are however excluded from the coverage. No additional charges are needed for this coverage.\nMost home insurance has coverage on damages to trees, plants, shrubs, and lawns when the damages are a result of theft, vandalism, fire, lightning, riot, or an aircraft crash. There is a maximum $500 of claims you can make.\nIf food inside your refrigerator become spoiled due to power outage from any peril covered by your insurance policy, depending on your insurer and the area you live, this may be covered. However, the food spoilage brought about by power outage should be one that is a result of any covered peril like that of lightning, windstorm, hail, or fire. Some insurance companies have this automatically whereas others have it as additional.', 'While homeowners insurance policies will cover your home under many circumstances, these policies also often have limits. It is important to know your policy insurance limit to avoid unexpected out-of-pocket costs.\nWhat Is an Insurance Limit?\nA policy limit refers to the most an insurance company will pay for a specific type of coverage. This means each type of coverage on your policy, such as dwelling, personal liability, and personal property, will have its own limit. Any remaining expenses are your responsibility once you reach the policy’s limit. For example, if your homeowners insurance has a limit of $300,000 for dwelling coverage but the damage done to your home exceeds that amount, your insurance will only pay up to $300,000 and any remainder will be your responsibility to pay out of pocket.\nLimits vs. Deductibles and Premiums\nYou will often hear insurance terms like limit, premium, and deductible. They are all important components of your homeowners insurance policy. A limit is the maximum amount the insurer will pay for specific coverage, such as dwelling, personal property, or liability. If your policy has a $400,000 personal liability limit, then your insurance will only pay up to that amount for a personal liability claim if someone is injured on your property.\nA deductible is an amount that you, as the policyholder, will pay if there is a covered claim before the insurance limit steps in to provide coverage. For example, if your dwelling deductible is $1,000 and a tree falls on your roof and causes $4,000 of damage, you must pay $1,000 out of pocket before your insurer will cover the remaining $3,000.\nA premium is the recurring cost you pay to keep your insurance policy active. This is usually paid monthly, quarterly, biannually, or annually.\nHow Insurance Policy Limits Work for Home Insurance\nThe limits on your homeowners insurance policy clearly outline the maximum amount you will receive if there is a covered loss. There are specified limits for most coverages, including dwelling, personal property, and liability. The coverage amounts can be found on your homeowners declarations page within your policy.\nSetting Your Home Insurance Policy Limits\nWhen considering policy limits, factor in current market costs of building materials and consumer goods, the risk of where your home is located, and any minimum requirements that may be in a written contract or agreement with you. Some banks will require you to have a specific insurance-to-value amount, or if your home has a Home Owners Association (HOA), they could require higher liability limits than what is standard on a homeowners insurance policy. Bear in mind that the higher the limits are, the more premium you will pay.\nTypes of Homeowners Insurance Limits\nA homeowners insurance policy contains several insurance limits, and you should always read your policy in its entirety. Some coverage limits include:\n- Personal property: The items in your home that are not part of the structure of your home are considered personal property. This includes everything from your clothing to your silverware. Usually, this amount will be calculated as a percentage of the dwelling amount you select. However, it can be increased if you have a lot of personal property in your home. Keep an inventory that you can access easily just in case you need to file a claim.\n- Dwelling: Dwelling coverage is concerned with the actual structure of your home, like the walls and roof. The limit is determined by a combination of factors, including lender requirements and facts about the home, such as square footage and building materials. If you get to choose your limit, which is not always the case, be sure to check on the costs of materials, as that can change significantly.\n- Liability: Should someone get hurt while on your property or you damage someone else’s property, the liability limit will step in if it is a covered claim. The liability limit is typically around $100,000 to $500,000, depending on the insurance carrier. Consider a higher limit if you are in a high-traffic area for people on your property.\n- Loss of use: Loss of use coverage limits vary between insurance companies, but it is often a percentage of the dwelling amount. If your house is so damaged by a covered peril that you need to temporarily relocate while it is repaired, this coverage provides help. Some carriers will pay for hotels, apartments, and meals up to the policy limit or for a specified amount of time.\nInsurance Riders and Limits\nMany homeowners use riders or endorsements to provide extra coverages to their standard homeowners insurance policies. Just like all the other specified coverages, riders have maximum insurance limits as well. Some common homeowners insurance policy riders are:\n- Scheduled personal property: You may have some valuable possessions in your home, like art, jewelry, or guns. Most homeowners insurance policies put a sub-limit on these items, meaning within your personal property limit of $100,000, jewelry specifically may have a limit of $2,500, art may have a limit of $5,000, and so on. A scheduled personal property rider allows you to give each high-value item its own specific limit on the policy so you aren’t stuck paying the difference should there be a claim.\n- Water backup: Water damage from a drain that is backed up or sump pump overflow is typically excluded from a standard homeowners policy, but can be added with an endorsement. This would allow you to set a separate limit for this type of peril.\n- Identity theft: Since technology is crucial to our everyday lives, identity theft has become a real issue. You can add an identity theft rider to your policy to cover things like legal fees and other costs you incur due to the theft.\nWhen You Need More: Umbrella Policies\nMany homeowners insurance policies have a liability limit of $100,000 to $500,000, and if you feel that is not enough, you may want to consider an umbrella policy. You and your family may need personal umbrella insurance if you exceed the limits on your auto and homeowners policies. Without the right level of coverage, your assets, home equity, retirement savings, and more could be at risk. An umbrella policy covers personal injury liability, including slander, defamation, and libel; incidents abroad; rental liability that can come in handy when you are on vacation; and legal and defense fees.']	['<urn:uuid:de1ad592-2e21-495b-99f5-1d21819c0672>', '<urn:uuid:73173f69-6ba4-471b-aa4c-2e31f5c708f4>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T22:13:20.996306	27	98	2326
89	Can beans help control blood sugar spikes?	White kidney beans contain substances that help block carbohydrate absorption by limiting the effects of enzymes like amylase. However, you would need to take supplements to achieve a significant effect, as eating the foods alone wouldn't provide enough of these substances.	"['What Factors Slow the Absorption of Carbohydrates?\nWhen carbohydrates are quickly digested and absorbed, they can cause spikes in your blood sugar levels. While there are some misconceptions about the effects of eating high-fat or high-protein foods along with carbohydrates, eating carbohydrates with fiber or foods containing certain other components can help slow carbohydrate absorption and minimize its effects on your blood sugar levels.\nTypes of Carbohydrate\nSugars are the most quickly absorbed type of carbohydrate, with starches a close second since it doesn’t take very long for your body to break the bonds between the many sugar molecules that form the starch. In contrast, the third type of carbohydrate, fiber, can slow down the emptying of your stomach and the absorption of sugars and starches. This is one of the reasons why foods made with whole grains are healthier than those made with refined grains, since refined grains have been stripped of much of their fiber.\nFat and Carbohydrate Absorption\nEating foods containing fat along with those containing carbohydrate isn’t a good way to slow down carbohydrate digestion. Although fat also slows down the emptying of the stomach and the absorption of sugar into your blood, it doesn’t seem to affect the amount of insulin released after consuming carbohydrates, according to a study published in “Diabetologia” in 1984. Because of this effect, diets high in fat may increase insulin resistance, which is a precursor to diabetes. Another study, published in the ""European Journal of Clinical Nutrition"" in 2005, found a similar effect, with added fat increasing insulin levels while slowing gastric emptying and decreasing blood sugar.\nProtein’s Potential Effects\nAlthough diabetics are sometimes advised to eat protein-rich foods along with those containing carbohydrates, protein doesn’t actually slow the absorption of carbohydrates, according to an article published in “Diabetes Spectrum” in 2000. Different types of protein have differing effects on the absorption of carbohydrates, according to a study published in “Metabolism” in December 2012. While both fast-absorbing protein like that found in whey or soy and slower-absorbing protein like the casein in milk increased insulin levels when consumed with carbohydrates, they also decreased the effectiveness of the insulin, delaying the absorption of the sugars from the blood. Slow-absorbing protein had less of an adverse effect on insulin function, making it the better option.\nSome foods, including white kidney beans, contain substances that help block the absorption of carbohydrates by limiting the effects of the enzymes needed to break down these carbohydrates. One such enzyme is amylase, which is found in saliva and starts the carbohydrate-digestion process, according to an article published in the “Journal of the American College of Nutrition” in 2009. You most likely wouldn’t get enough of these substances to have an effect on carbohydrate absorption through eating foods alone, however; you would have to take supplements to achieve a significant effect.\n- MedlinePlus: Carbohydrates\n- TeensHealth: Carbohydrates and Diabetes\n- Diabetologia: Effect of Co-Ingestion of Fat on the Metabolic Responses to Slowly and Rapidly Absorbed Carbohydrates\n- Food and Agriculture Organization of the United Nations: Physiological Effects of Dietary Fiber\n- FAO: Digestion, Absorption and Energy Value of Carbohydrates\n- Metabolism: Combining Protein and Carbohydrate Increases Postprandial Insulin Levels but Does Not Improve Glucose Response in Patients With Type 2 Diabetes\n- Diabetes Spectrum: Protein Controversies in Diabetes\n- Journal of the American College of Nutrition: Bean Amylase Inhibitor and Other Carbohydrate Absorption Blockers: Effects on Diabesity and General Health\n- European Journal of Clinical Nutrition: The Effects of Fiber Enrichment of Pasta and Fat Content on Gastric Emptying, GLP-1, Glucose, and Insulin Responses to a Meal\nBased in Massachusetts, Jessica Bruso has been writing since 2008. She holds a master of science degree in food policy and applied nutrition and a bachelor of arts degree in international relations, both from Tufts University.']"	['<urn:uuid:1246dee9-40ec-43db-98ca-fd259bf6041d>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T22:13:20.996306	7	41	636
90	longest single zipline hawaii islands	The Flyin Hawaiian Zipline on Maui boasts the longest zipline in the state, with a single stretch measuring 3,200 feet.	"['Ziplining in Oahu, Hawaii\nPopularized in rain forest destinations such as Costa Rica, ziplining has taken hold in the Rainbow State as well. The activity is far more prevalent on Oahu\'s less urbanized neighbor islands, but one operator has established a zipline course on Hawaii\'s most populous isle. After trying a run on Oahu, check out the ziplining on the other islands.\nGo Zip Oahu at Bay View Mini-Putt and Zipline\nSomewhat different from most people\'s notion of ziplining, Bay View Mini-Putt and Zipline\'s (gozipoahu.com) course, operated by Go Zip, does not traverse a tropical forest canopy. Rather, it sends you hurtling at 25 mph over 400 feet of mini golf course. Located in Kaneohe, near Honolulu proper, Bay View Mini-Putt offers the first --- and as of publication, only --- zipline attraction on Oahu. As you soar across the mini golf course, which was rated one of the best in the nation by ""USA Today,"" you\'ll be treated to panoramic views of Kaneohe Bay and the Oahu coastline. All riders must weigh between 60 and 250 lbs.; no one under the age of 6 may participate in the zipline tour. Closed-toe shoes are required. The operator of Go Zip Oahu is one of the nation\'s leading builders of zipline and adventure courses. If you travel to another island during your time in Hawaii, you will be able to put your Oahu zipline experience to good use on other Go Zip-designed courses.\nGo Zip on Maui\nOahu-based Go Zip Hawaii, along with its sister company ERI, was instrumental in designing more than half of Maui\'s zipline operations. Piiholo Ranch (piiholozipline), for example, offers zipline canopy tours through the eucalyptus forests near Makawao. Dedicated to plant restoration in the West Maui Mountains, Flyin Hawaiian Zipline (flyinhawaiianzipline.com) boasts the longest in the state, with a single stretch measuring 3,200 feet. Go Zip/ERI also designed the country\'s largest zipline course, located at northwest Maui\'s Kapalua resort area and operated by Kapalua Adventures (kapalua.com). Several non-Go Zip courses also are available on Maui. The national chain Skyline Eco Adventures (zipline.com) runs ziplines on the slopes of Haleakala Volcano, as well as near the Ka\'anapali resort area just north of Lahaina. Skyline\'s Maui operation at Haleakala was the first zipline tour in the United States; now the company has ziplines on the Big Island and the mainland as well. Meanwhile, Maui Zipline Company (mauizipline.com) sends you soaring over the lush landscape of the Maui Tropical Plantation.\nGo Zip on the Big Island\nAmong the Big Island\'s four zipline tours, Go Zip/ERI designed and built The Umauma Experience (umaumaexperience.com) on Mauna Kea\'s slopes, which boasts two miles of ziplines that crisscross triple-tiered Umauma Falls. Other zipline tours include one operated by Skyline Eco Adventures (zipline.com), which soars over the 250-foot Akaka Falls area near Hilo. Big Island Eco Adventures (bigislandecoadventures.com) offers a zipline tour through rain forest, and over streams and waterfalls in the North Kohala Mountains. The tour leaves from the Waikoloa resort area and begins with an off-road drive to the jumping-off point. The World Botanical Gardens is home to Zip Isle Zip Line Adventures (zipisle.com) on the Hamakua Coast north of Hilo.\nGo Zip on Kauai\nA handful of operators offer zipline tours and adventures on the Garden Isle. Among these, Oahu-based Go Zip/ERI helped design and improve the course at Princeville Ranch Adventures (adventureskauai.com), which offers ziplining in combination with other activities such as swimming, hiking, paddling and horseback rides. Just Live! (justlive.org) is another Go Zip/ERI course that offers ecotours on Kauai. Other zipline operators on Kauai include Outfitters Kauai (outfitterskauai.com), which offers a few zipline adventures. One boasts an uninterrupted glide of 1,800 feet. Kauai Backcountry Adventures (kauaibackcountry.com) offers zipline tours over 17,000 acres of former plantation. Sweeping mountain and jungle vistas are your reward for stepping off the edge.\nLeaf Group is a USA TODAY content partner providing general travel information. Its content is produced independently of USA TODAY.']"	['<urn:uuid:4d1d11ff-e026-4da5-b39e-a4f773102e56>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T22:13:20.996306	5	20	660
91	How do market-penetration and segmentation strategies affect customer relationships?	Market-penetration pricing focuses on quickly winning a large customer base through low prices, as demonstrated by IKEA's approach. In contrast, customer segmentation focuses on maintaining relationships by understanding specific customer needs, evaluating factors like lead times, contract requirements, and price sensitivity. Segmentation allows for more nuanced relationship management by considering factors such as customer tenure, strategic partnerships, and growth potential, while market-penetration primarily relies on price appeal to maintain customer relationships.	['Whether it’s freight constraints, labor availability, supply shortages, inventory challenges, trade laws, or any other supply chain disruptions, navigating an ever-changing market can put businesses at risk of both losing customers and eroding profitability.\nTo stay profitable during supply chain disruptions, it’s more important than ever to ensure you understand your customer needs – and the costs to serve that customer, product, or service. One pricing strategy that can help improve profitability is customer segmentation – that is, the evaluation of unique aspects of your customers that place them in peer groups. By doing this, businesses can manage and optimize pricing actions, policies, programs, and other operational standards across similar-behaving customers.\nDetermine Optimal Customer Segmentation Relevant to Supply Chain Disruptions:\n- Are there certain geographical areas that are more complex, expensive, or slower to service?\n- Where are your highest-value customers located?\n- Do your freight policies put you in an advantage or disadvantage?\n- Does the cost to serve your customers vary by channel, market, or customer type?\n- Do certain customer rebate or incentive programs have thresholds that are no longer profitable?\n- Are smaller customers receiving better programs than larger customers because of the channel they are buying through?\n- Are certain end-markets seeing higher demand, greatly impacting your ability to stock and service at the same levels?\n- Have you seen a change in profit margin for historically stable customers?\n- Have you seen an uptick or decline in new customers due to changing market demand?\n- How has this impacted your wallet share? Your ability to quote new business?\n- Have you seen changes in lead time affect customer relationships or contracts?\n- Are certain customers willing to pay any price just to purchase your product or service?\nEach of these items can be tied back to a cost-to-serve element that affects profitability. For example, you may consider adjusting your freight policy for customers with lower margin that need product shipped to harder-to-get-to regions. While that may sound simple, and perhaps obvious, our real-world customers are usually in a more complex scenario. Perhaps existing contracts restrict certain price or program adjustments, or your large, multi-regional customer won’t accept regional-based price policies. It might be that your lowest margin customers are also the largest % of your revenue, or that your warehouse is full of product that those customers don’t care about because unpredictable shipments left you overstocked – leaving no room for the product they do care about. These scenarios illustrate why customer segmentation can help optimize pricing, especially when supply chain disruptions occur.\nIdentify Your Price Drivers (e.g. Customer Traits Key to Your Business Success):\n- High revenue\n- Strategic partners\n- Long tenure\n- Growth potential\n- Frequent buyers / high volume\n- High margin\n- Low price sensitivity or high difficulty to switch suppliers\n- Within a stable or growing industry\nBy identifying and prioritizing the ideal customer profile and behavior, you can begin to segment customers based on these price drivers. Ideally, these customer drivers are aligned to business objectives and also work in tandem with your product and order price drivers (or segmentations). Illustrated below are examples of key price drivers for optimal customer segmentation:\nThen, as shown in the table below, this customer segmentation provides clearer pricing guidance when determining how to optimize price changes when supply chain disruptions occur.\nIf you’re getting started with customer segmentation, make sure you have the right data that can be used to generate actionable knowledge. And if you already have your customer segmentation in place, improve your ROI with micro-segmentation of customer, product, and order factors that work together to generate the ideal target price.\nWhen supply chain disruptions hit, there is no one-size-fits-all answer. Knowing who your best customers are and how to take care of them is key to maintaining – and growing – relationships and ultimately improving profitability and driving impact to your bottom line.', 'Pricing strategies tend to change as a product goes through its product life cycle. One stage is particularly challenging: the introductory stage. This is called New Product Pricing. When companies bring out a new product, they face the challenge of setting prices for the very first time. Two new product pricing strategies are available: Price-Skimming and Market-Penetration Pricing. Let’s learn more about these two new product pricing strategies.\nPrice-Skimming – New Product Pricing\nThe first new product pricing strategies is called price-skimming. It is also referred to as market-skimming pricing. Price-skimming (or market-skimming) calls for setting a high price for a new product to skim maximum revenues layer by layer from those segments willing to pay the high price. This means that the company lowers the price stepwise to skim maximum profit from each segment. As a result of this new product pricing strategy, the company makes fewer but more profitable sales.\nMany companies inventing new products set high initial prices in order to skim revenues layer by layer from the market. An example for a company using this new product pricing strategy is Apple. When it introduced the first iPhone, its initial price was rather high for a phone. The phones were, consequently, only purchased by customers who really wanted the new gadget and could afford to pay a high price for it. After this segment had been skimmed for six months, Apple dropped the price considerably to attract new buyers. Within a year, prices were dropped again. This way, the company skimmed off the maximum amount of revenue from the various segments of the market.\nHowever, this new product pricing strategy does not work in all cases. Price-skimming makes sense only under certain conditions. The product’s quality and image must support the high initial price, and enough buyers must want the product at that price. Also, the costs of producing smaller must not be so high that they overshadow the advantage of charging more. And finally, competitors should not be in sight – if they are able to enter the market easily and undercut the high price, price-skimming does not work.\nMarket-Penetration Pricing – New Product Pricing\nThe opposite new product pricing strategy of price skimming is market-penetration pricing. Instead of setting a high initial price to skim off each segment, market-penetration pricing refers to setting a low price for a new product to penetrate the market quickly and deeply. Thereby, a large number of buyers and a large market share are won, but at the expense of profitability. The high sales volume leads to falling costs, which allows companies to cut their prices even further.\nMarket-penetration pricing is also applied by many companies. An example is the giant Swedish furniture retailer Ikea. By introducing products at very low prices, a large number of buyers is attracted, making Ikea the biggest furniture retailer worldwide. Although the low prices make each sale less profitable, the high volume results in lower costs and allows Ikea to maintain a healthy profit margin.\nIn order for this new product pricing strategy to work, several conditions must be met. The market must be highly price sensitive so that a low price generates more market growth and attracts a large number of buyers. Also, production and distribution costs must decrease as sales volume increases. In other words, economies of scale must be possible. And finally, the low price must ensure that competition is kept out of the market, and the company using penetration pricing must maintain its low-price position. Otherwise, the price advantage will only be of a temporary nature.']	['<urn:uuid:d88c3c52-23e7-44a5-bdac-21847a0d8aab>', '<urn:uuid:d5ee65ad-faa7-4dbe-b9ea-d449607f7e7c>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T22:13:20.996306	9	71	1252
92	compare depth of field factors camera lens telescope eyepiece field of view what affects viewing area	For camera lenses, depth of field (range where objects look sharp) varies with lens opening size, subject distance, and focal length - becoming greater as the lens opening decreases, subject distance increases, or focal length decreases. For telescope eyepieces, the field of view is measured as apparent field of view (AFOV) in degrees, ranging from narrow (25°-30°) to extremely wide angle (80°+), and the true field of view is calculated by dividing AFOV by magnification. Both systems are affected by focal length, but they measure different aspects of the viewing area.	"['A Guide on Choosing the Best Lenses for Telescope\nThe lenses or eyepieces allow you to focus the light collected by the telescope, so you can observe a sharp image of the object or area where the telescope is pointing. It may seem like a small link in the chain, but it has a big effect on your telescope’s optical system, and finding the right eyepieces will greatly increase its potential.\nWith so many options to choose from, getting the right set of eyepieces for you and your telescope can seem a bit tricky. This guide offers some insights and explanations about the different types of Lenses for Telescope, specifications and how it all connects to optimize your astronomy and astrophotography sessions!\nFocal Length and Magnification\nFocal length is an important specification that must be considered when determining the magnification, also known as power, of the eyepiece and telescope with which it is used. The following formula will help you determine the magnification based on the specifications of your eyepiece and telescope:\nMagnification = Focal length of the telescope (mm) / Focal length of the eyepiece (mm)\nThe 20 mm eyepiece of the 2000 mm telescope (2000/20) gives you 100 power (100x), which makes objects appear 100 times closer to you through the telescope than they look to your naked eye.\nField of view: Apparent and true\nThe apparent field of view of the eyepiece (AFOV) is expressed in degrees (°). This is how much of the sky can be seen from end to end only through the eyepiece. AFOV range from narrow (25 ° – 30 °) to extremely wide angle (80 ° or more).\nThe true field of view of the eyepiece is the angle of the sky that is visible through the eyepiece when attached to the telescope. The true field can be calculated using the following formula:\nTrue field = visible field / magnification\nSuppose, for example, that you have an 8-inch Schmidt-Cassegrain telescope with a focal length of 2000 mm and a 20 mm eyepiece with a field of view of 50 °. The magnification will be 2000 mm / 20 mm = 100x. The true field would be 50 \\ 100 or 0.5 ° – approximately the same visible diameter as the full moon.\nLong Eye Relief\nEye Relief refers to the distance between your eye and the eyepiece lens when the image is in focus. The relief of the eyes is traditionally proportional to the focal length: The shorter the focal length, the shorter the relief of the eyes. However, some of the more modern eyepiece designs provide long-eye relief, regardless of the focal length, which is especially useful for those who wear glasses. If you want to keep your glasses while using a telescope, the relief of the eyepiece is an important feature that must be taken into account (we recommend looking at the eyepieces for long-eyed relief).\nHow many Lenses for Telescope do I really need?\nAlthough there is no specific number of Lenses for Telescopes to own, with a few different telescope eyepieces you have a better chance of reaching the optimal power for the particular object you are observing, given the sky conditions at the time. You will usually want to start with low power (i.e., a long focal length of the eyepiece, for example 25 mm or 30 mm) to get the object in the field of view of the telescope. Then you can try an eyepiece with a little more power (shorter focal length, maybe 18 mm or 15 mm) and see if the view looks better. If this happens, swap the eyepiece for even more power, etc., until you reach this “sweet spot” where the brightness of the image, the scale of the image, and the amount of visible detail combine to form the most pleasing view.\nWhat about Barlow lenses?\nYou can also choose a long focal length eyepiece with comfortable eye relief and use image amplifiers to increase power, such as a Barlow lens. Barlow increases the effective focal length of the lens, increasing the magnification. The idea is that two eyepieces and a Barlow will give you the flexibility to zoom in on four eyepieces and will give higher magnifications with less powerful eyepieces.\nThe use of different eyepieces can significantly increase the flexibility and functionality of any telescope.\nWhat is a Zoom Eyepiece?\nThe zoom eyepieces are a single eyepiece that you can adjust to change its focal length, which changes the magnification. You just insert the eyepiece into your telescope and simply rotate the zoom eyepiece barrel to zoom in or out of whatever you are looking at.\nThey tend to occupy the middle level of focal length / magnification, so you may need very high and low magnification eyepieces (short and long focal lengths, respectively), but a zoom one can handle everything between you.\nTelescope Eyepiece Sets\nLenses for Telescope Sets are a great idea for anyone new to backyard astronomy. For one price, they provide a selection of eyepieces and filters to improve the versatility of your telescope setup.\nThe Eyepiece Set offers package of various eyepieces, conveniently providing a full array of different magnifications and powers in one place. This lets you find numerous eyepieces that are all compatible with your telescope.. The big three astronomy manufacturers, Celestron, Orion and Meade, produce their own eyepiece kits and we have selected the best one here in this article.\nBasic Tips to Follow When Shopping for Eyepieces\n- Consider the focal length of your telescope, or telescopes, to make sure the eyepiece will provide an appropriate magnification to suit your needs.\n- If you wear eyeglasses while using a telescope, pay attention to the eye relief specification of different eyepieces, as ample eye relief can improve comfort and ease-of-use while wearing corrective lenses.\n- Depending on your observing goals, consider the apparent field of view of your eyepiece choices.\n- If versatility is paramount, consider a zoom eyepiece or Barlow lens to increase the number of possible magnifications to use.', 'KODAK CUSTOMER SERVICE PAMPHLET - AA-3\nSome Questions and Answers about Camera Lenses\nYour camera lens functions in much the same way as the lens of the human eye.\nBasically, the lens collects light rays reflected from a subject and, unlike a\npinhole, focuses these rays to form a sharp image.\nWhat makes a lens work?\nA camera lens can be made from a piece of glass or plastic which has two\nopposite regular surfaces, either both curved or one curved and the other flat.\nMost camera lenses are actually made up of a number of lens ""elements"" which\nfunction together and are simply referred to as a ""lens"".\nAs the light rays that are reflected from the subject pass through the camera\nlens, they are bent. The extent to which they bend is controlled by the\ncomposition of the lens and the curves of the lens surface. In a properly\ndesigned lens, all the light rays from the same part of a subject will meet at\na point behind the lens. A sharp image of the subject is formed at the point\nwhere these light rays meet. The film must be located at this point for the\npicture to be in sharp focus.\nWhat is the focal length of a lens and what does it do?\nIn very simple terms, focal length is the distance between the optical center\nof the lens and the film when the lens is focused at infinity. The focal length\nof the lens of most adjustable cameras is marked on the lens mount. The focal\nlength is usually given in millimetres or inches.\nThere is a direct relationship between the focal length of a lens and the image\nsize of the subject record ed on film: the longer the focal length, the larger\nthe image on the film. For example, if Lens A has a focal length of 50 mm, and\nLens B has a focal length of 150 mm. The subject is the same size and the same\ndistance from the lens in both situations.\nIn this case, the image produced by Lens B, which has a focal length of 150 mm,\nis three times as large as the image produced by Lens A, which has a focal\nlength of 50 mm. A lens of long focal length produces a larger image than one\nof short focal length.\nWhat are lens openings, and how are they determined?\nWhen we talk about a lens opening, we mean the opening (usually called\naperture) that lets the light through the lens to expose the film. The sizes of\nlens openings are usu- ally expressed in terms of f-numbers, for example, 1/2.8\nand f/4. The f-numbers are determined simply by dividing the focal length of\nthe lens by the diameter of the aperture.\nFor example, if the focal length of the lens is 100 mm and the aperture is 25\nmm, the f-number is f/4 (100/25 = 4). So when you have a lens opening of 1/4,\nyou know that the aperture is only 1/4 of the focal length. Similarly, f/8\nmeans that the aperture is 1/8 of the focal length, f/11 means the aperture is\n1/11 th of the focal length, etc. When you understand that f-numbers indicate\nthe size of the aperture as a fraction of the focal length, it\'s easier to\nunderstand that the smaller the f-number, the larger the lens opening.\nSome typical f-numbers used in expressing a series of lens openings, from large\nto small, are f/2, f/2.8, f/4, f/5.6, f/li, f/16, and f/22.\nWhat does relative aperture mean?\nLet\'s look at two lenses, each with a 9 mm aperture and focused on the same\nperson. Both lenses transmit the same amount of light, but Lens A has a focal\nlength of 50 mm and Lens B has a focal length of 150 mm. With the 150 mm lens,\nthe image of the subject produced on the film will be three times as large as\nthe image produced with the 50 mm lens. While both lenses transmit the same\namount of light reflected from the subject, the light is spread over an area\nnine times as large with the 150 mm lens. For this reason the image made on the\nfilm by the 150 mm lens is less bright. So while both lenses have a physical\naperture of 9 mm, the longer-focal-length lens has a smaller relative aperture.\nIt would be difficult to take properly exposed pictures if, for example, f/8 on\na long-focal-length lens and f/8 on a short focal length lens didn\'t mean the\nsame thing from an exposure point of view. Fortunately, they do but for this to\nbe true, the f/8 on the long focal length lens must be a physically larger\nopening than the opening of f/8 on the shorter focal length lens. Both lenses\nhave the same _relative_ aperture but their _physical_ apertures are of\nWhy must I use a larger-than-normal lens opening when I use a lens-extension\nFor making extreme close-up pictures with some advanced cameras, you\ncan use an extension tube or bellows to extend the camera lens. These devices\nallow you to get close to your subject and still get a sharp picture.\nHowever, since the f-numbers on the camera are based on a normal lens-to-film\ndistance, the marked f-numbers are not a true indication of the image\nbrightness reaching the film when the lens-to-film distance is increased by a\nlens- extension device. Since the image on the film is less bright when you use\na lens-extension device, you should make an exposure compensation by using a\nlarger lens opening or slower shutter speed. If your camera has a built-in\nmeter it will automatically indicate the correct exposure settings, no\nadditional compensation is necessary. If you\'re using a 35 mm camera without a\nbuilt-in meter, you can use the following table as a guide to determine the\namount of exposure increase necessary with an extended lens.\nExposure Increase for Extended Lens 35 mm Cameras\nWidth of Subject Area Open Lens by Or Multiply Exposure\n(inches) (f-stops) Time by\n11 1/3 1.3\n5 1/8 2/3 1.6\n3 1/4 1 2\n2 1/4 1 1/3 2.5\n2 1 1/2 2.8\n1 1/4 1 2/3 3.2\n1 1/8 2 4\n1 2 1/2 5.7\n3/4 3 8\nFor cameras with built-in meters, use the exposure recommended by the meter.\nWhat is depth of field?\nThe distance range within which objects in a picture look sharp is called depth\nFrom a practical point of view. depth of field varies with the size of the lens\nopening, the distance of the subject focused upon, and the focal length of the\nlens. Depth of field becomes greater as\n1. The size of the lens opening decreases.\n2. The subject distance increases.\n3. The focal length of the lens decreases (and subject distance remains\nWhat is an aspheric lens?\nThe surface of the majority of lenses made today is a segment of a sphere. An\naspheric lens, on the other hand, has a curved surface which is not part of a\nsphere. Aspheric lenses are more difficult to manufacture than normal lenses\nand are not as common.\nAn aspheric lens will correct various lens aberrations that would ordinarily\nrequire several more lens elements to correct. In addition to having fewer\nelements, a lens with an aspheric element is lighter and usually more expensive\nthan a normal lens.\nWhat Is a color-corrected lens?\nA color-corrected lens is one that brings light rays of different colors\nreflected from the same part of the subject into focus at the same point behind\nthe lens. A short history lesson will help you understand why color-corrected\nlenses are important.\nAt one time, only black-and-white film was available. At first, black-\nand-white film was sensitive only to blue light. Later it was orthochromatic,\nthat is, sensitive to both blue and green light. With many of the lenses used\nat that time, light rays of different colors that came from the same part of\nthe subject did not come to focus at the same point behind the lens. But as\nlong as the film could see only blue and green light, it didnÕt make too much\ndiffeence when red light rays didnÕt come to focus at the same point as the\nblue and the green light rays.\nHowever, orthochromatic films weren\'t ideal. For example, since these films\ncouldn\'t see red light, red objects (like lips) registered as black on the\nprints. For this reason, panchromatic films were developed. Because\npanchromatic films are sensitive to all colors, they are capable of rendering\ncolors in proper degrees of black and gray. Now that the new films could see\nred and other light rays, it was necessary to make a lens that would bring all\nthe light rays into focus at the same point.\nDifferent lens formulas and different types of glass were developed to provide\nproper, sharp register of all the light rays. These color-corrected lenses are\nessential for color photography but were needed long before there was any such\nthing as color film.\nWhat is a coated lens?\nA coated lens is one coated with a thin layer of special material that reduces\nlight reflections from the air-glass surfaces of the lens. Most photographic\nlenses available today are coated.\nLetÕs see what happens when the lens is uncoated. When light strikes any\nair-glass surface of an uncoated lens, a small percentage of that light will\nreflect back from the surface and not go through the lens. In a\nmultiple-element lens, each air-glass surface will reflect some of the\nimage-forming light that should reach the film. Most of this wasted light will\njust be reflected from the surface back out through the front of the lens and\nwill be lost. However, some of these light rays may be reflected a second time\nfrom the surface element of the lens so that they do reach the film. Because of\nthe several angles of reflection, such light rays will not reach the film at\nthe point where they should and will degrade the quality of the image.\nSince the lens coating reduces the amount of light that is reflected from an\nair-glass surface, it contributes to a clearer, crisper image and makes the\nlens more efficient by reducing light loss.\nIf you look at a coated lens from an angle, the surface will appear colored.\nHowever, the coating on a lens does not affect the color of pictures taken\nthrough the lens. If you look through the lens, the coating is colorless.\nAPPROXIMATE POSITION OF SUBJECT AND\nm = magnification\nF = focal length\nF = f-number\nx = distance of image from focal point or distance that\nlens is extended from infinity setting\nu = subject distance\nv = image distance\nh = height of subject\nhÕ= height of imag&\nAll dimensions must be expressed in the same unit of measure.\nTo convert dimension in divide by\nmillimetres to metres 1000\ncentimetres to metres 100\ninches to metres 39.4\nfeet to metres 3.28\nmillimetres to inches 25.4\nMeasuring u and v from a point midway between the front element and the rear\nelement of the lens is accurate enough for practical use with a normal lens\n(not telephoto or wide-angle). The formulas that do not include v are valid for\ntelephoto lenses and wide-angle lenses when u is large enough so that any\ninaccuracy in measuring u from the center of the lens is insignificant.\nThe fundamental relationship between focal length, image distance, and subject\n1 1 1\n- = - + -\nF v u\nFormulas that are more directly useful and some examples follow:\nhÕ v v-f f\nM = - = - = --- = ---\nH u F u-F\nv = ----- = mu = (m+1)F\nu + v = --------------- F\nFv v | 1 |\nu = ---- = - = | - + 1 | F\nv-F m | m |\nExample 1: How long must a room be for you to photograph groups 10 feet wide\nwhen you use a lens with a focal length of 8 inches on a 4 x 5 inch camera?\nSolution: Allow 41/2 inches for image on horizontal axis of negative. Work in\ninches, so 10 feet = 120 inches.\nThen m = ----- - .038\n| 1 |\nand u = | --- + 1 | F =\n(26.3 + 1) F = 27.3 x 8 = 218 inches = 18+ feet\nThis answer gives the lens-to-subject distance. You will also need to add at\nleast 7 feet to allow space for the camera, photographer. background\nseparation, etc. The minimum room length is therefore 25 feet. The room width\nmust be at least 15 feet in order to accommodate the group and lights.\nF = -------- = ---------\n| 1 | m + 1\n| - + 1 |\n| m |\nExample 2: For a room 20 x 32 feet and a 21/4 X 21/4 inch camera, what is the\nlongest focal-length lens feasible for photographing a scene 10 feet wide?\nSolution: Since you need about 7 feet of room length for working space, the\nmaximum lens-to-subject distance available is 25 feet (32-7) or 300 inches; u =\n300. You should allow at least 1/8 inch of space on either side of the\nnegative. The usable width of the negative is then 2 inches. Since the width of\nthe subject is 10 feet (120 inches), the magnification (m) equals 2 divided by\n120, or .017 The formula now reads:\nF = ----------- = -------- =\n1/.017 + 1 59 + 1\n--- = 5\nAnswer: 5 inches (127 mm) is the maximum usable focal length.\nLens Movement from Infinity Position:\nx = ---------\nu - F\nField Size (front-element focusing lenses):\nField width = negative width x ---\nEffective f-Number for Lens Extension:\nThe effective f-number is greater than the indicated f-number because of the\nincreased image distance (lens-to-film distance). When the subject distance u\nis less than 8 times the focal length of the camera lens, use one of the\nfollowing formulas to determine the required exposure compensation. The\nformulas are valid for any subject distance.\nv x f\nEffective f-number = ------- = f (m + 1)\nWhere v = lens-to-film distance or focal length plus lens extension from\ninfinity focus, f = f-number indicated on lens-opening scale and F = focal\nlength. For close-up pictures with lens extension, use the effective f-number\nobtained from the first formula when determining your exposure, or compensate\nyour exposure time directly by using the second formula.\nNote: Exposure compensation is made automatically with some cameras through\nthe lens exposure meters. Correction may or may not be necessary with flash.\nFixed Circle of Confusion\nCamera Most Widely Used (in inches)\n8 mm movie .0005\nSuper 8 movie .00065\n16 mm movie .001\n110 (13 X 17 mm) .0012\n126 (28 x 28 mm) .002\n135 (24 X 36 mm) .002\nRoll film .005\n4 x 5-inch and F/1720 critical use\nlarger or F/10OO liberal use\nDEPTH OF FIELD\nDepth-of-field computations are made on the basis of a fixed circle of\nconfusion or on a circle of confusion equal to a fraction of the focal length.\nLenses of different focal lengths used at the same f-number have the same depth\nof field for equal image sizes. As a general rule, one-third of the depth of\nfield is in front of the subject and two-thirds is behind the subject. An\nexception to this rule is extreme close-up lenses, including those made with\nclose-up lenses, where depth of field is about equal on both sides of the\nMethod A, Fixed Circle of Confusion:\nF = focal length of lens\nf = f-number setting\nH = hyperfocal distance\nu = distance for which camera is focused\nd = diameter of circle of confusion\nH x u\nNear limit of depth of field (measured from camera lens) = ----------\nH + (u-F)\nH x u\nFar limit of depth of field (measured from camera lens) = ----------\nH - (u-F)\nHyperfocal Distance (near limit of depth of field when lens is set at infinity):\nf x d\nMethod B, Circle of Confusion a Fraction of the Focal Length of the Lens:\nu = distance focused upon.\n0 = angular size of circle of confusion. For critical definition, 0 is 2 minutes\nof arc and the linear size of the circle of confusion is approximately F/1720\nL = effective diameter of lens = ---\nLDF = limit of depth of field\nu squared tan 0\nNear LDF: (measured from plane focused on) = -----------------\nL + u tan 0\nu squared tan 0\nFar LDF: (measured from plane focused on) = -----------------\nL - u tan 0\nKODAK CUSTOMER SERVICE PAMPHLET - AA-3']"	['<urn:uuid:034db3e6-c967-438a-9fed-b16374d16283>', '<urn:uuid:d4fdf461-bc89-4830-972c-c857bf84e2b5>']	factoid	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-12T22:13:20.996306	16	91	3820
93	What's the main difference between the duration of a Batobus boat tour in Paris and a Hobie Cat sailing experience in Mont Choisy when it comes to time spent on the water?	A Batobus boat tour in Paris takes 1 hour and 36 minutes for the full circuit, while a standard Hobie Cat sailing experience in Mont Choisy lasts 60 minutes. However, the Hobie Cat sailing can be extended to 120 minutes for an additional fee.	['Fancy exploring Paris in a far more interesting way than by metro or by bus? Then why not try the boat! This article will tell you more about this unusual mode of transport around Paris. Are you dreaming of standing up on the deck shouting “I’m the king of the world!” with your hair blowing in the wind and the stunning city of Paris in the background ? Or do you simply just want to see another side of Paris ? Well, this is the article for you! Here are two Parisian boats that will take you along the river Seine.\nBatobus in Paris\nWhat is the Batobus?\nThe Batobus is definitely an out-of-the-ordinary mode of transport! This is a boat that takes you all along the Seine to see the main sites of Paris.\nHow does it work?\nThe idea is that it works just like a bus : you get on and off wherever you like. Each station is next to a museum or a Parisian monument.\nYou can catch this river shuttle at 8 different stations: the Eiffel Tower, the Musée D’Orsay, Saint-Germain des Prés, Notre Dame, the Jardin Des Plantes, Hôtel de Ville, the Louvre and the Champs-Elysées. But make sure you know which way you’re going!\nThe Batobus runs every 25 minutes from 10am to 7pm in the winter and every 20 minutes from 10am to 9:30pm in the summer. The full circuit takes 1 hour and 36 minutes.\nYou can purchase tickets online, at the airport (Paris Charles de Gaulle 1 and 2), or at each Batobus stop. There are prices for 1 day, 2 days, one year and for a special treasure hunt for children. All of the Batobus prices can be found here..\nWhat are Bateaux Mouches?\nBateaux Mouches are the go-to tourist boats in Paris and are very popular! Carrying passengers for over 60 years, these glass boats let you discover the beauty of Paris from the Seine, both by day and night. The Bateaux Mouches also offer wonderful lunch and dinner cruises, with world-class food, for a romantic rendez-vous on the Seine.\nHow does it work?\nThe cruises last approximately 1 hour 10 minutes and enable you to see about twenty of the city’s most prestigious landmarks.\nBoats leave from the Pont de l’Alma, on the right bank, in the 8th arrondissement. They pass the Concorde bridge, the Musée du Louvre, the Hôtel de Ville, the Conciergerie, the Ile de la Cité, Notre Dame Cathedral, the Ile Saint Louis, the Pont Neuf, the Musée d’Orsay, Invalides, the Eiffel Tour and finish at the Palais de Chaillot before returning to their starting point.\nTo choose your departure time, we recommend that you check the travel information here. Boats usually depart every 30 to 40 minutes.\nPrices vary from 5.5 euros (kids) to 13.5 euros (adults) for a simple cruise. A meal on-board one of the Bateaux Mouches can cost anything from 55 to 263 euros (see here).\nYou can buy your Bateaux Mouches tickets online, by phone or at the departure counter on the Pont de l’Alma.', 'Hobie Cat Sailing Experience\n|Operating Days||All Days|\n|Transportation||Offered as Supplement|\n|Comment||Activity Time: 10h00 – 12h30 OR 13h30 – 17h00. 3 participants max per hobie cat. Knowledge of swimming is compulsory! Children should be accompanied by adult.|\nHobie Cat Sailing at Mont Choisy Mauritius (with Instructor)\nEnjoy an amazing experience with our Hobie Cat Sailing activity!\nIf you love thrilling sensation, speed ride and the touch of the wind on your skin, this activity will fit you perfectly.\nHobie Cat is, above all, a story of pleasure, of FUN, of fantasy, and of FEELING. This is why Hobie Cat sailing was chose as one of the best ways to experience Mauritius Island. With the Hobie Cat sailing, you will feel the real meaning of FUN!\nSail through the turquoise sea water, and let the gust of the wind carry you over the most beautiful lagoons of our Island. It will be your ideal escape to paradise as it is one of the best place to enjoy Hobie Cat sailing all year round with perfect winds and idea sea condition.\nThis great activity brings you to Mont Choisy one of the most beautiful beach found in the northern coast of Mauritius. Stretching almost 3 kilometers from Trou aux Biches to Pointe aux Canonniers, this is the island’s longest beach: a gently curving strand of golden sand lined with a shady grove of Filao trees on one side and by gentle, crystal-clear blue water on the other.\nSo, join us for an amazing Hobie Cat sailing experience with a professional and experienced instructor by your side. Try Hobie Cat sailing for a non-motorized activity. Enjoy the peaceful surround of the beach from the water while sailing offshore from the beach, we never forget the pleasure of sailing!\nPlan of your Hobie Cat Sailing activity day:\n- Arrival at the departure point situated at Coral Azur hotel (North of Mauritius) where you will meet the team of professional instructors.\n- Receive safety briefing instructions of about 5 – 10 min on land from the professional instructor.\n- 60 minutes of sailing on-board the Hobie Cat (possible to also extend the sailing time for a supplement)\n- Sail on the Hobie cat with our professional instructor, roughing around 3 sq. miles of water\n- Return to the departure point and relax on the beach\nAbout the Hobie Cat Sailing Activity\nJoin our professional team on the beach of Mont Choisy for an unforgettable Hobie Cat ride. Your activity starts with some safety instructions and a short briefing by the experienced instructor.\nAs part of this activity, a professional and expert instructor will accompany you during the whole session. The Hobie Cat sailing with instructor is mainly for beginners who want to experience and feel the thrill of this amazing activity, no need to be a sailor to ride this marvelous catamaran enjoying the stunning view of the coastline of Mont Choisy.\nIf ever you want to sail the Hobie Cat on your own, you may ask the instructor during the activity. This catamaran is very easy to sail and allow you to have fun even in light wind.\nThe protected inner bay has light cross-shore breezes and is perfect for learning, while the stronger winds outside the reef are ideal for blasting moments in controlled sessions out at sea.\nDuring the Hobie Cat sailing the instructor will show you unbelievable areas, close to the reef.\nThe ones who look after sliding sensations will have unforgettable moments in such an idyllic place.\nFeel the wind on your face, and the adrenaline rush while hitting each little waves.\nWe care for our customer! – Safety first:\nThe activity package starts with a full briefing on safety instructions and several useful tips on your Hobie Cats sailing. Two safety boats and a dedicated rescue team are always on standby. Only the best quality and advanced equipment is offered including also life vests and trapeze harnesses if necessary – all for your maximum safety & comfort.\nAlso, you will be provided with life jacket which is compulsory for this activity, for security reason.\n- Operating Days: All days\n- Operating time: (Anytime during the following schedule)\n- In the morning: 10:00 – 12:30\n- In the Afternoon: 13:30 – 17:00\n- Departure point: From Coral Azur Hotel, Mont Choisy\n- Duration: 60 minutes\n- Minimum age: 6 years and above. However, children between 6-10 years are recommended to be accompanied by an adult.\n- Maximum number of people: 3 participants at a time or 4 children\n- What to bring: Swimwear, suitable shoes, towels, water, suncream, caps\n- Price for the Hobie Cat Sailing, for up to 2 persons for 60 minutes: 49 Euro\n- Price for the Hobie Cat Sailing, for up to 2 persons for Double duration of 120 minutes: 95 Euro']	['<urn:uuid:5237c772-893a-4433-a3a1-6a4f209dbea4>', '<urn:uuid:17d88c82-ae6f-4e38-92e0-94fb1a6c47b5>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T22:13:20.996306	32	44	1321
94	I'm curious about why snow is so important for the western United States. How does snowfall affect the environment and communities in this region?	Snow plays a crucial role in shaping both ecosystems and economies of western North America's semi-arid landscapes. It generates the streamflow that allows agriculture and cities to flourish. Snowfall patterns impact multiple aspects of the environment, including: the onset of spring, the amount and timing of streamflow runoff, the probability of forest fires, the size and distribution of glaciers, and the elevation and position of alpine treeline and meadows.	"[""The Unusual Nature of Recent Snowpack Declines in the North American\nCitation: Pederson, G. T., S. T. Gray, C. A. Woodhouse, J. L. Betancourt, D. B. Fagre, J. S. Littell, E. Watson, B. H. Luckman, and L. J. Graumlich, 2011. The Unusual Nature of Recent Snowpack Declines in the North American Cordillera. Science, 9 June 2011: 1201570Published online 9 June 2011 [DOI:10.1126/science.1201570]\nl Abstract l Reprint l\nSnow shapes the ecosystems and economies of the semi-arid landscapes of western North America by generating the streamflow that allows agriculture and cities to flourish. The patterns of snowfall across space and through time impact aquatic and terrestrial ecosystems by governing processes such as: 1) the onset of spring, 2) the amount, timing, and temperature of streamflow runoff, 3) probability forest fires, 4) size and distribution of glaciers, and 5) the elevation and position of alpine treeline and meadows. We're understandably concerned when scientists report that snowpack in the West has been steadily declining since the 1950s and obvious questions that come to mind include: 1) Is this normal and part of some long-term pattern of waxing and waning conditions?, or 2) Does this reflect recent anomalous trends towards declining snow the West? To answer these questions, we need records that encompass multiple centuries thereby allowing us to assess trends and cycles in snowpack.\nOur project seeks to address this gap using tree-rings to reconstruct snowpack over several centuries for three key high-mountain watersheds: the Upper Colorado River basin, the Upper Missouri River basin, and the headwaters of the Columbia River. Snowpack reconstructions were produced at multiple watershed scales (Click here for map), and the resulting high-resolution maps of past snowpack provide insights into natural controls on snowpack variability linked to changes in the Pacific Ocean basin (e.g., El Niño, Pacific Decadal Oscillation). Additionally, both data and mapping tools have been developed as planning and research resources for water managers and research scientists. This work is particularly relevant because the three target regions form the headwaters for the West's three most important watersheds.\nIn order to calibrate tree-ring data to snowpack data, we have assembled large spatial databases using recently collected and existing tree-ring chronologies and records of April 1st snow water equivalent (SWE) in the headwaters of our three target watersheds. The final reconstructions have been completed, and results indicate that the tree-ring based reconstructions skillfully capture decade-scale and longer-term variability in snowpack at multiple watershed scales.\nIn western North America snowpack has declined in recent decades, and further losses are projected through the 21st century. Here we evaluate the uniqueness of recent declines using snowpack reconstructions from 66 tree-ring chronologies in key runoff generating areas of the Colorado, Columbia and Missouri River drainages. Over the past millennium, late-20th century snowpack reductions are almost unprecedented in magnitude across the northern Rocky mountains, and in their north-south synchrony across the cordillera. Both the snowpack declines and their synchrony result from unparalleled springtime warming due to positive reinforcement of the anthropogenic warming by decadal variability. The increasing role of warming on large-scale snowpack variability and trends foreshadows fundamental impacts on streamflow and water supplies across the western USA.\n- Data Tables and Resources\n- Figures and Mapping\n- Data Animations:\n- Image Gallery\nDOI News Release\nProject co-authors and key data contributors:\nGreg Pederson, U.S. Geological Survey | NOROCK and University of Arizona\nStephen Gray, University of Wyoming\nConnie Woodhouse, Geography Department and Laboratory of Tree Ring Research, University of Arizona\nLisa Graumlich, School of Natural Resources, University of Arizona\nDaniel Fagre, U.S. Geological Survey, Glacier Field Station\nJulio Betancourt, U.S. Geological Survey, National Research Program\nJeremy Littell, Climate Impacts Group, University of Washington\nBrian Luckman, University of Western Ontario\nEmma Watson, Environment Canada\nDave Meko, University of Arizona, Laboratory of Tree-Ring Research\nTroy Knight, St. John’s University, Minnesota\nFunding Support from:\nNational Science Foundation (NSF) | Geography and Regional Science – Grant# 0620793\nNational Science Foundation (NSF) | Ecology – Grant# 0734277\nU.S. Geological Survey (USGS) | Western Mountain Initiative""]"	['<urn:uuid:5cdb20b6-098d-4d7f-bc50-86c1f1ce2b16>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T22:13:20.996306	24	69	667
95	What is the expected effective dosing regimen of adalimumab for treating ankylosing spondylitis and psoriatic arthritis according to published data?	According to published data and the approved product label, adalimumab 40 mg every 14 days should be effective for treating ankylosing spondylitis and psoriatic arthritis.	['Track topics on Twitter Track topics that are important to you\nIn Spain, there is little data available concerning the evolution and treatment of Psoriatic Arthritis by rheumatologists and dermatologists. This study is designed to obtain more data about the clinical and therapeutic management of Psoriatic Arthritis in Dermatology and Rheumatology Medical centers.\nObservational Model: Case-Only, Time Perspective: Retrospective\nPublished on BioPortfolio: 2014-08-27T03:34:15-0400\nThis study will be an open label pilot study to explore the utility of thermography in assessing response to Golimumab treatment in Psoriatic Arthritis (PsA). Ten patients fulfilling the C...\nhOKT3gamma1 (Ala-Ala) is a man-made antibody that is commonly used to prevent organ rejection. The purpose of this study is to determine whether hOKT3gamma1 (Ala-Ala) is safe and effective...\nBased on published data and according to the approved product label for ankylosing spondylitis and psoriatic arthritis, it can be expected that adalimumab 40 mg every 14 days should be eff...\nThe purpose of this study is to determine an optimal abatacept dosing regimen for the treatment of patients with active arthritis due to psoriatic arthritis who have had a prior inadequate...\nThe purpose of this study is to conduct full psychometric testing of the Early Arthritis for Psoriatic Patients (EARP) questionnaire in Australian, Korean and Chinese populations\nThe objective of the present study was to evaluate the sensitivity and specificity of the Toronto Psoriatic Arthritis Screen II (ToPAS II), Psoriatic Arthritis Screening and Evaluation (PASE), Psorias...\nThe objective was to explore the link between a patient acceptable symptom state (PASS) and patient-perceived impact in rheumatoid arthritis (RA) and psoriatic arthritis (PsA).\nTo compare the screening performance of the Psoriasis Epidemiology Screening Tool (PEST), Psoriatic Arthritis Screening and Evaluation (PASE) and Early Arthritis for Psoriatic Patients (EARP) question...\nTo evaluate the current needs for joint surgery in patients with psoriatic arthritis (PsA).\nThe objective of the present study is to evaluate arthritis-like findings on MRI studies of the forefeet of healthy volunteers versus patients with symptomatic rheumatoid arthritis (RA) or psoriatic a...\nA type of inflammatory arthritis associated with PSORIASIS, often involving the axial joints and the peripheral terminal interphalangeal joints. It is characterized by the presence of HLA-B27-associated SPONDYLARTHROPATHY, and the absence of rheumatoid factor.\nHeterogeneous group of arthritic diseases sharing clinical and radiologic features. They are associated with the HLA-B27 ANTIGEN and some with a triggering infection. Most involve the axial joints in the SPINE, particularly the SACROILIAC JOINT, but can also involve asymmetric peripheral joints. Subsets include ANKYLOSING SPONDYLITIS; REACTIVE ARTHRITIS; PSORIATIC ARTHRITIS; and others.\nA rare, benign rheumatologic disorder or syndrome characterized by hyperostosis and soft tissue ossification between the clavicles and the anterior part of the upper ribs. It is often associated with the dermatologic disorder palmoplantar pustulosis, particularly in Japan. Careful diagnosis is required to distinguish it from psoriatic arthritis, OSTEITIS DEFORMANS, and other diseases. Spondylitis of pustulosis palmaris et plantaris is one of the possible causes; also, evidence suggests one origin may be bone infection. Bone imaging is especially useful for diagnosis. It was originally described by Sonozaki in 1974.\nArthritis, especially of the great toe, as a result of gout. Acute gouty arthritis often is precipitated by trauma, infection, surgery, etc. The initial attacks are usually monoarticular but later attacks are often polyarticular.\nAn aseptic, inflammatory arthritis developing secondary to a primary extra-articular infection, most typically of the GASTROINTESTINAL TRACT or UROGENITAL SYSTEM. The initiating trigger pathogens are usually SHIGELLA; SALMONELLA; YERSINIA; CAMPYLOBACTER; or CHLAMYDIA TRACHOMATIS. Reactive arthritis is strongly associated with HLA-B27 ANTIGEN.\nA joint is where two or more bones come together, like the knee, hip, elbow, or shoulder. Joints can be damaged by many types of injuries or diseases, including Arthritis - inflammation of a joint causes pain, stiffness, and swelling with ...\nAccording to the National Arthritis Data Workgroup, an estimated 6 million people in the United States report having experienced gout at some point in their lives. In fact, gout is the most common form of inflammatory arthritis in men over the age of 40....\nArthritis Fibromyalgia Gout Lupus Rheumatic Rheumatology is the medical specialty concerned with the diagnosis and management of disease involving joints, tendons, muscles, ligaments and associated structures (Oxford Medical Diction...']	['<urn:uuid:f00dd11e-2b2b-4209-9e90-f9e9252b843b>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T22:13:20.996306	20	25	699
96	I'm renovating my bathroom - do I need safety glass near the bathtub?	Yes, you should consider using safety glass near your bath or shower since the finished floor level will be higher when measured from the shower tray or bath itself. You can use either toughened glass, which shatters into harmless pieces if broken, or laminated glass, which holds together when broken.	['The glass plays the biggest part in keeping your new windows and doors energy efficient. It forms the largest part of most windows and doors. Glass is also required to provide very important safety features for all doors and particular locations of windows. The provision of safety glass ensures safety comes first.\nIt is a legal requirement that toughened safety glass must be fitted to all replacement doors. This legal requirement for toughened safety glass also extends to windows near floor level and other “critical locations”.\nThe Building Regulations for England & Wales have approved this rule and give clear information where toughened safety glass is required. The same rules apply when existing windows are being replaced or for new windows fitted into new buildings.\nWhat are “critical locations” requiring safety glass?\nThe critical area in a window or a door is the glazing that you are likely to come into contact with as you move around a house or commercial building day to day. This possible contact with glass means that glass must be shielded or protected from impact if glass is to break it must break in such a way that is not going to cause injury, glass must resist impact without breaking. This where you often see a pane of glass ‘shattered’ upon impact but not broken.\nFor any single, double or triple glazed replacement windows or doors, the following types as well as locations will need safety glass.\nAll glazing from the internal finished floor up to a height of 1500mm must be safety glass.\nAll doors whether partly glazed or fully glazed must have toughened safety glass\nWindows must be fitted with toughened safety glass where the window is within 800mm of the finished floor level internally.\nTop-lights above doors are not required legally to have toughened safety glass. However even with clear glass, the toughening process produces a different tint to the glass so we would always advise toughened glass in top-light areas as well to match the rest of the window glazing.\nThere are other areas where you may wish to consider the use of safety glass. If you are fitting a window near your bath or shower the finished floor level will actually be higher if taken from the shower tray or the bath itself.\nWith laminated glass it is effectively two pieces of glass with a film in between. In the event of a glass breakage, whilst the glass will break it will hold together. Toughened glass uses a toughening process different from laminated glass. Whilst it is hard to break toughened glass, should it break it will shatter into thousands of small harmless pieces.\nThe choice of toughened or laminated glass is of-course up to you. Laminated glass tends to be more expensive and heavier than toughened glass but if does often offer a reduction in sound pollution especially if you go for an acoustic laminate. If you want maximum security laminated glass is the best option, it will hold together when broken unlike toughened. Both types are suitable and acceptable under legal requirements and building regulations.']	['<urn:uuid:18f305c1-ef09-4b91-a752-254e83d007bc>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T22:13:20.996306	13	50	517
97	earliest bahamas boxing medal history	The first boxing medal for The Bahamas was won by Nat Knowles, who earned a silver medal at the Central American and Caribbean Games in 1974.	['Boxing in The Bahamas\nBy Frederick Sturrup\nEarly reports indicate that organized professional boxing began during the1930s but the sport really took off in the subsequent decade when Charlie Major Sr. returned home from NewYork to take up residence again. The former world class high jumper came up with a challenging approach for the times. He went into the boxing promotion business. He knew that if he could just cement the idea of professional boxing cards in the minds of the Bahamian people.\nHe started out with a few boxing cards featuring local boxers only. Once there was a good bit of interest in the boxing shows, he then called on his many sports contacts in the United States to assist in connecting him with noted managers, trainers and matchmakers.\nThe result was a sports revolution. Major Sr. was soon bringing into the country on a regular basis some of the best boxers in the world. He brought to The Bahamas, great ring masters such as Joe Louis, Jimmy Carter, Joe Brown, and Willie Pep. He was responsible for a massive upturn of the general sports profile in the country. Major Sr. was indeed the first Bahamian to provide a regular competitive forum for world elite athletes.\nProfessional boxing with Major Sr. as the dean of the sport enjoyed glorious eras. Out of the Bahamian boxing environment came Yama Bahama and Gomeo Brennan who blazed an early trail internationally during the 1950’s and 1960’s. The Bahamas became the glamorous television star. Brennan won the British Empire (Commonwealth) middleweight title. Then during the 1970s, Elisha Obed attained the crowning achievement of world champion. He won the World Boxing Council junior middleweight title.\nThe sport on the professional scene grew from strength to strength.\nBut for the first three decades of boxing in the country there was not an amateur progamme.\nCharlie Major Sr. again figured in another rather pleasant breakthrough for the national sports programme. Former national heavyweight champion Bert Perry came up with the idea and Major Sr. along with Fred Sturrup, Virginius Knowles and Amos Ferguson joined forces to be co-founders of the Amateur Boxing Association of The Bahamas.\nThe sport was enhanced by the new boxing platform. The amateur programme added new enthusiasm from the inception in 1969. By 1972, The Bahamas had Olympic representatives in middleweight Nat Knowles and welterweight Gary Davis. In 1974, Knowles won the first medal for the country, a silver medal at the Central American and Caribbean Games. In the ensuing years all aspects boxing have gone from strength to strength.\nThe major boxing organizations functioning in the country are:\nThe Bahamas Boxing Commission, with full legislated control over professional boxing; the Amateur Boxing Federation of The Bahamas (ABFB ) that has sole jurisdiction over amateur boxing and the Pan American Caribbean Boxing Organization (PACBO ) that has its head office based in The Bahamas. PACBO is a regional body that has an agreement with the ABFB to assist in the national development of the amateur program.\nThe amateur clubs affiliated with the ABFB are:\n· Carmichael Knockout Club\n· Lionheart Club\n· Bahamas Youth Club.\n· Cassius Moss Club\n· Pinewood Club\n· Champions Boxing Club\n· Freeport YMCA Club\n· Genesis Club\n· Yama Bahama PACBO Youth Club\n· Inagua Boxing Club']	['<urn:uuid:bd16051c-20d4-48f3-a83b-075d94e19687>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T22:13:20.996306	5	26	551
98	How many German ships were involved in the Scapa Flow incident, and what happened to the German sailors who tried to sink them?	The German fleet at Scapa Flow consisted of 74 warships. When the German crews scuttled the ships, British forces intervened, resulting in 9 German sailors being killed and 16 wounded. A total of 1,774 Germans were recaptured after attempting to escape in lifeboats.	"[""How EIVA’s Academic Licence assisted in the challenge of surveying the Scapa Flow for a maritime archaeology operation\nThe Grand Scuttle of Scapa Flow is where Germany’s remaining WWI fleet of 74 warships intentionally sank itself off Scotland’s Orkney Islands to avoid handing over the High Seas fleet as part of the 1919 peace terms. The scuttle made headlines globally in June 1919, as the 74 most powerful vessels of the undefeated High Seas surface fleet interned at Scapa Flow made their way down into the waves on the orders of Admiral von Reuter.\nDue to a fortunate series of circumstances, Admiral von Reuter managed to sink his fleet upon learning the Versailles peace terms, a mere two days before the planned seizure of the fleet by the British. The Royal Navy attempted to stop the foundering of the ships and managed to beach 20 of the 50 torpedo boats, but only managed to prevent the sinking of one battleship and three light cruisers. Most of the vessels have been salvaged, leaving 9 surviving wrecks at Scapa. The wrecks of Scapa Flow have officially become part of the UNESCO’s Underwater Cultural Heritage, as they have now been under water for 100 years.\n3D multibeam model of SMS Cöln, made for 'Scapa 1919: The Archaeology of a Scuttled Fleet'\nIn 2017, marine archaeologist Dr Innes McCartney of Bournemouth University joined the Sea War Museum Jutland to compile an account of the Scapa Flow, now published in the book ‘Scapa 1919: The Archaeology of a Scuttled Fleet’. Dr Innes McCartney is known as an ‘academic partner’ to EIVA, and utilises EIVA’s NaviSuite software under the terms of an Academic Licence.\nA meticulous survey was carried out on the 9 shipwrecks of Scapa – the 3 battleships SMS König, Markgraf, Kronprinz Wilhelm; the 4 light cruisers SMS Brummer, Karlsruhe, Cöln, Dresden and the 2 torpedo boats SMS S54 & V83. The survey was to be compared with the 2006 ADUS multi-beam survey, utilising the unique opportunity to study the site formation processes that took place in the decade that passed. To construct an accurate narrative of the day’s events, as well as the processes affecting these wrecks, the remaining wrecks of Scapa Flow were multi-beam surveyed and assisted by EIVA’s NaviSuite software for both data acquisition and post-processing, resulting in sophisticated 3D models of these historic legacies.\nThe solution in the words of Dr Innes McCartney\nAn excerpt from The Sea War Museum Jutland multibeam survey, 2017\n‘The Sea War Museum Jutland carried out a multibeam survey of Scapa Flow over a 10-day period in January 2017. As the museum’s affiliated archaeologist, I was present throughout and was involved in the planning of the project and the subsequent processing and presentation of all of the multibeam data shown in this book.\nDue to their relatively shallow depth, the nine remaining wrecks are ideal shipwreck targets for a multibeam survey. Aside from the wrecks, we also wanted to use the multibeam to search and map as much of the former German anchorage, as well as the salvage areas around it, as we could in the time allocated. We also wanted to scan a number of other very shallow areas. To acquire the data needed we employed two survey vessels: the 2,065-tonne survey ship Vina, our base of operations, and Limbo, a small day boat which can operate in very shallow water.\nThe Sea War Museum Jutland’s two survey vessels used to survey Scapa Flow: Left, MV Vina, and right, the Limbo being deployed from Vina during the survey\nBoth vessels are equipped with identical Reson 7125 multibeam systems and they use EIVA NaviSuite software. The two vessels can be seen above. As it was, Limbo proved exceptionally useful because a number of areas were simply too shallow or enclosed for Vina to work in. Full credit goes to the team who worked on Limbo every day in sometimes choppy conditions. It was not a task for those who get seasick. The data Limbo gathered on the shallow sites, such as SMS S36, are unrivalled in detail.\nThe survey ended up covering an area of around 40km2. The data were readied for archaeological analysis as shown below, with the greatest care being taken to acquire the highest resolution possible on each specific shipwreck site.\nThe full extent of the area of coverage of the 2017 multibeam survey where the focus was on the German anchorage, but all the other major shipwrecks were also scanned\nMultibeam systems create a sound pulse in a fan of up to 512 individual beams from an echosounder under the hull. The returning soundwave is picked by an antenna array and the directional information is processed to produce a swath of depth readings in three dimensions. Multibeam is primarily used by hydrographic surveyors to acquire data relating to depth of water and type of seabed. Its ability to record objects on the seabed in three dimensions makes it a useful tool for surveying shipwrecks. This is particularly the case when the wrecks are very large or when there is low visibility or marine growth that make surveys by more traditional diving methods challenging.\nThe shipwreck survey data we acquired came in the form of a point cloud, made up of millions of individual depth readings. A point cloud can be processed in several different ways to maximize its potential archaeological value. The means by which the shipwreck data from the Scapa survey were processed is given below, which uses the example of the battleship wreck of SMS Markgraf.\nThe original point cloud gathered over the SMS Markgraf and how it was processed to produce the survey results (part 1)\nThe results of the multi-beam survey\nStarting at the top, Image A shows the original point cloud once it is drawn out from the original survey data. It has been coloured to give height readings from the seabed. It shows every depth point recorded over the entire shipwreck. The challenge for surveyors on objects which stand up off the seabed, such as this, is to get point readings from the vertical aspects of the object. In a hull-mounted multibeam system, the ‘top-down’ nature of the soundings means that points generally accumulate on horizontal surfaces, as can be clearly seen.\nIn Image B the point cloud has been processed into a ‘Digital Terrain Model’ (DTM), shown from above in plan view and coloured to show height range. The DTM plan view is usually used for seabed mapping, and it creates excellent site maps which are used to depict every wreck site. Accurate measurements can be made from them and, as with all the data, it can be georeferenced into the maps of the survey. Its limitations are evident when covering larger upstanding objects, when a curtaining effect is seen in three dimensions, as shown in Image C.\nThe original point cloud gathered over the SMS Markgraf and how it was processed to produce the survey results (part 2)\nIn order to avoid this effect and study the wreck in three dimensions the point cloud is processed in another way to create a ‘hybrid model’. The point cloud is manually cut into sections which separate the seabed from the wreckage. These separate point clouds are then coloured differently. In Image D the initial result is shown, with artificial ‘illumination’ from directly above. This is suitable for most applications, but in the case of large solid objects the absence of points on some vertical surfaces creates the false impression that the wreck is see-through and hollow inside.\nFinally, to give a more accurate visual impression, occlusion objects are added to the interior of the point cloud. The final solid-looking model can be seen in Image E. I first came across the use of occlusion objects in the 2006 ADUS survey of the wrecks, which produced excellent results. Our hybrid models were prepared in the same way in order to analyse how the wrecks have deteriorated in the 11 years between surveys. Image E clearly shows how the edges of decks can be seen poking out of the side of the wreck from areas where the original outer armour of the ship has been removed by salvage.’\nSMS Markgraf multibeam model made for 'Scapa 1919: The Archaeology of a Scuttled Fleet' from survey data gathered in 2017\nFacts and figures\nGermany’s WWI fleet of 74 warships sank itself in June 1919\nThere are 9 remaining shipwrecks of Scapa – the 3 battleships SMS König, Markgraf, Kronprinz Wilhelm; the 4 light cruisers SMS Brummer, Karlsruhe, Cöln, Dresden and the 2 torpedo boats SMS S54 & V83\nThe multibeam survey of Scapa Flow lasted over a 10-day period in January 2017\nTwo survey vessels were involved in the operation: the 2,065-tonne survey ship Vina, and Limbo, a small day boat which can operate in very shallow water\nBoth vessels were equipped with identical Reson 7125 multibeam systems\nThe survey covered an area of around 40km2"", 'When the Armistice was declared in November of 1918, hostilities of the Great War were officially over and a cease fire went into effect. In the final seconds on the Western Front, there was a fury of gunfire in the final seconds before the deadline, as soldiers competed to see who could claim to have fired the last shot of the war. Different sources claim different names as the last death of the war, but since men were scattered in so many places, exactly who died last in the seconds just before the cease fire is uncertain. Some were no doubt killed hours or even days later in isolated places that didn’t get the word of the cease fire on time.\nDespite this confusion, however, we can say for certain that the last combat deaths of the First World War took place on June 21, 1919…..in northern Scotland! This is the story.\nWhen the Armistice was signed ending the First World War, the German Navy was in a difficult position. The allied powers were undecided whether to scuttle the ships, or divide them up as spoils of war among the allied nations. If they were divided up, exactly who got what and how would it affect the balance of power in Europe in the future? While the allies tried to figure this out, they decided to intern the German surface ships at Scapa Flow, the gigantic British Naval Base in the north of Scotland. A total of 72 German vessels, including several battleships, were anchored in Scapa Flow awaiting final disposition.\nThe German command of this dreary mission was given to Admiral Ludwig von Reuter. While the Allies negotiated the fate of the German High Seas Fleet, the ships were still under the command of von Reuter and still manned by their German crews. As may be imagined, the morale of the Germans was close to rock bottom. In addition to being defeated, they sat day after day in one of the bleakest places on earth, watching the fog and rain. They could not go ashore or even visit other ships. The food was miserable, mail was slow, and many of the men were close to mutiny. A few already had. Von Reuter found maintaining discipline increasingly difficult as they awaited a decision on their fate.\nTurning their ships over to the Allies to be distributed to their former enemies was a bitter pill to swallow, and von Reuter planned to scuttle the ships if he got the chance. The British, however, were alert to such a move, and how much cooperation von Reuter might get from some of his fellow officers was uncertain, so the long wait continued. Von Reuter secretly made preparations and awaited his chance.\nThe months passed and most of the German crews were taken off and sent back to Germany. This actually made things easier for von Reuter, because it gave him a way to retain only officers and men he could trust. Finally, the Treaty of Versailles was scheduled to be signed at noon on June 21, 1919. This would formalize the final disposal of the ships and allow the British to take possession from their German crews. Admiral Sydney Fremantle, the British commander planned to seize the ships on June 21st. Instead of seizing the ships in the morning, however, Fremantle decided to take advantage of some rare good weather and take most of the British ships out to conduct some long-overdue torpedo drills. The seizure would take place in the afternoon upon his return.\nSeizing his chance, von Reuter signaled the German ships to hoist the Imperial German flag and scuttle. Water rushed into the ships, with the smaller ones sinking first. The British, alerted to the situation, sent smaller gunboats to stop the Germans, who were by now escaping in lifeboats. At this point, several British boarding parties swarmed about several of the ships, but met resistance from the Germans. The British, considering the Germans had made themselves enemy combatants once again, opened fire on the Germans, killing 9 and wounding 16. All together, 1774 Germans were recaptured. The nine deaths were the last combat deaths of the First World War; over six months later and in the north of Scotland.\nOf the 74 interned ships, 54 were sunk, including the big battleships. Some of the ships were re floated and some were salvaged. Although the British were angry that the Germans had gotten the better of them, they were relieved that the whole thorny question of who was going to get the ships was now moot. In ensuing years, more of the wrecks would be salvaged and only a few now remain as dive sites.']"	['<urn:uuid:7f655b40-17cb-4d7a-98c5-b9e65cede6f6>', '<urn:uuid:8dfa1ee1-7581-4225-b6b4-b8d6a04e2577>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T22:13:20.996306	23	43	2282
99	What are the main benefits of rebound therapy?	Rebound therapy, which uses trampolines for therapeutic exercises, provides multiple benefits. It helps facilitate and promote movement and balance, improves fitness, can increase or decrease muscle tone, helps with relaxation, improves sensory integration, enhances concentration and communication skills. Additional benefits include stimulation of the digestive system, improved bowel function, internal organ massage, and clearing of toxins from the body.	['What is rebound therapy?\nRebound Therapy is used to facilitate movement, promote balance, support an increase or decrease in muscle tone, promote relaxation, sensory integration, it improves fitness and exercise tolerance and to improve communication.\nRebound therapy uses trampolines to provide therapeutic exercises to people with a wide variety of disabilities and additional needs. The therapy involves using the moving bed of the trampoline to promote movement in the participant. By carrying out basic through to highly technical physiotherapy techniques on the trampoline, the therapy can provide many therapeutic and physiological benefits:\nFacilitate and promote movement and balance, Improve fitness, Increase or decrease muscle tone, Help relax the participant, Improve sensory integration, Improve concentration and even improve communication skills.\nRebound Therapy is one of the most popular activities for children with special needs in UK SEN schools.\nIt is also a very popular activity for people with disabilities of virtually all kinds and for all ages.\nSome of the benefits of Rebound Therapy include the developing and improvement of:\nOther benefits include:\nStimulation of digestive system,\nImproved bowel function,\nInternal organ massage,\nClearing of toxins from the body.\nRebound Therapy aims to be accessible to all, as all can benefit. However, there are 6 exceptions to this rule. Unfortunately 6 medical conditions make it too unsafe for a person to rebound or take part in any kind of trampolining (and many other sports).\n1. Atlanto-Axial Instability (AAI)2 : A condition associated with 10-40% of people with Down’s Syndrome. Weakened ligaments is normal in people with Down’s Syndrome, causing slack joints. This may cause a weakness in the AtlantoAxial joint of the first (Atlas) and second (Axis) vertebrae, right below the skull. This makes sufferers of AAI prone to a slippage of these vertebrae which can cause brain damage and paralysis. This Diagram shows the position of the Atlanto-Axial joint and how AAI can affect vertebrae position when moving.\n2. Detaching Retina: This disorder is caused when the retina (the thin layer at the back of the eye, responsible for light detection) starts to separate from the eye socket. It can cause blindness if not treated early. Trampolining can cause the retina to completely detach, leading to total blindness.\n3. Spinal Rods: This is a common treatment for Scoliosis (a sideways curvature of the spine, which can also cause the spine to twist and which can pull the ribcage out of position and disrupt the position of vital organs). After spinal fusion surgery active sports are not allowed for at least 6-12 months. Trampolining may disrupt any rods/screws or fusions in the spine made during surgery- worsening the patient’s condition.\n4. Growth Hormone Deficiency5 (Dwarfism): Someone with GHD may have skeletal and joint stability problems that can worsen with trampolining.\n5. Osteogenisis Imperfecta6 (Brittle Bone Disease): A congenital disease that means a gene is affected causing a deficit in the production of collagen, a major building block for bones. The condition causes extremely fragile bones that will be prone to breakages and fractures during trampolining.\n6. Pregnancy: Trampolining during pregnancy can cause disruptions to the womb and has the potential to cause harm to an unborn baby. Also, the extra strain of such a high energy sport may cause damage to the mother whose vital organs are already functioning at a higher than normal level.']	['<urn:uuid:4e409554-2aaf-48c7-bca3-3b2391ad8672>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T22:13:20.996306	8	59	554
100	share control with children during cancer treatment	You can share control with children during cancer treatment in two main ways. For younger children, offer limited but realistic choices, such as letting them choose which family friend will drive them to practice. For older children, include them in open-ended discussions about family decisions, like whether to postpone a trip or choose an alternative activity. Always discuss options with your partner first to agree on acceptable outcomes.	['Information central for parenting with breast cancer.\nTaking Care of Your Family\nPreserving Continuity and Routine\nChildren thrive on routine. It makes them feel safe and secure. Lots of change, on the other hand, can make kids anxious and agitated. When you have breast cancer, maintaining a routine can be difficult at a time when your kids need it most. You can help keep your kids feeling safe and secure by minimizing change, reassuring them, sharing control, and limiting surprises.\nMinimize change. Disruption in family routines is inevitable when a mother has breast cancer. Treating this disease is much like taking on a second job, so moms need help carrying on with the family’s daily activities. Here are some tips for minimizing the changes to our children’s routines:\nTransportation - Try to keep some continuity in the people who help transport your children. For example, instead of having a different person drive your children to school every day, try to have one person drive the kids to school for a week.\nMeals - You can help keep mealtime familiar even when friends and family members offer to cook for you. Many moms are reluctant to give people direction about cooking for their family because they fear offending the cook or appearing ungrateful. Yet most volunteer cooks will be happy to have some direction. They want your family to enjoy the meal they prepare for you. Here are a few suggestions:\n- Be frank with volunteer cooks. Tell them what your children will (and won’t) eat. You can say, “I am so grateful that you’ve offered to cook for us. You should know up front that my kids are very picky eaters. I don’t want you to go to a lot of trouble to prepare something they won’t eat. Would you like some suggestions about what to make?”\n- Suggest food combinations that your children are used to eating. You can say, “Tacos would be terrific. My kids love them! They would be especially happy if you would include some corn. I always serve corn when we have tacos, and I know that familiar touch would make them especially happy.”\n- Offer the brand names of specific products. You might say, “If you’re making a special trip to the store, my kids really like Prego Traditional spaghetti sauce. That would be a great choice.”\n- Post information online. If you use the internet to communicate with your helpers, you can give them information about meal suggestions, brand names, food allergies and intolerances, and even some favorite recipes. See Online Communication in the Resources section of this website.\nOf course, don’t get carried away with making demands on people who are kind enough to cook for you. Only you can prepare familiar meals exactly the way your children like them. Your kids will get used to trying new or differently prepared foods. If you’re up to it, consider thanking the cook by phone, e-mail, or with a brief note in the mail.\nBedtime - Routines make bedtime easier for the whole family, and they help your child feel safe and secure. If your illness prevents you from carrying on with your normal routine, try changing either the routine or the person who does it (such as your partner, a grandparent, or a babysitter).\nIn other words, it’s easier for your children if Grandma does her best to follow your child’s regular bedtime routine. Alternatively, come up with a temporary routine that you’re capable of doing yourself, such as reading to your child in a chair instead of reading to him in bed.\nReassure them. Reassure your children that their needs will be met, that you still love them, and that most of the changes related to your illness will be temporary. While this may be obvious to adults, children need reassurance about these basic facts. They have rich imaginations that can attach mistaken meaning to changes in routine. Your children may need to hear reassurances like these:\n“I love you as much as ever, even if the way I show you my love changes for awhile.”\n“I will make sure that you have everything you need, I just may not be able to do it for you myself.”\n“Some things might change for awhile, like who drives you places or what you have for dinner, but this won’t last forever.”\n“I can’t put you to bed tonight, but it’s not because I don’t want to or don’t love you. My heart wants to, but my body just isn’t up to it right now.”\nBesides talking to your kids, you can reassure them with your actions. Put an unexpected note in their lunch boxes or send them periodic e-mail messages. You can even send younger kids a note in the mail. They will be thrilled to tear open a letter addressed to them, even if it comes from someone in their own house.\nShare control. Being in control helps people feel secure. You can share this feeling with your children by giving them some control over the changes in your household. You can share control by offering choices and by inviting open-ended discussions.\n- Offer choices. You can include younger children in decision-making by giving them limited choices. Only offer choices that are realistic and that you can accept.\nYou might say something like this: “I can’t take you to practice tomorrow because I have an appointment with my doctor. Would you rather have Mike’s mom or Ben’s mom drive you?”\nYour child might not want to make a decision, or he might insist that you drive. You can say, “I really want to drive you, too, but it’s just not going to be possible tomorrow. Would you like to decide or should I decide for you?”\n- Invite open-ended discussions. Older children can be included in open-ended conversations about some changes that will affect the family. Be sure to discuss the issue with your partner first, so you agree in advance on what outcomes will be acceptable to you both.\nThis is an example: “My doctor doesn’t want me to go on our hiking trip later this month. Dad and I talked about it, and we had a few ideas. We could postpone the trip until next year or you could go without me. We could also go somewhere closer to home instead. What do you think we should do?”\nLimit surprises. While it’s not always possible, informing kids about changes in advance makes the changes easier to accept. (Be cautious with telling younger kids about potentially scary changes, like upcoming surgery. By giving them too much notice, they have a lot of time to worry.)\nGiving your kids a calendar will help reduce their anxiety. A calendar keeps them informed and shows them that changes are temporary. Some kids will enjoy making a calendar with you, and some kids will want to cross off each day that passes. They’ll also have activities to look forward to, like sleepovers, playdates, and birthday parties.\nHere are some suggestions for making calendars for your family:\n- Tell your kids that events on the calendar will likely change, but that you’ll do your best to make sure they know about the changes in advance.\n- You can buy a paper calendar or a wipe-off calendar for the wall, desk, or bulletin board. You also can make one yourself. If you’d like to make a calendar, you can download one for free at www.calendarsthatwork.com or click on Calendar in the Tools section of this website.\n- These are some items to consider including in your calendar:\n- After-school activities\n- Birthday parties\n- School programs\n- Hot lunch days\n- Religious school\n- Field trips\n- If you choose to make a calendar, consider using clip art for young children. Calendars are helpful even for children who cannot read. Just seeing repeating patterns of pictures, like a soccer ball indicating soccer practice every week, is reassuring. You can find free clipart at the Microsoft website. You can also find clipart for some common children’s activities in the Tools section of this website.\n- If you make one calendar for the entire family, consider using different colors or symbols to indicate which family members participate in each activity.\n- Consider sharing the calendar with babysitters, family members, teachers, carpool drivers, and anyone else who is involved with the daily care of your children.\n- Minimize confusion by indicating when your calendar was last updated by writing “Updated on [date]” in one corner.']	['<urn:uuid:70bbd29a-31b9-4877-9d6e-b54267185dc3>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T22:13:20.996306	7	68	1425
