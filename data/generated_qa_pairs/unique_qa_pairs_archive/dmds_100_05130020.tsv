qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	gold mining australia historical significance environmental damage effects mining communities ecosystems	Gold mining in Australia had profound historical significance, transforming the country in the 1850s by adding over a million people in just two decades and fostering an independent working spirit that shaped Australian identity. However, gold mining also causes severe environmental damage including erosion, loss of biodiversity, contamination of soil and water, destruction of habitats, and creation of unsustainable wastelands. The mining activities affect both the mining communities through socio-economic changes and the surrounding ecosystems through deforestation and chemical contamination.	"['Mining – Gold, Coal, Copper and Tin – The Songs\nSONGS FROM THE MINES OF AUSTRALIA\nI have a long-standing interest in mining in Australia. I suspect I was inspired when, as a young lad in the early 1960s, I came across a small pamphlet of songs put together by the British singers A. L. Lloyd and Ewan MacColl. It was titled ‘Shuttle and Cage’ and contained songs from the northern English coalfields. It was also around the time I started thinking about undertaking some song and story collecting in Australia. I was curious as to why there were so few mining songs here, especially since the role of minerals in our history. I assumed, correctly, that the few pioneer collectors like John Meredith and Norm O’Connor, went for the obvious – the bush song tradition, which, by its nature, was primarily concerned with rural pursuits, especially shearing, droving and bush life. When I decided to go bush with a tape recorder the first destinations were mining communities like Cessnock, Kurri Kurri, Lithgow and Kapunda. The more I looked the more I realised we had overlooked our industrial folksongs. Yes, I did collect many songs and ditties and the very first album I produced for my fledgeling Larrikin label was titled ‘Man of the Earth’ and contained mining songs. I kept collecting mining songs and, in 2015, assembled a large book which I called ‘The World Turned Upside-down’, for that is exactly what the discovery of gold did to Australia. I also produced an album of some of the songs, ably helped in the studio by Luke Webb, a fine singer and musician, and Marcus Holden, my longtime music producer and accompanist.\nHere are two traditional singers from the Warren Fahey Collection in the National Library of Australia. Both were recorded in Broken Hill, 1973.\n- The Miner’s Dream of Home\n- Don’t Go Down The Mine, Daddy/The Miner\nThe discovery of gold indeed turned the world upside-down. This song, with its chorus of “Going up to Summerhill, the gold mine for to see” was published in Poems Written in Youth by William Walker (Sydney 1984) with a note that the song was written “at the outbreak of the Gold Diggings in 1851.” prog is an early Australian term for food. It was written as a parody of ‘Oh Susannah’\nLuke Webb sings the title track from the accompanying album ‘The World Turned Upside-down’\nThe following in the introduction to ‘The World Turned Upside-down’ ebook.\n“There’s a hell of a lot of gold in Australia – and a bloody lot of earth mixed in with it!”\n(Quoted by Rad Dawson, Forrester’s Beach, NSW, recorded by Warren Fahey, 1974)\nAustralia’s first payable gold was discovered by Edward Hargraves at Summerhill Creek, near Bathurst, NSW in 1851. Gold mania immediately swept the colonies. Prospectors cancelled their trips to California and thousands of clerks, labourers and servants ‘bolted’ from their work to head west to the newly named ‘Ophir’ goldfields. “If this is gold country,” said the Colonial Secretary, “it comes on us like a clap of thunder, and we are scarcely prepared to credit it.”\nBell’s Life in Sydney and Sporting Reviewer, established in 1845, always regularly published topical verse and, on 31 May 1851, in an article headed “On the gold-digging epidemic now raging in New South Wales”, it offered this ditty predicting the Australian goldfields would prove far larger than California.\n‘It’s Off to the Diggings We’ll Go’ sung by Luke Webb, Warren Fahey and Marcus Holden.\nThe story of gold mining in Australia has been told many times but this book offers no ordinary facts and figures history – it is more of an emotional history – from the men and women who were diggers or first-hand observers on the goldfields during the 1850s gold rushes and the later company mines. It is also a songbook and assembles many of the early songs circulated through the oral tradition, concert halls, goldfield shanties and published songsters and in reader contributions to newspapers and magazines. Many of these songs are published for the first time.\nAustralia has been blessed with minerals. It has been our saving grace and curse for centuries. The goldrush of the mid 19th century changed Australia forever adding over a million people in just two decades. It was also important socially as the gold diggers forged an independent working spirit that became a major part of who we now are as a people. Shearers and, later on, coal miners inherited this independent spirit and it resulted in Australia claiming the world’s first democratically elected socialist government.\nToday’s Australia is still mineral rich and, thankfully, it has helped us ride through the so-called Global Economic Crisis. We still battle with side issues of workplace relations, environmental sustainability, increasing reliance on partnerships with relatively new partners like China and India, and that age-old balance of who actually owns mineral wealth. It is important we know our mining history so we can properly establish where we are today, and where we should be headed.\nIn this book, there are some fascinating diary reports from the goldfields, extracted from digitized manuscripts and other print sources. The Australian TROVE project has been digitizing early newspapers and magazines for several years and now offers researchers millions of pages and has been a boon to revealing new material, especially in my folklore work in locating rare songs. Frustratingly the scanning of early printing often results in gobbledygook and one needs to painstakingly correct the type extracts.\nMost of the songs in the collection come to us from anonymous writers however there are two exceptional contributors – Charles Thatcher, better known as ‘The Colonial Minstrel’ and Joe Small, a well-known colonial singer and music publisher. The role of these ‘popular’ songwriters and music publishers cannot be underestimated for their songs definitely traveled far and wide. There is some illuminating new insights to both Small and Thatcher including an observation of an actor who worked with him in Ballarat who pointed out that “Thatcher was not much of a singer but hugely popular as a satirist.”\nHere is a song about shipping agents, the men who arranged passage on the many ships in and out of our harbours. Some trips were back to Europe, Asia or across to the New Zealand goldfields, or around Australia to a new rush. Some of the unscrupulous agents took advantage of the ‘innocent’ diggers and, after receiving their passage money, were never seen again. Charles Thatcher didn’t hold back in exposing them and you can imagine him belting this one out in a goldfields hotel to the tune of ‘Campdown Races’.\nLuke Webb (on banjo) sings Thatcher’s ‘The Shipping Agents’\nWherever possible I have provided traditional tunes for the songs, most of these were specified in the original publication or come to us from versions collected in the oral tradition. A handful are offered as ‘suggested tunes’ having been married to verse by singers in the 20th century. The fact that so many published songs, in newspapers, magazines and songsters, specified traditional airs is a testament to the importance placed on such tunes in the 19th century. There are several databases offering midi files to traditional tunes. I recommend The Digital Tradition. I have also included, where possible, references to Australian recordings, including some of my own, for recorded songs.\nThere are two aspects of the songs that I would highlight. One is the influence of American minstrel music on our gold songs and the other is the art of parody. Minstrel music was extremely popular in Australia from the late 1840s and became particularly popular in the 1850s and 60s when several major American minstrel troupes, mostly black face, toured extensively. Parody, including parodies based on minstrel songs like ‘Oh Susanna’, had wide circulation and were, understandably, an easy way to get a song to travel. You will also find some good honest doggerel but remember these sometimes awkward rhymes delighted many a lonely miner as he sat by the campfire at night.\nWarren Fahey sings ‘The Mines of Australia’, a song he collected in the early 1970s and typical of the mournful verse written by many a dejected digger. One suspects the story was a common one, and not very nice at that!\nTwo friends travel to Australia to dig for gold but only one, the singer of the song, returns. It appears as a Victorian melodrama of foul play where our remorseful singer has likely hit his friend on the head with a shovel and pocketed his share of the gold. I recorded this song from two singers in the nineteen seventies, Mrs Dee Clarke, Sydney, and Cyril Duncan, Brisbane.\nWarren Fahey sings ‘Pint Pot & Billy’, a song he recorded from the singing of Cyril Duncan, Hawthorne, Qld, in 1974.\nHow revealing some of the miner’s diaries are, be they of emigrant’s travelling to Australia, usually with a mixture of trepidation, optimism and fear, or those written in a crude tent, in the goldfields. We are fortunate to have these windows into the hearts of these adventurous people. There are also many reports, personal accounts and reminiscences of colonials observing life on the goldfields – how they travelled, how they lived and their successes and disappointments.', ""Mining has several bad effects. It leaves behind a huge hole after mining is done. Secondly it damages natural beauty. A beautiful landscape which once existed is now a huge piece of dug up earth.\nEnvironmental Effects. Environmental issues can include erosion, formation of sinkholes, loss of biodiversity, and contamination of soil, groundwater and surface water by chemicals from mining processes. In some cases, additional forest logging is done in the vicinity of mines to create space for the storage of the created debris and soil.\nThe effects of mining in Africa have left large-scale devastation when companies do not honour their responsibility. Because mining areas are left in an unsustainable condition, plant species and wildlife are threatened and these areas are at risk of becoming lifeless wastelands.\nThe Impact and Effect of Illegal Mining (galamsey) towards the Socio-economic Development of Mining Communities: A Case Study of Kenyasi in the Brong Ahafo Region Adjei Samuel1, N.K.Oladejo1, I.A. Adetunde2, * 1University for Development Studies, Department of Mathematics, Navrongo. Ghana.\nSome of the major effects of mining on the environment are as follows: Minerals are the natural resources which play an important role in the economic development of the country. But the extraction and mining of these natural resources leads to some adverse effect on our environment as well.\nMar 09, 2017· The mining industry has the potential to disrupt ecosystems and wipe out wildlife populations in several different ways. Here's how mining affects the environment and wildlife. Habitat Loss; Mining can lead to the destruction of habitats in surrounding areas. The …\nModern mining is an industry that involves the exploration for and removal of minerals from the earth, economically and with minimum damage to the environment. Mining is important because minerals are major sources of energy as well as materials such as fertilizers and steel.\nApr 25, 2017· Mining is the extraction of minerals and other geological materials of economic value from deposits on the earth. Mining has the potential to have severely adverse effects on the environment including loss of biodiversity, erosion, contamination of surface water, ground water, and soil.\nSome gold can be found by panning in rivers; heavy gold will remain in the pan, whereas lighter rocks and minerals float out. This small-scale form of gold mining has little effect on the body of water, but the large-scale practice of mining gold from ore can have tremendous negative effects on water quality.\nMining can effect the earth because first, deforestation, and because mining requires large portions of land to be removed before they can start mining, lots of trees and plants are removed.\n1.1 PHASES OF A MINING PROJECT There are different phases of a mining project, beginning with mineral ore exploration and ending with the post-closure period. What follows are the typical phases of a proposed mining project. Each phase of mining is associated with different sets of environmental impacts. 1.1.1 Exploration\nFeb 07, 2018· The effects in such cases can be devastating for the environment. Be it due to ignorance of the regulations or just a freak accident, incidents like the Guyana spill of 1995 may occur again. This highlights the fact that issues like mining's effect on the environment are worth some serious deliberation.\nAug 26, 2010· Dust, radon and mercury impact miners' health. Dust, radon and mercury impact miners' health. ... Miners Face Health Risks, Even on Good Days ... mining …\nThe effects of mining coal on the environment. There are 2 ways to mine coal – Strip Mining and Underground Mining – both ways have their own impact to the environment and health. We know it but coal is such a cheap energy source that we don't want to let go of it. The negative effects of coal mining cannot be disputed:\nApr 21, 2019· The human health effects due to cyanide leach gold mining are not well documented, and this is no exception in Montana. The State of Montana has done no formal studies to specifically study mine-related health effects. Pegasus, the last mining company at Zortman-Landusky, started to fund a health study with the $1.7 million supplemental money from the 1996 settlement, but because …\nADVERTISEMENTS: Some of the major environmental effects of mining and processing of mineral resources are as follows: 1. Pollution 2. Destruction of Land 3. Subsidence 4. Noise 5. Energy 6. Impact on the Biological Environment 7. Long-term Supplies of Mineral Resources. Mining and processing of mineral resources normally have a considerable impact on land, water, […]\npositive and negative effects of mining on the environment. Mankind has been mining for precious metals since 42000 years ago and that's a staggeringly long time ago and that's exactly how long our species has been digging into the ground, to harvest its precious metals.\nDownload Coal Mining sounds ... 76 stock sound clips starting at $2. Download and buy high quality Coal Mining sound effects. BROWSE NOW >>>\nMining affects the environment by exposing radioactive elements, removing topsoil, increasing the risk of contamination of nearby ground and surface water sources, and acidification of …\nApr 20, 2015· Effects of Mining. Coal mining, the first step in the dirty lifecycle of coal, causes deforestation and releases toxic amounts of minerals and heavy metals into the soil and water. The effects of mining coal persists for years after coal is removed.\nJul 25, 2018· Environmental impacts from fossil fuel pollution are rapidly increasing in regions that have the highest concentrations of fuels. There are multiple effects of mining fossil fuels. Drilling and mining practices take a substantial toll on local water sources, biologic life and natural resources.\nPublished by the American Geosciences Institute Environmental Awareness Series. ... How can metal mining impact the environment? PDF version. Material adapted from: Hudson, T.L, Fox, F.D., and Plumlee, G.S. 1999. Metal Mining and the Environment, p. 7,20-27,31-35,38-39. Published by the American Geosciences Institute Environmental Awareness Series.\nMining operations usually create a negative environmental impact, both during the mining activity and after the mine has closed. Hence, most of the world's nations have passed regulations to decrease the impact. Work safety has long been a concern as well, and …\nEffects of mining on aquatic resources are both physical and chemical in nature. Most of earthmoving activities of mining occurred well before the enactment of laws designed to protect aquatic resources - particularly the 1977 Federal Water Pollution Control Act.\nThe former is known as underground mining, the latter as strip mining or mountaintop removal. Either process contributes a high level of damage to the environment: #12 Noise pollution. One of the most obvious (albeit perhaps least harmful) environmental effects of coal mining is noise pollution.\nMining has an adverse effect on soil quality. Soil degradation is the prime impact. Another impact is deforestation and loss of fauna and flora.\nThe impact of mining on the environment and the effects of mining techniques need to be more advanced with the utilization of modern equipment to be unintrusive to the environment. Economic growth is high on the agenda of leading countries, sustaining …\nMining is an inherently invasive process that can cause damage to a landscape in an area much larger than the mining site itself. The effects of this damage can continue years after a mine has shut down, including the addition to greenhouse gasses, death of flora and fauna, and erosion of land and habitat.\nNov 14, 2016· After mining is over, the land is left as barren land. The effects of mining sometimes vary depending on what is mined out, but these are some of the general effects you will see in all mine-areas. I'm not an expert when it comes to health impact on miners, but here are some of the things I know will affect them-\nJul 08, 2017· In coal mining, the extraction, crushing, and transport of coal can generate significant amounts of airborne respirable (extremely fine) coal dust. Dust less than 10 microns in size (cannot be seen with the eye). In non-coal mining, stone, and san...\nEnvironmental impacts of mining can occur at local, regional, and global scales through direct and indirect mining practices. Impacts can result in erosion, sinkholes, loss of biodiversity, or the contamination of soil, groundwater, and surface water by the chemicals emitted from mining processes. These processes also have an impact on the atmosphere from the emissions of carbon which have ...\nApr 04, 2017· The Dangerous Effects of Illegal Mining. April 4, 2017 Environmental Issues Written by Greentumble. Illegal mining has been ravaging our planet for. decades. Not only is illegal mining riskier from a safety perspective for those who choose to participate, but it encourages reckless behavior and leads to outcomes that have negative long-term ...""]"	['<urn:uuid:cdbace15-e817-420d-85a9-c787aee88c45>', '<urn:uuid:11ce18f5-51f7-4dfa-a26b-c4a5796e1622>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T00:20:10.754925	11	80	3006
2	legal punishment progress Nebraska made fight human trafficking recent years keywords	In recent years, Nebraska has passed laws to assess the scope of human trafficking in the state, implement a task force to analyze the problem, create harsher penalties for traffickers, and allow victims to clean up their criminal records. Additionally, the Attorney General's office has increased resources for law enforcement education and trafficking prosecutions.	['Venzor: Support for human trafficking victims\nRecently, the state Judiciary Committee advanced LB519, introduced by Sen. Julie Slama of Auburn. LB519 has become an omnibus bill for several different human trafficking proposals. Included in this package is LB516, introduced by Sen. Patty Pansing Brooks of Lincoln, which the Nebraska Catholic Conference supported earlier this session during public hearing. The legislation is now in the process of being debated by the entire legislative body.\nLB516 is a critical step forward for Nebraska in combatting the modern-day slavery of human trafficking. The legislation designates all children in Nebraska who are being sold for sex (one major form of human trafficking) as abused and neglected children, which ensures that they are subsequently connected with needed support services.\nCATHOLIC SOCIAL TEACHING\nThe church’s teaching on human trafficking stems from the fundamental fact of our God-given human dignity. As Pope Francis has recognized, “Every human being, man, woman, boy and girl is made in God’s image” (“Declaration on International Day for the Abolition of Slavery,” Dec. 2, 2014) and, therefore, each person deserves the respect and dignity that is theirs by right of being a child of God.\nHuman trafficking undermines this precious reality by seeing and using the human person as a commodity, a product for distribution and sale – as Pope Francis has noted, “to satisfy an immoral desire.”\nGiven this severe abuse of human dignity, it is no surprise that Pope Francis has also used strong language to condemn human trafficking. He has called human trafficking an “atrocious scourge,” an “aberrant plague” and an “open wound on the body of contemporary society.”\nFor these reasons, the church has recognized human trafficking for what it truly is and commenced the work of abolishing it. Most notably, religious sisters across the world – and even in our own state – have been in the trenches combatting human trafficking, rescuing and restoring victims and bringing traffickers to justice and rehabilitation.\nPROGRESS IN NEBRASKA\nIn the past several years, Nebraska has made major advancements to recognize, educate about, and end human trafficking. For example, the Legislature has passed laws to determine the scope and extent of human trafficking in the state, implement a task force to more closely analyze the problem of human trafficking, create harsher penalties for human traffickers, and allow human trafficking victims to clean up their criminal records.\nAttorney General Doug Peterson has also increased resources in his office to further educate law enforcement personnel across the state and to increase prosecutions of human traffickers.\nREMEDYING A PROBLEM\nCurrently, youth being trafficked in Nebraska are treated differently depending on the person who is the reported trafficker. If the trafficker is a parent or caregiver, the child is identified as abused or neglected and the Nebraska Department of Health and Human Services (DHHS) gets involved. If the trafficker is someone other than the parent or caregiver, law enforcement officials are primarily responsible for meeting the child’s need.\nHowever, law enforcement officials often lack the support services and resources to properly assist a child who has been the victim of human trafficking.\nLB516 remedies this problem by ensuring that every child who is a victim of human trafficking is properly identified as abused or neglected. This designation triggers a response by DHHS, which is better equipped to serve the child (and family) through the trauma of human trafficking. The services included may be safe and appropriate shelter, food assistance, medical and mental health care, substance abuse services and/or education, among others.\nNebraska has taken important steps to curb the demand for human trafficking by instituting stricter penalties for traffickers and users of trafficking. But legislation like LB516 seeks to provide more holistic remedies to the evil of human trafficking by ensuring that victims are properly cared for. Such policies become a sign of mercy and restoration for victims who, once healed, can be an even greater testament and resource in the goal of abolishing this form of modern slavery.\nIn your generosity, take a moment to let your legislator know that you support LB516 and other trafficking measures that assist victims. And please pray for the conversion of all those who diabolically engage in the human trafficking industry.\nTom Venzor is executive director of the Nebraska Catholic Conference, with headquarters in Lincoln. Contact him at firstname.lastname@example.org.']	['<urn:uuid:eb991803-7564-499f-a95a-882cea6e6196>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-13T00:20:10.754925	11	54	721
3	What role do information access and expertise play in new tech deployment?	Information access and expertise are critical barriers in technology deployment. For green technologies, over 60% of respondents reported a lack of reliable and available information, despite information coming from government and environmental protection organizations. Companies struggle to obtain reliable benchmarks and data about technology effectiveness. In the AI domain, the challenges are similar but more complex. Organizations often lack staff with combined expertise in both cybersecurity and AI. Additionally, when using vendor-provided AI solutions, companies struggle to understand the underlying models and assumptions. This knowledge gap creates risks, as organizations need to independently test AI security and understand how these systems protect their data.	"['Over 60% participants agreed general lack of reliable and available information in Green Technologies\nHONG KONG, CHINA - 23 April 2018 - Green Council\'s ""GreenTech Alliance"" (GTA), from 21 November to 12 December 2017, commissioned the Hong Kong Baptist University\'s School of Communication to conduct an email and/or in-person survey study of enterprises in various sectors. The aims were to understand the different practices and attitudes among employers and employees regarding use of green technologies at work, respondents\' willingness to accept green technology, and the challenges they faced in doing so.\nThese findings, in turn, would provide insight into why green technologies continue to be difficult to popularise in Hong Kong, and inform the formation of practical suggestions on how to improve the situation going forward.\nThere are two versions of the survey: one for enterprises and one for employees. The enterprise version is answered by the business owner or senior management. The employee version is answered by company employees. The survey hopes to paint a full and accurate ""on-the-ground"" picture of how companies attempt to introduce and implement green technology. The study gathered responses from 306 practitioners from 81 companies and ultimately led to an array of new insights.\nThe survey found that, in principle, 80.3% of enterprise respondents and 81.1% of employee respondents supported the company\'s introduction of energy-saving and waste reduction technology programmes. But when the costs of such technologies or programmes were mentioned, only between 30%--40% of enterprises expressed clear support.\nAbout half of the companies noted that they were ""50-50"" -- as in they were open to the idea of these technologies or programmes but found the costs to be concerning. As for employees, about 70% of interviewees were willing to commit additional resources or change their working habits to accommodate the use of green technologies. In general, employees are more receptive to the ideas than enterprises overall.\nCost-effectiveness and economic incentives are important conditions for the popularisation of green technologies. Enterprises are especially concerned about the cost of implementation. They generally hoped that, in addition to the environmental benefits, green technologies could bring financial benefits to their business.\nMeanwhile, over 80% of respondents (both enterprises and employees) agreed that green technology is an effective way to reduce energy consumption and waste production, suggesting an overall positive outlook. Overall, both groups of respondents acknowledged that in addition to the environmental benefits, green technology can improve resource efficiency and reduce company operating costs over the long run. Many respondents agreed that the business sector can play a leading role in promoting green technology and contributing to Hong Kong\'s energy conservation and waste reduction efforts.\nOverall, while enterprises and employees are willing to support increased adoption of green technologies, the lack of relevant information and economic incentives were noted as major roadblocks to promoting green technology solutions. The study shows that most of the information provided to enterprises and employees are from the government and environmental protection organisations.\nHowever, over 60% of all respondents noted a general lack of reliable and available information. Meanwhile, enterprises and employees are in favour of policies that provide economic incentives for the introduction and application of green technology programmes to offset the costs and risks involved. This is especially favoured by enterprises.\nGreenTech Alliance\'s Three Key Recommendations\n1. Government introduction of tangible incentives\nFor enterprises, operating costs are consistently a top consideration. Also, the government is generally the biggest driver of new initiatives. With these two understandings in mind, international examples prove that governments promoting the use of green technology can provide incentives such as tax rebates, which help companies eliminate the fundamental obstacle to implementation cost.\nWith green technology having kicked off to a slow start in Hong Kong, there is also the key question of whether local government policy is up-to-date or not. If new green technologies are to be developed in Hong Kong, the government may need to exercise policy flexibility and understand the need to deploy resources in a timely manner to facilitate the adoption of green technologies in the city.\n2. Environmental groups and professional bodies setting green technology benchmarks and related information platforms\nEnterprises are willing to adopt green technology, but actual implementation has yet to be popularised. One major cause is that companies cannot efficiently obtain reliable benchmarks, data and information. There is a clear need for further analysis on the types of technologies that different companies should use (relative to company type and scale) and their effectiveness, with evidence or testimonials of results.\nRelevant environmental groups and professional bodies should offer their expertise to provide a suitable rating or certification system for green technology assessment. This would empower enterprises to select the most suitable solution for their needs and budget from a wide array of sources. Meanwhile, these groups should cooperate with the government to promote long-term development of green technology.\n3. More discussions and engagement within the industry for stakeholders to express their needs\nDuring the study, enterprise respondents would occasionally mention the needs and difficulties in adopting green technology. This included a lack of channels for them to obtain required information and put forward actual demands.\nIn response to this, the GreenTech Alliance will actively organise discussion activities or provide online platforms for companies and research institutes to have more meaningful exchanges and discussions. This should improve transparency and information flow regarding green technology-related subject matter.\nMs Linda W P Ho, Chief Executive Officer of the Green Council and Convenor of the GreenTech Alliance said: ""This survey of enterprises and employees is the first formal study on the business sector\'s receptiveness of green technologies. Green technology is an inevitable part of our future. If an enterprise can become an early adopter and promote these technologies, it will greatly enhance the enterprise\'s competitiveness.\nWhether you are an employer or employee, the fact remains -- we are now all living in the age of technological innovation. Keeping up with the times will be key to staying competitive and relevant. Through this survey, the GreenTech Alliance hopes that this survey will help the government, environmental groups, professional bodies and industry professionals join hands to research and formulate the best plan possible to tackle Hong Kong\'s low rate of green technology adoption, and help local companies flourish -- in terms of both financial success and environmental protection.""\nAt the press conference, GreenTech Alliance also hosted several industry representatives to discuss the issues and prepare further recommendations for the government and industry professionals:\n*Ms Novam Ng, Marketing Director, IngDan\n*Mr Bruce Lee, Executive Director, INAX Technology Limited\n*Mr Jason Ngan, Chairman, Investment Committee and Smart Healthcare SIG of Smart City Consortium\nAbout GreenTech Alliance\nThe GreenTech Alliance is a non-profit initiative launched by the Green Council. It aims to unite all green technology pioneers and experts to develop ideas, improve communication channels and help promote the development of green technology. Official website: www.greentechalliance.hk\nAbout Green Council\nThe Green Council is a non-profit organisation and certification body established in 2000, with the aim to promote and assist the Hong Kong business community to integrate the concept of environmental protection into production processes and management systems, leading to a greener Hong Kong. With the motto of ""Conservation begins with Education"", the Green Council is fully committed to provide continued education and trainings on sustainable procurement, environmental management, waste management, energy conservation, etc.\nMeanwhile, the Green Council is dedicated to organise various green projects such as the Hong Kong Green Label Scheme, Hong Kong Green Purchasing Charter, Hong Kong Green Awards, International Coastal Cleanup Hong Kong, Hong Kong Green Day, Green Run, Green Carnival, etc. Official website: http://www.greencouncil.org', 'Your company has started to use Artificial intelligence (AI), but are you effectively managing the risks involved? It’s a new growth channel with the potential to boost productivity and improve customer service. However, particular management risks need to be assessed in cybersecurity. Start by considering AI trends to put this risk in context.\nWhy Is AI an Emerging Cybersecurity Threat?\nArtificial intelligence is a booming industry right now with large corporations, researchers, and startups all scrambling to make the most of the trend. From a cybersecurity perspective, there are a few reasons to be concerned about AI. Your threat assessment models need to be updated based on the following developments.\nEarly Cybersecurity AI May Create a False Sense of Security\nMost machine learning methods currently in production require users to provide a training data set. With this data in place, the application can make better predictions. However, end-user judgment is a major factor in determining which data to include. This “supervised learning” approach is subject to compromise if hackers discover how the supervised process works. In effect, hackers could evade detection by machine learning by mimicking safe code.\nAI-based Cybersecurity Creates More Work for Humans\nFew companies are willing to trust their security to machines. As a result, machine learning in cybersecurity has the effect of creating more work. WIRED magazine summarized this capability as follows: “Machine learning’s most common role, then, is additive. It acts as a sentry, rather than a cure-all.” As AI and machine learning tools flag more and more problems for review, human analysts will need to review this data and make decisions about what to do next.\nHackers Are Starting to Use AI for Attacks\nLike any technology, AI can be used for defense or attack. Researchers at the Stevens Institute of Technology have demonstrated that fact. They used AI to guess 25% of LinkedIn passwords successfully after analyzing 43 million user profiles in 2017. In the hands of defenders, such a tool could help to educate end users on whether they’re using weak passwords. In the hands of attackers, this tool could be used to compromise security.\nThe Mistakes You Need to Know About\nAvoid the following mistakes, and you’re more likely to have success with AI in your organization.\n1. You Haven’t Thought Through the Explainability Challenge\nWhen you use AI, can you explain how it operates and makes recommendations? If not, you may be accepting (or rejecting!) recommendations without being able to assess them. This challenge can be mitigated by reverse engineering the recommendations made by AI.\n2. You Use Vendor-provided AI Without Understanding Their Models\nSome companies decide to buy or license AI from others rather than building the technology in house. As with any strategic decision, there’s a downside to this approach. You can’t trust the vendor’s suggestions that AI will be beneficial blindly. You need to ask tough questions about how the systems protect your data and what systems AI tools can access. Overcome this challenge by asking your vendors to explain their assumptions about data and machine learning.\n3. You Don’t Test AI Security Independently\nWhen you use an AI or machine learning tool, you need to entrust a significant amount of data to it. To trust the system, it must be tested from a cybersecurity perspective. For example, consider whether the system can be compromised by SQL injection or other hacking techniques. If a hacker can compromise the algorithm or data in an AI system, the quality of your company’s decision making will suffer.\n4. Your Organization Lacks AI Cybersecurity Skills\nTo carry out AI cybersecurity tests and evaluations, you need skilled staff. Unfortunately, there are relatively few cyber professionals who are competent in security and AI. Fortunately, this mistake can be overcome with a talent development program. Offer your cybersecurity professionals the opportunity to earn certificates, attend conferences, and use other resources to increase their AI knowledge.\n5. You Avoid Using AI Completely for Security Reasons\nBased on the previous mistakes, you might assume that avoiding AI and machine learning completely is a smart move. That might’ve been an option a decade ago, but AI and machine learning are now part of every tool you use at work. Attempting to minimize AI risk by ignoring this technology trend will only expose your organization to greater risk. It’s better to seek proactive solutions that leverage AI. For instance, you can use security chatbots such as Apollo to make security more convenient for your staff.\n6. You Expect too Much Transformation from AI\nGoing into an AI implementation with unreasonable expectations will cause security and productivity problems. Resist the urge to apply AI to every business problem in the organization. Such a broad implementation would be very difficult to monitor from a security point of view. Instead, take the low-risk approach: apply AI for one area at a time, such as automating routine security administration tasks, and then build from there.\n7. Holding Back Real Data from Your AI Solution\nMost developers and technologists like to reduce risk by setting up test environments. It’s a sound discipline and well worth using. However, when it comes to AI, this approach has its limits. To find out whether your AI system is truly secure, you need to feed it real data: customer information, financial data, or something else. If all this information is held back, you’ll never be able to assess the security risks or productivity benefits of embracing AI.\nAdopt AI with an Eyes Wide Open Perspective\nThere are certainly dangers and risks associated with using AI in your company. However, these risks can be monitored and managed through training, proactive management oversight, and avoiding these seven mistakes.']"	['<urn:uuid:7986aa85-4e77-408f-84ff-f50b1c18ed46>', '<urn:uuid:75c20593-63bc-45e3-ac40-3a0820eb44e8>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T00:20:10.754925	12	104	2218
4	How do privacy rules differ between phone calls and public conversations when it comes to recording people, and what are the legal consequences of recording without consent?	In Oregon, for phone calls, only one person needs to consent to recording the conversation - this is why 911 dispatchers can legally record calls. However, for in-person public conversations, all participants must be informed that they're being recorded, even in public spaces where there's no reasonable expectation of privacy. If someone is not specifically informed about being recorded, it constitutes a crime. In Illinois, unauthorized recordings could be considered harassment, especially if used to threaten or intimidate others. According to Illinois law, harassment through electronic communications, which includes recorded communications, is typically a Class B misdemeanor for first offenses (punishable by up to 6 months in jail) and a Class A misdemeanor for second offenses (up to a year in jail).	"['In common law the jury has the power to decide the law according to the dictates of their conscience. The right of juries to judge both the law and the facts -- to nullify the law if it chose were a part of British constitutional law since the days of William Penn.\nIn Oregon, the Constitution states ""the jury shall have the right to determine the law, and the facts under the direction of the Court as to the law, and the right of new trial, as in civil cases."" [Art.1, Section 16]\nJuries in Oregon have the power to serve, in effect, as the final legislature of the land concerning the law in a particular case. Essentially, the law must conform with the understanding of a reasonable person.\nIf an electronic device is involved like a telephone, in Oregon, only one person needs to consent. This is why a 911 dispatcher can record the conversation. [City of Lake Oswego v. Mylander, 84 Or App 15, 733 P2d 455 (1987)]\nFurther, if you can intercept and record a police broadcast which is unscrambled and the public has free and ready access. This is because the statute does ""not apply to the receiving or obtaining of the contents of any radio or television broadcast transmitted for the use of the general public."" [STATE v. BICHSEL B70 854; CA A47766.]\nIn Oregon Statues, however, like many States, everyone in the conversation must be informed when their conversation is being recorded. If they are not specifically informed that their conversation is being obtained it is deemed a crime.\n[STATE v. KNOBEL86-454-M; CA A43491. 777 P.2d 985 (1989). 97 Or.App. 559. ]\nHere is where the arbitrary and ambiguous interpretation of the law begins to unfold. Without their knowledge or consent, you can record, including the police, over the phone or the radio. However, in public, where they have no reasonable expectation of privacy, they cannot be recorded without their knowledge?\nTo slightly re-frame it, it is okay to record with one person\'s permission what you might think is private on a phone or radio telecommunications. However, what you consider to be a public conversation with a public official, you must have the knowledge and permission of all involved. While you are being given orders by an officer who may be violating your rights, you are supposed to rouse yourself from the shock of intrusion to tell them ""I am recording your conversation with me.""\nThe statute does not prohibit taking or transcribing notes of a conversation, which are rarely accurate and subject to considerable error. However, if you have proof of the truth with a recording, it is .\nThe Oregon Supreme Court claims this is not over-broad nor unnecessarily over burden the First Amendment in its protection of the press\' need to gather news, yet admits the police may be biased in their version of the events. [State v. Knobel, 777 P. 2d 985 - Or: Court of Appeals 1989]\nPotentially, you could have a computer on the spot to parse the voice of a speaker without actually recording the voice of anyone involved. Of course, you are depending on the software to correctly interpret what is being said and that is never wholly accurate. So, then, the absurdity of inaccuracy inherent in transcribing is preferred over the best source of testimony - the original voice recording. How is the digital recording of voice materially different than taking down quotes, except for inherently having more accuracy?\nAt what point do the courts put the burden on public officials for what they say? When does Oregon, in fact, say, any governmental interest in protecting conversational privacy is not implicated when officials are performing duties in public places? When do we acknowledge the chilling effect of criminalizing public recordings unnecessarily restricts First Amendment rights to a free press?\n[Am. Civil Liberties Union of IL v. Alvarez]\n[Glik v. Cunniffe, 655 F.3d 78 (1st Cir. 2011]\n[733 F. Supp. 465: Connell v. Town of Hudson 1990]\nGazette Publishing Co. v. Cox, Cause No. IP 65-C-528 (S.D.Ind. May 2, 1967).\n[Channel 10, Inc. v. Gunnarson, 337 F. Supp. 634, 638 (D. Minn. 1972)]', 'What is Harassment?\nHarassing conduct in Illinois is knowing conduct which is not necessary to accomplish a purpose that is reasonable under the circumstances that would cause a reasonable person emotional distress and does indeed cause emotional distress to another. Generally, it is conduct which is intentionally directed at someone and annoys, threatens, intimidates, alarms or puts that person in fear of their safety.\nA person commits harassment through electronic communications when they use electronic communication for any of the following purposes (720 ILCS 5/26.5-3):\n- Making any comment, suggestion or request which is obscene with an intent to offend;\n- Interrupting, with the intent to harass, the telephone or electronic communication service of any person\n- Transmitting to any person, with the intent to harass, any file, document or communication which prevents use of the telephone or electronic communications device;\n- Transmitting or knowingly inducing a person to transmit an electronic communication for the purpose of harassing another person who is under 13 years of age, regardless of whether they consent to the harassment, if the defendant is at least 16 years of age at the time of the commission of the offense;\n- Threatening injury to the person or to the property of the person to whom an electronic communication is directed or to any of his or her family or household members; or\n- Knowingly permitting any electronic communications device to be sued for any of the purposes mentioned above.\nElectronic Communications may encompass any transfer of signs, signals, writings, images, sounds, data by wire, radio, electromagnetic, photoelectric or photo-optical systems, as well as communications by telephone, cellular phone, computer, pager, emails, instant messages, text messages or voice mail.\nMost first-offense harassment charges in Illinois are Class B misdemeanors, meaning that the maximum penalty is up to 6 months in the county jail. Second-offense harassment charges are generally Class A misdemeanors, which can be punished by up to a year in the county jail.\nIf an individual consents to the actions above, criminal liability is negated. However, they must be 13 years old or older. Therefore, if a person is 12 years old and consents to transmission of any of the above communications, and the sender is at least 16 years old, the sender may still be charged and convicted of Harassment through Electronic Communications. It would also be raised from a misdemeanor to a felony. A second, or subsequent, violation of this law mandates a minimum jail term of 14 days or 240 hours of community service.\nThe Illinois crime of Transmission of obscene messages is defined in 720 ILCS 5/26.5-1\nThe Illinois crime of Harassment by telephone is defined by 720 ILCS 5/26.5-2\nThe Illinois crime of Harassment through electronic communications is defined by 720 ILCS 5/26.5-3\nWhat can I do if I believe that I am being Harassed?\nIf some is simply being unpleasant, you should try your best to stay away from them and avoid all communication with them. If you continually feel threatened or unsafe (if someone, your first step should be to call the police. Oftentimes, this can stop the harassing behavior. If your issue is still not resolved, you can choose to seek legal help and/or a court Order of Protection. You can contact us for a free initial consultation to determine your options.\nWhat is defamation, libel and slander?\nIf you are the victim of defamation or harassment you need to understand the law. Certain elements need to be met, and these vary based on the type of help you are seeking. There is often confusion between defamation, libel and slander. There are also differences when false statements are being made online, although courts treat defamation on the internet similar to offline defamation.\nFirst, what is Defamation?\nDefamation is an unprivileged false statement of fact which harms the reputation of a person or a company. It encompasses both libel and slander. It is any falsehood, spoken or written, that is meant to incite hate or contempt toward another party.\nSo what is libel?\nWhen defamation is in written form, such as on a website or a comment on an article, website, bulletin board, or post online, it is termed libel. This could also include negative reviews or blog posts.\nWhat is slander?\nDefamation that is not in written form falls under the label of slander. This could encompass such things as podcasts, audio files, transcribed videos, etc – it is spoken.\nCan I stop someone from spreading negative things about me?\nThere are certain elements that need to be met in order to be able to proceed with a defense against defamation.\nFirst, it will need to be proved that there was a publication to one other than the person defamed, that is understood to be concerning the person being defamed. The statement must be proven to have been made to a third party.\nWe will also have to prove that the statement is a false statement of fact. It cannot merely be an opinion. A fact can be proven to be true or false, unlike opinions. Opinions are typically not actionable as defamation. Keep in mind that merely labeling a statement as being an “opinion” does not make it so. Courts look at whether a reasonable reader or listener could understand the statement as being a proven fact or falsity – a statement of verifiable fact.\nIn addition to being false, the statement of fact must harm your reputation or the reputation of your company or business.\nThe false statement that is causing harm must be made without adequate due diligence or research into the truthfulness of the statement. Or, you can attempt to prove that the false statement of fact was made with full knowledge that it is false.\nThere is an additional requirement of proving actual “malice” if you are a celebrity or public official – you will need to prove that the person spreading the information intended to cause harm or acted with reckless disregard of the truth in making the statements. A private person claiming defamation only has to prove that the offender acted negligently – as in, a “reasonable person” would not have published the defamatory statement. A public figure will need to show that the offender published with knowledge of the falsity or in reckless disregard for the truth.\nIllinois is a Per Se State. This means that you can sue an offender without having to prove damages to your reputation or economic well-being if the defamation is inherently slanderous or libelous. For example, stating that the plaintiff is engaging in criminal activity, being a danger to people, calling into question the plaintiff’s ethics, or stating that they are engaging in sexual immorality. These are defamatory per se because the harm to the plaintiff’s reputation is inherent in the statement.\nWho are public figures?\nPublic figures are people who are actively sought to influence the resolution of a matter of public interest. This would include government employees, city attorneys, senators, presidential candidates. It also includes someone who may be a limited purpose public figure – someone who voluntarily participates in a discussion about a public controversy and has access to the media to get his or her own view across. People can become involuntary limited-purpose public figures, such as an air traffic controller on duty at the time of a fatal crash, due to their role in a major public occurrence or members of an activist group who spoke with reporters at public events. Corporations are not always public figures, they are judged by the same standards as individuals.\nIn Illinois, the standard deadline for filing a defamation lawsuit in Illinois is one year after the defamatory statement is made, according to 735 ILCS 5/13-201, which sets this deadline for the filing of any civil action seeking a remedy “for slander, libel or for publication of matter violating the right of privacy.” If you are under 18 at the time the statement was made or under a legal disability, you will have two years to file the lawsuit upon reaching the age of 18 or having the legal disability removed (735 ILCS 5/13-211). If the person who made the statement was out of state for any period of time beginning on the date the statement was made, their absence will not be counted as part of the one-year filing period. (735 ILCS 5/13-208) Illinois recognizes the Single Publication Rule for Statue of Limitations purposes.\nWhat if someone is accusing me of defaming them?\nTruth, opinion, privilege and fair reporting are viable defenses in Illinois defamation cases.\nWebsites and webmasters are not held responsible for defamatory third-party content, under Section 230 of the Communications Decency Act.\nCease and Desist Letters\nCease and desist letters are so popular because of the variety of circumstances in which they may be used, and the weight they carry with those who receive them. These letters are the best way to make a formal request for someone to stop an activity that you believe is violating your legal rights. Cease and desist letters provide a clear notice – stop your unlawful behavior or face consequences in a court of law. The most common uses for using a cease and desist letter are outlined below.\n- Stop Trademark and Copyright Infringement\n- Stop Invasions of Privacy, Stalking and Harassment\n- Stop Libel or Slander\n- Stop a Nuisance\n- Stop Breach of Contract\n- Stop Property Boundary Encroachment\n- Stop Debt Collectors\nA cease and desist letter is a “demand letter” sent to an individual or business by a third party who believes that their legal rights have been infringed. Failure to comply with a cease and desist order is punishable by the courts, although a cease and desist letter by itself is not a guarantee of a lawsuit. If you are the recipient of a cease and desist letter, an attorney can help counsel and guide your response efforts. They will analyze the demand with your position in mind in order to determine whether the claim is frivolous or legitimate. This analysis guides a response strategy, which can range from a simple response detailing your denial of claims; filing a declaratory judgment, or steering negotiations and settlement in your favor.']"	['<urn:uuid:5c852fb8-d5d6-4a46-af38-decfb0f2079a>', '<urn:uuid:89908fb9-30d3-4e30-97f5-2c0d44f48d4f>']	open-ended	direct	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-13T00:20:10.754925	27	122	2409
5	accuracy improvements compare traditional survey stakes vs 3d modeling in crime scene and construction	3D modeling provides better accuracy than traditional survey stakes in both domains. In construction, 3D modeling with GPS and machine control eliminates the need for ongoing grade checking and allows precise automated adjustment of equipment, increasing efficiency by 15-25%. Similarly in crime scenes, 3D modeling enables more accurate analysis and simulation of the criminal process through precise digital reconstruction, replacing traditional manual documentation methods with advanced techniques like laser scanning and image-based modeling.	"['The reconstruction of crime scene plays an important role in digital forensic application. This article integrates computer graphics, sketch-based retrieval and virtual reality (VR) techniques to develop a low-cost and rapid 3D crime scene presentation approach, which can be used by investigators to analyze and simulate the criminal process. First, we constructed a collection of 3D models for indoor crime scenes using various popular techniques, including laser scanning, image-based modeling and geometric modeling. Second, to quickly obtain an object of interest from the 3D model database, a sketch-based retrieval method was proposed. Finally, a rapid modeling system that integrates our database and retrieval algorithm was developed to quickly build a digital crime scene. For practical use, an interactive real-time virtual roaming application was developed in Unity 3D and a low-cost VR head-mounted display (HMD). Practical cases have been implemented to demonstrate the feasibility and availability of our method.\n Bevel T, Gardner RM. (2008). Bloodstain pattern analysis with an introduction to crime scene reconstruction: CRC Press.\n Bostanci E. (2015). 3D Reconstruction of Crime Scenes and Design Considerations for an Interactive Investigation Tool. Computer Science. 896-900.\n Buck U, Naether S, Räss B, Jackowski C, Thali MJ. (2013). Accident or homicide–virtual crime scene reconstruction using 3D methods. Forensic science international. 225:75-84.\n Clair ES, Maloney A, Schade A. (2012). An Introduction to Building 3D Crime Scene Models Using SketchUp. Journal on the Association for Crime Scene Reconstruction. 18:29-47.\n Cignoni P, Callieri M, Corsini M, Dellepiane M, Ganovelli F, Ranzuglia G. (2008). MeshLab: an Open-Source Mesh Processing Tool. Eurographics Association. 129-136.\n Ebert LC, Nguyen TT, Breitbeck R, Braun M, Thali MJ, Ross S. (2014). The forensic holodeck: an immersive display for forensic crime scene reconstructions. Forensic science, medicine, and pathology. 10:623-6.\n Eitz M, Richter R, Boubekeur T, Hildebrand K, Alexa M. (2012). Sketch-based shape retrieval. ACM Trans on Graphics. 31: 1-10.\n Fisher M, Ritchie D, Savva M, Funkhouser T, Hanrahan P. (2012). Example-based synthesis of 3D object arrangements. ACM Transactions on Graphics. 31-135.\n Gee AP, Escamilla-Ambrosio PJ, Webb M, Mayol-Cuevas W, Calway A. (2010). Augmented crime scenes: virtual annotation of physical environments for forensic investigation. Proceedings of the 2nd ACM workshop on Multimedia in forensics, security and intelligence. 105-10.\n Gibson S, Howard T. (2000). Interactive reconstruction of virtual environments from photographs, with application to scene-of-crime analysis. ACM Symposium on Virtual Reality Software and Technology. 41-8.\n González-Jorge H, Zancajo S, González-Aguilera D, Arias P. (2015). Application of Kinect gaming sensor in forensic science. Journal of forensic sciences.60:206-11.\n Hołowko E, Januszkiewicz K, Bolewicki P, Sitnik R, Michoński J. (2016). Application of multi-resolution 3D techniques in crime scene documentation with bloodstain pattern analysis. Forensic science international. (267):218-27.\n Howard TL, Murta AD, Gibson S. (2000). Virtual environments for scene of crime reconstruction and analysis. Electronic Imaging: International Society for Optics and Photonics. 41-48.\n Jancosek M, Pajdla T. (2011). Multi-view reconstruction preserving weakly-supported surfaces. Computer Vision and Pattern Recognition (CVPR), 3121-3128.\n eTeks. (2017). Homepage of Sweet home 3D, Retrieved on October 21, 2017 from http://www.sweethome3d.com/.\n Leipner A, Baumeister R, Thali MJ, Braun M, Dobler E, Ebert LC. (2016). Multi-camera system for 3D forensic documentation. Forensic science international. 261:123-8.\n Maksymowicz K, Tunikowski W, Kościuk J. (2014). Crime event 3D reconstruction based on incomplete or fragmentary evidence material–Case report. Forensic science international. 242:e6-e11.\n Ma M, Zheng H, Lallie H. (2010). Virtual reality and 3D animation in forensic visualization. Journal of forensic sciences. 55:1227-31.\n Qian L, Fan Y, Zhou M, et al. (2016). Manifold Ranking for Sketch-Based 3D Model Retrieval. Transactions on Edutainment XIII. Springer Berlin Heidelberg,149-164.\n Sansoni G, Cattaneo C, Trebeschi M, Gibelli D, Poppa P, Porta D, et al. (2011). Scene-of-Crime Analysis by a 3-Dimensional Optical Digitizer: A Useful Perspective for Forensic Science. The American journal of forensic medicine and pathology. 32:280-6.\n Tung ND, Barr J, Sheppard DJ, Elliot DA, Tottey LS, Walsh KA. (2015). Spherical Photography and Virtual Tours for Presenting Crime Scenes and Forensic Evidence in New Zealand Courtrooms. Journal of forensic sciences. 60:753-8.\n Zobel J, Moffat A. (2006). Inverted files for text search engines. ACM computing surveys (CSUR). 38:6-18.\nRen, Pu; Shui, Wuyang; Liu, Jin; Fan, Yachun; Zhao, Wenshuo; and Zhou, Mingquan\n""A Sketch-based Rapid Modeling Method for Crime Scene Presentation,""\nJournal of Digital Forensics, Security and Law: Vol. 13\n, Article 8.\nAvailable at: https://commons.erau.edu/jdfsl/vol13/iss1/8', 'Technology is transforming nearly every industry, and construction is no exception. One form of tech that has recently had a substantial impact on the construction industry is three-dimensional (3D) modeling. 3D models have a major role in modern construction projects, as they can improve productivity and ease of work.\n3D modeling for earthworks and machine control can increase equipment operation accuracy, enhance worksite efficiency and reduce costs, among other benefits. So, how does this technology work, and how can you apply it to your next project?\nWhat Is 3D Modeling?\nThe term “3D modeling” refers to the process of creating a three-dimensional representation of an object using specialized software. This representation, called a 3D model, can convey an object’s size, shape and texture. You can create 3D models of existing items, as well as designs that have not yet been built in real life.\nIn construction, 3D models of a worksite can be used for machine control. These replicas incorporate the points, lines and surfaces that make up the physical environment. They use coordinate data that identifies the location of horizontal and vertical points relative to a reference point. Due to these spatial relationships, you can view the representation from various angles.\nMachine control uses various positioning sensors to provide machine operators with feedback on things like target grades and bucket or blade position. The machine operators can reference the 3D model to ensure they are completing work accurately. GPS technology enables workers to locate the replica’s points in the field, and sensors on machines tell them where they are relative to the model’s points.\nThese control processes help crews translate the 3D model into reality by guiding equipment to construct the lines, points and surfaces precisely as described in the representation. Teams may also use 3D models for project, design and environmental compliance reviews. These models also help during pre-bidding, allowing contractors to test out various designs and communicate ideas.\nThe History of 3D Modeling\nThe methods and technologies used today for 3D earthworks modeling would not exist without developments in civil surveying and various types of 3D modeling.\nYou can trace the history of 3D earthworks modeling back to ancient times. Ancient Egyptians constructed the pyramids with early surveying techniques and used geometry to re-establish farmland boundaries after flooding along the Nile River. In ancient Rome, civil surveying became a recognized profession, and surveyors created measurement systems to evaluate and create records of conquered lands.\nEuclid, who is known as the founder of geometry and lived in ancient Greece, developed ideas that inspired many modern surveying and 3D modeling techniques. Many years later, in the 1600s, French mathematician Rene Descartes invented analytic geometry — also called coordinate geometry — which is foundational to 3D earthworks modeling.\nMoving forward to the 18th century, European surveyors discovered they could use various angle measurements taken from different areas to identify a precise location — a technique known as triangulation. New surveying tools, such as measuring wheels, circumferentors, Kater’s compasses and Gunter’s chains, began to gain popularity. Meanwhile, English mathematicians James Joseph Sylvester and Arthur Cayley developed matrix mathematics, which is what enables today’s computer-generated images to display reflections or light distortions.\nLater, surveyors began to use steel bands and invar tapes. These tools eventually gave way to technologies such as electromagnetic distance measurement (EDM) and global positioning satellite (GPS) equipment. Surveyors switched from compasses to theodolites, which measured horizontal and vertical angles using a rotating telescope. They then transitioned to using total stations, which are electronic transit theodolites equipped with EDM technology. These advancements enable them to measure both angles and distances.\nThen, the first commercially available computer-aided design (CAD) systems — which turn survey data into visual representations — were released. The first 3D graphics company, Evans & Sutherland, appeared in 1968. Over the next several decades, CAD programs became more advanced and more widely available.\nIn the machine control field, users began shifting from the use of survey stakes — which surveyors manually set up, and machine operators read visually — to 3D modeling. Various technologies came together to enable 3D earthworks modeling, including:\n- CAD, which turns survey data into a 3D model.\n- GPS, which allows engineers to pinpoint precise locations.\n- Light Detection and Ranging (LiDAR), a remote sensing technology that uses a pulsed laser to measure variable distances.\n- Aerial photogrammetry, which enables engineers to extract topographical data from aerial photographs taken by drones.\n- Point-cloud modeling, which involves using laser scanning technology to create a set of three-dimensional data points used to create a model.\nWhat Are 3D Models Used For?\n3D replicas are a prevalent form of technology, but what industries use 3D modeling? Many sectors use 3D modeling for numerous purposes. Some concepts include:\n- Planning buildings using architectural visualization.\n- Giving 3D tours in the real estate sector.\n- Creating video games and movies.\n- Conducting academic research.\nModels have several uses in construction as well, and new techniques are always emerging. Here are a few ways 3D models are used in construction:\n1. Machine Control\n3D modeling enables more accurate, efficient and cost-effective machine control. Instead of using traditional survey stakes, machine operators can see the job site on a screen while in the cab. A system of sensors guides the machine based on the 3D model’s measurements.\nEquipment such as excavators, backhoes and bulldozers are equipped with on-board computers, and the blades and buckets include GPS devices. You can either set up a GPS base station at the worksite or subscribe to a GPS service. Whichever system type you choose, it will communicate with the receivers on your machines.\nThe 3D model is referenced to GPS coordinates and loaded onto your equipment’s on-board computers. These computers can then communicate with GPS receivers and machinery controls. As the device moves throughout the site, the GPS records where it is located at all times. As the blades and buckets on your machinery move, the GPS pinpoints their position.\nThe computer can automatically adjust the blades or buckets to the required excavation depths or surface elevations. This ability enables smooth, accurate grading of roads, sidewalks and parking lots and more.\n2. Site Layout\n3D models can also be useful for communicating site layout, including the location of utility equipment and landscape elements.\nYou can map the location of electrical equipment, for example. That can include electrical service slabs, light poles and connections for signs, kiosks, decorations and other electrically powered elements. A 3D model helps electricians set these connections up quickly and accurately.\nYou can also use 3D mapping technology to map other utilities, including gutters, water and wastewater piping, natural gas lines and more. Charting the layout of utilities gives crews more confidence about their placement and provides them the information they need to place this equipment at any time.\nA 3D model can also include elements such as landscaping, curbing, benches and nearly any other site feature. Accessories such as benches and playground equipment require a base and connection. Knowing where these elements will go can enable crews to prepare them earlier in the process and avoid re-digging later.\n3. Progress Reports and As-Builts\n3D models can also be useful for communicating project progress and creating as-builts, which are revised drawings submitted at a project’s completion. You can gather new data throughout an assignment to create updated 3D models, showing what the site currently looks like. A 3D model created after a project ends can be used throughout the lifecycle of the facility for purposes such as maintenance, operations and asset management.\nBenefits of Using 3D Models for Earthworks\nUsing 3D models for earthworks and machine control can provide numerous advantages, including:\n- Increased plan accuracy: Creating 3D models uncovers conflicts, inconsistencies and other issues in plans before construction begins, which reduces rework and costs.\n- Increased accuracy in the field: Because the machines have the same data the surveyor does, machine operators have an easier time following project plans. Workers won’t have to rely solely on contours when navigating a worksite. The 3D replica’s surface is also built to the landscape’s actual vertical and horizontal geometry.\n- Lower surveying costs: Using 3D modeling eliminates the need for ongoing grade checking, which reduces surveying costs. Having lower surveying costs can help you win more jobs and earn higher revenue over time. The additional money can also allow you to upgrade equipment and hire employees as your company expands.\n- More efficient machine operation: Machinery operates more efficiently because it moves precisely according to the 3D model’s measurements. 3D modeling helps you accomplish more with your equipment in less time. The increased efficiency also reduces fuel, repair and maintenance costs.\n- Lower raw materials costs: 3D modeling techniques help you hit the mark the first time around and use materials more effectively. This enhanced productivity reduces raw materials costs because you’ll need fewer supplies for each job. This benefit is sustainable and cost-effective.\n- Reduced labor costs: With 3D machine control modeling, many of the machine operator’s duties are automated, which helps them work more quickly and make fewer errors — this quality increases individual worker efficiency, reducing labor costs.\n- Improved communication: You can use 3D models to communicate project information in an approachable, visual way with various stakeholders. If everyone has a common understanding of the material, they’ll have a smoother time sharing ideas and suggestions.\n- Increased number of uses: You can set up the data once and then use it for various purposes, including grading, utilities and hardscaping. You can also make adjustments to the information as needed for subsequent assignments.\n- Reduced project costs: Using a 3D model can reduce project costs by a total of four to six percent, according to a report by the U.S. Department of Transportation’s Federal Highway Administration. In earthmoving alone, 3D models can increase efficiency by 15 to 25 percent.\nHow Are 3D Models Created?\nTo create a 3D model, you must first gather survey data. You can accomplish this by using various technologies, including LiDAR and aerial photogrammetry. The initial survey records the locations of physical features and key points, which serve as a baseline. You can then scan the area using LiDAR technology to create data point clouds representing the physical components of a site. These point clouds combine with 3D modeling software to build the 3D representation.\nWhen Take-Off Professionals receives the survey data files for a project, we first ensure we have all the necessary information about the job requirements and the scope of work for which our customer is responsible. We then build the 3D model based on the plans we receive. During this process, we adjust errors in the designs and take notes about potential changes.\nOnce we have completed the 3D model to plan, we alert the engineers to any areas of concern and propose fixes as needed. We continue to revise the model and suggest changes until every detail is correct.\nTo begin a 3D modeling project, we need three things:\n- CAD files: You can ship us your CAD files or upload them to our site. We can use various file formats, including industry-standard formats such as .DWG and .DXF within AutoCAD, plus numerous proprietary formats. We can process any kind of CAD package from Carlson Construction, AutoCAD, Micro Station and others.\n- Paper plans: We also need either physical paper plans or scans of paper plans. You can upload scanned files or send them to us on a CD. Keep in mind that it is often cheaper to ship rather than scan.\n- Work order: You will also need to fill out a work order, which will include details about the project’s scope. You can submit a work order through our website.\nSome of the elements that may be included in a 3D model for machine control, depending on the project, include:\n- Parking lot surface\n- Roads with vertical and horizontal alignment information\n- Subgrade road model that extends beyond the back of the curb\n- Large islands and building area curbs\n- Small island curbs with grading\n- Building pads, including blow-ups if requested\n- Retention and sheet grading areas\n- 2D linework of utilities or full 3D utility layout\n- Existing conditions\n- Points for the layout of objects built for the surface, such as buildings and curbs\nWork With a 3D Model Expert\nAt Take-Off Professionals, we create approximately 1,000 machine control models for our clients every year. We employ a team of engineers and technical staff who are experts in building 3D models for the construction industry, and we don’t use subcontractors like many of our competitors do. We have groups working across all four major time zones in the U.S. to ensure we’re always there for our clients.\nWe’ve been in business for more than two decades and have established a reputation for timeliness, accuracy, attention to detail and excellent customer support. We’ve also created an exclusive platform that our clients can use to upload their files in a secure, user-friendly environment. This additional measure ensures placing a work order is fast and easy.\nLearn more about how our data and modeling services can help you win more bids, reduce your costs and complete projects accurately and efficiently by contacting us today.']"	['<urn:uuid:1e0040c0-8e3a-4485-8d1d-58a786b62b73>', '<urn:uuid:a9c292b6-9f24-485c-9a12-e470b591bd8a>']	factoid	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-13T00:20:10.754925	14	73	2926
6	I'm curious about boiling water and cooking - if I'm making quinoa, does putting a lid on the pot really make the water boil faster, and how long should I expect to wait?	Yes, water does boil faster with a lid because it prevents evaporation and condensation, allowing heat to transfer more efficiently to the water. Generally, water takes about 10 minutes to reach its boiling point of 212°F at sea level, though this can vary depending on altitude and whether you're using a lid. Once you've got your water boiling, for quinoa specifically, you'll want to use a 2:1 ratio of water to quinoa, bring it to a boil, then cover and simmer for 15 minutes until the water is absorbed.	['In this brief guide, we will answer the question, “what is the boiling point of water in Fahrenheit?” and discuss whether the boiling point of water changes with altitude, and what method of heat transfer is boiling water?\nWhat is the boiling point of water in Fahrenheit?\nAccording to the Fahrenheit scale, water boils at 212° F.\n212 degrees Fahrenheit is the boiling point of water. At this temperature, water will begin to boil and turn into steam.\nBoiling water is a process that occurs when water reaches its boiling point. Water begins to boil at 212 degrees Fahrenheit (100° C). The boiling point of water is a physical property of water that determines its temperature at which it turns from a liquid into a vapor and begins to escape as vapor through evaporation.\nA boiling liquid has reached its boiling point when it starts to expand rapidly and violently, forming bubbles on the surface. This boiling takes place because the molecules in the liquid are moving faster than they can remain dissolved in the liquid; this leads to an increase in kinetic energy.\nWhen enough energy has been added, these molecules break free from one another and move around independently until they eventually leave by vaporization or phase change during or just after boiling.\nDoes the boiling point of water change with altitude?\nThe boiling point of water does change with altitude, but not in the way you might think. The reason for this is that the pressure of air at higher altitudes decreases as you move up in altitude. As a result, water takes more time to boil than it would at sea level.\nWhich boils faster: Water or milk?\nMilk will boil faster.\nMilk will boil faster than water because it has a higher heat capacity. This means that the amount of energy required to create a change in temperature for milk is lower than it is for water. So, milk will boil faster than water.\nWhat is the boiling point of a liquid?\nThe boiling point of a liquid is the temperature at which the vapor pressure of the liquid equals atmospheric pressure.\nThe boiling point of a liquid depends on its chemical composition, molecular structure, and physical properties such as temperature and pressure.\nWhat is the vaporization and vapor pressure of water?\nVaporization is the process of changing a liquid into a gas. Vaporization occurs when you heat something up until it reaches its boiling point, and then release it from its container. The vapor pressure of a substance is the pressure exerted by its vapors at equilibrium.\nHow to boil water?\nHere is a simple way to follow in order to boil water:\nFirst, put your pot on the stove. Then, make sure that it is turned off. Next, fill your pot with water until it is about halfway full.\nTurn on your stove and let it heat up. The water should start bubbling after about five minutes, and you can adjust how much time you give the pot by how large or small your pot is. Once the water has reached a rolling boil (it should start bubbling vigorously), reduce the heat to low and let it simmer for at least 2 minutes before removing it from heat.\nWhat method of heat transfer is boiling water?\nBoiling water is both conduction and convection.\nBoiling water is a very good example of a heat transfer method because it combines both conduction and convection.\nHow long does water take to boil?\nWater boils at 212 degrees Fahrenheit. Because of this, it takes approximately 10 minutes for the water to boil at sea level.\nWater takes about 10 minutes to boil at 212°F. The actual amount of time it takes for water to reach its boiling point depends on the altitude and atmospheric pressure, and whether or not it is being heated in a pot with a lid.\nWater boils at 212°F when all of the following are true:\n- The water’s temperature is 212°F\n- It has reached boiling point\nDoes water boil faster with a lid?\nYes, water does boil faster with a lid.\nWhen you put a lid on your pot, it prevents evaporation and condensation from taking place. This means that the air inside the pot is more tightly packed than it would be if there were no lid, which means that more heat can be transferred to the water.\nIn this brief guide, we have addressed the question, “what is the boiling point of water in Fahrenheit?” and discussed other questions related to the subject, such as does the boiling point of water change with altitude, and what method of heat transfer is boiling water?', 'Quinoa On A Renal Diet\nLearning more about the variety of starches out on the market is invaluable. Quinoa, pronounced (Keen – wah) is a versatile product that is actually a seed. Looking at it for a renal diet, you can see that it has many of the desired properties we want in a meal.\n1 cup dry quinoa, has 626 calories, 24 gm protein, 109.1 gm of carbohydrate, and 12 gm fiber. You should realize that it expands to 4 times it’s size when cooked! So, 1 cup dry = 4 cups cooked quinoa! Wow, that really makes for a full stomach on just a little bit of food. You could eat 1 cup quinoa for 160 calories, 6 gm protein, 28 gm carbohydrate and 3 gm of fiber. That is a healthy side dish, and you don’t even need to add meat! As a person on a renal diet, if you need to restrict protein, you can easily eat this and add some low potassium vegetables to the meal and you are set for a filling dish that didn’t go over your needs but filled you up. And you can feed this to your whole family.\nA Simple Introduction To Quinoa\nThis is a food that is both old and new; as an ancient staple and a rediscovered foodie favorite. If you have noticed quinoa at all, you know what everyone is saying about it. Can it be that quinoa is the perfect food for you and your family? Could quinoa be the one new ingredient you try this year that changes the way you cook? Quinoa is all that and a powerhouse of nutrition, flavor and texture. Let’s take a brief look at this super-food and why it is quickly becoming all the rage.\nWhat is Quinoa?\nThose who are not yet familiar with quinoa, may think of it as a grain, or at least a replacement for grains. The little tiny disc is actually a seed of a plant in the same family as beets, chard, and spinach. These nutritious, amino acid rich seeds are light and fluffy when cooked, with a little snap to it. You’ll also find quinoa in a variety of beautiful colors such as gold, red, and even black.\nWhile relatively new to the US market, quinoa has been cultivated in Peru, Chile, and Bolivia for over 50 centuries (that’s right, I said centuries!) and is a staple food in their diets. The Incas considered quinoa a sacred food and referred to it as the “mother seed,” which is why we often refer to it as “Gold of the Incas.”\nWhen Spanish conquistadors were trying to gain control of the South American indigenous people, they destroyed the fields in which quinoa was grown, and outlawed the farming and sale of quinoa. In1980, two Americans re-discovered the health and nutrition potential of quinoa and started cultivation in Colorado. Today, quinoa is finding its way into homes and restaurants all over the map.\nFor being such a tiny little seed, quinoa is a complete protein, containing all nine essential amino acids. Quinoa is also a great source of manganese, magnesium, folate, phosphorus, and lysine, which is essential for tissue growth and repair. The minerals contained in relatively high amounts may also be especially helpful for those people who have consistent migraines, diabetes, and atherosclerosis.\nOne of the most highly valued aspects of quinoa for many people is it is gluten free. Those looking for alternatives for wheat and other gluten foods can turn to quinoa in several forms to replace the gluten in their diets.\nHow to Eat Quinoa\nThe quinoa seeds are naturally covered by a saponin residue that is bitter to the palate. This is one defense mechanism the plant has to fend off the occasional passing critter that wants a snack. While commercial cultivation processes remove much of the saponin that coats the seed, it is still a good idea to rinse the seeds in cold water to make sure the process is complete. However, there are many brands that are pre-rinsed. You can use your own judgment.\nOnce you have the quinoa rinsed, bring a pot of one part quinoa and two parts water to a boil, cover, and simmer slowly for fifteen minutes, or until the the water is absorbed and the quinoa is tender. If you want to keep more of the natural nutty flavor, you can dry roast the seeds before cooking them. Put the quinoa in a skillet over medium heat and toss, just until the quinoa becomes fragrant.\nNow you have a multitude of options for preparing your quinoa. Whether you like it hot or cold, you can put it into a salad or in a soup. You can also form your cooked quinoa into patties with a variety of ingredients. Go ahead and add cooked quinoa to your favorite pancake or muffin recipe for a brand-new take on healthy eating.\nNo matter if you are looking for a gluten free alternative to grains, or you are a person on a renal diet who is just looking for something new to experiment with, or you want to give your nutrition a real boost, give quinoa a try. With all the possibilities, you could easily create a quinoa creation for every day of the year!']	['<urn:uuid:c133e24d-07e2-4913-8c4a-204f7289bf69>', '<urn:uuid:acf9ef1f-2532-450c-94b2-682d3d88094c>']	open-ended	with-premise	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-13T00:20:10.754925	33	89	1673
7	what clean photography lens items carry	For cleaning camera and lens, you need a hurricane blower for dust removal at home, lens-cleaning tissues for use at home, and two microfibre cloths for use during walks and hikes.	['Lightweight mirrorless cameras need accessories to match. Expert mirrorless photographer David Cleland opens up his kit bag and gives us some tips on what to carry…\nFujinon 35mm, ISO 200, 1/220, f/1.8. All images by David Cleland\nIf you have just invested in your first mirrorless camera you may find you are embarking on a journey of lightweight, liberating, portable photography.\nWhether this is your first camera or you have moved to mirrorless from a DSLR, you are probably taking a fresh look at your accessories with the focus on travelling light. Here’s what I think you’re best off carrying.\nA good bag\nBillingham Hadley Pro Original\nCamera bags are a strange phenomenon, and many photographers I know have at least two bags in their collection. I think camera bags are the most often rotated and replaced camera accessory in anyone’s list – they all offer different functions and what works for hiking doesn’t necessarily work for street photography.\nThink hard about what you need and balance fashion with function. Do your homework – if you are a walker you might find a backpack bag is the ideal, whereas a messenger bag may be preferable for everyday work. My tip is to make sure there is enough room for your non-camera equipment: phones, wallets, waterproof trousers, food, etc. A quality bag is a worthwhile investment and will last a lifetime.\nBatteries in a mirrorless camera system by their very portable nature are small, and you may find you get fewer exposures per charge than with a DSLR. I personally hate having to think about the battery meter so I pack three batteries for each shoot, one in the camera and two fully charged spares in the bag. I have one pocket in my bag for charged batteries and another especially for depleted packs. I habitually stick with the official branded batteries or alternative brands purchased from reputable dealers such as WEX.\nA good, lightweight tripod\nMy advice is to avoid cheap plastic tripods. Although an aluminium or carbon fibre tripod may feel expensive at the time, a few years down the line it will still be holding your camera steady in the extremes.\nFujinon 14mm, ISO 12,500, f/2.8, 30 seconds\nAlthough you shouldn’t underestimate the quality of your supplied kit lens, you will eventually want to start growing your collection. Many photographers prefer prime lenses over zooms – prime lenses offer a fixed focal length i.e. you can’t zoom (although you have heard photographers talking about zooming with their feet). Prime lenses are generally faster (meaning they offer a larger maximum aperture) and some would claim they offer better image quality over zoom lenses.\nThat said, modern zoom lenses offer a full range of focal lengths, which is particularly useful if you want to travel light (one lens is lighter than two or three). A higher-end zoom lens will offer a constant aperture throughout the zoom range, for example the new award-winning Fujinon 16-55mm has a constant f/2.8 aperture, so it is just as fast at 16mm as it is fully zoomed in at 55mm.\nFujinon 18-135mm, at 18mm, f/4.5 ISO 400, 1/6400\nThere generally isn’t one lens that suits every need, so consider how you want to use your camera and research the best lens for the job. The images in this post all use a different type of lens to offer a few examples. It is worth looking at your Lightroom library to see the type of image you tend to shoot most often and using this information for the basis of lens investment.\nA lens hood\nFuji X100 / X100S Lens Hood with Adapter Ring\nMost lenses now come with a lens hood attachment. These are great for reducing glare but also protect the end of the lens when walking in the mountains. If you are an X100 user then the lens hood attachment is well worth the investment.\nLens cleaning kit\nI tend to use just three products in my camera and lens cleaning process. I have a hurricane blower that remains at home for removing dust from both my camera and lenses. Secondly a packet of lens-cleaning tissues – like the blower, these remain at home and are used generally before I head out on a landscape shoot. Finally I take two microfibre cloths with me on walks and hikes. I take two, not because I lose them but they tend to get wet in the mountains so a backup is always useful.\nAs you develop your photography skills you will definitely want to invest in some neutral density filters, especially if you are shooting landscape or long exposure photography.\nThere are two key types of filter to consider. The screw-on type such as the Tiffen ND filter, which screw directly to the front of the lens. These can offer great value for money if you only plan to use a single lens for landscapes, but it is important to remember that lenses tend to come with a range of different filter sizes, so a screw-on filter will be limited to just one size.\nFujinon 14mm, f/10, ISO 200, 1/80\nThe second type of filter involves using a holder into which the filter (or filters) will slide. One such example is the Lee Seven5 range. This filter system tends to be initially more expensive, but the big advantage is that the same filters can be used with all your lenses with the purchase of a low-cost filter ring. I currently use the Seven5 system, and when I buy a new lens I also buy the appropriate filter ring, costing around £25. It is often a bonus to find that some lenses have the same filter size, such as the Fujifilm 16mm and Fujifilm 18-135mm (both 67mm), so the one filter ring works with both.\nA good strap\nI rarely use the manufacturer’s camera strap with my camera as it is definitely more comfortable to carry it over my shoulder when walking in the mountains. Blackrapid offer a massive range of sling and wrist straps ideal for walking with the camera but also allowing instant access. The company have also released a backpack attachment ideal for hiking that will convert your back into a sling strap.\nFujinon 27mm ISO 200, 1/2900, f/2.8\nI think there are many photographers who, when they aren’t out taking photos, are quietly reading reviews and investigating their next purchase. Investing in the right gear can result in a lifetime of service. It is all about doing your homework, understanding your needs, and investing in the right solution.\nAbout the Author\nDavid is a documentary and landscape photographer covering everything from dramatic long-exposure landscape photography through to live music. David is also an official Fujifilm X photographer. You can see more of his work on his website or follow him on Twitter.']	['<urn:uuid:25f594e1-8b3e-40ec-bac1-f78e8e2e8b10>']	factoid	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-13T00:20:10.754925	6	31	1140
8	What's larger: Khufu pyramid's workforce housing or underground tunnels of Djoser?	According to the documents, Djoser's pyramid has substructure tunnels and passages totaling about three and a half miles in length. While worker towns at pyramid bases included bakeries and medical centers, their exact size is not specified, making a direct size comparison impossible.	['|The Great Pyramid of Khufu has always\nfascinated people because it is the only ancient wonder of the world that exists\ntoday. It is also possible people are fascinated because Khufu’s pyramid,\nespecially the interior, is very complex. The modern entrance to the pyramid was\ncreated in the Ninth Century A.D. by el-Mamoun son of Haroun el-Rhasied. The\ntrue entrance is above this one. This passage goes down through the pyramid, and\nthen connects to another corridor that ascends to the King’s and Queen’s\nChambers. The original passage continues downwards into an unfinished chamber\ndirectly under the pyramid. Discussion about the purpose of these chambers and\nthe complexity of the pyramid is varied and ongoing.\nSerious scientific work on the Great Pyramid began in 1993, when we closed the pyramid for the first time for a full year. It was part of a plan to institute a rotational system at Giza, closing one pyramid a year while leaving the other two open, in order to balance conservation with tourism. During conservation it was found that the interior of the Great Pyramid had a humidity of 85 percent. Most of this was due to tourism, as each person inside the pyramid deposits approximately 20 grams of water through breathing. This water then becomes salt, which in 1993 covered the surface of the Grand Gallery; also, many cracks could be seen inside.\n|The pyramid needed to be cleaned, and a system to permanently lower humidity had to be developed. One idea was to clean the airshafts in the third chamber, the so-called King’s Chamber, and put machines inside of them to create a ventilation system. I talked to Rainer Stadelmann, the director of the German Archaeological Institute in Cairo at that time, who in turn arranged for a robotic expert, Rudolf Gantenbrink, to come and do work through the Institute. Gantenbrink designed a robot called Webwawat to investigate the airshafts in the third chamber. The robot was also sent inside the shafts in the so-called Queen’s Chamber, and made an intriguing find in the room’s two shafts. In the south shaft, Webwawat was stopped at 208 feet in front of a door or small stone with two copper handles. The left handle had lost a piece sometime in antiquity, which was lying about six feet in front of the door/stone. The northern shaft in the so-called Queen’s Chamber was blocked after approximately 27 feet. Gantenbrink published his information on the web at www.cheops.org. Gantenbrink’s work was the most in-depth work to be conducted on the shafts of the so-called Queen’s Chamber. The history of the investigation of these shafts begins in September 1872, when the British engineer Waynman Dixon discovered the openings of the north and south shafts of the Queen’s Chamber. Dixon pushed a wire through the joints of the masonry of the south wall, and realized there was a hollow space behind. He then chiseled through the wall to reveal the shaft. He looked for a shaft in the equivalent area of the north wall and found one. When he lit a candle and placed it in the southern shaft, there was a slight draft.|\n|In the south\nshaft, Dixon and his associate James Grant found a small bronze hook. The north\nshaft yielded a granite ball and a portion of cedar-like wood. They became known\nas the Dixon Relics. Both sets of artifacts lay amongst rubble at the bottom of\nthe sloping shaft. The relics were taken to England, recorded by astronomer\nRoyal Scotland, and returned to Dixon – after which they disappeared. Report of\nthe discovery of the relics was made in “Nature” December 26, 1872, including a\ndrawing of the items.\nIn 1993 a search led to the discovery of the ball and hook in the British Museum, where they still are today. The piece of cedar-like wood remained missing until 2001, when it was traced at the Marischal Museum, Aberdeen. Although they are sure it is in their collection, they haven’t yet located it as they are in the process of moving.\nGantenbrink’s exploration of the shaft in 1993 revealed a long piece of wood lying in the sloping portion of the shaft. Its cross-sectional area and general appearance were similar to the piece of wood found by Dixon. It is possible that the short piece of wood reported by Dixon simply broke from the larger piece. A modern metal pole found alongside the piece of wood supports this theory. No mention of the pole is made in Dixon’s reports, but it is now thought to have been lost when Dixon and his colleagues were “treasure-hunting” in the shaft. They probably broke the small piece of wood from the longer piece while they were manipulating the metal pole, but did not report it.\n|Some suggest that carbon dating the wood\nwould allow accurate dating of the Pyramid because wood must have been left in\nthe shaft when the Pyramid was constructed (given that the shaft was sealed) but\nI contend that this is not absolute. Wood may been placed in the shaft after\nconstruction via the shaft’s exit, if one exists.\nEgyptologists have multiple explanations for these shafts. Some believe that they were used for ventilation, but this cannot be true as they do not open up to the outside. Others believe that they have an astronomical function; the southern shaft connected to the star Sirius, and the northern shaft linked to Minoris, Ursa, and Beta. Stadelmann believes that these shafts are not for ventilation, but are tunnels through which the king’s soul will rise to the stars that never darken.\nI believe that the shafts from the so-called Queen’s Chamber likely have no function, as they were blocked from the inside. If they had a religious function, they should have been left open, as were the shafts of the third burial chamber (the King’s Chamber). Since these open outside of the pyramid, I believe that Khufu’s soul was meant to travel through them. The south shaft was intended for Khufu to use as the sun god Ra. The south shaft opens exactly between the two boat pits to the south of the Pyramid. Khufu would take the two boats and use them as solar boats – one for the day trip, one for the evening trip. The north shaft was made for the soul of Khufu as Horus to travel to the stars in order to emerge from them as the sun god.\n|In order to understand the purpose of the\nshafts of the so-called Queen’s Chamber, more work had to be done. The German\nInstitute in Cairo had the concession to the Great Pyramid, and I could see that\nthey were not interested in completing the work on the shafts. It was impossible\nto sign the concession to Gantenbrink as he is an individual, and the\nantiquities law in Egypt only allows for concessions to be granted to\ninstitutions. So I decided that the Supreme Council of Antiquities (SCA) would\ndo the work. I asked Tim Kelly of National Geographic Television to design a\nrobot to probe the shafts, and I would head the expedition. I believed there was\nnothing behind this door at all but that it was very important for scholars and\nthe public alike to know that there is nothing behind it. Archaeologists would\nknow that even empty space is important.\nNational Geographic designed a robot and called it the Pyramid Rover, and the Permanent Committee of the SCA acknowledged the project as an Egyptian endeavor. The date for the television program was set for September 17. Before the show, I went to Hong Kong and Singapore to publicize it, and Mark Lehner went to Australia, India, and Spain right after the show. I, as well as my colleagues at National Geographic, wanted to be sure that we were honest and people were prepared for whatever find (or lack thereof) that we might make.\n|One of the main goals of the documentary was\nto show the public the evidence of the people who built the pyramids. I was to\ntalk about tombs, show graffiti which names the work gangs that built the\npyramids, and even go inside of the Step Pyramid of Djoser for the first time.\nThe substructure of this pyramid consists of tunnels and passages and rooms with\na total length of about three and a half miles. I believe that the interior was\nnever shown in a film before, and that no living Egyptologist has entered it.\nOne day before the show, we found out by Altrosonic that the door in the southern shaft of the Great Pyramid is about six centimeters thick, which implied that there was something behind that door. We decided to drill a three-millimeter diameter hole in the door so we could send a camera behind it. In the last minute of the show, the camera was sent in, and I saw the second door 21 cm. behind the first door. It is not similar to the first in that it looks as if it is screening or covering something. There were also cracks all over the surface. I was so happy to see it, but I couldn’t understand why we had another door.\nThe show was well received all over the world by the public, and was rated “great” by Fox Television in the United States. Half a billion people in China watched the show. Newspapers all over the world covered it to a level that was never done for any television program before.\n|A few days after the show, we sent the\nrobot into the northern shaft. Gantenbrink and Dixon both were only able to\nprobe 27 feet because of a turn in the shaft. After further investigation, it\nseems that the turn was made in order to avoid intersecting the Grand Gallery,\nimplying that the shafts were cut after the Grand Gallery’s construction. The\nPyramid Rover went through this shaft and was stopped after a total of 208 feet,\nin front of another door with copper handles. It is in the exact same location\nas the first door in the southern shaft and is very similar to it. Behind this\ndoor there is likely another door exactly .70 feet away from the first, exactly\nlike the south shaft.\nThe copper handles in the first doors in both the north and south shafts are similar to those on the canopic jar box of Tutankhamun at the Cairo Museum. The two copper handles were used for ropes to pull the canopic jars. The doors themselves are made of fine white limestone from Tura, and it seems as if their handles allowed them to be pulled inside the shafts, to the same location.\n||The presence of these doors in the Great\nPyramid creates many questions. One idea is that the doors are a challenge that\nthe king must face before his travel into the afterlife. It is written in the\nPyramid texts that the king will face bolts before he travels; perhaps this is a\nreference to the door’s copper handles. Yet if this is true, why is Khufu’s\npyramid the only one with the doors? Also, why do they not exist in the shafts\nof the third chamber? Logically, they should be where the king’s body is buried.\nIt is possible that these doors are evidence that Khufu’s burial chamber might\nbe hidden somewhere inside of his pyramid.\nHowever, these doors made the story about the Great Pyramid of Khufu more exciting, especially because the second door in the south shaft does not look like the first one and also the door in the north shaft is located in the same place as the one in the south shaft and appears similar to it with its two copper handles.\n|After the robot sent its probe through the handled slab in the southern shaft, another, more roughly hewn limestone slab, containing visible cracks was found.|\nWe are planning to clean the south shaft from outside to learn if it does open to the outside. If it does, then it is possible that it was a symbolic door for the king to use in crossing to the Netherworld. If it is sealed, we have to return to the Westcar Papyri and read how Khufu was looking for the documents of the god Thoth to help him with the design of his pyramid. Only further research into the shafts can reveal their function, solving one of the many mysteries of the Great Pyramid.\nBACK to The Plateau Homepage', 'Pyramids – Tombs of Kings\nPyramids €“ Tombs of Kings\nWritten for the KidsKnowIt Network by:\nWhen people around the world think of Egypt, they think of pyramids. While other peoples of the ancient world built these interesting structures, the countryside of Egypt is best known for them. These ancient structures stretch up from the sandy desert ground toward the skies, timeless reminders of the power of the kingdoms of this historic country.\nMost pyramids are three or four sided structures, meeting at the top in a triangular, pointed tip. The solid base and small top made for a solid building design. The largest pyramid, the Great Pyramid of Khufu, covers an amazing 13 acres and originally stood 488 feet (149m) high. Due to wear and theft of the beautiful limestones used as the outer walls, the structure now stands only 455 feet (138m) high. Around 1,300,000 bricks were used, but not bricks like we think of today. The €˜little’ cut limestone pieces weigh 5,500 pounds each and the larger ones weigh 33,000 pounds!\nPyramid building became more and more efficient, with early pyramids, such as the step pyramid tomb of Pharaoh Djoser, in ruins. Built on a poor foundation, the pyramid architect did not carve the stones so that they leaned slightly inward, making the weight of the stones lock the pyramid into place. Later structures, such as the three Great Pyramids of Giza, were put together much better and have stood the test of time with little damage.\nSome people believe that a huge slave population built the pyramids. While some slave labor probably played an important role, not all the workers were slaves. For four months of the year, the Nile River floods, making it impossible for farmers to work the land. It is likely that, during this time, farmers worked alongside the year-round workers to construct the massive monuments.\nArcheologists now believe that the larger pyramids only took about 20 years to build, with a work force of 100,000 men. (Some female skeletons have also been unearthed that show the results of hard labor, so some workers may have been women.) But conditions were not as harsh as once believed. Large towns have been discovered at the bases of pyramids that included everything from bakeries to medical care centers. Pharaohs made sure their workers were strong and able to work hard.\nSo why would so much effort and so many man hours be put into a monument? Egyptians believed that pharaohs were gods. One of dozens of gods, the Pharaoh was the protector of the heavens and of the sun god. Once the ruler passed away, he became god of the dead. Even though he was gone from the earth, a part of his spirit stayed with his body. Because of this, his body stayed important to the Egyptians. They made sure it was mummified (kept from decay), that he had food, weapons and sometimes even slaves in his pyramid-tomb, and that his final resting place was elegant €“ fit for a king! Not all Egyptian pyramids held the bodies and property of Pharaohs and their queens. Some were erected by wealthy individuals who just wanted to be remembered after they died. Whether pharaoh or simple Egyptian, the age of the pyramids lasted approximately 1000 years, ending in 1700 BC.']	['<urn:uuid:ae15de18-01d2-4ced-96ac-7e02bd07940e>', '<urn:uuid:e730bc4f-4369-4bf9-b75a-9ea8822ded40>']	factoid	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-13T00:20:10.754925	11	43	2646
9	I'm new to investing and wondering about T-bills - what makes them considered risk-free investments, and how do they fit into a diversified investment strategy?	T-bills (Treasury bills) are considered risk-free investments because they mature in as little as a few weeks (but no more than one year), are issued by the government which has a very high credit score, have a fixed return, and can be easily sold. The amount of risk associated with T-bills is so small that they're treated as risk-free assets, though they offer very low returns similar to CDs or some savings accounts. In terms of diversification strategy, T-bills play an important role as part of asset allocation diversification, which involves spreading investments across different asset classes including stocks, bonds, and cash equivalents. They can help reduce the impact of market volatility on overall investment performance - while stock markets might experience downturns, the stable performance of T-bills can help offset some losses. This makes them particularly valuable for investors with lower risk tolerance or those seeking to balance riskier investments in their portfolio.	['Risk Aversion in Corporate Finance\nThere are certain risks that no amount of diversification can eliminate. Specific risk is any risk associated with an individual investment and holds the possibility of being eliminated or greatly minimized through diversification. Default risk on a bond, liquidity risk on the corporation underlying a stock, and the risk of a building losing value in the real estate market are all specific risks.\nYou can minimize specific risks through diversification. This strategy can’t stop the entire economy as a whole from going down the toilet, though. Sometimes, no matter how perfect an individual asset is or how well a portfolio is diversified, a nation’s economy goes down the crapper and everything loses value. The risk of that happening is called systematic risk.\nTo minimize systematic risk, you can either diversify internationally (national economies tend to change at different rates just like individual investments), become very good in economics (many people tried to warn everyone about the coming 2007 financial collapse), or just hope to get very lucky.\nYou may ask, though, Didn’t you say something about risk-free investments in the chapter on bonds? Why, yes. Short-term, fixed-rate, highly liquid assets issued by organizations with great credit scores are considered to be risk-free.\nBasically that boils down to T-bills, which mature in as little as a few weeks but no more than one year, are issued by the government (which has a very high credit score), have a fixed return, and can be easily sold. The amount of risk associated with T-bills is so small that they’re considered risk-free.\nThe problem is that they also offer very, very low returns, on par with CDs or some savings accounts at credit unions. Still, you do get a financial return on these without incurring any risk, and risk-free assets are the assets against which all other investments, considered risky assets, are compared.\nThe risk-free rate is the annual return on a risk-free asset, so any investment that has more risk than the risk-free asset must also offer at least proportionally as much return. Otherwise, it’s in the best interest of the investor to buy only risk-free investments.\nHow much risk a corporation takes depends on how risk averse the investing manager of that corporation is. Many people are willing to take on far more additional risk just for the chance of generating a little bit of extra return.\nSome crazy people take on extra risk when they could generate just as much financial yield from a lower-risk investment. Some onlookers think that those individuals are addicted to the risk, kind of like compulsive gamblers.\nOften the amount of risk aversion that a corporation has depends on its timeline. Portfolios with short-term goals are usually more risk averse because they have less time to make up for any losses. Long-term portfolios can ride out any losses from systematic risk by waiting for the economy to regain strength.\nExactly how do you measure an investor’s risk aversion? Well, simply asking investors wouldn’t work. Saying very averse is a bit subjective, so it doesn’t help us mathematically.\nThere are a number of ways to measure how risk averse a particular corporation or investor is. Insurance agents like to measure risk aversion in terms of how much insurance a person needs, often measured as the total potential loss should the insured asset/person experience a worst-case scenario. Many financial advisors measure risk aversion in terms that don’t actually utilize a risk function, instead opting to utilize only their time horizon.\nThey choose the lowest-risk investments available that are likely to generate the necessary returns within the time horizon. Those who simply seek to maximize returns for their client often take the average cyclical duration of investments into consideration; in order to avoid nearing the end of the portfolio time horizon during a recession, they gradually shift the focus of the portfolio toward less risky investments as the end gets closer.\nModern portfolio theory utilizes something called an aversion function. An aversion function is measured by determining how much additional return a corporation must think is possible to be willing to take on just one additional unit of risk. Risk is measured, in this exercise, as the probability of loss (p), while (1 – p) is the probability that no loss will be experienced.\nThis is true because 1 = 100%, and p is any number between 0 and 1. So, if p is 0.4, there’s a 40 percent chance of loss and a 60 percent chance (1 – 0.4) that no loss will be experienced. This strategy is completely hypothetical, however, as a method of measuring risk aversion.\nIn the extreme, a corporation that’s completely neutral to risk — in other words, willing to take on any amount of risk for additional gain — has a risk aversion of 0. This is often true for extremely low-risk assets, such as the difference between a Treasury bill and a Treasury note. A risk aversion of less than 0 means taking on additional risk without the expectation of additional gain, which is insane.\nSo, as a simple matter, the aversion function is a curved line that measures how much additional return must be generated for a single unit of additional risk. The function changes depending on how much risk has already been incurred by the corporation, but is measured by dividing the percentage change in expected returns required by the corporation to take on a percentage change in risk.\nAnother way to measure risk aversion is in terms of the risk premium demanded by an investor. Mathematically, it looks like this:\nA = [E(rm) – rf] / σm\nA = Risk aversion\nE(rm) = Expected returns on risky assets required to attract the investor\nrf = Rate of return on risk-free assets\nσm = The amount of risk in risky assets\nThis method provides more of a spot ratio of risk aversion rather than the dynamic one provided in the previous method. Folks who are mathematically inclined combine the best of both methods. The goal is simply to help you understand the role of risk aversion; that is, that investors who are more risk averse require higher returns to invest in riskier assets than investors with low risk aversion.', 'As a beginner in the investment world, one of the most crucial concepts to grasp is the idea of investment diversification. Diversification is a risk management strategy that involves spreading your investments across different assets to minimize the impact of any one investment on your overall portfolio. By diversifying, you can potentially reduce the volatility of your portfolio and increase the likelihood of achieving consistent returns over the long term. This strategy is based on the principle that different assets perform differently under different market conditions, and by investing in various assets, you can potentially offset losses in one asset class with gains in another.\nWhen you diversify your portfolio, you are essentially spreading your risk across different investments, such as stocks, bonds, real estate, and commodities. This can help protect your portfolio from significant losses if a particular asset class underperforms. For example, if you have all your investments in stocks and the stock market experiences a downturn, your entire portfolio could suffer. However, suppose you have a diversified portfolio with a mix of stocks, bonds, and other assets. In that case, the impact of a stock market downturn may be mitigated by the performance of other asset classes.\nIn addition to reducing risk, diversification can also potentially enhance returns. Spreading your investments across different asset classes can help you capture the performance of multiple markets and industries. This means that even if one market segment is underperforming, other segments may be doing well, which can help balance your overall returns.\nFor beginners in the investment realm, grasping the concept of diversification is pivotal. Diversifying across assets, sectors, and regions minimizes the impact of market volatility, fostering stability and consistent performance.\nTo effectively manage investment risk, consider diversifying your portfolio across various asset classes, including fixed income through bond funds, exploring alternative investments, and balancing exposure to both domestic and international markets, while also staying attuned to fluctuations in interest rates.\nThe blog delves into various diversification types, strategies, and tools, emphasizing the importance of balancing risk and return. It highlights key practices and mistakes to avoid and encourages exploring options like mutual funds or ETFs for instant diversification. Embracing these principles is fundamental for building a resilient investment portfolio aligned with individual goals, laying the groundwork for a successful financial future.\nThe Importance of Diversifying Your Investment Portfolio\nThe importance of diversifying your investment portfolio cannot be overstated. Diversification is a fundamental principle of sound investing essential for managing risk and maximizing potential returns. Without diversification, your portfolio may be overly exposed to a single asset class or market sector risks, leaving you vulnerable to significant losses if that asset class or sector underperforms.\nOne of the key benefits of diversification is its ability to reduce the overall volatility of your portfolio. You can potentially smooth out the peaks and valleys of your portfolio’s performance by spreading your investments across different asset classes. This can provide a more stable and consistent investment experience, which may be particularly important for investors with a lower risk tolerance.\nAnother important aspect of diversification is its potential to improve risk-adjusted returns. Diversifying your portfolio can also help you achieve a more favorable balance between risk and return. This means that for a given level of risk, you may be able to achieve higher returns by diversifying your investments across different asset classes and market sectors.\nTypes of Investment Diversification\nThere are several ways to diversify your investment portfolio, each offering benefits and considerations. The most common types of investment diversification include asset allocation, sector, and geographic diversification.\n1. Asset Allocation:\nAsset allocation diversification involves spreading your investments across different asset classes, such as stocks, bonds, and cash equivalents. By diversifying your portfolio in this way, you can potentially reduce the impact of market volatility on your overall investment performance. For example, if the stock market experiences a downturn, the performance of your bond investments may help offset some of the losses in your stock holdings.\n2. Sector Diversification:\nThis involves investing in different industry sectors, such as technology, healthcare, and consumer goods. Sector-based portfolio diversification can reduce the impact of sector-specific risks on your investment performance. For instance, if the technology sector experiences a downturn, the performance of your investments in other sectors may help mitigate the impact on your overall portfolio.\n3. Geographic Diversification:\nGeographic diversification involves investing in different regions or countries around the world. By diversifying your portfolio geographically, you can effectively reduce the impact of country-specific risks on your investment performance. That way, if the economy of a particular country experiences a downturn, the performance of your investments in other countries may help offset some of the losses.\nStrategies for Diversifying Your Investment Portfolio\nWhen it comes to diversifying your investment portfolio, there are several strategies you can consider to achieve a well-balanced and resilient portfolio. These strategies include asset allocation, dollar-cost averaging, and using mutual funds and exchange-traded funds (ETFs).\n- Asset allocation is a strategy that divides your investment portfolio across different asset classes, such as stocks, bonds, and cash equivalents, based on your investment goals, risk tolerance, and time horizon. When you carefully allocate your investments across different asset classes, you can potentially reduce the impact of market volatility on your overall portfolio and enhance your risk-adjusted returns.\n- Dollar-cost averaging is a strategy that involves investing a fixed amount of money at regular intervals, regardless of market conditions. This approach can help smooth out the effects of market volatility on your investment returns and potentially reduce the risk of making poor investment decisions based on short-term market fluctuations.\n- Mutual funds and ETFs are investment vehicles that offer built-in diversification by pooling together the resources of many investors to invest in a diversified portfolio of stocks, bonds, or other assets. By investing in mutual funds or ETFs, you can achieve instant diversification across a wide range of investments without selecting and managing individual securities yourself.\nEvaluating Risk and Return in Diversified Portfolios\nWhen evaluating the performance of a diversified portfolio, it’s essential to consider both the risk and return characteristics of the portfolio. This involves assessing the potential risks involved in the portfolio’s investments and the expected returns that those investments may generate.\nOne way to evaluate the risk of a diversified portfolio is to consider the standard deviation of its returns. Standard deviation measures the variability of a set of returns around its mean. A lower standard deviation indicates that the returns are less volatile, while a higher standard deviation indicates that the returns are more volatile. You can gain insight into the level of risk associated with the portfolio’s investments by assessing the standard deviation of a diversified portfolio.\nIn addition to evaluating risk, assessing the potential returns of a diversified portfolio is vital. This involves considering the historical performance of the portfolio’s investments and the expected future returns based on the current market conditions and economic outlook. Analyzing the potential returns of a diversified portfolio offers valuable insights into both the investment opportunities and challenges that the portfolio may encounter in the future.\nRebalancing and Monitoring a Diversified Investment Portfolio\nOnce you have established a diversified investment portfolio, it becomes paramount to regularly monitor and rebalance your portfolio to ensure that it remains aligned with your investment goals and risk tolerance. Rebalancing involves adjusting the allocation of your investments to bring them back in line with your target asset allocation, while monitoring involves keeping an eye on the performance of your investments and making adjustments as needed.\nOne approach to rebalancing a diversified investment portfolio is to set specific thresholds for each asset class within your portfolio. For example, you may decide to maintain a target allocation of 60% stocks and 40% bonds. Suppose the stock market experiences a significant rally and causes your stock allocation to exceed 60%. In that case, you may need to sell some stocks and buy more bonds to rebalance your portfolio to its target allocation.\nIn addition to rebalancing, it’s important to monitor the performance of your diversified portfolio on an ongoing basis. This involves regularly reviewing the performance of your investments, assessing the impact of market conditions on your portfolio, and making adjustments as needed to ensure that your portfolio remains aligned with your investment goals and risk tolerance.\nTools and Resources for Investment Diversification\nVarious tools and resources are available to help investors diversify their investment portfolios and make informed investment decisions. These tools and resources include asset allocation tools, investment research platforms, and financial advisors.\n- Asset allocation tools are software programs or online calculators that can help investors determine the optimal allocation of their investments across different asset classes based on their investment goals and risk tolerance. Utilizing an asset allocation tool empowers investors with valuable insights, guiding them in crafting a robust and balanced investment strategy through effective portfolio diversification.\n- Investment research platforms provide investors access to various financial data, research reports, and investment analysis tools to help them make informed investment decisions. These platforms may offer information on different asset classes, market sectors, and individual securities, as well as tools for evaluating investment opportunities’ risk and return characteristics.\n- Financial advisors are professionals who can provide personalized investment advice and guidance to help investors develop and implement a diversified investment strategy. Collaborating with a financial advisor offers investors the advantage of personalized insights and recommendations explicitly crafted for their unique financial goals and circumstances, providing a tailored approach to financial success.\nInvestment Diversification for Beginners\nFor beginners, investment diversification may seem overwhelming initially, but it is a fundamental principle that can help lay the foundation for a successful investment journey. To start diversifying your investment portfolio, it’s important to begin by understanding your investment goals. By assessing these factors, you can gain insight into the appropriate asset allocation for your portfolio and the level of diversification that may be suitable for your individual circumstances.\nOne simple way for beginners to diversify their investment portfolio is by investing in mutual funds or ETFs. These investment vehicles offer built-in diversification by pooling together the resources of many investors to invest in a diversified portfolio of stocks, bonds, or other assets. Investing in mutual funds or exchange-traded funds (ETFs) offers beginners the opportunity to attain diversification across a broad spectrum of investments swiftly. This allows them to benefit from a varied portfolio without the necessity of individually selecting and managing specific.\nAnother approach for beginners to diversify their investment portfolio is to consider investing in index funds, which are designed to track the performance of a specific market index, such as the S&P 500. Index funds offer broad diversification across many individual securities within the index, exposing beginners to a wide range of investment opportunities with a single investment.\nInvestment Diversification Best Practices\nSeveral best practices can help you achieve a well-balanced and resilient investment strategy when diversifying your investment portfolio. These best practices include setting clear investment goals, regularly reviewing and rebalancing your portfolio, and staying informed about market conditions and economic trends.\nSetting clear investment goals is essential in developing a diversified investment strategy. By defining your investment objectives, risk tolerance, and time horizon, you can gain clarity on the appropriate asset allocation for your portfolio and the level of diversification that may be suitable for your individual circumstances.\nRegularly reviewing and rebalancing your portfolio is another key best practice for investment diversification. By monitoring the performance of your investments and making adjustments as needed to bring your portfolio back in line with your target asset allocation, you can ensure that your portfolio remains aligned with your investment goals and risk tolerance.\nStaying informed about market conditions and economic trends is also crucial for adequate investment diversification. By keeping up-to-date with the latest developments in the financial markets and the broader economy, you can gain insight into the potential opportunities and challenges that may impact your investment portfolio, allowing you to make informed investment decisions.\nInvestment Diversification Mistakes to Avoid\nWhile diversifying your investment portfolio is an essential risk management strategy, investors should avoid several common mistakes to ensure that their diversification efforts are effective. These mistakes include over-diversification, neglecting to rebalance your portfolio, and failing to consider your investment goals and risk tolerance.\n- Over-diversification occurs when investors spread their investments across too many different assets to the point where the potential benefits of diversification are diluted. This can lead to suboptimal investment performance and increased portfolio management complexity. Focus on achieving a well-balanced and resilient investment strategy that aligns with your investment goals and risk tolerance to avoid over-diversification.\n- Neglecting to rebalance your portfolio is another common mistake that can undermine the effectiveness of your diversification efforts. Without regular monitoring and rebalancing, your portfolio may become skewed towards certain asset classes or market sectors, leading to unintended exposure to specific risks. To avoid this mistake, regularly review the performance of your investments and make adjustments as needed to bring your portfolio back in line with your target asset allocation.\n- Failing to consider your investment goals and risk tolerance is a fundamental mistake that can impact the effectiveness of your diversification strategy. Without a clear understanding of your investment objectives and risk tolerance, you may struggle to develop a well-balanced and resilient investment portfolio that aligns with your circumstances. Assess your investment goals and risk tolerance and use this information to guide your diversification efforts.\nInvestment diversification is a foundational principle of prudent investing, offering a means to manage risk and optimize potential returns. By strategically allocating investments across diverse asset classes, market sectors, and geographic regions, the potential impact of market volatility on the overall portfolio can be mitigated, achieving a more favorable risk-return balance. A well-diversified investment portfolio positions individuals for long-term success, instilling confidence in navigating the complexities of financial markets.\nUnderstanding the significance of diversification, implementing effective strategies, and avoiding common mistakes enables the construction of a resilient investment portfolio aligned with specific goals and risk tolerance. For beginners, dedicating time to grasp the intricacies of investment diversification lays the groundwork for a successful investment journey built on principles of diversification and risk management.\nStart your journey to investment diversification today and build a resilient investment portfolio that aligns with your financial goals and risk tolerance. Take the first step towards a more secure financial future today by booking a free appointment with us.\nEnjoy This Article? You May Also Like:\n- Debunking the Myth: Real Simple Ways to Manage Debt and Ensure Financial Success\n- Strategies for Building a Strong Financial Foundation for Long-Term Independence\n- Wealth Management Process: What You Should Know from A Financial Advisor’s Perspective\n- The Benefits of a Managed Asset Portfolio\n- Key Strategies for Successful Portfolio Management']	['<urn:uuid:9986063e-15b8-42c3-93ca-e1fb2e9afc54>', '<urn:uuid:893ea3a2-52cf-4ca5-b356-fe929cc622d8>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T00:20:10.754925	25	154	3506
10	power consumption data centers annual cost environmental impact	Data centers consume massive amounts of electricity - about 205 terawatt-hours in 2018, representing 1% of global electricity consumption. In the US alone, data centers used 91 billion kilowatt-hours in 2013, equivalent to powering New York City twice over. This consumption is projected to reach 140 billion kilowatt-hours by 2020, costing American businesses $13 billion annually and emitting nearly 100 million metric tons of carbon pollution per year. This makes reducing data centers' carbon footprint increasingly critical as climate change becomes a more pressing global issue.	"[""Making a data center more environmentally friendly means studying your facility's power and resource usage and learning what potential changes can make a difference. You can take steps such as replacing inefficient assets with newer ones or partnering with green vendors to create a more sustainable data center.\nAccording to some studies, data centers accounted for approximately 205 terawatt-hours of electricity use in 2018, or roughly 1% of all electricity consumption globally -- enough to power some small American cities. The ongoing digitization of the world continues to fuel demand for energy, which means data centers must take an active role in reducing their carbon footprints as climate change becomes an increasingly pressing global issue.\nDeveloping green data centers must become a priority for companies across industries and geographic locations. Consider eight steps you can take to make your data center more sustainable -- an essential task from both a business perspective and an environmental one.\nTrack your base usage\nFirst, you must know how much power and how many resources your data center currently uses. Start by tracking overall electrical usage. Then you can dive deeper into the numbers to forecast future use. For example, break your electrical use down into HVAC, server, infrastructure, network and storage consumption. Once you have a sense of your baseline, you can start looking at ways to improve efficiency.\nRight-size your servers\nRunning your servers 24/7 might actually lead you to underutilize them. Some servers might only process requests during certain times of the day, while others might run applications infrequently or simply no longer serve a purpose. Server monitoring tools like Zabbix, Netreo and PRTG Network Monitor can help admins track server utilization to determine which server functions you can consolidate onto fewer machines, which you can virtualize and which you can decommission altogether.\nModify the temperature\nData center HVAC systems tend to be designed to use more air conditioning than the data center really requires. Newer data center assets can safely run at higher temperatures, so data center facilities can keep the overall temperature a little warmer in order to reduce the load on HVAC.\nRearrange your data center\nYou can increase the efficiency of your data center by rearranging it based on energy consumption and temperature. Use smart layouts such as hot and cold aisle configurations to group warmer assets together and take advantage of HVAC vent placement.\nSuch layouts require you to understand the locations of your intake and outtake vents in the facility in order to place assets appropriately. You can then place additional units in hotter zones for supplemental cooling and reduce your overall electricity costs and demands on your HVAC system.\nReplace older assets with more efficient ones\nMany legacy assets use more power, generate more heat and have lower physical tolerances than newer ones. Newer servers, switches, racks and HVAC technologies have higher-efficiency processors and components. Include these new assets when appropriate for your data center, such as during end-of-life or sunset processes or during replacement maintenance processes. You can also replace physical servers with virtual ones or move certain resources to the cloud to cut down on the number of physical technologies you use.\nInvest in smart facilities management\nIT service management requires you to gather and store a lot of information about your data centers, including power consumption and data loads. By analyzing that data, you give your environmental control systems the insights it requires to optimize your asset usage, thus reducing power consumption and HVAC loads.\nAI-powered monitoring tools can use machine learning to analyze facility data to create a power usage effectiveness forecasting model. Some data centers also use AI-powered tools to autonomously manage HVAC functions at their facilities, in partnership with IoT sensors that feed continuous temperature data to the system. The software then analyzes the data and automatically changes the HVAC system to ensure that temperatures remain at optimal levels at all times. Google has used this technology to reduce energy consumption by 40% in its data center cooling systems.\nInvestigate alternate green technologies\nOrganizations seeking to reduce their carbon footprints can consider many green alternatives, such as geothermal cooling, wind power and hydroelectric power. For example, Verne Global uses geothermal and hydroelectric technologies in their Keflavik, Iceland, data center to naturally cool their facility, TierPoint's data center in Spokane, Wash., combines internal geothermal cooling technology with Washington's green electricity generation options to reduce their footprint, and Iron Mountain's underground data centers take advantage of natural cooling.\nCheck which green options are available to your organization, because the development of new green technologies means you can always find new ways to reduce your carbon footprint.\nPartner with green vendors and organizations\nDevelop green partnerships with vendors and organizations that offer more sustainable options. U.S.-based data centers can partner with the EPA to identify the best green power products for their locations. You can check for a vendor's or provider's sustainability level through the Carbon Disclosure Project, the RE100 and Sustainalytics to see their commitment to renewable energy."", 'Data center energy efficiency is at the forefront of hot topics for data centers. And, for good reason. Data centers use a truly astonishing amount of energy each year. The Natural Resources Defense Council (NRDC) noted just how much energy data centers use, “In 2013, U.S. data centers consumed an estimated 91 billion kilowatt-hours of electricity, equivalent to the annual output of 34 large (500-megawatt) coal-fired power plants (and, the NRDC notes, the equivalent of enough electricity to power all the households in New York City twice over). Data center electricity consumption is projected to increase to roughly 140 billion kilowatt-hours annually by 2020, the equivalent annual output of 50 power plants, costing American businesses $13 billion annually in electricity bills and emitting nearly 100 million metric tons of carbon pollution per year.” Improving energy efficiency in a data center is incredibly important but can seem like a daunting task for many data center managers who often are not even certain where to begin, if they have the budget to make changes, or if the changes they make will truly make an impact.\nOne of the first, and most practical ways to begin a shift towards improving energy efficiency is to take a real look at energy usage. What is using the most energy, should it be using that much energy and can anything be eliminated? Often, energy is being wasted on ghost infrastructure or outdated energy-draining equipment. But, as many data center managers know, it can be difficult to keep track of all of the infrastructure in a data center or truly know what is using the most energy of inefficiently using energy. That is why a good DCIM plan is important so that data center managers can work with the most current information rather than outdated information and make well-informed decisions going forward. Once you have sufficiently audited data center energy usage and are able to make well-informed decisions for improvement you can move onto the next step. Next, make immediate changes to improve energy efficiency while also looking at long term improvements. Long term improvements are incredibly important and it is wise to look at how to remain sustainable in the future but while making decisions for the future you can make some immediate changes such as hot aisle/cold aisle arrangements or other containment options that may help improve energy efficiency. While other improvements are more difficult or costly to implement, containment arrangements can be made and executed relatively quickly and will make a big impact. While making immediate changes it is important to get budgetary approval for bigger changes and, once approved, begin moving forward with changes that will help your data center remain efficient in the future. This most likely means upgrading equipment to the most current, energy efficient options. All equipment has a lifespan and once they are getting a big old they will likely become energy inefficient. If you have budgetary approval to make improvements with heating and cooling options as well it is a great choice to improve your cooling ability within a data center because cooling is typically one of the biggest expenses in a data center. Lastly, explore green options like making a switch to more fully utilizing cloud storage or implementing cooling with outside air sources so that you can be as energy efficient as possible now and in the future.']"	['<urn:uuid:62d8205e-3542-4de1-883d-e1ac7c3b3e5a>', '<urn:uuid:b4b75be2-0c67-4150-9fbe-83b6f17b1cfb>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T00:20:10.754925	8	86	1400
11	fungal spores transmission cryptococcus neoformans histoplasma capsulatum	Both fungi are transmitted through inhalation of spores, but they are found in different environments. Cryptococcus neoformans spores are found in soils contaminated with bird feces, while Histoplasma capsulatum spores are commonly found in soil in river valleys, particularly in the central and eastern United States, and are associated with both bird and bat droppings.	"['Cryptococcus neoformans var. grubii H99 Database\nCryptococcus neoformans is an opportunistic fungal pathogen that may cause meningitis in immunocompromised individuals. Often found in soils contaminated with bird feces, C. neoformans enters its host through the lungs via inhalation of spores. In healthy individuals the infection is usually controlled and asymptomatic, but in immunocompromised hosts the fungus can spread via the blood to infect the central nervous system and cause meningitis that is uniformly fatal if untreated. Few antifungal agents exist and drug-resistant strains are emerging.\nC. neoformans, a member of the Basidiomycota phylum of the fungi, is more closely related to mushrooms than to ascomycete fungi like Saccharomyces cerevisiae or Candida albicans or filamentous fungi such as Neurospora crassa or Aspergillus nidulans.\nThe March 2013 release of gene predictions for the serotype A isolate Cryptococcus neoformans var. grubii H99 is based on a chromosome-based genome assembly provided by Dr. Fred Dietrich at the Duke Center for Genome Technology. Using RNA-seq from strand-specific and non-strand-specific libraries, we have updated the set of 6,962 predicted proteins in collaboration with Dr Guilhem Janbon at the Pasteur Institute. This work was supported by NIAID and by a grant from ANR (2010-BLAN-1620-01 program YeastIntrons) to GJ. A file mapping annotations between the current and previous versions may be downloaded here.\nSequencing Cryptococcus neoformans Serotype A\nThe Cryptococcus neoformans Serotype A sequencing project reflects a collaboration between the Fungal Genome Initiative at the Broad Institute and Fred Dietrich at the Duke Center for Genome Technology.\nThe Center for Genome Technology at Duke University Medical Center contributed ~2X coverage in plasmid reads. The Broad Institute of MIT and Harvard produced additional whole-genome shotgun sequence from 4 kb & 10 kb plasmids, 40 kb Fosmids and 110 kb BACs. The H99 BAC library was constructed by Klaus Lengeler and Joseph Heitman at Duke University, finger printed and end sequenced by Jim Kronstad and the Vancouver Genome Sequence Centre, and provided by these investigators to the Broad Institute. Genomic DNA for the H99 libraries was provided by James Fraser in the Heitman lab at Duke University. All the reads were assembled with Arachne.\ncDNA was provided by Dr. Doris Kupfer at University of Oklahoma. RNA of samples for strand-specific library construction and sequencing were provided by Yuan Chen at Duke University\nWhat is Cryptococcus neoformans?\nCryptococcus neoformans is an encapsulated fungal pathogen causing fatal meningitis in humans. The infection, initiated by inhalation into the lungs, occurs mainly in immunocompromised individuals, but can also occur in healthy individuals. Cryptococcus neoformans is usually found in tissues in the yeast form. Infection of the brain and meninges is the most common clinical manifestation. In immunocompetent individuals, the initial infection is usually controlled and asymptomatic and the organism remains dormant in a lymph node complex, much like tuberculosis. Reactivation occurs in immunocompromised hosts where the fungus can spread via the blood to infect the central nervous system. Once C. neoformans reaches this stage it can cause meningitis that is uniformly fatal if untreated. Few antifungal agents exist and drug-resistant strains are emerging.\nThere are four serotypes of C. neoformans. The serotype D was the first serotype chosen for sequencing studies at The Institute for Genome Research and Stanford Genome Technology Center because of its advanced genetic tools. However, more than 90% of clinical isolates and more than 99% of isolates from AIDS patients are of the more divergent serotype A strains. Sequencing a serotype A strain will increase our understanding of this disease and comparative studies between the serotypes A and D will help us define the genome structure and the development of pathogenicity.\nCryptococcus neoformans is unique among the most common human fungal pathogens in that it is a basidiomycete, thus it is evolutionarily divergent from the more common pathogenic ascomycetes (e.g., Candida albicans) and more closely related to wood rotting fungi (e.g., Phanerochaete chrysosporium), mushrooms (e.g., Coprinus cinereus), and plant pathogens (e.g., Ustilago maydis). Cryptococcus neoformans elaborates two specialized virulence factors, the polysaccharide capsule, which inhibits phagocytosis, and melanin, which serves as an antioxidant. The typical vegetative form of C. neoformans is the yeast form. The organism can also undergo sexual reproduction and form basidiospores. Sexual reproduction appears to occur much less frequently in nature than asexual or vegetative reproduction.\nMost isolates of C. neoformans are haploid. The size of the genome is approximately 19 Mb with 14 chromosomes. Cryptococcus neoformans has a defined sexual cycle involving mating between cells of the MATalpha and MATa types. Thus, classical genetic approaches can be applied to study this organism.\nHome page images and credits\nFrom left to right:\n- Encapsulated yeast cells\n- Sexual spores from Serotype A mating\n- Encapsulated yeast and sexual spores on defined media\n- Basidia head bearing four spores at distinct positions\n- Hyphae with a clamp cell, yeast cells and sexual spore chains\nSource: Rajesh Velagapudi, and Joseph Heitman, M.D, PhD., Duke University\nScanning Electron Micrographs taken at ""Center for Electron Microscopy"" with the help of Valerie Knowlton (NCSU).\nThe phylogenetic tree was provided by Dr. Jason Stajich, UC Berkeley. For details please see ""A fungal phylogeny based on 42 complete genomes derived from supertree and combined gene analysis"" David A. Fitzpatrick, Mary E. Logue, Jason E. Stajich and Geraldine Butler. BMC Evolutionary Biology 2006, 6:99\nFunding support for genome sequencing of H99 was provided by the National Human Genome Research Institute, the National Institute of Allergy and Infectious Disease, and by the French National Research Agency (ANR) (2010-BLAN-1620-01 program YeastIntrons).', ""Acute pulmonary histoplasmosis is a respiratory infection that is caused by inhaling the spores of the fungus Histoplasma capsulatum.\nHistoplasma capsulatumis the name of the fungus that causes histoplasmosis. It is found in the central and eastern United States, eastern Canada, Mexico, Central America, South America, Africa, and Southeast Asia. It is commonly found in the soil in river valleys. It gets into the soil mostly from bird and bat droppings.\nYou can get sick when you breathe in spores that the fungus produces. Every year, thousands of people with a normal immune system worldwide are infected, but most do not become seriously sick. Most have no symptoms or have only a mild flu-like illness and recover without any treatment.\nAcute pulmonary histoplasmosis may happen as an epidemic, with many people in one region becoming sick at the same time. People with weakened immune systems (see Symptoms section below) are more likely to:\n- Get the disease if exposed to the fungus spores\n- Have the disease come back\n- Have more symptoms, and more serious symptoms, than others who get the disease\nRisk factors include traveling to or living in the central or eastern United States near the Ohio and Mississippi river valleys, and being exposed to the droppings of birds and bats. This threat is greatest after an old building is torn down and the spores get into the air, or when exploring caves.\nMost people with acute pulmonary histoplasmosis have no symptoms or only mild symptoms. The most common symptoms are:\n- Chest pain\n- Joint pain and stiffness\n- Muscle aches and stiffness\n- Rash (usually small sores on the lower legs)\n- Shortness of breath\nAcute pulmonary histoplasmosis can be a serious illness in the very young, older people, and people with a weakened immune system, including those who:\n- Have HIV/AIDS\n- Have had bone marrow or solid organ transplants\n- Take medicines that suppress their immune system\nSymptoms in these people may include:\n- Inflammation around the heart (called pericarditis)\n- Serious lung infections\n- Severe joint pain\nExams and Tests\nTo diagnose histoplasmosis, you must have the fungus or signs of the fungus in your body. Or your immune system must show that it is reacting to the fungus.\n- Antibody tests for histoplasmosis\n- Biopsy of infection site\n- Bronchoscopy (usually only done if symptoms are severe or you have an abnormal immune system)\n- Complete blood count (CBC) with differential\n- Chest CT scan\n- Chest x-ray (might show a lung infection or pneumonia)\n- Sputum culture (this test often does not show the fungus, even if you are infected)\n- Urine test for Histoplasma capsulatum antigen\nMost cases of histoplasmosis clear up without specific treatment. People are advised to rest and take medicine to control fever.\nYour health care provider may prescribe medicine if you are sick for more than 4 weeks, have a weakened immune system, or are having breathing problems.\nWhen histoplasmosis infection is severe or gets worse, the illness may last up to 6 months. Even then, it is rarely fatal.\nAcute pulmonary histoplasmosis can get worse over time, or can become long-term (chronic) pulmonary histoplasmosis (which doesn't go away).\nHistoplasmosis can spread to other organs through the bloodstream (dissemination). This is often seen in infants, young children, and people with a suppressed immune system.\nWhen to Contact a Medical Professional\nCall your provider if:\n- You have symptoms of histoplasmosis, especially if you have a weakened immune system or have recently been exposed to bird or bat droppings\n- You are being treated for histoplasmosis and develop new symptoms\nAvoid contact with bird or bat droppings if you are in an area where the spore is common, especially if you have a weakened immune system.\nDeepe GS. Histoplasma capsulatum (histoplasmosis). In: Bennett JE, Dolin R, Blaser MJ, eds. Mandell, Douglas, and Bennett's Principles and Practice of Infectious Diseases. 8th ed. Philadelphia, PA: Elsevier Saunders; 2015:chap 265.\nKauffman CA. Histoplasmosis. In: Goldman L, Schafer AI, eds. Goldman's Cecil Medicine. 25th ed. Philadelphia, PA: Elsevier Saunders; 2015:chap 332.\nSchmitt BH, Rosenblatt JE, Pritt BS. Laboratory diagnosis of tropical infections. Infec Dis Clin N Am. 2012;26(2):513-554. PMID: 22632652 www.ncbi.nlm.nih.gov/pubmed/22632652.\nReview Date 3/13/2016\nUpdated by: Jatin M. Vyas, MD, PhD, Assistant Professor in Medicine, Harvard Medical School; Assistant in Medicine, Division of Infectious Disease, Department of Medicine, Massachusetts General Hospital, Boston, MA. Also reviewed by David Zieve, MD, MHA, Isla Ogilvie, PhD, and the A.D.A.M. Editorial team.""]"	['<urn:uuid:df2769c5-64f3-424e-9519-c719526f3d15>', '<urn:uuid:73361a85-9fba-42ea-9eac-4121d7ee00fd>']	open-ended	direct	short-search-query	similar-to-document	comparison	expert	2025-05-13T00:20:10.754925	7	55	1662
12	How does Alzheimer's affect the brain structure and daily functioning?	Alzheimer's causes overall brain tissue shrinkage, widened sulci (furrows), and enlarged ventricles. This physical deterioration leads to progressive loss of daily functions - from early difficulty with routine tasks and judgment to later inability to control bodily functions, requiring constant care. The disease typically lasts 8-10 years but can extend to 20 years.	['Here is a brief overview of the parts of the human brain and a brain with Alzheimer’s disease.\n© 2000 – 2012 American Health Assistance Foundation\nThe image on the left is a side view of the outside of the brain, showing the major lobes (frontal, parietal, temporal and occipital) and the brain stem structures (pons, medulla oblongata and cerebellum).\nThe image on the right is a side view showing the location of the limbic system inside the brain. The limbic system consists of a number of structures, including the fornix, hippocampus, cingulate gyrus, amygdala, the parahippocampal gyrus and parts of the thalamus. The hippocampus is one of the first areas affected by Alzheimer’s disease.\nBrain With Alzheimer’s Disease\nThese images represent a cross-section of the brain as seen from the front. The cross-section on the left represents a normal brain and the one on the right represents a brain with Alzheimer’s disease. © 2000 – 2012 American Health Assistance Foundation\nIn Alzheimer’s disease, there is an overall shrinkage of brain tissue. The grooves or furrows in the brain, called sulci (plural of sulcus), are noticeably widened and there is shrinkage of the gyri (plural of gyrus), the well-developed folds of the brain’s outer layer.\nIn addition, the ventricles, or chambers within the brain that contain cerebrospinal fluid, are noticeably enlarged. In the early stages of Alzheimer’s disease, short-term memory begins to fade (see box labeled ‘memory’) when the cells in the hippocampus, which is part of the limbic system, degenerate.\nThe ability to perform routine tasks also declines. As Alzheimer’s disease spreads through the cerebral cortex (the outer layer of the brain), judgment declines, emotional outbursts may occur and language is impaired. As the disease progresses, more nerve cells die, leading to changes in behavior, such as wandering and agitation.\nIn the final stages of the disease, people may lose the ability to recognize faces and communicate; they normally cannot control bodily functions and require constant care. On average, the disease lasts for 8 to 10 years, but individuals with Alzheimer’s can live for up to 20 years.\nGlossary of Terms for an Anatomy of the Brain\nAmygdala – limbic structure involved in many brain functions, including emotion, learning and memory. It is part of a system that processes “reflexive” emotions like fear and anxiety.\nCerebellum – governs movement.\nCingulate gyrus – plays a role in processing conscious emotional experience.\nFornix – an arch-like structure that connects the hippocampus to other parts of the limbic system.\nFrontal lobe – helps control skilled muscle movements, mood, planning for the future, setting goals and judging priorities.\nHippocampus – plays a significant role in the formation of long-term memories.\nMedulla oblongata – contains centers for the control of vital processes such as heart rate, respiration, blood pressure, and swallowing.\nLimbic system – a group of interconnected structures that mediate emotions, learning and memory.\nOccipital lobe – helps process visual information.\nParahippocampal gyrus – an important connecting pathway of the limbic system.\nParietal lobe – receives and processes information about temperature, taste, touch, and movement coming from the rest of the body. Reading and arithmetic are also processed in this region.\nPons – contains centers for the control of vital processes, including respiration and cardiovascular functions. It also is involved in the coordination of eye movements and balance.\nTemporal lobe – processes hearing, memory and language functions.\nThalamus – a major relay station between the senses and the cortex (the outer layer of the brain consisting of the parietal, occipital, frontal and temporal lobes).\nAmerican Health Assistance Foundation/ Alzheimer’s Disease Research\nLast Review: 01/10/12\n© 2000 – 2012 American Health Assistance Foundation. All rights reserved.', 'Watching someone you love—whether it’s a parent, a spouse, or an old friend—struggle with Alzheimer’s disease is never easy. Once this irreversible condition takes hold, it progressively worsens as time passes.\nSymptoms of dementia, which can disrupt cognition, memory, and reasoning, are hallmarks of this disease. You can support your loved one by taking the time to understand the various stages of Alzheimer’s and their symptoms.\nThe Global Deterioration Scale (GDS) is a tool used by doctors who treat Alzheimer’s and other forms of dementia. While no two individuals experience the same progression of the disease, these seven stages can serve as a general guide.\nBeing aware of the first warning signs of Alzheimer’s can make it easier to notice changes in a loved one while the disease is still in its early stages—and when treatment may help to slow down the worsening of symptoms.\nKnowing what to expect in terms of symptoms is one of the best ways to plan for a loved one’s future care and treatment.\nStage One: No Cognitive Decline\nIn this early phase, you may not be able to detect any outward symptoms. The disease doesn’t cause any noticeable difficulties with daily living, and the individual is able to function normally and maintain his or her independence.\nReasoning and memory remain intact and in an evaluation, the individual shows no signs of dementia.\nDuring this stage, a PET scan of the brain is the only diagnostic tool that can reveal Alzheimer’s. Friends, children, and loved ones might not notice any changes until stage two or three.\nStage One Symptoms:\nStage Two: Very Mild Cognitive Decline\nIn stage two, both doctors and loved ones may still not pick up on small changes in memory, cognition, and behavior.\nSymptoms of Alzheimer’s at this stage are usually subtle, and may not interfere with the individual’s ability to work, socialize, and function independently.\nOften, small changes, such as misplacing familiar items or forgetting words and names, are only noticeable to the individual. But these symptoms can also be part of the normal aging process. They don’t always indicate Alzheimer’s.\nStage Two Symptoms:\n- Misplacing familiar objects\n- Difficulty finding words\n- Forgetting some names\nStage Three: Mild Cognitive Decline\nBy stage three, mild symptoms become apparent, but an Alzheimer’s diagnosis can still be difficult. The symptoms may not be serious, though they appear to be worsening.\nMemory problems and forgetfulness start to interfere with the individual’s ability to function independently at home, at work, or in social situations.\nStage Three Symptoms:\n- Forgetting information they just read\n- Repeating questions that have already been answered\n- Difficulty remembering plans or staying organized\n- Difficulty remembering names of people they just met\n- Difficulty traveling to new locations\nStage Four: Moderate Cognitive Decline\nBy stage four, a doctor should be able to diagnose the patient through an interview evaluation. Stage three symptoms may become more obvious and new symptoms—such as difficulty completing complex, everyday tasks—may appear.\nMood changes can also appear at this stage. Some individuals with Alzheimer’s might show symptoms of depression or appear to withdraw from their social circles.\nStage Four Symptoms:\n- Forgetting personal details\n- Difficulty with complex tasks such as finances, cooking, shopping, or planning a dinner party\n- Forgetting the month or the season\n- Mood changes\nStage Five: Moderately Severe Cognitive Decline\nAt this stage, your loved one may need assistance with a number of daily tasks. They might forget where they are or what time it is, leading to confusion and disorientation.\nYour loved one might be unable to recall personal details, such as their phone number or address.\nThey might struggle to choose clothes appropriate for the time of year. Though personal hygiene is still possible at this stage, it may take prompting or assistance.\nStage Five Symptoms:\n- Requires help to choose appropriate clothing\n- Needs prompting for personal hygiene tasks, such as bathing\n- Significant confusion or disorientation\n- Inability to recall personal details\nStage Six: Severe Cognitive Decline\nStage six marks a serious decline. Memory problems are magnified. The individual may lose awareness of their surroundings.\nWith close friends and family, the individual may recognize faces but forget names, or mistake one person for someone else.\nPersonality changes also set in by stage six and can include delusions, paranoia, suspicion, and mood swings.\nAt this stage, personal hygiene may become impossible. They may need help going to the bathroom. Wandering is common, and sleeping habits may change too.\nStage Six Symptoms:\n- Loss of awareness of surroundings, environment, and recent events\n- Requires assistance with personal hygiene tasks, such as bathing and using the toilet\n- Noticeable personality and mood changes\n- Inability to recall personal history or experiences\n- Changes in sleeping habits\nStage Seven: Very Severe Cognitive Decline\nBasic abilities, such as eating or speaking, tend to disappear during the seventh and final stage of Alzheimer’s disease.\nYour loved one may start to show a lack of control over most physical movements and functions, including bladder function, walking, swallowing, or sitting upright.\nFor the individual, recognizing and communicating with close family and caregivers becomes difficult.\nStage Seven Symptoms:\n- Difficulty swallowing\n- Requires assistance eating\n- Incontinence and loss of bowel control\n- Single-word or incoherent speech, such as groans, grunts, or moans\nIf you or someone you love is struggling with Alzheimer’s Disease, you can help by learning more about Alzheimer’s care, treatment, and medication.\nThe information in this article should not be taken as professional medical advice. If you are having issues or have health-related concerns, you should see your personal physician.']	['<urn:uuid:d1b029c8-979e-427c-83ff-0809961f8bd3>', '<urn:uuid:e54b4dba-4f44-4e76-81de-2cc984c761f7>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T00:20:10.754925	10	53	1550
13	Looking at self-renewal pathways in disease states, what's the key contrast between how human cancer cells and normal stem cells maintain their ability to keep dividing without hitting senescence?	The key contrast is that cancer cells show dysregulated and excessive telomerase activity compared to normal stem cells. While stem cells require carefully regulated Bmi1 expression to maintain self-renewal and prevent premature senescence through controlled gene repression, cancer cells abnormally overexpress telomerase in ≥85% of cases, allowing unlimited proliferation. Additionally, stem cells still experience some telomere erosion during aging despite expressing telomerase, whereas cancer cells can fully prevent telomere shortening through their aberrantly high telomerase levels.	['|Home | About | Journals | Submit | Contact Us | Français|\nStem cells generate the differentiated cell types within many organs throughout the lifespan of an organism and are thus ultimately responsible for the longevity of multicellular organisms. Therefore, senescence of stem cells must be prevented. Bmi1 is required for the maintenance of adult stem cells in some tissues partly because it represses genes that induce cellular senescence and cell death.\nMany tissues are maintained throughout the lifespan of an organism by a small number of adult stem cells. These cells are unique in that they have both the ability to give rise to new stem cells via a process called self-renewal and the ability to differentiate into the mature cells of a tissue. To maintain tissue homeostasis, stem cells have developed strict regulatory mechanisms to self-renew, differentiate, and prevent premature senescence and apoptosis (see review, ref. 1). The recent observation that Bmi1, a Polycomb group repressor, is essential for the self-renewal of adult murine hematopoietic stem cells (HSCs) and neuronal stem cells, in part via repression of genes involved in senescence, suggests that stem cells have evolved specific mechanisms to repress senescence and to prolong their capacity to proliferate. In this Perspective, we discuss the possible role of Bmi1 in the prevention of senescence in stem cells.\nHSCs are among the best-characterized stem cells. The existence of these cells was proven using clonal assays and retroviral marking (2, 3). Flow cytometry was then used to isolate HSCs based on cell-surface marker expression (4, 5). Subsequently, other types of somatic stem cells such as neuronal stem cells from the peripheral and central nervous systems have been identified (6, 7).\nStem cells possess three fundamental properties (1). First, they must self-renew, allowing the maintenance of the original stem cell population. Self-renewal is a cell division in which one or both of the daughter cells are stem cells that retain the same developmental potential as the mother cell. In contrast, proliferation is a more general term that refers to all types of mitosis, whether they yield stem cells, restricted progenitors, or terminally differentiated cells. Second, stem cells must be able to differentiate into multiple types of mature cells in order to replace the mature cells that turn over in adult tissues. Third, the total number of stem cells is strictly regulated via both extrinsic and intrinsic mechanisms, resulting in the stability of a stable stem cell pool (8–11).\nSenescence is a state in which a cell no longer has the ability to proliferate. Since stem cells maintain many tissues during the lifetime of an animal, it follows that stem cell senescence must be prevented to maintain an organ throughout life. Several studies suggest that cellular senescence is accompanied by changes in gene expression, which might be regulated by epigenetic mechanisms. In support of this hypothesis, histone deacetylase inhibitors, which decondense chromatin and activate the transcription of some genes, can induce a senescence-like state in human fibroblasts (12), suggesting that conversion of some heterochromatin to euchromatin may be a feature of replicative senescence (13, 14). Other studies suggest that chromatin condensation and subsequent downregulation of certain genes might regulate senescence. Senescence accompanies changes in nuclear morphology and formation of a distinct chromatin structure, called senescence-associated heterochromatic foci (SAHF) (15). SAHF do not contain active transcription sites, and they recruit heterochromatin proteins to the genes that are to be stably repressed during senescence. It was shown that SAHF contained the retinoblastoma protein (pRB) in the E2F-responsive promoters, such as cyclin A and proliferating cell nuclear antigen promoters, and silenced the expression of E2F-responsive genes during senescence but not during quiescence (15). Formation of SAHF and silencing require an intact pRB pathway, since inhibition of p16Ink4a prevents SAHF formation and leads to DNA replication. These results provide a molecular mechanism for the maintenance of the senescent state and demonstrate the importance of pRB as a tumor suppressor.\nHSCs have an impressive regenerative potential, as demonstrated by transplantation experiments using limited numbers of cells. In mice, serial transplantation is possible for four to six passages, suggesting that individual HSCs are capable of extensive self-renewal but may not be immortal. Even though HSCs express telomerase (16, 17), it is not sufficient to completely prevent telomere erosion during aging (18). Overexpression of the catalytic subunit of the telomerase enzyme in hematopoietic cells prevents telomeres from shortening during serial transplantation of bone marrow. However, even HSCs overexpressing telomerase could be serially transplanted no more than four times, as is the case with wild-type HSCs; this suggests that a telomere-independent mechanism regulates replicative senescence of mouse HSCs during serial transplantation (19). On the other hand, telomerase-deficient HSCs can be serially transplanted only twice, accompanied by an increased rate of telomere shortening, indicating that telomerase is nonetheless needed to prevent premature loss of telomere function during serial transplantation (20, 21).\nSince epigenetic events such as histone modification have been implicated in senescence, it follows that genes involved in chromatin remodeling and gene expression, such as members of the Polycomb and Trithorax families, might be directly involved in decisions that affect stem cell fate, including self-renewal, senescence, and possibly aging. Polycomb and Trithorax proteins form large multimeric structures, which can lead to repression or activation of gene expression, respectively, via a concerted process of chromatin modifications (22, 23).\nBoth HSCs and neuronal stem cells express high levels of Bmi1 (24–26), a member of the Polycomb group of transcription repressors that was initially identified as an oncogene cooperating with c-myc in a murine model of lymphoma (27, 28). Bmi1 has a RING finger at the amino-terminus and a central helix-turn-helix domain. The RING finger domain is required for the generation of lymphoma in Eμ-Bmi1 transgenic mice (29, 30). Postnatal mice lacking Bmi1 exhibit defects in hematopoiesis, skeletal patterning, neurological functions, and development of the cerebellum (31).\nIt has recently been shown that Bmi1 is necessary for efficient self-renewing cell divisions of adult HSCs as well as adult peripheral and central nervous system neural stem cells, but that it is less critical for the generation of differentiated progeny (25, 26). Transplantation of Bmi1–/– fetal liver cells resulted in only transient hematopoietic cell reconstitution, suggesting that the transplanted mutant fetal liver HSCs failed to generate more HSCs but gave rise to multipotent progenitors that could sustain hematopoiesis for up to 4–8 weeks. Similarly, Bmi1 is needed for the maintenance of neural stem cells found in both the central and peripheral nervous systems. As with HSCs, the reduced self-renewal of Bmi1-deficient neural stem cells led to their postnatal depletion in vivo, but the proliferation and survival of committed progenitor cells were essentially normal (26). Given the broad ranges of phenotypic changes in Bmi1-deficient mice, including posterior transformation and neurological abnormalities (31), and its broad tissue distribution (32), it is likely that Bmi1 regulates the self-renewal of other types of somatic stem cells.\nmay also play a key role in some types of cancer (33–35). In approximately 11% of cases of mantle cell lymphoma, the malignant cells have a three- to sevenfold amplification of Bmi1 DNA and express high levels of the protein, implicating this gene in this invariably lethal form of lymphoma. In a mouse model of leukemia, Bmi1 was essential for the maintenance of leukemic cells (36). Enforced expression of Hoxa9/Meis-1 in both normal and Bmi1-deficient mouse fetal liver cells, followed by transplantation, initially resulted in infiltration of the bone marrow by cells that looked like acute myeloid leukemia (AML) blasts, and mice developed a bone marrow infiltrate that resembled AML. However, only Bmi1 wild-type AML could be serially transplanted. Taken together with the detection of high levels of Bmi1 in human AML stem cells (25), these results suggest that Bmi1 is also required for the self-renewal of leukemic stem cells.\nIn WI-38 human fetal lung fibroblasts, Bmi1 is downregulated when the cells undergo replicative senescence, but not when they are quiescent. Additionally, Bmi1 extends replicative lifespan but does not induce immortalization when overexpressed (37). In the absence of Bmi1, both the p16Ink4a and the p19Arf genes from the Ink4a locus are expressed (38). Lifespan extension by Bmi1 is mediated in part by suppression of the p16Ink4a-dependent senescence pathway and requires an intact pRB pathway, but not the p53 tumor-suppressor protein. The RING finger and helix-turn-helix domains of Bmi1 were required for lifespan extension and p16Ink4a suppression. Furthermore, a RING finger deletion mutant acted as a dominant negative, inducing p16Ink4a and premature senescence (37).\nNormal mouse embryonic fibroblasts (MEFs) reach replicative senescence after seven passages in culture, whereas MEFs from Bmi1–/– mice show a premature-senescence phenotype at the third passage. This was correlated with increased expression of p16Ink4a. Re-expression of Bmi1 in Bmi1–/– MEFs prevented premature senescence (28). Overexpression of Bmi1 gave a proliferative advantage and extended MEF lifespan. Furthermore, unlike human fibroblasts, Bmi1 could immortalize MEFs.\nGene-profiling studies suggest that Bmi1 modulates HSC self-renewal through the regulation of genes important for stem cell fate decisions, as well as survival genes, antiproliferative genes, and stem cell–associated genes (Figure (Figure1)1) (25). The previously mentioned Bmi1 target, the Ink4a locus (28), encodes p16Ink4a and p19Arf using different promoters (38). Enforced expression of p16Ink4a and p19Arf in HSCs led to senescence and apoptosis, respectively (25). In neural stem cells, p16Ink4a deficiency partially restored the ability of Bmi1-deficient stem cells to self-renew (26). Figure Figure22 illustrates regulation of the cell cycle and senescence by p16Ink4a and p19Arf. During the cell cycle, pRB is hyperphosphorylated by the cyclin D/cyclin-dependent kinases 4 and 6 (cyclin D/Cdk4/6) complex (39). The hyperphosphorylated pRB is unable to bind and inhibit E2F transcription factor, allowing transcription of E2F target genes that are important for the G1/S transition, such as DNA polymerase II, cyclin E, p19, myb, and dihydrofolate reductase (40). This allows cell cycle progression. In the absence of Bmi1, p16Ink4a is upregulated and prevents binding of Cdk4/6 to cyclin D, inhibiting the kinase activity. This results in hypophosphorylated pRB, which then binds E2F and inhibits E2F-mediated transcription, leading to cell cycle arrest and senescence (39). p19Arf sequesters mouse double minute 2 (MDM2) and inhibits p53 degradation, resulting in p53-mediated cell cycle arrest and apoptosis (41, 42). Point mutations and deletion of p16Ink4a and p19Arf are frequently found in many types of human cancers, which implicates them as key regulators of immortalization and/or senescence checkpoints.\nMice lacking Bmi1 showed induction of both p16Ink4a and p19Arf in various hematopoietic and neuronal tissues (25). Overexpression of p16Ink4a and p19Arf in adult HSCs induced cell cycle arrest and apoptosis via the pRB and the p53-dependent pathway, respectively. Double deletion of the Bmi1 and p16Ink4a/p19Arf genes partially rescued the phenotypes observed in Bmi1-deficient mice (28), suggesting that p16Ink4a, p19Arf, and p53 are downstream effectors of Bmi1 that are involved in the control of the proliferation and survival of HSCs during self-renewing cell divisions (Figure (Figure2).2). Therefore, Bmi1 maintains the HSC pool in part by repressing genes involved in cellular senescence. Increased expression of the p53 target gene Wig1 in Bmi1–/– bone marrow suggests that the p19Arf pathway may have been activated in Bmi1–/– hematopoietic cells. Wig1 is a double-stranded RNA-binding protein and inhibits tumor growth in vitro, suggesting that it may function in stress-induced p53 responses (43). The observation that p53-deficient mice have increased numbers of stem cells is consistent with the notion that p53 might be a downstream effector of Bmi1 (44). In addition, some of the Hox9 family of genes are also affected in Bmi1-deficient hematopoietic tissues and neurospheres (25, 26). Determination of the relative contribution of each of these pathways to the regulation of HSC self-renewal will require careful analysis of the HSCs from double- or triple-knockout mice.\nThere is evidence that Bmi1 might regulate telomerase expression in human mammary epithelial cells (MECs) and might play a role in the development of human breast cancer. Bmi1 is overexpressed in several breast cancer cell lines and postselection human MECs immortalized with human papilloma virus E6 oncogene, which abrogates the p53/p21waf pathway (45), suggesting that Bmi1 might be involved in immortalization. Postselection MECs can be obtained by regular feeding of a heterogeneous population of MECs from primary mammary tissue. During this process, the p16Ink4a gene is progressively silenced and not expressed in postselection MECs (46, 47). Overexpression of Bmi1 in postselection MECs bypasses senescence, extending replicative lifespan and immortalizing MECs. This is associated with human telomerase reverse transcriptase (hTERT) expression, which leads to induction of telomerase activity. Although hTERT is a direct target of c-Myc–induced transcription in MECs (48, 49), Bmi1 appeared to act independently of c-Myc. Since Bmi1 is a transcription repressor, induction of telomerase is probably mediated by an indirect mechanism. Deletion analysis of the Bmi1 protein suggested that the RING finger, as well as the conserved helix-turn-helix domain, was required for its ability to induce telomerase and immortalization. These data suggest that Bmi1 directly or indirectly regulates telomerase expression in MECs and might play a role in the development of human breast cancer. However, Bmi1 induction of telomerase is cell type specific; Bmi1 fails to induce telomerase in fibroblasts (45). This is consistent with the observation that Bmi1 overexpression did not immortalize human fibroblasts (37). It is not known whether Bmi1 is involved in telomere function in normal breast stem cells.\nBmi1 maintains the stem cell pool by preventing premature senescence, either through repression of genes involved in senescence or perhaps through induction of telomerase to prevent telomere shortening. It is very likely that Bmi1 is important for maintenance of multiple types of somatic stem cells, since it is widely expressed and Bmi1-deficient mice have developmental defects in other organs. Bmi1 is also important for maintenance of leukemic stem cells and perhaps other tumorigenic stem cells; therefore, Bmi1 could be used as a molecular target to induce senescence in cancer stem cells (50).\nSince Bmi1 maintains the HSC pool size and regulates key genes implicated in senescence and aging, it is of interest to determine whether expression of Bmi1 and its target genes changes during stem cell transplantation and/or aging. Whether stem cells undergo senescence during aging is controversial (51–53). In C57BL mice, in which most HSC studies have been performed, HSC numbers increase with age without losing overall function (54–56). However, HSC senescence might occur during aging in certain other strains of mice (57, 58). The number of times that HSCs can reconstitute the bone marrow of lethally irradiated mice is limited in serial-transplantation experiments. This observation might be either a result of an intrinsic stem cell aging program that occurs only when stem cell proliferation far exceeds that seen during normal aging, or a result of damage to the stem cells that is secondary to the stress of the transplant. In either model, it is possible that the loss of stem cell activity is mediated by Bmi1 or its downstream targets.', 'Telomeres and Telomerase\nThe ends of eukaryotic linear chromosomes end in repetitive DNA repeats called telomeres. In humans, chromosomes end in ~5-15 kilobase pairs of the tandem repeat 5′-TTAGGG-3′; this is terminated with a few hundred nucleotides of single-stranded TTAGGG repeats, leaving a 3′-overhang. Telomeric DNA is further bound with sequence-specific binding proteins in a six-protein complex termed ‘shelterin’.\nTogether, these structural features provide a protective cap to distinguish the end of the chromosome from an internal double-strand break, thereby protecting chromosome ends from aberrant DNA repair and recombination.\nA dynamic feature of telomeres is that they shorten during every cycle of DNA replication and cell division; this shortening is a consequence of the inability of the DNA replication machinery to fully replicate to the end of a linear template. In humans, telomeres shorten by ~50-150 base pairs through each cycle of DNA replication. When telomeres have shortened sufficiently, the cellular senescence program is activated, resulting in permanent withdrawal from the cell cycle. Telomere shortening is a powerful and essential tumour suppressor mechanism; unless nascent premalignant cells can prevent it from occurring, they will eventually stop dividing and be unable to form a tumour. Cells that have high or unlimited proliferative capacity must have a mechanism to counteract telomere shortening.\nTelomerase is the ribonucleoprotein enzyme complex that catalyses the addition of telomeric DNA repeats. The catalytic protein component, Te\nranscriptase (hTERT in human telomerase), catalyses nucleotide addition onto the 3′-end of the telomere and contains the conserved reverse transcriptase motifs common to retroviral RTs. The RNA component (hTR in human telomerase) directs nucleotide addition by providing a template for reverse transcription that is complementary to the telomeric repeat sequence.\nOnce nucleotide addition has reached the end of the template, the enzyme translocates a distance of six nucleotides to repeat the nucleotide addition cycle. Thus, telomerase is able to replace the telomeric DNA repeats that are lost during the process of DNA replication.\nA telomere maintenance mechanism is essential for a cell to bypass senescence as a response to short telomeres. While most normal cells lack a telomere maintenance mechanism, excessive and dysregulated telomerase activity is strongly associated with cancer. By expressing abnormally high levels of telomerase, cancer cells are able to avoid telomere shortening and thus bypass senescence, thereby becoming immortal. Telomerase is expressed in ≥85% of all human cancers and is considered a prime target for anticancer therapeutics through the development of telomerase inhibitors.\nResearch in the CBU\nOur research projects are collectively directed at understanding the properties of telomerase at the cellular and biochemical level, including association with its telomeric DNA substrate, active recruitment of telomerase to the telomere, telomerase enzymology, and structure, with the long-term aim of using this knowledge to rationally design small-molecule inhibitors of telomerase as potential anti-cancer therapeutics. Much of our research is enabled by our ability to over-express and purify human telomerase on large-scale.\nBelow are descriptions of specific projects on-going in the Cell Biology Unit for fellow researchers that may be interested in collaboration, or prospective students wishing to carry out their Honours or PhD research degrees in the CBU:']	['<urn:uuid:1f221e54-43a6-4908-a225-d7b3b2ce480f>', '<urn:uuid:fb0bf2f7-ce51-4af2-9c66-00f07f8fb207>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-13T00:20:10.754925	29	76	2987
14	How does light deprivation help gardeners extend their growing seasons beyond natural flowering cycles?	Light deprivation helps extend growing seasons by manipulating how much light plants receive daily. Different plants require varying amounts of light - some need 12 hours, others 18 or 6 hours per day. These different light requirements are partly why plants flower at different times throughout the year. By controlling a plant's light exposure, farmers and gardeners can grow plants outside their regular flowering cycles. This is achieved by using methods to shield crops from sunlight for specific periods, allowing growers to help plants flourish even when it's not their natural growing season.	['Learning the art of organic gardening is a terrific way to put healthier and more natural foods into your diet. Shopping at organic grocery stores is an expensive proposition—something that serves as a barrier to entry for many families interested in following organic diets. However, when you can grow your own organic fruits and vegetables, you can enjoy the benefits of an organic diet without the high costs associated with shopping organic.\nThe problem is that organic gardening, for many of us, is not possible on a year-round basis. Even if you can fill your fridge with organic fruits and veggies during the spring, summer, and fall, the question of what to do in the wintertime becomes a pressing one. Even if your area doesn’t have a particularly rough winter season, you are still at the mercy of Mother Nature and her standard biological flowering cycles for different plants. While you can buy many types of fruits and veggies year-round at the nearest grocery store or organic co-op, enjoying crops out of season is more difficult when you are trying to grow just about everything on your own.\nExpanding Your Growing Season with Light Deprivation Strategies\nFarmers and gardeners have long understood the role of light in plant growth. It was more recent that growers realizedhow important darkness is in the growth cycle. Plants need to enjoy a nice balance of light and dark to flower and grow with maximum health. The balance is different from one plant to the next: some plants need 12 hours of light a day. Some need 18. Some need six.\nThis variation is partially responsible for why different plants flower at different times throughout the year. It stands to reason, thus, that a farmer or gardener could grow a plant outside of its regular flowering cycle by manipulating how much light it receives every day.\nThis type of manipulation in plant light exposure is known in the world of gardening as light deprivation. This concept, same as our understanding of how light influences the growth of a plant, is not exactly new. When you try to grow plants in your house, you might place them on the windowsill to make sure they are getting enough sunlight. When you grow plants with a grow light, you are manipulating the amount of light the plants are getting each day—a form of light deprivation.\nSay you are trying to grow a specific type of plant that is only supposed to get five hours of light exposure a day. If you wanted to try light deprivation, you might use tarps to shield the crop from the sun for most of the day. By manipulating the amount of light exposure that the plant is receiving, you can help it flourish while also giving yourself a better chance of successfully growing the plant out of season.\nThe Disadvantages of Manual Light Deprivation\nOf course, there are a few major disadvantages to this method of light deprivation. First of all, if you are trying to grow any crop in bulk, using tarps to block out the sunlight will never be an efficient way of getting the job done. Secondly, using tarps for light deprivation is an imperfect practice in the first place. What if you go away for a few days or weeks and need to leave your plants behind? Leaving the plants uncovered and exposed to the sunlight all day would risk their health. Leaving them in the dark would starve them for sunlight and result in the death of your plants. You would have to pay someone to pull and replace the tarps each day.\nWith manual light deprivation, you also have to depend on your memory to keep your plants alive. For light deprivation to be successful, you need to follow a consistent schedule. If you forget to deal with your tarps one day, the mistake could cost you your plants.\nUsing Automated Light Deprivation\nThe good news is that there is a better way of using light deprivation for gardening or farming. Specifically, a light deprivation greenhouse can allow for automated light deprivation. Instead of having to pull tarps off your plants every day, an automated light dep greenhouse can do the job for you. By using a series of motors and blinds, the greenhouse can allow for periods of sunlight exposure and darkness—according to a pre-programmed schedule. As a result, modern farmers and gardeners can enjoy the perks of light deprivation growing without the inconvenience of the manual tarp-pulling method of old.']	['<urn:uuid:6344ae5b-4b29-465e-ade0-fc46a8d20243>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T00:20:10.754925	14	93	756
15	what number of instruments play typical classical sonata movement	A sonata is a work for one or two instruments, though since Bach's time (when trios were called sonatas), the term is not applied to works for more than two instruments.	"['SonataSonata (From Ital. sonare, to sound), in music, originally merely a piece ""played"" as opposed to cantata, a piece sung, though the term is said to have been applied once or twice to a vocal composition. By the time of Arcangelo Corelli two polyphonic types of sonata were established, the sonata da chiesa and the sonata da camera.\nThe sonata in its main classical significance is a work for one or two instruments consisting of a group of movements, four movements being the full scheme; the last movement in the same key as the first; each movement normally in one tempo, complete in design, independent from the other movements in themes, but aptly related to them in key and style; and constructed in the sonata forms. Though, since the time of Bach (when trios were called sonatas), the term is not applied to works for more than two instruments, the full (and even the normal) characteristics of this most important of all instrumental art-forms are rarely revealed except in trios, quartets, &c., and symphonies. A movement is a piece of music forming a complete design, or at least not merely introductory; and within such limits as either to contain no radical change of pace or else to treat changes of pace in a simple and symmetrical alternation of episodes. The first complete movement of a sonata seldom leads without break to the others, except in modern examples; but the later movements are often connected.\nThe sonata da chiesa, generally for one or more violins and bass, consisted normally of a slow introduction, a loosely fugued allegro, a cantabile slow movement and a lively finale in some such binary form as suggests affinity with the dance-tunes of the suite. This scheme, however, is not very clearly defined, until the works of Johann Sebastian Bach and George Friderich Handel, when it becomes the sonata par excellence and persists as a tradition of Italian violin music even into the early 19th century in the works of Boccherini.\nThe sonata da camera consisted almost entirely of idealized dance-tunes. By the time of Bach and Handel it had, on the one hand, become entirely separate from the sonata, and was known as the suite, partita, ordre or (when it had a prelude in the form of a French opera-overture) the overture. On the other hand, the features of sonata da chiesa and sonata da camera became freely intermixed. But Bach, who does not use those titles, yet keeps the two types so distinct that they can be recognized by style and form. Thus, in his six solo violin sonatas, Nos. 1, 3 and 5 are sonate de chiesa, and Nos. 2, 4 and 6 are called partitas, but are admissible among the sonatas as being sonate da camera.\nThe sonatas of Domenico Scarlatti are a special type determined chiefly by those kinds of keyboard technique that are equally opposed, on the one hand, to contrapuntal style, and, on the other hand, to the supporting of melodies on a lifetess accompaniment. Longo\'s complete collection of Scarlatti\'s sonatas shows that, short of the true developed sonata-style, there is nothing between the old sonata da chiesa and Beethovenishish experiments in unorthodox \'complementary keys\' that Scarlatti does not carry off with a delightfully irresponsible ""impressionism"" that enables him to be modern in effect without any serious modern principle. Great, however, as the variety of his forms is now known to be, and numerous as are the newly published slow movements, the normal Scarlatti sonata is that which the concerl-player popularizes; fireworks in binary form, with a perfunctory opening, a crowd of pregnant ideas in the complementary key, and, after the double bar, a second part reproducing these ideas as soon as possible in the tonic. The sonatas of Paradies are mild and elongated works of this type with a graceful and melodious little second movement added. The manuscript on which Longo bases his edition of Scarlatti frequently shows a similar juxtaposition of movements, though without definite indication of their connexioii The style is still traceable in the sonatas of the later classics, whenever a first movement is in a uniform rush of rapid motio1s, as in Mozart\'s violin sonata in F (Kochel\'s Catalogue, No. 377), and in several of Clementi\'s best works.\nSee also: sonata form']"	['<urn:uuid:fbd7ee53-6b27-4821-a180-991446787630>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T00:20:10.754925	9	31	717
16	learning electric motors why does my brushed motor have sparks coming from it	Sparking occurs at the brush contacts when the reversal of current (commutation) is not completed during the short circuit time. This is called poor commutation and can damage the commutator surface due to overheating. To achieve ideal commutation, the reversal of current should be completed by the end of the commutation period, which is approximately 1/500 second.	['The voltage generated in the armature, placed in a rotating magnetic field, of a DC generator is alternating in nature. The commutation in DC machine or more specifically commutation in DC generator is the process in which generated alternating current in the armature winding of a dc machine is converted into direct current after going through the commutator and the stationary brushes.\nAgain in DC Motor, the input DC is to be converted in alternating form in armature and that is also done through commutation.\nThis transformation of current from the rotating armature of a DC machine to the stationary brushes needs to maintain continuously moving contact between the commutator segments and the brushes. When the armature starts to rotate, then the coils situated under one pole (let it be N pole) rotates between a positive brush and its consecutive negative brush and the current flows through this coil is in a direction inward to the commutator segments.\nThen the coil is short circuited with the help of a brush for a very short fraction of time (1⁄500 sec). It is called commutation period. After this short-circuit time the armature coils rotates under S pole and rotates between a negative brush and its succeeding positive brush. Then the direction is reversed which is in the away from the commutator segments. This phenomena of the reversal of current is termed as commutation process. We get direct current from the brush terminal.\nThe commutation is called ideal if the commutation process or the reversal of current is completed by the end of the short circuit time or the commutation period.\nIf the reversal of current is completed during the short circuit time then there is sparking occurs at the brush contacts and the commutator surface is damaged due to overheating and the machine is called poorly commutated.\nPhysical Concept of Commutation in DC Machine\nFor the explanation of commutation process, let us consider a DC machine having an armature wound with ring winding. Let us also consider that the width of the commutator bar is equal to the width of the brush and current flowing through the conductor is IC.\nLet the commutator is moving from left to right. Then the brush will move from right to left.\nAt the first position, the brush is connected the commutator bar b (as shown in fig 1). Then the total current conducted by the commutator bar b into the brush is 2IC.\nWhen the armature starts to move right, then the brush comes to contact of bar a. Then the armature current flows through two paths and through the bars a and b (as shown in fig 2). The total current (2IC) collected by the brush remain same.\nAs the contact area of the bar a with the brush increases and the contact area of the bar b decreases, the current flow through the bars increases and decreases simultaneously. When the contact area become same for both the commutator bar then same current flows through both the bars (as shown in fig 3).\nWhen the brush contact area with the bar b decreases further, then the current flowing through the coil B changes its direction and starts to flow counter-clockwise (as shown in fig 4).\nWhen the brush totally comes under the bar a (as shown in fig 5) and disconnected with the bar b then current IC flows through the coil B in the counter-clockwise direction and the short circuit is removed.\nIn this process the reversal of current or the process of commutation is done.\nMethods of Improving Commutation\nThere are three methods of sparkles commutation:\n- Resistance Commutation\n- Voltage Commutation\n- Compensating Windings']	['<urn:uuid:1ef48815-7213-43c7-9e19-d780644502aa>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T00:20:10.754925	13	57	613
17	northeastern united states maple production challenges economic benefits today	Maple production in the northeastern United States faces natural challenges requiring specific conditions - temperatures around 45°F during days and below freezing at nights. Despite these challenges, the industry has significant economic impact. In New York State alone, about 1500 producers with nearly 1.5 million taps generated 332,000 gallons of syrup worth approximately $7.5 million in 2008, with a $30 million impact on the state economy. Modern producers have adapted by incorporating agritourism, with farms like Silloway Maple offering tours, demonstrations, and direct sales to maintain profitability and follow the principle that 'a farmer has to sell something every day.'	"['The Maple Moon or Moon of the Worm\nMarch is a month of transition between winter and spring, often with an unpredictable season almost all its own. The temperatures in Upstate New York can range from an unusually warm 80 F down to sub zero Fahrenheit. This pulsating tension between the cold winter and warmer weather patterns and increasing daylight coincides with the vernal equinox, the Pascal Moon, and the Easter season to the European mind.\nBefore the French explorers of the 16th century and English, French, and Dutch settlers of the 17th century, the indigenous peoples knew the full moon near the equinox as the Worm Moon. This was when the worms started bringing castings to the surface, and robins started to reappear after their winter\'s absence.\nThere were other names for the moon by some tribes. The Crow Moon honored the late winter flocking of crows. In the far north, the Crusty Moon recognized the particular hard crusting of snow in late winter. Some tribes came to call it Sap or Maple Moon. This latter name evolved from the rising flow of the sap as days start to warm and winter changes to spring in fits and starts.\nEarly origins of Maple\nThe exact origins of the discovery of maple sap and its distillation, or processing, into syrup is impossible to determine with any firm historical accuracy. While a few authorities think the processing of the sap was taught to the Indians by the Europeans, they are in the minority. Most historians believe that various indigenous tribes had a knowledge of this sweet prior to contact with the Europeans. Lack of firm archeological proof and a lack of written records from the various tribes leave the debate open to conjecture and dispute, though there is much to support that the Indians did know about, and use this sweet, albeit in a much cruder fashion than is common today.\nMaple trees observed during his explorations of Quebec were written about by Jacques Cartier in 1540. Recognition of the Indians using the refined sap as sugar and syrup dates from about 1557 in writings of Andre Thevet. Details of collection and distillation of sap by the Micmac Indians of eastern Canada were noted by Marc Lescarbot in 1606.\nBefore the Europeans came, the eastern woodlands were populated by numerous tribes with similar customs. From eastern Canada, Quebec, and New England to the Great Lakes states, they celebrated many of the same moons and seasonal festivals, and ate mostly the same types of game. These varied but slightly from region to region. They often, however, spoke distinct languages in various language groups.\nIndian Legends on the Origins of Maple Syrup\nThere is a common myth, with many tribal variations, that the Creator originally made life too easy for his People, with maple trees having a syrup that flowed year round. One day, Glooskap (this name has many variations) arrived at a village and found it strangely quiet. No children or dogs came to greet him, the gardens were over grown with weeds, and the cooking fires were dead. He found the villagers lying in the maple grove, with the delicious sap running into their mouths from the trees.\nGlooskap had special powers. Using a birch bark bucket, filled with water from lake, he rose above the trees and filled the trees with water until the sap ran thin. Then he encouraged his People of the village with a fiery speech. In this exhortation, he berated them for being lazy, and said as punishment the Creator was going to have the sap run only in the late winter. But, he urged them to take heed that when this happened, they would still be able to enjoy this special sweet, though only at this special time of year.\nAnother legend tells that an Algonquin or Iroquois Indian chief, Woksis, discovered this sweet sap in the following manner. One day, at the time the time of the melting snow, as he prepared to go hunting in a meager season of want and little game, he took his ax out of a maple tree where he had struck it a few inches into the bark the night before. His squaw happened to have a wooden or birch bark basket underneath, which collected the sap. Thinking that her warrior husband had filled it already with water, the squaw Moqua used the sap to cook some meat, most likely venison, though one source says moose meat. Upon his return, he was surprised by the sweet odor of the cooking meat. When eaten, the meat was sweet. They soon realized that this sweetness came from the sap of the maple tree.\nWhile details are sketchy, this evolved into an annual festival of sorts, celebrated in traditional ways by the Onondagas in Central New York as recently as the 1940s. While this time of year was usually called the Worm Moon, some tribes began to call it the Maple Moon.\nSoon they began to have a maple festival to celebrate this sweet that was available only during this time of change from winter to spring. Maple sap and syrup became a major source of sweetening, rivaling honey. Both of these sweets were an important food to the Iroquois., comprising about 12% of their diet.\nIndian Methods of Sap Collection and Syrup Production\nLacking metal working capabilities prior to the European contact, the Iroquois methods of obtaining and evaporating the sap were crude. They hacked or gouged the tree with hatchets or axes, which often killed the tree. They used bowls, of ceramic or white birch bark to collect sap. Different bowls of pottery or wood, usually troughs of hollowed out trunks, were used for boiling the sap. They placed hot stones from the fires into the containers of sap. This brought the sap to a boil. The hot stones were periodically replaced to continue the process. Occasionally, if conditions were ideal, they froze the sap, peeling off the frozen surface daily. They threw away the ice and ate the residue underneath.\nMaple season was fleeting and unpredictable, and it remains so to this day. High quality syrup production requires warm days to about 45 F or so alternating with nights below freezing. If days are exceptionally warm, or too many nights remain above freezing, the quality of the sap collected and the syrup produced suffers. There are large variations of the sap run, from nine days to 57 days, the average being 37 days of collection.\nEuropean Adaptations and Expansion of Maple Production\nDuring the almost two centuries of contact the Iroquois and Algonquins had with the French, English, and Dutch, the Europeans learned about this maple production, and started to make improvements with their superior technologies. The fickleness of nature cooperating by providing sufficiently warm days and sub-freezing nights resulted in the attempts at moving maple syrup production into Virginia by Thomas Jefferson and Benjamin Rush, among others, to be modestly successful at best, and finally abandoned. Maple production was much better quality and more reliable in New England, New York, and Quebec. While maple syrup production has occurred at one time or another in some 30 (present) US states, it was, and is, more common and of better quality in the north.\nBy the time of American Independence and subsequent dispersal of most of the Indians from the northeast, the Europeans had evolved methods of improved production of the maple sap and its sweet products that only very gradually changed over the next century and a half. It became a northeastern tradition of the short transitional season between winter and spring that provided a sweet loved by most Americans to this day.\nThe Maple Weekends in March in New York that have become increasingly popular in recent years owe their existence to the accidental discovery of this tasty treat by an Indian chief several centuries ago. As such, it is an important part of our historical heritage, as well as the specific natural conditions in our region that make it possible.\nReferences - Books\nLawrence, James M., and Martin, Rux. Sweet Maple: Life, Lore & Recipes From the Sugarbush. Shelburne, Vermont: Chapters Publishing Ltd., 1993. Co published by Vermont Magazine, Montpelier, Vermont, 1993.\nKlees, Emerson. Legends and Stories of the Finger Lakes Region. Rochester, NY: Friends of Finger Lakes Publishing, 1995\nSchery, Robert W. Plants for Man. Englewoods CLiffs, N.J.: Prentice-Hall, Inc., C.1952, 4th printing 1959.\nThe Old Farmers\'s Almanac, editions for 2008, 2009, 2010. Yankee Publishing, Dublin, NH.\nReferences - Internet\nThe Natural world Full Moon Names and Dates\nWakarusa (Indiana) Maple Syrup Festival\nOne Iroquois Legend\nA Sure Sign of Spring in New York State\nMaple Syrup production is largely restricted to the northeastern United States and eastern Canada. This is the natural range of sugar and black maple, the two most commonly tapped species. Silver and red maple sap is higher in water content and provides a poorer quality end product.\nThere is little doubt that the Iroquois and other woodland Indians knew about the maple sap. There are several legends of just how they discovered this sweet, an important seasonal addition to their diet. As they lacked metal tools prior to European contact, their means of collection and refining maple sap were crude.\nThe European settlers gradually improved upon Indian methods of collecting and refining sap in many ways. These evolved into ""traditional"" ways of producing syrup that were pretty standard for perhaps a century or more. Only in the latter half of the 20th century did most producers go ""hi-tech"" in production methods that are commonly used today. However, some small producers and museums still produce small amounts of syrup using largely pre-mechanized methods. This is largely for the benefit of tourists and visitors, placing the methods of production in historical context.\nMaple Producers and Festivals in New York State\nNew York State is the second leading producer of maple syrup, second only to Vermont. According to statistics from the New York State Maple Producers Association, about 1500 producers statewide, with nearly 1.5 million taps made 332,000 gallons of syrup in 2008. The average cost per gallon of finished syrup was $33.50 in 2007. The final value of the crop was about $7.5 million, with an impact of $30 million on the New York State economy.\nAccording the published statistics in the Utica Observer-Dispatch of March 18, 2009, maple syrup production in selected counties was as follows in 2005:\nOneida County: 32 producers with 2,100 gallons of syrup.\nHerkimer County: 19 producers with 1,400 gallons of syrup.\nLewis County: 141 producers with 30,000 gallons of syrup.\nMany small maple sugar producers have in recent years had small scale maple festivals at this time of year. Producers and historical museums often sponsor special events and demonstrations. The Farmer\'s Museum in Cooperstown has Sugaring Off Sundays, with breakfast (pancakes and maple syrup, of course) and maple syrup production demonstrations. The Herkimer Home outside Little Falls had its 34th annual maple celebration on the first weekend in April 2009. Other museums or producers have had such programs usually in late March and early April.\nNew York State Maple Producers Association.\nWhat began in the late 1990s as a one day event promoting maple products by a small group of Wyoming County maple producers has evolved into a festival celebrated across New York State. This effort has been coordinated by the New York State Maple Producers Association. Over 100 large and small producers from over 40 counties participate on the last two weekends in March. Typically, there are pancake breakfasts, sales of syrup and candy, and demonstrations on past or current production methods. A list of participating producers is available from their web site listed in references below.\nWith this regional agricultural product, the maple industry in New York State is on the verge of expanding quite rapidly. Such coordination of an industry that is historically and commercially important is important to its success as an industry. If people in this area are committed to promoting tourism, whether to local residents or outside visitors, this is a model to be studied as largely successful in the general field of local agriculture.\nThere is discussion of local maple producers banding together even more and forming a cooperative in Northern New York for bottling and packaging of their maple products. This would provide the largely rural maple producers a more effective means of getting their products to market, as well as providing jobs to an area with limited employment.\nThe American Maple Museum\nIn Crohgan, New York (northeast of Lowville on State Route 812), this museum is one of the few maple oriented places that is open outside of maple sap season. The museum presents one of the largest collections of vintage equipment used in maple production over the years. They open Memorial Day to late June on Friday, Saturday and Monday, and daily except Sunday July 1 to around Labor Day. Their web site is a good source of information on all aspects of historical and current maple sap collection and processing.\nThe maple industry in New York State is on the verge of tremendous expansion. If such a cooperative comes into existence, it could expand the impact of maple sugar products on the local and regional agricultural economy. In any event, the maple sap season of late February to mid April is a seasonal activity which clearly marks the transition from winter to spring in the Upper Mohawk Country of New York State.\nNew York State Maple Producers Web Site, Maple Weekend http://www.mapleweekend.com/index.html\nUtica Observer-Dispatch, March 18, 2009\nMaple Museum, Croghan\nLawrence, James M., and Martin, Rux. Sweet Maple: Life, Lore & Recipes From the Sugarbush. Shelburne, Vermont: Chapters Publishing Ltd., 1993. Co published by Vermont Magazine, Montpelier, Vermont, 1993\nDISCLOSURE OF MATERIAL CONNECTION: The Contributor has no connection to nor was paid by the brand or product described in this content.', 'A typical day at Silloway Maple, a 70-year-old maple sugaring business in Randolph Center, Vermont, looks a lot like it did 50 years ago. Neighbors, local school children and out-of-towners tour the facility and sample hot, fresh syrup. But in today’s world of “agritourism,” Silloway depends on these tours to turn a profit.\n“There’s a new word – agritourism – that’s become very popular,” said Silloway’s owner Bette Lambert. “It’s how farmers increase exposure to consumers, show off their products and hopefully sell some and educate people on how [farmers] really do live. Sometimes there are misconceptions about animal care or care of the land, but if you take excellent care of the animals and the land, it makes the most profit.”\nRecently for maple sugar farms, part of making more money has been opening the farm gate by hosting open houses and tours. For Lambert, the sugaring bug is for sharing.\nSilloway Maple has a lot to share besides experience and product. For one, there’s a new solar-powered sugarhouse with a second floor platform that offers a spectacular view of the boiling process. Visitors bask in the clouds of steam, embracing the photo opportunity.\nLast summer, Silloway also put on a tour with a group of dairy farms organized by the Chamber of Commerce. The farm’s lone sugar tour included a tapping demonstration – first with an old-fashioned brace and bit, and then with a battery drill – and a maple sugar covered nut-making demonstration. Family passed out homemade raised doughnuts and syrup.\nA project designed and built by Lambert’s son Paul, the 30-foot by 60-foot sugar house took less than a year to complete and is powered by 70 solar panels. As the sugar house manager, Paul is referred to by Lambert as “the real energy behind the sugaring operation”.\n“People always want to know if we boil with the sun,” says Lambert. “We do not, but it provides the energy for our reverse osmosis.”\nIn all, Silloway taps an ever-expanding 6,100 trees and makes 2,500 to 3,000 gallons of syrup a year. It sells some in bulk, some at farmer’s markets and some through mail orders on its website (http://sillowaymaple.com).\nFor the two-day association tour, the concern is always the same.\n“We always hope we have sap to boil,” Lambert said, who otherwise offers products, demonstrations and even a film on the story of sugaring. “There’s nothing like being there when we’re boiling.”\nNext up, Lambert noted she’s interested in accepting bus tours, anything to keep up an old adage her grandfather used to say, “A farmer has to sell something every day.”\nThough the sugarhouse was always open for visitors, these days it’s about hoping to sell some syrup and other maple products, coupled with the “passion for sharing our passion – the pure, delicious products we make, our love for farming, the unpredictability of it, the weather challenges and family,” she said.\nA Group Effort in Pennsylvania\nIn its 12th year, the Northwest Pennsylvania Maple Association Taste and Tour, held last March, involved 15 sugarhouses in three counties (Crawford, Erie and Warren) across the state.\nThe Sagertown-based association hosts several other events throughout the year including its annual meeting earlier this year where they covered industry-related topics covering vacuum tubing installation, evaporator systems, reverse osmosis and syrup hydrometers. Association president and event chairman Bill Phillips noted that the annual tour event is one of their biggest of the year offering visitors an education as well as a dispelling of some sugarmaking misconceptions.\n“A lot of people have heard about maple syrup, but they still buy it at the grocery store and think it’s maple. They don’t realize that it’s called pancake syrup (at the grocery store),” he said. “A lot (of people) haven’t even seen a maple tree, and they don’t realize why maple syrup costs so much until they’re at the house and see all the work that goes into making a gallon.”\nThe taste and tour event featured many award winning producers like Laura Dengler and Bob Kent of “How Sweet It Is” in Sagertown. The 1,100-tap farm won 2013 Best of Show at the Crawford County Fair for its maple sundaes, milkshakes and hot dumplings.\nMaple Lane Maple Farm, out of Union City, earned the Farm Show’s 2014 Best of Show designation. Owner Greg Walberg and his family have been producing award-winning syrup at their 1,000-plus tap farm since 1944. The open house tour also included Casbohm Maple Products which earned the 2014 Best of Show at the Crawford County Fair.\nOther regional farms like Hurry Hill in Edinboro, earned a 2013 Premiere Exhibitor at the Pennsylvania Farm Show and offered several agritourism attractions like a wagon tour of its woods to learn how pure maple syrup is made. Hurry Hill Farms also included a museum featuring “Tree to Table” exhibits, sugaring antiques, maple production displays and children activities.\nIt’s good for public relations, as well as profits. Phillips, who also runs Fort LeBoeuf Maple Farm in Waterford, Pennsylvania, noted he sold nearly 100 percent of the maple items produced up to that point. By year’s end, Fort LeBoeuf’s open house results amounts to 30-40 percent of all sales.\nPhillips said having this event is essential for the farms as it gives a boost to their bottom lines while educating the customers on maple production. As it turns out, learning can turn into a fun game. He stated that as the taste and tour evolved, the yearly open houses have morphed into a tourist sport known as “sugarhouse hopping.”\n“We’re growing every year, reaching new people,” Phillps said. “When they visit the farms, they ‘sugarhouse hop’ to about 2 or 3 farms in the same day, then maybe 2-3 more the next day. Overall , it takes a few years to get through them all.”\nThat growth has translated into increasing sales for the farms, Phillips noted, as consumers learn that maple is much more than sauce on a pancake. It’s an investment in a natural, gourmet product.\n“When we started this, people would come into a sugarhouse and buy a half-pint of syrup as if they were obligated to do so,” he said. “But as they come back year after year, they realized they enjoy the products and purchase a half-gallon the next time. As they go through the process, they understand it better and appreciate the work that goes into it.”\nCover Photo by LifesizeImages/istockphoto.com']"	['<urn:uuid:2f6f3abd-3f65-49a8-8fda-8fb7eecf0a4c>', '<urn:uuid:706a535f-dbbd-42a6-9930-8a4aff6cff69>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T00:20:10.754925	9	100	3401
18	As someone interested in environmental issues, I've heard that our planet's resources are being used up too quickly. What are governments and businesses doing to address this problem?	Both governments and businesses are taking significant steps to address resource depletion. The Global Footprint Network has found we're consuming 1.75 times more resources than the planet can regenerate, leading to several initiatives. At the government level, China has made circular economy a national priority through five-year plans and provides financial incentives for vehicle remanufacturing. The European Union has implemented a Circular Economy Action Plan focusing on electronics, ICT, and textiles industries. Japan has released its Circular Economy Vision to enhance dialogue between companies and investors. On the business side, companies like Multivac are creating sustainable packaging solutions that reduce plastic usage and extend food shelf life, while Patagonia has been implementing second-hand initiatives and recycling experiments. International organizations like UNCTAD are also supporting developing countries through programs focusing on sustainable manufacturing and pollution reduction.	['The economic system is consuming natural resources at a rate that is not any more sustainable; the Global Footprint Network estimates that we are consuming 1.75 times more resources than the planet can regenerate, producing negative impacts on several natural ecosystems ranging from the depletion of resource stocks to biodiversity losses. In an effort to address environmental degradation and prevent potential supply shortages while supporting economic growth, many solutions have been proposed (e.g., UNCTAD, 2020); in this paper, we specifically introduce the circular economy (CE) approach. We aim at illustrating how this approach can increase resilience at a company and economic system level; also, we aim at providing clear and practical insights into how multinational enterprises (MNEs) and other actors of the global economy such as consumers, suppliers, policymakers, and international organizations can contribute to the implementation of the CE framework.\nResilience in the Context of Resource Scarcity\nResource extraction has more than tripled since 1970, from 27 billion tons to 92 billion tons in 2017, while the annual average material demand grew from 7 tons to over 12 tons per capita in the same period (Fig. 1). Global emissions almost quadrupled in the last decades, reaching now 35 billion tons of CO2 emission per year, and the amount of waste produced has reached over 2 billion tons of municipal solid waste every year; by 2050, this value is expected to increase to 3.4 billion tons. Also, climate-related issues and geopolitical tensions are increasingly determining disruptions in the supply of raw materials and volatility of their price, thus showing the vulnerability of the economic system towards resource scarcity–related risks.\nIn this context, resilience – the capability to anticipate, cope, recover from, and adapt to both periodic shocks and major disruptions – is becoming increasingly crucial (Dau & Moore, 2020; Linkov et al., 2013). In particular, multinational corporations that operate on a global scale are at higher risk of disruptions and should develop a robust resilience strategy, i.e. should be able to prevent and minimize the negative impact of scarcity-related shocks, recover from them quickly, adapt to the changes that may follow, and maintain or even improve competitiveness by taking advantage of emerging opportunities (Oh & Oetzel, 2022; Sutcliffe & Vogus, 2003).\nCircular Economy as a Pathway to Resilience\nThe CE framework is rising as a feasible option to transit to a more resource-efficient system in which economic growth is decoupled from material and resource depletion and supply risk, and to achieve resilience at a company and system level (Garcia-Saravia Ortiz-de-Montellano & van der Meer, 2022; Kennedy & Linnenluecke, 2022). CE is a systemic change whereby economic and business opportunities can be combined with environmental and social benefits and generate long-term resilience (Ellen MacArthur Foundation, 2015). Specifically, the CE paradigm aims to replace the linear take–make–use–dispose production pattern and aims to reduce the amount of resources used and waste generated, keep products and materials in use for as long as possible, and recover valuable materials and resources from waste (Bocken, De Pauw, Bakker, & Van Der Grinten, 2016). In doing so, it emphasizes the implementation of the 3R principles of Reduce, Reuse, and Recycle in production and consumption processes (Kirchherr, Reike, & Hekkert, 2017). All this suggests a shift in the way resources are used and waste is managed toward a system that minimizes resource consumption and negative environmental impacts and maximizes resource efficiency.\nHowever, this transition from a linear to a circular economy cannot be implemented through scattered actions; the far-reaching scale of the compulsory changes necessitates the support and coordination of lead actors of the global economy (Fehrer & Wieland, 2021) and MNEs are in the position to be at the forefront of this transition given their scope and power.\nThe Key Role of MNEs in CE Transition\nSeveral MNEs are embracing the CE paradigm and have already introduced circularity in their business models and global activities. For instance, Stellantis, one of the leading global carmakers and mobility providers, has recently announced the creation of a CE Business Unit to introduce circular principles in its business model. Namely, the aim is to extend the life of vehicles and parts ensuring that they last for as long as possible, and to return materials and end-of-life vehicles to the manufacturing loop for new vehicles and products. Also, it aims to create within-country loops for products and materials, and to quadruplicate revenues from recovered and remanufactured parts, as well as revenues from recycling in the coming decade.\nMNEs adopting a circular approach are not limited to the manufacturing industry. Several cases can be found also in the agri-food sector and among service providers and energy and commodity producers (Ku, 2022). For instance, Arla, one of the largest international dairy cooperatives in the world, while committing to the mainstream circular projects related to food packaging, is also investing in a bio-economic project to reconvert resources and waste streams into value-added products. Schneider Electric, a multinational provider of automation digital solutions, introduced circular solutions that cover its entire value chain activities, from the sourcing of green materials to the design of products easy to disassemble, reuse and recycle at the end of life, and the offer of several services of repair/refurbishment/recycling. In 2020, the company claimed that thanks to the take-back and recycling program they avoided 157,000 metric tons of primary resource consumption.\nIn the energy industry, Enel, an Italian MNE operating in the electricity sector, has increasingly adopted the CE paradigm since 2016 and its circular approach covers all the company’s business areas from power generation to distribution and infrastructure development. The company also collaborates with suppliers in the redesign of the value chain toward circularity.\nThus, the CE paradigm is a means for the MNE to develop resilience as it reduces the dependence on raw materials and the fragility of the supply chain by diminishing the exposure to disruptions, and provides MNEs with new profit opportunities and product differentiation strategies. First, CE can enable MNEs to prevent and minimize the negative impacts of a shock since it allows cost saving and optimization of the company processes, reducing the need for new materials and fuel supplies and, hence, the exposure to supply chain risks. Second, adaptive responses range from the extension of product life cycle through repair and reuse strategies to inner recovery and recycling loops of energy and materials. This reduces the revenue losses or cost increases caused by supply shortages of all kinds. Third, by adopting a CE approach, MNEs can maintain competitiveness even in the face of shocks, by increasing their profit opportunities through the generation of new revenue streams deriving from the sale of components and from the control of the second-hand markets after the extension of the useful life cycle of their products, and from waste management and diversification into new markets of recycled materials or products.\nIn addition, by improving the value chain’s transparency, the CE paradigm promotes better control of supply chain criticalities and quick identification of bottlenecks; in fact, robust collaboration among partners favors adaptation, flexibility, and speed of response to any possible emerging issue. Lastly, the CE approach encourages companies to innovate and develop new business models that are based on circular principles increasing their resilience to market competition by taking advantage of emerging opportunities.\nMoreover, in so doing, MNEs also promote community and societal resilience. In fact, by shifting to a CE paradigm, MNEs not only improve their own resilience, but also may produce a cascading effect on the global value chains (GVCs) they orchestrate. They can support the development of local circular loops and the reduction of greenhouse gas emissions of the entire GVC. Also, the new circular loops extend the number of activities and actors operating in the value chain, and favor greater collaboration and coordination among partners, as well as monitoring of materials and supply flows. An example is offered by the partnerships between fashion and textile companies that adopt voluntary standards toward circularity and online platforms or providers of new materials. Finally, increased monitoring of activities, which is required in the circular approach, contributes to the accounting of corporate social and environmental achievements. This, in turn, allows the company to fulfill the interest of consumers, employees, and stakeholders and demonstrate its compliance with existing regulations (or stimulate the introduction of new standards).\nCompanies are increasingly conscious of the necessity to adopt environmentally and socially responsible practices and identify solutions to manage a rising number of risks (Cuervo-Cazurra, Doh, Giuliani, Montiel, & Park, 2022). Large corporations have the required economic and human resources to enable, finance, push and inspire other value chain actors to evolve according to the circular paradigm. However, many of them are simply not aware of the potential of CE strategies yet (Benito & Fehlner, 2022).\nThe shift toward the CE paradigm and the related resource efficiency and resilience requires innovation in technologies; MNEs play a crucial role also in the development of such innovations both investing directly in R&D activities and collaborating with innovative start-ups or research centers. The recent report on Patents for Tomorrow’s Plastic (EPO, 2021) shows that traditional big MNEs are technological leaders in recycling technologies for plastic (waste recovery, plastic-to-product recycling, and chemical and biological recycling) and alternative to plastic solutions (biodegradable and compostable plastics or plastics designed for easier recycling). Top innovators include Procter & Gamble, which in the period 2010-2019 applied for 38 International Patent Families (IPFs) related to waste recovery inventions, 91 related to product recycling, and more than 1600 related to bioplastic technologies in the three sectors of healthcare, packaging, and cosmetics and detergents. In the same period, Sabic and Honeywell are the top applicants in the field of chemical and biological recycling with 309 and 287 IPFs, respectively.\nThe Need for Global Collaborative Efforts\nMNEs are not the only enabler of the CE shift. Indeed, the circular paradigm requires the involvement and support of several actors, namely consumers and the public sector, operating at different levels. Consumers can orientate MNEs toward the adoption of a CE paradigm by demanding products that are designed for longevity, repairability, and recyclability. Additionally, they can support the CE transition by choosing to buy second-hand products, participating in sharing economy platforms and recycling their products. As an European Commission report of 2018 indicates, consumers are willing to engage in CE practices and the lack of information regarding product durability and reparability as well as the lack of sufficiently developed markets (e.g., for second-hand products, renting, leasing, or sharing services) is one of the main barriers for a behavioral shift (Cerulli-Harms, Porsch, Suter, et al., 2018).\nIn addition, consumers can also exert pressure on MNEs by demanding greater transparency and accountability for their actions, and by participating in collective actions or supporting organizations such as NGOs or consumer groups that advocate for more sustainable business practices. For example, in 2010, reports by NGOs exposed Nestle’s palm oil sourcing practices as being linked to deforestation and human rights violations, leading to widespread consumer backlash. As a result, Nestlé pledged to only source traceable palm oil and support the conservation and restoration of forests.\nOn the side of public policies, governments and public authorities are central to setting directions, rethinking the taxation system, and developing infrastructures and services that stimulate transparency and actions of the private actors. Also, governments’ actions are necessary to support MNEs in addressing trade-offs they may experience in the CE transition. For instance, the introduction of CE technologies can be initially difficult and expensive for MNEs before becoming cost-effective, as shown by the renewable energy example. Also, the transition to a CE paradigm may not be equally smooth in all the economic sectors and new circular solutions may need time to become profitable. Thus, it may not be desirable for all to pursue CE objectives. In this respect, several countries and regions all over the world recognize the support that CE can give to the construction of a resilient society and have already introduced frameworks and plans to support its implementation. China identified the CE as a national priority in the late 2000s developing since then 14 five-year plans. As a part of its strategy, China supports vehicle remanufacturing sector by providing financial incentives for enterprises to increase collection rate of components and creation of circular loops. The government is also supporting the development of technological advancement and standards and the collaboration among industrial actors and research institutions.\nLikewise, Japan released its Circular Economy Vision in 2020, and among other objectives, it aims to enhance the dialogue between companies adopting the CE principles and technologies, and investors and financial institutions, thus recognizing the necessity of a multi-stakeholder approach in this transition.\nAt a transnational level, the European Union recognizes the shift to a CE as one of the main strategies to prevent and face potential shortages of critical raw materials and construct a resilient European supply chain. In fact, the new European Circular Economy Action Plan, published in 2020, presents a detailed set of measures to be implemented in the next five years to enhance the circular transition in Europe. Among the principal measures, it aims to enhance the durability, reusability, and reparability of products in the electronics, ICT, and textiles industries, increasing their recycled content and restricting single-use and premature obsolescence.\nHowever, to accelerate the shift to the CE paradigm, national and transnational policies are not sufficient, as UNCTAD, UNIDO, and World Bank show with their commitment to multilateral cooperation in circular economy initiatives and the uptake and dissemination of circular practices.\nAn inspiring example from a close sector is a program developed in 2019 by the UNCTAD in collaboration with the UK Foreign Commonwealth and Development Office (FCDO) aimed at supporting Sub-Saharan African and South Asian countries. The primary objectives of the project include funding research and technology development aimed at mitigating the negative impacts of manufacturing activities, with a particular focus on addressing the issue of plastic pollution.\nTable 1 provides an overview of the key stakeholders involved in the transition to a circular economy, along with examples of best practices that represent some essential actionable insights for achieving this shift. To further advance the achievements of the single actors, it is crucial to develop an internationally coordinated policy framework that, taking into account the global nature of value chains and production systems, facilitates the trade of circular products and materials through harmonization of standards, promotes a change in consumption behavior through labeling and learning programs, and stimulates MNEs to adopt the CE principles in various contexts and institutional settings.\nAbout the Authors\nCristina Di Stefano is an Assistant Professor at the Department of Management, Economics and Industrial Engineering of the Politecnico di Milano (Italy). She holds a Ph.D. in Industrial and Information Engineering and Economics from the University of L’Aquila and had a 3-years PostDoctoral experience. Her research interests include Circular Economy and Global Value Chains reconfiguration and resilience. Her research has been published in national and international journals and conference proceedings.\nStefano Elia, Ph.D. in Management Engineering, is Associate Professor at Politecnico di Milano (Italy). He was ‘‘Dunning Visiting Fellow’’ at University of Reading, ‘‘Visiting Research Fellow’’ at the University of Leeds, and ‘‘Marie Curie Fellow’’ at Shanghai Jiao Tong University. His research interests deal with multinational firms from emerging countries, offshoring and reshoring of service and manufacturing activities, micro-foundation and behavioral economics applied to international business, digital technologies and internationalization, and sustainable international business. He authored several book chapters and refereed articles in leading national and international journals.\nPaola Garrone is Professor of Business and Industrial Economics at Politecnico di Milano University. She does research in the fields of business strategies and sustainable development, and economics and policy of utilities. Her most recent publications and research projects concentrate on sustainability-oriented innovation, circular economy and cross-sector partnerships, mainly in the context of food and water systems (https://orcid.org/0000-0002-0953-5007).\nLucia Piscitello (email@example.com) is Professor of International Business at Politecnico di Milano. Her research interests cover internationalization and innovation strategies of MNEs, and the impact of FDI on host and home economies. Her recent studies focus on digital technologies, resilience and reconfiguration of GVC. She is Visiting Professor at the Henley Business School, University of Reading, UK. She is Fellow of the European International Business Academy (EIBA) and of the Academy of International Business (AIB).\nIn particular, the cooperative treats and re-uses water extracted from whey during production processes, reducing the need to use drinking water supplies (www.arlafoodsingredients.com/about/responsible-nutrition/stronger-planet/circular-bioeconomy/wastewater-re-use/). Also, taking the whey waste created during cheesemaking, the company is producing food, feed, bio-based products, and bioenergy (www.arlafoodsingredients.com/about/responsible-nutrition/stronger-planet/circular-bioeconomy/).\nOver the last decade, Patagonia, the US producer of outdoor apparel, has been piloting a few second-hand and rental initiatives and recycling experiments, collaborating with new ventures and established partners (https://www.patagonia.com/stories/our-quest-for-circularity/story-96496.html).\nEach international patent family (IPF) covers a single invention and includes exclusively patent applications sought in more than one country.\nIt is worth observing that the field is highly concentrated, as 12% and 19% of all IPFs are generated by the top five and top ten applicants, respectively.\nThe UN’s Human Development Report (2022) shows that prices of utility-scale solar photovoltaics decreased by 89% between 2009 and 2019 and compared to 1991, lithium-ion batteries are 97% cheaper (https://hdr.undp.org/system/files/documents/global-report-document/hdr2021-22pdf_1.pdf).\nSustainable Manufacturing and Environmental Pollution Programme (SMEP). More information is available at: https://smepprogramme.org', 'Making Metal Production Greener\nJohn visits a small German town where SMS Group services the global steel and metals industry with innovative production technology and processes that contribute to circular economies, and significantly reduce CO2 emissions.\nFeaturing: SMS Group\nDisposable, Sustainable Food Packaging\nJohn travels to Baltimore, Maryland to find out how AmerCareRoyal, a global resource for more than 6,000 disposable products used by the food service industry, makes degradable, easily recyclable packaging.\nThe company’s PrimeWare products are made from sugar cane, corn, and bamboo fiber. This packaging makes food look appealing, keeps it fresh, to avoid waste – and it’s sustainable too!\nPackaging that Prevents Food Waste\nJohn travels to southern Germany, to discover an intriguing solution to one of the world’s most serious environmental problems: food waste.\nMultivac creates food packaging options that can significantly reduce plastic usage and utilize highly renewable materials – all while extending the shelf life of valuable food which would otherwise be wasted. These innovative solutions also offer high recycling potential, adding to their sustainability.\nClosing the Loop from Plastic Production to Re-Use\nNext, John goes to Channahon, Illinois to discover why all plastic is not created equal. One company, INEOS Styrolution, is educating people about the many valuable and essential benefits of plastics like polystyrene.\nINEOS Styrolution is dedicated to advancements in plastics recycling technologies. From production to re-use, they are closing the loop to keep plastics in a circular economy and out of landfills.\nRecycling 800,000+ Vehicles\nJohn visits a gigantic 21st century salvage yard in Casa Grande, Arizona. He finds out how LKQ Corporation, the world’s largest auto recycler, processes more than 800 thousand cars every year at their worldwide facilities.\nRecycled parts from these end-of-life vehicles include electronics, transmissions, tires, metals, headlamps – even usable oil. In all, LKQ’s recycling team scours vehicles for more than 130 parts that are resold rather than dumped into a landfill. They’re used for car repairs, or repurposed for new items such as playgrounds, steel parts, and roads. The result: less pollution and reduced global warming.\nGhost Nets Endangering Marine Life are Given New Life\nJohn travels to Denkendorf, Germany, to see how discarded plastic fishing nets called “ghost nets” that endanger marine life are given new life.\nFeaturing: Object Carpet\nObject Carpets uses Econyl yarn, regenerated from nylon waste, to produce beautiful, durable rugs and carpets for homes and buildings. The company recycles tons of material that would otherwise become landfill waste – while avoiding the need to use petroleum in producing virgin fibers.\nRepurposing Green Steel into American Infrastructure\nJohn goes to Mesa, Arizona, where Commercial Metals Company shows us how yesterday’s scrap becomes tomorrow’s steel.\nFeaturing: Commercial Metals Company\nCMC uses an innovative micro mill process called “continuous EAF” or electric arc, to melt scrap metal – which requires much less energy than standard steel-making. CMC truly takes green steel to the next level – saving scrap metal headed for landfills, and repurposing it into American infrastructure.\nCrushing It – A Monster Appetite for Metal\nWe all know how important metals, plastics, and materials are to society in our buildings, infrastructures, and automobiles but what about when they reach the End of Life?\nPaving the Road to the Future\nReducing landfills and problematic waste streams with innovative value-added solutions used in road construction of our Nation’s Highways.\nFeaturing: Close The Loop\nToday’s Eco-Friendly Apparel\nSustainable and innovative apparel done responsibly.\nCleaning Up the Planets Oceans\nA powerful partnership’s tide turning recycling program that also helps impoverished communities have a better standard of living.\nFeaturing: SC Johnson\nThe World’s Most Recycled Material\nSteel is the world’s most continuously recycled material and fundamental to achieving a circular economy. See how one firms commitment to innovation has led to a more sustainable process for recycling steel while conserving one of Earth’s most precious resources-Water.\nFeaturing: AK Steel\nThe Unsung Hero of the American Food Supply\nDry beans, a global food source provides protein, high level of fiber & a good balance of vitamins and minerals. They also have a positive impact to the soil. See how one family’s commitment to social and environmental responsibility is helping their local community.\nFeaturing: Bush Brothers\nSports and Entertainment & The Power to Unite People\nThe changing landscape of sports and entertainment, focusing on educating the public and fans on the importance of environmental sustainability and stewardship.\nInnovative Solutions to Protect and Preserve Food\nOver 30 percent of global food production is wasted. With an ever-increasing world population and food production, limiting food waste has become a critical factor for supporting a circular economy and the environment. Learn how innovation in food packaging helps sustain the environment and protects food.\nFeaturing: TC Transcontinental\nTackling the Problem of Waste Materials at a Molecular Level\nTwo innovative technologies are at the forefront of addressing the needs of the worlds growing population at a molecular level, Advanced Circular Technology and Carbon Renewal Technology.\nA Holistic Approach to Packaging\nAs companies and consumers demand more sustainable practices, one company is manufacturing packaging in a sustainable and environmentally friendly manner.\nFeaturing: Veritiv Corporation\nManaging Waste through Sustainable Recycling\nCreating systems for the sustainable recycling of materials in a challenging market.\nFeaturing: Casella Waste Systems Inc\nMany of the plastics today can be used again indefinitely. The onus shifting from the consumer who has reduced, reused, and recycled plastics, to companies that now have a huge responsibility to transform how plastics are created and viewed from the start.\nFeaturing: Husky Injection Molding Systems\nA popular series airing on Fox Business Network, EARTH is an award winning produced television series that spotlights groundbreaking environmental initiatives and the innovative companies that are making them happen.\nFind Us On\nFox Business Network\nAvailable on every cable provider\nChannel slots vary on each system\nDirecTV – Channel 359 & Channel 1359 (VOD)\nDish Network – Channel 206\nVerizon FiOS – Channel 117 (SD) & Channel 617 (HD)\nAT&T U-verse – Channel 211 (SD) & Channel 1211 (HD)']	['<urn:uuid:3a5766dd-4446-44c8-8ed7-1c1f85a357c3>', '<urn:uuid:6ea84734-79b4-40e3-918b-40b3b6ca5aff>']	open-ended	with-premise	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-13T00:20:10.754925	28	135	3880
19	looking for information about degas art collection in paris apartment what did he collect	Degas amassed such a large art collection that it occupied an entire floor of his Paris apartment. He was particularly drawn to works by Ingres and Delacroix, who were the two biggest influences on his development as a painter. He also collected works by potential rivals and patronized struggling artists like Sisley and Pissarro before they became successful.	['The inspiration for this show was the National Gallery’s acquisition of a painting by Camille Corot, “the Italian Woman”, from the estate of Lucian Freud after the artist’s death in 2011. It turns out that over 70 works in the gallery were previously owned by artists. The organisers have selected 35 of these, together with loans from other collections, to explore the phenomenon of “painters’ paintings”.\nWhy do artists collect other artists’ work? Fundamentally, one supposes, for the same reasons other people do: there’s no doubt that Freud dearly loved his Corot, which was prominently displayed over the fireplace in the drawing room of his house. Being an artist himself, though, of course, there was more to it than that.\n“I go and see pictures rather like going to the doctor”, Freud once said: “to get some help”. Often, the paintings artists owned directly influenced their art. Another of Freud’s prized possessions was a small Cézanne, “Afternoon in Naples”, a bawdy brothel scene. It inspired a painting by Freud called “After Cézanne”, which he described as a “cousin” of the earlier work.\nCézanne was also a huge influence on Henri Matisse, the next artist-collector to be considered here. Matisse bought Cézanne’s “Three Bathers” while he was still a struggling artist, pawning his wife’s engagement ring to do so, so the story goes. The painting served as a sort of talisman for Matisse: he called it “his greatest treasure”, which had “sustained me morally in the critical moments of my venture as an artist”.\nThe Impressionist painter Edgar Degas amassed so many works of art that his collection occupied an entire floor of his Paris apartment, and he gets two rooms in this show. Degas was particularly attracted to Ingres and Delacroix, the two biggest influences on his development as a painter.\nAnother strand in the story is the long-established practice of artists exchanging their own work with other artists. Freud often swapped paintings with Frank Auerbach, and their friendship is demonstrated here by an amusing birthday card which Auerbach sent Freud, depicting the two of them carousing in a pub. Matisse enjoyed a similar relationship with Picasso, once saying: “Only one person has the right to criticise me: Picasso”. Picasso’s portrait of his muse Dora Maar is said to have inspired the pathos of Matisse’s last great work, the Chapel of the Rosary at Vence. Degas swapped paintings with artists such as Manet and Blanche, although, being Degas, he sometimes returned them after arguments. He bought works by potential rivals, such as Jean-Louis Forain (“he paints with his hands in my pockets”). To his credit, Degas also patronised hard-up artists, including Sisley and Pissarro, before they were successful.\nUnfortunately, the exhibition rather loses its way in its later stages, as the organisers try to cram in works owned by four British artists – Lord Leighton, G. F. Watts, Sir Thomas Lawrence and Sir Joshua Reynolds. They’d have done better to concentrate on just one of them. Consider, for example, Reynolds, who wrote: “Works of art are models you are to imitate, and at the same time rivals you are to combat”. Reynolds’ complicated relationship with his bitter rival Thomas Gainsborough is worthy of an exhibition in its own right. In 1782 Reynolds bought Gainsborough’s “Girl with Pigs” for 100 guineas, saying it was the best thing he’d ever done. Shortly afterwards, it was rumoured that he was trying to exchange it for a Titian.\nTitian crops up in the last room, too, where there’s a wonderful juxtaposition. Two of the gallery’s Titians were owned by the Flemish artist Sir Anthony Van Dyck (in all, he owned 19 paintings by the great Venetian master). They are displayed alongside a pair of Van Dyck’s double portraits, the gallery’s own “Lords John and Bernard Stuart” and “Thomas KIlligrew and William, Lord Crofts(?)”, from the Royal Collection. Just four paintings, but what a fabulous interplay of colours, converging and diverging lines, gestures and glances. It’s a classic dialogue across the centuries by two great artists.']	['<urn:uuid:abbdf3c3-b415-4c7e-828a-e4d962a31d86>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T00:20:10.754925	14	58	671
20	How do polio eradication efforts connect with reproductive health services?	The polio eradication infrastructure through GPEI connects with reproductive health services as part of broader health initiatives. GPEI's infrastructure supports national health services, with over 6,260 health professionals trained on COVID-19 response in Pakistan, while reproductive health programs include essential components like child survival services, particularly vaccination programs. This includes polio eradication through Pulse Polio Programmes and prevention of vaccine-preventable diseases, which are key elements of both GPEI's work and reproductive child health programs.	"['For countries that have yet to eradicate polio, the devastation of an epidemic like coronavirus is all too familiar. Pakistan and Afghanistan, where the wild poliovirus remains endemic, are now also grappling with COVID-19 outbreaks, confronting unique challenges as these countries battle both viruses simultaneously.\nGiven the infrastructure and footprint of the Global Polio Eradication Initiative (GPEI), the organization and its reach are crucial in combating the COVID-19 pandemic. That is why, for the next six months, wherever the polio program has a presence, GPEI is going to deploy their staff, resources, and surveillance and testing capabilities to the frontlines COVID-19 efforts.\nFortunately, polio workers are well-equipped to assist national health services in response to the pandemic.\n""This has always been the ‘Plus’ in ‘PolioPlus.’ In other words, the infrastructure built up to eradicate polio regularly helps respond to other disease outbreaks or humanitarian disasters,"" Oliver Rosenbauer, GPEI spokesperson at WHO, told Global Citizen.\nIn Pakistan, GPEI has already trained 6,260 health professionals on COVID-19 response, in addition to their existing polio-related duties, and trained over 280 surveillance officers on COVID-19 surveillance. In Afghanistan, field staff are using routine visits to local health facilities to check and report back COVID-19 cases, and health care staff are being trained to respond to the novel coronavirus. In Nigeria, WHO polio program medical staff and field officers are working to mitigate further spread amid multiple COVID-19 outbreaks, pulling insight from years of battling polio. These are just snapshots of GPEI efforts underway in 13 countries that will continue unabated as the virus continues to spread.\nGPEI dates back to 1988, when polio paralyzed more than 350,000 children across 125 countries every year. The disease has been reduced by 99% — more than 18 million people are walking today who would otherwise have been paralysed, and more than 1.5 million lives have been saved, according to GPEI.\nBut the COVID-19 outbreak threatens that life-saving progress.\n""Given that operationally, polio vaccination campaigns are close-contact activities, they are incompatible with the current physical distancing recommendations associated with the COVID-19 response efforts,"" Rosenbauer said.\nThat is why GPEI has had to temporarily delay immunization campaigns.\nUnfortunately, Rosenbauer fears that there could be a re-emergence and further spread in countries that have yet to eradicate wild poliovirus.\nTo mitigate the spread and preserve crucial progress made, GPEI will continue its polio surveillance activities to monitor the evolution of the situation and return in full force to eradication efforts as soon as it is safe to do so.\nA silver lining, GPEI said, is that people are less mobile amid the pandemic, with many borders restricted or closed all together.\n""This could mean that perhaps the risk of spread might be slightly alleviated. However, that is purely conjecture and really the priority is to resume vaccinations as rapidly as the situation safely allows,"" Rosenbauer said.\nRightfully so, he says that all eyes are on the coronavirus pandemic, but that doesn’t mean we can turn a blind eye to other ongoing crises.\n""We must not forget that as long as children in Pakistan are at risk of polio paralysis, and if we do not finish the job, then all children in the world, wherever they live, could one day be affected by polio,"" he said. ""It is therefore also critical that all necessary resources are mobilised, so that the emergency operational plans are rapidly and efficiently implemented once the program resumes operations.""', 'The International Conference of Population and Development (ICPD) 1994 established an International consensus on a new approach to policies to achieve population stabilisation. Fertility reduction should be addressed at the level of broad social policy, including reduction of gender discrimination in education, health care and income generation. Reproductive health programmes should focus the needs of actual and potential clients, not only for limiting births but also for healthy sexuality and child bearing. In India, the implications of the reproductive health approach would be to shift the focus from the use of family planning as a tool intended essentially for population stabilisation, to use family planning as one among a constellation of interventions that would enable women and men to achieve their personal reproductive goals without being subjected to additional burdens of disease and death associated with their reproduction.\nWorld Health Organization (WHO) has defined reproductive health as follows:\n""Within the framework of WHO\'s definition of health as a state of complete physical, mental, and social well-being, and not merely the absence of disease or infirmity; reproductive health addresses the reproductive processes, functions and systems at all stages of life. Reproductive health therefore implies that people are able to have a responsible, satisfying and safe sex life and that they have the capability to reproduce and the freedom to decide, if when, and how often to do so. This definition focus on right of men and women to be informed of and to have access to safe, effective, affordable, and acceptable methods of fertility regulation of their choice, and the right to access to appropriate health care services that will enable women to go safely through pregnancy and childbirth and provide couples with the best chance of having a healthy infant"".\nEssential Components of RCH Programme\n1. Prevention and management of unwanted pregnancy.\n2. Maternal care that includes antenatal, delivery and postpartum services.\n3. Child survival services for newborns and infants.\n4. Management of Reproductive Tract Infection (TRIs) and Sexually Transmitted Infections (STIs).\nThe Government have power to restrict any unit, and to take samples of effluents and to get them analysed in Central or State laboratories. Whoever fails to comply with any provision of this Act is punishable with the imprisonment or with fine or with both. Second or third time breaking of the law is further punishable. Under the provision of this Act Central Pollution Control Board was established to fulfill its object.\nMajor Elements of RCH Programme\nA. Reproductive Health Elements\nResponsible and healthy sexual behaviour\nInterventions to Promote Safe Motherhood\nEssential Obstetric Care for All\nPrevention of Unwanted Pregnancies: Increase Access to Contraceptives\nPregnancy and Delivery Services\nFirst Referral Units (FRUs) for Emergency Obstetric Care\nManagement of RTIs/STDs\nInfertility & Gynecological Disorders\nReferral facilities by Government /Private Sector for Pregnant Woman at Risk\nReproductive Health Services for Adolescent Health\nGlobal Reproductive Health Strategy\nB. Child Survival Element\nEssential New Born Care\nPrevention and Management of Vaccine Preventable Disease\nUrban Measles Campaign\nElimination of Neonatal Tetanus\nCold Chain System\nPolio Eradication: Pulse Polio Programmes\nHepatitis B Vaccine\nGlobal Alliance for Vaccine and Immunisation (GAVI)\nDiarrhea Control Programme and ORS Programme\nPrevention and Control of Vitamin A deficiency among children']"	['<urn:uuid:773adc38-0e98-47c0-9514-4d0196ea5bc8>', '<urn:uuid:26b6b972-c4fe-4b43-9c52-1069fa5ec3eb>']	open-ended	direct	concise-and-natural	similar-to-document	three-doc	expert	2025-05-13T00:20:10.754925	10	74	1103
21	What preparation work is needed before installation, and what tools are required for hardwood floors?	Before installation, several preparation steps are necessary. The subfloor must be dry, smooth, level and free from structural defects. For concrete subfloors, moisture content should be under 4% and need at least 60 days drying time. Contaminants like paint, oil, wax grease, and dirt must be removed. For wood subfloors, loose floorboards should be fastened to avoid creaking. Additionally, the flooring must acclimate to the home's atmosphere for several days before installation. As for tools, the necessary equipment includes tape measure, nail set, flooring nails, drill bits, safety goggles, boots, knee pads, dust masks, and circular, table and miter saws.	['Expansion & Contraction of Hardwood Floors\n“The flooring shop stated to depart 10mm for wood floor expansion in the walls…the boards are lifting started.” Ill informed advice such as this frequently results in floor problems. Many flooring sales agents not really acquainted with wood flooring assume expansion is equivalent to laminate floors. The truth is that 10mm isn’t enough for wood flooring.\nWhat’s An Expansion Area? So Why Do We Want It?\nExpansion areas are spaces left round the perimeter of rooms, against fixed objects for example posts, thresholds, hearths, skirting boards, along with other stationary products built or guaranteed in to the framing structure of the house. Despite the fact that Hardwood floors used is not living and breathing still it reacts to moisture alterations in the atmosphere. Cells within the wood will require on or absorb moisture once the relative humidity is high, or when uncovered to water. Expansion happens, and also the hardwood grows, for insufficient a clearer term, over the grain (width) from the plank (see below, not every will react by doing this). On the other hand when air moisture levels decrease, moisture content evaporates, shrinking from the wood will occur.\nWhenever we say spaces, they’re understood to be installing the hardwood floors as much as and from the fixed objects. For instance “we advise a minimum of 15mm promising small to medium area-18mm expansion on bigger areas in the skirting board or wall if new skirting to become fitted.”\nHardwood floors can respond to the existence of moisture. Within the dry winter heating several weeks, moisture can leave the wooden flooring resulting in the floor to contract slightly in dimensions, which could leave slight gaps in between each floor board. Within the summer time several weeks once the humidity is greater, the hardwood floors will expand and also the gaps will appear reduced. If there’s an excessive amount of moisture within the subfloor it’ll make the wood planks to cup, or buckle. Almost all problems associated with hardwood floors result from improper installation and sub floor preparation. For this reason it’s important when installing hardwood floors that you simply stick to the correct suggested installation methods through the manufacturer.\nDirect Glue Installations\nAn immediate glue installation requires using a flooring adhesive applied directly to the subfloor (never directly affect it’s). This process may be used on concrete and wooden subfloors. When done properly this is actually the most stable way of fitting wood flooring because the elastic flooring adhesive enables the ground to grow and hire no problems.\nIf you’re lounging more than a concrete subfloor you’ll first need checked the concrete moisture, when the floor includes a moisture content well over 4% then either hold back until the sub floor dries to satisfy this level or make use of an appropriate moisture barrier, we advise Sika Mb Primer to make sure no moist increases up to your new floor.\nAlso observe that new concrete slabs require no less than two months drying time before covering all of them with a wood floor.\nAll Concrete sub-floors should be dry, smooth, level and free from structural defects. When the concrete sub floor is uneven we advise using sika latex self leveling compound to level the subfloor. The concrete should also be free from contaminants i.e. paint, oil, wax grease, dirt and curing compounds (the reason behind this is you require the self leveling compound to bond towards the sub floor).These could be removed chemically or robotically as the local home improvement store and they’ll sell the correct floor cleaner, but don’t use solvent-based strippers under any conditions. Using residual solvents can make issues with the connecting of flooring glues. You should ensure an effective bond between your glues and concrete and wood panels.\nFor those who have a wood subfloor you will have to lay a plywood base within the existing floor boards (we advise using 4mm -6mm exterior grade plywood and screw lower every 15cm across the edges and also at 20cm times throughout all of those other board while using 1 ” deck screws) before installation, this can then provide you with a smooth and level surface that you should install you hardwood floors onto.\nYou will have to leave a 15mm expansion gap round the perimeter from the room for big areas over 6 meters long we advise 18mm expansion gap. To keep this gap during installation spacers are supplied within the kits. Once you have installed the hardwood floor you are able to cover this expansion gap with scotia or new skirting boards The 15mm expansion gap must also remain in doorways in which the hardwood floor follows through in one room to a different we sell on this web site a complete selection of door profiles available to fit your floor.\nNailed Lower Installations\nNailed lower installation may be the traditional method to install hardwood flooring and could be done either directly onto wooden subfloor or on the concrete subfloor by using battens. When fixing battens onto concrete you may need a power-actuated nailer they are driving with the battens and connect into the concrete, first of all construct polyethylene moisture sheet then plywood within the battens to produce a new sub floor. We advise 12mm exterior grade plywood and 2inch x 4inch battens. Lay battens around perimeter at ½ inch in the wall, leave ¼ inch gap each and every finish. Squeeze first batten so its center is 16 inches in the outdoors fringe of the perimeter batten. Construct the rest of the battens over the area 16 inch times. Check you battens are level using an amount gauge you may want to use cedar plank shims to even out dips within the concrete, slide the shims underneath the battens until level. (If uncertain we advise talking to an expert fitter with this method).\nHardwood flooring could be nailed into existing wooden subfloors for example floorboards or plywood although not into chipboard floors as these aren’t sufficiently strong to carry their nails. The floorboards should be in good shape and level otherwise you should lay 6mm exterior grade plywood lower first.\nTo prevent creaking, check within the subfloor and safely fasten any loose floorboards. For additional soundproofing it’s also advisable to utilize a standard foam underlay.\nWhenever you nail lower a good wood floor you utilize a Porta Nailer they are driving a nail in a 30 degree position with the tongue and in to the subfloor. Porta Nailers could be hired from the good hire shop but they are costly to purchase.\nAgain you will have to leave a 15mm expansion gap round the perimeter from the room. To keep this gap during installation spacers are supplied within the installation kits.', 'Tools needed for installing hardwood flooring include;\n- Tape measure\n- Nail set\n- Flooring nails\n- Drill bits\n- Safety goggles/ boots\n- Knee pads\n- Dust masks\n- Circular, table and miter saws\nStep by Step Guide to Install Hardwood Floors\nChoosing the flooring\nBefore installing your flooring, you need to choose on the type of wood floor you need and their things to determine first before purchasing your hardwood flooring.\nWhat to consider before purchasing hardwood flooring\nEstablishing the Square footage\nBefore beginning to install any flooring be it laminated or hardwood flooring, you need to measure the room to check the square footage per box. This is done by measuring the width and the length of the room then multiplying them to get the square-foot measurements. It is recommended to buy 10 to 15% more boards than you require to cover the waste and cuts when installing the floor.\nMeasurements of the adjacent rooms\nAdditionally, remember to measure the floors in the adjacent rooms. You generally need to match the thickness of the floors of the adjoining rooms, so you don’t have a slot of uneven floor fittings in the house. You always want to ensure uniformity in the floor fittings to avoid repairs overtime. Measurements of the floors of the adjoining rooms will help you in installing the floor in the right way to match the thickness.\nBuying the flooring and acclimating it to your home.\nWhen you have your flooring, carry it into the house or the region it will be installed and let it adapt to the atmosphere in the space for a couple of days. The moistness of your home can influence the flooring, so you never need to install it that day you buy it.\nSet up the subfloor and take out the baseboards\nset up the sub-floor that the hardwood is being installed. remove the old floor covering to discover old sections of the sub-floor underneath. Check the floor to ensure its pleasant and level and do any important work level the subfloor on the off chance that you have to. Thump down any old nail heads around the room to be installed the flooring, take out any staples and anything standing up from the floor. Afterward, completely clean the region with a shop vac.\nThe layout of the flooring\nNext, you need to decide the design of your flooring in your space. spread out certain pieces or measure the width of the board’s sheets to figure out where they’ll arrange. this to ensure that you won’t have any fragment slices or crazy formed pieces to manage during installment. Remember that numerous old homes walls are out of square so modify your estimations as needs are. On the off chance that you discover you’ll have a clumsy cut piece, adjust your pieces where you will line them up, so you don’t have any truly slim pieces at the end to manage.\nAdditionally, figure out what format will enable you to cut the least pieces. When you have your layout figured out, start installing a vapor boundary on the floor. Red Rosin paper is a modest item that has been around for a while, hence there are different items for this. Spread it out in the room and cover it by 4-6″ with each row. Now you’re prepared to begin installing your hardwood flooring. On your first lines, you may need to face nail the columns set up with a nailer to hold them up.\nEnsure you leave an extension hole around the edges of the floor at each divider. This takes into consideration the flooring to grow and contract with changes in season and moistness. Pursue any markings you spread out on the floor during the layout stage to ensure you’re laying the ground surface effectively. When you have the initial board’s rows installed, you can utilize the floor nailer. use a little bit of the real flooring as a tap square to ensure the pieces are solidly situated in their furrows before shooting them in the right place.\nDoor casing trimming\nWhen you get to door frames, utilize a bit of the flooring to go about like a drop check to remove the base of the casings at the tallness of the flooring and afterward slide the new ground surface underneath. Try not to cut flooring around them since it looks awful. Additionally, broadening the floor into the focal point of the supports and afterward, later you can return and install a change or limit piece to match the floors from different adjoining rooms together.\nTransition and finish all the pieces\nGo through the whole room checking for any errors and when you get to the last line, face nail it closer to the wall. You can fix any face nail gaps with similar wood putty or spread them with the baseboard or shoe forming that you pick. finish nail should not at any point be seen or be noticed by anyone on the floor.\nFinally, fix any edge pieces. These might be extra items you have to buy as a major aspect of your flooring request. Consult with your flooring retailer and let them recognize what sorts of changes you need. you can now install whatever baseboard or shoe forming you pick, and you are done with this venture.\nFor home owners, the best wood depends on the price, hardness, and color. A certain wood may be good for one person and not preferred by another. Before installing your flooring, you need to choose on the type of wood floor you need and their things to determine first before purchasing your hardwood flooring. It’s usually down to the home owner’s lifestyle and preference when choosing. When choosing for woods you need to consider their reaction to changes in weather and temperature changes. You need to consider where you are installing the wood flooring either in the basement or other rooms.\nOne must find the balance between the woods because there is no wood that is better than the other, considering one of the challenges of solid wood’s ability to contract and expand in different natural humidity and variations. Remember to acclimate your wood for a few days before the installation process. Buy extra board to cover for the wasted ones during cutting,']	['<urn:uuid:baaf2937-e227-490c-a71e-fb7131d26644>', '<urn:uuid:a5cedf4e-f63c-46f4-bed9-72f7da59d889>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T00:20:10.754925	15	100	2177
22	How do the stone barns in Cominac and Sirdalen House deal with weather protection?	The stone barns in Cominac have stairstep-shaped gables with 'peyrous' slabs that protect the pinion walls and thatch from rot and strong winds. Their steep roofs (45°) allow quick water runoff and drying. Meanwhile, Sirdalen House uses concrete walls without gutters, allowing natural water staining and moss growth that helps integrate it with the surrounding landscape and provides natural protection.	"['The Stone Barns of Cominac\nThe stairstep shape of their gables is peculiar to this valley\nThe hamlet of Cominac sits on a wide, sunny plateau above the Garbet river valley, 5 km from the village of Ercé in the Couserans. The site is celebrated for its stunning view of the Mont Valier and above all for the unusual profile of the many stone barns that dot the vast pastures: their pinions have a kind of stairstep along the edges.\nThe roofs of these barns used to be thatched with rye or wheat straw. These cereals were grown on the numerous terraces surrounding the village. It was the roofing material of the poor, for it cost them only time and sweat.\nThese cultivations no longer exist, and the terraces have been invaded by the forest. Consequently, the barns are no longer covered with thatch; it has been replaced by slate and unfortunately also by corrugated metal.\nThe walls are constructed of large stones while the flat slabs are of shale. These slabs that form the stairsteps are called ""peyrous"" in the patois of the Couserans and served to:\n- protect the pinion walls\n- make a stop for the thatch and protect it so that it didn\'t rot too quickly and become vulnerable to strong winds\n- facilitated access to the ridge (""cliero"" in patois) although that was only a secondary purpose\nAt the ridge of the roof the thatch overlapped from the north side to cover and protect the other (this same system is still used for slate roofs). The higher side is the ""bad side"", that is the one most exposed to the weather. The bundles of straw were bound head down and fixed in place by laths of ash or willow.\nThe slant of a thatched roof is very steep (about 45°) so that rainwater runs off easily and the straw can dry quickly. A thatched roof has a long life (40-50 years) but must be repatched every year.\nThis type of architecture is also found in the Hautes Pyrenees in the valley of Campan.\nSome visitors wonder why such a small hamlet is surrounded by so many barns. The inhabitants of Cominac were principally cattle and dairy farmers, as the steep hillside and mountain climate made this the only viable type of agriculture. It therefore required a great deal of hay to feed the animals in winter. The farmers had available to them many small meadows scattered on the mountainside outside the village, linked by narrow footpaths. They constructed a barn on each meadow or group of meadows to store the hay and starting in November the cows and sheep were kept close to the barns furthest from the village, grazing on the grass with hay supplementing. When the hay was exhausted the animals were moved to the next barn closer to the village, and so by midwinter the animals were in the barns closest to the village, sparing the farmers a long, tiring walk in the snow to tend to the cows. In summer the cows and sheep were (and still are) driven to the high mountain pastures, called the transhumance, while the meadows of Cominac are hayed.', 'Sirdalen House is a holiday house designed by Filter Arkitekter, an architecture studio based in Oslo, Norway. The house is located in the Sirdalen Valley, in Sirdal, a municipality in Vest-Ader county, in southern Norway. Sirdalen House is surrounded by a gorgeous mountainous landscape, providing its owners the perfect getaway space all year round.\nProject description: The plot for the house is situated on a very steep terrain, near one of the ski-slopes of the Ålsheia ski-center. Given the location and the steep plot it had been desirable that the house be dug into the landscape, so it would act as an element integrated into the nature, both winter and summer. From the backside, the terrain continues naturally over the roof. Likewise, the meeting of the side walls and the terrain is preserved, built up and planted in such a way that the straight modern lines find harmony and transition into the natural surroundings in an exciting way.\nSince the house is built using concrete, it will with time acquire a surface stained with water naturally running over the roof and the side walls, which have no built in gutters. Eventually it will also be grown in with moss, so that the concrete will get the same color palette as the rocky hillside around the house. From the back and the sides the house will, after some years, become integrated snuggly into the terrain and not very visible. The surrounding area, including the driveway, will appear as natural landscape – it will be only the house’s straight man-made lines of glass and concrete that will stand out.\nThe house has two floors. The lower floor contains a garage, storage, technical room, a sauna and a living room with a fireplace. The main floor is organized with four equivalent bedrooms around a large common room. The common living room is backed against the terrain and oriented towards south with a full façade-opening. A height difference in the floor divides the kitchen and dining space from the rest of the common living room.\nIn this project, there was a strong desire from the client to have a maintenance-free house, preferably built in concrete. Given the steep plot, which is also subject to landslides, and the need to build the house into the terrain, concrete was a natural choice.\nThe surrounding nature is very present in the house, and it is this that prevents the concrete house feeling cold. It was also important for us to build in wooden elements into the hard concrete-palette of the house. The kitchen, doors, railings and sauna are all executed in wood to soften the impression.\nSome of the main technical challenges were the soil horizontal pressure from the terrain and the large construction span over the living room and kitchen. Solving these problems together with construction engineers was both challenging and rewarding. Norwegian Geotechnical Institute – NGI joined the project from an early phase and made a study which gave guidelines for building and dimensioning of the house. It was also important to make the elegant insulated concrete and glass elements possible. Here the glass-master was of great help. All glass elements are executed with adhesion details, in order to minimize frames and sills.\nThank you for reading this article!']"	['<urn:uuid:94c5d86e-7527-41e3-b403-2e826d2f855a>', '<urn:uuid:4e920c3d-b656-4182-b85d-41035dbc31b5>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T00:20:10.754925	14	60	1077
23	ideal features green manure plant	An ideal green manure crop should have low water and nutrient requirements, quick farming to produce abundant biomass, deep rooting system for nutrient mining, low fibrous material for quick decomposition, high capacity to fix atmospheric nitrogen, quick establishment and growth, tolerance to adverse conditions, and adequate Rhizobium nodulation potential.	['Introduction to Green Manure in Organic Farming\nOrganic farming depends on organic manures such as farmyard manure, compost, and green manure, etc. Therefore, green manure is one of the most important types of manure used in organic farming. Green manure crops broadly defined as crops grown for the benefit of the soil. The green manuring crops improve the humus, organic carbon, and nitrogen and soil microbial growth. Green manure crop leads to the addition of organic matter to the soil.\nA Step by Step Guide to Green Manure in Organic Farming\nGreen manures are a key part of organic farming, and they serve many different purposes. They prevent soil erosion, improve soil structure, can control weed growth, and most importantly, increase the soil’s fertility. Green manure plays an important role in sustainable annual cropping systems. This method of organic fertilizing has several benefits for the home gardener as well. It is used to define specific crop varieties that are grown into the soil to improve its overall quality. Green manure crops will give a huge range of benefits to future crops and your soil. They are grown completely for the benefits and not for grazing or harvest. Green manure is generally used to improve the soil, for organic matter, nutrients, or to control weeds. Instead of harvest the crop, the green manure is left on the soil surface or worked into the soil. These crops incorporated into a rotation add important benefits but they are a low-cost addition. The advantages of a green manure crop far outweigh the nutrients that they add back into the soil. Also, they feed the soil which feeds the plant rather than the other way around. In feeding the soil a green manure crop feeds all the soil organisms and also improves the fertility of the soil.\nBy growing green manure crops you provide soil microbes with a boost as well as food and an environment they can thrive in. Then, these microbes will convert the nutrients in the green manure into available nutrients for crops. By stimulating these soil organisms and proving a food source, they convert the unavailable nutrients in the soil into available nutrients. By improving the biological activity in the soil you will also improve the soil conditions.\nGreen manures are plants that are grown to benefit the soil nutrients. Green manures are the organic way to;\n- Improve soil fertility, including adding valuable nitrogen levels\n- Develop the soil structure, giving better drainage or water retention\n- Suppress weeds\n- Attract beneficial insects and other predators\nCrop rotation with Green Manure in Organic Farming\nGrowing green manure crops as part of a crop rotation is an important part of organic farming. They are useful when grown before crops that need a lot of nutrients.\nGreen manures can be used in rotation;\n- Whenever there is no crop in the ground, rather than leaving the land bare and allowing weeds to grow and crop nutrients to leach out of the soil.\n- As break crops, when there is a short time between main crops.\n- The timing of sowing is also important. The green manure should be ready to dig in before the crop next is sown. Then, there should not be a long gap between digging in the green manure and planting the next crop. This is to prevent essential nutrients from the green manure from leaching out of the soil, before being taken up by the next crop.\nAdvantages of Green Manure in Organic Farming\nThe purpose of green manure varies depending on each situation but some of the benefits are;\n- The deep-penetrating roots of these crops break open the deep layer of a hardpan of the soil.\n- Brings up the nutrients from the deeper soil layer and make it available to the shallow-rooted crops upon its decomposition.\n- Enrich the soil with biologically fixed nitrogen, adds organic matter, and other macro and micronutrients to the sail.\n- The organic matter added to the soil through green manures acts as food for microorganisms.\n- Green manure protects the soil from erosion and they absorb nutrients from the deeper soil layers.\n- The green plant material buried stimulates the activity of the micro-organisms inhabitant the soil. Then, they respire and decompose the organic matter CO2, which helps in producing carbonic acid. The carbonic acid decomposes the soil essential minerals to release plant nutrients that bind in them.\n- Green material on decomposition produces certain organic acids that enhance the availability of certain plant nutrients such as phosphorus, calcium, potassium, magnesium, and iron.\n- The green manuring crop absorbs soil nutrients and protects them against leaching losses.\n- It improves the soil structure, moisture-holding capacity, and infiltration of water, thus decreasing the runoff and erosion.\n- Increasing organic matter and soil humus\n- Increased Nitrogen fixation\n- Protection of the soil surface\n- Prevention of erosion\n- Maintaining or improving soil structure\n- Reduced susceptibility to leaching\n- Provide readily obtainable nutrients to the next crop\n- Reducing leaching losses\n- Suppressing weeds\n- Reducing pest and disease problems\n- Providing supplementary animal forage\n- Drying and warming the soil\nTypes of Green Manure Crops\n- Bahar Treatment in Pomegranate for High Quality and Yields: A Step-by-Step Guide to Implementation\n- Mobile Veterinary Units in India: Implementation in States\n- Moringa as Feed for Livestock: Moringa Fodder Crop Yield Per Acre\n- National Beekeeping and Honey Mission (NBHM): Features, Schemes, and Benefits\n- Management of Cutworms in Chilli: Prevention and Control With Organic, Chemical, Cultural Practices\n- Best Fertilizer for Tinda: Organic, Natural, Homemade, NPK Ratio, When and How to Apply\nGreen manures are mainly two types;\nGreen manures can be legumes or non-legumes.\nLegumes (clover family)\nLegumes develop on their roots (in association with special bacteria) nodules that can take nitrogen from the air and fix it into a form that the plant can use. Though, this can be utilized by crops grown after the legume has been ploughed and incorporated into the soil.\nLegumes are considered to be nitrogen-fixing but this will only happen in the presence of correct strains of Rhizobium bacteria. These crops make excellent green manures as they have low carbon-to-nitrogen ratios (C: N), which results in a quick release of nitrogen. Because of this, they add nitrogen quickly to the soil but the amount of organic matter contributed to the soil is limited over the long-term.\nNon-legumes do not fix nitrogen but can provide useful organic matter that might otherwise be leached. Non-legumes are primarily used to increase biomass. Some green manure crops like winter wheat and winter rye can also be used for grazing. Some non-legume green manures are quick growing and can be incorporated within gaps in production during the growing season.\nCharacteristics of an Ideal Green Manure Crop\nIdeal green manure crops have the following characteristics;\n- The green manure has low water and nutrient requirement.\n- It should be quick farming to produce abundant biomass.\n- These crops have a deep rooting system, facilitating nutrient mining from subsurface soil.\n- The biomass produced has low fibrous material to facilitate quick decomposition.\n- It has a high capacity to fix atmospheric nitrogen.\n- Capable of establishing and growing quickly.\n- Tolerant to adverse climatic conditions like drought, waterlogging, high temperature and low temperature, etc. and tolerant to pests and diseases.\n- It should possess adequate Rhizobium nodulation potential and must be an effective nitrogen fixer.\n- Green manure should be capable of growing fast and capable of accumulating sufficient fixed Nitrogen in 4 to 6 weeks.\n- Easy to incorporate and quickly decomposable.\nSowing and Fertilization of Green Manure in Organic Farming\nThe green manure crop seeds can be sown from May to June and ploughed down in July. Wheat-fields in north India can be green manured with sun hemp, cowpea, dhaincha, green gram, and black gram, etc. Normally, a higher seed rate is recommended for green manuring. Organic fertilization of green manures with phosphatic fertilizers can be done by broadcast because it improves the availability of phosphorus to the succeeding crop as compared to phosphorus applied to the succeeding crops.\nSelection of Suitable Crop for Green Manure in Organic Farming\nThe selection of crop sown for green manure must be done by the land, climate, and the objective. For green manure, crops have the following properties;\n- Early growing crop.\n- The green manure crop must have a soft stem, branches and leaves are soft and more in number so that it can decompose and the soil can get more and more organic matter.\n- The crop must be of tap root so that it can absorb the nutritive elements from the deep. In alkaline and saline land, the deep-rooted crop is necessary for drawing internal water.\n- The symbiotic bacteria present in the roots fix the free nitrogen obtainable in the atmosphere and provides it to the plants.\n- The green manure crop should be drought resistant. It should also bear a water logging situation. The green manure crop is disease and insect resistant and the capacity of seed production is more.\nOrganic Matter Management with Green Manures\nWhen green manure crops are incorporated into the soil they break down to form soil organic matter. This is important as green manures act as a store for nutrients that are released when they are returned to the soil. Cereals and grasses are high in carbon-containing material and young growth is slightly carbonaceous. As the crop ages and develops more fibrous, the carbon-to-nitrogen (C: N) ratio increases and the material becomes more carbonaceous.\nGenerally, the practice of green manuring in India can be classified in two ways.\nA) Green leaf manuring\nWhere the application of green leaves and twigs of trees, shrubs, and herbs collected from plants grown in field bunds, wastelands, degraded lands, and nearby forest. They are turned down or mixed into the soil 15 to 30 days before sowing of the crops mainly depending on the tenderness of the foliage or plant parts are known as green leaf manuring.\nB) On-farm green manure\nOn-farm green manure is also called legume green manuring. In this system, the short duration legume crops are grown and buried in the same site when they attain the age of 60 to 80 days after seed sowing. This system of on-site nutrient resource generation is prevalent in northern and southern parts of India, where rice is the main crop in the existing cropping systems. Almost any crop can be used for green manuring, but legume crops are preferred because of their ability to fix nitrogen from the air. Green manuring with legumes such as peas, clovers, and lentils, etc. is called legume green manuring. These crops must be turned into the soil before the setting of seeds. It can be profitably used on lands where it was not possible to add animal manures.\nOrganic Nitrogen Management with Green Manures\nSeveral factors influence the release of nitrogen from green manures including soil temperature and moisture. Normally, the nutrient release will be slower at lower soil temperatures because the soil organisms that breakdown organic matter has lower biological activity or work slower at lower temperatures.\nShort term soil nitrogen improvement – Fast cultivated green manure crops such as crimson clover, Persian clover, and fenugreek can be grown in short breaks between cash crops to boost nitrogen levels in the soil. These annual legume crops are used in intensive horticultural systems between vegetable crops. As legumes will fix nitrogen when the soil is above 8C they are effective between April and August.\nLong term soil nitrogen enhancement – Slower growing perennial legumes such as red and white clover, and alfalfa are used to add nitrogen to the soil over a long period. These crops are slow to establish but are persistent, so reduce the need for resowing. They are most sown in a mixture with grasses and are used for silage or grazing in extensive livestock systems while they improve the soil.\nPreventing nitrogen leaching – If soil is left bare for any length of time, rainfall will leach (or wash) nitrogen and other nutrients out, particularly on lighter ground. In many situations reducing leaching is important in maintaining soil fertility than fixing nitrogen. Then, this is true during the winter season, when legumes are slow to establish and fix little nitrogen. Fast-growing crop species with a deep root system are best for preventing leaching.\nYou should not miss this: Clay Soil Treatment Methods.\nMajor Role of Green Manure in Organic Farming\n- Green manure is a type of organic fertilizer where an entire fresh plant or plant part is directly used as manure in agricultural lands, without any prior composting.\n- It increases the soil fertility and productivity by the direct addition of nitrogen and also improves the soil structure, water-holding capacity, and microbial population of soil by the addition of organic matter. Green manuring is practiced based on the suitability of soil and climatic conditions.\n- Green manuring is natural farming. Green manuring helps to improve the soil properties such as physical and chemical properties. It is a method of substituting a basket of compost with a handful of seeds.\n- To improve soil health green manuring is one of the best alternatives and meets the nutritional requirement of the following crop. A periodical application of organic matter is, more important to replace the loss of humus, which is essential for maintaining the soil health in good condition by promoting the growth of microorganisms and by enhancing the supply of nitrogen.\n- Green manuring crops increase the biological activity in the soil and these crops improve soil structure. Green manure crops help in reducing soil erosion. Green manure crops help to increase the supply of nutrients available to plants. These crops help in reducing leaching losses. Also, green manuring crops help to suppress weeds, reducing pest and disease problems, providing supplementary animal forage.\nProblems and Disadvantages of Green Manures\nThere are potentially some problems with the use of green manures in organic farming. These are;\n- Tilling in a heavy nonleguminous crop with a high C: N ratio can result in depressed nitrogen uptake by the next following crop.\n- In low rainfall areas, green manure crops can deplete soil moisture to the point that the succeeding main crop will suffer from drought.\nIn case if you are interested in this: Organic Gardening Questions and Answers.']	['<urn:uuid:3e8e5def-578b-4d46-86ae-e3c82827522a>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T00:20:10.754925	5	49	2412
24	What distinguishes the data management approaches of major stock exchanges like the Philadelphia Stock Exchange from the structure of S&P 500 index investments in terms of diversification and company tracking?	The Philadelphia Stock Exchange, as one of North America's primary marketplaces, focuses on trading individual stocks, equity options, index options and currency options, requiring complex data tracking of cusips, stock codes, and exchange codes. In contrast, S&P 500 index investments offer a simpler, more diversified approach by automatically tracking about 500 of America's largest companies in a single investment vehicle. This makes S&P 500 index funds more broadly diversified, as they represent hundreds of America's best companies across various industries, reducing the need for individual stock selection and complex trading mechanisms.	"['The Philadelphia Stock Exchange founded in 1790 is the oldest organized stock exchange in the nation. As one of North America\'s primary marketplaces for the trading of stocks, equity options, index options and currency options, the PHLX continues to be a market leader in the development and introduction of innovative new products and services. Q-learning: is a value-based Reinforcement Learning algorithm that is used to find the optimal action-selection policy using a Q function. DQN: In deep Q-learning, we use a neural network to approximate the Q-value function. The state is given as the input and the Q-value of allowed actions is the predicted output. 2. (Datastream local codes can be used to create a Datastream list). If the cusip is less than 9 digits the leading 0\'s could be missing, and need to be added. 7525108 (cusip) goes to U00752510 (DS local code). A 6-digit cusip usually indicates a company and 10 is added to give the primary listing. ... Compustat stock exchange codes.\nWRDS-> Compustat -> Global -> Fundamentals Annual-> do a key word search for ""exchange"" ""Stock Exchange Code"" is found via this key word search. Click on the question box marker next to this item. It says that the ""exchg"" variable has the information needed and references the Compustat manual for the full list of codes. Search: Crsp Exchange Code. 9790519 6 Daily CRSP Value 126 0 Only available to subscribers of CRSP 10 U stock market and includes large-, mid-, small-, and micro-cap stocks regularly traded on the New York Stock Exchange and Nasdaq Header Exchange Code (numeric) HSICCD Participation from Market Makers and ECNs is strictly voluntary and as a result, these Participation from Market Makers and. There are hundreds of stock exchanges around the world, including those in the U.S. However, many of those stock exchanges are small, or trade options or securities that are outside of our scope. So we currently have in our database stocks traded on 104 stock exchanges around the world, including 8 stock exchanges in the U.S.\nAmerican Stock Exchange, and Nasdaq specify that the minimum vote be held out of votes ... other firms in the same two-digit SIC code for the same period ... Source CRSP Compustat or SEC filings Compustat or SEC filings Compustat or SEC filings Compustat Or SEC filings S&P stock guides SEC filings CRSP CRSP Compustat or SEC filings Burch. CRSP Stock Files The Center for Research in Security Prices provides comprehensive security price data via two primary stock files, the NYSE/AMEX file and the NASDAQ file. These files are composed of master and return components, available separately or combined. ... Exchange Code SICCD: Standard Industrial Classification Code. Definition. The Global Company Key or GVKEY is a unique six-digit number key assigned to each company (issue, currency, index) in the Capital IQ Compustat database. It is a company (issue, currency, index) identifier similar to a TICKER symbol. It represents the primary key for a company that is an index constituent. Stock prices are available for the end of each month, but information ... which contains the share codes (SHRCD) and exchange codes (EXCHCD). Shortly after ... Now from COMPUSTAT, gvkey, CUSIP Issuer Code (cnum), Exchange Listing & S&P Major Index Code (zlist), Fiscal Yearend Month of Data (fyr), Fiscal Year (yeara),.\nCompustat Variables. Company Name Ticker Symbol CUSIP Stock Exchange Code Fiscal Year-End ADDZIP -- Postal Code CITY -- City IPODATE -- Company Initial Public Offering Date LOC -- Current ISO Country Code - Headquarters NAICS -- North American Industry Classification Code SIC -- Standard Industry Classification Code STATE -- State/Province. June 26, 2019. Trade in Goods and Services: Measuring Domestic and Export Flows in Buyer-Supplier Data . Maria D. Tito 1. According to Statement No. 14 and Statement No. 131 of the Financial Accounting Standards, public firms are required to disclose major customers--that is, customers that account for at least 10 percent of their total sales. 2 This public filing requirement is a source of. rankings from Compustat.1 Because Compustat\'s coverage is incomplete prior to 1978, we ... the 2-digit SIC code of the new and the old company are the same. Out of the 101 firms in our sample, there are seven cases where a ... subject to the Securities and Exchange Commission (SEC) reporting requirements) or because.\nticker, CUSIP, ISIN, address, and industry codes reflect only the most current information. Earlier name changes and identifiers are not retained. Historical identifiers have date ranges in which they are valid. Compustat Global includes both header and historical industry codes but other identifiers are header data. Jan 11, 2022 · Data description. S&P Compustat provides financial information on over 28,000 North American, publicly-held companies from the 1950s to present. The database includes a wide range of income, balance sheet, flow of funds and supplemental data, across 340 annual and 120 quarterly items, with point-in-time snapshots from 1987 to present.. Jan 02, 2020 · Compustat\'s documentation indicates that this code runs from 0 to 20. However, as can be seen above, my code consists of 3 digits. Does anyone know where I can find the meaning of this 3-digit code. It looks like this in my dataset: Code: * Example generated by -dataex-. To install: ssc install dataex clear input double exchg 104 104 104 104 ....\nSearch: Crsp Exchange Code. About Crsp Exchange Code. Quarterly footnote codes 45 Common Stock - Per Share Carrying Value 45 Annual footnote code 46 ... CUSIP Issuer Code (COMPUSTAT Business Information Files) 61 Data Quarter 61 Data Year (COMPUSTAT Business Information Files) 62 ... Exchange Listing and S&P Major Index Code 105 Exchange Rate Effect (Statement of Cash Flows) 106 Excise Taxes 107. Exchange Commission. Compustat files also contain information on aggregates, industry segments, banks, market prices, dividends, and earnings. ... publicly held companies trading common stock and wholly owned subsidiaries trading preferred stock or debt. • Research files (res, ... exchange code, SIC code, counts of ""array events"" (number of.\nJune 26, 2019. Trade in Goods and Services: Measuring Domestic and Export Flows in Buyer-Supplier Data . Maria D. Tito 1. According to Statement No. 14 and Statement No. 131 of the Financial Accounting Standards, public firms are required to disclose major customers--that is, customers that account for at least 10 percent of their total sales. 2 This public filing requirement is a source of. CBOE Indexes: The CBOE (Chicago Board Options Exchange) Volatility Index® is a key measure of market expectations of near-term volatility conveyed by S&P 500 stock index option prices. Compustat - Capital IQ: Compustat is a database of U.S. and Canadian fundamental and market information on active and inactive publicly held companies. 2013 bmw 5 series heater blower not working. romeo and juliet act 2 scene 2 quiz baofeng can receive but not transmit; 44 xl shot cartridge. club style dyna paint jobs; blake mckeown where is. Crsp Exchange Code. The symbols are ""behind the dot"" codes for shares traded on the New York Stock Exchange because extension symbols are added after a period after a NYSE ticker name.  Other Finalists ""Active ETF of the Year"" 2018: TrimTabs All Cap U. Getting information on your company Pages 3-30 2.\nCompustat Item Name: CUSIP Issuer Code The CUSIP Issuer Code, assigned by the CUSIP Service Bureau, identifies each company in Standard & Poor\'s Compustat software. It is a six-character code consisting of numbers in the first three positions and either an alpha or numeric character in the fourth, fifth, and/or sixth positions. 25a Celková kompenzácia CEO Compensation Compustat CUSIP Data. Keysets Compustat items can be qualified by a set of secondary keys. This collection of secondary keys and values create a keyset that assigns a numeric code and mnemonic tag to each unique collection. Each keyset represents different output series. free stuff microcenter; ninja 400 gear position sensor. level 2. · 2 yr. ago. first door simply put item frame on each redstone lamp with a head on each frame. press the left button once then the right button. then go back to the left frame and rotate the head 7 times. wait for noteblock sound. then go to right frame and rotate that head 7 times. door will open. 1.\nThe CUSIP that CRSP reports is the so-called ``header CUSIP\'\' , which is the last CUSIP belonging to that PERMNO. A PERMNO may have had several different CUSIPs in the past. These are called the ``names CUSIPs\'\' (NCUSIPs) by CRSP. Suppose I have data about stocks that only has historical CUSIPs as identifiers. Jun 03, 2021 · The Standard Industrial Classification Codes that appear in a company\'s disseminated EDGAR filings indicate the company\'s type of business. These codes are also used in the Division of Corporation Finance as a basis for assigning review responsibility for the company\'s filings. For example, a company whose business was Metal Mining (SIC 1000 .... .\nCompustat stock exchange codes 1903a3 stock parts bltouch anycubic mega x lada 2107 price power book 2 season 2 episode 8 release date These codes are returned in the exch field of all market data requests. You can use this table to represent the exchange as text. ID. Name. A. NYSE MKT. B. NASDAQ OMX BX. C.. COMPUSTAT ® (Global) Annual. Common Stock 48 Annual & Quarterly footnote codes 49 Common Stock - Per Share Carrying Value 49 Annual footnote code 49 Common Stock Equivalents - Dollar Savings 49 Common Stock Float Shares - Canada 50 Company Name 50 Compensating Balance 50 Contingent Liabilities - Guarantees 51 Annual footnote code 51. NAICS Codes. 2012 NAICS Descriptions. 452319. All Other General Merchandise Stores. 452990. All Other General Merchandise Stores. 517311. Wired Telecommunications Carriers. 517110.\n4 Entities in Compustat have a four-digit SIC code. We take the two-digit major group numbers 60-67 as finance, ... 8 CRSP covers stock market data on companies listed on major stock exchanges (NYSE, AMEX, NASDAQ); Compustat includes large companies on the NYSE, AMEX, and NASDAQ, including companies comprising the. As Compustat states, ""if the date of a company\'s initial public stock offering is not available, the first trading date in the major exchange is used Number of operating companies (CRSP share codes 10 and 11) listed on major exchanges at quarter-end Extract CRSP Data for NYSE and AMEX Common Stocks */ /* Merge historical codes with CRSP Monthly. Stock Exchange Code:: Code indicating primary stock exchange on which issuer\'s/borrower\'s common stock trades (e.g. N). AGENTROLECODE. Loan Manager\'s Role (Codes Listed Down the Page):: Thomson Reuters code for the roles of managers (Agents, Co-Agent, Lenders/Participants). Role is displayed as codes listed down the page. CRSP data files are split into two kinds of files: ""header"" and ""data"" files International money transfers can be expensive If you are not getting any exchange 0-3 companies, you are probably using CRSP/Compustat Merged (CCM) rather than Compustat data CRSP returns calculations use the distribution codes to determine whether a.\nAsked 31st Jul, 2016. Henry Balani. For US data , it is available in CRSP. For non USA data it is not available. I need to calculate both Value & Equal Weighted Returns for EU based firms and it. Average operating profit to assets is 7%, with an average market to book of 1.7 times. Cash and cash equivalents represents 8.4% while long-term tangible assets represent 42% of total assets. The market capitalization of the Mexican stock exchange represents around 31% of Mexico\'s GDP during the period of study. Inflation, on average, is 8.9%. Compustat Item Name: Canadian Index Code — Current. This two-digit code identifies companies that are current members of the Toronto Stock Exchange 300 (TSE 300). This item contains either a code of 81 to indicate membership in the TSE 300 or appears blank for companies that are not members of the TSE 300..\nkingdom anime season 3 episode 5 onlinenifty archive gay storieshimars missile costhunter rx10 installation manualskymovies ltdmega nz twittersideloader modpacklawsuit compensation email spamepo obo igi nla\nyalla shoot football livemami x golpoangular expand div on clickkubejs docsriotfall script pastebin 2022singer 1911 serial numberspathfinder 2e core rulebook pdf download freecamila brooklyn and silas nolan novel pdfbackrooms game\nqbcore itemsstfc plundered cargo systemsedexcel physics syllabus a levelgorebox free downloadboot fairs thanet 2022f1 2021 fanatec csl ddklarity kratom trainwreck reviewkirishima x listener asmrcost of drilling a water well in wyoming\nadofai modsguitar effects chain diagramshadowrocket apple id 2022hyper sonic in sonic 3 onlinestreet fighter resurrection chapter 2matlab for loop through arraymerlin clash ac86uprintable chinese calendar 2022samsung t295 combination file\ntiraj bolet new yorkdial tcp i o timeoutwickr drug dealers melbourne 2022max pika stats gpobatch file echo multiple linesb1 unit 11 progress testgmod beatrun modmilitary whip antennavelonews change of address\nfree clone websiteclash free yamlmotorola wifi calling settinglist of non vbv bins 2022wife sex stories first black cockoptimizing box weights hackerrank solutionflutter dashboard template freeleggett and platt remote control manualsex surrogate porn video\nxnxx young teenwife anal sex forcedcum inside that pussyperiscope rising 150tbi idle adjustmentwhat is the length of the hypotenuse of the triangle belowrtl8821cu linux driverspeeduino mapsfile weekly unemployment claim phone number\n30 kw hub motorfileboom reseller codefree clone websitepie in the face meaningtwitter sentiment analysis python kaggleprepare journal entries to record the following transactionseft cheat sourceside scroller level design ideasharry potter y la piedra filosofal\nmy hot wife with blackpasswall openwrtdcfs board payments for 2022oracle execute stored procedure with parametersitch io free otome gamesauto hatch pet simulator x downloadxenon hub script blox fruitdramatizeme actorspowerflex 755 manual fault codes\nangels of death 40k episode 1 freemake your own wordlebudweiser clydesdales hitch schedule 2022old women amateur pornmeggnut discordtronxy firmwarenfs most wanted download ps3asics gel nimbus 24 reviewiptv m3u telegram group\n- View Librarian Colloquium Presentation.ppt from MGMT 100 at University of Pennsylvania. Navigating WRDS: Overview of Financial Data on WRDS CRSP, Compustat, and IBES Denys Glushkov Rabih\n- In Worldscope one can use ISIN-, SEDOL- or DS-codes, in Orbis you need BvD, ISIN or tickers to retrieve the data. Updates: Compustat is being updated once a year, Worldscope and Orbis has daily updates. Manuals. Annual report data in Bloomberg; Annual report data in Compustat Global; Annual report data in Compustat North America\n- An overview of S&P Compustat Fundamentals. Compustat is a comprehensive market and corporate financial database published by Standard and Poor\'s, covering thousands of companies worldwide, with info dating as far back as 1950. Compustat is a leading source of intelligence for financial market professionals, investors, and academics.\n- Available at this web site are: (1) AMADEUS, which includes over 5,000,000 European private and public companies, (2) BankScope, which has financial data on 13,000 banks worldwide, and (3) OSIRIS with financial data on over 38,000 globally listed companies. Path on WRDS Server: /wrds/crsp/sasdata/a_stock. File Name.\n- The CUSIP that CRSP reports is the so-called ``header CUSIP\'\' , which is the last CUSIP belonging to that PERMNO. A PERMNO may have had several different CUSIPs in the past. These are called the ``names CUSIPs\'\' (NCUSIPs) by CRSP. Suppose I have data about stocks that only has historical CUSIPs as identifiers.', 'Standard & Poor’s 500 index funds are among today’s most popular investments — and it’s little wonder why. The S&P 500 index on which these funds are based has returned an average of about 10 percent annually over time and represents hundreds of America’s best companies. With an S&P 500 index fund you own the market, instead of trying to beat it.\nIn fact, legendary investor Warren Buffett has long advised investors to buy and hold an S&P 500 index fund. So if you’re considering one for your portfolio, here’s what you’ll need to know to get started.\nIndex funds explained\nAn index fund is a type of investment fund – either a mutual fund or an ETF – that is based on an index. An index is a preset collection of stocks, and an index fund merely mimics the composition of the stocks in the index, rather than trying to pick which stocks will outperform. So an index fund is a passively managed investment, only adjusting its holdings when the underlying index changes.\nAn index fund is typically created around a specific theme. For example, there are indexes for companies based on their geographic location (such as the U.S.), their size (large companies, as in the S&P 500), their sector (such as semiconductors or healthcare), or whether they pay dividends. An index might also consist of only bonds, or only bonds of a certain quality and duration.\nHow to invest in an S&P 500 index fund\nIt’s surprisingly easy to buy an S&P 500 fund. You can set up your account to buy the index fund on autopilot, so you’ll almost never have to monitor the account, or you can enter your trades manually.\n1. Find your S&P 500 index fund\nIt’s actually easy to find an S&P 500 index fund, even if you’re just starting to invest.\nPart of the beauty of index funds is that an index fund will have exactly the same stocks and weightings as another fund based on the same index. In that sense, it would be like choosing among five McDonald’s restaurants serving exactly the same food: which one would you go with? You’d probably select the restaurant with the lowest price, and it’s usually the same with index funds.\nHere are two key criteria for selecting your fund:\n- Expense ratio: To determine whether a fund is inexpensive, you’ll want to look at its expense ratio. That’s the cost that the fund manager will charge you over the course of the year to manage the fund.\n- Sales load: If you’re investing in mutual funds, you’ll also want to see if the fund manager charges you a sales load, which is a fancy name for a sales commission. You’ll want to avoid this kind of expense entirely, particularly when buying an index fund. ETFs don’t charge a sales load.\nS&P 500 index funds have some of the lowest expense ratios on the market. Index investing is already less expensive than almost any other kind of investing, even if you don’t select the cheapest fund. Many S&P 500 index funds charge less than 0.10 percent annually. In other words, at that rate you’ll pay only $10 annually for every $10,000 you have invested in the fund.\nSome funds are even less expensive than that. Here are five of the best S&P 500 index funds, including one that’s completely free, as well as some other top index funds.\nIn investing, paying more doesn’t always translate into better returns. In fact, the relationship between fees and returns is often reversed. Since these funds are largely the same, your choice is not a “make or break” decision – you can expect to get the performance of the index, whatever that is, minus the expense ratio or any fees you’re paying. So costs are an important consideration here.\nSelect your fund and note its ticker symbol, an alphabetical code of three to five letters.\n2. Go to your investing account or open a new one\nAfter you’ve selected your index fund, you’ll want to access your investing account, whether it’s a 401(k), an IRA or a regular taxable brokerage account. These accounts give you the ability to purchase mutual funds or ETFs, and you may even be able to buy stocks and bonds later, if you choose to do so.\nIf you don’t have an account, you’ll need to open one, which you can do in 15 minutes or less. You’ll want one that matches the kind of investments you’re planning to make. If you’re buying a mutual fund, then try to find a broker that allows you to trade your mutual fund without a transaction fee. If you’re buying an ETF, look for a broker that offers ETFs without commissions, a practice that has become the norm.\nThe best brokers offer thousands of ETFs and mutual funds without a trading fee. Here is Bankrate’s list of best brokers for beginners.\n3. Determine how much you can afford to invest\nYou don’t have to be wealthy to begin investing, but you should have a plan. And that plan begins with figuring out how much you’re able to invest. You’ll want to add money regularly to the account and aim to hold it there for at least three to five years to allow the market enough time to rise and recover from any major downturns.\nThe less you’re able to invest, the more important it is to find a broker that offers you low fees, because that’s money that could otherwise go into your investments.\nOnce you’ve figured out how much you can invest, move that money to your brokerage account. Then set up your account to regularly transfer a desired amount each week or month from your bank. Or you can set up your 401(k) account to move money from each paycheck.\n4. Buy the index fund\nOnce you know the S&P index fund you want to buy and how much you’re able to invest, go to your broker’s website and set up the trade.\nStick to the broker’s easy trade entry form, which often appears at the bottom of the screen. Input the fund’s ticker symbol and how many shares you’d like to buy, based on how much money you’ve put into the account.\nIf you’re able to move money into the brokerage account regularly, many brokers allow you to set up an investing schedule to buy an index fund on a recurring basis. This is a great option for investors who don’t want to remember to place a regular trade. You can set it and forget it.\nAs a result, you’ll be able to take advantage of the benefits of dollar-cost averaging, which can help you reduce risk and increase your returns.\nWhat is the S&P 500?\nWhile there can be almost any number of indexes, the most famous ones are based on the Dow Jones Industrial Average, the Standard & Poor’s 500 and the Nasdaq 100.\nOf these, the S&P 500 Index has come to be seen as the bellwether for the American stock market. It contains about 500 of the largest companies in the United States, and when investors talk about “beating the market,” the S&P 500 is often considered the benchmark.\nIn contrast, the Dow Jones Industrials contains just 30 companies, while the Nasdaq 100 contains about 100 companies. While the holdings of these indexes do overlap, the S&P 500 contains the widest variety of companies across industries and is the most broadly diversified of those three indexes.\nWhy do investors like S&P 500 index funds?\nS&P 500 index funds have become incredibly popular with investors, and the reasons are simple:\n- Own many companies: These funds allow you to hold a stake in hundreds of stocks, even if you own just one share of the index fund.\n- Diversification: This broad collection of companies means you lower your risk through diversification. The poor performance of one company won’t hurt you as much when you own many companies.\n- Low cost: Index funds tend to be low cost (low expense ratios) because they’re passively managed, rather than actively managed. As a result, more of your hard earned dollars are invested instead of paid to fund managers as fees.\n- Solid performance: Your returns will effectively equal the performance of the S&P 500, which has historically been about 10 percent annually on average over long periods.\n- Easy to buy: It’s much simpler to invest in index funds than it is to buy individual stocks, because it requires little time and no investing expertise.\nThese are the biggest reasons that investors have turned to the S&P 500 in droves.\nIs an S&P 500 index fund a good investment?\nAs long as your time horizon is three to five years or longer, an S&P 500 index fund could be a good addition to your portfolio. However, any investment can produce poor returns if it’s purchased at overvalued prices. But that hasn’t proven to be an issue for these funds, with investors enjoying about 10 percent annual returns on average over long time periods.\nConsider buying into the fund over a period of time using a method known as dollar-cost averaging. By doing this, you’re spreading out buy points and avoiding the practice of “timing the market.” This approach can help you take advantage of any market downturns that happen on occasion.\nBuying an S&P 500 index fund can be a wise decision for your portfolio, and that’s one reason that Warren Buffett has consistently recommended it to investors. It’s easy to find a low-cost fund and set up a brokerage account, even if you only have basic knowledge of what to do. Then you’ll be able to enjoy the solid performance of the S&P 500 over time.\nEditorial Disclaimer: All investors are advised to conduct their own independent research into investment strategies before making an investment decision. In addition, investors are advised that past investment product performance is no guarantee of future price appreciation.']"	['<urn:uuid:e27ebeb1-4218-4e16-b0f1-33c06dc80148>', '<urn:uuid:a2e9725f-b756-4fb0-a365-a42266d739f7>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-13T00:20:10.754925	30	91	4172
25	How does the Art Institute use science to study art?	The Art Institute of Chicago uses scientific research and conservation techniques to study their art collection. They have a Conservation Science Initiative that involves collaboration with universities and scientific institutions, using equipment and expertise in materials science, chemistry, and computer science. Their research has covered various objects from ancient Chinese jades to Picasso's paintings, with findings published in scientific journals. The museum's conservation department also performs technical analysis to understand how objects were made and their origins.	"['CHICAGO, IL.- The Art Institute of Chicago\nhas been awarded a $2 million challenge grant by the Andrew W. Mellon Foundation to aid the museum\'s efforts in conservation and scientific research on its collections. Of this generous grant, $1.5 million, to be matched by $500,000, will be used to endow a new position for an associate conservation scientist within the museum\'s Department of Conservation. The remaining $500,000 of the grant will support, over a four-year period, the continuation and expansion, in both depth and scope, of the art conservation and scientific research collaboration the museum has recently undertaken with colleagues at Northwestern University and Argonne National Laboratory.\n""The work of our world-class conservation department is absolutely vital to the mission of the Art Institute,"" said James Cuno, President and Eloise W. Martin Director of the museum. ""The department is charged not only with maintaining the collection but also with performing highly technical analysis of our objects to better understand how they were made and where they came from. This research has led to many conferences, colloquia, and scientific papers that showcase the state-of-the-art techniques and processes of the conservation department while also revealing a great deal about the art objects we hold for the public. We are deeply grateful to the Mellon Foundation, whose early support was critical to the development of the department and whose continuing support, with this award, allows us to continue this significant and often revelatory work.""\nThe conservation department, now led by Frank Zuccari, Grainger Executive Director of Conservation, was founded in 1962. Since that time, the Art Institute has developed extensive facilities for the conservation of paintings, works on paper, textiles, photographs, three-dimensional objects, and books. The staff in the department consists of 20 conservators, museum professionals who work in concert with curators to preserve the collection and conduct intensive study of the approximately 270,000 objects--from many cultures and all time periods, from ancient to modern--in their care. The results of their research are shared with a variety of audiences through exhibitions, exhibition catalogues, scholarly catalogues of the permanent collection, articles in the museum\'s semi-annual Museum Studies journal, presentations at professional meetings, publication in professional and scientific journals, public lectures, and tours of the galleries and conservation facilities.\nThe Mellon Foundation grant will both strengthen the Art Institute\'s research capabilities and allow the museum to continue to develop its Conservation Science Initiative, spearheaded by Francesca Casadio, A. W. Mellon Senior Conservation Scientist at the Art Institute, and supported in part by the first ever National Science Foundation grant to the Art Institute in 2007. The Conservation Science Initiative is a national model for integrative and cross-disciplinary collaboration among museums, universities, and scientific institutions with the goal of enhancing the field of cultural heritage science in the U.S. This initiative has not only provided access to the latest equipment and expertise used for materials science, chemistry, and computer science but has also offered opportunities for young scientists to understand the various applications of scientific tools to the care, preservation, and conservation of art objects.\nThe work of the Conservation Science Initiative has already produced significant results, including articles in both scientific journals and general interest publications. Its objects of study have ranged from ancient Chinese jades and bronzes to prints by the Mexican artist José Guadalupe Posada to house paints used by Pablo Picasso and the Art Institute\'s monumental painting by Henri Matisse, Bathers by a River, which will be the subject of a major exhibition next spring. The efforts of the collaboration have been published in such journals as Analytical Chemistry, Applied Spectroscopy, Journal of Mathematics and the Arts, and Analytical and Bioanalytical Chemistry.']"	['<urn:uuid:0ebcc06e-4954-45f8-8d12-045f90479da7>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T00:20:10.754925	10	77	610
26	nutrients contained horse timothy hay	Good quality timothy hay contains up to 10% protein, is high in fiber, provides moderate carbohydrates, and when green it is rich in well-balanced vitamins and minerals.	['The Different Types of Hay for Horses\nComponents of Hay\nGood quality, fresh hay, then, contains all the different nutrients your horse requires:\n- Protein – used for growth and repair of tissues\n- Carbohydrates – used for energy\n- Fats – used to store energy and as insulation against cold\n- Vitamins A, B complex, C, and E – used for a variety of metabolic processes\n- Minerals, most importantly calcium and phosphorus, which are used predominantly for skeletal growth and maintenance\n- Fibre – essential for function of the digestive system\nThe way in which types of hay differs is generally not in which nutrients they contain, but the proportions in which those nutrients are present. Nutrient density also varies from one cutting to another, so to really balance your horse’s diet, it’s best to have a hay sample tested to ensure you know exactly what’s in it. Many farmers will have this done as a matter of routine to check for quality.\nA general baseline to check for hay quality is in the percentage of crude protein. An adult horse in moderate work only needs about 10% protein in his diet, while growing, performance and breeding horses will need more.\nTypes of Hay\nClover hay is very similar to alfalfa, with a very high protein content and excellent palatability. Its\nCa:P ratio is also generally unbalanced, but the advantage of clover hay is that clover is usually planted alongside grass and then cut and baled with the grass hay, so the two hays are already mixed. This boosts the quality of a grass hay crop and eliminates the fuss of having to feed two different hays to your horse. However, some sneaky horses will pick out all the clover and leave the grass, so you have to keep a close eye on your horse’s intake.\nTypes of grass hay vary across the world, but in the US, timothy hay is by far the most popular. Good quality timothy is up to 10% protein, which is sufficient for most horses. It is high in fibre and thus optimises the horse’s digestive system while supplying a moderate amount of carbohydrates. Good quality, green timothy hay is also rich in well-balanced vitamins and minerals. Bermuda grass and orchard hays are very similar to timothy hay and can be fed almost interchangeably according to what’s available – and cheaper – in your area. Timothy hay is a safe bet, but almost always more expensive than bermuda grass or orchard hay. The only real disadvantage of these grass hays is that they do not contain enough carbohydrates and protein for performance or breeding horses, so must be supplemented with alfalfa or concentrate feed. Later in the season when the hay has lost its greenness, it may also be low in vitamins and minerals, so a balancer may need to be fed.\nAnother type of grass hay is oat hay. This hay has a similar protein content to other grass hays, but is significantly higher in carbohydrates. For a skinny horse who needs to pack on some pounds or a performance horse who needs some extra energy, oat hay can be great, but it is disastrous to feed to insulin resistant, overweight, or laminitic horses. It may also cause some horses to become hot and unmanageable to ride.']	['<urn:uuid:a816f16a-24dc-4a8f-b207-58b61f6dd7ae>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T00:20:10.754925	5	27	555
27	What animal inspired new developments in prosthetic limbs?	Deer antlers inspired new developments in prosthetic limbs. University College London researched how deer antler protrusions are protected by soft tissue to prevent infection, which could be applied to osseointegration in prosthetics.	['Here’s a question for you. What do deer antlers, fighter pilot helmets and elite sports cars have in common? The answer – because you give up, right? – is that they have all been used to inspire the next generation of Paralympic sports equipment. Here’s the story.\nTime was that the Paralympics were a low-publicised footnote to the Olympic Games. Today the Paralympic movement has surged in popularity, with athletes who have achieved star status. As well as an enhanced profile, Paralympic athletes are also enjoying the benefits of increasingly sophisticated sports equipment – from wheelchairs designed by elite sports car manufacturers to prosthetics inspired by deer antlers. Make no mistake. The Paralympics have become a fertile seedbed for engineering innovation.\nEngineering brains add brawn to wheelchair basketball\nImage courtesy of BBC\nWheelchair basketball: fast, furious, often brutal. Player’s wheelchairs must be speedy, balanced and agile, as well as strong enough to withstand powerful collisions. Lots of them. That’s why Team GB’s 2012 wheelchair basketball team were grateful for the engineering chops of wheelchair manufacturer RGK, who joined forces with BMW and Loughborough University to create a revolutionary design specifically for the rigours of wheelchair basketball.\nTrue innovation rarely comes from tweaking the tried and tested. RGK went right back to the drawing board with its wheelchair design, starting from scratch in CAD and iterating its new design based on rigorous automated stress analysis. By building each chair’s frame from aerospace-grade aluminium, the team improved the shock resistance of Team GB’s wheelchairs while reducing weight by around 2kg. Each chair was also made-to-measure, with personalised seats based on 3D scans of the players’ physical dimensions and biomechanics.\nThe result: wheelchairs that were more comfortable, more manoeuvrable and more stable.\nFighter pilot engineers join wheelchair racing’s need for speed\nImage Courtesy of BAE Systems\nOf course, when you swap the basketball court for the race track, speed matters more than strength. Just ask BAE Systems, who formed a partnership with UK Sport’s wheelchair programme and donated £1.5m of engineering services. Part of that included thorough requirement-capture analysis – the same process BAE use to develop fighter pilot helmets. Cue extensive interviews with Team GB’s wheelchair athletes, which revealed a huge amount about how the individual athletes interfaced with their chairs and some of the limitations they faced.\nIn a first for Paralympic sport, BAE also gave Team GB’s athletes access to their wind tunnel – used with famous prevalence by Team GB’s gold rush cyclists. The results from these wind tunnel tests helped to calculate the optimum posture and position for wheelchair racing athletes to promote comfort and speed. This contributed to the development of the first British wheelchair with a chassis made entirely from carbon fibre. It’s six times stiffer than conventional aluminium racing chairs, which reduces drag and increases speed.\nDo deer antlers hold the secret to the future of sprinting?\nPerhaps the image most synonymous with modern Paralympics is that of the blade runners hurtling towards the finish line. Already an engineering marvel, each blade must be tailored to the unique biomechanics and weight of each runner in terms of energy stored and returned. This is achieved by tweaking the shape of the blade as well as the layers of composite to provide the perfect balance of spring and absorption.\nImage Courtesy of The Food Champions\nNow manufacturers are looking to innovate further by improving the interface between the stump and the prosthetic. Challenges include spreading pressure evenly, reducing heat and friction and avoiding the build-up of sweat – all of which can lead to tissue damage and bacterial growth. To help overcome this, prosthetic manufacturer Blatchford has developed a silicon socket that can be laser drilled to drain sweat from the interface. Meanwhile, blade manufacturer Össur has been developing an interface that integrates more directly with the stump, pushing the remaining muscle through holes in the socket to improve control.\nBut the ultimate future interface could come in the form of osseointegration, where the prosthesis connects directly to the bone via an implant. The challenge of course with an implant that breaks the skin is keeping the site free from infection. Though, back in 2006, University College London completed research that mimics the way the protrusion of a deer’s antlers are protected by soft tissue to prevent infection and maintain the integrity of the skin as a defensive barrier. Fascinating stuff.\nThe Paralympics: a hotbed of engineering innovation\nAs with many sports disciplines, the smallest competitive gains can be the difference between going for gold and going home empty handed. Add in the strict parameters governing the design and function of Paralympic equipment and it’s clear that the trend of major innovation isn’t set to stall anytime soon. Future disability sports technology will be increasingly customised for its user – enabling the next generation of Paralympians to go harder, faster, stronger and push the limits of what’s possible within the Paralympic movement.\nYou may also be interested in:']	['<urn:uuid:ddd176c5-6b1f-47d1-a3f4-6037bf3cbcd8>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T00:20:10.754925	8	32	831
28	sustainability reporting impact financial non financial integration	There is a growing integration between financial and non-financial reporting in sustainability. Companies now need to report climate-related financial risk disclosures in their annual reports following TCFD Recommendations. The SFDR requires detailed reporting of Principal Adverse Impacts (PAIs) of investment decisions, including environmental and social impacts, which must consider both direct and indirect exposures. This integration extends beyond just reporting - sustainability is becoming embedded across core business operations rather than remaining a marginal activity, requiring companies to demonstrate tangible connections between sustainability initiatives and financial outcomes.	['The sustainability sector: a fertile job market for new ACAs\n30 July: The sustainability sector offers a bright future for ACAs beginning their careers, KPMG’s Director of Sustainability Services Richard Betts tells ICAEW Insights.\nThe planet and the next generation\nUntil relatively recently, sustainability didn’t seem very pertinent to many people, explains KPMG’s Richard Betts. Most people had heard about climate change, but it didn’t seem relevant and people couldn’t connect with it.\nThe Australian bush fires, other climate-related crises on top of the COVID-19 pandemic, have acted as a wake-up call for many. Climate change is no longer a vague threat that might affect some people somewhere; it’s a problem that is happening now which needs to be acted upon now, says Betts.\nSustainability is intrinsically linked to intergenerational fairness because the actions we take now need to ensure that the ability of future generations to meet their own needs is not compromised.\nThe great momentum we have seen in sustainability initiatives over the past few years have partly been caused by a public opinion shift that connects the future security of their children and grandchildren with a sustainable planet. People want future generations to thrive, and this will only happen through action taken in the immediate present.\nA new paradigm\n“There needs to be a connection”, Betts explains. “There can’t be accountability until we can make a tangible connection with our lives. While terms like future generations can seem very vague, recent disasters have shown that this is a problem that affects us now and, unless we turn things around, it will get a lot worse in the coming years.\n“We are essentially saying we need a new paradigm. We need to see the bigger picture. We’ve tended to consider only the direct financial costs and returns rather than also indirect costs and impacts, like those on the environment. We’ve also tended to focus predominantly on the short-term (for example, companies focusing on short-term results, employees on how to pay the next bill, or politicians on how to win the next election).\n“However, due to the rapid increase in the world’s population and size of the global economy in recent decades, we are already exceeding the planetary boundaries for many resources and can no longer continue in the same way as before.”\nThe sustainability sector as a job market for new ACAs\nBetts also emphasises that sustainability is a growth area, despite difficulties in the economy in the short term due to COVID-19. The sector is still extremely active and there will be a lot more work done in the coming years.\nThe majority of large accountancy firms now have sustainability teams, and these could be a great focus for ACAs searching for work.\nBetts describes his career trajectory: “I never thought about working in sustainability originally. I started in financial audit, transferred to KPMG’s sustainability department and took to it well. The ACA training gave me a really good foundation and transferable skills. Doing the ACA and working in financial audit gives you discipline and rigour, project management skills and attention to detail. Even though our data is not financial data, you still need the same skills.”\nSustainability is a rapidly developing area and with many companies only just getting started, ACAs can bring added value in many different ways.\nSustainability work matches the ACA skillset\nSustainability initiatives are also moving away from being marginal activities to become integrated into and across core business. The necessary changes won’t happen if one part of a company is sustainable, but the rest is unsustainable or business as usual.\nDecarbonisation, for example, should affect the whole business, explains Betts.\n“At KPMG, we have sustainability engagements working with our financial audit teams but also with our tax and advisory teams,” he said.\nBetts has worked on a wide range of solutions in over 20 countries. These include non-financial reporting and assurance (for example GRI Standards, CDP, ISAE 3000, AA1000 and IIRC Integrated Reporting), sustainability strategy and implementation, sustainable financing, environmental due diligence, climate change advisory services (TCFD and climate risk, GHG monitoring and reporting, decarbonisation and renewable energy), social value and circular economy. His ACA provided transferable skills enabling him to thrive in these fields.\nThere are many initiatives around auditing non-financial impact, integrating key non-financial information into primary annual reports. Aligning with the TCFD Recommendations requires companies to report climate-related financial risk disclosures within their annual report.\nThere are also many important recent developments focused on interconnected standard-setting. In the future, companies will need to report far more extensively on non-financial data that is now material to all businesses. More ACAs will be needed to do the necessary work.', 'Sustainable finance is a major priority for lawmakers, and the Sustainable Finance Disclosure Regulation (SFDR), which entered into effect in March 2021, is a key concern for asset managers. New Level 2 technical standards entered into effect on 1st January 2023. CACEIS expert analysis.\nWhen the SFDR was drafted, the European regulator’s objectives were clear: direct capital towards sustainable investment, combat “greenwashing”, integrate a sustainability criterion into investment risk management, encourage transparency and long-term investment, and categorise funds based on their commitment to non-financial ESG criteria: funds with a sustainable investment objective (”Article 9”), those that promote non-financial characteristics (”Article 8”) and others that do not integrate any explicit sustainability objective (”Article 6”).\n“Compliance with this regulation presents a real challenge for our clients, because it involves understanding a new set of procedures: set up an appropriate risk management system, ensure full commitment from management bodies, train and keep employees up to date, and digitise processes by adapting their tools, which means taking on higher costs”, said Eliane Méziani, Senior Advisor - Public Affairs at CACEIS.\nThe new Level 2 standards (Regulatory Technical Standards, ”RTS”) in force since the beginning of the year more precisely define the rules for implementing SFDR, thereby eliminating uncertainty in the interpretation and application of this regulation.\nThey have the same objective: better protect end-investors by improving the disclosure of environmental, social and governance (ESG) information. They require financial institutions to report the main adverse effects of their investment decisions and the sustainability characteristics of their range of financial products.\nFrom now on, management firms and other financial stakeholders concerned must clearly state their stance on adverse impacts - Principal Adverse Impacts or PAIs, namely by responding to the following questions: Have they taken these impacts into account? If not, why not? In what way?\nAs such, funds classified under “Article 8 and 9”, i.e. those that integrate sustainability into the investment process, must provide clear justifications in their regular disclosure documents, such as the annual report and prospectus, and must maintain a dedicated section on their website entitled “Information on sustainability factors”. Sustainability factors include environmental and social matters, respect for human rights and the fight against corruption.\nCalculating impacts for PAI reporting must include direct and indirect exposure via the funds, funds of funds, derivatives, holding companies and SPVs. Managers must therefore examine the underlying investments and consider the resulting negative impacts. Publication of these impacts will be mandatory from next spring (June 2023).\nUnfortunately, this information may be unavailable or unreliable, meaning managers have few options: They can submit questions to the issuers and companies concerned, call on non-financial data providers, for a cost, or they can use their own internal resources to analyse the data from a sectoral and qualitative standpoint.\n“The new mission for the European Supervisory Authorities (ESMA, IOPA, EBA) for 2023 mainly aims to streamline the regulatory framework in this area and specifically integrate a review of PAI indicators, in particular with a view to refining the content of all adverse impact indicators and their respective definitions, applicable methodologies, presentation, etc.”, said Eliane Méziani.\nTowards conviction-based sustainable finance\nThe problems encountered and lack of clarity will continue well beyond 1st January 2023. However, financial players are working to address these concerns and ensure greater compliance with SFDR requirements by establishing best practices and implementing standards such as the European EET (European ESG Template).\n“Until publication of the Level 2 standards, SFDR was used more by asset managers as a marketing label (Articles 6, 8 and 9), undoubtedly due to the lack of a standardised and universally recognised label. However, this isn’t the purpose of this regulation, which is primarily aimed at ensuring transparency. The RTS are perfectly in line with this, and actively work to combat greenwashing,” added Eliane Meziani.\nIf a manager indicates that their investment fund meets the SFDR Level 2 sustainability criteria, they will have no choice but to strictly adhere to the sustainability criteria under these reporting requirements.\nAt the end of 2022, the introduction of Level 2 automatically entailed an overhaul of the range of investment funds in order to precisely align with the purpose of SFDR: offer the end-investor transparency over the sustainability characteristics of investment funds as well as the ability to compare them. In fact, there was an increase in recategorisations as the Level 2 rules came into effect. Most of these reclassifications involved upscaling from “Article 6” funds to “Article 8” funds. Many managers took a cautious approach when SFDR came into effect and classified most of their funds in the lowest category. Others opted to reclassify Article 9 funds as Article 8 funds, which is less demanding in terms of sustainability credentials.\nAs part of this effort to combat greenwashing, ESMA (the European Securities and Markets Authority) launched a consultation at the end of November with a view to issuing guidelines on the use of ESG terms in fund names, the use of the terms “green”, “sustainable”, “light green”, “dark green”, “impact”, etc., which remain unregulated to date.\nAs a leading asset servicer, CACEIS seeks to support its clients in successfully meeting this important challenge. “To this end, we’ve partnered with Clarity AI with the aim of offering our clients a complete range of solutions in terms of ESG, impact and climate risk management, thereby ensuring regulatory compliance,” explained Laurent Majchrzak, Group Head of Products at CACEIS.\nThe Clarity AI platform, which is available on the CACEIS Connect Store in the OLIS portal, analyses over 50,000 companies, 300,000 funds, 198 countries and 188 local authorities through the use of machine learning and big data, to measure and optimise the sustainable impact of investments and calculate PAIs. It offers direct access to a broad coverage of high-quality ESG, climate and impact-related data, as well as tools to meet SFDR’s regulatory reporting requirements, in addition to the requirements of the EU Taxonomy, TCFD (Task Force on Climate-related Financial Disclosures) and EET (European ESG Template).\n“This practical and efficient solution lets clients manage all of their sustainability objectives and commitments while avoiding complicated and costly IT integration,” added Laurent Majchrzak.']	['<urn:uuid:b98d7705-b640-4b56-83f8-5dd9b35df88f>', '<urn:uuid:df2ad9f1-f696-4727-8701-ff501d01d63d>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T00:20:10.754925	7	87	1798
29	What types of advanced medical training tools are available nowadays for teaching airway management, and how do they use modern technology?	Modern airway management training combines traditional simulators with advanced technology. The tools include the AirSim Bronchi Model with realistic tongue texture and jaw movements for bronchoscopic examination, an Airway Management Trainer for practicing intubation and ventilation techniques, and the Difficult Airway Trainer for advanced procedures like cricothyrotomy. These devices are enhanced by Virtual Reality (VR) and Augmented Reality (AR) technology, which provides an immersive environment for risk-free practice of intricate procedures.	['The healthcare sector, more than ever, demands constant learning and adaptation. With the rapid evolution of medical science and the urgency to train healthcare professionals effectively, technology plays a pivotal role.\nAnd what better way to integrate technology into this dynamic sector than through gadgets? Here’s how the latest tech tools and innovations can facilitate superior education in healthcare.\nHow to Utilize Technology to Improve Education in the Healthcare Sector\nIn a domain as dynamic and crucial as healthcare, the role of education is paramount. Keeping pace with the myriad advances in medical science requires an educational approach that’s both innovative and effective.\nAs the world becomes more digitized, the healthcare sector is witnessing a revolution in the way professionals learn and train, all thanks to gadgets and technological tools.\nVideo Learning: A Comprehensive Approach to Digital Education\nIn an age where digitization is the norm, Video learning emerges as a keystone in healthcare education. Its significance transcends mere convenience. With advancements in filming technologies and accessibility to the internet, these digital classrooms have become centers of knowledge dissemination.\nThe power of Video learning lies not just in its ability to illustrate and demonstrate but also in its vast reach. Modern platforms, optimized for various devices from smartphones to personal computers, offer a wealth of content.\nThis mode of learning transcends geographical boundaries. Students, irrespective of their location, have the opportunity to access knowledge from global experts, opening doors to techniques, theories, and cases that might be rare in their vicinity.\nThis globalized approach to education ensures that learners get a holistic view, prepping them for myriad challenges.\nAI-Driven Learning: Revolutionizing Personalized Education\nThe wave of Artificial Intelligence (AI) has touched numerous sectors, and education in the healthcare domain is no exception. The inception of AI-Powered Bots and Applications is reshaping the dynamics of traditional learning.\nWhat sets AI-driven learning apart is its ability to “understand” the learner. Unlike standard educational tools, these smart tools register the learning patterns, strengths, and areas that require improvement for each individual.\nBased on these insights, the content is curated, ensuring that the learning experience is bespoke.\nBut the wonders of AI don’t stop at content curation. These intelligent systems actively interact with students, answering queries, providing clarifications, and even suggesting further reading or modules based on individual progress.\nSuch proactive learning aids can be invaluable, especially in a field as intricate and ever-evolving as healthcare.\nThe integration of AI in learning not only makes the process more efficient but also ensures that the content resonates with the learner, maximizing retention and application of knowledge. In essence, AI-driven tools are like personal tutors, dedicated to enhancing the learning journey of each student.\nThe Magic of VR and AR in Healthcare Education\nVirtual Reality (VR) and Augmented Reality (AR) are not just for gamers. In the healthcare sector, these technologies provide an immersive environment, perfect for a range of educational applications.\nFrom simulating intricate surgeries to visualizing drug interactions at a cellular level, VR and AR headsets offer students a risk-free platform to practice, learn, and explore.\nInteractive Tablets and Smartboards\nThe conventional methods of using textbooks and 2D diagrams are gradually being overshadowed by more interactive and immersive tools. Interactive Tablets and Smartboards have emerged as powerful devices to project 3D representations of the human anatomy, allowing students to grasp complex structures and processes in a more intuitive manner.\nThe ability to rotate, zoom, and peel through layers gives learners a holistic understanding, deepening their insights into the marvel that is the human body.\nWearable Gadgets: Bridging the Gap Between Theory and Real-life Medical Practice\nIn the rapidly evolving technological landscape, wearable gadgets have carved a niche for themselves. While the general populace might identify them as tools for tracking daily health metrics like steps taken or heart rate, their potential in healthcare education is profound and far-reaching.\nDelving Deep into Patient Simulation\nThe essence of wearable gadgets in healthcare education lies in their ability to simulate intricate patient scenarios. Instead of just relying on textbook cases, students can now witness real-time vitals and health parameters through these devices.\nFor instance, a smartwatch could be programmed to replicate the heartbeat patterns of a patient experiencing arrhythmia or a fitness band could simulate oxygen saturation levels in certain respiratory diseases.\nThis dynamic and real-time representation offers students an unprecedented hands-on approach to understanding diseases, their manifestations, and potential interventions.\nIt’s one thing to read about a condition and entirely another to see it unfold in real time. Such immersive experiences foster deeper understanding and retention of knowledge.\nEnriched Training Through Real-world Experiences\nApart from simulating diseases, wearable gadgets also provide a continuous stream of data, helping students understand the progression of certain conditions and the efficacy of interventions.\nFor example, observing how vitals change post-administration of a drug can give invaluable insights into its effects and potential side effects.\nThe integration of wearable technology in healthcare education equips students with a more pragmatic understanding of medical situations. It moves them away from abstract learning and plunges them into the heart of real-life medical scenarios.\nThis transition from theoretical knowledge to practical understanding ensures that budding healthcare professionals are not just well-informed, but also adept at handling real-world challenges.\nThe confluence of technology and healthcare education is creating a paradigm shift in the way professionals are trained. By integrating gadgets and advanced tech tools into the curriculum, the healthcare sector is set to produce professionals who are not just well-versed in theory, but also adept at harnessing technology for practical applications.\nThe fusion of gadgets and education heralds a promising era for healthcare professionals worldwide.', 'Banner Simulation System has a wide array of equipment used to teach and improve skills sets.\nAdvanced Catheterization Trainer\nTrainer used for urethral and supra-pubic catheterization training. Skills taught include:\n- Aseptic catheterization technique\n- Catheter placement\n- Fluid management (Irrigation)\n- Catheter removal\nAdvanced Venous Management Arm\nTrainer used for practicing the skills related to venipuncture, phlebotomy and IV cannulation.\nAirSim Bronchi Model with Nasal Passage\nAirway simulator used for training all critical airway management procedures. Skills taught include:\n- Visualization of internal features during bronchoscopic examination\n- Tongue has a realistic shape, texture and response\n- Jaw mandible allowing jaw thrust and realistic mouth opening together with the full range of jaw and mouth movements\n- Neck offers a full range of extension, flexion, lateral and rotational movements\nAirway Management Trainer\nUpper torso and head simulates complications when practicing a variety of intubation, ventilation, and suction techniques. Skills taught include:\n- oral and nasal intubation\n- use of LMA (Laryngeal Mask Airway)\n- demonstration of Sellick Maneuver and laryngospasm\nInfant training arm with bony structures used to practice the technique of neonatal radial artery puncture. Skills taught include:\n- Percutaneous puncture in radial artery site\n- Blood infusion for blood backflow in syringe\nBaby IV Trauma Module\nSimulated infant head with internally molded scalp veins for practicing neonatal peripheral venous access. Skills taught include:\n- Peripheral IV line insertion and removal for fluid and medication administration after patient stabilization\n- Flash to confirm proper placement\n- Maintenance and securing of line\nNeonate infant used for the practice of lumbar puncture techniques.\nThe trainer is positioned in the lateral decubitus position. Realistic interchangeable spine with spinal cord may be palpated for location of correct puncture site and fluid may be infused.\nBreastmilk Hand Expression Trainer\nModel for demonstrating the techniques required for the hand expression of breast milk.\nUsed for teaching and learning tool for midwives, health visitors and other health care practitioners.\nCentral Venous Trainer\nModel of human torso with detachable right arm designed for the insertion and care. Skills taught include:\n- CVC and PICC line\n- External jugular and subclavian catheters\n- Blood withdrawal, Heparinization and fluid infusion\n- Dressing and suturing techniques\nCranial Trauma Management\nCast from an actual road-accident victim. Presents students with a variety of trauma conditions. Palpable fractures include:\n- Open depressed skull fracture\n- Le Fort I & III\n- Nasal fracture\n- Bilateral mandibular fractures\n- Fracture of C-6 vertebra\n- Unequal pupils\nDifficult Airway Trainer\nAnatomically correct difficult airway management skills using manual maneuvers and mechanical devices. Skills taught include:\n- Oropharyngeal and nasopharyngeal airway insertion\n- Endotracheal and right mainstem intubation\n- Needle or surgical cricothyrotomy\n- Retrograde intubation\n- Percutaneous transtracheal jet ventilation\nInfant Airway Management Trainer\nThe Infant Airway Trainer has the realistic anatomy of a three-month-old infant for teaching and practicing basic and advanced airway management skills.\nLumbar Puncture Trainer\nUsed for practicing lumbar puncture and epidural procedures. Skills taught include:\n- Palpation of lumbar spinous process\n- Needle positioning and insertion\n- Collection of cerebrospinal fluid (CSF)\n- CSF pressure measurement\nNuclear, Biologic and Chemical (NBC) Module used to replicate the effects of toxic gasses on the central nervous system by producing realistic, frothy, bodily fluids that emanate from tear ducts, mouth and nose.\nTrainer is used with Simman.\nNG Tube and Trach Care Simulator\nTorso task trainer for instruction in respiratory conditions and the practice of gastrointestinal care via nasal and oral access. Lungs and stomach may be filled with fluid. Skills taught include:\n- Tracheostomy care\n- NG tube insertion and removal\n- NG tube irrigation, instillation, and monitoring\n- Feeding tube insertion and removal\n- Gastric lavage and gavage\nNeonatal Intubation Trainer\nLifelike upper torso trainer allows teaching of airway skills on the newborn baby. Trainer has a realistic anatomy of a newborn baby. Example of skills taught include:\n- Intubation (oral and nasal)\n- Bag-Valve-mask ventilation\n- Correct tube placement can be checked by practical inflation chest\nNursing Anne Training Modules\n- Breast Exam Module facilitates training in the recognition of breast abnormalities, sizes of nodules, and relative locations by simulating multiple abnormalities found during breast examinations.\n- Mastectomy Module is used to practice post-surgical mastectomy care procedures.\n- Fundus Skills and assessment module is used for post-partum skills training designed to teach fundus assessment and fundal massage.\n- Wound care and assessment set contains wounds that can be added to your Nursing manikin simulator for realism.\nPediatric Training Modules\nAdd-on trauma modules for the MegaCode Kid used for a wide range of trauma conditions. Features include:\n- Head skin with zygomatic lacerations\n- Chest skin with 1st, 2nd and 3rd degree burns\n- Right-hand dog bite\n- Right-hand electrical wire burn entry\n- Right leg tibia fracture\n- Left-thigh closed femur fracture\n- Left-foot crushed toes\n- Right-foot electrical wire burn exit\nPeripheral Venous Access\nUpper torso model with right arm designed for practicing insertion, care, and removal of peripherally inserted central catheter lines.\nMovable chin simulates occlusion of jugular.\nAnatomically correct cephalic, basilic, and median basilic veins, jugular, subclavian, and superior vena cava. Palpable ribs allow measurement of proper catheter length externally. Infusible veins allow realistic blood return in flashback chamber.\nSimMan Nursing Modules\nAdd-on nursing modules for SimMan Simulator used for a wide range of trauma conditions. Features include:\n- Chest surgical incision with skin staples (lay-on)\n- Abdominal packing and irrigation wound (lay-on)\n- Abdominal surgical incision lay-on with nylon sutures and Penrose drain\n- Infected colostomy stoma\n- Forearm suture module, gluteal and ventro-gluteal pressure ulcer\n- Thigh packing and irrigation wound, thigh suture\n- Below knee amputation, varicose vein leg with stasis ulcer\n- Diabetic foot with gangrenous toes and heel decubitus ulcer\nSmall Wound Model (Vac Therapy)\nWound module allows for the demonstration and practice of skills associated with Negative Pressure Wound Therapy. Skills taught include:\n- Applying wound dressings and set-up\n- Subsequent management of Vacuum Therapy Units.\nAdd-on trauma module for the Simman Simulator used in trauma life support and lifesaving first-aid scenarios. Features include:\n- Trauma head cover features an avulsed ear, avulsed eye, lacerations of the lip, neck, and scalp, nasal bleeding, impaled object, and face contusions.\n- Seat belt contusion with compound clavicle fracture (lay-on)\n- 1st, 2nd, and 3rd degree burn arm\n- Hand with open fracture of index finger, exposed tendons, and multiple lacerations (sleeve)\n- Abdominal wound with exposed vicera (lay-on)\n- Compound femur fracture\n- Crushed foot with exposed bone and soft tissue and amputation of the small toe (right)\n- Projectile entry / exit wounds (small and large caliber)\nWound Care Model\nAllows training in wound cleansing, classification, staging and assessment, measurement of wound length, depth, undermining, and tunneling. Skills taught include:\n- pressure ulcers\n- anal fistula\n- suture incision']	['<urn:uuid:45faccf3-68a0-4a38-95ea-b7ba9fcd39f5>', '<urn:uuid:8f89e5f0-bbb2-4c29-b081-b907a72963cf>']	factoid	direct	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-13T00:20:10.754925	21	71	2077
30	differences engagement versus immersion meaning	While often used interchangeably, immersion and engagement are distinct concepts on a spectrum of attentiveness. Immersion occurs when people slip into a manufactured reality and lose themselves in a fictional world - what J.R.R. Tolkien called 'the enchanted state.' In contrast, engagement refers to when a story prompts audience action, such as sharing on social media or having post-visit conversations.	['Innovation in and by museums is no longer a frightening or alien concept, and is increasingly being woven into the cultural fabric of museums. Disciplinary boundaries have become more porous, and museums have become skilled at incorporating social media and new technologies.\nIn turn, the museum experience has evolved dramatically, becoming interactive, participatory, and engaging in ways that would have been unimaginable twenty years ago. Indeed, as experiments and techniques mature, immersive storytelling – multisensory, complex, and deeply encompassing – has become the next frontier for museums.\nThis is for two reasons. First, audience expectations have changed dramatically, especially in response to the ubiquity of digital entertainment, video games, and the Internet. That is, audiences develop new kinds of expectations in response to the technologies we adopt, and people are no longer content to sit back passively. They want to feel part of the experience.\nSecond, advances in technology —cheaper screens, greater image quality, and increased computing power— have opened exciting possibilities for storytellers, artists, and designers, and offer new ways to shape how people learn about art, science, and history.\nImmersion and Engagement\nImmersion and engagement are typically used interchangeably, but immersion is not the same as engagement. A useful way of understanding them is not as separate states of being, but as part of a spectrum of attentiveness.\nImmersion is what happens when people are not merely informed or entertained but actually slip into a manufactured reality. In other words, it is the experience of losing oneself in a fictional world. J.R.R. Tolkien, the creator of the famed Lord of the Rings trilogy, described it as “the enchanted state.”\nEngagement takes place when a story or message provokes some sort of action among the audience – a tweet, Instagram share, or post-visit conversation over coffee.\nExperiments and Opportunities\nMuseums excel at mounting exhibitions that are engaging and convey stories. And the desire to “immerse” visitors and offer intense, memorable experiences, is certainly not new. Dioramas, models, and displays have always been used. But rarely do we feel transported into a different reality. Nina Simon, author of The Participatory Museum, observes that exhibitions excel at using a variety of objects and narrative devices to tell broad stories about a time period, particular area of research, set of objects, or concept. Yet specificity and detail, the powerful cornerstones of immersive stories, are rarely found in museum exhibitions.\nIndeed, what this new frontier of immersive storytelling foregrounds is a desire for something more physically interactive, more multisensory, that awakens our bodies and senses. Moreover, compared to exhibits with traditional text labels and didactic panels, immersive exhibits have a much wider appeal – from children to older visitors, to persons with visual or hearing impairments, or visitors where language can be a barrier to engagement.\nSo the question is – how are museums tackling the challenge of immersive storytelling?\nThe answer is, through experimentation. We’re seeing a proliferation of museums who are installing sound and light shows, interactive multi-user learning stations, and multi-media simulations. Some are dabbling with sensory enhancement, and others are plunging forth with forays into virtual reality.\ncaption PLACE-Hampi, an immersive installation created by Jeffrey Shaw and Sarah Kenderdine, leading scholars and artists in virtual reality technology.\nOverall, these examples show that technology can be particularly effective when attempting to create cognitively rich environments, with multiple channels of sensory engagement that convey strong narratives. Indeed, new technologies tend to give rise to creative new ideas, and storytelling is usually one of the places that artists, technologists, and audiences tend to flock to first.\nStorytelling, in its original form, has always been a living thing – a dialogue, something you could interrupt or physically respond to. Somewhere along the way, we came to expect storytelling as something we experienced passively. Technology has changed our expectations, and what we see is the re-introduction of those physical effects, and interactivity.\nCreating immersive storytelling environments is exciting not just for the technology it involves, but for the impact it has on helping people view themselves as part of the story, which in turn enables people to participate and store learning with more commitment. And for museums, who can arguably go further than books or films as far as storytelling is concerned by unpacking a narrative in physical space, this next frontier, is a promising one.\n-Hanna Cho, Producer & Project Manager']	['<urn:uuid:29a89f30-9e85-4ea3-8764-2fa9af9f56c3>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T00:20:10.754925	5	60	728
31	technical improvements comparison newcomen original steam engine versus watt modifications changes power generation	The Newcomen engine used a vacuum to draw down a piston in a cylinder, with only a single power stroke that produced a jerky motion. Watt made several significant technical improvements between 1781 and 1788, including more efficient steam utilization, a double-acting piston that provided power on both up and down strokes, a rigid three-bar linkage replacing the flexible chain connection, a device to convert reciprocating motion to rotary motion, and a centrifugal governor for speed regulation. These modifications made Watt's engine particularly suitable for driving textile mills, while Newcomen engines had mixed success in driving machinery due to their jerky motion.	"['James Watt Facts\nThe British instrument maker and engineer James Watt (1736-1819) developed an efficient steam engine which was a universal source of power and thereby provided one of the most essential technological components of the early industrial revolution.\nJames Watt was born on Jan. 19, 1736, in Greenock, Scotland, the son of a shipwright and merchant of ship\'s stores. He received an elementary education in school, but of much more interest to him was his father\'s store, where the boy had his own tools and forge and where he skillfully made models of the ship\'s gear surrounding him. In 1755 he was apprenticed to a London mathematical instrument maker; at that time the trade primarily produced navigational and surveying instruments. A year later he returned to Scotland. By late 1757 Watt was established in Glasgow as ""mathematical instrument maker to the university.""\nAbout this time Watt met Joseph Black, who had already laid the foundations of modern chemistry and of the study of heat. Their friendship was of some importance in the early development of the steam engine.\nInvention of the Steam Engine\nIn the meantime, Watt had become engaged in his first studies on the steam engine. During the winter of 1763/ 1764 he was asked to repair the university\'s model of the Newcomen steam engine. After a few experiments, Watt recognized that the fault with the model rested not so much in the details of its construction or in its malfunctioning as in its design. He found that a volume of steam three or four times the volume of the piston cylinder was required to make the piston move to the end of the cylinder. The solution Watt provided was to keep the piston at the temperature of the steam (by means of a jacket heated by steam) and to condense the steam in a separate vessel rather than in the piston. Such a separate condenser avoided the large heat losses that resulted from repeatedly heating and cooling the body of the piston, and so engine efficiency was improved.\nThere is a considerable gap between having a good idea for a commercial invention and in reducing it to practice. It took a decade for Watt to solve all the mechanical problems. Black lent him money and introduced him to John Roebuck of the Carron ironworks in Stirlingshire, Scotland. In 1765 Roebuck and Watt entered into a partnership. However, Watt still had to earn his own living, and his employment as surveyor of canal construction left little time for developing his invention. However, Watt did manage to prepare a patent application on his invention, and the patent was granted on Jan. 5, 1769.\nBy 1773 Roebuck\'s financial difficulties brought not only Watt\'s work on the engine to a standstill but also Roebuck\'s own business. Matthew Boulton, an industrialist of Birmingham, England, then became Watt\'s partner, and Watt moved to Birmingham. He was now able to work full time on his invention. In 1775 Boulton accepted two orders to erect Watt\'s steam engine; the two engines were set up in 1776 and their success led to many other orders.\nImprovements in the Steam Engine\nBetween 1781 and 1788 Watt modified and further improved his engine. These changes combined to make as great an advance over his original engine as the latter was over the Newcomen engine. The most important modifications were a more efficient utilization of the steam, the use of a double-acting piston, the replacement of the flexible chain connection to the beam by the rigid threebar linkage, the provision of another mechanical device to change the reciprocating motion of the beam end to a rotary motion, and the provision of a centrifugal governor to regulate the speed.\nHaving devised a new rotary machine, the partners had next to determine the cost of constructing it. These rotary steam engines replaced animal power, and it was only natural that the new engine should be measured in terms of the number of horses it replaced. By using measurements that millwrights, who set up horse gins (animal-driven wheels), had determined, Watt found the value of one ""horse power"" to be equal to 33, 000 pounds lifted one foot high per minute, a value which is still that of the standard American and English horsepower. The charge of erecting the new type of steam engine was accordingly based upon its horsepower.\nOn Watt\'s many business trips, there was always a good deal of correspondence that had to be copied. To avoid this irksome task, he devised letter-press copying, in which, by writing the original with a special ink, copies could be made by simply placing another sheet of paper on the freshly written sheet and then pressing the two together.\nWatt\'s interests in applied chemistry led him to introduce chlorine bleaching into Great Britain and to devise a famous iron cement. In theoretical chemistry, he was one of the first to argue that water was not an element but a compound.\nIn 1794 Watt and Boulton turned over their flourishing business to their sons. Watt maintained a workshop where he continued his inventing activities until he died on Aug. 25, 1819.\nFurther Reading on James Watt\nExcellent biographies of Watt are H. W. Dickinson and Rhys Jenkins, James Watt and the Steam Engine (1927), and Dickinson\'s James Watt (1936). Eric Robinson and A. E. Musson, James Watt and the Steam Revolution (1969), is a documentary history that commemorates the bicentenary of Watt\'s patent for the separate condenser in his steam engine and includes extracts from Watt\'s personal letters and other documents not before published. For background material see H.W. Dickinson, A Short History of the Steam Engine (1939), and T. S. Ashton, The Industrial Revolution (1948).', '||This article includes a list of references, but its sources remain unclear because it has insufficient inline citations. (April 2010)|\nThomas Newcomen (/, /; February 1664 – 5 August 1729) was an English inventor who created the first practical steam engine for pumping water, the Newcomen steam engine. He was an ironmonger by trade and a Baptist lay preacher by calling. He was born in Dartmouth, Devon, England, to a merchant family and baptised at St. Saviour\'s Church on 24 February 1664. In those days flooding in coal and tin mines was a major problem, and Newcomen was soon engaged in trying to improve ways to pump out the water from such mines. His ironmonger\'s business specialised in designing, manufacturing and selling tools for the mining industry.\nThomas Newcomen was a lay preacher and a teaching elder in the local Baptist church. After 1710 he became the pastor of a local group of Baptists. His father had been one of a group who brought the well-known Puritan John Flavel to Dartmouth. Later one of Newcomen\'s business contacts in London, Edward Wallin, was another Baptist minister who had connections with the well known Doctor John Gill of Horsleydown, Southwark. Newcomen\'s connection with the Baptist church at Bromsgrove materially aided the spread of his steam engine, as Jonathan Hornblower (both father and son) were involved in the same church.\nDeveloping the steam engine\nNewcomen\'s great achievement was his steam engine, developed around 1712, combining the ideas of Thomas Savery and Denis Papin he created a steam engine for the purpose of lifting water out of a tin mine. It is likely that Newcomen was already acquainted with Savery, whose forebears were merchants in south Devon. Savery also had a post with the Commissioners for Sick and Hurt Seamen, which took him to Dartmouth. Savery had devised a ""fire engine"", a kind of thermic syphon, in which steam was admitted to an empty container and then condensed. The vacuum thus created was used to suck water from the sump at the bottom of the mine. The ""fire engine"" was not very effective and could not work beyond a limited depth of around thirty feet.\nNewcomen replaced the receiving vessel (where the steam was condensed) with a cylinder containing a piston based on Papin\'s design. Instead of the vacuum drawing in water, it drew down the piston. This was used to work a beam engine, in which a large wooden beam rocked upon a central fulcrum. On the other side of the beam was a chain attached to a pump at the base of the mine. As the steam cylinder was refilled with steam, readying it for the next power stroke, water was drawn into the pump cylinder and expelled into a pipe to the surface by the weight of the machinery. Newcomen and his partner John Calley built the first successful engine of this type at the Conygree Coalworks near Dudley in the West Midlands. A working replica of this engine can be seen at the Black Country Living Museum nearby.\nLater life, and death\nComparatively little is known of Newcomen\'s later life. After 1715 the engine affairs were conducted through an unincorporated company, the \'Proprietors of the Invention for Raising Water by Fire\'. Its secretary and treasurer was John Meres, clerk to the Society of Apothecaries in London. That society formed a company which had a monopoly on supplying medicines to the Navy providing a close link with Savery, whose will he witnessed. The Committee of the Proprietors also included Edward Wallin, a Baptist of Swedish descent; and pastor of a church at Maze Pond, Southwark. Newcomen died at Wallin\'s house in 1729, and was buried at Bunhill Fields burial ground on the outskirts of the City of London: the exact site of his grave is unknown.\nBy 1733 about 125 Newcomen engines, operating under Savery\'s patent (extended by statute so that it did not expire until 1733), had been installed by Newcomen and others in most of the important mining districts of Britain and on the Continent of Europe: draining coal mines in the Black Country, Warwickshire and near Newcastle upon Tyne; at tin and copper mines in Cornwall; and in lead mines in Flintshire and Derbyshire, amongst other places.\nThe Newcomen engine held its place without material change for about 75 years, spreading gradually to more areas of the UK and mainland Europe. At first brass cylinders were used, but these were expensive and limited in size. New iron casting techniques pioneered by the Coalbrookdale Company in the 1720s allowed bigger cylinders to be used, up to about 6 feet (1.8 m) in diameter by the 1760s. Experience led to better construction and minor refinements in layout. Its mechanical details were much improved by John Smeaton, who built many large engines of this type in the early 1770s; his improvements were rapidly adopted. By 1775 about 600 Newcomen engines had been built, although many of these had worn out before then, and been abandoned or replaced.\nThe Newcomen Engine was by no means an efficient machine, although it was probably as complicated as engineering and materials techniques of the early 18th century could support. Much heat was lost when condensing the steam, as this cooled the cylinder. This did not matter unduly at a colliery, where unsaleable small coal (slack) was available, but significantly increased the mining costs where coal was not readily available, as in Cornwall. Newcomen\'s engine was gradually replaced after 1775 in areas where coal was expensive (especially in Cornwall) by an improved design, invented by James Watt, in which the steam was condensed in a separate condenser. The Watt steam engine, aided by better engineering techniques including Wilkinson\'s boring machine, was much more fuel efficient, enabling Watt and his partner Matthew Boulton to collect substantial royalties based on the fuel saved.\nWatt subsequently made other improvements, including the double-acting engine, where both the up and down strokes were power strokes. These were especially suitable for driving textile mills, and many Watt engines were employed in these industries. At first attempts to drive machinery by Newcomen engines had mixed success, as the single power stroke produced a jerky motion, but use of flywheels and better engineering largely overcame these problems. By 1800, hundreds of non-Watt rotary engines had been built, especially in collieries and ironworks where irregular motion was not a problem but also in textile mills. (see reference (2) below).\nDespite Watt\'s improvements, Common Engines (as they were then known) remained in use for a considerable time, and many more Newcomen engines than Watt ones were built even during the period of Watt\'s patent (up to 1800), as they were cheaper and less complicated. Of over 2,200 engines built in the 18th century, only about 450 were Watt engines. Elements of Watt\'s design, especially the Separate Condenser, were incorporated in many ""pirate"" engines. Even after 1800 Newcomen type engines continued to be built and condensers were added routinely to these. They were also commonly retro-fitted to existing Newcomen engines (the so-called ""pickle-pot"" condenser).\nSurviving Newcomen engines\nIn 1964 the Newcomen Society of London arranged for a Newcomen engine at Hawkesbury Junction, Warwickshire to be transferred to Dartmouth, where it can be seen working using a hydraulic arrangement instead of the steam boiler. According to Dr. Cyril Boucher of the Newcomen Society, this Newcomen Memorial Engine dates from about 1725, with new valve gear and other parts added later.\nPerhaps the last Newcomen-style engine to be used commercially – and the last still remaining on its original site – is at the Elsecar Heritage Centre, near Barnsley in South Yorkshire. The only Newcomen engines that can be shown working are believed to be the Newcomen Memorial Engine at Dartmouth and the replica engine at the Black Country Museum in Dudley, West Midlands. There is currently a major project in progress to conserve the Elsecar Newcomen, its house and mineshaft, and to drive it using hydraulics. The project is due to be completed by Summer 2014.\n- Rolt & Allen, p. 33.\n- Morris, Charles R. Morris; illustrations by J.E. (2012). The dawn of innovation the first American Industrial Revolution (1st ed.). New York: PublicAffairs. p. 42. ISBN 978-1-61039-049-1.\n- Russell, Ben. ""In pursuit of power"". http://sciencemuseum.org.uk. Retrieved 19 March 2015.\n- (Eric Preston,Thomas Newcomen of Dartmouth, Dartmouth History Research Group 2012, ISBN 1-899011-27-7)\n- Transactions of the Newcomen Society 35, October 1962\n- Tom Banks (23 February 2012). ""Purpose designs Britons of distinction stamps"". Design Week.\n- Jenkins, Rhys (1936). Savery, Newcomen and the Early History of the Steam Engine in The Collected Papers of Rhys Jenkins. Cambridge: Newcomen Society. pp. 48–93.\n- Rolt, Lionel Thomas Caswell (1963). Thomas Newcomen. The Prehistory of the Steam Engine. (1 ed.). Dawlish: David & Charles. p. 158.\n- Preston, Eric (2012). Thomas Newcomen of Dartmouth and the Engine that Changed the World. Dartmouth: Dartmouth and Kingswear Society and Dartmouth History Research Group. p. 60. ISBN 1-899011-27-7.\n- Rolt, Lionel Thomas Caswell; John S. Allen (1977). The Steam Engines of Thomas Newcomen (2 ed.). Hartington: Moorland Publishing Company. p. 160. ISBN 0-903485-42-7.\n- Kanefsky, John; John Robey (1980). ""Steam Engines in 18th-Century Britain: A Quantitative Assessment"". Technology and Culture (Baltimore: The Johns Hopkins University Press) 21 (2): 161–186. doi:10.2307/3103337. JSTOR 3103337.\n- Musson, A. E.; Eric Robinson (1969). Science and Technology in the Industrial Revolution. Manchester: Manchester University Press. pp. 393 (Chpt. XII). ISBN 0-7190-0370-9.']"	['<urn:uuid:7cf4bb48-a4f6-4748-8aca-2640f53647fc>', '<urn:uuid:f0b9db0c-55a7-4114-9dbe-8ef61b92a90d>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-13T00:20:10.754925	13	102	2528
32	In what ways do regulatory mechanisms for sugar processing differ between bacterial systems and pancreatic function in maintaining glucose homeostasis?	In bacterial systems, glucose regulation works through catabolite repression, where glucose prevents the synthesis of cAMP, which is needed to activate the CAP protein for transcription of genes involved in other sugar metabolism. In human pancreatic function, regulation occurs through insulin secretion from beta-cells, which directly controls glucose entry into cells. When this pancreatic regulation fails, it results in diabetes, either through lack of insulin production (Type I) or cellular resistance to insulin (Type II), leading to elevated blood glucose levels.	"['| web Review of Todar""s virtual Textbook of Bacteriology. ""The Good, the Bad, and the Deadly"" tag words: regulation, metabolism, enzyme induction, enzyme repression, lactose operon, lac operon, trp operon, tryptophan operon, catabolite repression, feedback inhibition, repressor, inducer, allosteric protein. || || |\nPrint this Page\nRegulation and also Control of management in Bacteria (page 5)(This chapter has 5 pages)© Kenneth Todar, PhD\nEnzyme Induction is still considered a form of an unfavorable controlbecausethe impact of the regulation molecule (the energetic repressor) is todecreaseor downregulate the rate of transcription. Cataboliterepression is atype that positive control of transcription, since a regulatoryproteinaffects an increase (upregulation) in the rate of transcriptionof one operon. The process was uncovered in E. Coli and wasoriginallyreferred to as the glucose effect because it was discovered thatglucoserepressed the synthesis of particular inducible enzymes, also thoughtheinducer the the pathway was existing in the environment. The discoverywasmade throughout study that the regulation of lac operon in E. Coli.Sinceglucose is degraded by constitutive enzymes and lactose is initiallydegradedby inducible enzymes, what would take place if the bacterium was grown inlimitingamounts the glucose and also lactose? A plot the the bacterial development rateresultedin a diauxic development curve which showed two unique phases ofactivegrowth (Figure 9). During the first phase of exponential growth, thebacteriautilize glucose together a source of energy until all the glucose isexhausted.Then, after a an additional lag phase, the lactose is utilized during asecondstage the exponential growth.\nYou are watching: What is the role of glucose in catabolite repression?\nFigure 9. The Diauxic GrowthCurve that E. Coli get an impression in limiting concentrations of a mixtureofglucose and also lactose\nDuring the period of glucose utilization, lactose is not utilizedbecausethe cells space unable to transport and cleave the disaccharide lactose.Glucose is always metabolized an initial in preference to other sugars. Onlyafter glucose is totally utilized is lactose degraded. The lactoseoperonis repressed even though lactose (the inducer) is present. Theecologicalrationale is the glucose is a far better source of power than lactosesinceits utilization requires two less enzymes.\nOnly after glucose is exhausted are the enzymes because that lactoseutilizationsynthesized. The secondary lag during diauxic growth represents thetimerequired because that the complete induction that the lac operon and also synthesis ofthe enzymes crucial for lactose utilization (lactose permease andbeta-galactosidase).Only climate does bacterial expansion occur in ~ the expense of lactose. Sincethe accessibility of glucose represses the enzymes for lactoseutilization,this form of repression ended up being known together catabolite repressionorthe glucose effect.\nGlucose is well-known to repress a large number of inducible enzyme inmanydifferent bacteria. Glucose represses the induction that inducibleoperonsby inhibiting the synthetic of cyclic AMP (cAMP), a nucleotidethatis forced for the initiation of warrior of a big number ofinducibleenzyme systems including the lac operon.\nThe duty of cyclic a cAMP is complicated. CAMP is required toactivatean allosteric protein dubbed CAP (catabolite activator protein)which binding to the promoter lid site and stimulates the binding of RNAppolymerase come the promoter for the initiation that transcription. Thus,toefficiently encourage gene warrior of the lac operon, not only mustlactose be present to inactivate the lac repressor, but cAMP should beavailableto tie to lid which binding to DNA to facilitate transcription. Inthepresence of glucose, adenylate cyclase (AC) task is blocked. ACis required to synthesize cAMP indigenous ATP. Therefore, if cAMP level arelow, cap is inactive and also transcription does no occur. Intheabsence of glucose, cAMP levels are high, lid is caused by cAMP,and transcription occurs (in the existence of lactose).\nMany positively managed promoters, such together the lac promoter, arenot totally functional in the presence of RNAp alone and also requireactivationby CAP. Lid is encoded through a different Regulatory gene, and also is existing inconstitutive levels. Lid is energetic only in the visibility of cAMP. Thebindingof cAMP to CAP reasons a conformational adjust in the protein allowingitto bind to the promoter near the RNAp binding site. CAP deserve to apparentlyinteract with RNAp to increase the price of operon warrior about50-fold.Positivecontrol of the lac operon is depicted in number 10.\nFigure 10. Cataboliterepressionis positive manage of the lac operon. The impact is rise in therate of transcription. In this case, the lid protein is activated bycAMPto bind to the lac operon and facilitate the binding that RNA polymeraseto the promoter to transcribe the gene for lactose utilization.As a type of catabolite repression, the glucose effect serves a usefulfunction in bacteria: it calls for the cells to use the finest availablesourceof energy. For plenty of bacteria, glucose is the many common and also readilyutilizablesubstrate because that growth. Thus, the inhibits indirectly the synthesis ofenzymesthat metabolize poorer resources of energy.END that CHAPTERPrevious PageReturn to web page 1© Kenneth Todar, Ph.D. All civil liberties reserved. - www.ugandan-news.com\nSee more: Why Does Suicide Is My Only Option, Suicide Is My Only Option: Suicidewatch\nKenneth Todar, phd | home | Table of contents\n| © 2020 Kenneth Todar, phd - Madison, Wisconsin |', ""The Dish on Diabetes\nWhen studying medicine, you become intricately familiar with the various organs and systems of the body. One of my favourite organs is the pancreas, a tadpole-shaped structure that lies behind the stomach and connects to the first part of the small intestine (the duodenum). The pancreas is a mighty little complex powerhouse that has numerous functions essential to the body. It provides digestive enzymes to facilitate breakdown of carbohydrates, protein and fats, and secretes bicarbonate in its ductal network to neutralize the acidic juices as food passes into the small intestine from the stomach.\nOne of the most interesting functions of the pancreas, however, has to do with blood sugar control, as part of the endocrine system. Contained within the pancreas are specialized cells, known as beta-cells, that produce and release insulin. Insulin is a small compound that acts in a similar fashion to a key, unlocking the ‘doors’ (transporters) on cells to allow glucose to enter and be used as an energy source. Every cell in the body subsequently has a specialized lock (receptor) on its surface that interacts with insulin to facilitate this process and lower blood sugar levels. Without these two essential components of the system, the body cannot properly use its primary energy source to fuel essential organs, such as skeletal muscle, heart and brain.\nDiabetes mellitus occurs when there is a dysfunction of either the key or the lock within this system. Type I diabetes, typically with an onset in childhood, occurs when the pancreas fails to produce enough insulin keys to unlock the door that allows glucose to enter the cell. This is due to an autoimmune reaction, where the body’s own immune defence system begins to destroy the beta cells in the pancreas, causing an absolute insulin deficiency. Type II diabetes occurs when there is a problem with the lock on the surface of the cell that prevents insulin from unlocking the door. This is known as insulin-resistance, typically associated with a family history of type II diabetes, a diet high in carbohydrates and fats, and a sedentary lifestyle. Unfortunately, once a disease of adulthood, we are seeing increasing cases of type II diabetes in children as well. In both types of diabetes, the end result is a persistent elevation of the blood glucose levels.\nBut why do we care about high blood sugar?\nAn absolute insulin deficiency, such as that found within Type I diabetes, is in fact a medical emergency as it can lead to a life threatening condition known as diabetic ketoacidosis (or ‘DKA’) when untreated. More chronically, however, elevated levels of glucose in the bloodstream have damaging effects on blood vessels and can lead to cholesterol plaques within arteries, known as atherosclerosis. This affects both large and small vessels, increasing the risk of stroke, heart attack, renal injury, blindness, loss of sensation in the feet, poor wound healing, amputations and recurrent infections. As you can imagine, this can have severe consequences on quality and longevity of life.\nSymptoms & Treatment\nClassic symptoms of diabetes include excessive and prolonged thirst with increased urination (in volume and frequency) and hunger. The treatment for type I diabetes is replacement of insulin, an essential hormone within the body. For type II diabetes, a multi-dimensional treatment approach usually works best, involving lifestyle changes and pharmacology. This includes maintaining a diet low in carbohydrates and saturated fat, with lots of leafy greens, protein and unsaturated fats, otherwise known as the ‘Mediterranean Diet’. Furthermore, exercise is key in helping to regulate blood sugars in type II diabetes, with the best effects seen with at least 150 minutes per week of moderate to vigorous exercise (meaning the heart rate should elevate). While there is no prevention for type I diabetes, healthy diet and exercise can have extremely positive effects on warding off the onset of type II diabetes in susceptible individuals, a growing epidemic in North America.\nWhile diabetes is a chronic disease, the good news is there are healthcare teams available to help control it. With the support of physicians, dieticians, optometrists, nurses and pharmacists diabetes can be managed so that people are able to achieve happy and healthy lives!\nDiagnosis and classification of diabetes mellitus. Diabetes Care. 2010 Jan. 33 Suppl 1:S62-9.\nInzucchi SE, Bergenstal RM, Buse JB, Diamant M, Ferrannini E, Nauck M, et al. Management of hyperglycaemia in type 2 diabetes: a patient-centered approach. Position statement of the American Diabetes Association (ADA) and the European Association for the Study of Diabetes (EASD). Diabetologia. 2012 Jun. 55(6):1577-96.\nHaffner SM, D'Agostino R Jr, Mykkanen L, Tracy R, Howard B, Rewers M, et al. Insulin sensitivity in subjects with type 2 diabetes. Relationship to cardiovascular risk factors: the Insulin Resistance Atherosclerosis Study. Diabetes Care. 1999 Apr. 22(4):562-8\nBoussageon R, Bejan-Angoulvant T, Saadatian-Elahi M, Lafont S, Bergeonneau C, Kassaï B, et al. Effect of intensive glucose lowering treatment on all cause mortality, cardiovascular death, and microvascular events in type 2 diabetes: meta-analysis of randomised controlled trials. BMJ. 2011 Jul 26. 343:d4169.\nWing RR, Lang W, Wadden TA, Safford M, Knowler WC, Bertoni AG, et al. Benefits of modest weight loss in improving cardiovascular risk factors in overweight and obese individuals with type 2 diabetes. Diabetes Care. 2011 Jul. 34(7):1481-6.\nEsposito K, Maiorino MI, Ciotola M, Di Palo C, Scognamiglio P, Gicchino M, et al. Effects of a Mediterranean-style diet on the need for antihyperglycemic drug therapy in patients with newly diagnosed type 2 diabetes: a randomized trial. Ann Intern Med. 2009 Sep 1. 151(5):306-14.""]"	['<urn:uuid:d370c191-47d8-49ea-883f-a9b4eeb37e11>', '<urn:uuid:285f55c1-ef22-450b-978e-23cdec9b9362>']	factoid	direct	verbose-and-natural	distant-from-document	three-doc	expert	2025-05-13T00:20:10.754925	20	81	1729
33	microglia phagocytosis function epilepsy brain	In epilepsy, microglia cells, which are responsible for eliminating dead neurons through phagocytosis, become 'blind' and unable to find or destroy dead neurons. This leads to a buildup of dead neurons that damage neighboring neurons and triggers an inflammatory response, causing further brain damage.	"['Today I\'ll point out a brace of recent research materials, all of which focus on the aging brain. A great deal of aging research is focused on the effects of aging in the brain, in part driven by the large level of investment in Alzheimer\'s disease research, and this overlaps with ongoing efforts to understand how the brain works: how it gives rise to the mind and how specific functional aspects of the mind work, all the way down to the level of proteins and intricate cellular structures such as synapses. While some researchers restrict themselves to investigation and observation, trying to fill in the large blank spaces on the map of the brain, others are working to find ways to repair some of the damage and reverse some of the declines. Forms of cell therapy are perhaps the closest to being broadly useful at the present time, but all sorts of ways to clear out cellular garbage and unwanted metabolic waste - such as the amyloid associated with Alzheimer\'s disease - are headed in the general direction of viability for clinical development.\nNot this year, but certainly within the next decade, new classes of treatment will arrive, therapies that can at least partially address some of the fundamental causes of functional decline in the aging brain, rather than trying to patch over the consequences as so much of present day medicine does. Initially these therapies will be highly restricted, available only in trials, or for patients in the late stages of neurodegenerative conditions. That is the outcome that the current regulatory system forces upon us. There will be a mild but sweeping revolution then, I hope, as treatments for the causes of aging become a reality, that will tear down the ridiculous systems of regulation that stifle development, with the result that effective therapies will become far less costly and far more widely available. A treatment that can address the causes of age-related disease is a treatment that should be undertaken by everyone on a regular basis, not just those who are heavily damaged by the processes of aging, and not just those groups that unaccountable bureaucrats decide should gain access. Today, the foundations for that future are still being built, one incremental step at a time, but is never too early to plan ahead.\nWhen neurons die, their remains need to be eliminated quickly so that the surrounding brain tissue can continue functioning. A type of highly specialised cell known as microglia is responsible for this process which is called phagocytosis. Neurons are known to die during the convulsions associated with epilepsy. But contrary to expectations, in this condition the microglia are ""blind"" and incapable of either finding them or destroying them. Their behaviour is abnormal. And the dead neurons that cannot be eliminated build up and damage the neighbouring neurons further, which leads to an inflammatory response by the brain which harms and damages it even further. This discovery opens up a new channel for exploring therapies that could palliate the effects of brain diseases. In fact, the research group that authored this work is right now exploring the development of drugs to encourage this cleaning up process, phagocytosis.\nThe DNA damage response (DDR) is a complex biological system activated by different types of DNA damage. Mutations in certain components of the DDR machinery can lead to genomic instability disorders that culminate in tissue degeneration, premature aging, and various types of cancers. Intriguingly, malfunctioning DDR plays a role in the etiology of late onset brain degenerative disorders such as Parkinson\'s, Alzheimer\'s, and Huntington\'s diseases. For many years, brain degenerative disorders were thought to result from aberrant neural death. Here we discuss the evidence that supports our novel hypothesis that brain degenerative diseases involve dysfunction of glial cells (astrocytes, microglia, and oligodendrocytes). Impairment in the functionality of glial cells results in pathological neuro-glial interactions that, in turn, generate a ""hostile"" environment that impairs the functionality of neuronal cells. These events can lead to systematic neural demise on a scale that appears to be proportional to the severity of the neurological deficit.\nResearchers have successfully reduced the symptoms and slowed the progression of Huntington\'s disease in mice using healthy human brain cells. The research entailed implanting the animals with human glia cells derived from stem cells. One of the roles of glia, an important support cell found in the brain, is to tend to the health of neurons and the study\'s findings show that replacing sick mouse glia with healthy human cells blunted the progress of the disease and rescued nerve cells at risk of death. Conversely, when healthy mice were implanted with human glia carrying the genetic mutation that causes Huntington\'s, the animals exhibited symptoms of the disease. The researchers believe that the healthy human glia were able to essentially stabilize and perhaps even rescue neurons by restoring the normal signaling function that is lost during the disease.\nWhile strokes are known to increase risk for dementia, much less is known about diseases of large and small blood vessels in the brain, separate from stroke, and how they relate to dementia. Diseased blood vessels in the brain itself, which commonly is found in elderly people, may contribute more significantly to Alzheimer\'s disease dementia than was previously believed, according to new study results. The study analyzed medical and pathologic data on 1,143 older individuals who had donated their brains for research upon their deaths, including 478 (42 percent) with Alzheimer\'s disease dementia. Analyses of the brains showed that 445 (39 percent) of study participants had moderate to severe atherosclerosis - plaques in the larger arteries at the base of the brain obstructing blood flow - and 401 (35 percent) had brain arteriolosclerosis - in which there is stiffening or hardening of the smaller artery walls.\nThe study found that the worse the brain vessel diseases, the higher the chance of having dementia, which is usually attributed to Alzheimer\'s disease. The increase was 20 to 30 percent for each level of worsening severity. The study also found that atherosclerosis and arteriolosclerosis are associated with lower levels of thinking abilities, including in memory and other thinking skills, and these associations were present in persons with and without dementia.\nThe adult brain has some ability to adapt to changes in its environment. This ability is, in part, related to neurogenesis and gliogenesis. Neurogenesis modifies neuronal connectivity in specific brain areas, whereas gliogenesis ensures that myelination occurs and produces new supporting cells by generating oligodendrocytes and astrocytes. Altered neurogenesis and gliogenesis have been revealed in a number of pathological conditions affecting the central nervous system, indicating that modulation of the processes involved in adult neurogenesis and gliogenesis may provide a plausible strategy for treatment. Compared to neurogenesis, gliogenesis occurs more prevalently in the adult mammalian brain. Under certain circumstances, interaction occurs between neurogenesis and gliogenesis, facilitating glial cells to transform into neuronal lineage. Therefore, modulating the balance between neurogenesis and gliogenesis may present a new perspective for neurorestoration. These processes might be modulated toward functional repair of the adult brain.\nResearchers have shown that the dopamine D2 receptor is linked to the long-term episodic memory, which function often reduces with age and due to dementia. This new insight can contribute to the understanding of why some but not others are affected by memory impairment. In this study, a PET camera was used to examine individual differences in the D2 system in a group consisting of 181 healthy individuals between the age of 64 and 68. All participants also had to take part in an all-inclusive performance test of the long-term episodic memory, working memory and processing speed along with an MRI assessment (which was used to measure the size of various parts of the brain). Researchers could see that the D2 system was positively linked to episodic memory, but not to working memory or to processing speed by relating PET registrations to the cognitive data. Researchers could also see that the D2 system affects the functioning of the hippocampus in the brain, long linked to long-term episodic memory.']"	['<urn:uuid:cbd9d6eb-fcf7-40a7-9649-5d45896a0329>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-13T00:20:10.754925	5	44	1345
34	looking to buy rental property what are main requirements for seller financing deal	For a seller financing deal, there are two main requirements. First, the seller must either have no mortgage on the property, or the mortgage balance must be less than or equal to the buyer's down payment. Second, you need a seller who is willing to defer their payment over time instead of receiving a lump sum payment.	['Get Creative with Seller Financing\nWhat is Seller Financing?\nSeller financing, or owner financing, is a financing strategy where the seller takes the place of the bank. The conveyance takes place now, and the seller is paid over time in installments, with interest. The buyer will usually execute a mortgage and promissory note at closing, just like they would do with an institutional lender.\nWhen does Seller Financing make sense?\nSeller financing can be a perfect fit for the right situation, but it won’t make sense in the majority of deals. First, you have to have a seller who either has no mortgage, or the balance on the mortgage is less than or equal to the buyer’s down payment. There are two reasons this is critical: one, the seller is going to take a mortgage from the buyer, and they will want to make sure that mortgage is in first position to protect their security interest; and two, all mortgages have a “due on sale” clause, which means that conveying the property is a default on any existing mortgages and trigger’s the lender’s right to foreclose.\nYou also need to have a Seller who is willing to defer their payment over time. Given the choice, the majority of people prefer to have a smaller amount in a lump sum, rather than a slightly larger amount spread over several years. This is especially true if the reason they are selling is to use those funds to purchase another property or asset in the short term. However, there are benefits to the seller deferring payment. The total amount will usually be higher since they will charge whatever interest rate they negotiate with the buyer; also, the tax impact will be spread out over a few years rather than all at once, which could be useful in a larger tax strategy.\nYou’re going to want to have a special buyer for a seller financed transaction as well. A buyer may turn to seller financing by necessity, if there is some reason conventional financing is unavailable. This could be because of some problem with the buyer’s credit (a foreclosure in their recent past, for instance), or because of a problem with the property that makes it unfinanceable. A buyer may also look for a seller-financed transaction even if they could obtain a conventional mortgage loan, since every aspect of seller financing is negotiable. A savvy buyer may be able to negotiate better terms, or perhaps just custom terms that fit their strategy better than a conventional loan would.\nHere is an example of a classic seller-financing transaction. A landlord who owns their rental property outright has become tired of managing the property and tenants. They are getting ready to retire, and want a steady income, which is the reason they bought the investment property to begin with. An investor approaches the landlord, and offers to purchase the property and take over the leases. The buyer’s goal is to purchase three buildings this year, and the landlord’s building fits the profile perfectly. In order to achieve the buyer’s goal, she can only come up with 3% down on each transaction, so traditional financing is out. The buyer offers 3% down if the seller will finance the rest. They agree to an interest rate that is slightly higher than a conventional rate, but lower than hard money. The loan is amortized over 30 years, but with a 10-year balloon, so the seller knows he will be paid in full during his retirement. They close the deal, the buyer signs a promissory note and mortgage in favor of the seller at closing, and five years into the loan the property has appreciated enough that the buyer can refinance with a traditional lender, which she does and pays the seller in full.']	['<urn:uuid:2e4b9450-a602-43e4-b599-349c8f92731e>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T00:20:10.754925	13	57	634
35	I'm using my iPad for flight planning and wonder if I can review recent ATC instructions while using other apps - is there a way to replay recent communications while multitasking?	The FlightLink app supports iOS multitasking modes, including split-screen and slideover views, allowing use alongside other EFB apps. You can tap anywhere in the waveform display to instantly replay the last 2 minutes of recorded audio, which is perfect for double checking clearances.	['Everything is connected these days, from home automation and security systems to cars and video cameras, allowing you to monitor and control these devices remotely with a mobile app on an iPhone or iPad. Connected systems are becoming more and more common in aviation too, providing the ability to transfer flight plans, GPS, weather and more between your iPad and the avionics in the instrument panel.\nThis technology has also made its way into aviation headsets, and Lightspeed’s FlightLink app allows you to wirelessly record in-flight audio, take notes and customize headset settings. The latest version adds several new features and an improved interface that is optimized for all the latest iPhone and iPad models. Here’s how it works.\nGetting started with FlightLink\nThe FlightLink app is completely free and is compatible with all Lightspeed headsets manufactured since 2012, including the Zulu 2 and 3, Sierra, Tango, and PFX. Before getting started, you’ll want to open up your headset’s battery cover, remove the batteries and turn on switch #6, which enables FlightLink compatibility.\nNext, power on the headset and pair it wirelessly to your iPad or iPhone using Bluetooth.\nRecording Audio and Notes\nAfter connecting your Lightspeed headset to the app, you now have the ability to wirelessly record all activity over the intercom, including inbound and outbound transmissions, and in-cabin conversation. Simply press the large silver Record button located on the left side of the screen, and the app will begin recording audio. Once the recording starts, you’ll see a waveform display on the left side of the screen showing the last 2 minutes of audio recording. You can tap anywhere in this range and instantly play back that audio, which is perfect for those times you need to double check a clearance.\nWhile you can only do an instant replay of the last two minutes of recorded audio, there are no limits to the total amount of audio you can record. This makes it especially useful in the flight training environment, as it allows you to create a permanent archive of all flight communications for postflight briefing and training. After stopping a recording, the Library section of the app allows you to replay it, edit the name and share the audio file. It includes all the standard sharing methods, including email and AirDrop, and also allows you to save the M4A audio file to the iOS Files app.\nThe FlightLink app also includes an interactive notepad, allowing you to take notes alongside the audio recording. You can use your finger, stylus, or the advanced features of the Apple Pencil 2, like the double-tap gesture to switch between writing modes.\nFlightLink supports all the iOS multitasking modes too, including split-screen and slideover views, allowing you to use the app alongside your favorite EFB app.\nAdvanced PFX Controls\nThe FlightLink app brings additional functionality when paired with the Lightspeed PFX app, including a variety of personal preference features:\n- Customize audio settings for different aircraft or users\n- Fine tune audio response for maximum voice clarity and music fidelity\n- Use the Voice Clarity option to boost frequencies common in human speech without impacting the quality of music from auxiliary devices\n- View remaining battery life with the battery level indicator\n- Enable/disable audible low battery alert\n- Choose Auto Shutoff delay intervals\n- Save custom settings in up to three personal profiles\nThe FlightLink app is available free as a free download in the app store. Learn more about Lightspeed’s advanced headsets here.']	['<urn:uuid:9fd629ca-9454-4aeb-8285-8113113b3b5c>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T00:20:10.754925	31	43	584
36	cardiff battalion monmouth regiment world war 2 roles	The Cardiff Battalion did not participate in World War 2 as it had been disbanded in 1918. In contrast, the 3rd Monmouthshire Regiment served in World War 2, fighting throughout the campaign in North West Europe after crossing to France on June 13th, 1944. During ten months of fighting, they suffered 267 killed and 889 wounded.	['On the 19th of November, 1914 the War Office had given it’s sanction to the new battalion, then on the 23rd the recruiting began. The recruiting campaign for this new battalion involved military demonstrations, public meetings, open air concerts, appeals in the work place, at the picture houses and at football matches throughout Cardiff and it’s surrounding areas. Overall the open air concerts proved to be the best way of recruiting men.\nPlaced in Command of the 16th Battalion was Captain Frank Gaskell and by the time this all came together The Great War had already being going on for 16 weeks.\nThe battalion was known and still is known as the ‘Cardiff Battalion’ or ‘The Cardiff Boys’ but in actual fact many of it’s recruits came from other areas of South Wales.\nThe battalion was known to have strong links with Cardiff Rugby Club as a few of it’s internationals joined, including Major Fred Smith who took command after the death of Captain Frank Gaskell on May 17th, 1916.\nDuring the November of 1914 the 16th (Service) Battalion became attached to the 130th Brigade in the 43rd Division. Later in April, 1915 the formation became the 115th Brigade in the 38th (Welsh) Division.\nThe 115th Brigade included;\n2nd Bn Royal Welsh Fusiliers: 1918 -1918\n17th Bn Royal Welsh Fusiliers: 1915 - 1918\n10th South Wales Borderers: 1914 - 1918\n11th Bn South Wales Borderers: 1915 - 1918 (Disbanded)\n16th Bn Welch Regiment: 1914 - 1918 (Disbanded)\nTowards the end of December, 1914 the battalion was moved from Porthcawl to Colwyn Bay, North Wales. The next eight months were spent there training before they were moved to Winchester in August 1915 with other units of the 38th (Welsh) Divison. They had their finally visit to Cardiff in the November of that year before embarking to France on the 4th of December, 1915.\nThe 16th (Service) Battalion War Diary shows that they were in the following places on the following dates:\n· St. Quentin:6/12/1915\n· St. Vasst:30/12/1915\n· Riez Bailleul:16/1/1916\n· Croix Marmuse:25/1/1916\n· La Pannerie:16/2/16\n· Les Choqeaux:17/3/16\n· Givenchy Lez La Bassee:28/3/1916\n· Nevuf Berquin Estaires:14/4/1916\n· La Gorque:1/5/1916\n· Riez ?: 9/5/1916\n· Moated Grange Sector:13/5/1916\n· Riez Bailleul:18/5/1916\n· Moated Grange Sector:22/5/1916\n· La Gorque:26/5/1916\n· La Gorque:10/6/1916\n· Monchy Breton:15/6/1916\nThe battalion experienced major loses in the Battle for Mametz Wood. Most of these casualties occurred on the 7th of July, 1916 when the battalion came under heavy machine gun fire from Flatrion Copse and Sabot Copse.\nBelow is an entry taken from the 16th (Service) Battalion War Diary dated 7th July, 1916.\n“8.30am Bn. under orders drawn up on their own side of slope facing MAMETZ WOOD in lines of platoons with a 2 platoon frontage. 11/SWB in support 10/SWB in reserve. Our artillery ceased firing at wood at 8.30am + first lines of Bn. proceeded over the crest of the slope but came instantly under heavy machine gun frontal fire from MAMETZ WOOD, enfilade fire from FLATIRON COPSE + SABOT COPSE + the German Second System, which now between MAMETZ WOOD + BAZENTIN LE PETIT WOOD, Bn. suffered heavily + has to withdraw to their own side of crest. Bn. made two more attacks but position was much too exposed for any hope or success + orders were received to cease operation. 11/SWB attempted to approach the wood through a gulley running between CATERPILLAR WOOD, slope mentioned above but machine gun fire drove them back. Our losses:- 6offs, killed, 6 wounded, 268 OR’s killed, missing or wounded. Weather very wet, this adding greatly to exhaustion of troops Bn. received orders to return to their Bivouac. Moved off 10.30pm Arrived 4.am 8/7/16”\nThe men who survived Mametz went to fight in the following places:\n· Hebuterne Sect:15/7/1916\n· Hebuterne Sect:22/7/1916\nThe battalion also sustained heavy loses in the Third Battle for Ypres.\n(After those dates I am unable to read the writing on the War Diary).\nThe battalion later disbanded on the 27th of February, 1918.\nAs the following extract from that day shows.\n“From today the 16th/Welch ceases to exist as a battalion” 27th February 1918.\nAn Interview with Albert Evans, British private 16th Battalion Welch Regiment\nThe group photograph is an image of a few of the men including Frederick Rowlands who served with the 16th (Service) Battalion.', 'History of the 3rd Batt. the Monmouthshire Regt.\nA brief history\nof 3rd Battalion The Monmouthshire Regiment\nTwo Companies of volunteers were originally raised in Abergavenny during the Napoleonic Wars of 1793-1815, but after the fall of Napoleon they were disbanded. However, in 1859, because of the panic caused by the hostile tone of the French and the defenceless state of this Country, a number of companies of Rifle Volunteers were raised in Monmouthshire including ‘H’ Company at Abergavenny.\nIn July 1885, the 3rd Corps of The Monmouthshire Rifle Volunteers became the 4th Volunteer Battalion The South Wales Borderers. During the Boer War, the battalion provided numerous volunteers who gained the battalion’s first Battle Honour “South Africa 1900-02”. The value of these volunteers was recognised in 1908, when, under the Lord Haldane reforms, the Militia, Yeomanry and Volunteers were combined to form the single Territorial Force, the forerunner of today’s Territorial Army. The 4th Battalion became the 3rd Battalion The Monmouthshire Regiment (TF) with its headquarters at Abergavenny and companies at Abertillery, Blaina, Cwm, Ebbw Vale, Sirhowy and Tredegar.\nThe 3rd Monmouths served with great distinction in France and Belgium during the 1st World War with the Regiment gaining 25 further Battle Honours. On 8th May 1915 during the 2nd Battle of Ypres, the battalion made one of the most gallant stands in military history when in obeying the order to stand to the last man in, the battalion was practically annihilated, without giving an inch of ground to the enemy, the battalion lost 703 in killed and wounded; all but a handful of officers and men remained. The survivors were merged with those of 2nd Battalion The Monmouthshire Regiment which had suffered a similar fate. The officers and men of battalion were awarded 1 CMG, 2 DSOs, 12 MCs, 13 DCMs and 22 MMs.\nWhen in 1922, the Territorial Army re-formed, 3rd Monmouths re-emerged as 3rd (Brecknockshire and Monmouthshire) Battalion The Monmouthshire Regiment (TA) with its headquarters at Abergavenny and\ncompanies at Abertillery, Blaina, Cwm, Ebbw Vale, Nantyglo and Tredegar. Although the Territorials of Monmouthshire had been allowed in 1908 to adopt a title different from that of the Line Regiment of their County, the South Wales Borderers had always provided their Permanent Staff Instructors and many Adjutants. It was therefore fitting that an Army Order of 1929 should clarify the position by constituting the Monmouthshire battalions as “part of the Corps of The South Wales Borderers”.\nDuring the 2nd World War the Brecknocks and 3rd Monmouths assumed their separate identities again. The 3rd Monmouths served at home until 1944, when they crossed to France on 13th June and fought throughout the campaign in North West Europe. It was with this battalion that Corporal Ted Chapman gained the Victoria Cross on 2nd April 1945; the only Victoria Cross to be awarded to a member of the Corps of The South Wales Borderers during the 2nd World War. In ten months of fighting, the battalion suffered 267 killed and 889 wounded.\nWhen the Territorial Army was re-constituted in 1947, the 3rd Monmouths became 637 (3rd Mons) Heavy Anti-Aircraft Regiment Royal Artillery (TA). In May 1953 in recognition of it’s contribution during two World Wars, the Regiment was granted the freedom of the Borough of Abergavenny. On the disbandment of the Anti-Aircraft Command in 1955, the Regiment amalgamated with 638 Light Anti-Aircraft Regiment. The Regiment finally disappeared in re-organisation of the Territorial Army in 1967. Although there are no Territorial Army units based in Abergavenny today, the spirit and traditions of The Monmouthshire Regiment are proudly maintained in the county by the Regular and Territorial Battalions of The Royal Regiment of Wales (24th/41st Foot).\nDate of coverage04/11/2005\n- No categories\nThese are the most popular locations used on the site. The more popular the keyword, the bigger the font size.\nView all the locations used in this catalogue.']	['<urn:uuid:0e2d78e1-80c3-4e37-8628-8e6945d3c9de>', '<urn:uuid:23af8420-0c5d-4181-8f7e-51260f58edee>']	factoid	direct	short-search-query	similar-to-document	comparison	novice	2025-05-13T00:20:10.754925	8	56	1372
37	What elements should be included when documenting severity levels in aviation risk assessment, and how do safety regulators evaluate the credibility of hazard effects?	When documenting severity levels, organizations should include tangible definitions for financial damage (e.g., Level 1: < $1,000), environmental damage (e.g., Level 1: Slight/no effect), personnel injury (e.g., Level 1: Slight/no injury), and operational impacts. For evaluating credibility of hazard effects, safety regulators consider multiple indicators including crew workload, exposure time to hazard, and aggravating factors. They also assess the effectiveness of mitigation measures (like TCAS for mid-air collision) and examine the total system's possibilities to affect outcomes when determining the most severe consequences.	['What Is a Risk Matrix\nA risk matrix is a tool you will use to assess safety concerns. The matrix allows you to rank risk and provide visual cues on safety acceptability.\nThe risk matrix is a standard tool for assessing safety issues in aviation SMS and is recommended by ICAO and other regulatory authorities for assessing issues.\nYou should develop your risk matrix at the onset of managing reported issues. It involves:\n- Defining criteria for likelihood and severity;\n- Defining acceptability; and\n- Documenting your risk matrix for educational purposes.\nDeveloping a risk matrix means both USING it and DOCUMENTING it to ensure consistency.\nRelated Aviation SMS Risk Matrix Articles\n- What Is a Risk Matrix and Risk Assessment in Aviation SMS\n- How to Define Your Risk Matrix in Aviation SMS\n- How to Define Severity and Likelihood Criteria on Your Risk Matrix\nProcess for Defining Acceptable Level of Safety\nAcceptable Level of Safety (ALoS) is the level of risk your company is willing to “take on” or “tolerate” for any given safety issue. How you define ALoS will have wide-ranging implications for how you manage safety issues.\n- Define the size and color scheme on your risk matrix, such as\n- 4x4 grid, 5x5 grid, or 6x6 grid (5x5 is standard)\n- Green, Yellow, Red scheme (this is standard color scheme)\n- Green, Yellow, Orange, Red scheme\n- Define and document each level of Severity on your risk matrix, such as\n- Financial damage that defines each level of severity\n- Human damage that defines each level of severity (i.e., minor injury, loss of life, multiple loss of life, etc.)\n- Operational damage that defines each level of severity\n- Define and document each level of Likelihood on your risk matrix, such as\n- Likelihood based on frequency of occurrence (i.e., 1/1000 operations)\n- Likelihood based on approximate chances of happening (i.e., “Has happened before in company,”)\n- Document what risk assessments on the matrix are considered “acceptable”\n- Provide further documentation guidance that department heads and other assessors can use the matrix.\nHow to Define Size of Risk Matrix\nThe size of your risk matrix is moderately important, but only so far as it allows you to:\n- Make your risk matrix easy to use; and\n- Be more detailed about each level of severity/likelihood.\nThe bigger your grid, the more specific each level of severity and likelihood can be.\nGenerally, most risk matrices are a 5x5 grid, though sometimes you may see organizations use:\n- 4x4 grid; and\n- 6x6 grid.\nA 5x5 grid is the size we generally recommend you use and is the size provided as an example by ICAO, but if you have a specific reason for making it smaller or bigger, you may do so.\nOn the risk matrix grid, you will have:\n- The vertical side be labeled with numbers for each level of Severity; and\n- The Horizontal side is labeled with letters for each level of Likelihood.\nThus, any grid is a composite of a letter and number, such as 2B, 5D, etc.\nHow to Document Each Level of Severity of Risk Matrix\nIt’s extremely important that you define the criteria for each level of severity on your matrix. These definitions of each level will provide guidance during risk assessments. You should provide tangible definitions of severity for each level in terms of:\n- How much financial damage is associated with each level;\n- Level 1: < $1,000\n- Level 2: $1,000 - $50,000\n- Level 3: $50,000 - $200,000\n- And so on.\n- How much environmental damage is associated with each level;\n- Level 1: Slight/no effect\n- Level 2: Minor effect\n- Level 3: Localized effect\n- And so on.\n- Personnel injury\n- Level 1: Slight/no injury\n- Level 2: Minor Injury, no hospitalization\n- Level 3: Major injury, hospitalization\n- And so on\n- No effect\n- Minor degradation\n- Single significant mission failure element\n- And so on.\nYou may have an additional qualifying element you wish to define for Severity that is relevant to your organization.\nRelated Aviation SMS Risk Assessments Articles\n- How to Justify Severity of Risk Assessments\n- 5 Questions to Ask before Making Risk Assessment\n- 4 Pillars | What Is Risk Assessment in Aviation SMS\nHow to Document Each Level of Likelihood of Risk Matrix\nYou also need to define the criteria for each level of likelihood. Generally, aviation service providers define criteria for likelihood by either:\n- Number of expected occurrences per a given number of operations; or\n- Relative chances.\nFor the number of expected occurrences, you might define it as:\n- E (low likelihood): 1/20,000 operations;\n- D (medium-low likelihood) 1/5,000 operations;\n- C (medium likelihood): 1/1,000 operations;\n- And so on.\nThis can be a good method if you have significant historical data to analyze occurrences per number of operations for specific hazards.\nWe recommend that most providers use a relative likelihood criteria, as it’s fairly straightforward and convenient for SMS of all ages and sizes. You might define these criteria as:\n- A (low likelihood): Has happened in the industry, but very rare;\n- B (medium-low likelihood): Has happened in the company in the last 5 years;\n- C (medium likelihood): Has happened in the company in the last 2 years;\n- D (medium-high likelihood): Has happened in the company in the last 9 months; and\n- E (high likelihood): Has happened in the company in the last 3 months.\nThese are just loose examples, and you should define your chances based on the size, complexity, and number of operations in your company.\nNOTE: you can set level A as your high likelihood or low likelihood, as there is no set direction your matrix needs to go.\nHow to Choose Color Scheme of Risk Matrix\nYou need to choose a color scheme for your risk matrix. This color scheme will give visual clues for which assessments are “acceptable” and which aren’t. Your colors will be assigned to each grid in a diagonal pattern, as you can see on the image to the right.\nThe standard color scheme is:\n- Green (acceptable);\n- Yellow (moderately acceptable, but may require further mitigation); and\n- Red (unacceptable, emergency situation).\nThis being said, some organizations adopt a more specific matrix, usually being:\n- Green (acceptable, no periodic review required);\n- Yellow (moderately acceptable, requiring periodic review of issue); and\n- Orange (unacceptable); and\n- Red (emergency situation).\nThe color scheme matters because it affects how issues assigned with that risk level will be managed:\n- “Acceptable” risks require no further action on the part of management to mitigate safety;\n- “Unacceptable” risks require actions (CPAs) to mitigate risk.\nHow to Define Acceptable Level of Safety\nDefining an acceptable level of safety is simply documenting which severity/likelihood composites on your risk matrix are acceptable, and which aren’t acceptable. Documenting this involves:\n- Filling in your color scheme to reflect acceptability and unacceptability; and\n- Actually writing out in plain terms which color(s) are and aren’t acceptable.\nRelated Articles about Acceptable Level of Safety\n- What Is Acceptable Level of Safety\n- How to Define Acceptable Level of Safety (ALoS) in Aviation SMS\n- 3 Levels of Aviation SMS Operational Excellence\nProviding Further Resources for the Risk Matrix\nFinally, you should provide some further resources on how to use the risk matrix. This can simply be:\n- Bullet points steps for using the risk matrix;\n- Further considerations for using the risk matrix; or\n- Which colors require which actions.\nThese types of further guidance should be documented in your Safety Policy so that new employees or department heads can consult it when they have questions about performing a risk assessment.\nLast updated January 2023.', 'If you wish to contribute or participate in the discussions about articles you are invited to join SKYbrary as a registered user\nRisk Assessment is an evaluation based on engineering and operational judgement and/or analysis methods in order to establish whether the achieved or perceived risk is acceptable or tolerable.\nRisk is the assessed potential for adverse consequences resulting from a hazard. It is the likelihood that the hazard’s potential to cause harm will be realised. (ICAO Doc 9859)\nRisk shall mean the combination of the overall probability, or frequency of occurrence of a harmful effect induced by a hazard and the severity of that effect. (European Commission Regulation 1035/2011)\nRisk assessment is performed to determine the magnitude of risk and to establish whether measures are needed to contain it within defined limits. Risk assessment does not represent an end in itself, but should contribute to controlling risks to an acceptable or tolerable level.\nAmendments to several Annexes to the Chicago Convention applicable since November 2009 introduced harmonised requirements for the implementation of Safety Management Systems (SMS) by aviation service providers. Aircraft operators and other aviation service provider organisations should establish and apply a formal risk management process within the framework of the organisational SMS. Risk management shall ensure that risks are systematically analysed (in terms of probability of occurrence and severity of hazard effects), assessed (in terms of tolerability) and controlled to an acceptable level (by implementation of mitigation measures).\nAircraft operators and aviation service providers shall also define those levels of management with authority to make decisions regarding safety risks tolerability.\nRisk Assessment is the second step in the risk management process. Once hazards and their effects have been determined during the first step by means of hazard identification, an analysis is required to assess the probability of the hazard effects occurring and the severity of these effects on aircraft operation. ICAO Doc 9859 - Safety Management Manual highlights the importance of distinguishing between hazards (the potential to cause harm) and risk (the likelihood of that harm being realised during a specified amount of risk exposure).\nRisk assessment is based on the evaluation of the following criteria: the severity of a hazard, the probability (frequency) of its occurrence and tolerability of its effects.\nSeverity of Hazards\nThe ultimate criterion used to assess the severity of hazards is the impact on the safety of an aircraft and its occupants and other persons who may be directly affected. Elements to be considered in the severity assessment would include a number of indicators, such as crew workload, exposure time to the hazard, aggravating factors etc. Another group factors to be taken into account are the means of mitigation that are considered acceptable by the safety regulator, for example the effective use of TCAS as mitigation means for mid-air collision hazard.\nThe severity of hazards will be determined by the credible effects on the safety of aircraft, when the outcome of all the weaknesses, potential failures and safeguards (barriers) which may exist in the relevant operational environment have been taken into consideration. For example, the most severe effect (consequence) will only be chosen in such cases when the total system has exhausted its possibilities to affect what continues to happen and only chance determines the outcome, for example the ingestion by aircraft engines of birds greater than they are designed and certificated to withstand and continue functioning where this occurs simultaneously to more than one engine.\nA credible assessment of the severity of hazard effects requires detailed knowledge of the environment of operations and the services (functions) to be performed.\nAn example of hazard severity classification matrix is provided in the related article on hazard identification.\nProbability of Occurrence\nThe estimation of the probability of a hazard occurring (or in other words the interval of exposure in which a hazard effect may manifest itself) is usually achieved by means of structured review using a standard classification scheme.\nIn some cases, data may be available that will allow the making of direct numerical estimate of the probability of occurrence. This is usually the case when estimating the probability of failure of hardware components of a system. Extensive data are often available on historical component failure rates.\nHowever, the estimation of the probability of occurrence of hazards (and their effects) which are associated with human error is not straightforward. Unless there is a very high capture rate of relevant occurrence data which has been appropriately stratified, it may be difficult to find meaningful empirical data and subjective assessment will then be all that is possible. As with the estimation of the`severity of a hazard, the development of informed judgments from a structured review by people with extensive experience in their respective fields applied to a standard classification scheme will be the best substitute for absolute values.\nThe probability classification scheme shown below is extracted from ICAO Doc 9859 - Safety Management Manual. It specifies the probability as qualitative categories, but also includes numerical values for the probabilities associated with each category.\nBoth probability of occurrence of a hazard effect and the severity potential of that effect, need to be taken into account when deciding on the tolerability (acceptability) of a risk. It is a common practice to use a risk classification matrix in support of this two-dimensional judgement.\nAn example of a risk classification matrix used in ATS is provided below. It has been extracted from ICAO Doc 9859 - Safety Management Manual. Severity is ranked as Catastrophic, Hazardous, Major or Minor, with a descriptor for each indicating the potential severity of consequences. Probability of occurrence is ranked through five different levels of qualitative definitions, and descriptors are provided for each probability of occurrence.\nNumerical values may be assigned in order to weigh the relative importance of each level of severity and probability. A composite assessment of risk, to assist in comparing risks, may then be derived by multiplying the severity and probability values.\nDepending on the approaches and methodologies used, risk can be expressed in various ways, for example:\n- Number of fatalities for a period of time;\n- Loss rates (e.g. number of fatal accidents per kilometres or miles flown/flight hours flown etc.);\n- Probability of serious accidents in certain time span or per flight hours flown;\n- Expected value of losses versus annual operating revenue;\nThroughout the aviation industry, many different versions of risk assessment matrices are available. Some definitions and categorisations vary, but the general concept remains the same. Examples of Risk Assessment and Mitigation in ATM from EUROCONTROL and Predictive Risk Matrix used by FAA for airline operations can be viewed here.\nThe output from risk classification is used to determine the risks the organisation should act upon. Decision making will require clearly defined criteria about acceptable or tolerable risk and unacceptable risk (see “Acceptable Level of Safety” in Safety Planning article). The assessment of tolerability (acceptability) is critical in making rational decisions to allocate the limited organisational resources against those risks posing greatest threats and this process often may require a cost-benefit analysis. ICAO explains the process of defining risk tolerability by the following:\n“Having used a risk matrix to assign values to risks, a range of values may be assigned in order to categorise risks as acceptable, undesirable or unacceptable. These terms are explained below:\n- Acceptable means that no further action needs to be taken (unless the risk can be reduced further at little cost or effort);\n- Undesirable (or tolerable) means that the affected persons are prepared to live with the risk in order to have certain benefits, in the understanding that the risk is being mitigated as best as possible;\n- Unacceptable means that operations under the current conditions must cease until the risk is reduced to at least the tolerable level.”\nVarious strategies and approaches can be used by aircraft operators and aviation service providers in order to reduce the unacceptable risks to tolerable levels. This third and very important step of risk management is discussed further in the Risk Mitigation article.\nQuantitative and Qualitative Methods for Risk Assessment\nAccording ICAO Doc 9859 - Safety Management Manual, there are many options - formal and less formal - to approach the analytical aspects of risk assessment. For some risks, the number of variables and the availability of both suitable data and mathematical models may lead to credible results with quantitative methods (requiring mathematical analysis of specific data). However, ICAO states that few hazards in aviation lend themselves to credible analysis solely through quantitative methods. Typically, these analyses are supplemented qualitatively through critical and logical analysis of the known facts and their relationships.\nFederal Aviation Administration in Advisory Circular 150/5200-37 (Introduction to SMS for Airport Operators), suggests that determination of severity should be independent of the probability of occurrence, and vice versa, the probability of occurrence should not be considered when determining severity. Over time, quantitative data may support or alter the determinations of severity and probability, but the initial risk determinations will most likely be qualitative in nature, based on experience and judgment more than factual data.\n- ICAO Doc 9859 - Safety Management Manual, Third Edition - 2013;\n- ICAO Integrated Safety Management website;\n- Safety Regulatory Requirement - ESARR 4, Risk Assessment and Mitigation in ATM;\n- Draft Advisory Material for the Establishment of a Risk Classification Scheme for the Design of the ATM Functional System;\n- FAA/EUROCONTROL ATM Safety Techniques and Toolbox;\n- FAA Advisory Circular No.120-92, Introduction of Safety Management Systems for Air Operations;\n- FAA Advisory Circular No.150/5200-37, Introduction of Safety Management Systems (SMS) for Airport Operators.']	['<urn:uuid:02fa3022-e096-4b62-8a21-b1ea45801914>', '<urn:uuid:fe984493-753c-4f3e-8ce1-e6d1e9f5953f>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T00:20:10.754925	24	83	2897
38	How do different types of Doppler radar systems detect motion?	Different Doppler radar systems use distinct techniques to detect motion. True Doppler radar, used in police radar guns, sends out electromagnetic radiation at a precise frequency and measures the frequency of returning waves to calculate velocity. Pulse-Doppler radar, used in meteorology, sends pulses of radiation that allow for radial velocity calculations and can be sent with varying frequency. The WSR-88D system specifically detects two types of cloud motions - it calculates both the storm's speed and direction, while also detecting internal storm motions. All these systems work based on the Doppler effect, where the frequency changes relative to the observer's motion, with higher frequency during approach and lower during recession.	"[""Doppler radars help us get ready in the morning. They tell us whether to pack an umbrella or leave the coat at home. And they help us navigate conversations with anyone — because we all have a strange fascination with weather. But Doppler radars also used for more than just weather tracking — doppler radars can be found in aviation, police speed guns, radiology, health care and missile systems.\nHow does this invisible phenomenon work? In short, Doppler radars send beams of electromagnetic signals toward a desired target. The signals use sound reflection to analyze how the returned signal's frequency has been altered. But there's more to it.\nThe Doppler Effect\nThe Doppler effect (or shift) is named after the Austrian physicist Christian Doppler, who in 1842 proposed that a wave's frequency changes for an observer moving close to its source. The frequency is higher during its initial transmission, identical at the moment of passing, and lower during the recession.\nIt's commonly experienced when a car siren or horn passes, which is why the sound is always higher when the vehicle is approaching than when it is receding. With the naked ear, you can get a general idea of how far and fast the vehicle is traveling.\nSince light waves have no medium, we use the Doppler effect in terms of the motion of the source relative to its observer (the radar). Obviously, this requires a much more complex and calculated formula than what's audible to the human ear.\nTrue Doppler radar, in its simplest form, is found in police officer's radar guns, which determine the speed of a motor vehicle. The electromagnetic radiation is sent out at a precise frequency, then the radar uses the frequency of the wave on the return path to calculate a targeted object's velocity.\nHowever, one of the most common known examples of Doppler use, meteorology, uses a different technique called Pulse-Doppler radar.\nGIF courtesy of Tumblr, accuweather\nThis sends the beam in a swirling, circular motion, which we often see on TV, shown in the GIF above. When a beam strikes a raindrop, hailstone or other precipitation, the doppler can calculate the distance, intensity and direction from the reflected beam. This radar can be so sensitive that it can track wind-blown particles and bird migration.\nWhile True Doppler sends a direct beam, Pulse-Doppler sends pulses of radiation that allow for calculations of radial velocity. It can be sent out with varying frequency and carrier cycles, using the formula below to track.\n33 Stunning Weather Photos From Readers Around the World\nThe U.S. Navy used continuous-broadcast (or FM) radar during World War II so that aircrafts could perform night combat operation, such as approaching target ships and training guns on enemy aircraft. They were also used as navigation aids, helping pilots accurately determine wind speed.\nThese early Dopplers relied on analog filters to calculate velocity, which were riddled with inaccuracies, due to the extra weight. It wasn't until the 1970s, when modern microprocessors were invented, that Doppler radar became more practical for weather and air traffic control.\nToday, this technology is so advanced that we can determine weather complications weeks in advance. So next time you're stuck at the airport due to adverse weather when the skies appear to be clear and sunny, you can thank your nearest Doppler radar.\nWhy the Weatherman Is Wrong\nSo if Doppler is right, why is the weatherman so often wrong? For one thing, he's not always wrong, we just remember the times when he is (see also: Snowquester). The other thing to keep in mind is that the weather is very dynamic and constantly changing. There's a lot of data in weather, and forecasts have actually gotten far better over the years, though they're clearly not perfect. Newer prediction models are incorporating these massive amounts of data to help meteorologists understand dynamic weather patterns, which will lead to more accurate forecasts to help us get dressed for day — and know whether to grab the umbrella.\nImage courtesy of iStockphoto, Anthro"", 'NOAA TECHNOLOGY TAKES WEATHER DETECTION TO NEW HEIGHTS SINCE PALM SUNDAY TORNADO OUTBREAK\nAs the April 11, 1965 , Palm Sunday tornado outbreak devastated\nparts of four midwestern states, the NOAA Weather Service (NWS)\nforecasters tracked storms with surveillance equipment that today\nwould be termed rudimentary at best.\nAdvances in electronics and related technologies over the ensuing\nyears have proven to be a boon for agency forecasters in their\nefforts to save lives and prevent property damage. The rise of\nDoppler radar and weather satellites opened the doors for the massive\namounts of data deciphered by today’s forecasters to give\ntimely and accurate forecasts and warnings.\nRadar Technology and Weather Service Modernization\nWeather Service radar operators on April 11, 1965, could see\nonly green blobs on their radar scopes and were required to have\nvisual confirmation from the ground to issue tornado “alerts.” Radars\nwere scattered across the country, with large areas not served\nat all, in a combination of ""network"" and ""local\nwarning"" radars. Operators were able to tell where a storm\nsystem was located, but only the most experienced could provide\nmuch detail about a given storm.\nThe multi-billion dollar NWS modernization of the 1990s brought\nstate-of-the-art technologies to both radar and satellite programs.\nDoppler radar and weather satellites in geosynchronous orbit of\nthe Earth provided forecasters with the tools needed to take weather\nwarning capabilities to new levels.\nAs the 1950s radars (as well as some upgraded in the 1970s) approached\nobsolescence, the NWS joined the Department of Defense and the\nDepartment of Transportation (specifically the Federal Aviation\nAdministration) to create design parameters for a new generation\nof Doppler weather radars that are able to see inside storm systems\nand better track their motion. The Weather Surveillance Radar 1988\nDoppler (WSR-88D) was designed specifically to monitor weather\nsystems and tie into modern computer systems that turned technological\ndreams of the 1960s into routine surveillance in the 21st Century.\nEmploying the Doppler effect to track frequencies of winds moving\ntoward the radar and winds moving away from the radar, the WSR-88D\nobserves the presence and calculates the speed and direction of\nmotion of severe weather elements such as tornadoes and thunderstorms.\nThe WSR-88D provides quantitative area precipitation measurements,\nimportant in hydrologic forecasting of potential flooding. The\nsevere weather and motion detection capabilities contribute to\nimproved accuracy and timeliness of NWS warnings. The technology\nhas helped the NWS increase tornado warning lead times; improve\nthe detection and measurement of damaging winds, severe turbulence,\nwind shear and hail associated with severe thunderstorms; improve\nforecasts of the location and severity of thunderstorms; and improve\nthe accuracy in identifying threatened areas and substantially\nreduced the number of false alarms.\nRadar detects the presence and location of an object by bouncing\nan electromagnetic signal off of it and measuring the time it takes\nfor the signal to return. This measurement is used to determine\nthe distance and direction of the object from the radar, such as\nparticles of water, ice or dust in the atmosphere. Radar signals\nreflected from a moving object undergo a change in frequency related\nto the speed of the object traveling toward or away from the radar\nantenna. The return signal is different for objects\nmoving away from the radar and objects moving toward the radar.\nThe WSR-88D detects two motions associated with clouds. The radar\ncalculates both the speed and direction of motion of a severe storm.\nIt also detects internal motions of the storm and certain unique\ninternal motions can be a precursor of tornado formation. For example,\na developing tornado can be detected forming above the Earth before\nit reaches the ground. This means earlier detection of the precursors\nto tornadoes, as well as data on the direction and speed of tornadoes\nonce they form.\nAs part of the 1990s modernization, the NWS, the Department of\nDefense and the FAA deployed more than 160 radars across the country.\nThe integrated network of radars with overlapping coverage of the\nentire United States and its island territories from Guam to Puerto\nRico dramatically enhanced the NWS’ ability to safeguard\nlife, property and commerce.\nSatellites Boost Weather Surveillance\nModern-day weather satellites play their own important role in\ntracking severe weather. In 1965, the first satellite created for\nweather surveillance was still two years from reality. Those early\nsatellites bore little resemblance to the geostationary satellites\nthat provide images seen daily on television weather casts across\nthe country today.\nThe first weather satellite, designated the Applications Technology\nSatellite 3 (ATS 3), was launched Nov. 5, 1967. The ATS 3 was\na set of six NASA spacecraft created to explore and flight-test\nnew technologies and techniques for communications, meteorological\nand navigation satellites. The major objective of the early ATS\nsatellites was to test whether gravity would anchor the satellite\nin a synchronous orbit (22,300 miles above the earth), allowing\nit to move at the same rate the Earth turns, thus seeming to remain\nstationary. The tested satellites also collected and transmitted\nmeteorological data and functioned, at times, as communications\nsatellites to the Pacific Basin and Antarctica . ATS satellites\nprovided the first color images from space as well as regular cloud\ncover images for meteorological studies.\nSynchronous Meteorological Satellites\nTo provide improved meteorological data on worldwide weather\nphenomena for improved forecasting, NASA launched two Synchronous\nMeteorological Satellites (SMS): SMS 1 was launched May 17, 1974,\nand SMS 2 was launched Feb. 6, 1975. After the SMS 2 launch, NASA\nturned over the geostationary satellite program to NOAA for operation.\nNOAA acquired additional spacecraft identical to SMS and gave them\nthe new name Geostationary Operational Environmental Satellite\n(GOES). The SMS series included the first operational satellite\nin the NOAA system.\nGeostationary Operational Environmental Satellites (GOES)\nBuilt by Philco-Ford, the early GOES satellites were spin-stabilized\n(the spinning of the satellite kept it in a stable orientation\nwith Earth), which meant they viewed the Earth only about ten percent\nof the time. NOAA operated these satellites from 1975 until 1994,\nwhen NOAA introduced a new generation of three-axis stabilized\nspacecraft labeled GOES I-M. GOES satellites, currently in operation,\nview the Earth 100 percent of the time and provide round-the-clock\nsurveillance of the continental United States and coastal waters.\nThe United States operates two meteorological satellites in geostationary\norbit about 22,300 miles above the Earth; one over the East Coast\nand one over the West Coast. NOAA’s National Environmental\nSatellite, Data and Information Service (NESDIS) operates the GOES\nseries. New GOES satellites will be launched as required to keep\nthe system operational. Space Systems/Loral (formerly Ford Aerospace)\nis the GOES I-M development contractor.\nGOES satellites provide data for severe storm evaluation and\ninformation on cloud cover, winds, ocean currents, fog distribution,\nstorm circulation and snow melt, using both visual and infrared\nimagery. They also receive transmissions from free-floating balloons,\nbuoys and remote automatic data collection stations around the\nworld. The imagery is also used to estimate rainfall during thunderstorms\nand hurricanes to help issue flash flood and flood warnings.\nBecause they are above a fixed point, the satellites provide\na constant vigil for atmospheric triggers for severe weather conditions\nsuch as tornadoes, hail storms, hurricanes and flash floods. When\nthese conditions develop, the satellites monitor storms and track\nPolar Orbiting Satellites\nNOAA weather and climate forecasters have a second set of space-based\ntools for monitoring severe weather and the atmosphere: the Polar\nOperational Environmental Satellite (POES). While GOES craft maintain\na synchronous orbit to stay in one location above the Earth, POES\ncraft circle the Earth in a sun-synchronous orbit (at an altitude\nof 450 nautical miles), in an almost north-south orbit, passing\nclose to both poles. One POES crosses the equator in early morning\nand the other crosses in early afternoon. Operating as a team,\nthese satellites ensure data for any region of the Earth are no\nmore than six hours old.\nPOES spacecraft are used primarily for long-range weather and\nclimate forecasting as well as search and rescue operations initiated\nby distress beacon signals. They also provide forecasters with\ninformation on cloud cover, storm location, temperature and heat\nbalance in the Earth’s atmosphere.\nPat Slattery, NOAA National Weather Service: (816) 891-8914']"	['<urn:uuid:3955ae10-2af6-4e37-bd01-7031752d7275>', '<urn:uuid:97294e7e-c67e-4c9e-abae-e4ec25afa9b8>']	open-ended	direct	concise-and-natural	similar-to-document	three-doc	expert	2025-05-13T00:20:10.754925	10	110	2032
39	How do MRI and anatomical markers analyze motion, and what are their key limitations?	MRI provides non-invasive 3D imaging of joint bony structures and surrounding soft tissues, allowing analysis of true joint motion. However, MRI imaging has limitations - it can only capture static postures and requires subjects to lie down, preventing analysis of natural motion patterns. Anatomical markers using tantalum implants in bone tissue can show real bone movements, but this technique is limited to post-operative conditions with synthetic prostheses, carries radiation exposure risks from X-rays, and like MRI, only allows analysis when patients are positioned horizontally rather than during normal activities.	['The skin marker method has been widely applied to investigate cervical kinematics. This approach provides a non-invasive means to understand normal and abnormal cervical motion. Soft tissue artifacts (STA) has been regarded as the most critical source of error in human movement analysis (Leardini et al., 2005); it has been widely investigated in the lower limb (Akbarshahi et al., 2010; Gao & Zheng, 2008; Ryu, Choi & Chung, 2009) and upper limb (Cutti et al., 2005).\nVarious invasive techniques have been used to investigate the STA of human body parts, including intracortical pins (Reinschmidt et al., 1997), external fixators (Cappozzo et al., 1996), and percutaneous trackers (Manal et al., 2000). These invasive methods are suitable for patients who require surgery and those who volunteer as research subjects. Medical techniques have been widely used in cervical disease diagnosis and motion analysis, as well as in the validation of surface marker placement accuracy (Gadotti & Magee, 2013). Compared with Roentgen photogrammetry (Tashman & Anderst, 2002) and video fluoroscopy (Wu et al., 2007), magnetic resonance imaging (MRI) (Ishii et al., 2006; Nagamoto et al., 2011) has the advantage of involving less radiation. In recent decades, MRI application has expanded from clinical imaging to biomechanical kinematic analyses because of the technique’s low radiation (Karhu et al., 1999). Although the kinematic characteristics of the spinal system are complex, STA investigation during spinal motion has been conducted previously and reported as follows.\nMörl & Blickhan (2006) revealed the linear relationship between the 3D movement of skin markers and the corresponding lumbar vertebrae in different postures using open MRI. Hashemirad et al. (2013) confirmed the validity and reliability of the surface skin marker method aided with fluoroscopy in measuring intersegmental mobility at L2–L3 and L3–L4 during lateral bending. For the cervical spine, Wu et al. (2007) explored the rotational angles of C2, C3, C5, and C7 levels by skin marker method aided with fluoroscopy during flexion/extension and lateral bending. The study demonstrated the feasibility of the skin marker method in obtaining cervical segmental rotational angles. Moreover, MRI technology is a desirable method because it allows the non-invasive 3D imaging of true joint bony structures and the surrounding soft tissues (Sangeux et al., 2006).\nIn this study, the cervical skin movement of young normal adults in flexion and lateral bending is preliminarily measured by the 3D MRI technique. Spatial displacement of skin markers and corresponding vertebral landmarks are obtained, including the direction and magnitude of relative movement.\nMaterials and Methods\nThree male volunteers participated in the study. The mean (standard deviation) age, body mass, and height were 24 (1) years old, 64 (2) kg, and 170 (5) cm, respectively. All subjects provided informed consent prior to testing and have no history of cervical trauma, bone pathology, and arthritic or other inflammatory disorders. All the study designs and consent forms were approved by the Institutional Ethical Review Board Committee of Jilin University, Changchun, China (No. 20130629).\nThree hemisphere-shaped MRI markers with a diameter of 5 mm were attached to the nape of each subject. Each marker was applied to mark the spinous processes of the second (C2), fifth (C5), and sixth cervical vertebrae (C6), as palpated in a flexed posture.\nEach subject was instructed to lie down on his back and then assume different postures during scanning, including 3 positions in flexion and 5 positions in lateral bending (Fig. 1). In this work, the maximum ranges of motion in flexion and lateral bending lied within the average ranges of the young people as reported by Jaejin & Myung (2015). The motion angles approximated to the intermediate positions during each motion patterns were selected as follows, respectively.\nThe neutral position (Fig. 1A) and two other positions in which the neck was flexed at angles 18°(Fig. 1B) and 42°(Fig. 1C) were determined using a protractor with respect to a vertically hanging object. The positions were fixed by placing polyurethane plates under the subject’s head. The neutral position (Fig. 1F) and four other positions in which the neck was extended at different angles were determined by a round-shaped board with 5°scales. The four positions were as follows: left bending at 15°(LB15°; Fig. 1E), left bending at 30°(LB30°; Fig. 1D), right bending at 15°(RB15°; Fig. 1G), and right bending at 30°(RB30°; Fig. 1H).\nThe two series of cervical postures, including flexion and lateral bending, were imaged using a 1.5T commercial MR system (Signa CV, General Electric, United Kingdom) along with a torso phased array coil. A 3D fast gradient recalled acquisition in the steady state pulse sequence was used with repetition time/echo time of 3.9 ms/1.8 ms. The slice thickness was 1.8 mm with no interslice gap. The flip angle was 15°with a 24 cm field-of-view and a 256 × 224 in-plane acquisition matrix. Imaging of one position lasted for approximately 3.5 min. The acquisition data were saved in Dicom format and processed by the Mimics software (V10.01; Materialise, Leuven Belgium). Each skin marker was then digitized using the point at the marker center. Meanwhile, the positional information of the tip of each spinous process was digitized in three anatomic planes. The position relationships between skin markers and corresponding spinous processes were clearly shown in the sagittal plane, the ends of short blue lines were pointed at the skin markers and corresponding vertebral landmarks, respectively (Fig. 2). The global reference frame was defined as follows. The +Z axis pointed upward and positioned parallel to the field of gravity, whereas the +X and +Y axes lied in a plane perpendicular to the Z axis and pointed toward the anterior and left lateral directions, respectively.\nThe special positions of each spinous process and corresponding skin surface marker at each observed posture were measured. The following equations were used to determine the spatial 3D (X−i, Y−j, Z−k) skin displacement of each cervical level at each different posture: (1)\nThe skin displacements were calculated by subtracting the displacement of the skin marker from the corresponding displacement of the vertebral landmarks. In the abovementioned equations, , , and represents the vertebral landmark displacements in the X, Y, and Z directions, respectively. Similarly, , , and represents the skin marker displacements in the X, Y, and Z directions, respectively. The absolute value of Si,j,k denotes the amplitude of the skin movement. The sign of Si,j,k represents the direction of the relative movement between vertebral landmarks and the corresponding skin markers. In particular, +Si implies that the vertebral landmarks moved relative to the corresponding skin markers along the +X direction.\nThe skin deformation of three cervical levels during flexion and lateral bending were investigated, and the results are shown in Tables 1 and 2. The tables display the amplitude and directional information of cervical skin deformation, from which the linear relationship between the motion of vertebral landmarks and the corresponding skin markers was deduced.\n|Rotations||Vertebrae||S (mm)||Range of |S| (mm)|\n|Rotations||Vertebrae||S (mm)||Range of |S| (mm)|\nAmplitude and directional analysis of skin deformation\nTable 1 lists the skin deformation amplitudes of the three levels (C2, C5, and C6) in the two neck flexion postures in three directions. Data show that the range of skin shift amplitude was 0–12.5 mm in the X direction, 0–13.5 mm in the Y direction, and 0–13.5 mm in the Z direction. Meanwhile, Table 2 lists the skin shift amplitudes of the three cervical levels at four postures in lateral bending. Results show that the range of skin shift amplitude was 0–21 mm in the X direction, 0.5–20.5 mm in the Y direction, and 0.5–16 mm in the Z direction. During flexion, the maximum skin displacements in both X and Y directions occurred at the C2 level, giving values of 8.5 and 7.2 mm, respectively. By contrast, the maximum skin displacement in the Z direction (up to 9 mm) occurred at C6. During lateral bending, the maximum skin displacements in the X, Y, and Z directions were noted on at the C2 level, with values of 14.5, 20.5 and 7.2 mm, respectively.\nFigures 3 and 4 reveal the motion of vertebral landmarks and the corresponding skin markers in flexion and lateral bending, respectively. The initial motion of vertebral landmarks and skin markers in the neutral position was eliminated to analyze the influence of skin deformation on the different cervical motion periods. The results demonstrated that the direction of movement of most of the skin markers in flexion could estimate the vertebral landmarks. In lateral bending, the motion directions of the skin markers of the upper cervical spine could estimate those of the corresponding vertebral landmarks both in the X and Y directions, as well as that of the lower cervical spine in the Y direction. Moreover, the motion directions of the skin markers of the lower cervical spine, as well as those of all cervical levels, were opposite those of the corresponding vertebral landmarks in the X and Z directions, respectively. These results imply that the skin movement characteristics differ between the upper and lower cervical levels, as embodied in the motion directional and amplitude differences. The discrepancies can be explained by the skin deformation mechanism caused by the complex cervical muscle deformation.\nFor the amplitude information during flexion, Fig. 3A shows that the skin displacement at the C5 level was larger than those of the other two levels in the X direction during the entire movement process. Meanwhile, Fig. 3B reveals that the maximum skin displacement in the Y direction occurred at the C2 level, followed by the C5, and then C6 levels. Fig. 3C shows that the maximum skin displacement in the Z direction occurred at the C6 level, then at the C5 and C2 levels.\nDuring lateral bending, Fig. 4A shows that the maximum skin deformation in the X direction occurred at the C5 level and then the C6 and C2 levels. Meanwhile, the skin deformation presented symmetry in this direction. Fig. 4B displays that the maximum skin deformation in the Y direction occurred at the C6 level during left bending, whereas the maximum skin deformation occurred at the C5 level during right bending. Fig. 4C reveals that the maximum skin deformation in the Z direction occurred at the C2 level, followed by C6, and then C5 levels. The skin deformation was also symmetrical in this motion direction.\nLinear relationship analysis of the motions of the skin markers and vertebral landmarks\nTo quantify the relative movement relationship between skin markers and the corresponding vertebral landmarks, regression equations were analyzed. All data obtained above were included. Correlation coefficients were adopted as tools to determine the degree of correlation between two variables, particularly, the displacements of skin marker T andvertebral landmark Y. The linear correlations between these displacements were calculated. The simple linear regression equation was employed in this work (Mörl & Blickhan, 2006), as follows: (2) The vertebral landmark displacement Y can be calculated using the superior skin marker displacement data T, estimated regression coefficient b, and intercept a. The change in postures led to a variation in marker and vertebral displacements. The Pearson’s correlation coefficients of the magnitudes of these displacements were calculated (0.07 < R < 0.97).\nThe linear relationships among the motions of the skin markers and vertebral landmarks in flexion and lateral bending are displayed in Fig. 5. For flexion, the correlation coefficients of the displacements of the vertebral landmarks and corresponding skin markers in the X direction were 0.95. This value was higher than those in the Y and Z directions at 0.77 and 0.30, respectively. For lateral bending, the correlation coefficient was 0.94 in the Y direction, corresponding to a primary direction. Meanwhile, the correlation coefficients were low in both the X and Z directions.\nThis work aimed to qualitatively and quantitatively investigate the cervical skin deformation at both upper and lower cervical levels. Three postures during flexion and five postures during lateral bending were involved. Furthermore, the linear correlation coefficients between the displacements of the skin markers and corresponding vertebral landmarks were investigated.\nThe motion direction of skin markers could correctly estimate that of the corresponding vertebral landmarks in flexion. In lateral bending, the motion directions of the skin markers could estimate the corresponding vertebral landmarks in the Y direction. By contrast, discrepancies were noted in the X and Z directions. The skin markers estimated the motion direction of the vertebral landmarks in the primary motion directions for the two motion patterns. However, in other directions outside of the primary motion plane, prudence was needed when skin markers were used to obtain the vertebral landmark motion. Moreover, the displacements of skin deformation observed in the study were slightly larger than those provided by Wu et al. (2007). With the aid of fluoroscopy, Wu et al. (2007) conducted measurements, in which the skin displacement might have been offset during the movement process. In the present study, no rectification was applied despite that the entire measurement was performed using the MRI technique. The natural motion patterns of cervical spine were performed although the images were obtained in static postures. The results showed that the amplitude of skin deformation increased with increasing cervical motion range. Larger ranges of cervical motion produced higher skin deformation amplitudes, which might be attributed to the greater participation of neck muscles. Moreover, in flexion, the correlation coefficients between the motions of skin markers and corresponding vertebral landmarks were high (R > 0.77) in the X and Y directions, but were poor in the Z direction. In lateral bending, the correlation coefficient was high (R > 0.94) in the Y direction. In the primary motion planes, the motion amplitude of vertebral landmarks can be estimated by the skin markers through linear regression equations both in flexion and lateral bending.\nThe displacement compensation approach has been used by Ryu, Choi & Chung (2009) to significantly reduce errors in keen kinematic variables by 25%–60%. Similarly, the displacement investigation of cervical skin deformation could provide a reference for the compensation of cervical angular information. Opinions on the biodiversity of different people prevail (Leardini et al., 2005). However, Gao & Zheng (2008) recently verified the inter-subject similarity of soft tissue deformation during level walking between two subjects. The sample size involved in the study potentially influenced the result. Such findings verified the prevailing opinion that skin deformation presented biovariability. However, the small sample size probably decreased the variability and increased the inter-subject similarity. Therefore, future studies involving larger sample sizes should be performed to completely verify the existing hypothesis on human cervical skin deformation. Objective subject information, such as age, sex, height, and gravity, may have affected the result. In the technical perspective, the limitation of this study may lie in the use of special self-made positional approach, the limited choice of landmarks, and the limited position of subjects. Further study needs to include completed vertebral landmarks, and more positions should be researched as well. However, additional information on study subjects should also be provided to accurately achieve a deeper understanding of the skin deformation mechanism. Moreover, with the widespread use of the cervical finite element model, the skin deformation results obtained in the study can be adopted to assist in the in vivo subject-specific validation of the finite element model of the cervical spine.\nA non-invasive MRI technique was implemented to investigate the skin deformation of several cervical segments in flexion and lateral bending. The amplitude and directional information on skin deformation in three directions was investigated. Results showed that the motion patterns of cervical skin deformation presented biovariability. This work offered reference for the further investigation of the cervical skin deformation mechanism.', 'Kinematic assay of knee joints alludes to the designation of tibiofemoral bonds conduct due to the feasible spans of motion for knee joint arrangements. This form of inquisition principally demands accurate estimation of the movement of part bones in the knee joint and helps to understand physiological and injured joint paths, which ensure the efficient exploitation of rehabilitation joint components.\nAnatomical and Tracking Markers in Defining Knee Kinematics\nKnee joint kinematics is a complex system which includes trajectories of flexion, extension as well as internal and external rotation (Bonnin 2012).\nThe use of anatomical markers provides implant performance and implant bearing under the Roentgen Stereo Analysis. This ingress involves implanting tantalum landmarks in the osseous tissue and then capturing roentgenogram. Radiogram is performed through the joint region in two different slopes with a purpose to estimate the three-dimensional movement volume (Weidow 2006).\nHire our qualified writers!\nNot enough time to create an assignment by yourself?Order now\n- on time delivery\n- original content\n- quality writing\nThe computer tomography dissection provides a detailed static image of the structural bones in the knee articulation showing the placement of the positioned implants in the tibia and femoral bones (Catani 2013). The knowledge of axis and coordinates of the beads allows determining the precise site of the component bones in the knee joint.\nResearchers found out the movement segments of the body using surface markers. Appropriate gesture information is transmuted from skeletal movements using basic canons of analytical geometry and planimetry. Relative scorer shapes and coordinates the cluster on the skin placement which depicts the vector of knee junction. Video tracing is based on the use of a number of synchronistic registrators placed over the researched area (Catani 2013). The person has repulsive spheres fastened to the skin at operational points. Using cameras registering the sites of each marker from separate perusal corner, the siting of the notes in the video sequence from each of the cameras can be used to admeasure the 3D locus of each notch on the skin.\nTherefore, anatomical markers can show a real picture and trajectories of relevant bone movements by implanting nodes in the bone tissues; on the other hand, tracking markers are placed on the skin but not implanted in inner tissues.\nGet a price quote:\nInaccuracies and Assumptions of the Motion Analysis Technique\nWhile the accuracy of anatomical markers is regarded to be permissible, there are some principal hindrances of the method. First of all, the process can only be the assumption for the post-operative conditions of synthetic knee prostheses due to placing tantalum implants into the bone during knee substitution operations (Schreiber 1998). The exposure to the X-rays during the procedure of computer tomography has an associated risk of initiating mutation of cells that leads to cancer. The x-ray images of the knee articulation seized while a person is positioned horizontally, so the analysis of tomography results does not permit registration of the knee joint so that the patient carries out his/her usual practice (Ramsey 1999).\nVisual tracking allows sub-millimeter kinematic investigation using skin assembled scorekeepers and is very prompt for tracking numerous objects. However, the key impediment of this procedure is that the markers on the skin can move independently from the basal bone structures due to the pliability of the friable muscle strip between the hard bone structure and moving skin-fixed indicator (Gushue 2005). This irrespective motion causes soft tissue artifacts and inaccuracies. Imprecisions in kinematic records due to skin shift artifact are intrinsic with motion analysis.\nThe joint kinematics quantified in three dimensional vision is a better source of information to comprehend the interactions between knee joint components. During flexion, the two main bones in a knee joint, the tibia and femur, rotate around several specific axes in differently positioned planes. This type of motion requires that the joint kinematics should be evaluated through computer tomography based on the same principal. There are several kinematic analysis techniques whose workability differs in terms of invasiveness, diapason of movement rectification, exactness, and ease of use (Masum et al. 2014).\nThe Biomechanical Features of the Knee for Osteoarthritic and Post Total Knee Replacement\nThe physiological movements of the knee have been registered and extensively characterized using the goniometry kinematic data gathered from gait analysis involving anatomical or tracing skin markers. As Scuderi 2010 reported, “Inert domain of stir in the knee articulation has been narrated to be from 162° in condition of full extension to 42° in features of full flexion”. Allen and Mann (2012) stated that “during physiological walking, the femorotibial angle turnes from 155° at paw-strike to 135° at the end of the stance phase and 90° in the mid-swing phase; at the trot, the total sagittal plane angular excursion is around 60-75°.” Spins in uninjured knee are accounted to range from 8° of external rotation to 5° of internal rotation during the swing stage and from 2° of external rotation to 2° of internal rotation during the stance milestone (Weidow 2006).\nAngular disfigurements of the knee lead to varus and valgus deformities and flexion contractures during osteoarthritis. Varus malformations cause overwhelment of the medial knee compartment. From the biomechanical point of view, the knee articulation is comparatively outstretched in the sagittal plane, adducted in the frontal plane, and outwardly rotated in the transverse plane (Alfonso 2011). Adversely, valgus deformities overburden the lateral knee compartment. Biomechanically, the knee undertakes a site of flexion in the sagittal plane, abduction in the frontal plane, and internal rotation in the transverse plane (Bashaw & Tingstad n.d.). Some researches illustrate that less serious injured man with osteoarthritis of knee joint walks with lower peak adduction moments compared to healthy subjects (Henriksen et al. n.d.). Hard-injured sick man with osteoarthritis of a knee joint behaves with larger edge adduction moments than less severe patients or viable persons (Scott 2012). The comparatively sumptuous adduction moments in difficult patients compared with less severe patients could be summoned by divergences in mechanical axis interrelation, which corresponds to both examined disease severity and adduction moments (Weidow 2006).\nThe structure of the contemporary total knee replacement implants is designed with the aim of restorating physiological movements at the knee articulation, chiefly flexion-extension and internal-external rotation. The implants should also be resistant to varus-valgus angulation and cranial-caudal translation.\nSpike knee adduction angle during pacing diminishes to 37% (in 6 months), but rises to 53% during preoperative stages ayear after TKR treatment. At 1-year propulsive phase, knee adduction moment and impulse usually reduce (64% and 78% of preoperative levels, accordingly) (Cassidy 2009). In the conventional curves of adduction moment for the preoperative period, 6-month and 1-year computations after endoprotheses placement into knee joint, peak braking moment forgathered at nearly 30% of the stance phase, whereas peak propulsive moment strikes at approximately 65 – 70% of the stance phase (Scott 2012).\nSome investigation found out the long-term effects of total knee replacement on the biomechanics stating that most patients still evict insolvent step samplers that are similar to their gait patterns preoperatively. In the frontal plane, examiners state that adduction moment is ameliorated 6 months after surgery, but protractedly retrogresses back to higher preoperative levels after 1 year (Jones 2004).\nIn the transversal plane, changes occur when intramedullary insert point positioned to lateral or medial hole during the surgery of total knee replacement. A more valgus distal femoral cut can be made if the entry hole is too lateral, while a more varus distal femoral cut can be made if the hole is too medial (Pickering & Armstrong 2012).\nAnalysis of Appendix 1\nIn accordance with Appendix 1, it could be suggested that indicators meet physiological standards in patient B. The patient C is diagnosed with osteoarthritis; accordingly, patient A has a knee implant after total knee replacement. Such a proposition is proved by graphs of patient B that correspond to the norm. Therefore, the schedule of knee flexion for patient B corresponds to the physiological parameters and the schedule of patient C complies with those with osteoarthritis. Patients with osteoarthritis of knee junction had a significantly lower knee flexion (nearly 10.3°) while a healthy human should have nearly 18.0° (Alfonso 2011). Patient C with osteoarthritis of knee articulation also had significant lower knee flexion of 54.8°±5.5°. Patient A has apparently endoimplant of knee joint in the form of rotary prosthesis. It simulates the natural movement of the knee hinge with rotational moves. It allows increasing the degree of rotation of the lower leg outwards or inwards in flexion. This ensures a natural gait and secure management of the joint during movements. Prosthesis is fixed to the femur and tibia by means of femoral stems. This assertion is substantiated by the following indicator: kinematics of the knee joint goes beyond the norm and acquires negative values (flexion, abduction, rotation, etc.).\nIn conclusion, on the basis of this brief excursion into biomechanics, it is possible to imagine the complexity of the knee joint as a biomechanical system. Basic knowledge of the biomechanics of the knee joint is necessary for the correct formulation of the indications for reconstructive operations, adequate prosthesis implantation, and proper conduct of the rehabilitation program after surgery.']	['<urn:uuid:04877028-ba25-44ad-a087-38578718b826>', '<urn:uuid:231b3234-15a8-41d1-ab06-194c759b2925>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T00:20:10.754925	14	89	4106
40	microeconomics macroeconomics main differences examples	Microeconomics studies individual and business decisions about resources and prices, with examples like consumer budgets and local business decisions. Macroeconomics studies the economy as a whole, focusing on factors like unemployment, inflation, and GDP.	"[""INTRODUCTION TO ECONOMICS Exercise 1 1. What determines that a resource be scarce? Why is scarcity important in defining Economics as a science Resources are seen as being scarce, when the wants exceed the resources. The fundamental problem of economics relates to the choices made in the face of limited resources and unlimited wants. 2. Read your local newspaper and economic magazines. Explain the difference between Microeconomics and Macroeconomics. From your research, give three examples of microeconomic and macroeconomic issues.\nMicroeconomics relates to the study of households and firms and the interaction between these different economic actors. Macroeconomics, however, relates to the study of the economy as a whole, and in particular problems pertaining to growth, unemployment and inflation. Examples of microeconomic issues: 1. price ceilings/floors 2. positive/negative externalities 3. elasticity Examples of macroeconomic issues: 1. inflation 2. unemployment 3. economic growth (GDP) 3. Regarding the concept of Opportunity Cost, comment how your opportunity cost of going to the movies may be affected when: a.\nYou have a final exam the following day b. The University will be shut down for a month starting today c. The same movie will be shown on TV tomorrow a) increase b) decrease c) increase 4. Explain if the following statements are true or false, Explain your reply d. The opportunity cost of choosing a certain activity is equal to the sum of all the benefits of all the alternatives not chosen. e. Opportunity cost is a subjective measure a) False - the opportunity cost of choosing a certain activity is equal to the cost of the next best alternative forgone. ) True – opportunity cost relates to the individual, in that they must choose how to maximize their own utility/happiness, and opportunity cost can vary from individual to individual. 5. Under what conditions is it possible to expand the Production Possibility Frontier (If it is not possible, explain your reply) The PPF can expand during periods of technological innovation or the discovery of new resources. Or through the process of trade. 6. a) England 2/6 = 1/3, USA 1/5 b) Wheat = England, Clothes = England c) England = Wheat, USA = Clothes ) England should produce wheat and USA clothes. Using the following table Hours of work needed to produce one unit | England| USA| Wheat| 2| 1| Clothes| 6| 5| | | | a. What is the opportunity cost of producing wheat in England and in the USA b. Which country has an absolute advantage in the production of wheat, and of clothes c. Which country has a comparative advantage in producing wheat and producing clothes d. Which country should specialize in the production of wheat? And of clothes? a. GRAPH b. GRAPH c. GRAPH\nOrder custom essay Introduction to Economics Exercise 1 with free plagiarism report\nThe Price is above market equilibrium: If the price exceeds the equilibrium price, a surplus occurs and there will be competition among the sellers and price will come down to the equilibrium point. This way the equilibrium is tried to maintain in each of the cases. d. GRAPH Price is below the market equilibrium: If the price is below the equilibrium a shortage occurs and there will be a competition among the buyers and the price will rise to the equilibrium level. This is also another way to restore the equilibrium though there are changes that are occurring in the market in the prices. . The inferior good demand will decrease when consumer income rises. These goods are affordable and adequately fulfill their purpose, but as more costly substitutes that offer more pleasure become available, the use of the inferior goods diminishes. f. A complementary good is a good with a negative cross elasticity of demand. This means the demand for a good will decrease when the price of the complementary good is increased. g. If there is a rumor that price for this good will increase, demand will probably increase too. Consumers want to purchase the product before any increase.\nAll the market will be affect; we will see an increase of demand for main and complementary goods. Shortage will probably appear and price will increase to search equilibrium. Rumor will be reality. 7. Using the following demand and supply table Price| Demand| Supply| 1. 25| 8| 28| 1. 00| 14| 24| 0. 75| 20| 20| 0. 50| 26| 16| 0. 25| 32| 12| a. Graph the demand curve and the supply curve b. Identify the equilibrium price and equilibrium quantity c. Assume a price of 1. 00, Identify if there is shortage or surplus in the market and how the market forces behave to reinstate the equilibrium d.\nNow assume the price is 0. 50 and repeat the question above e. Now assume this is the market behavior for an inferior good, what would happen if the consumers income would increase from $ 25 to $ 32 a week f. Assume this markets complementary good? s price increases, what would happen in this market g. Assume there is a rumor that the price for this good will increases in the following days. How do expectations affect this market? What will be the final outcome? For all the questions above you must support your answer with a graph. 8.\nPrice elasticity of demand : (0,1)/(-0,2) = -1,5 The demand is elastic and sensitive to price changes. 9. Price elasticity of demand : ((0,38-0,5)/0,5) = -0,24 -24% ((26000-10000)/10000) = 1,6 160% (-0,24)/(1,6) = -0,15 The demand is elastic and sensitive to price changes. 10. a. Vodka: luxury good or superior good, making a vodka more expensive can increase its perceived value as a luxury good to such an extent that sales can go up, rather than down. b. Table Salt: necessity good, necessity goods are usually produced by a public utility.\nWe can't live without and won't likely cut back on even when times are tough c. Furniture: normal good, this does not necessarily refer to the quality of the good, but an abnormal good would clearly not be in demand, except for possibly lower socioeconomic groups. d. Perfume: luxury good or superior good, making a perfume more expensive can increase its perceived value as a luxury good to such an extent that sales can go up, rather than down. e. Beet: inferior good, we can replace beet by others foodstuffs more expensive, better quality or that give more pleasure like a meat. f.\nSugar: necessity good, necessity goods are usually produced by a public utility. We can't live without and won't likely cut back on even when times are tough. 11. Decision making is based on costs and benefits. That means you make purchases if the marginal benefits are greater than the price. Thus, when you decide to make a purchase, you are likely receiving something worth more to you than what you had to spend to buy it, or else you wouldn’t have bought it. That net benefit, minus the equilibrium price, is consumer surplus. 12. GRAPH b. With a price of 4$, Bert buys 2 bottles.\nHis CS for 2 bottles: (7-4) + (5-4) = 3 + 1 = 4$ c. With a price of 2$, Bert buys 3 bottles. His CS for 3 bottles: (7-2) + (5-2) + (3-2) = 9$ 13. GRAPH A price ceiling occurs when the government puts a legal limit on how high the price of a product can be. In order for a price ceiling to be effective, it must be set below the natural market equilibrium. When a price ceiling is set, a shortage occurs. A price floor is the lowest legal price a commodity can be sold at. Price floors are used by the government to prevent prices from being too low.\nFor a price floor to be effective, it must be set above the equilibrium price. 14. a. 4P – 80 = 100 – 2P 6P = 180 P = 30$ Q = 40 The equilibrium price is 30$ and the equilibrium quantity is 40 bushels b. GRAPH c. (0. 5) x (20) x (40) = 400 $ Consumer surplus (0. 5) x (10) x (40) = 200 $ Producer surplus 400 + 200 = 600 $ Surplus Equilibrium 15. f. The difference between marginal utility and total utility The marginal utility of a good or service is the gain or loss from an increase or decrease in the consumption of that good or service.\nAll goods and services have their own marginal utility. The total utility is the sum of all marginal utilities for a good or service. Total utility is the aggregate sum of satisfaction or benefit that an individual gains from consuming a given amount of goods or services. The more the person consumes, the larger his or her total utility will be. g. The principle of diminishing marginal utility Diminishing marginal utility is to use or consume a good or service with the most important or efficient utility for us.\nDid you know that we have over 70,000 essays on 3,000 topics in our database?"", 'How would you describe microeconomics and macroeconomics? A macroeconomic definition includes unemployment, interest rates, inflation, and GDP. Microeconomics are characterized by consumer equilibrium, individual income, and savings.\nWhat Is Macroeconomics And Examples?\nEconomic activity is studied by examining the economy as a whole. In macroeconomics, employment, inflation, productivity, interest rates, the foreign trade deficit, and the federal budget deficit are examined. The study of the U.S. economy is an example of macroeconomics. It is a job that requires employment.\nWhat Is An Example Of Microeconomics?\nSupply, demand, competition, and prices of goods are some examples of microeconomics. An example of microeconomics would be how a young couple plans their first-home budget.\nWhat Is Difference Between Microeconomics And Macroeconomics?\nA microeconomic study is a study of individual and business decisions regarding the allocation of resources and prices of goods and services. A macroeconomic study is a study of the decisions made by governments and countries. In economics, the term refers to the entire industry and economy rather than just a few companies or individuals.\nWhat Is A Good Example Of Macroeconomics?\nEconomic output, unemployment rates, and inflation are examples of macroeconomic factors. Governments, businesses, and consumers all monitor these indicators of economic health.\nWhat Are Examples Of Microeconomics?\nWhat local businesses decide to allocate their funds for.\nA city’s decision to spend a surplus of government funds.\nAn area’s housing market, which is a neighborhood’s housing market.\nA local business is produced.\nWhat Are The Two Examples Of Macroeconomics?\nMacroeconomics studies can be divided into two categories.\nThe study of price behavior in India (i).\nStudy of unemployment in India (ii).\nWhat Is Macroeconomics Explain?\nThe macroeconomy deals with the structure, performance, behavior, and decision-making of an economy as a whole, or aggregate. Economic research focuses on long-term growth and short-term business cycles, respectively.\nWhat Is Microeconomics And Its Example?\nA microeconomic study examines how people and businesses allocate resources and determine the prices at which they trade goods and services. Microeconomics, for instance, examines how a company can maximize its production and capacity to lower its costs and become more competitive.\nWhat Is A Real Life Example Of Microeconomics?\nThe study of microeconomics examines how individuals and businesses make decisions about how to use limited resources most effectively. You can apply its principles to everyday life situations-for instance, when you’re renting an apartment-by using them. It is not uncommon for people to have limited amounts of time and money.\nIs An Example Of Microeconomics Theory?\nA microeconomic example would be how a local business allocates its funds. A city’s decision to spend a surplus of government funds. An area’s housing market, which is a neighborhood’s housing market.\nWhat Is The Difference Between Microeconomics And Macroeconomics Class 11?\nThe study of macroeconomics focuses on aggregate factors such as national output, income, and prices. Microeconomics deals primarily with individual income, output, and price of goods. Macroeconomics focuses on aggregate factors such as national output, income, and prices. A macroeconomics study focuses on issues such as employment and household income.\nWhat Is The Difference Between Macro And Micro Economics Quizlet?\nMicroeconomics and macroeconomics differ primarily in the following ways. The term macroeconomics refers to groups of individuals, while microeconomics refers to countries as a whole. The microeconomics focus on individual markets, while the macroeconomics focus on aggregate markets (forests).\nWhich Is Easier Macroeconomics Or Microeconomics?\nMicroeconomics are more difficult than macroeconomics at the entry level because they require at least a minimal understanding of calculus-level mathematical concepts. In contrast, entry-level macroeconomics are understood primarily by logic and algebra.\nHow Is Macroeconomics Used In Everyday Life?\nIn addition to the news about the macroeconomy, the price you pay for goods and services, the tax you pay on income, and the effects of macroeconomic policy on interest rates, macroeconomics are also a part of our daily lives. You are also affected by macroeconomic events and policies in other countries.']"	['<urn:uuid:5bc5d894-c42a-460c-ae5e-be2063780f96>', '<urn:uuid:c9edd936-26c6-4fff-a59f-669bd587e129>']	factoid	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-13T00:20:10.754925	5	34	2161
41	What is the relationship between cosmic rays hitting Earth and the Sun's activity cycle, and how do satellites help track these interactions?	Satellites help monitor Earth's atmosphere and track how it changes with space weather, including the interactions with cosmic rays. Galactic cosmic rays (GCRs) are charged particles from events like supernovae that constantly rain through space and can penetrate Earth's magnetosphere and atmosphere. The intensity of GCRs at Earth is directly related to solar activity - during solar maximum, intense magnetic fields extending from the Sun divert cosmic rays away from the solar system, reducing GCR intensity at Earth to half of what it is during solar minimum. Some satellites are placed in special orbits, including polar orbits and LaGrange points between Earth and the Sun, to help monitor these space weather interactions.	"['How do Satellites & Spacecraft Monitor Space Weather?\nSatellites and other spacecraft help us observe space weather. They collect data about the Sun, Earth\'s magnetosphere, Earth\'s atmosphere, and space weather throughout the rest of our Solar System.\nSpacecraft that Watch the Sun\nSome spacecraft watch the Sun. They have special telescopes for looking at the Sun and the Sun\'s atmosphere. Some can ""see"" the Sun in wavelengths, like X-rays, that cannot be viewed from Earth\'s surface. Some spacecraft carry instruments called coronagraphs that make artificial eclipses. Astronomers use coronagraphs to study the Sun\'s atmosphere.\nSpacecraft that Measure the Solar Wind\nThe solar wind of charged particles flows outward from the Sun. It carries the Interplanetary Magnetic Field (IMF) with it. Spacecraft measure the solar wind and the IMF. They record the speed and density of the solar wind. They measure the strength and direction of the IMF. Some satellites and other spacecraft make these measurements near Earth. Other spacecraft record observations of the solar wind and IMF near other planets, on their way to comets or asteroids, or in the depths of ""empty"" space between the planets.\nSpacecraft that Observe Magnetospheres\nSome planets and moons have magnetic fields. Earth has a fairly strong magnetic field. Planets and moons with strong magnetic fields make ""magnetic bubbles"", called magnetospheres, around themselves in space. Charged particles swirl around in magnetospheres in complex patterns. Satellites orbiting Earth observe our planet\'s magnetosphere. Spacecraft that fly by or orbit other planets with magnetic fields measure the magnetospheres there. They measure the strength and direction of the magnetic field all around the planet. They observe radiation belts, measure the density of charged particles, and record the strengths of electrical currents.\nWatching the Aurora from Space\nHave you ever seen the Northern (or Southern) Lights? The aurora, as these lights are also called, can often be seen from places at high latitudes on Earth - like Scandinavia, Greenland, Canada, Alaska, Russia, and Antarctica. Guess what? They can also be seen from space! Astronauts have taken pictures of aurora. Spacecraft that fly in higher orbits observe the whole auroral oval around a pole at once. The Hubble Space Telescope and some other spacecraft have also taken pictures of auroras at the poles of other planets!\nKeeping an Eye on the Atmosphere\nSpace weather causes changes in Earth\'s atmosphere. Satellites help us keep track of how the atmosphere changes. One part of the atmosphere, the ionosphere, has lots of electrically charged particles in it. Stations on the ground work with satellites to beam radio waves through the ionosphere. The particles in the ionosphere affect the radio signals, allowing scientists to measure the ionosphere. They can see how the ionosphere changes as space weather changes.\nSpecial Orbits for Space Weather Spacecraft\nSome of the spacecraft that observe space weather are in special orbits. Some satellites loop over the North and South Poles in polar orbits that let them watch auroras. Some hover at a special ""LaGrange point"" between Earth and the Sun that lets them detect the solar wind a little while before it gets to our planet.', 'I was asked to explain the various sources for Space Weather. So here we go:\nSolar activity describes events that take place on the sun, while space weather describes the change in solar activity as seen on our planet, by technology, and in the magnetic organization of the solar system.\nA Coronal Mass Ejection (CME) occurs when a prominence suspended above the surface of the Sun erupts and sends millions of tons of material into space. This cloud of charged particles is generally confined within a magnetic field (like a magnetic bubble), expanding and traveling out through the solar system at speeds from about 200 km/s up to a staggering 2000 km/s. When directed toward Earth, a CME typically arrives 2-3 days after eruption but in exceptional cases can arrive in less than 24 hours.\nA solar flare is a bright flash of X-rays seen during an energetic explosion in an active region of the Sun. It’s usually seen as a large burst of X-rays, but may also have a coincident bright flash of white light. A flare lasts a matter of minutes but releases an immense amount of energy. During solar flares the sun can be 1000 times brighter in X-rays than usual.\nA high-speed stream (HSS) is like a powerful gust in the solar wind. Near the edges of a HSS are regions of high particle density and strong magnetic fields, while inside the stream the density and field are low and the temperature and velocity are high. The flow velocity inside a HSS can reach 300 to 1000 km/s. In some cases they will produce interplanetary shocks. A HSS comes from a coronal hole, a dark region in the solar corona, and is most common during the declining phase of the solar cycle. A HSS can last a long time: we often see a HSS every 27 days as its home coronal hole rotates into the right position.\nEarth’s magnetosphere is a bubble created around us by our magnetic field, which protects us from most of the particles the Sun throws at us. When a CME or high- speed stream arrives at Earth, it buffets the magnetosphere. If the arriving solar magnetic field is directed southward, it interacts strongly with Earth’s oppositely oriented magnetic field. As a result, Earth’s magnetic field is peeled open like an onion, allowing energetic solar wind particles to stream down the field lines to hit the atmosphere over the poles. At Earth’s surface, a magnetic storm is seen as a rapid drop in Earth’s magnetic field strength (typically a drop of 30 to 500 nT in 1-2 hours). This decrease lasts about 6 to 12 hours, after which the magnetic field gradually recovers over a period of several days.\nA constant rain of galactic cosmic rays (GCRs) passes through all of space. These charged particles are accelerated to extremely high energies by galactic events such as supernovae (the catastrophic collapse of a star), although some may come from outside the Milky Way. Their high energy allows GCRs to penetrate through Earth’s magnetosphere and deep into the atmosphere. As they enter the atmosphere, they collide with molecules of oxygen and nitrogen and produce millions of tiny fragments. The GCR ionization rate (roughly the radiation dose) reaches a maximum at an altitude of about 12 km (40,000 ft) and then decreases toward Earth’s surface. The intensity of GCRs at Earth is highest during solar minimum. This happens because the intense regions of magnetic field extending out from the sun into the solar system during solar maximum divert the charged cosmic rays away from the solar system. As a result, the GCR intensity at Earth during solar maximum is half the GCR intensity during solar minimum.\nMain Photo Credit: NASA, Artist’s depiction of an active Sun that has released a Coronal Mass Ejection. CMEs are magnetically generated solar phenomenon that can send billions of tons of solar particles, or plasma, into space that can reach Earth one to three days later and affect electronic systems in satellites and on the ground.']"	['<urn:uuid:5f745cf1-b4c7-4e97-9aa7-33f61e8aafac>', '<urn:uuid:2a570bfa-efbb-458e-a03a-3f1efe78eee5>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T00:20:10.754925	22	112	1193
42	When cutting wood on a table saw, which side will have rough edges or splinters if any appear?	On a table saw, any splintering will occur on the underside of the workpiece, as its blade cuts on the downward rotation. This is the reverse of how a portable circular saw works.	['Quick Tip: Table Saw Techniques\nLearn the different ways you can use your table saw.\nYour table saw can perform a remarkable number of tasks. There’s probably enough to fill a website, especially when additional equipment like dados and molding cutters are added to the tool repertoire. But the several basic table-saw techniques we’ll discuss here are those you are most likely to need at the work-site, including cutoff work (that is, crosscutting boards at a ninety-degree angle) and miter sawing (angle cutting), ripsawing, resawing and rabbeting.\nCrosscutting and miter sawing. Preset the miter fence to the angle desired, whether it’s a square cut (in which case the miter setting will be zero degrees) or a reading you got using your miter gauge to match an existing surface. Once the miter fence is set to the desired angle, the workpiece is held flush to the face of the gauge with one hand, while the other balances the piece. Only after you are in position should you turn on the saw. Then push the gauge and workpiece together toward the blade.\nRipsawing. Position the rip fence parallel to the blade, measuring the distance from the fence to a tooth that is bent (set) from the near side of the blade. Once the fence is in place, tighten its built-in clamp to fix it in position.\nHow wide will the piece be after it is ripped? The narrower the piece, the greater the need for a push stick to power the piece through – and to keep your fingers away from – the blade.\nRe-sawing. If you require a board thinner than the stock you have on hand, re-sawing may be the answer. Re-sawing is essentially a two-step ripping process. You begin by presetting the rip fence to the thickness you desire; then you run the workpiece through the saw on its edge. Now, turn the piece over and run it back through, so that the blade will make a matching cut that reaches to the first kerf.\nRabbeting. Especially in restoration work, some moldings need to be rabbeted – that is, a deep lip or “rabbet” is cut onto their edge. Rabbet joints, new or old, are stronger than simple butt joints. Making rabbet cuts for such joinery is simple on a table saw.\nTwo cuts are required. To make the first one, you must set the height of the saw blade to cut the piece to the depth of the rabbet; then you need to set up the fence so that the cut will be the appropriate distance from the edge of the stock. Make the cut as you would any rip cut.\nTo make the second cut, you need to reset the blade height and the fence so that a cut made perpendicular to the first will create the desired rabbeted edge. This cut is made with the board turned ninety degrees so that it is perpendicular to the tabletop.\nSet up, cut with care, and a rabbet appears – not quite by magic.\nThinking ahead. Make it a habit, before you turn on your saw, to think about the cut to be made. Are you wearing your eye protection? What about your ears?\nAnd where are your hands with respect to the blade? If the workpiece is short and doesn’t require two hands to balance it, put the unneeded hand behind your back or otherwise locate it out of the way.\nAfter the cut, where will the waste fall? And what about the piece you want? Will it be balanced, or do you need to locate a sawhorse or other support to keep it level?\nAre you cutting with the good side up? The table saw is the reverse of the portable circular saw, as its blade cuts on the downward rotation, meaning that any splintering will occur to the underside of the workpiece. Where appropriate, use a feather board and push stick.\nStay in line. As with any saw cut, keep in mind which side of the line you wish to cut. The kerf on a typical table saw is an eighth of an inch wide, so cutting to the wrong side of the line will result in a significant error.']	['<urn:uuid:0ac13975-3ad5-4ff8-a818-9c6fdb1678ab>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T00:20:10.754925	18	33	704
43	cape floral region mapungubwe heritage compare significance	Both are significant World Heritage sites in South Africa but for different reasons. The Cape Floral Region was recognized by UNESCO in 2004 for its outstanding terrestrial biodiversity and unique fynbos vegetation. Meanwhile, Mapungubwe Cultural Landscape was inscribed in 2003 for its evidence of human cultural interchange, being the largest ancient kingdom in Africa, and demonstrating the impact of climate change on a civilization between AD 900-1300.	"['Celebrating our natural heritage\nBy Mahlatse Mapheto\nHeritage can be defined as our inheritance, which is what the past has conceded to us, what we value in the present and what we choose to preserve for future generations. This is comprised of cultural and natural heritage. Cultural heritage includes tangible references such as artefacts or any work of art, monuments, cultural landscapes and buildings. This also includes intangible references such as language, knowledge systems, myths, beliefs, memories and traditions. The term ‘natural heritage’ is said to pre-date what we now describe as bio-diversity, which are the different categories of flora and fauna within a specific habitat area, that are considered to be important and desirable as they are the foundation of ecosystem services to which human well-being is intimately linked.\nSouth Africa has 9 World Heritage sites which are classified as either cultural or as natural heritage sites – including culturally significant natural landscapes and biodiversity. The Mapungubwe Cultural Landscape, Robben Island, Cradle of Humankind, Khomani Cultural Landscape and the Richtersveld Cultural and Botanical landscape, are regarded as our Cultural Heritage Sites. Our Natural Heritage Sites include the Barberton Makhonjwa Mountains, iSimangaliso Greater St Lucia Wetland Park, Vredefort Dome and the Cape Floral Region Protected Areas.\nThe Cape Floral Region has one of the richest bio-diversity collections for a region of its size and is comprised of 8 protected areas namely;\n- Table Mountain National Park\n- De Hoop Nature Reserve\n- The Boland Mountain Complex\n- The Groot Winterhoek Wilderness Area\n- The Swartberg Mountains\n- The Boosmansbos Wilderness\n- The Cedarberg Wilderness Area\n- And Baviaanskloof Mega Reserve\nThe World Heritage Convention Act 49 of 1999 (WHCA) ratified the Cape Floral Region as World Heritage Site. This was later promulgated by the United Nations Educational, Scientific and Cultural Organisation (UNESCO) as a World Heritage site, in 2004, largely because of its importance as a centre of great terrestrial bio-diversity. It is described by the organisation as having ‘outstanding universal significance to humanity’ as it has ecological and biological processes associated with the fynbos vegetation, unique to the Cape Floral Region.\nAs one of the richest biodiversity areas in the world, coupled with the internationally acclaimed Table Mountain, the Cape Floral Region is an area of great heritage significance, and this should be celebrated and conserved for the benefit and enjoyment of future generations.\nThe Table Mountain Fund is a capital conservation trust found within the Cape Floral Region aimed at providing catalytic funding towards conservation projects focusing on the protection and preservation of this natural heritage site, while also providing socio-economic opportunities to local communities. The Table Mountain Fund has a Pride Programme which is aimed at instilling a sense of appreciation of the diverse flora and fauna which occur within the Cape Floral Region. This is done by encouraging individuals to gain a better understanding of the diversities within the Cape Floral Region and thus gain a sense of respect for our shared natural heritage, with the hope of developing a shared priority for the preservation of our natural heritage in the process.\nThe preservation of our natural heritage is important as it encourages communities to take responsibility for their natural landscape and participate in the development and conservation of their heritage, which in essence, creates a greater sense of pride and belonging.', 'History of the Park\nMapungubwe: Becoming a National Park\nThe Mapungubwe area became a focus of agricultural research in the 1920s through the efforts of a prominent botanist, Dr. I.B. Pole Evans. Pole Evans was instrumental in the creation of the Botanical Survey Advisory Committee which was tasked with coordinating botanical research throughout the Union of South Africa. One of the network of botanical and research stations set up by the Botanical Survey was situated in the Mapungubwe area. In 1918 the government, at the request of General Smuts, set aside a block of nine farms in this area as a preserve for wildlife and natural vegetation. A few years later this became known as the Dongola Botanical Reserve.\nPole Evans set about expanding the Dongola Botanical Reserve. By the early 1940s the reserve had grown to include 27 farms, including Greefswald, the property on which the Mapungubwe Hill is situated. Pole Evans lobbied to have the reserve proclaimed as a national park, with the support of Prime Minister Jan Smuts. In 1944 Minister of Lands, Andrew Conroy proposed the formation of the Dongola Wild Life Sanctuary which would include 124 farms, 86 of which were privately owned. This proposal was strongly opposed by the National Party, then the official opposition in parliament and the National Parks Board of Trustees. In one of the longest running debates in the history of the South African parliament, supporters argued that it was necessary to conserve the country\'s natural assets, that the land set aside for the proposed reserve was unsuitable for agricultural purposes and that the area had a rich archaeology which should be protected. Those opposed to the establishment of the reserve argued that it was unacceptable to alienate agricultural land for wildlife conservation, to expropriate private land or to evict people from land they had occupied for generations. The debate, which has become known as the ""Battle of Dongola"", resulted in the declaration of a much reduced area as the Dongola Wildlife Sanctuary, after members of the ruling United Party voted in favour of the proposal. The National Party won the elections in 1948, and the sanctuary was abolished in 1949. Expropriated farms were returned to their original farmers, farms owned by the state were allocated for resettlement and funds returned to donors.\nIn 1967 another proposal to protect the area was initiated and the Vhembe Nature Reserve, comprising three farms, including Greefswald, was established as a Transvaal provincial reserve.\nIn 1993 De Beers Consolidated Mines, which had established the Venetia Limpopo Nature Reserve on land that adjoins Greefswald, called for the area to be declared a national park. In 1995 the South African National Parks Board and the Limpopo provincial government signed an agreement committing themselves to the establishment of the new national park. The Vhembe Dongola National Park was proclaimed on 9 April 1998.\nThe Vhembe Dongola National Park was renamed Mapungubwe National Park and opened officially on Heritage Day, 24 September 2004.\nIn the 21st century Mapungubwe has been embraced as a site of significance by South Africans and the international community. The Mapungubwe National Park was declared in 1998. The Mapungubwe Cultural Landscape (MCL) was declared as a National Heritage Site in 2001 and it was inscribed on the World Heritage List in 2003.\nThe MCL was inscribed on the World Heritage List in 2003 because it is believed to be of outstanding universal value for the following reasons:\n- The MCL contains evidence for an important interchange of human values that led to far-reaching cultural and social changes in southern Africa between AD 900 and 1300.\n- The remains in the MCL are a remarkably complete testimony to the growth and subsequent decline of the Mapungubwe State which at its height was the largest kingdom on the African subcontinent.\n- The establishment of Mapungubwe as a powerful state trading through the East African ports with Arabia and India was a significant stage in the history of the African sub-continent.\n- The remains in the MCL graphically illustrate the impact of climate change and record the growth, and then decline, of the Kingdom of Mapungubwe as a clear record of a culture that became vulnerable to irreversible change.\nThe remains of this famous kingdom,when viewed against the present day fauna and flora, and the geo-morphological formations of the Limpopo/Shashe confluence, create an impressive cultural landscape of universal significance.\nMapungubwe: An Archaeological Site\nIn December 1932, Ernst van Graan, his son Jerry and three other men searched the farm Greefswald, where both Mapungubwe and K2 are located, for a sacred hill rumoured to hold the treasure of kings. Arriving eventually at the steep-sided and seemingly unscaleable hill, a local man, known only as Mowena, reluctantly pointed the way to a narrow path well concealed in a cleft in the rock. Reaching the top of the hill, the men found remnants of stone walls and large quantities of potsherds, some iron tools and copper and glass beads scattered on the ground. They returned the next day with spades and soon unearthed ancient graves, three which were different and probably belonged to royalty. The first, probably a woman, was buried in sitting position facing west. She wore gold bangles around her ankles and there were gold and glass beads in the grave. The second grave was a tall middle-aged man, also sitting up and facing west. He wore a necklace of gold beads and cowrie shells and some objects covered in gold foil, one resembling a crocodile. In the third grave, probably also from a man, a golden bowl, scepter and a golden rhino were found. This rhino has become a ymbol for Mapungubwe. The men split the gold between them but one, Jerry van Graan, realising the possible significance of the find, sent a few small pieces of gold together with a letter to his former professor, Leo Fouche at the University of Pretoria. Professor Fouche immediately mounted an expedition to recover and secure the newly found treasures and the surrounding environment for archaeological research. Excavations in the 1930s unearthed 23 graves on top of Mapungubwe Hill. Three were different and probably belong to high royalty.\nIn June 1933 the government purchased the farm Greefswald, gave the University of Pretoria the right to excavate for a period of five years and constituted an Archaeological Committee at the university to take charge of the excavations. Exploratory excavations in 1933 were followed by large-scale excavations between 1934 and 1940. Unfortunately the lack of proper recording procedures during this time means that valuable evidence may have been lost. Work on the site was halted during World War II and limited, but more systematic excavations were undertaken in the 1950s and 1960s. After 1970 widespread excavations were undertaken at K2 and Mapungubwe. These were aimed at establishing a firm database by testing, correcting and supplementing the earlier research, coming to an understanding of the Iron Age settlement sequence and reconstructing the way of life of the sites\' inhabitants. This information is available in a range of specialist reports, academic publications and guide-books. In the late 1990s, after the area was designated as a National Park, extensive rehabilitation and stabilisation work was undertaken to secure the archaeological sites. Research over the past decade has focused on the material held in the University of Pretoria\'s Mapungubwe Collection, gathered over the 60-year period of excavations. The Mapungubwe Museum, established by the University of Pretoria in 2000, serves as a centre for the conservation and display of the collection, disseminating information about Mapungubwe in all its diversity.\nBetween 1933 and 1998 the remains of about 147 individuals were excavated from the Mapungubwe Cultural Landscape. These included individuals buried on Mapungubwe Hill and in the K2 area. These human remains were excavated from their graves and placed in the collections of the University of Pretoria, the University of the Witwatersrand and Ditsong Museum where they were used for research purposes. Individuals and organisations representing the descendants of the people of Mapungubwe, including the Vhangona Cultural Movement, the Lemba Cultural Association, the San Council, the Tshivula Royal Family, the Ga-Machete Royal Family and the Leshiba Royal Family came together after 1994 to request government to facilitate the process of repatriation and reburial of these human remains. The National Department of Environmental Affairs and Tourism was tasked with facilitating this process. After extensive consultation, the human remains were symbolically released to representatives of the descendants on 29 October 2007. The remains were reburied on 20 November, following a cleansing ceremony.\nMapungubwe: A Cultural Landscape\nTwo of the earliest plant-eating dinosaurs, Plateosauravus (Euskelosaurus) and Massospondylus, are known to have lived in the area now known as the Mapungubwe Cultural Landscape (MCL). Plateosauravus, the oldest South African dinosaur, lived about 210 million years ago and is believed to have grown to be about 10 metres long. Massospondylus, the most common South African dinosaur, lived about 195 million years ago and is believed to have grown to between three and six metres long. These two species are similar in many ways. They both had long tapered necks and tails and elongated cylindrical bodies and they both walked on all fours, standing upright on their hind legs in order to reach the succulent young fronds of cycads and seed ferns. Like many other dinosaurs, these used their hands to manipulate objects like branches or to prop or pull themselves up against objects.\nThe first inhabitants of the Mapungubwe Cultural Landscape (MCL) who arrived about 300,000 years ago were people of the Earlier Stone Age (ESA). They walked upright but their brains were not as well developed as ours. Many ESA stone tools have been found at open-air sites such as Hackthorne and Keratic Koppie. The tools include handaxes, picks, cleavers and scrapers that were made from local rocks such as quartz, chert, chalcedony and occasionally dolerite. Between 250,000 and 130,000 years ago the heavy tools of the ESA were replaced by lighter and more technologically advanced Middle Stone Age (MSA) tools. At Kudu Koppie and Parma Farm these tools, made by physically modern people, include parallel-sided blades and triangular points which were hafted to make hunting spears. There is evidence from a rock shelter at Balerno that by 11,000 years ago, small groups of people of the Later Stone Age (LSA), ancestors of the San (Bushmen), had begun to live in the Mapungubwe Cultural Landscape. Tools associated with this and other LSA sites such as Thudwa Shelter at Little Muck, include carefully prepared blades and scrapers made from fine grained rocks, such as quartz or chalcedony. The tools are generally smaller than those of the MSA. Towards the end of the LSA, about 3,000 years ago, people in the MCL began painting images of animals and people on the walls of rock shelters to record their beliefs and rituals. MSA and LSA tools accumulated on the floor of rock shelters, together with bones of animals they ate and decayed plant matter that built up in layers when people returned to these places repeatedly. Analysis of the remains shows that the LSA inhabitants lived mainly off small animals such as small antelope, tortoises, hares and fish caught by hunting and snaring.\nHunter-gatherers living in the Mapungubwe Cultural Landscape were probably surprised around the first century AD when LSA herders and early framers entered the area By AD 450, an even more dramatic change had taken place, as Early Iron-Age farmers moved into the area, bringing with them farmers with them domesticated cattle, sheep and goats, crops such as sorghum and millet, and skills such as iron- and copper-working and pottery-making. How did these three groups interact with each other? What effect did the influx of newcomers have on the people already living there? And what effect did the newcomers have on the landscape? Excavations in rock shelters in the MCL show that hunter-gatherers did not move away during the first millennium AD, and were in fact more active. This is reflected in larger numbers of stone tools, bone tools, ostrich eggshell and Achatina shell beads and ochre, as well as new items such as glass beads and pottery. The increased activity is believed to be the result of interaction with farmers who took over land and resources previously available to hunter-gatherers in the region. The hunter-gatherers therefore stayed for longer periods in rock shelters and other places less favoured by the farmers and created new economic networks, taking agricultural produce, pottery, metal and glass beads in exchange for hides and wild meat. Hunter-gatherers continued to make rock paintings during this period to emphasise their ownership of rock shelters and their powers of rain-making.\nBy about AD 900, Zhizo people had moved into the area, establishing settlements in the MCL, eastern Botswana and south-west Zimbabwe. At least 25 Zhizo settlements have been identified in the MCL. In most cases these settlements are set back from the Limpopo and other rivers, in areas where crops could be cultivated without threat of destruction by floods or marauding elephants. Schroda is the largest of the Zhizo settlements in the MCL and is generally referred to as the Zhizo capital. Located on a rocky plateau close to the Limpopo River, Schroda was home to between 300 and 500 people. Zhizo presence in the MCL diminished after about a hundred years, and while some settlements remained, the chiefdom moved westwards into Botswana. The area was soon dominated by a new group, the Leopard\'s Kopje people.\nHunter-gatherers were no longer able to move freely around the landscape to gather wild plant foods and to hunt easy prey. The differences in the belief systems and social organisation of farmers and hunter-gatherers made it difficult for them to inter-marry, and the perceived benefits of the goods and services that each group had to offer probably also changed over time. Zhizo farmers may have valued the services of the \'first people\' who were considered to have power over nature and the supernatural in terms of rain-control, as well as local knowledge of plants and herbal remedies. Hunter-gatherers would have valued the metal tools, cultivated grains and glass beads that they received from the Iron Age farmers.\nPeople making Leopard\'s Kopje pottery began to live in the Mapungubwe Cultural Landscape about a thousand years ago. The largest settlement was at the site now known as K2, but there were many smaller villages in and around the Limpopo floodplain. The name \'K2\' was given to the site by archaeologist Guy Gardner who was trained in Egypt where the word \'Kom\' means a mound or midden, a heap of discarded food remains, artefacts, etc disc, built up by people living in the same place over many generations. K1 is a smaller mound to the east of K2. The Leopard\'s Kopje farmers cultivated crops on the flat lands between the hills where they could take full advantage of the seasonal flooding of the Limpopo River and its tributaries. They made a substantial impact on the natural landscape, clearing land for the cultivation of crops, dumping refuse and burning old houses and kraals. Even today, a thousand years later, middens and kraals are still easily identifiable because very little vegetation grows on them. Animal bones excavated from these settlements show that the people ate meat from sheep, goats and cattle, supplemented with game and fish from the river. Seeds preserved from cultivated crops include sorghum, millet, beans and cowpeas. . Metal artefacts included arrowheads, spearheads, hoe blades, beads, bangles and wire. Small quantities of iron and copper ore and slag show that metal working took place in and around the settlement. Almost a hundred graves were found at K2. Information about them can be found in Cairn Three.\nBetween AD 1220 and AD 1300, Leopard\'s Kopje people settled on and around Mapungubwe Hill. This site is renowned as the centre of southern Africa\'s first indigenous kingdom. The kingdom flourished as the result of trade with the Indian Ocean network. The emergence of a class-based society is reflected in the separation of an elite class with a sacred leader who lived on top of the hill, and commoners who lived on the plains below. At the height of its importance, between AD 1220 and AD 1300, the Mapungubwe Cultural Landscape sustained a population of at least 9,000 people. Mapungubwe is arguably best known for the gold artefacts found in graves on the hill in the 1930s. Three of these graves contained gold beads and other items confirming the high status of the individuals who lived and were buried on the hilltop.\nHave a question? Why not ask on the forums?\nView and download maps.\nDid You Know?\nMin: 21°C / Max: 34°C']"	['<urn:uuid:37bdab13-c8b8-4d69-ad8d-cf16c39fdc94>', '<urn:uuid:c99e0d2b-641f-4195-898f-4f9a4ea27232>']	factoid	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-13T00:20:10.754925	7	67	3338
44	Does El Niño affect worldwide temperatures and climate change?	El Niño does release heat into the atmosphere and increases global temperatures. For instance, during the strong 2015/2016 El Niño episode, it contributed about 0.2°C to the overall 1.1°C temperature rise above preindustrial values observed in 2016. Regarding climate change, while there isn't enough historical data to show clear impacts on El Niño's properties, some evidence suggests that El Niño's effects on rainfall may increase in the future and more extreme ENSO events might occur, though this remains under research.	['What is El Niño Southern Oscillation (ENSO)?\nWhat is El Niño?\nThe El Niño Southern Oscillation (ENSO) is a large-scale climatic phenomenon that originates in the tropical Pacific but affects global climate patterns. The warm phase is known as El Niño, and the cold phase is La Niña. El Niño occurs irregularly every two to seven years and peaks in winter.\nThe term El Niño, Spanish for ‘child’ or ‘the Christ child’, was first used by fishermen along the coasts of Ecuador and Peru to refer to a warm ocean current that typically appears around Christmastime and lasts for several months.\nWhat causes an El Niño event?\nIn a normal year, when ENSO is inactive, the equatorial Pacific trade winds blow from east to west. The winds push the warmer water towards the west, and colder water rises up from deeper in the ocean to replace it. This creates an east-west difference in sea surface temperature and hence an east-west difference in sea level pressure that maintains the trade winds and so drives a positive feedback loop.\nDuring an El Niño year, the east-west SST difference weakens, the pressure difference weakens, and the trade winds and their effects on the ocean weaken, so the eastern Pacific warms further. However, during a La Niña year, the opposite happens, the east-west temperature difference strengthens, the pressure difference strengthens, and the trade winds and their effects on the ocean strengthen, so the east Pacific cools further.\nSuch changes in sea surface temperatures affect the atmosphere over vast areas, with local and global repercussions. Locally, the associated atmospheric circulation changes drive increased atmospheric convection and precipitation over the central and eastern Pacific, while rainfall is reduced over the western Pacific. In addition, these changes have remote impacts throughout the tropics and at higher latitudes via the atmosphere, especially when ENSO is at its strongest in winter.\nHow do we measure El Niño?\nA network of ocean buoys measures water temperature, currents and the wind. Satellites also provide surface temperature and current data, providing the ability to monitor ENSO in real-time. We can also predict El Niño months in advance as we can measure the heat content of the upper ocean, one of the precursors for the onset of El Niño and use our climate models to predict its evolution. Although various criteria exist, we usually say an El Niño event is underway when sea surface temperatures in the equatorial Pacific (officially called Niño region 3.4) rise 0.5°C above the historical average for at least three months in a row.\nImpacts of El Niño and La Niña\nThe most substantial impacts are experienced by those countries near the tropical Pacific origin of ENSO. Changes in surface temperatures, winds and moisture affect rainfall intensity and patterns, leading to extreme events such as flooding and drought. During an El Niño event, Peru, Ecuador and the south-eastern parts of South America receive heavy rainfall. In northern Brazil, drier conditions or even drought results. Indonesia, South Asia and parts of Australia are also more likely to experience drought during El Niño. The change in weather patterns associated with El Niño can significantly impact the economy, particularly agriculture, water resources, fisheries and public health. ENSO also strongly influences the occurrence and intensity of tropical cyclones, and Atlantic hurricane activity weakens during El Niño but strengthens during La Niña. In the UK, we experience the socio-economic impacts of an El Niño event, partly through increased food prices. There are effects on the jet stream and European weather, especially in late winter when El Niño increases the chances of cold snaps. La Niña increases the chances of wet and stormy conditions.\nHow does El Niño affect global temperature?\nEl Niño releases heat into the atmosphere and increases subsequent global temperatures. For example, global average temperatures for 2016 were around 1.1°C above preindustrial values and the strong El Niño episode of 2015/2016 partly contributed. However, researchers have concluded that the warming from El Niño is only accountable for about 0.2°C of this overall figure.\nEl Niño and climate change\nBecause of the large event-to-event variations of El Niño, we don’t have enough past years of observations to show a clear impact of climate change on its properties. However, there is now some evidence that the effects of El Niño on rainfall may increase in the future and that we may even see more extreme ENSO events, but these remain active research questions.']	['<urn:uuid:5c500c11-408a-4bc2-b27d-e1b63643da23>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T00:20:10.754925	9	80	739
45	can power company dig in my backyard	Yes, if your property has a utility easement, the utility company has the right to enter your land to perform maintenance, repairs, or alterations on their equipment or services. This includes accessing and repairing utility lines that run through, over, or under your land.	['If your property is subject to a utility easement, it means that a utility – private or government – has a non-ownership right to use your property for the purposes of providing and maintaining the specified utility. That probably sounds a bit convoluted, so let’s break it down.\nUtility easements are the path through which things like power, natural gas, water, and wastewater travel. A utility taking an easement doesn’t technically convey any ownership of the land. It’s an “interest in real property,” which is a fancy way of saying that an easement takes rights to use the land without actually owning it. These can be temporary or permanent, though in some cases, the difference is minimal.\nCan the utility company come onto my property?\nYes, that’s an essential part of utility easements. If equipment or services are damaged, require maintenance, or need altering, utility easements give utilities the right to enter your land to do their work. If there’s trouble with gas, electric, or other utility lines that run through, over, or under your land, the easement gives the utility company the right to access and repair them. An easement may also enable them to erect utility poles or towers on your property.\nAn overview of easements\nThe easiest way to understand the different kinds of easements is to think of it as “the right of another to use your land for a specified purpose.”\nYou still own the land, but you can’t just do anything you want with it anymore. You’re not allowed to interfere with the easement holder’s use and enjoyment of the easement. On the other hand, the easement holder – the utility in this case – can only use the land for the stated purpose of the easement.\nThere are plenty of examples of non-utility easements, such as shared driveways or driveway easements across your land so a neighbor can access private property. You might grant someone an easement to come fish in your pond. Utility easements are another animal entirely.\nPermanent utility easements\nA permanent utility easement (PUE) is a permanent right to access and use your land for utility service purposes, including maintenance, upgrades, and so on. The easement will remain even if you sell the property; it “runs with the land.” When a government entity like GDOT takes a right of way, they take the property. A permanent utility easement means they’re taking rights to use the property for a specific purpose, permanently. You still own it. They just get to use it.\nExamples of PUEs include power line easements across your property, drainage easements, or even things like buried utilities such as gas, power, or data lines.\nNote: Utility easements are almost always permanent.\nWhen do permanent utility easements happen?\nUtility easements are often taken when there is a road widening project. Existing power lines must be moved off the new road and onto your property, for example. Road work can push other utilities like gas lines, cable, and telephone poles onto your property as well. Unlucky property owners may lose land to GDOT for a road and then lose even more to utility easement relocation!\nIf the county or city needs to install water or sewer service, they may also grab an easement. These installations can be very destructive to properties in their path, as they are often engineered without regard to things like fences, trees, sheds, and clear spaces. Another common example of a PUE is when a city or county is installing sewer or water lines. Sewer lines aren’t always even servicing the property they cross but may result in large, unsightly manhole covers for maintenance and repair access.\nWhat are Georgia’s utility easement laws?\nThe law allows GDOT, municipalities, and utilities to take easements on your land. The rules governing a utility easement are typically spelled out in a PUE deed, and can give the utility the ability to:\n- Enlarge the utility – install larger pipes, for example\n- Repair and maintain the utility\n- Allow new improvements to be made to the utility\n- Allow heavy trucks to park in the easement area\n- Disallow the property owner from parking in the easement area\n- Forbid the property owner from excluding trespassers\n- Prevent the property owner from improving or constructing anything that impedes access to the easement at any time\nGDOT, counties, cities, and utility companies may have different wording in their Permanent Utility Easement deed language, but they will be broadly similar.\nThese agreements are usually a mess of legalese and often take a great deal more than is necessary. We highly recommend hiring an experienced eminent domain attorney to review any easement agreement before you sign it. You may be signing away more than you think, and you may be entitled to compensation.\nDoes my property already have a utility easement?\nTo discover existing easements on your property, you’d want to do a title search, which can be very complex and time consuming. This should have been discovered when you purchased the property, as an attorney or title company would have done the necessary research. Easements typically transfer with the land, so if a prior owner agreed to an easement, you’re stuck with it.\nThere are, of course, smaller and more common easements that service almost every property. If you have cable and internet, you probably already have an easement for those services. But that does not mean utility companies can abuse their rights.\nHow might utility companies abuse an easement?\nEasements are, ideally, very clearly defined. Georgia law limits their scope in some cases, but the PUE itself should also specify use and area. Also note that easement agreements are typically very broadly worded, giving the utility more leeway in their activities.\nIf the utility exceeds its authorized area or activities (though unlikely), it’s basically trespassing on your private property. If the utility negligently damages or destroys unrelated property in the exercise of their easement, they may owe you compensation.\nNote: Developments that want to use an easement for additional traffic may exceed the easement.\nCan I remove a utility easement from my property?\nProbably not. In the case of a PUE, the language in the easement agreement is almost always permanent. Even a temporary easement may last an uncomfortably long time and will be difficult, if not impossible, to end before its allotted expiration.\nThat said, in rare cases, the utility may abandon an easement it no longer needs.\nWhat are my utility easement rights?\nYou are entitled to use and enjoyment of the easement area, but within the limitations defined by the easement. Many homeowners and property owners get frustrated because they’re not sure what they can and can’t do. Perhaps they have a large easement area and either don’t want to maintain the area or conversely worry that it might become desolate and useless.\nThe truth is, it’s still your property regardless of the easement, and you must maintain it just as any property owner. And it need not become a wasteland.\nDepending on the type of PUE and the language in the deed, you may be able to fence the area, plant a garden, install flower beds or decorative bushes, or do other things that don’t interfere with the easement. The risk, in these cases, is that if the easement holder needs to damage or tear up these improvements for some reason, they’re probably not required to compensate you.\nHow do I calculate the value of a utility easement?\nYou may be entitled to compensation for the easement taking. What you may be owed depends on how much the easement diminishes the value of, and your enjoyment of, the land. Remember that you retain ownership, so you’re not technically losing the land where they’re taking the easement. You’re simply losing some control over it.\nEvery property, and therefore every easement, is unique. It follows that the amount of compensation you should seek for an easement is likewise variable and depends on many unique circumstances.\nEffects of easements: Setbacks, acreage, and tax value\nA utility easement affects your property in some ways you might not think of. Here are a few examples.\nSetbacks – Most counties and municipalities have regulations that govern the construction of “improvements” – anything to be built on the land. One of these regulations may be a “setback” requirement; a specific distance an improvement must be from the property line. For example, the county may require that a fence be built at least 10 feet away from property lines.\nThe good news is that easements typically do not count against setback ordinances. The bad news is that the easement may not allow you to build what you want, regardless. Consult the PUE deed.\nAcreage – Sometimes, size matters. Some property owners, commercial and residential, are deeply vested in the acreage because it greatly affects the value of the property. Commercial owners may need a certain amount of acreage to account for runoffs and permeable surface requirements, while a residential property owner may want to add another building and need the acreage to meet zoning requirements.\nThe good news is that easements typically do not reduce acreage. If you have 0.65 acres with a 0.13 acre easement, you generally still have 0.65 acres. The bad news, again, is that your desired use may not be permitted by the PUE, making the acreage size moot.\nTax Value – While taxes are going to vary depending on location, an easement may allow you to reduce that tax burden. If your land is worth less due to the existence of a utility easement, you may not have to pay as much tax on it, though this won’t be true in every case. This can get really complicated, and is something you should discuss with your eminent domain attorney.\nAppraisals, value, and easements\nI have seen government appraisers calculate anywhere between 30% and 90% of the full land value for utility easement compensation.1\nHow can an eminent domain lawyer help me?\nWhen you’re faced with a legal process and the accompanying legal document that will fundamentally change your rights, it’s a good idea to have a lawyer on your side. For many people, property is the largest single investment they’ll make in their lives. For businesses, property rights can make or break multimillion-dollar projects.\nOur eminent domain team has over 85 years of combined experience and can help you:\n- Translate legalese into plain language. Easement agreements are packed with dense language. They may hide additional infringements in hard-to-understand jargon. They can bundle multiple easements into one, and you might not realize it. We’ve read hundreds and hundreds of easement documents. We know what they mean, and we can look for these hidden add-ons so you know exactly what you’re agreeing to.\n- Fight for maximum compensation. From the government to the local utility company, others have the right to take easements on your land. However, as the property owner, you should get fair compensation for what they take. We can help calculate the current and future damage to the value of your land and help you seek the highest possible compensation. And if the agreement is unfair or the compensation offer unreasonable, we can fight on your behalf – even in court if needed.\n- Leverage a network of professionals to build your case. We’ve cultivated a list of non-legal professionals with very specific expertise. We know who to call, for what, and when – and who can get things done for you. From eminent domain appraisals to engineers to surveyors and environmental professionals, we have the human resources to help us determine the true value of your land.\nLearn 2 More: About Our Formidable Team\nGet help grappling with utility easements\nThis article is not an all-encompassing list of the issues you may face if someone takes a utility easement on your property. If a utility company or government project burdens your private property with an easement, whether it’s for water pipes or power lines, we strongly encourage you to contact an attorney.\nOur team handles eminent domain cases, large and small. From a small residential plat to intricate, vast commercial property, we can evaluate your case and give you an experienced perspective on the taking. We have former state DOT attorneys on our team who are well-versed in the tactics utilities use, and how to try to ensure you’re treated fairly. Reach out to the Georgia Eminent Domain Law Firm by calling 1-888-391-1339 or filling out our contact form today.']	['<urn:uuid:4a02e96f-831d-4a94-b6b6-59eba792374f>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T00:20:10.754925	7	44	2094
46	Could you explain why forming good money habits early in life provides significant long-term advantages?	Developing financial habits early provides three key advantages. First, habit acquisition is easier when you're young, making it nearly impossible to break those good money practices once established. Second, making financial mistakes early gives you ample time to recover and learn from them, preventing future repetition. Third, there's the power of compound interest - investing money early allows it to grow exponentially over decades, while also preventing compound interest from working against you in cases like debt.	['Your financial health and potential for building wealth are dependent on your habits. Short of winning the lottery or receiving a massive inheritance, the best financial habits involve those little decisions you make every day and every week.\nDeveloping these habits all at once is problematic. First, there is a steep learning curve. Second, not all strategies are equally effective at every age. The best approach is to gradually master your financial habits as you grow older. There are good money habits to build at every stage of your life.\nIn General: The Earlier, the Better\nFor the most part, the earlier you learn these financial habits, the better. If you’re able to develop habits ahead of your age, you’ll stand to benefit. There are three reasons for this:\n- Habit acquisition. It’s easier to learn new things and build good habits when you’re young. If you establish good money practices early enough, it will be nearly impossible to break those habits.\n- Mistake adjustment. If you employ a habit but make a mistake in its execution, it can destabilize your financial track. Making that mistake early gives you ample time to recover from that mistake and learn from it. That way, you never repeat it again.\n- There’s also the power of compound interest to consider. Investing money early allows compound interest to grow that money exponentially over decades. It also prevents compound interest from working against you. A good example is in the case of debt.\nIn Your Teens\nYour teenage years won’t come with much responsibility or many opportunities to make a significant income. Accordingly, there aren’t many financial habits you’ll need to focus on. These three are a good start:\n- Saving money. Saving money is one of the best habits to learn early. In your teens, you’ll be tempted to spend every cent of your incoming paychecks. However, learning to squirrel away at least a portion of your earnings will always be beneficial.\n- Tracking your spending. This is also the best time to start habitually tracking your spending. Instead of buying what you feel like, when you feel like it, write down how much you spend in each of several categories. Then, compare expenditures with income. This will provide you with the budgeting skills necessary to last a lifetime.\n- Opening and using. You probably don’t “need” a credit card in your teens. However, it’s useful to have as an emergency option. Plus, it’s ideal for starting to build credit, which you’ll need in the future. Consider opening a savings, checking, and credit card account in your name. Then, focus on managing them responsibly.\nIn Your 20s\nIn your 20s, you’ll be out of school and ready to start your career so you need these financial habits:\n- Checking your credit score. Checking your credit score is free. Therefore, it’s good to get in the habit of checking it regularly. Knowing your credit score is valuable for making big-ticket financial decisions. It can direct you to weaknesses in your credit report to work on improving them.\n- Contributing to a 401(k) or similar program. If your company offers a 401(k) or a similar investment program with a company match, take advantage of it. Company matches are essentially free money.\n- Mastering your student loans. The average college student graduates with $30,000 in debt. If you don’t start addressing it now, the power of compound interest will make that debt even harder to manage. You don’t have to pay your debt down right away. However, you should have a solid long-term plan in place.\n- Minimizing your credit card balances. Have one or two credit cards you regularly use. But, it’s important to get in the habit of keeping those balances low. If you accumulate too much debt, it could take over your life.\n- Living below your means. This is the best way to generate more savings over the long term. Since you’ll be spending far less than you make, you’ll naturally end up with more money to save or invest every month. Opt for less expensive housing and save money on fees and subscriptions.\n- Setting short-term and long-term goals. Get in the habit of setting and following both short-term goals (like saving up for a home down payment) and long-term goals (like investing $5,000 a year). With good goal planning and execution, all your other financial efforts will become easier.\nIn Your 30s\nOnce you’re in your 30s, you’ll have established career momentum to do these:\n- Establishing a comprehensive emergency fund. You should have an emergency fund in your 20s. However, by your 30s, that fund should be comprehensive. That means big enough to cover several months of expenses in case you lose your job or face some other catastrophe.\n- Setting a course for retirement. This is when you’ll need to start thinking about your retirement goals. When do you want to retire? What accounts will you rely on to do it?\n- Taking advantage of your credit. You’ve built and checked on your credit for the past decade or two. Now’s the time to start taking advantage of it. Buy a house you can afford or open new lines of credit to finance your business idea.\n- Learning the value of insurance. Understand the value of insurance and take advantage of it for your financial interests. For example, you’ll want a good policy to cover your health, home, car, and other important assets. Plus, you may want to choose a different deductible or coverage policy that best suits your needs.\n- Renting or buying (as appropriate). Know the advantages of renting versus buying in your area. There may not be a need to rush into buying a home or you may miss out on significant equity by renting. Every location is different.\n- Navigating marriage and children. Consider the financial implications of marriage (if you’re planning to get married) and the expenses associated with raising children. Planning a family responsibly can mean the difference between affording a comfortable lifestyle and succumbing to debt.\n- Planning for your children’s futures. If you’re planning to send your kids to college, start thinking about college savings (or a similar savings strategy for your children). For example, you may choose to open a 529 college savings plan and contribute regularly to it.\n- Investing as a monthly expense. Think about investments and retirement savings as a monthly expense. These become necessary, regular expenditures for the sake of your future. Don’t let your other living or entertainment expenses distract you.\nIn Your 40s\nIn your 40s, you’ll have mastered the vast majority of important financial habits, but there are still a few to learn:\n- Rebalancing your portfolio. Now, you should be in the habit of routinely rebalancing your portfolio. This includes adjusting your assets to favor the current market conditions or to help gradually reduce risk as you prepare for retirement. Slowly move toward a bond-heavy distribution of assets in your investment portfolio.\n- Prepping for divorce. No matter how happy you are currently, there’s a significant possibility that your marriage will end in divorce. This event can be financially devastating to one or both parties. It’s imperative that you plan how to handle those expenses.\n- Intelligently managing your assets. If you’ve had good financial habits for the past few decades, you should have significant assets to manage. These include properties, vehicles, and other investments. Make sure you’re managing them intelligently and improving their resale costs. Also, cite them properly on your taxes and sell them when appropriate.\n- Splurging when appropriate. Retirement isn’t everything. By now, you should know the difference between a healthy splurge and reckless spending. However, don’t be afraid to pamper yourself every once in a while. Start doing more things that make you happy.\nBeyond Your 40s\nAt this point in your life, your efforts should focus on maintaining the status quo and learning from your past mistakes. Additionally, it’s about concentrating your savings and investments on retirement prep. By the time you hit 50, all these financial habits should have prepared you for a stable future. There should be enough resources and experiences to help you achieve long-term goals.\nDon’t regret your financial decisions. Take the time to learn and master these money habits as early as possible. And, be grateful for the discipline you exercised.']	['<urn:uuid:225ca937-41c3-4d20-9003-8564149c21e4>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T00:20:10.754925	15	77	1400
47	I'm trying to explain to my kid where we are in space - what is the Milky Way and where is Earth located in it?	The Milky Way is our home galaxy, which is a spiral galaxy about 100,000 light-years wide. Earth is located in a part called the Orion Arm, which is a 15,000-light-year-long branch of stars that forms a curved path between two major spiral arms: the Perseus Arm and the Sagittarius Arm. Our Sun sits on this Orion Arm, which was once thought to be unimportant but has recently been discovered to be a major arm in its own right. We are located about 27,190 light years from the center of the galaxy, where a supermassive black hole called Sagittarius A* resides.	['Adapted for publication and use by the Big History Project from the essay “Galactic Position System,” by Sarah Scoles, published on aeon.co.\nExtended learning opportunity: Curious to dig even deeper into this big topic? Resident astrophysicist Cameron Gibelyou, educator at the University of Michigan and BHP’s support for all “star stuff” held an exchange October 8–12. You can check out the archive of this conversation here and learn more!\nNote from the BHP Team: In the Big History Project curriculum, we discuss how our understanding of the origin and structure of the Universe has changed over time. This topic necessarily involves asking: Where are we in the Universe? That is, we humans. How do we fit into the bigger picture? How do we know? And does this matter?\nWe’re thrilled to add to the course this article on the galactic body we call home – the Milky Way. Among the questions the article raises: Do we know where we are in the Milky Way? How have new scientific insights and discoveries informed this knowledge? Does it matter that we can point to the Milky Way and locate Earth’s position?\nWe hope you enjoy the read.\nIf you’re eager for more, you can read one BHP teacher’s response to the article (in which he argues that yes, it matters that we know where we are in the Milky Way)..\nDo You Know Where You Are in the Milky Way?\nThe artist Jon Lomberg kneels next to a plant in the Galaxy Garden. He points to the cheap earring that pierces a speckle on one of the plant’s leaves. This, he says, pointing to the stud, represents the Sun. He is standing 30 feet from the middle of the 100-foot-wide garden in Hawaii. The garden is an accurate scale model of the Milky Way galaxy, where every step takes you 2,000 light years from where you stood before. Here, the speckles on the leaves of the plants stand in for stars. The plants are arranged in spiral arms that reach out from the garden’s center, where a fountain plays the part of an enormous black hole. Along the spiral limbs are colorful plants where nebulae, gas, and dust would be.\n- The Galaxy Garden resides in the Paleaku Peace Sanctuary in Honaunau, Hawaii. Photo courtesy and © Simon Bell.\n- The gold sphere represents a magnified Solar System. Photo courtesy and © Simon Bell.\nWhen Lomberg stands up, he looks down at our yellow star. “If you put a basketball-sized sphere around where Earth is, everything you would be able to see in the night sky would be in that sphere,” he says, adding that other stars are too distant and dimmed by dust to be seen. “When you see the whole garden,” Lomberg continues, “without any math, or any need for explanation, the scale becomes apparent.”\nIt is difficult to picture ourselves in the Milky Way. We have no problem placing ourselves somewhere like California, or the United States. Even second-graders can point to a spot on a globe and say: “I live there.” And by third grade, kids can tell you where Earth is in relation to other planets, and that the planets orbit the Sun. Beyond that? Everything else is so distant that we often lump it all into the “out there” category, so that Pluto and Polaris might as well be in the same spot. Perhaps that’s why, as Lomberg says, many people think a galaxy is the same as a solar system.\nOnly in the past few decades have we become scientifically and technologically advanced enough to know where we are located in our 100,000-light-year-wide galaxy. But making sense of the galaxy isn’t easy for anyone, including astronomers. We were born and will live our lives inside the Milky Way. Imagine being trapped inside a building and trying to determine what the building looks like from the outside.What shape is the building? When was it last painted? What kind of roof does the building have? You can count the number of bathrooms in the building, peer out of a window and describe what the building is made of. But putting those pieces together into an entire picture is like trying to understand your brain by using your brain. You’ll never have a complete view of what it looks like from the outside.\nNow, imagine you’ve never even heard of something called a building. That’s the situation astronomers found themselves in until the 1920s. The Milky Way has always streaked our sky, a stripe spreading from horizon to horizon. Its name comes from its fuzzy, lane-like appearance. But it wasn’t until Galileo Galilei trained his telescope on it 400 years ago that we knew it was made of stars.\nAs centuries passed, telescopes grew larger, allowing astronomers to detect fainter objects. Looking deeper into space than ever before, astronomers were able to see spiral-shaped clouds. Thinking that they were just clouds of gas within our own galaxy, astronomers called them spiral nebulae. After all, if you don’t know an object’s true size, it might be small and nearby, or distant and huge. And in the late nineteenth century, as far as we knew, our cosmos was the whole cosmos. We had learned long ago that the Earth is not the hub of the Solar System, but it wasn’t until we were able to see the spiral clouds that we understood that our galaxy was one of many.\nThe Milky Way looks and behaves like any other spiral galaxy, and the Universe contains tens of billions of those. Wherever we look in the Universe, we see the laws of physics at work. These laws cause gas to be molded into stars, and stars into galaxies. Peering out of our cosmic window, we can observe these structures to learn about our own.\nA Hubble Space Telescope image of galaxy UGC 12158, which is thought to resemble the Milky Way in appearance. Photo by ESA/Hubble & NASA. Public domain.\nIn fact, we know that from the side, the Milky Way is a thin disk with a bulge in the middle. From above or below, it resembles a hurricane, with spiral arms where rain bands would be. Four of these arms curl from the eye of the storm. The Perseus Arm and the Sagittarius Arm wrap around our side of the Milky Way, while the Scutum-Centaurus and Norma-Cygnus arms form a near-mirror-image on the other side.\nA stream of stars forms a curved path between Perseus and Sagittarius. It is on this 15,000-light-year-long branch, called the Orion Arm, that our Sun sits. For a long time, astronomers believed the Orion Arm was unimportant. Recently, they discovered our arm is a major arm in its own right, like Perseus and Sagittarius.\nA detailed annotated artist’s impression showing the structure of the Milky Way. Photo by NASA/JPL-Caltech/ESO/R. Hurt. Public domain.\nThat’s a pretty basic discovery to make so late in the game. But nearly all knowledge of galactic limbs comes from invisible parts of the light spectrum. Unlike the study of visible astronomical objects, which began with Galileo’s telescope, the study of these invisible parts is just beginning. Scientists didn’t discover the spiral arms until the 1950s, with the first radio surveys of hydrogen gas.\nAll spiral arms lead to the Milky Way’s center. There, a bar of stars in long, looping orbits forms a tapered shape, like the center of a cat’s eye. This eye is 27,000 light years across – nearly as wide as the distance between Earth and the center of the galaxy – and pokes out of the galaxy’s plane at a 45-degree angle. Above and below this midsection, nearly 200 spheres orbit. Each sphere is packed with hundreds of thousands of ancient stars.\nAt the very center of the galaxy, 27,190 light years from Earth, sits the Milky Way’s most significant object. Our fastest spacecraft would take 475 million years to arrive at its boundary.Although this supermassive black hole, called Sagittarius A*,seems to be making every atom its satellite, this is an illusion. Everything orbits the center of mass, which is the point where the gravitation from all directions is the same. Just as the Earth spins on its axis and revolves around the Sun, the whole Milky Way whirls around the center of mass. Even Sagittarius A*, very slightly offset from the galaxy’s true center, does a delicate jig.\nThe region around the black hole is dense with dust, which blocks and scatters visible light. Because no optical telescopes could see to the center, astronomers did not find evidence of Sagittarius A* until 1974, when radio telescopes became powerful enough to pinpoint the precise location of a strangely compact source of radiation at the center of the galaxy.\nThe center of the Milky Way galaxy, with the supermassive black hole Sagittarius A*located in the middle, is revealed in these images. Photos by: X-ray: NASA/UMass/D.Wang et al., IR: NASA/STScI. Public domain.\nIf you zoom out of the Milky Way entirely, you’ll find 26 known dwarf galaxies orbiting us. It’s likely that there are 1,000 more out there, some nearly invisible, made mostly of dark matter or cold hydrogen gas. Beyond them lie the other members of the Local Group. Fifty-four of these galaxies are known. They travel together through space, and their gravitational center lies between our galaxy and its nearest large neighbor, the Andromeda galaxy. Around the Milky Way and Andromeda, 12 other large galaxies form a protective circle known as the Council of Giants.\nThe Local Group is part of the Virgo Supercluster, which contains more than 100 galaxy groups and clusters. But even this structure is cosmically insignificant. Just as Earth is one of many planets, the Sun is an average star.Just as the Milky Way is a predictable spiral galaxy, the Virgo Supercluster has 10 million massive relatives.\nThat number is so large that it hardly means anything to most of us. However, imagine that the Galaxy Garden really is the galaxy. The visible Universe would be the size of Earth, and it would be immaculately landscaped with hundreds of billions of Galaxy Garden clones, all congregating in clusters and superclusters, with long filaments of gas and dark matter connecting the superclusters.\nAn illustration of the location of Earth in the Universe, in a series of nine frames. Image by Andrew Z. Colvin – own work, CC BY-SA 4.0.\nEven if we can’t imagine the scale of the Universe, we’re still able to situate ourselves within it. Lomberg envisions a world in which every third-grader knows where the “You are here” label goes on a galactic map and can give you directions from there to the Scutum-Centaurus Arm. ”The Milky Way is as important to your self-identification as the fact that you live on planet Earth, or you live in the Solar System,” he says. “We may be insignificant, but we figured out that we’re in a galaxy. For insignificant nothings, that’s pretty good.”\nAbout the author: Sarah Scoles writes about space science for a variety of publications, including Slate, Popular Science, and Discover. She lives in San Jose.\nCover image: Milky Way Panorama – The Pinnacles Desert, Western Australia, by Trevor Dobson. CC BY-NC-ND 2.0']	['<urn:uuid:83e489d6-4e8f-4281-a0d9-56a6e66cc497>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T00:20:10.754925	25	100	1871
48	orionids peak time rates versus quadrantids viewing difficulties	The Orionids reach maximum activity on October 21/22 with rates increasing to 15-20 meteors per hour at peak, offering a broad maximum with similar rates on nights before and after maximum. In contrast, the Quadrantids, despite being one of the top 3-4 best meteor showers of the year, have a very narrow peak lasting only about 12 hours, making them easy to miss. Additionally, the Quadrantids occur in early January when cold weather and post-holiday exhaustion often keep observers indoors.	['During this period the moon reaches its first quarter phase on Tuesday October 20th. On that date the moon will be located 90 degrees east of the sun and will set near 2300 (11pm) local daylight saving time (LDT) as seen from mid-northern latitudes. This weekend the waxing crescent moon will set during the mid-evening hours allowing perfect viewing the more active morning hours. The estimated total hourly meteor rates for evening observers this week is near 4 as seen from mid-northern latitudes (45N) and 3 as seen from tropical southern locations (25S). For morning observers the estimated total hourly rates should be near 30 as seen from mid-northern latitudes (45N) and 20 as seen from tropical southern locations (25S). The actual rates will also depend on factors such as personal light and motion perception, local weather conditions, alertness and experience in watching meteor activity. Rates during the evening hours are reduced during this period due to moonlight. Note that the hourly rates listed below are estimates as viewed from dark sky sites away from urban light sources. Observers viewing from urban areas will see less activity as only the brightest meteors will be visible from such locations.\nThe radiant (the area of the sky where meteors appear to shoot from) positions and rates listed below are exact for Saturday night/Sunday morning October 17/18. These positions do not change greatly day to day so the listed coordinates may be used during this entire period. Most star atlases (available at science stores and planetariums) will provide maps with grid lines of the celestial coordinates so that you may find out exactly where these positions are located in the sky. A planisphere or computer planetarium program is also useful in showing the sky at any time of night on any date of the year. Activity from each radiant is best seen when it is positioned highest in the sky, either due north or south along the meridian, depending on your latitude. It must be remembered that meteor activity is rarely seen at the radiant position. Rather they shoot outwards from the radiant so it is best to center your field of view so that the radiant lies at the edge and not the center. Viewing there will allow you to easily trace the path of each meteor back to the radiant (if it is a shower member) or in another direction if it is a sporadic. Meteor activity is not seen from radiants that are located below the horizon. The positions below are listed in a west to east manner in order of right ascension (celestial longitude). The positions listed first are located further west therefore are accessible earlier in the night while those listed further down the list rise later in the night.\nThese sources of meteoric activity are expected to be active this week.\nThe Gamma Piscids (GPS) were first discovered by A. K. Terentjeva in a study of fireball sources. This is a weak shower active from October 14 through the 21st with maximum activity occurring on the 17th. At maximum the radiant is located at 01:10 (017) +17. This position lies in central Pisces, 5 degrees northwest of the faint star known as Eta Piscium. Rates of less than 1 per hour are expected, even at maximum. With an entry velocity of 21 km/sec., the average Gamma Piscid meteor would be of slow velocity.\nThe Southern Taurids (STA) are currently active from a radiant located at 02:36 (039) +12. This position lies in southern Aries, 3 degrees northwest of the 4th magnitude star known as Mu Ceti. These meteors may be seen all night long but the radiant is best placed near 0100 local daylight time (LDT) when it lies on the meridian and is located highest in the sky. Rates at this time should be near 3 per hour regardless of your location. With an entry velocity of 29 km/sec., the average Southern Taurid meteor would be of slow velocity.\nThe Eta Taurids (ETT) were discovered by Sirko Molau and Juergen Rendtel using data from the IMO video network. These meteors are active from Oct 15-29 with maximum occurring on the 24th. The radiant is currently located at 03:20 (050) +22. This area of the sky is located in eastern Aries, 5 degrees southwest of the bright open cluster known as the Pleiades. Current rates should be near 1 per hour as seen from the northern hemisphere and less than 1 as seen from south of the equator. With an entry velocity of 45 km/sec., most activity from this radiant would be of medium-swift speed.\nThe Orionids (ORI) reach maximum activity on Wednesday night/Thursday morning October 21/22. Unlike most major showers the Orionids offer a broad maximum with similar rates on the nights prior and after maximum. This weekend the radiant lies at 06:12 (093) +16, which places it in northeastern Orion, 5 degrees west of the 2nd magnitude star known as Alhena (Gamma Geminorum). This area of the sky is best placed near 0500 LDT, when it lies highest above the horizon. Rates this weekend would be near 10 per hour no matter your location. Rates should increase to 15-20 per hour at maximum With an entry velocity of 67 km/sec., most activity from this radiant would be of swift speed.\nThe Epsilon Geminids (EGE) are active from a radiant located at 06:48 (102) +27, which places it in northwestern Gemini, 3 degrees northeast of the 3rd magnitude star known as Mebsuta (Epsilon Geminorum). This is fairly close to the Orionid radiant so care should be taken to separate these two sources. The best way to do this is to include both radiants within your field of view so that meteors can be easily traced back to their source. This area of the sky is best placed in the sky during the last hour before dawn, when it lies highest above the horizon in a dark sky. Current rates should be near 2 per hour as seen from the northern hemisphere and 1 when view south of the equator. This should increase to near 3 per hour at maximum (Oct 22). With an entry velocity of 70 km/sec., most activity from this radiant would be of swift speed.\nThe October Lyncids (OLY) were discovered by Peter Jenniskens and verified by IMO video data. This weak shower is active from October 16-24 with no distinct night of maximum activity. The date listed in the shower table is the midpoint of the activity curve. The radiant is currently located near 07:16 (109) +53. This position lies in a remote area of central Lynx, halfway between the stars Omicron Ursae Majoris and Menkalinan (Beta Aurigae). This area of the sky is best placed in the sky during the last hour before dawn, when it lies highest above the horizon in a dark sky. Current rates should be less than 1 per hour no matter your location. With an entry velocity of 68 km/sec., most activity from this radiant would be of swift speed.\nThe Tau Cancrids (TCA) are active from October 9-25, with maximum activity occurring near the 17th. This shower was discovered by Peter Jenniskens and verified by IMO video data. The radiant currently located at 09:00 (135) +30. This position lies in northern Cancer, close to the 4th magnitude star known as Iota Cancri. This area of the sky is best placed in the sky during the last hour before dawn, when it lies highest above the horizon in a dark sky. Current rates should be near 1 per hour as seen from the northern hemisphere and less than 1 as seen from south of the equator. With an entry velocity of 69 km/sec., most activity from this radiant would be of swift speed.\nThe Leonis Minorids (LMI) are active from October 17-27 with maximum activity occurring on October 22nd. This radiant is currently located at 10:24 (156) +38, which places it in northeastern Leo Minor, close to the position occupied by the fourth magnitude star Beta Leonis Minoris . The radiant is best placed just before dawn when it lies highest in a dark sky. This shower is better situated for observers situated in the northern hemisphere where the radiant rises far higher into the sky before the start of morning twilight. Rates at maximum should be near 4 per hour for those in the northern hemisphere and 2 per hour as seen south of the equator. At 60km/sec., the average Leonis Minorid is swift.\nAs seen from the mid-northern hemisphere (45N) one would expect to see approximately 10 sporadic meteors per hour during the last hour before dawn as seen from rural observing sites. Evening rates would be near 2 per hour. As seen from the tropical southern latitudes (25S) morning rates would be near 7 per hour as seen from rural observing sites and 2 per hour during the evening hours. Locations between these two extremes would see activity between the listed figures.\nThe list below offers the information from above in tabular form. Rates and positions are exact for Saturday night/Sunday morning except where noted in the shower descriptions.\n|SHOWER||DATE OF MAXIMUM ACTIVITY||CELESTIAL POSITION||ENTRY VELOCITY||CULMINATION||HOURLY RATE||CLASS|\n|RA (RA in Deg.) DEC||Km/Sec||Local Daylight Saving Time||North-South|\n|Gamma Piscids (GPS)||Oct 17||01:10 (017) +17||21||01:00||<1 – <1||IV|\n|Southern Taurids (STA)||Oct 10||02:36 (039) +12||29||02:00||3 – 3||II|\n|Eta Taurids (ETT)||Oct 24||03:20 (050) +22||45||02:00||1 – <1||IV|\n|Orionids (ORI)||Oct 22||06:12 (093) +16||67||06:00||10 – 10||I|\n|Epsilon Geminids (EGE)||Oct 22||06:48 (102) +27||70||06:00||2 – 1||II|\n|Oct. Lyncids (OLY)||Oct 20?||07:16 (109) +53||68||08:00||<1 – <1||IV|\n|Tau Cancrids (TCA)||Oct 17||09:00 (135) +30||69||09:00||1 – <1||IV|\n|Leonis Minorids (LMI)||Oct 22||10:24 (156) +38||60||10:00||2 – 1||II|', 'Tonight brings the peak of the best meteor shower you have probably never seen. The best showers of the year are almost always August’s Perseids and December’s Geminids. Number 3 and 4 are usually October’s Orionids or tonight’s shower, the Quadrantids.\nThe reason I say the Quads are probably the best shower you’ve never seen is two-fold. First as a northern shower, they take place in the dead of winter and only a few days after New Year’s. If the exhaustion from the Holiday’s season doesn’t keep most people inside then the cold definitely will. Also unlike most showers which have broad peaks which last a few days, the peak of the Quads is very narrow. Even if you are observing on the peak night, you can miss much of the show if you are off the peak by only 12 hours.\nThe International Meteor Organization predicts this year’s Quads peak to take place at ~19:30 UT on the 3rd which suggests the best viewing will be in Asia. But… predicting the peak time for this shower is always difficult so pretty much anywhere on Earth may see the best. The only way to know is to get out and look.\nBob Lunsford has posted an excellent guide to observing the Quads at the American Meteor Society (AMS) website. Please check it and the AMS (of which I am their Secretary) out.\nFor many years, astronomers were uncertain as to which comet caused the Quadrantids. No known comets was visible on a similar orbit even though the narrowness and strength of the meteor stream suggested it was created recently. We now know that the asteroid (196256) 2003 EH1 is the likely parent body of the Quads. Even though today it appears as nothing more than an asteroid it was a comet in the past and a rather bright one when seen in 1490. Earlier this year I observed 2003 EH1 with the Vatican Obs/Univ. of Arizona VATT 1.8-m as seen in the image below.\n[I forgot to add that yesterday’s Earth impacting asteroid, 2014 AA, is not related to the Quadrantid meteor shower. The asteroid and the meteors have very different orbits and the fact that they both intersected the Earth on the same day (or two) is not only a coincidence but shows just how crowded space is with debris.]\nCo-added R-band image of the Quadrantids parent body (196256) 2003 EH1 taken on 2013 Sep. 14.25 UT with the Vatican VATT 1.8-m. At the time the object showed no cometary activity. Credit: Carl Hergenrother/Vatican Obs./University of Arizona.\nLast night was another clear night in Tucson. Though 28 meteors were detected, only 2 were possible Quads. Tonight should see a huge increase in Quadrantid meteors.\nObs Date(UT) Time TOT SPO ANT AHY COM DAD DLM JLE QUA\nSAL 2014-01-02 12h 33m 28 23 1 0 0 0 2 0 2\nSAL - SALSA3 camera in Tucson (Carl Hergenrother)\nTime - Total amount of time each camera looked for meteors\nTOT - Total number of meteors detected\nSPO - Sporadics (meteors not affiliated with any particular meteor shower)\nANT - Antihelions\nAHY - Alpha Hydrids\nCOM - Coma Berenicids\nDAD - December Alpha Draconids\nDLM - December Leonis Minorids\nJLE - January Leonids\nQUA - Quadrantids']	['<urn:uuid:22c57e10-5532-45ee-8a94-d186257d5358>', '<urn:uuid:5ba09323-2426-44f4-bf04-d9721bad1cf6>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T00:20:10.754925	8	80	2149
49	poland main tourist cities and impact of world war 2 on jewish population	Poland's major tourist destinations include Warsaw, Krakow, Gdansk, and Torun, known for their medieval architecture and cultural attractions. However, these cities also witnessed devastating events during World War II. The country was home to 3.5 million Jews before the war, but this population was tragically decimated during the Nazi occupation. Many cities, particularly Warsaw, were largely destroyed during the war but have since been rebuilt. Today, Poland's Jewish population has drastically declined to only 10,000-20,000 people, though the country preserves the memory of its Jewish heritage through various monuments and cultural sites.	['Life has actually not been simple for Poland, an eastern European nation that has actually been attacked and also ruined often times over the centuries. The nation endured strongly in The second world war when much of its residents, including its big Jewish populace, were rushed off to Nazi prisoner-of-war camp.\nThe Polish spirit, nonetheless, declined to pass away and also today the nation integrates middle ages style with dynamic social tasks to satisfy the requirements of modern-day travelers. A review of the finest locations to go to in Poland:\nThe middle ages community of Malbork, possibly much better recognized by the German name of Marienburg, is most widely known for its castle, which was gotten constructed in the 13th century by the Knights of the Teutonic Order as their head office, Europe’s biggest Gothic citadel is called after the Virgin Mary, the tutelary saint of the city and also castle.\nThe castle is really 3 castles, making it the globe’s biggest block castle. It took 230 years to construct the castle, a bulk of which was ruined throughout The second world war. Much of the castle has actually been brought back ever since.\nLublin, situated east of the Vistula, is an additional old city with a market location that might have gone back to the 6th century. Since it lies on Poland’s eastern boundary, it ended up being, early, a line of protection versus different intruders that ruined the city over the centuries.\nIt likewise was house to among the biggest Jewish neighborhoods in Poland. Proof of Lublin attaching Western and also Eastern societies can be discovered at the Holy Trinity Church that mixes Catholic and also Russian-Byzantine designs. Yet don’t be deceived by the rock roads and also middle ages style of the Old Community, as it flaunts a vibrant arts and also club scene.\nThe Bialowieza Woodland is a huge residue of the primitive woodlands that when covered much of Europe. The woodland straddles the boundary in between Poland and also the Republic of Belarus, and also there are boundary crossings for travelers walking or on bikes.\nThe Bialowieza Woodland is the only location where European Bison still stay cost-free and also living in the woodland as they when did throughout Europe. Wolves, Lynx, Red Deer, Swine, Elk and also Roe Deer are amongst its various other residents. While the bison are maintained within fenced locations, assisted excursions are readily available either walking or in horse-drawn carriages.\nTorun, situated on the Vistula River, is best recognized, possibly, as the birth place of Copernicus, yet it’s equally as popular for its old market location and also Gothic city center that the National Geographic Polska placed on its listing of the 30 most gorgeous locations worldwide.\nAs Torun left battle throughout The second world war, the city still flaunts various structures that go back to the Center Ages. Building on the city center began in the 13th century, with several churches, consisting of the Basilica of SS. John the Evangelist and also John the Baptist, going back to the 14th century. This church is a must-see for tourists thinking about Gothic paints and also sculptures, and also Baroque churches.\nTravelers that yearn for breathtaking appeal will certainly discover it in Tatra National forest, situated in southcentral Poland. Developed in 1954, the park is mostly woodlands, fields and also various rock developments covering the Tatra Mountains.\nSpelunkers might take pleasure in exploring 6 of the park’s 650 caverns that are open to the general public. The park likewise provides greater than 30 towering lakes in addition to the Wielka Siklawa falls that is 70 meters (230 feet) high.\nTatra, one of the most seen national forest in Poland, will certainly thrill walkers with its 270 kilometres (170 miles) of routes. There is a comparable national forest in the bordering component of Slovakia, likewise called the Tatra National forest.\nPupil tourists intending to satisfy their Polish peers may intend to go to Poznan, long called a scholastic facility and also house to Poland’s 3rd biggest college. The city hosts several global occasions, consisting of the Malta International Theater Event that occurs every summer season.\nSignificant websites are conveniently obtainable by walking the Royal-Imperial Course, a stroll established particularly for travelers. Professional athletes might take pleasure in a browse through to the fabricated lake of Malta, house to a ski incline, ice rink, and also pool.\nSituated on the Oder River, Wroclaw is the biggest city in western Poland. Over the centuries it has actually been controlled by Prussia, Poland, Germany and also Bohemia, yet has actually belonged to Poland given that 1945.\nThe previous resources of Silesia is still much less widely known as several of the various other locations to go to in Poland yet can absolutely contend when it concerns impressive style.\nPiece de resistance consist of the marketplace square and also the excellent Old Community Hall, St, Elizabeth’s Church with its monitoring deck neglecting the city, and also the biggest zoo in Poland. Cruising on the Oder River is a peaceful means to obtain a feeling for this middle ages city.\nAdditionally called Danzig, Gdansk is the biggest city in north Poland and also its primary port given that it rests on the Baltic Sea. Established around the 10th century, it has a combined political background; at various times it came from Germany and also Poland, and also was a complimentary state prior to completely coming to be a component of Poland after\nThe city rebuilt itself after the battle, recovering its Old Community, which is renowned for the Royal Roadway that Gloss kings took a trip on when seeing this historic city. The city likewise is house to St. Mary church, the biggest block church worldwide.\nThe resources of Poland may appropriately be contrasted to a Phoenix metro increasing from the ashes. Established around the 12th century, Warsaw was virtually ruined throughout The second world war, yet has actually reconstructed itself right into a flourishing historic and also social facility, full with a recovered Old Community.\nWhen called the “Paris of the North,” it likewise is renowned as the house of classic author Fryderyk Chopin. One more renowned resident was Renaissance astronomer Copernicus, that was birthed in Poland. Tourists of any ages will certainly take pleasure in a browse through to the Copernicus Scientific research Facility where hands-on tasks are plentiful.\nKrakow could be called a cloths to treasures city, given that it went from being a 7th century town to the 2nd essential city in Poland, being recognized for its social, imaginative, scholastic and also financial tasks.\nThroughout The Second World War, the Nazis rounded up Jews right into the Krakow Ghetto where they were later on sent out to prisoner-of-war camp; the motion picture Schindler’s Checklist focused around one guy’s initiatives to conserve the ghetto locals from elimination. Situated on the Vistula River, this previous Polish resources is simple to navigate, given that Krakow’s destinations emit out from Old Community, took into consideration the very best Old Community in the nation.\nMap of Poland\nFrequently Asked Questions\nCountry/group of nations\nIt is really the 9th biggest nation in Europe, and also with dimension comes range. When you go to Poland, you are not simply obtaining community squares and also special style. You likewise obtain coastlines and also woodlands, salt mines, wild animals and also the spectacular Tatra Mountains, plus a lot of gorgeous cities to explore.15-Jun-2019\nPoland is recognized for being the house of scrumptious pierogi, previous pope John Paul II, and also Europe’s many old old-growth woodland. It is likewise a nation abundant in special background and also sensational location, from the Tatra hills to the Baltic Sea. Continue reading listed below for several of one of the most important points to learn about Poland!Poland is recognized for being the house of scrumptious pierogi, previous pope John Paul IIpope John Paul IIJohn Paul II has actually long been attributed with contributing in lowering communism in Catholic Eastern Europe by being the spiritual ideas behind its failure and also a driver for tranquil transformation in Poland.https://en.wikipedia.org › wikiHoly See–Soviet Union relationships – Wikipedia, and also Europe’s many old old-growth woodland. It is likewise a nation abundant in special background and also sensational location, from the Tatra hills to the Baltic Sea. Continue reading listed below for several of one of the most important points to learn about Poland!17-Feb-2022\n– Poland’s 10 biggest cities are Warsaw, Kraków, Łódź, Wrocław, Poznań, Gdańsk, Szczecin, Bydgoszcz, Lublin and also Białystok.\n– Warsaw is Poland’s resources city and also it lies in the centre-east of the nation.\n– Krakow[SEE MAP] A climatic location to go to, Krakow’s beautiful Old Community is thrilling – fascinating churches and also old structures line its stunning squares.\n– Warsaw[SEE MAP] What is this?\n– Gdansk[SEE MAP]\n– Wroclaw[SEE MAP]\n– Poznan[SEE MAP]\n– Torun[SEE MAP]\n– Lublin[SEE MAP]\n– Katowice[SEE MAP]\nPoland is an Eastern European nation that has actually been attacked and also ruined often times over the centuries. Poland has actually experienced The second world war where its big Jewish populace was rushed off to Nazi prisoner-of-war camp. The Polish spirit declined to pass away and also today Poland integrates middle ages style with dynamic social tasks to satisfy the requirements of modern-day', '“Antisemitism is just fear of the unknown” our guide explained to us as we stared in disbelief at the remnants of the old walls of the Jewish Ghetto where thousands of Jews were forcefully crammed into to live on March 3, 1941. We all know the eventual conclusion, which has fortunately led to a universal human rights initiative built around the notion of “never again” driven by testimonies of survivors. In recent years, one of those survivors- Eva Mozes Kor- has become famous through her online tweeting and viral Facebook video advocating for the forgiveness of one’s enemy. Eva’s story of how she has come to forgive Dr. Mengele and other Nazis who experimented and tormented her in Auschwitz in 1944, traces her struggle with torment and heartbreak, followed by forgiveness and acceptance. And while Eva’s message of forgiveness is heartwarming and continues to resonate with millions, it is important that we truly understand what it is that she has gone through to bring herself to forgive.\nAlthough Eva’s story starts during her childhood in Transylvania Romania, her eventual deportation to Auschwitz-Birkenau makes Poland a pivotal place in her life. In order to tell her story better, and in an effort to get people to genuinely follow her message of forgiveness, Eva began bringing groups of people to Poland years ago so that they too could bear witness to the atrocities that took place here. This year I was fortunate enough to receive an educator scholarship to travel with Eva (and many others) to Krakow and Auschwitz to learn more about this terrible event, and more importantly to help Eva commemorate the loss of her family, all of whom perished in the final months before the end of the war. Although Eva was not Polish and did not reside in Krakow or any other part of Poland when the Nazi regime took control, in order to understand how the atrocities that took the lives of her family unfolded, it is first necessary to understand a little bit of Poland’s history during World War II, particularly in regards to the Jewish population. Fortunately, we were able to do this during our first few days visiting Krakow.\nI have to admit, when I first arrived in Krakow, I was thoroughly charmed with its giant medieval market square and pop-up farmer’s market and folk festival. As charming as this city is, like many places in Eastern Europe, there is a not too distant, dark history that lies just underneath the surface that can’t (and shouldn’t) be ignored. This became increasingly clear after visiting the Old Jewish Quarter and former ghetto on our first day of the tour.\nBefore World War II, there were 65,000 Jews in Krakow and 3 and 1/2 million in Poland. Today, there are only 10,000 to 20,000 Jews left in Poland, with a mere 120 to 200 Jews still living in Krakow. The decimation of this once thriving and vibrant people started well before Hitler marched into Poland in 1939. After creating policies to welcome and encourage European Jews to settle in Poland, King Kazimierz created a separate town to house the Jews who were facing backlash and false blame for starting a fire in the University center. From the 16th century to the beginning of the 20th century, the Jewish community in Krakow thrived, contributing to the culture and economy of Poland. Perhaps one of the most famous Jews was a woman named Helena Rubinstein who eventually emigrated to Australia where she invented the first modern mascara (Egyptians invented the original). Today, her house, along with many other famous Jewish homes, synagogues, and restaurants can be found on ulica Szeroka (Broad Street), which still functions as the main square of the old quarter, which ironically, is named Kazimierz, after the king.\nToday, Kazimierz has become somewhat of a hipters’ paradise blended with a celebration of traditional Jewish culture for those who come from all over the world to pay homage to this space of memory. Throughout the quarter, alongside microbrew bars, clubs, and hummus restaurants, one comes across unique wall murals, art, and live music celebrating the heritage of Judaism that the Nazis tried, unsuccessfully I might add, to wipe out so many years ago.\nAlthough the Jews were a vital part of pre World War II Krakow’s community, with the invasion of Germany and the start of the war, their fate took a deadly turn. With the start of the war, Jews from the surrounding countryside fled to Krakow, thinking it would be safer…they were wrong.\nThe ghetto, forever associated with Auschwitz, and eternalized in Steven Spielberg’s film, Schindler’s List, began in March 1941, after German Hans Frank infamously proclaimed he wanted to remove 50,000 Jews from Krakow to make it the “cleanest town”. At first, the plan was that there could only be 15,000 Jews put into the ghetto because that is the amount that would be needed for work in the factories. These original inhabitants of the ghetto are memorialized in the square of heroes with chairs.\nChairs were selected for the memorial to symbolize the fact that many Jews carried all of their furniture with them into the ghetto, only to realize there was no space for it, resulting in them throwing chairs out the windows into the square.\nLife in the ghetto became increasingly more difficult with special Jewish police forces set up so that Jews were actually forced to govern themselves. With such crowded conditions, the square became the only place where one could get any fresh air. It was also the place where one could come to sell and pawn items such as pots and gold watches. As the year wore on, less and less food was made available to those living inside the ghetto, with the average Jew getting only 250 to 300 calories per day. Of course, those Jews selected to work in Schindler’s factory were much better off. (More on that in a later post).\nWhile most of those who read this are familiar with Schindler’ story, many have likely not heard about the famous Eagle Pharmacy that also was a symbol of resistance. Just inside the ghetto walls, the pharmacy was run and owned by the only non-Jewish person inside the ghetto- Tadeusz Pankiewicz.\nThe pharmacy had existed before the ghetto and Pankiewicz was able to convince the Nazis to let him keep running it during the course of the war. Strategically, Pankiewicz hired three women of non-Polish decent who resided outside the ghetto to work for him. In this way, every morning these women would come to work and every evening they would leave, making it possible for them to smuggle in food, medicine, and supplies for the Jews, as well as relaying messages back and forth to loved ones and friends. One of the more crucial items that they smuggled in was hair coloring., as looking young and healthy was essential to survival. As a result of his efforts to help and hide many Jews throughout the existence of the ghetto, Pankiewicz has been acknowledged by Israel as being one of the “righteous among the nations.”\nIn 1942, the ghetto took a deadly turn when it was divided into to two sections; for those who could work (Ghetto A) and for those who couldn’t (Ghetto B). On what is now known as Bloody Thursday (June 4th, 1942) those Jews who were living in ghetto b were hauled off to Auschwitz to make room for more and more Jews who wee increasingly coming in from the country side. But as more Jews were coming into the ghetto, it was devastatingly shrinking in size. Also, in case there be any doubt as to what the fate of Jews might be, a wall went up around the ghetto that was designed in the shape of Jewish tombstones; a cruel form of Nazi propaganda that said to Jews the only way out was through death.\nOur guide made a point to acknowledge that only in Poland these concrete walls went up, suggesting that it was because the Poles would try to help the Jews if not, and in ghetto; s that were set up in other countries this wouldn’t be necessary. To be fair, even with the wall up, many Poles risked their lives and their family’s lives in order to provide food or false documents for Jews.\nToday, part of the wall still exists alongside a small building with a pharmacy downstairs and apartments upstairs, which led me to a question that would haunt me throughout the rest of the trip-how can people live in such close proximity to these spaces?\nIn the end, Jews were segregated into ghettos because they were deemed as “different” or “dangerous”. By ostracizing, segregating, and eventually deporting the Jews of Krakow to Auschwitz extermination camps, the Nazis began to carry out the first stages of the what would come to be known as the final solution. Walking in the footsteps of what was once a vibrant Jewish community, and then a dismantled but resilient community, I began to prepare for what it would be like to walk in their final footsteps; the logical conclusion of what happens when we denigrate and fear the unknown.']	['<urn:uuid:1573d5c4-3f75-4c26-af0a-03bc75824ec2>', '<urn:uuid:98b0f068-c7b6-4285-a958-88c656a20a9e>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T00:20:10.754925	13	92	3093
50	hereditary breast cancer surgery psychological care	For surgery, women with inherited mutations often choose bilateral mastectomy rather than lumpectomy due to the high risk of second breast cancer, as removing at-risk tissue reduces recurrence likelihood. For psychological care, studies show that Mindfulness-Based Stress Reduction (MBSR) significantly improves psychological symptoms and fatigue in breast cancer survivors, with benefits lasting at least 6 weeks post-treatment. This intervention includes meditation, body scan, and yoga components delivered through 2-hour weekly sessions over 6 weeks.	"[""Treatment decisions for people with hereditary breast cancer\nHereditary cancers are different than sporadic cancers in ways that can affect treatment choices. Some treatment decisions that may be influenced by genetic test results are listed below. If you are concerned that your cancer may be caused by an inherited mutation, you should contact a genetics expert to see if genetic testing is right for you.\n- PARP inhibitors for metastatic breast cancer: Lynparza and Talzenna are targeted therapies known as PARP inhibitors. Both agents have received FDA approval for treating\nmetastatic breast cancer caused by a BRCA mutation. The National Comprehensive Cancer Network (NCCN) added Lynparza as a preferred single agent treatment for people with Her2-negative, metastatic breast cancer who carry a BRCA1 or BRCA2 mutation. You can read more about PARP inhibitors here.\n- Choice of breast surgery: Because of the very high risk for a second (or third) breast cancer diagnosis, women who are diagnosed with breast cancer who test positive for an inherited mutation often choose bilateral mastectomy (surgical removal of both breasts) rather than lumpectomy and radiation. Mutation carriers who undergo mastectomy are less likely to develop a second breast cancer since the at-risk breast tissue has been removed. Our mastectomy section provides more information about surgical options.\n- Participation in treatment clinical trials: Some research studies are exploring new treatments to specifically to treat hereditary breast cancer. If you are interested in the possibility of participating in a clinical trial, it is best to express your interest when you are first diagnosed or have a recurrence and before you start treatment. Using our Research Search Tool, you can find clinical trials enrolling patients with hereditary breast cancer.\n- Oophorectomy vs. medication to induce menopause:\nBRCA mutation carriers are at increased risk for ovarian cancer. In young women with ER-positive breast cancer, treatment sometimes includes injections to shut down the ovaries' production of estrogen. Another option may be an oophorectomy (surgical removal of the ovaries), which lowers the risk for ovarian cancer as well. Oophorectomy may lower the risk for new breast cancers in BRCA carriers who have not have bilateral mastectomies.\n- Tamoxifen, aromatase inhibitors, or other hormonal therapies:\nTamoxifen is a drug used to treat ER-positive breast cancer. Aromatase inhibitors are drugs prescribed to post-menopausal women with breast cancer to reduce estrogen production by their fat cells and adrenal glands. These drugs are used to help prevent breast cancer recurrence in women with ER-positive or hormone-receptor positive cancers. Research on these drugs suggests that they not only lower the risk for subsequent breast cancers in women who have already been diagnosed, but also in those at risk who have not ever been diagnosed with breast cancer.\n- Use of chemotherapy agents:\nSome research studies show that women with\nBRCA1 mutations tend to develop more aggressive breast cancers than those in women who develop sporadic breast cancer. A small study suggests that women with BRCA1 mutations who received any chemotherapy had better outcomes than women who did not receive chemotherapy. Other research suggests that BRCA-positive individuals diagnosed with triple-negative breast cancer may respond particularly well to platinum-based chemotherapy.\nIf you are a breast cancer survivor making decisions about genetic testing can be confusing and you may want additional guidance or support. FORCE's Peer Navigation Program provides expert reviewed resources and 1:1 personalized peer support by specially trained volunteers who have experienced the very challenges you face."", 'Improve the Psychological Well-Being of Breast Cancer Survivors with Mindfulness\nBy John M. de Castro, Ph.D.\n“Mindfulness is a good resource for dealing with the physical and psychological symptoms of metastatic disease. Women who were more mindful tended to have lower symptoms of metastatic breast cancer, including pain severity and interference, fatigue, psychological distress, and sleep disturbance.” – Lauren Zimmaro\nReceiving a diagnosis of cancer has a huge impact on most people. Feelings of depression, anxiety, and fear are very common and are normal responses to this life-changing and potentially life-ending experience. But cancer diagnosis is not necessarily a death sentence. Over half of the people diagnosed with cancer are still alive 10 years later and this number is rapidly increasing. But, surviving cancer carries with it a number of problems. Anxiety, depression, fatigue and insomnia are common symptoms in the aftermath of surviving breast cancer. These symptoms markedly reduce the quality of life of the patients.\nMindfulness training has been shown to help with cancer recovery and help to alleviate many of the residual physical and psychological symptoms, including stress, sleep disturbance, and anxiety and depression. The Mindfulness-Based Stress Reduction (MBSR) program is a mindfulness training program that includes meditation practice, body scan, yoga, and discussion along with daily home practice. MBSR has been shown to be beneficial for cancer patients in general and also specifically for the symptoms of breast cancer survivors. So, it makes sense to further explore the effectiveness of MBSR training for the treatment of breast cancer survivors.\nIn today’s Research News article “Mindfulness-Based Stress Reduction in Post-treatment Breast Cancer Patients: Immediate and Sustained Effects Across Multiple Symptom Clusters.” (See summary below or view the full text of the study at: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7771358/ ) Reich and colleagues recruited breast cancer survivors and randomly assigned them to either usual care or to receive a 6-week, once a week for 2-hours, Mindfulness-Based Stress Reduction (MBSR) modified for breast cancer survivors. They were measured before and after training and 6 weeks later for worry, fear of cancer recurrence, anxiety, depression, perceived stress, mindfulness, symptom severity, sleep quality, fatigue, pain, cognition, and health-related quality of life.\nThey found with factor analysis that the measures fit into 4 clusters; pain, cognition, fatigue, and psychological. They found that in comparison to baseline the usual care, Mindfulness-Based Stress Reduction (MBSR) produced significant improvement in the psychological and fatigue clusters, but not the cognitive or pain clusters. These effects were still present 6 weeks later.\nThese findings suggest that Mindfulness-Based Stress Reduction (MBSR) is an effective treatment to relieve the psychological and fatigue symptoms of breast cancer survivors. This corresponds with prior findings that mindfulness improves the symptoms of breast cancer survivors and reduces anxiety, depression, and perceived stress, and improves emotional well-being and also reduces fatigue and improves sleep quality.\nThe observed improvements produced by Mindfulness-Based Stress Reduction (MBSR) markedly improves the quality of life and reduces the suffering of these cancer patients. These are clinically significant. It has been shown that an improved psychological outlook is associated with better physical recovery. Hence, these findings suggest that MBSR or other mindfulness training programs, should be incorporated into the routine care of breast cancer survivors.\nSo, improve the psychological well-being of breast cancer survivors with mindfulness.\n“Mindfulness is a state of mind which we can all acquire and use to support our wellbeing physically, emotionally and mentally. . . Having cancer, or specifically breast cancer, is no exception. Our cancer experiences take up a lot of energies, mental focus and can drain us emotionally. It is important to have a few tools to help us create ‘down’ and ‘out’ times, and to replenish and reconnect with who we are.“ – Breast Cancer Now\nCMCS – Center for Mindfulness and Contemplative Studies\nThis and other Contemplative Studies posts are also available on Google+ https://plus.google.com/106784388191201299496/posts and on Twitter @MindfulResearch\nReich, R. R., Lengacher, C. A., Alinat, C. B., Kip, K. E., Paterson, C., Ramesar, S., Han, H. S., Ismail-Khan, R., Johnson-Mallard, V., Moscoso, M., Budhrani-Shani, P., Shivers, S., Cox, C. E., Goodman, M., & Park, J. (2017). Mindfulness-Based Stress Reduction in Post-treatment Breast Cancer Patients: Immediate and Sustained Effects Across Multiple Symptom Clusters. Journal of pain and symptom management, 53(1), 85–95. https://doi.org/10.1016/j.jpainsymman.2016.08.005\nBreast cancer survivors (BCS) face adverse physical and psychological symptoms, often co-occurring. Biologic and psychological factors may link symptoms within clusters, distinguishable by prevalence and/or severity. Few studies have examined the effects of behavioral interventions or treatment of symptom clusters.\nThe aim of this study was to identify symptom clusters among post-treatment BCS and determine symptom cluster improvement following the Mindfulness-Based Stress Reduction for Breast Cancer (MBSR(BC)) program.\nThree hundred twenty-two Stage 0–III post-treatment BCS were randomly assigned to either a six-week MBSR(BC) program or usual care. Psychological (depression, anxiety, stress, and fear of recurrence), physical (fatigue, pain, sleep, and drowsiness), and cognitive symptoms and quality of life were assessed at baseline, six, and 12 weeks, along with demographic and clinical history data at baseline. A three-step analytic process included the error-accounting models offactor analysis and structural equation modeling.\nFour symptom clusters emerged at baseline: pain, psychological, fatigue, and cognitive. From baseline to six weeks, the model demonstrated evidence of MBSR(BC) effectiveness in both the psychological (anxiety, depression, perceived stress and QOL, emotional well-being) (P = 0.007) and fatigue (fatigue, sleep, and drowsiness) (P < 0.001) clusters. Results between six and 12 weeks showed sustained effects, but further improvement was not observed.\nOur results provide clinical effectiveness evidence that MBSR(BC) works to improve symptom clusters, particularly for psychological and fatigue symptom clusters, with the greatest improvement occurring during the six-week program with sustained effects for several weeks after MBSR(BC) training.']"	['<urn:uuid:a0d39458-531e-422e-93e9-6ce962ef7c26>', '<urn:uuid:43fbde26-2a32-41c7-a942-67e4791d425c>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T00:20:10.754925	6	74	1517
51	Looking at advanced fertility options - could you explain how the embryo transfer timing works in IVF, and what specific risks should patients be aware of during this process?	Embryo transfer can occur 1-5 days after fertilization, with many embryologists recommending day 5 transfer (blastocyst stage) for better implantation success and reduced multiple pregnancy risk. As for risks, these include potential embryo displacement into the cervix resulting in loss, or into fallopian tubes causing ectopic pregnancy, as well as mild bleeding and cramping during transfer. Additional risks include medication side effects, particularly ovarian hyperstimulation syndrome, and complications from oocyte retrieval such as infection, bleeding and trauma.	['Intracytoplasmic morphologically selected sperm injection (IMSI) is a modified form of the ICSI (intracytoplasmic sperm injection) technique, however the only difference is that it carefully selects the best quality sperm by magnifying 6000 times more than the conventional ICSI method.\nPatients with severe oligoasthenoteratozoospermia (low sperm count) can benefit from IMSI.\nIntracytoplasmic Sperm Injection (ICSI) is a method in which a single sperm is injected into the center of the cytoplasm of the egg to increase the chances of fertilization. ICSI procedure channels only one egg and one sperm needed to produce one embryo.\nIMSI Treatment Stages:\n1. Medication: The woman (recipient) will be prescribed medication to help control ovulation. Medications will also be prescribed to increase the egg supply.\n2. Keeping track of Progress: To keep track of the recipient’s progress, the clinic will monitor the eggs by vaginal ultrasound scans. They will also do blood tests to chart the rising levels of estrogen produced by the eggs, which will help keep track of how the eggs are maturing.\n3. The woman will receive a hormone injection to help the eggs mature 34-38 hours before the eggs are to be collected.\n4. Collecting the Eggs: Eggs are usually collected by the guidance of an ultrasound, which usually takes about 30 minutes. The doctor will insert a thin needle through the vagina into each ovary. The eggs will then be drawn into the needle. Eggs are collected by laparoscopy (a small telescope with a light attached). This procedure involves making a small incision in the stomach and extracting the eggs with a fine needle.\n5. Collecting the Sperm: Around the same time that the eggs are collected, a sperm sample will be taken from the male partner and stored for a short time. The sperm will be washed and spun, and then the healthiest sperm will be selected to fertilize with the egg. If donor sperm is being used, the sperm will be taken from the freezer and prepared in the same way.\n6. Fertilization: Once the sperm and egg are collected, they are combined in a Petri dish, and monitored for 20-40 hours. If fertilization has occurred, the resulting embryo is monitored for several days to follow its cellular division. In order for the fertilization to be successful, it must first divide into two cells, then four, and finally into eight. The embryo can then be transferred into the recipient’s uterus.\n7. Transfer Embryo: The embryo transfer can occur 1-5 days after fertilization occurs, depending on factors that the couple will discuss with the embryologist, such as low sperm count and embryo development. Same day embryo transfer, in which the embryos are transferred about 4 hours after fertilization, is becoming more common since it allows recipients to undergo the procedure on an outpatient basis. Most embryologists recommend that the recipient wait until day five to transfer the embryo, which is known as the blastocyst stage. The blastocyst transfer has also become quite common as it increases chances of successful implantation and decreases the likelihood of multiple pregnancies.\n8. Insemination: This process is the last stage of the IVF procedure. It is usually painless and does not require any anesthesia. The embryos are transferred into the uterus through the cervix with a catheter. Typically 2-4 embryos are transferred into the uterus to increase the chances of pregnancy. If done successfully, the patient can go home and should rest for the remainder of the day. A pregnancy test will be given 12-15 days after the embryo transfer. If the IVF is successful, a normal pregnancy can be carried out.\nRisks and side effects include possible displacement of the embryos into the cervix, resulting in a lost embryo, or in the fallopian tubes, resulting in tubal (ectopic) pregnancy; multiple pregnancies. If IVF is successful, a normal pregnancy will be carried out. If IVF with ICSI results in an unsuccessful pregnancy or no pregnancy, different methods of IVF should be considered, such as Egg Donor IVF, Sperm Donor IVF or Embryo Donor IVF.\nTransferring the embryo may cause mild bleeding and cramping.', 'What is IVF?\nIVF literally means allowing fertilization of the male and female gametes (sperm and egg) to occur outside the human body. IVF is an established treatment for many forms of infertility where an egg or eggs are extracted from the ovary or ovaries of a woman and mixed or injected with human sperm in the laboratory to achieve fertilization. After normal fertilization is achieved, the fertilized eggs are cultured in the laboratory for an appropriate period of time before they are placed inside a woman’s uterus (womb) with the intent of making the woman pregnant.\nThe first IVF baby in the world was born in 1978. At last count in 2012, over 5 million babies have been born worldwide as a result of IVF. There are many variations of the treatment depending on the medical indications and social circumstances, including use of donor eggs, donor sperm and/or gestational surrogates.\nWhen IVF Might Be an Option\nSome of the types of fertility that might be helped with IVF include:\n- Absent fallopian tubes or tubal disease that cannot be treated successfully by surgery\n- Endometriosis that has not responded to surgical or medical treatment\n- A male factor contributing to infertility, in which sperm counts or motility are low but there are enough active sperm to allow fertilization in the laboratory\n- Severe male factor in which sperm must be obtained surgically\n- Unexplained infertility that has not responded to other treatments\n- Previous sterilization including tubal ligation or vasectomy\n- Failed to conceive after an adequate trial of insemination with (super) ovulation\n- Genetic diseases that result in miscarriage or abnormal births\nSpecific Steps in IVF?\nAn IVF cycle typically includes the following steps or procedures:\n- Controlled ovarian stimulation: use of medications to grow multiple eggs\n- Monitor ovarian and uterine responses\n- Procurement of sperm sample\n- Retrieval of eggs from the ovary or ovaries\n- Fertilization of eggs: insemination or injection of sperm into the egg\n- Culturing of any resulting fertilized eggs (embryos)\n- Placement (transfer) of one or more embryo(s) into the uterus\n- Support of the uterine lining with hormones to facilitate and sustain pregnancy\nFor more information about each of the specific steps, please see page 5 of the Consent to ART page.\nIn some cases, additional procedures are employed. These include:\n- Intracytoplasmic sperm injection (ICSI) to increase the chance for fertilization\n- Assisted hatching of embryos to potentially increase the chance of embryo attachment (implantation)\n- Embryo Cryopreservation (freezing)\n- Pre-implantation Genetic Diagnosis/Screening (PGD)\nSuccess of IVF\nIVF is the most successful treatment modality for infertility. The success rate of a single IVF cycle of treatment varies from over 50% live birth rate per cycle in women under 35 years old, especially among women with good quality embryos, to less than 10% live birth per cycle in women 42 years old or older. The national average live birth rate in the U.S. for all women undergoing IVF between 2006 and 2008 is about 30% per cycle of treatment.\nThe number of embryos transferred influences the pregnancy rate and the multiple pregnancy rate. The age of the woman and the appearance of the developing embryo have the greatest influence on pregnancy outcome and the chance for multiple pregnancy. Unlike many countries in Europe, there is no federal or state legislation in the U.S. dictating the number of embryos to be transferred. Family Fertility Center follows the guidelines set by the Society for Assisted Reproductive Technologies, (S.A.R.T.) regarding the recommended limits on number of embryos to transfer.\nRisks of IVF\nRisks of IVF include:\n- Side effect of different medications used for IVF, the most serious of which is ovarian hyper-stimulation syndrome\n- Risks associated with oocyte retrieval such as infection, bleeding and trauma\n- Risks associated with pregnancy resulting from IVF\n- Risks to the child/children conceived as a result of IVF\nSchedule a Consultation\nIf you are experiencing a medical emergency, call 911 or go to the nearest emergency room.']	['<urn:uuid:41f1cb8c-c67d-49fe-a183-839bd1f483d0>', '<urn:uuid:622431f1-4b35-44a1-8345-ea03260db42e>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T00:20:10.754925	29	77	1350
52	what types of magnetic properties do superconducting materials have and how are they used in transportation technology	Superconducting materials exhibit complete diamagnetism, meaning they strongly repel magnetic fields. In iron-based compounds, this property coexists with the material's inherent magnetism, which comes from the alignment of unpaired electrons in multiple orbitals. These magnetic properties are particularly useful in transportation technology, especially in maglev trains where the strong diamagnetism allows the train to 'float' above the rails using magnetic force, resulting in high-speed, stable, and safe operation. Additionally, these magnetic properties enable the creation of superconducting magnetic thrusters for ships, where superconducting magnets installed in the hull generate strong magnetic fields that interact with electric currents in seawater to produce propulsion force.	"['Advancing Basic Science for Humanity\n07/13/2017 - Electron Orbitals May Hold Key to Unifying Concept of High-Temperature Superconductivity\nOriginally published by Brookhaven National Laboratory)\nJuly 6, 2017\nIron-based superconductivity occurs in materials such as iron selenide (FeSe) that contain crystal planes made up of a square array of iron (Fe) atoms, depicted here. In these iron layers, each Fe atom has two active electron ""clouds,"" or orbitals—dxz (red) and dyz (blue)—each containing one electron. By directly visualizing the electron states in the iron planes of FeSe, the researchers revealed that that electrons in the dxz orbitals (red) do not form Cooper pairs or contribute to the superconductivity, but instead form an incoherent metallic state along the horizontal (x) axis. In contrast, all electrons in the dyz orbitals (blue) form strong Cooper pairs with neighboring atoms to generate superconductivity. Searching for other materials with this exotic ""orbital-selective"" pairing may lead to the discovery of new superconductors.\nA team of scientists has found evidence for a new type of electron pairing that may broaden the search for new high-temperature superconductors. The findings, described in the journal Science, provide the basis for a unifying description of how radically different “parent” materials—insulating copper-based compounds and metallic iron-based compounds—can develop the ability to carry electrical current with no resistance at strikingly high temperatures.\nAccording to the scientists, the materials’ dissimilar electronic characteristics actually hold the key to commonality.\n“Scientists have thought that because the starting point for superconductivity in these two classes of materials is so different, you need different theoretical approaches to describe them,” said J.C. Séamus Davis, a physicist at the U.S. Department of Energy’s (DOE) Brookhaven National Laboratory and Cornell University, who led the team of experimental scientists. “Instead, we’ve been motivated to explore what is universal about these two systems. Ideally, there should be just one explanation.”\nDavis is a member of the Kavli Institute at Cornell for Nanoscale Science.\nScientists have generally understood that the mechanism of superconductivity in copper oxide compounds depends on the ability of electrons on adjacent copper atoms to pair up. Each copper atom has a single, unpaired electron in its outermost energy shell, or orbital. While the outermost electrons on adjacent copper atoms interact with one another strongly, they ordinarily stay locked in place, stuck in a “quantum mechanical traffic jam” with nowhere to go, Davis said. With no electrons moving, the material acts as a “strongly correlated” electrical insulator.\nSéamus Davis, a physicist at Cornell University and the U.S. Department of Energy\'s Brookhaven National Laboratory\nRemoving some of the electrons that reside on copper atoms results in electron vacancies known as holes. This alleviates the quantum traffic jam so that, when the material is cooled to a certain temperature, oppositely aligned electrons (magnetic partners where the “spin” of one electron points up and the adjacent one points down) form pairs and then become free to zip through the material unimpeded—a superconductor.\nIron atoms, which have a nucleus with smaller positive charge than copper, exert less pull on the circulating electrons. So instead of filling up electron orbitals, electrons in several outer energy orbitals remain unpaired, yet aligned with one another and electronically active. The alignment of unpaired electrons in multiple orbitals gives simple iron its strong magnetic and metal properties, so it’s easy to see why iron compounds would be good conductors. But it’s not really clear how they could become zero-resistance superconductors at high temperatures without the strong interactions that create a correlated insulating state in the copper based materials.\nTo address this conundrum, theoretical physicists began to consider the possibility that the unpaired electrons in iron’s different orbitals could take on very different roles. Perhaps unpaired electrons in one particular orbital could pair up with electrons in the same orbital on an adjacent atom to carry the supercurrent, while electrons in the other orbitals provide the insulating, magnetic, and metallic properties.\nThe custom-built Spectroscopic Imaging Scanning Tunneling Microscope used for these experiments stands one meter high, with cryogenic circuitry at the top for cooling samples to temperatures just above absolute zero (nearly -273 degrees Celsius). Inside, a needle with single atom on the end scans across the crystal surface in steps as small as 2 trillionths of a meter, measuring the electron tunneling current at each location. These measurements reveal the quantum wavefunctions of electrons in the material with exquisite precision.\n“The challenge is to find a way to see that some of the electrons are superconducting and some are insulating in the same crystal,” Davis said.\nThe research published in Science provides the first direct proof that such “orbital-selective” electron pairing takes place.\nThe theory team for this project—Andreas Kreisel (University of Leipzig), Peter Hirschfeld (University of Florida), and Brian Anderson (University of Copenhagen)—defined the electronic signatures that should be associated with each orbital on the iron atoms. Then, experimentalists Peter Sprau and Andrey Kostin (both of Brookhaven Lab and Cornell) used a scanning tunneling microscope at the Center for Emergent Superconductivity—a DOE Energy Frontier Research Center at Brookhaven Lab—to measure the energy and momentum of electrons in iron-selenide samples that were synthesized by Anna Bohmer and Paul Canfield at DOE’s Ames Laboratory. Comparing the measurements with the predicted electronic signatures allowed the scientists to identify which electrons were associated with each orbital.\nWith this information, “We can measure the binding energy and momentum of electrons in the ‘Cooper pairs’ responsible for superconductivity and identify which energy momentum characteristics they have—which orbital they’re from,” Davis said.\n“We were able to show that almost all of the electrons in Cooper pairs in iron selenide were from a particular lower energy orbital (the dyz orbital),” Davis said. The findings also imply that the electron in iron’s outermost orbital in iron selenide exhibits virtually insulating properties, just as it does in the copper oxide compounds.\n“Because iron selenide normally exhibits good metallic conductivity, how would one ever know that the electrons in this orbital are acting as they are in correlated insulators? This strongly interacting and virtually insulating state was hiding in plain sight!” he said.\nWith this outer-orbital insulating state, the iron compound has all the same requirements for superconductivity that the copper oxides do—a strong magnetic interaction (up/down pairing) of the almost localized electrons, and a metallic state that allows those pairs to move. The big difference is that in iron selenide, these contributions come from different electrons in three separate active orbitals, instead of the single electron in one active orbital in copper.\n“In iron you have the conductivity for free. And you have the magnetism for free, but it’s based on a different electron. Both coexist in the same atom,” Davis said. So once you have Cooper pairs, it seems there’s no need to add holes to get the current flowing.\nThis realization may broaden the search for new superconductors that can potentially operate under warmer conditions. Such higher high-temperature superconductors would be more practical for real world, energy-saving applications such as power lines or energy storage devices.\n“Instead of searching for new single-electron antiferromagnetic insulators like copper oxide to make high-temperature superconductors, maybe we should be searching for new highly magnetic, metallic materials that have properties like iron but in an orbitally selective arrangement,” Davis said. “This opens the world of materials science to many new types of materials that could be high-temperature superconductors.”\nThis research was funded by the DOE Office of Science, the Moore Foundation’s EPiQS Initiative, and a Lundbeckfond Fellowship.', 'Since superconducting ceramics have many excellent characteristics, such as complete conductivity and complete diamagnetism, the successful development and practical use of high-temperature superconducting materials will have an impact on the production of human society and the understanding of the structure of matter significant impact.It may bring about revolutions in many disciplines. Therefore, countries all over the world have invested a lot of manpower and material resources in research.\nThe application of high-temperature superconducting ceramics has the following aspects.\n1. In terms of power system\n(1) Transmission and distribution\nAccording to the zero-resistance characteristics of superconducting ceramics, extremely large currents and power can be transmitted over long distances without loss. And now the dielectric loss of cables and transformers often accounts for 20% of the transmitted power.\n(2) Superconducting coil\nIt can be made into a superconducting energy storage coil. The energy storage device made of it can store energy without loss for a long time, and directly store electromagnetic energy without energy conversion. The impact load of the power transmission system can be tracked and adjusted, and the peak The load is leveled.\n(3) Superconducting generator\nSince the resistance of superconducting ceramics is zero, the current density can reach (7~10)x10^5A/c㎡, and there is no need for an iron core, so there is no heat loss, and large-capacity, high-efficiency superconducting generators and magnets can be manufactured. Fluid generators, rotating electric machines, etc.\n2. In terms of transportation\n(1) Manufacturing superconducting maglev train\nDue to the strong diamagnetism of superconducting ceramics, the maglev train has no wheels and ""floats"" on the rails by magnetic force. It has high speed, stable operation, safety and reliability.\n(2) Superconducting magnetic thruster and space propulsion system\nFor example, ship electromagnetic propulsion device. The propulsion principle is: install a superconducting magnet inside the hull to generate a strong magnetic field in the sea water. At the same time, an electrode is placed on the side of the hull to generate a strong current in the seawater. In the seawater behind the stern, the magnetic lines of force interact with the current, and the seawater produces a strong driving force behind the hull.\n3. In terms of mineral processing and prospecting\nIn terms of mining and metallurgy, since all materials are diamagnetic or paramagnetic, superconductors can be used for mineral processing and prospecting.\n4. In terms of environmental protection and medicine\nIn terms of environmental protection, superconductors can be used to purify wastewater discharged from paper mills and petrochemical plants.\nIn terms of medicine and health, most organisms are diamagnetic, a few are paramagnetic, and a very few are strong magnetic. Superconductors can be used for wastewater treatment to remove bacteria, viruses, heavy metals and other poisons. In medicine, magnetic separation can be used to separate red blood cells from plasma. In addition, because certain bacteria, such as Staphylococcus albicans and cancer cells, are inhibited from growing in a strong magnetic field, it is being studied to heat the lesion with a low-frequency alternating strong magnetic field with drugs, which will kill the cancer cells.\n5. In terms of high-energy nuclear experiments and thermonuclear fusion\nThe superconductor\'s strong magnetic field is used to accelerate the particles to obtain high-energy particles, and the superconductor is used to manufacture instruments for detecting particle motion tracks.\nNuclear fusion is a new technology for obtaining huge energy. However, the controlled thermonuclear reaction must have the following conditions: ①Deuterium and tritium must be heated to 3x(10^7~10^8)K; ②Satisfy the Lawson criterion-the product of plasma density n and energy confinement time τ is greater than 10^ 14S/c㎡, this requires a large magnetic field with a volume of several 10m3 and a magnetization strength of 1x10^8A/m to confine the high-temperature deuterium and tritium plasma in a small space. Conductive magnet. The Lawrence Livermore Laboratory of the University of California, USA, has built a 600t large-scale NbTi magnetic mirror nuclear fusion experimental device MFTF, which has an energy storage of about 3000MJ.\n6. In electronic engineering\n(1) Use the Josephson effect of superconductors to increase the operating speed and reduce the size of electronic computers. The switching time of the Josephson tunnel junction is 10^(-12)s, the heat generated during ultra-high-speed switching is only 10^(-6)W, the power consumption is very small, and its operation speed is 50 times faster than that of silicon transistors. The heat is only 1/1000 or less. It is an important content of high-temperature superconducting ceramic application research to make it highly integrated and develop ultra-small and ultra-high-performance computers.\n(2) Devices made of superconductors, such as superconducting diodes, superconducting quantum interference devices, superconducting junction transistors, superconducting field effect transistors, superconducting magnetic flux sub-devices, etc.\nOf course, the application of high-temperature superconducting ceramics is far more than the above. With the research and development and practical use of high-temperature superconductors, its application range will continue to expand, and a new industrial revolution will appear at that time.']"	['<urn:uuid:0ef6ddf8-e514-444a-ac13-bc3752d4ef79>', '<urn:uuid:42bff865-8460-400a-b2df-52b557f8f2d8>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T00:20:10.754925	17	103	2056
53	traditional customs korean new year celebration	During Seollal (Lunar New Year), Koreans engage in several traditional customs. People dress in hanbok (traditional dress) and hold ancestral rites. They eat tteokguk (soup with thinly sliced rice cakes), which is believed to add a year to one's age. Children wearing hanbok bow to adults and receive cash gifts in return. However, in recent years, some families have simplified these celebrations, leading to concerns about younger generations neglecting these traditional holiday practices.	['There’s plenty in store for Seollal\nAs the country gears up for its second celebration of the new year, there are plenty of events in the works for those who want to get out of the house.\nKoreans consider the Lunar New Year, which falls on Sunday, and the accompanying three-day holiday to be a time for family gatherings and remembering traditions. People dress in hanbok (traditional dress), hold ancestral rites, play historical games and eat special dishes. Eating tteokguk (soup with thinly sliced rice cakes) is said to add another year to your age.\nChildren wearing hanbok also bow to adults and receive cash gifts.\nIn recent years, many families have simplified the celebration and, some fear, grown forgetful of the traditions underlying Seollal. Some have particularly criticized younger people for neglecting the holiday.\nHere are several traditional ways to reconnect with the Lunar New Year and spend the holiday, which runs from today to Monday.\nThose who want to see how ancestors celebrated the Lunar New Year should visit local palaces. At Gyeongbok Palace, visitors will be able to celebrate the holiday by bowing to parents at rooms inside the palace. The palace is also open for free to visitors tomorrow.\nAt Changdeok Palace, officials said they will invite 30 elderly people without families from a nearby community center tomorrow and give them a guided tour of the palace. Employees will also perform New Year’s bows for the guests and present them with a drawing that is believed to fend off evil spirits. Visitors can also drink Asian apricot tea in the garden.\nDeoksu Palace said it will share traditional food used in ritual ceremonies for ancestors with its visitors tomorrow. Those wearing hanbok will also receive Korean biscuits.\nMeanwhile, shrines in the Gyeonggi area have organized folk games for visitors. Events will be held at Yeongneung in Hwaseong; Donggureung in Guri; Gwangneung and Hongneung in Namyangju; and Youngneung in Yeoju. Hyeonchungsa Shrine in Asan and the Cemetery of Seven Hundred Righteous Fighters in Geumsan, both in South Chungcheong, have also arranged exciting events.\nVisitors wearing hanbok will be admitted free of charge to the four major palaces and the shrines for three days from today.\nAt theme parks\nSeoul Land in Gwacheon, Gyeonggi, has organized traditional games and performances from today to Monday. Games include pengichigi (top spinning), yutnori (five-stick board game), jegichagi (Korean shuttlecock), tuho (arrow throwing) and many more.\nGame-playing events will be held twice a day at Samcheonri Hill inside the theme park.\nIn addition, families of two or more can do the limbo, and all visitors can write wishes on balloons and launch them outside.\nAs for traditional performances, character dolls will form a pungmul (percussion) band. Band members will play traditional instruments such as the janggu (double-headed drum), kkwaenggwari (small gong) and jing (large gong). On Monday, Korean traditional dancer Jeong Ju-mi will hold a performance with others at the plaza in the heart of Seoul Land.\nFor more information, call (02) 509-6000 or visit www.seoulland.co.kr.\nLotte World, the “Land of Hopes and Dreams,” has also decided to go traditional for the holiday. Until Monday, a play is on offer that tells the Korean folktale of a magpie celebrating the New Year in its own way.\nThe beats of a samulnori team, a traditional percussion quartet, will also spice up the mood along with a sangmo dance performance, which is a representative Korean folk dance using a twirling sangmo (helmet-like hat with long ribbon).\nTraditional games will be available to visitors at the theme park during the holiday each day at 3 p.m, and a professional calligrapher will draw selected family mottos.\nForeigners can participate in a special talent show tomorrow at 6 p.m.\nVisitors wearing hanbok can purchase four day passes for just 15,000 won ($13.80) each, a 10,000 won discount .\nThe park invites foreigners without kin in Korea to enjoy themselves so that they do not feel gloomy while others are busy with family festivities.\nFor more information, call (02) 411-2000 or visit www.lotteworld.com.\nEverland in Yongin, Gyeonggi, has organized 12 different traditional games for families with children to enjoy.\nWorkers at Everland will be adorned in colorful hanbok to take pictures with guests free of charge.\nA kite expert will also be available to teach children, while adults can have a go at making tteok (rice cake).\nThe taekwondo and percussion performance “Bigabi” will be staged twice a day at the Grand Stage.\nFor more information, call (031) 320-5000 or visit www.everland.com.\nAquariums have arranged special events just for fish lovers. At Hanhwa Aqua Planet in Yeosu, South Jeolla, visitors can watch a beluga whale fetch yut sticks thrown by visitors.\nAt the COEX Aquarium, guests can watch divers play folk games with sardines under water. Three divers and sardines will bow to visitors and play neolttwigi, or Korean see-saw, standing up.\nThese events are held each day at 12:30 p.m., 2:30 p.m. and at 4 p.m. A group dance of 10,000 sardines is also a must-see.\nAt Jeonju Hanok Village\nAt Jeonju Hanok Village, various events will welcome visitors looking for a traditional way to spend time with their families during the holidays.\nGuests can have a go at playing folk games while making traditional food and handicrafts to take home as gifts.\nAt the traditional culture center within the hanok village, visitors can participate in a yutnori competition, make their own kites, eat traditional food and learn folk songs.\nAt the Handicraft Exhibition Center, visitors can make numerous items with hanji, or traditional Korean paper handmade from mulberry trees.\nAt the Choi Myung-hee Cultural Center, visitors can write letters to themselves to be sent a year later by the center.\nDrinkers can enjoy themselves at the Traditional Alcohol Museum and try the alcoholic beverages brewed by the museum. Classes also teach how to make makgeolli, or Korean rice wine.\nAt the Korea Tourism\nAt the Korea Tourism Organization Center in central Seoul, numerous activities have been organized for foreigners residing in Korea.\nThe center has been teaching foreigners how to play folk games since Thursday and has hanbok on hand to try.\nAt an origami event, participants can fold a lucky bag traditionally used by Korean children to carry cash gifts received during the Lunar New Year.\nThe events will continue until Feb. 16. For more information, call (02) 729-9600 or visit www.visitkorea.or.kr.\nYutnori(Five-stick board game): Yutnori is a traditional stick-throwing game. Five sticks representing “do,” “gae” “gul” “yut” and “mo” get thrown in the air in turns. The five sticks act as dice to move the tokens on the board.\nYeonnallig(Kite flying)i: Koreans don’t just fly regular kites during the Lunar New Year. They write well-wishing phrases or symbols on their kites and fly them as high as they can to wish for health and happiness. They also play games by competing to see who can cut another’s kite string first.\nJegichagi(Shuttlecock kicking): This is a Korean version of shuttlecock in which a player kicks a jegi up in the air and has to keep on kicking to prevent it from falling to the ground. It’s a one-versus-one game, and the player with the most consecutive kicks wins. Jegi looks like a badminton shuttlecock and is made of a small coin with paper or cloth attached to it.\nTuho(Arrow-throwing): In this game, players throw arrows into a narrow-necked jar. It used to be a form of entertainment for aristocrats in royal courtyards and noblemen’s residences. Each team tries to see how many arrows it can shoot into a vessel.\nNeolttwigi(Seesaw): This is a Korean version of seesaw that is played standing up. The object of the game is to stay on the seesaw through a series of jumps without falling down.\nPengichigi(Top-spinning): In this game, players try to knock over the opponent’s spinning top. The person whose top is still spinning is the winner. Pengichigi is popular during the Lunar New Year as the top spins very quickly on the icy ground.\nBy Yim Seung-hye [firstname.lastname@example.org]']	['<urn:uuid:90c8fbea-2178-424d-b45c-28875a1aeeaa>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T00:20:10.754925	6	73	1341
54	As a peacebuilding practitioner, I'm interested in understanding how peacebuilding processes promote sustainable peace, and how social media's design might affect conflict resolution between opposing groups. Can you explain the connection?	Peacebuilding promotes sustainable peace by creating self-supporting structures that address root causes of conflict and transform relationships. It includes creating spaces where people interact in new ways and develop new means of communication. When it comes to social media's role, research shows that platform design can significantly impact conflict resolution. For example, features that encourage moving contentious public discussions to private messaging have proven effective in resolving conflicts, similar to how we naturally move heated discussions to private spaces in person. Additionally, the way social media is designed affects how people interact with each other, with proper design elements helping users feel more supportive and kind during online conflicts.	['Selected Definitions of Peacebuilding\nThe following list is not exhaustive and only provides some of the most prominent academic and institutional definitions of peacebuilding.\nConflict Information Consortium, University of Colorado\nPeacebuilding is a process that facilitates the establishment of durable peace and tries to prevent the recurrence of violence by addressing root causes and effects of conflict through reconciliation, institution building, and political as well as economic transformation. This consists of a set of physical, social, and structural initiatives that are often an integral part of postconflict reconstruction and rehabilitation.\nPeacebuilding is the process of creating self-supporting structures that “remove causes of wars and offer alternatives to war in situations where wars might occur.” Conflict resolution mechanisms “should be built into the structure and be present there as a reservoir for the system itself to draw upon, just as a healthy body has the ability to generate its own antibodies and does not need ad hoc administration of medicine.”\nGlobal Partnership for the Prevention of Armed Conflict\nPeacebuilding involves addressing social and political sources of conflict as well as reconciliation.\nJoan B. Kroc Institute for Peace & Justice, University of San Diego\nStrategic Peacebuilding Principles:\n- Peacebuilding is complex and has multiple actors.\n- Peacebuilding requires values, goals, commitment to human rights and needs.\n- Peacebuilding goes beyond conflict transformation.\n- Peacebuilding cannot ignore structural forms of injustice and violence.\n- Peacebuilding is founded on an ethic of interdependence, partnership, and limiting violence.\n- Peacebuilding depends on relational skills.\n- Peacebuilding analysis is complex; underlying cultures, histories, root causes, and immediate stressors are essential.\n- Peacebuilding creates spaces where people interact in new ways, expanding experience and honing new means of communication.\n- Peacebuilding heals trauma, promotes justice and transforms relationships.\n- Peacebuilding requires capacity and relationship building at multiple levels.\nLederach, John Paul\n[Peacebuilding] is understood as a comprehensive concept that encompasses, generates, and sustains the full array of processes, approaches, and stages needed to transform conflict toward more sustainable, peaceful relationships. The term thus involves a wide range of activities that both precede and follow formal peace accords. Metaphorically, peace is seen not merely as a stage in time or a condition. It is a dynamic social construct.\nOrganization for Economic Cooperation and Development\n[Peacebuilding] includes activities designed to prevent conflict through addressing structural and proximate causes of violence, promoting sustainable peace, delegitimizing violence as a dispute resolution strategy, building capacity within society to peacefully manage disputes, and reducing vulnerability to triggers that may spark violence.\nSchool of Conflict Analysis and Resolution at George Mason University\nPeacebuilding is a term used within the international development community to describe the processes and activities involved in resolving violent conflict and establishing a sustainable peace. It is an overarching concept that includes conflict transformation, restorative justice, trauma healing, reconciliation, development, and leadership, underlain by spirituality and religion. It is similar in meaning to conflict resolution but highlights the difficult reality that the end of a conflict does not automatically lead to peaceful, stable social or economic development. A number of national and international organizations describe their activities in conflict zones as peacebuilding.\nUnited Nations Development Program\nPeacebuilding involves a range of measures targeted to reduce the risk of lapsing or relapsing into conflict by strengthening national capacities at all levels for conflict management, and laying the foundations for sustainable peace and development. Peacebuilding strategies must be coherent and tailored to the specific needs of the country concerned, based on national ownership, and should comprise a carefully prioritized, sequenced, and therefore relatively narrow set of activities aimed at achieving the above objectives. This office works specifically with peacebuilding in the context of conflict prevention.\nUnited Nations: Peacebuilding Support Office\nPeacebuilding is rather the continuum of strategy, processes and activities aimed at sustaining peace over the long-term with a clear focus on reducing chances for the relapse into conflict…. [It] is useful to see peacebuilding as a broader policy framework that strengthens the synergy among the related efforts of conflict prevention, peacemaking, peacekeeping, recovery and development, as part of a collective and sustained effort to build lasting peace. This office works specifically with peacebuilding in the context of postconflict reconstruction.\nUnited States Institute of Peace\nOriginally conceived in the context of postconflict recovery efforts to promote reconciliation and reconstruction, the term peacebuilding has more recently taken on a broader meaning. It may include providing humanitarian relief, protecting human rights, ensuring security, establishing nonviolent modes of resolving conflicts, fostering reconciliation, providing trauma healing services, repatriating refugees and resettling internally displaced persons, supporting broad-based education, and aiding in economic reconstruction. As such, it also includes conflict prevention in the sense of preventing the recurrence of violence, as well as conflict management and postconflict recovery. In a larger sense, peacebuilding involves a transformation toward more manageable, peaceful relationships and governance structures—the long-term process of addressing root causes and effects, reconciling differences, normalizing relations, and building institutions that can manage conflict without resort to violence. The US Government does not have a publicly available definition of peacebuilding, other than the definition provided by USIP.', 'Disrupted sleep, decrease life satisfaction and poor vanity are just some of the adverse psychological well being penalties that researchers have linked to social media. In some way the identical platforms that may assist folks really feel extra linked and educated additionally contribute to loneliness and disinformation. What succeeds and fails, scientists say, is a perform of how these platforms are designed. Amanda Baughan, a graduate pupil specializing in human-computer interplay on the College of Washington, research how social media triggers what psychologists name dissociation, or a state of diminished self-reflection and narrowed consideration. She offered outcomes on the 2022 Affiliation for Computing Equipment Laptop-Human Interplay Convention on Human Elements in Computing Techniques. Baughan spoke with Thoughts Issues editor Daisy Yuhas to elucidate how and why apps want to alter to present the individuals who use them better energy.\n[An edited transcript of the interview follows.]\nYou’ve proven how altering social media cues and shows may enhance well-being, even when folks strongly disagree on points. Are you able to give an instance?\nThe design of social media can have quite a lot of energy in how folks work together with each other and the way they really feel about their on-line experiences. For instance, we’ve discovered that social media design can truly assist folks really feel extra supportive and type in moments of on-line battle, offered there’s a bit little bit of a nudge to behave that method. In a single examine, we designed an intervention that inspired individuals who begin speaking about one thing contentious in a remark thread to change to direct messaging. Folks actually preferred it. It helped to resolve their battle and replicated an answer we use in-person: folks having a public argument transfer to a personal area to work issues out.\nYou’ve additionally tackled a unique downside popping out of social media utilization known as the 30-Minute Ick Issue. What’s that?\nWe in a short time lose ourselves on social media. When folks encounter a platform the place they will infinitely scroll for extra data, it may set off an identical neurocognitive reward system as in anticipating a profitable lottery ticket or getting meals. It’s a robust method that these apps are designed to maintain us checking and scrolling.\nThe 30-Minute Ick Issue is when folks imply to examine their social media briefly however then discover that half-hour have handed, and once they notice how a lot time they’ve spent, they’ve this sense of disgust and disappointment in themselves. Analysis has proven that individuals are dissatisfied with this ordinary social media use. Lots of people body it as meaningless, unproductive or addictive.\nYou’ve argued this expertise is much less a matter of dependancy and extra a problem of dissociation. Why?\nDissociation is a psychological course of that is available in many varieties. In the most typical, on a regular basis dissociation, your thoughts is so absorbed that you’re disconnected out of your actions. You could possibly be doing the dishes, begin daydreaming and never take note of how you might be doing the dishes. Otherwise you would possibly search immersive experiences—watching a film, studying a e-book or taking part in a recreation—that move the time and trigger you to overlook the place you might be.\nThroughout these actions, your sense of reflective self-consciousness and the passage of time is diminished. Folks solely notice that they dissociated in hindsight. Consideration is restored with the sense of “What simply occurred?” or “My leg fell asleep whereas we had been watching that film!”\nDissociation generally is a constructive factor, particularly if it’s an absorbing expertise, significant exercise or a wanted break. But it surely can be dangerous in sure instances, as in playing, or are available in battle with folks’s time-management targets, as with social media scrolling.\nHow do you measure folks’s dissociation on social media?\nWe labored with 43 contributors who used a customized cell app that we created known as Chirp to entry their Twitter accounts. The app let folks work together with Twitter content material whereas permitting us to ask them questions and take a look at interventions. So when folks had been utilizing Chirp, after a given variety of minutes, we’d ship them a questionnaire primarily based on a psychological scale for measuring dissociation. We requested how a lot they agreed with the assertion “I’m presently utilizing Chirp with out actually listening to what I’m doing” on a scale of 1 to five. We additionally did interviews with 11 folks to be taught extra. The outcomes confirmed dissociation occurred in 42 p.c of our contributors, they usually usually reported shedding observe of time or feeling “all-consumed.”\nYou designed 4 interventions that changed folks’s Twitter expertise on Chirp to scale back dissociation. What labored?\nProbably the most profitable had been customized lists and studying historical past labels. In customized lists, we pressured customers to categorize the content material they adopted, comparable to “sports activities” or “information” or “buddies.” Then, as an alternative of interacting with Twitter’s major feed, they engaged solely with content material on these lists. This method was coupled with a studying historical past intervention through which folks acquired a message once they had been caught up on the most recent tweets. Moderately than persevering with to scroll, they had been alerted to what that they had already seen, and they also targeted on simply the most recent content material. These interventions diminished dissociation, and once we did interviews, folks stated they felt safer checking their social media accounts when these modifications had been current.\nIn one other design, folks acquired timed messages letting them know the way lengthy that they had been on Chirp and suggesting they go away. In addition they had the choice of viewing a utilization web page that confirmed them statistics comparable to how a lot time they’d spent on Chirp prior to now seven days. These two options had been efficient if folks opted to make use of them. Many individuals ignored them, nevertheless. Additionally, they thought the timed messages had been annoying. These findings are fascinating as a result of quite a lot of the favored time-management instruments obtainable to folks appear like these time-out and utilization notifications.\nSo what may social media firms be doing in a different way? And is there any incentive for them to alter?\nProper now there’s a lot working towards individuals who use social media. It’s unimaginable to ever totally compensate for a social media feed, particularly when you think about the algorithmically inserted content material comparable to Twitter’s trending tweets or TikTok’s “For You” web page. However I feel that there’s hope that comparatively easy tweaks to social media design, comparable to customized lists, could make a distinction. It’s essential to notice that the customized lists considerably diminished dissociation for folks—however they did not considerably have an effect on time spent utilizing the app. To me, that factors out that lowering folks’s dissociation will not be as antithetical to social media firms’ income targets as we’d intuitively suppose.\nWhat’s most essential for folks utilizing social media now to know?\nFirst, don’t pile a bunch of disgrace onto your social media habits. Hundreds of individuals are employed to make you swipe your thumb up on that display screen and maintain you doing what you’re doing. Let’s shift the duty of designing protected and fulfilling experiences from customers to the businesses.\nSecond, get conversant in the well-being instruments which are already supplied. TikTok has a function that, each hour, will inform you that you simply’ve been scrolling for some time and may contemplate a break. On Twitter, customized lists are a function that already exists; it’s simply not the default choice. If extra folks begin utilizing these instruments, it may persuade these firms to refine them.\nMost essential, vote for people who find themselves occupied with regulating expertise as a result of I feel that’s the place we’re going to see the largest adjustments made.']	['<urn:uuid:7a473c9a-2a9f-45a1-8356-9071b7f65836>', '<urn:uuid:28ea44cd-4054-4290-8fcb-a13be7579900>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T00:20:10.754925	31	109	2171
55	who went to meet luther wittenberg june 1541	A deputation headed by John of Anhalt arrived at Wittenberg on June 9, 1541 to seek Luther's consent regarding the religious agreements reached at the Regensburg Conference.	"['Diet of Regensburg (1541)\nThe Colloquy of Regensburg, historically called the Colloquy of Ratisbon, was a conference held at Regensburg (Ratisbon) in 1541, during the Protestant Reformation, which marks the culmination of attempts to restore religious unity in the Holy Roman Empire by means of theological debate.\nDelegates from the various factions had met at Haguenau in 1540 and at Worms in January 1541 but the latter session of the Imperial Diet was adjourned by the Emperor Charles V as the Diet was preparing to meet at Regensburg. The subject for debate was to be the Augsburg Confession, the primary doctrinal statement of the Protestant movement, and the Apology of the Augsburg Confession, a defence of the Confession written by Philipp Melancthon.\nOn December 15, 1540, a secret conference took place between Johann Gropper, canon of Cologne, and Gerhard Veltwick, the Imperial secretary, on the one side and Butzer and Capito, the delegates of Protestant Strasbourg, on the other. The two sides agreed their positions on original sin and justification, but the promise made by the Catholic party at Haguenau, to negotiate on the basis of the Confession and Apology, was withdrawn.\nEarly in 1541, Butzer sent a draft of the conclusions to Joachim II, Elector of Brandenburg, with the request to communicate it to Luther and the other princes of the Protestant league. The document was essentially identical with the later so-called Regensburg Book, which formed the basis of the Regensburg Conference in place of the Augsburg Confession.\nIt was divided into twenty-three articles, some of which closely approached the Protestant view; but it decided no questions of dogma and did not exclude the Catholic positions. On February 13, 1541, the book reached the hands of Luther. In spite of the apparent concessions made in regard to the doctrine of justification, he perceived that the proposed articles of agreement could be accepted by neither party.\nOn February 23, 1541, the Emperor entered Regensburg. In consideration of his difficult political situation, especially of the threatened war with the Ottoman Turks and the negotiations of the French king with the Protestants in his country, it was his desire to pacify Germany. The conference was opened on April 5. The negotiators were Gropper, Pflug, and Eck on the Catholic side, under the oversight of the Papal Legate Cardinal Contarini, Bucer, the elder Johannes Pistorius, and Melanchthon for the Protestants. Besides the presidents, Count Palatine Frederick and Granvella, six witnesses were present, among them Burkhardt and Feige, chancellors of Saxony and Hesse respectively, and Jakob Sturm of Strasbourg.\nThe first four articles, on the condition and integrity of man before the fall, on free will, on the cause of sin, and on original sin, passed without difficulty. The article on justification encountered great opposition, especially from Eck, but an agreement was finally arrived at; neither Elector John Frederick nor Luther was satisfied with this article. With respect to the articles on the doctrinal authority of the Church, the hierarchy, discipline, sacraments, etc., no agreement was possible, and they were all passed over without result. On May 31 the book with the changes agreed upon and nine counter-propositions of the Protestants was returned to the Emperor. In spite of the opposition of Mainz, Bavaria, and the Imperial legate, Charles V still hoped for an agreement on the basis of the articles which had been accepted by both parties, those in which they differed being postponed to a later time.\nAs it was perceived that all negotiations would be in vain if the consent of Luther were not obtained, a deputation headed by John of Anhalt arrived at Wittenberg, where Luther resided, on June 9. Luther answered in a polite and almost diplomatic way. He expressed satisfaction in reference to the agreement on some of the articles, but did not believe in the sincerity of his opponents and made his consent dependent upon conditions which he knew could not be accepted by the Roman Catholics.\nBefore the deputation had returned from Wittenberg, the Roman party had entirely destroyed all hope of union. The formula of justification, which Contarini had sent to Rome, was rejected by a papal consistory. Rome declared that the matter could be settled only at a council, and this opinion was shared by the stricter party among the estates. Albert of Mainz urged the Emperor to take up arms against the Protestants. Charles V tried in vain to induce the Protestants to accept the disputed articles, while Joachim of Brandenburg made new attempts to bring about an agreement. With every day the gulf between the opposing parties became wider, and both of them, even the Roman Catholics, showed a disposition to ally themselves with France against the Emperor.\nOutcome of the Conference\nThus the fate of the Regensburg Book was no longer doubtful. After Elector John Frederick and Luther had become fully acquainted with its contents, their disinclination was confirmed, and Luther demanded most decidedly that even the articles agreed upon should be rejected. On July 5 the estates rejected the Emperor\'s efforts for union. They demanded an investigation of the articles agreed upon, and that in case of necessity they should be amended and explained by the Papal legate. Moreover, the Protestants were to be compelled to accept the disputed articles; in case of their refusal a general or national council was to be convoked. Contarini received instructions to announce to the Emperor that all settlement of religious and ecclesiastical questions should be left to the Pope. Thus the whole effort for union was frustrated, even before the Protestant estates declared that they insisted upon their counterproposals in regard to the disputed articles.\nThe supposed results of the religious conference were to be laid before a general or national council or before an assembly of the Empire which was to be convoked within eighteen months. In the mean time the Protestants were bound to adhere to the articles agreed upon, not to publish anything on them, and not to abolish any churches or monasteries, while the prelates were requested to reform their clergy at the order of the legate. The peace of Nuremberg was to extend until the time of the future council, but the Augsburg Recess was to be maintained.\nThese decisions might have become very dangerous to the Protestants, and in order not to force them into an alliance with his foreign opponents, the Emperor decided to change some of the resolutions in their favor; but the Roman Catholics did not acknowledge his declaration. As he was not willing to expose himself to an intervention on their part, he left Regensburg on June 29, without having obtained either an agreement or a humiliation of the Protestants, and the Roman party now looked upon him with greater mistrust than the Protestants.\n|This article does not cite any references or sources. (August 2012)|\n- This article incorporates text from a publication now in the public domain: T Kolde (1914). ""Regensburg, Conference of"". In Jackson, Samuel Macauley. New Schaff–Herzog Encyclopedia of Religious Knowledge (third ed.). London and New York: Funk and Wagnalls.\n- I. Sources\n- Klaus Ganzer (Hrsg): Akten der deutschen Reichsreligionsgespräche im 16. Jahrhundert. Band 3: Das Regensburger Religionsgespräch (1541). 2 Teilbände. Vandenhoeck & Ruprecht, Göttingen 2007.\n- Johannes Calvins Lebenswerk in seinen Briefen. Band 1. Mohr, Tübingen 1909, pp. 118–129.\n- II. Secondary literature\n- Hans-Martin Barth u.a.: Das Regensburger Religionsgespräch im Jahr 1541. Rückblick und ökumenische Perspektiven. Pustet, Regensburg 1992, ISBN 3-7917-1318-3.\n- Wolf Dieter Hauschild: Lehrbuch der Kirchen- und Dogmengeschichte. Band 2: Reformation und Neuzeit. 3. Auflage. Gütersloher Verlag-Haus u. a., Gütersloh 2005, ISBN 3-579-00094-2, pp. 145ff.\n- Athina Lexutt: Rechtfertigung im Gespräch. Das Rechtfertigungsverständnis in den Religionsgesprächen von Hagenau, Worms und Regensburg 1540/41. Vandenhoeck & Ruprecht, Göttingen 1996, ISBN 3-525-55172-X, (Forschungen zur Kirchen- und Dogmengeschichte 64), (Also: Bonn, Univ., Diss., 1994/95).\n- Otto Scheib: Die innerchristlichen Religionsgespräche im Abendland. Regionale Verbreitung, institutionelle Gestalt, theologische Themen, kirchenpolitische Funktion. Mit besonderer Berücksichtigung des konfessionellen Zeitalters (1517 - 1689). Harrassowitz, Wiesbaden 2009, ISBN 978-3-447-06133-9, (Wolfenbütteler Forschungen Bd. 122), pp. 186f.\n- Gehrhard B. Winkler: Das Regensburger Religionsgespräch 1541. In: Albrecht, Dieter (ed.): Regensburg - Stadt der Reichstage. Vom Mittelalter zur Neuzeit. Regensburg 1994, (Schriftenreihe der Universität Regensburg 21), ISBN 3-9803470-9-5, pp. 72–81.\n- Karl-Heinz zur Mühlen: Die Reichsreligionsgespräche von Hagenau, Worms und Regensburg 1540/41, Chancen und Grenzen des kontroverstheologischen Dialogs in der Mitte des 16. Jahrhunderts. In: Blätter für pfälzische Kirchengeschichte und religiöse Volkskunde (BPfKG) 72, 2005, ISSN 0341-9452, S. 319–334.']"	['<urn:uuid:cdc6a90a-2000-41aa-82e0-7c4ab89dd168>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T00:20:10.754925	8	27	1412
56	international recognition achievements compare journalist scientist	Both achieved significant international recognition in their respective fields. Rajagopalan won journalism's highest honor, the Pulitzer Prize, for her investigative reporting on China's Muslim detention camps, despite facing harassment from the Chinese government. Bar-Yam's scientific work has been recognized by major institutions including the World Economic Forum, United Nations, and his research has been cited by leading global media outlets like The New York Times and BBC, while also receiving grants from organizations like the Department of Defense and NIH.	"['Indian-origin journalist Megha Rajagopalan has won Pulitzer Prize exposing China’s mass detention camps for Muslim Uighurs and other minority ethnicites. Award is shared with two colleagues from an internet media, BuzzFeed News Reports IANS.\nIndian-origin journalist Megha Rajagopalan has won the US’ top journalism award, the Pulitzer Prize, for innovative investigative reports harnessing satellite technology that exposed China’s mass detention camps for Muslim Uighurs and other minority ethnicites.\nThe award in the international reporting category that she shared with two colleagues from an internet media, BuzzFeed News, was announced on Friday by the Pulitzer Board.\nAnother journalist of Indian-origin, Neil Bedi, won a Pulitzer in the local reporting category for investigative stories he wrote with an editor at the Tampa Bay Times exposing the misuse of authority by a law enforcement official in Florida to track children.\nThis is the 105th year of the Pulitzer Prizes awarded by a board at Columbia University’s Graduate School of Journalism in New York recognising the outstanding work.\nIn recognition of the proliferation of citizen journalism in the internet age, teenaged non-journalist, Darnella Frazier, was awarded a Pulitzer Special Citation for her courage in filming the killing of George Floyd, the African-American who died in police custody in Minneapolis last year.\nThe video clip made on her smartphone went viral and set off prolonged nationwide protests against police brutality and led to measures in many states and cities to reform policing.\nThe sight of a policeman kneeling on the neck of dying Floyd as he repeated, “I can’t breathe”, appealed to America’s conscience and led to a broader consideration of the problems faced by African-Americans.\nThe Board said her that her video “spurred protests against police brutality around the world, highlighting the crucial role of citizens in journalists’ quest for truth and justice”.\nRajagopalan and her colleagues used satellite imagery and 3D architectural simulations to buttress her interviews with two dozen former prisoners from the detention camps where as many as a million Muslims from Uighur and other minority ethnicites were interned.\n“I’m in complete shock, I did not expect this,” she said.\nAccording to the publication, she and her colleagues, Alison Killing and Christo Buschek, identified 260 detention camps after building a voluminous database of about 50,000 possible sites comparing censored Chinese images with uncensored mapping software.\nRajagopalan, who had previously reported from China but was barred from there for the story, travelled to neighbouring Kazhakstan to interview former detainees who had fled there, BuzzFeed said.\n“Throughout her reporting, Rajagopalan had to endure harassment from the Chinese government,” the publication said.\nThe series of stories provided proof of Beijing’s violation of Uighurs’ human rights, which some US and other Western officials have called a “genocide”.\nBedi and Kathleen McGrory were given their award for exposing “how a powerful and politically connected sheriff built a secretive intelligence operation that harassed residents and used grades and child welfare records to profile schoolchildren”, the Pulitzer Board said.\nBedi, who has a degree in computer science, is now a Washington-based reporter for ProPublica.\nHere is the full list of 2021 Pulitzer Prize winners:\nThe Pulitzer Prizes, usually announced at a ceremony at Columbia University, were announced remotely again this year for the second year in a row due to the coronavirus pandemic.\nThe Pulitzers are generally regarded as the highest honor that a U.S.-based journalist or organization can receive.\nHere are the 2021 Pulitzer Prize winners reported by poynter.org\nAwarded to Darnella Frazier, for courageously recording the murder of George Floyd.\nBreaking News Reporting\nAwarded to the staff of The (Minneapolis) Star Tribune for nuanced coverage of the death of George Floyd by Minneapolis police and the reverberations that followed\n- The staff of The Courier-Journal of Louisville, Kentucky, for coverage that contradicted the police narratives following the killing of Breonna Taylor and the aftermath\n- Helen Branswell, Andrew Joseph and the late Sharon Begley of Stat for their prescient coverage of the emergence of COVID-19\nAwarded to Matt Rocheleau, Vernal Coleman, Laura Crimaldi, Evan Allen and Brendan McCarthy of The Boston Globe for reporting that uncovered state governments’ systematic failure to share information about dangerous truck drivers\n- Dake Kang and the staff of The Associated Press for an investigation of China’s state secrecy and fatal consequences in the country’s response to the coronavirus outbreak and human rights abuses against Uyghurs\n- Margie Mason and Robin McDowell of The Associated Press for an examination of the abusive practices of international palm oil producers\nAwarded to Ed Yong of The Atlantic for a series of lucid pieces on the COVID-19 pandemic that anticipated the course of the disease, synthesized complex challenges, illuminated the U.S. government’s failures and provided clear context for the challenges it posed\nAwarded to Andrew Chung, Lawrence Hurley, Andrea Januta, Jaimi Dowdell and Jackie Botts of Reuters for an exhaustive examination, powered by data analysis of U.S. federal court cases, of the legal doctrine of “qualified immunity” and how it shields police who use excessive force from prosecution\nAwarded to Kathleen McGrory and Neil Bedi of the Tampa Bay Times for resourceful reporting that exposed how a powerful and politically connected sheriff built a secretive intelligence operation that used grades and welfare records to profile schoolchildren\n- Jack Dolan and Brittny Mejia of the Los Angeles Times for exposing failures in Los Angeles County’s safety-net healthcare system that led to monthslong wait times for some patients, including some who died before getting appointments\n- Staff of The Post and Courier of Charleston, South Carolina, for a look at how water levels in the city were rising faster than previously thought and the social, environmental and regulatory challenges climate change poses\nAwarded to the staffs of The Marshall Project, Alabama Media Group, The Indianapolis Star and the Invisible Institute for a yearlong investigation of K-9 units and the damage that police dogs inflict on Americans\n- Staff of The New York Times for detailed reporting on how the Trump administration repeatedly failed to respond to the coronavirus threat, including downplaying its seriousness\n- Staff of The Wall Street Journal for a series of stories documenting how nursing home residents were hit particularly hard by the coronavirus pandemic partially due to improper decisions from government officials\nAwarded to Megha Rajagopalan, Alison Killing and Christo Buschek of BuzzFeed News for a series of stories that used satellite imagery, architectural expertise and interviews with two dozen former prisoners to identify a vast new infrastructure built by the Chinese government for the mass detention of Muslims\n- Staffs of BuzzFeed News and the International Consortium of Investigative Journalists for a massive reporting project that led to revelations about the ongoing role of some of the world’s biggest banks in facilitating international money laundering and trafficking of goods and people\n- Staff of The New York Times for a masterful synthesis of stellar writing, powerful images and engaging interactives that illustrated how the world was unprepared for a fast-moving pandemic\n- Staff of The Wall Street Journal for an authoritative and deeply reported portrait of Xi Jinping and his increasingly authoritarian control of the state, economy and politics\nAwarded to Nadja Drost, freelance contributor to The California Sunday Magazine, for a gripping account of global migration that documents a group’s journey on foot through the Darién Gap, one of the most dangerous migrant routes in the world\nMORE FROM POYNTER: California Sunday Magazine closed last year. It just won a Pulitzer.\nAwarded to Mitchell S. Jackson, freelance contributor to Runner’s World, for a deeply affecting account of the killing of Ahmaud Arbery that combined vivid writing, thorough reporting and personal experience to shed light on systemic racism in America\n- Greg Jaffe and The Washington Post for deeply reported stories that powerfully depict the suffering endured by Americans who lost their jobs after the collapse of South Florida’s tourist economy in the pandemic\nAwarded to Michael Paul Williams of the Richmond (Virginia) Times-Dispatch for penetrating and historically insightful columns that led Richmond, a former capital of the Confederacy, through the painful and complicated process of dismantling the city’s monuments to white supremacy\n- Melinda Henneberger of The Kansas City Star for tenacious and deeply reported columns on failures in the criminal justice system\n- Roy S. Johnson of Alabama Media Group for evocative columns on race and remembrance written with style, urgency and moral clarity\nAwarded to Wesley Morris of The New York Times for unrelentingly relevant and deeply engaged criticism on the intersection of race and culture in America, written in a singular style, alternatively playful and profound\n- Craig Jenkins of New York magazine for writing on a range of popular topics, including social media, music and comedy, that contending with the year’s disarray and exploring how culture and conversation can flourish and break down online\n- Mark Swed of the Los Angeles Times for a series of essays that broke through the silence of the pandemic to recommend an eclectic array of recordings as entertainment and solace essential to the moment\nAwarded to Robert Greene of the Los Angeles Times for editorials on policing, bail reform, prisons and mental health that clearly and holistically examined the Los Angeles criminal justice system\n- Lee Hockstader of The Washington Post for a series of editorials that pushed for accountability in the shooting and killing of an unarmed man by U.S. Park Police.\n- Alan Wirzbicki and Rachelle G. Cohen of The Boston Globe for editorials that addressed a controversial local zoning fight that centered on the legacy of restrictive housing laws.\nNo prizes were awarded in this category\n- Lalo Alcaraz of Andrews McMeel Syndicate for cartoons drawn from an unabashedly specific Latino point of view that target the hearts and consciences of Americans\n- Ken Fisher, drawing as Ruben Bolling, for “Tom the Dancing Bug,” part of Andrews McMeel Syndicate, for an effective example of the alternative cartooning form that uses multiple panels to create a satirical take on policial issues\n- Marty Two Bulls Sr., freelance cartoonist, for innovative and insightful cartoons that offer a Native American perspective on news events\nBreaking News Photography\nAwarded to photography staff of The Associated Press for a collection of photographs from multiple U.S. cities that cohesively captures the country’s response to the death of George Floyd\n- Hassan Ammar, Hussein Malla and Felipe Dana of The Associated Press for a series of images that documented the uncertainty and devastation following a blast that rocked Beirut\n- Joshua Irwandi, freelance photographer, National Geographic, for a haunting photograph of a coronavirus victim in an Indonesian hospital who was wrapped in plastic and awaiting a body bag\nAwarded to Emilio Morenatti of The Associated Press for a poignant series of photographs that takes viewers into the lives of the elderly in Spain struggling during the pandemic\n- Staff of Getty Images for thorough coverage of the global impact of the pandemic\n- Tyler Hicks of The New York Times for searing images that capture the toll of the coronavirus deep in Brazil’s Amazon and how it ravaged the region’s Indigenous people\nAwarded to Lisa Hagen, Chris Haxel, Graham Smith and Robert Little of NPR for an investigative series on no-compromise gun rights activists that illuminated the deepening schism between American conservatives\n- Staffs of the Invisible Institute, The Intercept and Topic Studios for “Somebody,” a dogged and searing investigation of the murder of a young Black man in Chicago\n- Staff of NPR for courageous, on-the-ground reporting on the assassination of Iranian General Qassem Soleimani and its implications around the globe\nPublic Service Reporting\nAwarded to The New York Times for courageous, prescient and sweeping coverage of the coronavirus pandemic that exposed racial and economic inequities, government failures in the U.S. and beyond, and filled a data vacuum that helped local governments, health care providers, businesses and individuals to be better prepared and protected.\n- The Courier-Journal of Louisville, Kentucky, for comprehensive coverage of the police killing of Breonna Taylor and systemic racism in the police force\n- ProPublica for insightful coverage of the coronavirus pandemic\nMore from Poynter\n- Behind the Pulitzers: A look into the inner workings of journalism’s Super Bowl\n- What journalism students can learn from this year’s Pulitzer Prizes\n- Here’s some of the journalism that won past Pulitzers for covering pandemics and epidemics\n- Coronavirus and race (and not Trump) dominate the 2021 Pulitzer Prizes', 'Compassionate Science for Humanity\n“Since time immemorial, humans have complained that life is becoming more complex. But it is only now that we have a hope to analyze formally and verify this lament.”\n— Yaneer Bar-Yam\nYaneer Bar-Yam personifies a unique alchemy of heart and intellect. He has the mind of a world-renowned physicist and mathematician with the ethos of a humanitarian. Yaneer has pioneered the field of complex systems, studying the interconnectedness of how our world works for the benefit of humanity. As the founder and president of the New England Complex Systems Institute, he gets to the bottom of some of the world’s most complicated problems through applied research and unique mathematical modeling. His widely cited and impactful work spans energy markets, global supply chains, economic and financial crises, military strategy, political instability, neuroscience, cell biology, public health and pandemics, including Ebola and COVID-19.\nEvolving as a Humanist\nBorn into a family of brilliant scientists, mathematicians and intellectuals, Yaneer was predestined for a career in math and science.\nHe was born on August 29, at Beth Israel Hospital in Boston. His father, Zvi Bar-Yam, was born in Kraków, Poland, where he lived through the tragedy chronicled in the book and movie, “Schindler’s List.” As a survivor of the Holocaust, Zvi emigrated to Israel, where he met Yaneer’s mother, Miriam. They immigrated to the United States to initially study in Pittsburgh and then Connecticut, eventually moving to Brookline, Massachusetts, to raise their three children — Aureet, Yaneer and Sageet.\nZvi was a groundbreaking particle physicist and professor, trained at Carnegie Mellon University and MIT. And Miriam, educated at Harvard University, was a world-renowned authority on child developmental psychology. Zvi taught Yaneer mathematics even as his son was learning how to walk.\nIt was a joyous childhood, recalls Yaneer. “My mother believed in me. She made sure that I didn’t feel that I was better than others.” says Yaneer. “My father told me I don’t have to do everything. I just have to make a contribution.” He was close to his older sister, Aureet, and younger sister, Sageet. “Aureet taught me everything she knew in school,” says Yaneer.\nOne day in 1967, as an elementary student at Michael Driscoll School, in Brookline, Mass., Yaneer saw on television devastating images from halfway around the world that, unbeknownst to him, would become pivotal in charting the course of his life and work. The images were of starving children who were victims of the Nigerian civil war for the independence of a region called Biafra. Those images of wide-eyed, pot-bellied children touched Yaneer deeply and propelled him to make food security one of the pillars of his lifelong study of complex systems, offering real world, practical solutions to a pernicious problem.\nYaneer could have been anything he wanted to be. He loved math, science and chess, trained in piano and swam competitively, excelling in virtually everything he did.\nNaturally gifted, Yaneer finished high school in three years and graduated from MIT in 1978 after two years of study at the age of 18.\nAfter completing his bachelor’s degree in physics, he pursued applied physics at MIT and was awarded his Ph.D in 1984.\nMaking a Life-Changing Decision\nAn unexpected conversation took Yaneer on an entirely new trajectory in his career.\nYaneer may have continued on his academic path had it not been for a lecture he delivered on his research in theoretical physics. After his presentation, Yaneer asked an IBM scientist in attendance whether he had found the talk helpful. “No,” the scientist replied, bluntly adding that Yaneer’s analysis had zero practical impact on his applied work. Shocked by the negative review, Yaneer knew that he had to pivot and focus on meaningful, tangible outcomes from his research. It was a turning point in his life.\nFascinated by the emerging theory of complex systems – a rarefied, multidisciplinary study of how the parts of systems interconnect and collectively interact with their surroundings, Yaneer decided to apply his interest and physics training to find practical applications and real world solutions, becoming a joint postdoctoral fellow at MIT and IBM.\nOver the next decade, Yaneer held academic positions at the Weizmann Institute of Science and Cornell University, and he joined the faculty of Boston University as an associate professor of engineering.\nHis research addressed a diverse array of topics including the science of growing diamonds, the workings of neural networks and the underlying causes of climate change. But he found his true passion identifying the complex causes of global tragedies, such as food crises.\nAs a young academic, Yaneer experienced tragedy when his older sister, Aureet, died on January 7, 1991, at the age of 33, drowning as she tried to pull her dog, Flame, out of the icy water of Sandy Pond in Lincoln, Mass. Authorities botched the rescue effort in a long, drawn out series of failures. He not only lost a beloved sister but also a kindred spirit who had tutored him in the magic of mathematics when he was a young boy.\nAs a complex systems practitioner, Yaneer uses an innovative discipline of subterranean mathematical calculation to explain and predict global trends, such as food scarcity, ethnic cleansing, evolutionary biology, election outcomes, and pandemics, including COVID-19.\n“It’s the narrative of math. It’s all rooted in an understanding of multiscale analysis,” says Yaneer, who turned to Nobel Prize Laureate mathematician Ken Wilson’s 1970s principles of “renormalization group” to carry out his examination of complex systems.\n“Everybody studies in school how to use math. You have a set of equations, or you have an assumption. What can you prove? It’s set theory geometry,” says Yaneer. “When you know math really well, that’s really easy. The challenging part is making sure you have the right assumptions That’s the only thing that really matters. If you have the right assumption you can get the right conclusion.”\nRedoubling his efforts to make a tangible difference through applied science, Yaneer founded the New England Complex Science Institute in 1996. The institute is a multidisciplinary consortium of top academics who apply systems modeling and research to solve real-world problems.\nYaneer has attracted an illustrious community of international experts in complex science and related fields from Harvard, MIT, Yale and elsewhere working together to solve problems through applied science and modelling.\nLeading the Field\nYaneer had a prolific decade, publishing three books and more than 200 scientific papers.\nYaneer published his first book in 1997, a textbook, “Dynamics of Complex Systems,” while continuing to lecture around the world and creating an internationally recognized body of work with some of the top minds in the field. He holds four patents for some of his groundbreaking research.\nAs chair of the International Conference on Complex Systems, Yaneer has led an annual gathering of complex science experts for more than two decades. Through his Institute, he has published hundreds of expert presentations and papers on evolution, emergence, complexity, self-organization, scaling, informatics, time series, emergence of mind and engineering of complex systems.\nYaneer also has served as a visiting scientist at MIT Media Lab and a Visiting Scholar at both the Department of Molecular and Cellular Biology at Harvard and the Federal Reserve Bank of Boston.\nYaneer explored some of the principles of Complex Systems in a 2002 paper titled, “Complexity Rising: From Human Beings to Civilization, a Complexity Profile.” “Since time immemorial, humans have complained that life is becoming more complex,” the paper began, “but it is only now that we have a hope to analyze formally and verify this lament.”\nThe paper, Yaneer said, was a journey into developing and understanding the so-called “complexity profile,” quantifying the “relationship between independence interdependence, and the scale of collective behavior.”\nIn 2004, Yaneer published his second book, “Making Things Work,” applied complex systems science to solving problems in healthcare, education, systems engineering, international development, and ethnic conflict.\nHis contributions to the field of applied complex science theory began to draw the attention of major institutions and world figures. Yaneer was invited to present his research and its applications to the World Bank Institute, the National Science Foundation and the United Nations among others.\nHis work has received grants from the Department of Defense, the National Institutes of Health, the Centers for Disease Control and Prevention and the World Bank. Yaneer has lectured widely on his research before numerous universities and foreign governments, as well as the World Health Organization, the U.S. Military Academy at West Point and the National Counterterrorism Center, among others.\nChampioning Data-Driven Decision Making\nAs the world grew increasingly more complex with the proliferation of global travel, Yaneer continued to apply his expertise to tackling global challenges like food insecurity, financial systems, terrorism and pandemics.\nNotably, in 2006, his work with a graduate student, Erik Rauch, analyzing how international human travel and livestock transport impacted pandemics broke new ground in pandemic modelling. Yaneer completed his student’s work after Rauch fell to his death in a hiking accident in California’s Sequoia National Park at the age of 31.\nWhen officials at the United Nations and in the U.S. government later approached Yaneer for help in 2008, he eagerly accepted the challenge to help policymakers address global crises through complex systems analysis. The chairman of the U.S. Senate Financial Services Committee, Rep. Barney Frank (D-Massachusetts) enlisted Yaneer’s aid in understanding and predicting the workings of world financial systems amid the economic crisis. Later, Yaneer advised the U.S. intelligence and defense communities on the root causes of global political instability.\nExpanding His Contribution\nYaneer did some of his most ground-breaking mathematical modeling and predicted that a growing food crisis would politically destabilize the Middle East.\nIn 2010, he went so far as to predict what came to be known as the Arab Spring long before unrest roiled the region toppling governments and sparking political violence. He earned international recognition for his predictive work, which was cited in publications around the world. Yaneer later advised President Obama’s chief defense policy advisor, Air Force Colonel Troy Thomas, on Egypt’s continuing political revolution and the rise of Mohammed Morsi and the Muslim Brotherhood. Subsequently publishes his analysis to wide acclaim with his findings cited by The New York Times, The Wall Street Journal, and BBC among many other outlets.\nYaneer’s research on the dynamics leading to world food insecurity also garnered interest from global leaders and international organizations. He was invited to speak before the World Economic Forum in Davos and United Nations World Food Program on his key findings that agricultural policies in the developed world, especially the United States, had drastically disrupted global food supply chains and led to the unrest. At a dinner at the World Food Program’s summit, he sat alongside Archbishop Desmond Tutu, investment mogul and philanthropist Warren Buffet and his wife, Naomi, a constant support in his life and work. It was at that dinner, as he emotionally recounted to the illustrious guests his unforgettable memories of the 1967 Nigerian famine that Yaneer came to fully understand just how deeply those latent memories had impacted his life\'s work.\nYaneer’s work on food insecurity has earned accolades and recognition around the world.\nIn 2011, Wired Magazine named Yaneer’s work among top scientific discoveries and listed his scientific visualizations “Best of” that year. Two years later, in 2013, Vice’s Motherboard also recognized Yaneer’s work as “Best of.”\nHis research findings have been cited by The New York Times, The Wall Street Journal, The Washington Post, The Guardian, The Sunday Times, Die Zeit, Le Monde, Time, The Atlantic Monthly, Scientific American, Wired, Fast Company, Forbes, Slate, Mother Jones, Scientific-American, ABC News, Canada’s CTV, RT, BBC Radio, NPR Radio and Vice among others.\nEmbarking on a New Mission\nWhen the Ebola crisis began unfolding in West Africa in 2014, Yaneer quickly tapped his earlier work on global travel and pandemics and recognized a fundamental problem: what seemed like a local outbreak could have global repercussions. He turned his focus to finding a solution.\nBuilding on his earlier work on pandemics, Yaneer applied his complex systems research to find a way to end the outbreak. Partnering with international organizations and frontline workers, he developed mathematical models for arresting and eradicating the disease in Sierra Leone through containment, early detection and direct interventions. When the outbreak emerged in nearby Liberia the next year, Yaneer helped successfully apply that knowledge to slow and end the outbreak. He continued to advise U.S and international leaders and organizations on the pandemic risk and measures to halt the virus.\nLaunching a Movement\nMore recently, Yaneer has turned his attention to the COVID-19 pandemic. It’s a deeply personal crusade, having lost his mother to the deadly virus early in the pandemic.\nIn January 2020, building on his decades of expertise on pandemics, Yaneer co-authored a paper with “Black Swan” author Nassim Taleb, becoming one of the first pandemic experts to advise policymakers to limit international and domestic travel and take precautionary measures to slow the spread of the virus.\nSimultaneously, Yaneer also launched EndCoronavirus.org, a collaboration of more than 4,000 scientists, students, community leaders and other volunteers dedicated to eradicating COVID-19 through applied systems research.\nSince the COVID-19 outbreak, he has worked around the clock and across time zones to educate policy makers to help save lives, published op-eds with CNN and USA Today, advised leaders throughout the globe from Arizona to Israel and Ireland, recommending short but strict five-week phased lockdowns as a measured, localized, community based intervention to ending the pandemic.\nThrough his decades of rigorous research into complex systems such as wars, famines, pestilence, food scarcity, and pandemics, if there’s one thing Yaneer has learned, it’s this:\nmiscalculating the root causes of problems invariably results in flawed, even tragic, outcomes.\n“Wrong assumptions lead to wrong solutions,” says Yaneer. “The consequences are budgets squandered, years wasted, lives lost. The benefits of correct assumptions leading to tailored solutions are priceless. And you can only find the correct assumptions by starting with the right questions.”\nIn April 2020, Yaneer co-authors the paper, “COVID-19: How to Win,"" with scientist Chen Shen expanding on his recommendations to fight the virus. Yaneer consults with European governments on effective coronavirus response.\nThe next month, CNN.com publishes Bar-Yam’s op-ed warning that the state lockdown measures are insufficient, correctly predicting a summer resurgence of the virus. Yaneer helps to draft a petition to Virginia Governor Ralph Northam on behalf of hundreds of scientists and state residents calling for more urgent action against COVID-19.\nOn May 7, 2020, the human toll of the pandemic became starkly personal. That day, he wrote, “I have lost my mother to COVID,” he wrote on Twitter. “May her light shine bright forever in the places that truly matter.”\nYaneer reflected on the legacy of his mother, who had shaped his moral conscience as a scientist. “She taught me the meaning of the space of possibilities,” he wrote. “Aspiration never for self but always for improving the world for everyone. She gave me my responsibility.”']"	['<urn:uuid:57ad171e-3ee8-400c-ba8d-2e1c9c9449cf>', '<urn:uuid:f16dc65f-342e-4f5a-84aa-4655058ec9aa>']	open-ended	direct	short-search-query	similar-to-document	comparison	expert	2025-05-13T00:20:10.754925	6	80	4569
57	When comparing the Fujitsu Connected Cow project with SCR Heatime HR-LD System, which technology provides more detailed monitoring of cows' health and behavior?	The SCR Heatime HR-LD System provides more comprehensive monitoring as it combines both activity monitoring and rumination measurement in a single tag, continuously tracking reproductive health, nutrition, and well-being status in real-time. In contrast, the Fujitsu Connected Cow project primarily focuses on tracking cow movement through pedometers to detect breeding readiness by measuring walking activity during oestrus periods.	"['The appliance of agricultural science\nWe’ll send you a myFT Daily Digest email rounding up the latest Food Prices news every morning.\nThe cows on the Kimotsuki Daichi farm in Kagoshima on the southern tip of Japan look unremarkable, save for their ankles. Attached to the slender limbs, just above the foot, are bright orange devices. As they wander around, these pedometers measure the number of steps taken and feed the rate of activity into a data system, which analyses it every hour.\nUnlike human Fitbits, the purpose of this device is not to monitor bovine endurance or agility — instead its aim is to let the farmer know, via mobile phone, when the cow is ready to breed.\nThe system, developed by Fujitsu, the Japanese IT equipment and services company, is known informally as the Connected Cow project.\nFujitsu says when a cow is on heat — or begins oestrus — it walks on average six times more than usual. The optimal period for insemination only lasts 12-18 hours every 21 days. But two-thirds of the time oestrus begins at night, when the farmer is asleep. Missing this window can result in a low pregnancy rate and another 21-day wait before the next cycle, adding to farmers’ costs.\nThe system is just one example of how the deployment of technology is boosting farmers’ output and profits.\nKarl Verhulst, head of internet-of-things solutions in Fujitsu’s digital technology services business, says: “This is quite groundbreaking because our algorithms are quite precise. It’s accurate and we are also able to influence the gender of the calf.”\nSo far, 5,000 cows are using the Fujitsu Connected Cow service in Japan, Turkey and Poland. The company has embarked on a “proof of concept” trial with the University of Reading to encourage its uptake in the UK.\nThe leap from millennia of dairy farmers observing their cows for signs of oestrus to using cloud computing and wearable technology is pronounced and relatively sudden.\nSuch advances in technology will be needed to drive efficiency and yields in order to meet the growing demand for food over the next decade.\nThe world’s population is projected to rise from 7.4bn currently to 8.1bn in 2025, according to the UN’s Food and Agriculture Organization. Agriculture already accounts for 40 per cent of the world’s total land area. The FAO says there is some scope to increase land for agriculture in parts of sub-Saharan Africa and Latin America. But it expects demand for food to be met overwhelmingly by productivity gains.\nThe UN food agency’s latest annual agricultural outlook, produced alongside the OECD, forecast that 80 per cent of the increase in crop output would come from yield improvements.\nSuch technological advances do not come cheap. But the agricultural sector is suffering from a shortfall in investment. The FAO estimates that $83bn of additional investment is needed to meet the goals for 2050. That is equivalent to an annual rise of 50 per cent from current levels. Farmers are already by far the biggest investors — their capital exceeds that from governments and domestic corporations by a ratio of more than three to one, says the FAO.\nBut to encourage commercial investment, reliable statistics are needed to evaluate, monitor and measure farming activities, believes Sara Menker, founder and chief executive of Gro Intelligence, an agricultural data company.\nMs Menker became interested in agriculture in her former job as a commodities trader at Morgan Stanley.\n“Agriculture is a very fragmented industry and information about it has been captured in a fragmented, disorganised way,” she says. “There’s a lot of inefficiencies in markets today that I think can be eliminated once people have a much better understanding of them.”\nShe quit managing her multibillion-dollar options trading portfolio to establish her venture, which aims to provide that information, with backing from some former colleagues at the bank.\nGro Intelligence has developed a subscription-service software called Clews, which acts as a type of search engine to provide data analytics across a broad range of agricultural data. These include environmental data based on satellite imagery, crop production, trade flows and demographics.\nThe target audience is not so much the farmer as potential investors, insurers and policymakers.\n“It is the ecosystem around the farmer that is not as well-informed as people think,” she says. “If you don’t get policymakers, investors, corporates, the non-profit sector, all to use the same classification system in a common language, you will not solve some of these fundamental problems around food security,” Ms Menker says.\nNew technologies and better analytic tools can help attract the investment needed to boost global agricultural supplies. Achieving behavioural change is another way of securing food security.\nReducing waste is one aspect of much-needed change, given that up to one-third of food either spoils or is thrown away, according to the FAO.\nAnother is to eat less meat. Looking to 2025, the FAO says this is when demand will be greatest for meat, fish and dairy products, which in turn will lead to additional demand for animal feed, including coarse grains and protein meals.\nProagrica, the farming informatics business of Relx group (the former Reed Elsevier), says that protein intake has risen by 43 per cent in the daily diet, from an average of 355 calories a day in 1965 to 507 in 2014.\nKeeping up that rate of growth will put enormous pressure on natural resources. A aside from the amount of feed and land use involved, livestock produces 14.5 per cent of all greenhouse gas emissions, according to the UN.\nThis has encouraged investment in plant-based alternatives as a way of reducing demand for animals as food.\nPerhaps the most eye-catching and radical approach is that taken by Soylent, the nutritional meal replacement drink that became a Silicon Valley hit after its 20-something inventor Rob Rhinehart wrote a blog entitled “How I Stopped Eating Food”.\nHowever, widespread embrace of functional food is some way off. More promising, for the time being, are meat alternatives that seek to emulate the look, texture, sizzle and taste of meat. They are aimed not so much at vegetarians, but at keeping carnivores satisfied enough to cut back on meat in their diet.\nSo far, high-profile names, including Bill Gates, the former Microsoft chief, and Sergey Brin, Google co-founder, have invested in the sector.\nBut more recently, Tyson Foods, one of the world’s largest meat processors, took a 5 per cent stake in Beyond Meat, a California-based meat substitute company, saying its investment “underscores the growing market for plant protein”.\n“Beef” burgers without cows would also be one way of resolving bovine breeding problems.\nGet alerts on Food Prices when a new story is published', 'click a winner to view more information about their innovation.\nThe Apollo MilkSystem is a revolutionary milking unit that automatically applies post dip to the teat through the liner, and then sanitizes the cluster before it is attached to another cow. In addition to providing a better and more consistent application of the teat dip, it also helps to improve milk quality while at the same time reducing labor and increasing parlor throughput - the factors most critical to today\'s commercial dairy producers.\nThe economical high performance AX 51 Aerotech fans with ""air-foil"" technology allow for customized ventilation based on the application and building requirements, which maximizes efficiency and performance with significant energy savings. The direct drive component eliminates costly maintenance items, with no bearings to grease, belts or pulleys to replace.\nWith new genetic technologies readily available to dairies and feedlots searching for uniform, high-quality feeder cattle, the timing seemed perfect for Genex Cooperative and Wulf Cattle to partner in Breeding to Feeding. This program creates quality, efficient feeder calves from dairy cows and assures dairies will receive a premium for these calves. Genex representatives work with dairies to provide long term genetic consulting and develop a customized breeding strategy, while Wulf Cattle provides the genetics from Limousin and Limousin-cross sires, and facilitates the contracting of these calves to Wulf Cattle approved calf ranches at a guaranteed premium purchase price. Dairies can now create more profitable future replacements from top-end females while adding feedlot value to calves from lower-end cows, in addition to addressing the top priorities of conception and calving ease.\nThe Dairy Care365 Training Series was developed by Merck Animal Health to train, equip and support dairy producers in their continuous efforts to provide the best care possible for their dairy animals. The ongoing series includes best-management practices for animal handling and movement, animal health diagnosis and treatment, and other topics related to animal care on dairy farms. The training courses are offered in English and Spanish and allow managers to identify specific areas for improvement and additional training needs, as well as identify opportunities to provide one-to-one mentoring if needed and offer positive reinforcement to the workers.\nThe farmBolus represents a technology previously only available to research institutes, which is now available to farmers and vets for use in diet suitability studies to understand the early stages of sub-acute ruminal acidosis. Each farmBolus will provide 5 months of continuous pH and temperature monitoring per animal and data is recorded and stored on the bolus until it is obtained & evaluated through the antenna & tablet system.\nThe SCR Heatime HR-LD System is the first solution that combines activity monitoring and rumination measurement from a single tag, in an integrated system with long-distance communication. This unique combination delivers unmatched accuracy, in both heat detection and cow health surveillance, as it continuously tracks and records, in real time, the reproductive, health, nutrition and well-being status of each and every collared cow. The Heatime HR-LD System can be set to send text and email alerts of each cow\'s health and heat status, at specific time intervals or as statuses change. This allows farmers to manage critical reproduction and health events from anywhere, at any time, via their mobile phone.\nThe IDEXX Milk Pregnancy Test is a new tool for accurately confirming pregnancy in dairy cows from 35 days postbreeding. Dairy producers now can get early, accurate identification of open cows by sending milk samples directly to their milk testing laboratory or by simply requesting the test for samples they already submit for milk-quality testing. The IDEXX Milk Pregnancy Test is a laboratory-based test, validated for use on bovine milk samples. With 99% sensitivity and 97% specificity, it provides accuracy similar to that of palpation and ultrasound.\nNeogen\'s new Igenity Dairy Heifer Program offers the dairy cattle industry an unprecedented level of information on the genetic potential of replacement dairy heifers. The new genetic profile uses hair follicles to produce DNA test results that provide information on a heifer’s potential in key traits. Those traits are performance and health traits such as net merit, pregnancy rate, productive life in months and somatic cell score; and yield traits, such as the animal’s milk in fat pounds, fat percentage, protein pounds, and protein percentage; and numerous calving and type traits. The test can be performed soon after a calf is born, and is another tool in the decision-making process for breeding and heifer selection.\nUsing a patented technology, Semex is now able to measure an animal\'s immune response and their ability to combat infection and disease. These animals are considered High Immune Responders, and Semex\'s HIR sires are known as Immunity Plus bulls. This sire line gives dairymen the opportunity to breed healthy cattle that are naturally disease resistant. By using Immunity Plus sires, dairymen can incrementally reduce the disease incidence in their herds with each generation, since immune response is highly heritable at 25%, making genetic gain much easier.\nThe M I one sets itself apart from other milking robots because it utilizes one robotic application arm to service multiple milking boxes ”“ providing the highest capacity for the investment cost. It means that dairy producers only need to invest in one vacuum pump, one computer system and one washing system with their robot. The MIone also features revolutionary cow milking technology. A 3D camera simultaneously ""sees"" the teat cup and teat, for precise attachment. Once the teat cups are attached, teat cleaning, pre-dipping, drying, a milk quality check, stimulation, and milk harvest are executed in sequence - within the teat cup ”“ without reattaching the unit.\nThe People\'s Choice winner is the IDEXX Milk Pregnancy Test. This inaugural award garnered well over 3,000 votes.\nWinners were announced Tuesday, Oct. 1 at the Dairy Herd Management reception at the World Dairy Expo in Madison, Wis. We are looking forward to seeing you again next year.']"	['<urn:uuid:923f0c65-9749-4669-839a-ae6c6bdd05b8>', '<urn:uuid:fcd90e2a-8318-4f5a-a168-bb58911c72f4>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-13T00:20:10.754925	23	58	2100
58	vessel temperature impact tea coffee production	In tea brewing, preserving heat through proper vessel temperature is crucial for high-quality results, requiring pre-heated cups and brewing vessels, while for coffee production, temperature changes significantly impact yields, with a 2°C increase potentially causing up to 34% productivity loss in some regions.	['Method 2: The ‘Filter tea’ method\nThis method is where things get interesting. We begin using separate brewing and drinking vessels, and separate the leaves from the liquor using a filter of some sort.\nWe now have options to experiment with brewing times and tea strength, have more variables to play with and can also get more steeps from the tea.\nThe benefit of multiple steeps (aside from the fact that you get more tea!) is that you can start to really appreciate the different layers and depths of your tea.\nThe first infusion is often very light and fragrant. The second and third steeps tend to be more full and obvious, and the subsequent steepings reveal the deeper, less obvious notes of the tea.\nSome teas give up their best flavour and aroma quickly, and it’s all over within 3-4 steeps, and some can seem like they go on forever, revealing layer after layer as you steep long in to the night.\nTea lingo: Teas that last longer are generally referred to as ‘patient’.\nThe road to mastery begins here\nThis method puts the tools in your hands to begin exploring the wonderful, mysterious world of high end tea for yourself.\nThis is the proving ground that raises you up from nervous novice to ‘gong fu tea’ pro.\nThe best part?\nIt’s really easy to do, you don’t need to purchase expensive equipment, and when you do it right the quality of tea you can brew is genuinely high quality.\nI use this method when I am working or moving around. For example: I will have a bowl on my desk to drink from, and use a glass flask to brew the tea easily, as I move around the office or home.\nAnother common use case is when travelling. Often, you don’t have space to bring your entire gongfu set up, so levelling down using simple ‘filter tea’ principles is ideal.\nWhy ‘Filter Tea’?\nI call this method ‘filter tea’, because that’s the extra factor we bring in to level us up from Method 1: Leaves in method.\nThe concept of ‘filter tea’ is very broad, and simply means that we will filter the tea between brewing and drinking.\nThe classic ‘English Teapot’ (often referred to as ‘Western brewing’) fits into this method, as does a glass teapot or flask with a metal filter built in, a french press (commonly used for coffee) or even a strainer ball inside your cup (although I don’t like this method as you are brewing with metal in your tea).\nIf you do use a french press, try to use a new one, not one that has been used for coffee. Coffee has a particularly lingering presence, and tea is particularly absorbent and easily affected by other aromas. You will lose the lighter, more subtle notes of Living Leaf Tea if you are brewing in a ‘coffee flavoured’ pot.\nDon’t brew in plastic or metal\nI prefer to work with glass or clean ceramic / porcelain for this method. Avoid using plastic or metal as it will influence the flavour and energy of the tea negatively.\nYou will notice that all the pictures above (which were found online) have some element of metal touching the tea while brewing, with the exception of the french press, in which the metal part can be raised above the water while the tea is brewing.\nFor this reason, I prefer to use a glass flask with a smaller filter, like this:\nThe metal filter on these flasks is raised above the water when brewing, and can be easily removed from the flask when adding hot water.\nThe double-walled glass is great for preserving heat and they are also very clean and easy to use.\nI use it almost every day, especially when travelling or working.\nThese flasks are my number one recommendation for brewing Filter Tea, and you can purchase one directly from us if you like:\nThe Steps: How to brew ‘Filter Tea’\nFor this method you will need:\n- Loose leaf tea (all Living Leaf Teas work well with this method)\n- Hot water\n- A vessel to brew the tea in (See above)\n- A filter (usually attached to the brewing vessel)\n- A bowl or cup(s) to drink the tea from\nImportant note: make sure the cup(s) you intend to drink the tea from are larger than the vessel you choose to brew the tea in!\nYou need to be able to completely empty the brewing vessel after each infusion. If you can not, the remaining water will continue to brew the tea, and it will become stewed.\nBrewing this way takes about as much time as it does to prepare a filter coffee (maybe slightly faster), but less messy and you get multiple brews!\nThe steps are as follows:\nStep 1: Add leaves to your brewing vessel\nAdd tea leaves to your brewing vessel (teapot, glass flask, french press etc).\nA rough guide is about the same amount as you would find in a tea bag, but you may want to add more depending on the size of your pot (if you would usually add two tea bags to a large teapot for example, use the equivalent amount of dry leaves).\nStep 2: Add hot water and brew\nPour hot water into the brewing vessel and allow the tea to ‘brew’.\nYou may wait anything from 5 to 60 seconds, depending on how much dry leaf you used, how strong you like your tea, how hot the water is etc. Start with 10-15 seconds and see how the tea comes out.\nThis is going to be the fun part that you experiment with over time.\n(Note: in the picture my metal filter is above the water while brewing. Brewing with metal inside the water tends to influence the flavour of the tea negatively.)\nStep 3: Pour (filter) the tea\nPour the tea through the filter into your drinking cup(s).\nStep 4: Enjoy your tea & brew again\nYour tea is ready as soon as it is cool enough to drink. You may enjoy smelling the fragrance of the tea in the cup at this stage, and you can also try smelling the wet leaves and steam coming from the brewing cup. An enjoyable fragrance is one hallmark of a high quality tea.\nThe wet leaves (pictured) can be brewed multiple times, depending on how much leaf you used and how long each of your brews are. At least 3-4 brews is a general guideline for this method.\nRefining your Filter Tea Game\nBONUS STEP 1: Wash and ‘wake up’ your tea before brewing\nIf you’re ready to start working on your tea game, you can add add an extra step between steps 1 and 2.\nAfter you have added your leaves, but before you brew the tea, you can add some hot water (less than the amount of a full brew) give the tea a quick ‘swirl’ and then discard the water (using the filter, so the tea leaves stay in the pot!).\nThis extra step has the effect of cleaning the leaves, ‘waking them up’ (getting them ready to be brewed) and releasing some of the fragrance for you to enjoy.\nI personally find it very ‘hard’ to brew without this step. Once you begin brewing with a wash pour, you will find it makes a nicer first infusion.\n(Note: You have just taken your first step on the path to brewing ‘Kung Fu tea’ like a pro!)\nBONUS STEP 2: Pre-heat your cup and brewing vessel\nPreserving the heat is one of the most important aspects of brewing high quality tea (as we will learn about in ‘Method 3: Gong Fu Tea’ next). So for super bonus points, pre-heat everything by pouring hot water through your vessels before adding the dry leaf.\nAdd the hot water to the brewing vessel (including pouring it through the filter), then pour it out into your cup. Now add the dry leaf to the empty, but warm, brewing vessel.\nWait just a few seconds… and now smell the dry, warm leaf in the brewing vessel. Doesn’t that smell wonderful?\nNow go ahead with your wash pour or first infusion.\nThis method is super simple, and my ‘most used’ method as I go about my day (ie. I am not sat down at my gongfu tea table). It is also very flexible. As long as the principles of ‘Brewing vessel -> Filter -> Drinking cup’ are held to, you can experiment freely with what you use for each of the vessels and filter.\nTea can be infused this way 2-3 times at a minimum, and up to 5-6 or more depending how long you are brewing for, how much leaf you used and the temperature of the water.', 'Just like it’s affecting other sectors of agriculture, climate change is starting to pose problems to coffee production. Colombia, the third-largest coffee producer in the world, is going through some serious trouble keeping up production, a new study showed.\nCoffee is one of the biggest crops in Colombia. Up to 550,000 families depend directly on coffee production for their livelihoods and many more depend on it indirectly. Due to changing climate conditions, there have been increasing concerns about the future quantity and quality of the coffee yield in Colombia in the decades to come. A new study shows that people are right to be concerned.\nCoffee is very sensitive to unpredictable weather. Studies estimated that a 2ºC increase in temperature would lead to losses in productivity of up to 34% for Mexico and 20% for Brazil, respectively. When it comes to Colombia, the findings forecasted a 4–24% increase in yields in the scenario where temperatures increase not more than 2 ◦C. However, this is highly dependent on the region.\nPrevious studies have also shown that the mountain region, where coffee is grown in Colombia, is warming by 0.3 degrees Celsius (0.5 degrees Fahrenheit) per decade. Plus the number of hours of sunlight also has declined by 19% since the middle of the last century due to increasing cloud cover – with not enough sun to sustain a high level of production in some areas.\n“The Andes Mountains cross Colombia from its southwest to the northeast corner. Colombian coffee is currently growing in areas with different altitude levels, and climate impacts will likely be very different for low altitude and high-altitude regions,” Sandy Dall’Erba, co-author of the new study and agricultural researcher, said in a statement.\nDall’Erba and a group of researchers looked at climate and coffee production for the entire territory of Colombia. They worked with a data set of 521 coffee-producing municipalities that continuously registered at least one hectare of Arabica coffee production from 2007 to 2013. This high level of detail allowed researchers to identify significant regional variations.\nWorking with climate models, the researchers anticipated weather conditions from 2042 to 2061 and future coffee production for each municipal area. They found that low-altitude municipalities will be negatively affected by climate change over the coming years – with thousands of families seeing their livelihood affected in consequence.\n“Colombia is not going to experience reduced productivity overall. But when we look into the impact across municipalities, we see many differences that get lost in the national average. That has important implications for coffee growers who live in one municipality versus another,” lead-author Federico Ceballos-Sierra said in a statement.\nAt the national level, productivity of coffee production will increase 7.6% by 2061, the study showed. But the forecasts change when zooming in specific regions. High-altitude regions in Colombia (1,500 meters or 5,000 feet above sea level) are expected to have a 16% increase in productivity, while low-altitude regions would have an 8.1% decrease.\nThis means that a higher temperature because of climate change will affect the current prime coffee growing locations, turning them into areas too hot and dry for production, while benefiting areas that now are considered marginal for coffee production. Smallholder coffee growers will have to find ways to adapt, using strategies such as more frequency irrigation. Although the study focused on Colombia, many other coffee-growing areas likely face similar pressures.\n“Our research presents what we anticipate will happen 20 to 40 years from now, given current conditions and practices. Future studies can look into different adaptation strategies and their costs, and evaluate which options are best. Beyond the 40-year horizon we focus on, the prospects might be grimmer without adaptation. Production cannot keep moving to higher levels,” Dall’Erba said in a statement.\nThe study was published in the journal Agricultural Systems.']	['<urn:uuid:7fdbe3be-8368-406e-acd4-fe2c30c43a30>', '<urn:uuid:f5c13e63-d792-42bd-b1a6-5ed212e84dbc>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T00:20:10.754925	6	43	2106
59	How do first trimester screenings work, and what mental health risks follow childbirth?	First trimester screening combines blood tests (done between 9-14 weeks) and ultrasound (11-14 weeks) to check for chromosomal abnormalities. The blood test measures two proteins, while the ultrasound examines neck fluid thickness in the fetus. Results are typically available within 1-2 weeks. Regarding mental health after childbirth, postpartum PTSD affects about 13.6% of mothers up to 6 months after delivery. This condition requires four categories of symptoms: re-experiencing trauma, avoiding trauma-related stimuli, negative mood changes, and increased arousal/reactivity. These symptoms must persist for over a month to qualify for diagnosis.	"[""Prenatal Test: First Trimester Screening\nWhat Is the First Trimester Screening?\nThe first trimester screening (or first trimester screen) typically includes a maternal blood screening test and an ultrasound exam.\nWhy Is the First Trimester Screen Done?\nIt's important to remember that this is a screening test, not a diagnostic test. If the test shows there might be a problem, another test must be done to confirm or rule out a diagnosis.\nShould I Have a First Trimester Screen?\nThe screen is considered safe, but it's your decision whether or not to have it. The benefit of screening in the first trimester is more time for parents to prepare for or address any health problems their baby may have.\nWhen getting any screening, remember that there's a chance of false-positive or false-negative results:\n- False-positive results are when women whose results show high risk for a chromosomal abnormality go on to have healthy babies.\n- False-negative results are when women whose results show low risk have a baby with a chromosomal abnormality.\nTalk to your doctor if you have questions about these screenings or your results.\nWhat Happens During a First Trimester Screen?\nThis screening test ideally is done in two parts — a blood sample and an ultrasound exam:\n- The blood sample is taken with a finger prick or a regular blood draw. It measures the levels of two proteins. Abnormal levels can mean that the baby has a higher-than-average risk of a chromosomal abnormality.\n- During the ultrasound, the woman lies on her back while an ultrasound probe glides across her lower belly. Sound waves bounce off the baby's bones and tissues to make an image of the fetus. This can show the thickness of the space in the back of the baby's neck. Extra fluid in the back of the neck may point to a higher risk of a chromosomal disorder.\nWomen whose results show a high risk for carrying a baby with a chromosomal abnormality are offered diagnostic testing, usually through chorionic villus sampling (CVS) in the first trimester or amniocentesis in the second trimester.\nThose who are not shown to be high risk in the first trimester are still offered a second trimester screening. That screening, which consists of more blood tests, checks for chromosomal abnormalities and neural tube defects, and helps to confirm the findings from the first trimester screen.\nWhen Should I Have a First Trimester Screening?\nThe blood screening is usually done between 9 and 14 weeks. Women who also get an ultrasound have one between 11 and 14 weeks.\nWhen Are the Results Available?\nBlood screening results usually are ready within a week or two. Ultrasound results can be immediate.\nWhen both the blood test and ultrasound are done, doctors usually calculate the results together. This is called a combined first trimester screening.\nNot all doctors calculate a woman's risk this way. Some wait until after a woman has had other screenings in the second trimester. This is called an integrated screening. Other practitioners don't do first trimester ultrasounds, instead determining a woman's risk using the results of her first trimester and second trimester blood screenings. This is called a serum integrated screening.\nHow your doctor calculates your results depends on your age, health risks, and the services available at the provider's office."", ""Postpartum PTSD: Beyond Postpartum Depression in Maternal Mental Health\nSelf-care for women, including appropriate sleep, nutrition, and exercise, can help reduce the symptoms of PTSD.\nIntroduction to Postpartum Post-traumatic Stress Disorder (PTSD)\nMost people consider maternal mental health to be synonymous with postpartum depression. However, there are many other similar disorders, collectively described as perinatal mood and anxiety disorders (PMADs). One such disorder is postpartum post-traumatic stress disorder. While PTSD as a whole used to be characterized as an anxiety disorder in the fourth edition of the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV), it is now in a new chapter in DSM-V titled “Trauma- and Stress-Related Disorders.”\nIn general, PTSD affects approximately 10% of women during their lifetimes.1 There is a significant increase in the prevalence of PTSD surrounding childbirth, specifically in the postpartum period. One study that analyzed 212 primiparous mothers found the prevalence of PTSD symptoms to be 12.7% immediately after childbirth and 13.6% up to 6 months later.2 As Ami Baxi, MD, director of inpatient psychiatric services at Lenox Hill Hospital in New York City, New York, observed, “these numbers are equivalent to the prevalence data for postpartum depression and anxiety.” Unfortunately, there is a scarcity of literature on postpartum PTSD, resulting in limited physician knowledge. Dr. Baxi notes that “postpartum PTSD is underdiagnosed and undertreated.”\nSymptoms of Postpartum PTSD\nPostpartum PTSD is not a specific diagnosis in the DSM-V; it is a subset of the diagnosis of PTSD itself. In a postpartum woman, the trauma in question could be a negative perception of the birthing process, but previous traumas, such as a history of sexual abuse, can also qualify for the diagnosis of postpartum PTSD. The diagnosis requires the presence of 4 categories of symptoms. The first is persistent re-experiencing of the trauma, which can play out as dreams, thoughts, or flashbacks. Second is avoidance of stimuli associated with the trauma. For example, a patient may avoid the hospital where she gave birth to avoid the feelings she had during the delivery.\nThird, the patient should display negative changes in mood and cognition, such as the inability to remember details of the event, a depressed state of mind, feelings of detachment from others, and exaggerated negative views of the world. Patients often present with depressed mood and, as Dr. Baxi explains, “they are often misdiagnosed to have postpartum depression.” However, the treatment is different and, thus, it is important to understand the nuances of the two diseases.\nThe final category is an increase in arousal and reactivity. Patients often describe sleep changes and an inability to concentrate. Some women present with irritability and self-destructive behavior. To fulfill the criteria for PTSD, a patient must have symptoms that are clinically distressing for more than one month.1\nThere are a number of risk factors for postpartum PTSD. Contrary to common belief, neither a history of PTSD nor a complicated delivery is necessary for this diagnosis. However, a personal history of antenatal depression or anxiety, as well as previous trauma, are possible risk factors. A study by Montmasson, et al showed that social isolation, a history of abortion, infertility, and obstetrical factors are additional risk factors. A woman's perspectives on how long her labor lasted and obstetrical complications are significant risk factors.2 Therefore, women can develop postpartum PTSD even if the birthing process appears to have gone well.\nEmily Cook, PhD, LCMFT, owner at Emily Cook Therapy, LLC in Bethesda, Maryland, describes a woman who “had a fast and early labor during which the father was unavailable to participate emotionally or supportively. The symptoms of postpartum PTSD are deeply distressing on their own, and yet they are often compounded by a sense of shame or blame that the mother has failed herself or the baby by her trauma response.” We know, however, that actual birth trauma increases the risk of postpartum PTSD 10-fold.2\nAnother study by Zaers and colleagues looked at postpartum women and the predisposing factors for postpartum PTSD. The study found that the strongest predictor of PTSD was anxiety occurring late in pregnancy.3 Depression was found to be significantly comorbid, with almost one-quarter of the women expressing symptoms of depression. Finally, having a baby in the neonatal intensive care unit (NICU) showed an increased rate of postpartum PTSD.3 Dr. Baxi explains, “these symptoms are not temporary; they can last for many months.” This is an important population that would benefit from focused screening measures for postpartum PTSD.4\nIt is important that providers play a preventative role for patients. Self-care for women, including appropriate sleep, nutrition, and exercise, can help reduce the symptoms of PTSD. Also, discussions regarding the realities of the birthing process and what to expect can set a woman up for a healthier perception of the birthing process, which could avoid the development of postpartum PTSD.5\nA word about early pregnancy loss\nEarly pregnancy loss (EPL) affects about a quarter of women. Farren, et al did a prospective study of the emotional symptoms experienced by women who experienced an EPL, and found that the rates increased from 28% to 38% over the first 3 months. These women are generally not seen in follow-up, but given this data, this is a vulnerable population that needs to be further assessed for emotional difficulties.\nBrief overview of treatment options\nPostpartum PTSD is treatable. Medication management and therapy can both be effective treatments. The first line of medications often tried in these patients is selective serotonin receptor inhibitors (SSRIs). The therapy for postpartum PTSD is different from that of postpartum depression. As in patients who have PTSD, re-processing and re-framing are essential to treatment, and it is important for the work to revolve around the trauma.5\nDr. Cook notes that “therapy definitely provides relief and healing. Involving the father in treatment through couples therapy is a powerful part of the work. We tell and retell the birth story, going back in memory by honoring the intensity of experience and reframing positive elements and going forward to the present by paying attention to all the ways the trauma is impacting their bodies, minds, emotions, expectations, and interactions.”\nPatients struggling with postpartum PTSD experience a significant burden. In the short term, mothers are less likely to seek out medical care for themselves or their babies.5 They are also less likely to breastfeed. Imagine a woman trying to bond with her child after a traumatic birth; there is much difficulty with both attachment and bonding. In the long-term, patients are less likely to have another child, or to try to control the process via an epidural or a C-section, even when not medically indicated. These patients also endure challenges in their relationships, both emotionally and physically.5\nA recent study showed a decline in psychomotor speed and attention, learning, and working memory and a significant overall cognitive decline in women with PTSD. The consequences were worsened with the presence of comorbid depression.7 Dr. Baxi emphasizes, “there needs to be further education about postpartum PTSD and screening needs to be in place. Mothers are suffering and they need to be heard and validated.”\n- Diagnostic and Statistical Manual of Mental Disorders: DSM-5. 5th ed. Arlington: American Psychiatric Association, 2013.\n- Montmasson H, Bertrand P, Perrotin F, El-Hage W. Predictors of postpartum post-traumatic stress disorder in primiparous mothers [published online May 21, 2012]. J Gynecol Obstet Biol Reprod. doi:10.1016/j.jgyn.2012.04.010.\n- Zaers S, Waschke M, Ehlert U. Depressive symptoms and symptoms of post-traumatic stress disorder in women after childbirth. J Psychosom Obstet Gynaecol. 2008;29:61-71. doi:10.1080/01674820701804324.\n- Kim WJ, Lee E, Kim KR, et al. Progress of PTSD symptoms following birth: a prospective study in mothers of high-risk infants [published online April 19, 2015]. J Perinatol. doi:10.1038/jp.2015.9.\n- Postpartum PTSD risk factors symptoms. Postpartum Progress. www.postpartumprogress.com/postpartum-ptsd-risk-factors-symptoms. Updated 2011. Accessed April 15, 2017.\n- Farren J, Jalmbrant M, Ameye L, et al. Post-traumatic stress, anxiety and depression following miscarriage or ectopic pregnancy: a prospective cohort study. BMJ Open. 2016;6(11):10.\n- PTSD in women linked to cognitive impairment. ADAA. www.medscape.com/viewarticle/878399?nli d=114049_4401&src=wnl_dne_170411_mscpedit&uac=38562DX&impID=1326457&faf=1#vp_1. Updated 2017. Accessed April 12, 2017.""]"	['<urn:uuid:5bd5af40-d620-48eb-ab4a-a0031e2a9d65>', '<urn:uuid:23b6285a-6a59-4fc0-9e13-914c065acc57>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T00:20:10.754925	13	90	1893
60	Can I schedule automated updates between connected spreadsheet files?	Yes, you can schedule automatic updates between Excel and Google Sheets using Sheetgo's automation feature. Updates can be scheduled to run on an hourly, daily, weekly, or monthly basis.	['Google Data Studio is a powerful visual reporting tool that allows you to transform a sea of raw data into engaging and interactive dashboards and charts. The user-friendly pre-built data connectors mean you don’t need any programming knowledge to pull data from multiple different sources. And combined with Google’s sharing functions, teams can easily combine, filter and present their data together in a really professional format, with great graphics.\nAs you’d expect from a piece of Google software, Google Data Studio is designed to integrate seamlessly with data sources such as Google Ads, Google Analytics, BigQuery. On top of that, there are over 150 third-party connectors to fetch data from sources like Facebook, Ebay, LinkedIn and Mailchimp. If you want to use spreadsheets as your data source, you need to use Google Sheets.\nGot source data in Excel? No worries. Read on to find out how to create an automated system to import your data seamlessly from Excel to Google Sheets to Google Data Studio.\nStep 1: Use Sheetgo to connect your files\nSheetgo is a no-code automation tool for spreadsheets and other office apps. When you create a Sheetgo connection, you watch your data move from one spreadsheet to another automatically. Here we’ll show you how to use Sheetgo to create a workflow, bringing your data from Excel files to Google Sheets and then on to Google Data Studio.\nOnce inside the web app, click on Connect to start creating the Excel to Google Sheets connection.\nStep 2: Select your source data\nThe first step of the connection is to select your Excel file as your data source. If you store your Excel files in an online storage platform, you can access them straight from the Sheetgo web app and connect them to Google Sheets.\nIf you do not store your Excel files online yet, it is necessary to transfer them to your Cloud storage.\nYou could do that individually, one file at a time, but there are a few things to consider.\nAn effective dashboard or accurate report needs to be fed recent, up-to-date (or live!) data. Manually saving Excel files to Google Sheets is not efficient or reliable. Your data will quickly become out-of-date, plus it’s incredibly time-consuming to have to keep repeating the process. If you want to use source data from Excel files to feed your Google Data Studio reports, the best solution is to set up an automated system.\nAs an example, we have listed the steps below to sync your offline Excel files to your Google Drive automatically (note that you can do this automatically for any cloud storage solution).\nHow to save Excel to Google Drive automatically?\n- Start your own automated data flow by installing Google’s Backup and Sync software. Backup and Sync is designed as a way to safely back up all your files, but you can use it as way to sync Excel with Google Sheets.\n- Select the Excel files you want to use as a your data source and save them in the same folder. Select this folder to be backed up to your Google Drive with Backup and Sync.\nStep 3: Send to Google Sheets\nIn the next step, you need to select your data destination, the Google Sheets file you want to export your Excel data to.\nLet Sheetgo create a new Google Sheets file automatically, or choose one that’s already in your Drive.\nClick on Finish and save to create the connection between your Excel and Google Sheets file.\nStep 4: Automate the workflow\nFinally, at the workflow level, click on Automate to schedule the frequency with which you want the automatic updates between Excel and Google Sheets to run. This can happen on an hourly, daily, weekly, or monthly basis.\nStep 5: Connect your Excel files to Google Data Studio\nOk, so you’ve set up an automated system to pull data from your Excel file(s) to Google Sheets. Now you’re ready to connect to Google Data Studio.\n- Open Google Data Studio.\n- Click on Select Data Source, in the top right hand corner of the screen.\n- Select Google Sheets.\n- Select the spreadsheet, then the worksheet (tab).\n- Click on the blue Connect button in the top right-hand corner\n- You will now see an overview of all columns, fields and items. Click Add to report.\n- A popup appears, to verify that you have selected the correct data. Check it and click Add to report.\nYour connection is complete!\nYou have created a link from the Excel file on your hard drive to Google Data Studio, using Backup and Sync and Sheetgo.\nAny changes to the data in the original Excel file(s) on your computer, or the Google Sheets file in your Drive, will be reflected in your Google Data Studio reports.\nNeed help turning your Excel data into a dashboard?\nLooking for advice on how to convert all that data into an eye-catching and effective report or dashboard? Check out our post on how to create dashboards in Google Sheets and Google Data Studio.']	['<urn:uuid:1e98b35d-2838-4457-ad61-87eb0dafb707>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T00:20:10.754925	9	29	845
61	How has spectrum communication evolved from invention to current applications?	Spread spectrum was originally invented in 1941 by Hedy Lamarr and was first implemented during the Cuban Missile Crisis for naval applications. The technology has since evolved from purely military uses to diverse commercial applications including wireless LANs, bar code scanners, and microphones. In modern warfare, it has become crucial for electronic warfare systems, particularly in countering adaptive radar threats. Current systems like SEWIP use commercial off-the-shelf open-architecture approaches that allow rapid insertion of new capabilities, enabling quick upgrades even while systems are operational. The technology continues to advance with the integration of artificial intelligence and machine learning to handle increasingly complex electromagnetic environments.	['- What is FHSS and DSSS?\n- What is Spread Spectrum?\n- Does WIFI use spread spectrum?\n- Who invented spread spectrum?\n- What is spectrum in data communication?\n- What does S Fhss mean?\n- What is the demodulator used in Fhss?\n- What are the three main types of shift keying?\n- What are the advantages of spread spectrum?\n- Why CDMA is called spread spectrum?\n- What is PCIe spread spectrum?\n- What is processing gain in spread spectrum?\n- When was spread spectrum invented?\n- What are the two types of spread spectrum?\n- What are the applications of spread spectrum?\n- Should I disable Spread Spectrum?\n- What is DSSS modulation?\n- Which is the application of DSSS signals?\n- What is PN sequence?\n- What are spreading codes?\nWhat is FHSS and DSSS?\nTwo popular methods for using the spread spectrum are frequency-hopping spread spectrum (FHSS) and direct-sequence spread spectrum (DSSS).\nDSSS, in contrast, spreads the signal across a wider bandwidth than FHSS, creating a lower power density across the spectrum..\nWhat is Spread Spectrum?\nSpread Spectrum refers to a system originally developed for military applications, to provide secure communications by spreading the signal over a large frequency band. … These narrowband signals are easily jammed by any other signal in the same band.\nDoes WIFI use spread spectrum?\nBluetooth uses Frequency Hopping Spread Spectrum (FHSS) and is allowed to hop between 79 different 1 MHz-wide channels in this band. Wi-Fi uses Direct Sequence Spread Spectrum (DSSS) instead of FHSS. Its carrier does not hop or change frequency and remains centered on one channel that is 22 MHz-wide.\nWho invented spread spectrum?\nHedy LamarrHedy Lamarr (/ˈheɪdi/), born Hedwig Eva Maria Kiesler (November 9, 1914 – January 19, 2000), was an Austrian-American actress, inventor, and film producer. She was part of 30 films in an acting career spanning 28 years, and co-invented an early version of frequency-hopping spread spectrum.\nWhat is spectrum in data communication?\nSpectrum refers to the invisible radio frequencies that wireless signals travel over. … The portion used for wireless communication sits within that space and ranges from about 20 KHz to 300 GHz. Spectrum wavelengths are classified into different bands within the electromagnetic spectrum range.\nWhat does S Fhss mean?\nspectrum (FHSSFrequency-hopping spread spectrum (FHSS) is a method of transmitting radio signals by rapidly changing the carrier frequency among many distinct frequencies occupying a large spectral band. The changes are controlled by a code known to both transmitter and receiver.\nWhat is the demodulator used in Fhss?\nThe conventional frequency hopping modem implements the first step by using analog mixers to down-convert the modulated FSK signal to the intermediate frequency or baseband, and then uses an analog or a digital FSK demodulator to extract the received information.\nWhat are the three main types of shift keying?\nAs with analog modulation, there are three parameters of the carrier wave to vary and therefore three basic types of shift keying:Amplitude Shift Keying (ASK)Frequency Shift Keying (FSK), and.Phase Shift Keying (PSK).\nWhat are the advantages of spread spectrum?\nAdvantages of Spread SpectrumCross-talk elimination.Better output with data integrity.Reduced effect of multipath fading.Better security.Reduction in noise.Co-existence with other systems.Longer operative distances.Hard to detect.More items…\nWhy CDMA is called spread spectrum?\nCDMA is a form of a Direct Sequence Spread Spectrum (where the transmitted data is coded at a very high frequency) communication. In spread spectrum, to send data, the signal bandwidth is much larger than necessary to support multi-user access.\nWhat is PCIe spread spectrum?\nThe PCIE Spread Spectrum BIOS feature controls spread spectrum clocking of the PCI Express interconnect. When set to Down Spread, the motherboard modulates the PCI Express interconnect’s clock signal downwards by a small amount.\nWhat is processing gain in spread spectrum?\nAbstract: The processing gain of a spread-spectrum (SS) system is generally defined as the ratio between the transmission bandwidth and data bandwidth, and is a measure of its interference rejection capability.\nWhen was spread spectrum invented?\n1941Lamarr and Anthiel received a patent in 1941, but the enormous significance of their invention was not realized until decades later. It was first implemented on naval ships during the Cuban Missile Crisis and subsequently emerged in numerous military applications.\nWhat are the two types of spread spectrum?\nOut of many Spread Spectrums, this report analyses the two main spread spectrum systems which are the ‘Frequency Hopping and Direct Sequence’ and in addition an overview of the ‘Chirp Spread’ Spectrum.\nWhat are the applications of spread spectrum?\nCurrent applications of spread spectrum technology include wireless LANs (local area networks), bar code scanners, and microphones. This technology improves the efficiency and effectiveness of business processes, many of which are finding that wireless communications are requisite for success.\nShould I disable Spread Spectrum?\nGenerally, frequency modulation through spread spectrum clocking should not cause any problems. … Therefore, it is recommended that you disable the MCLK Spread Spectrum feature if you are overclocking the memory bus. You will be able to achieve better overclockability, at the expense of higher EMI.\nWhat is DSSS modulation?\nIn telecommunications, direct-sequence spread spectrum (DSSS) is a spread-spectrum modulation technique primarily used to reduce overall signal interference. … Each spreading-sequence bit, which is known as a chip, has a much shorter duration (larger bandwidth) than the original message bits.\nWhich is the application of DSSS signals?\nDirect sequence spread spectrum applications CDMA cellphone technology: The DSSS technique was used to provide a multiple access scheme that was used for 3G cellophane technology. Each mobile used a different access code or spreading code and this enabled multiple users to access the base station on the same frequency.\nWhat is PN sequence?\nTheory: Pseudo-Noise (PN) sequences are commonly used to generate noise that is approximately “white”. It has applications in scrambling, cryptography, and spread-spectrum communications. It is also commonly. referred to as the Pseudo-Random Binary Sequence (PRBS). These are very widely used in communication.\nWhat are spreading codes?\nA spreading code is a pseudo-random code that has a narrow ambiguity function, unlike other narrow pulse codes. In CDMA a locally generated code runs at a much higher rate than the data to be transmitted.', 'Cognitive electronic warfare: Countering threats posed by adaptive radars\nThreats posed by adaptive radars to electronic warfare systems are a colossal challenge for the U.S. Navy, but a combo of advanced signal processing, intelligent algorithms, and machine learning techniques are being developed to help warfighters detect and counter them.\nElectronic warfare (EW) systems – whether on land or aboard U.S. military ships and aircraft – tap the electromagnetic spectrum to sense, protect, and communicate. But, when necessary, these same systems can be turned against adversaries to deny their ability to disrupt or use radio, infrared, or radar signals.\nToday’s EW systems tend to rely on databases of known threats with predefined countermeasures, which can limit their ability to quickly adapt and respond to new advanced threats. Soon, these systems may increasingly be tasked with isolating unknown hostile radar signals within dense electromagnetic environments and responding quickly with effective electronic countermeasures.\nThreats from adaptive radars aren’t necessarily new, according to Joe Ottaviano, director, Electronic Warfare, for Lockheed Martin (www.lockheedmartin.com), but are challenging for EW systems to detect.\nYears ago, back when the U.S. Navy’s Surface Electronic Warfare Improvement Program (SEWIP) Block 1, 2, and 3 were formed, “it was a key requirement to be able to detect and counter adaptive radars to provide warfighters with the best information about the radar and its mission,” Ottaviano explains. “We need to do this while performing our mission – before the enemy knows we’re there. Block 2 is designed to deal with adaptive radars.”\nBAE Systems (www.baesystems.com), another defense contractor working within the realm of adaptive radars, was recently awarded a $13.3 million contract by the U.S. Defense Advanced Research Projects Agency (DARPA) to extend its work on the Adaptive Radar Countermeasures (ARC) project, with the goal of enabling airborne EW systems to counter new, unknown, and adaptive radars in real time.\nCognitive EW technologies developed by BAE Systems for the ARC project rely on advanced signal processing, intelligent algorithms, and machine-learning techniques.\nAdaptive radar vs. “unknown” radar\nAdaptive radars, for starters, shouldn’t be confused with “unknown radars,” both of which cognitive EW addresses, as John Tranquilli, technical director for signals and communications processing at BAE Systems, points out.\nBoth radar types do pose challenges for EW systems and warfighters, albeit with differences. “For the class of unknown radars, current systems rely on a database of threat knowledge to compare observed characteristics to a list of known radars,” Tranquilli explains. “When a radar comes up as unknown or is misclassified as some other radar – because it’s either a really new radar or an existing one that’s behaving outside prior known bounds – today’s systems have very limited ability to characterize the threat or turn around an appropriate response.”\nRadars are currently evolving away from fixed analog systems – which aren’t easy to add new capabilities to – toward digitally programmable variants that can easily add or change to unknown behaviors with agile waveform characteristics. “This challenge will increase the prevalence of ‘unknown radars’ and lead to truly adaptive radars,” Tranquilli continues.\nAdaptive radars, which pose an even greater challenge, can sense their environment and design transmission characteristics on the fly to maximize radar performance. “This allows a radar to have new waveforms – or new pulse processing – for every transmission, whether it’s improving target resolution or mitigating interference effects,” Tranquilli says.\nThe objective of cognitive EW “is to enable EW systems to autonomously characterize these threats, automatically generate effective countermeasures against them, and monitor the effectiveness of our countermeasures so we can adapt our techniques if they’re either ineffective or if an adaptive radar attempts to adapt around our responses,” he adds.\nRecognizing and countering adaptive radars\nHow complex is recognizing and countering hostile adaptive radars? It falls under the category of “very complex.”\nFor SEWIP, a significant amount of software was developed as part of Block 2 to specifically address these challenges. “While development is a challenge, testing is becoming even more of a challenge,” Ottaviano notes. “Defining what’s effective and how effective the detection and response are requires a tremendous effort. We’re working with our Navy partners to continue to improve how we define success within a highly adaptive environment … and right now it’s our biggest challenge.”\nFrom BAE Systems’ perspective, “adaptive radars challenge our ability to isolate the pulses from threatening radars from other hostile, friendly, and neutral signals, our ability to deduce the threat posed by the radars, and our ability to select and configure an appropriate response to achieve our desired effect,” Tranquilli says. “A host of adaptive and machine-learning algorithms are necessary because these challenges are too complex to design a simple series of rules around.”\nRole of machine learning and artificial intelligence\nWhat role are machine learning and artificial intelligence (AI) playing in adaptive radars? There’s already a “fair amount of intelligence built into the new operational systems such as SEWIP,” Ottaviano says.\nIs there a role for a fully autonomous system? “It depends on the application,” Ottaviano notes. “I can easily see small packages being fully autonomous within a five-year window. For systems that deal with weapons, you can view it as another tool within the toolbox to allow warfighters to own the electromagnetic battlespace.”\nFor its part, AI can be “a loaded term,” Tranquilli cautions. “Typically, it refers to systems that attempt to have algorithms replace the functions a human would perform in an operation,” he adds.\nBut Tranquilli thinks there’s likely “a place for AI in the near future of cognitive EW, because as threat environments become more contested and complex, it can help reduce the mental load on pilots and planners.” This is “autonomy,” a field in which BAE Systems is an active player.\nMachine-learning techniques, on the other hand, “involve data-driven processing that allows a system to not be beholden to a static set of features, databases, or rules,” Tranquilli points out. “I can’t speak to the level of adaptation and autonomy allowed by today’s systems, but as threat systems continue to advance they’ll adapt more quickly and change their behavior faster … so algorithms and decision engines that can outpace our adversaries are key to future success. This is the central tenant of Bob Work’s Third Offset Strategy – pursuing next-gen technologies and concepts – and is funding advances within the cognitive EW arena.”\nFuture cognitive EW\nInterestingly, in terms of the hardware involved, Lockheed Martin is finding that a commercial off-the-shelf (COTS) open-architecture approach allows a rapid insertion of new capability to continually improve the ability and performance of the SEWIP Block 2 system.\n“These systems can and have been upgraded in an immediate fashion while underway to continue to provide the warfighter with new tools,” Ottaviano says. “We’ve seen a resurgence of this capability now beyond the surface EW community … even moving into decoy and off-board platforms. This provides a quick time to solution because the approaches we’re taking aren’t hardware specific and don’t require long development cycles.”\nTranquilli sees a place for advanced hardware in the future of cognitive EW “because systems will need faster response times, more prevalent RF and spatial coverage, and broader sets of available responses,” he explains. “The great thing about cognitive EW is that it can be a scalable capability so portions of the technology can be implemented into systems that exist today, while advances in the hardware, firmware, and software resources will unlock the ability to get more power out of the technology.”\nLockheed Martin is also currently working on a new U.S. Navy development contract to provide MH-60 helicopters with enhanced EW surveillance and countermeasure capabilities against anti-ship missile threats. (Figure 1.)\nAs part of it, they’ve developed a system – known as the Advanced Off-Board Electronic Warfare Active Mission Payload (AOEW AMP AN/ALQ-248) – that is essentially a self-contained EW pod hosted by an MH-60R or MH-60S to provide the Navy with advanced anti-ship missile detection and response capabilities. It’s designed to work independently or with the ship’s onboard electronic surveillance sensor, SEWIP Block 2 AN/SLQ-32(V)6, to detect incoming missiles and evaluate where they’re going. AOEW then uses RF countermeasures to deter any incoming missiles.\n“Our system will help create a coordinated attack against these threats to keep our warfighters safe by controlling the electromagnetic spectrum and disrupting adversaries,” Ottaviano says.']	['<urn:uuid:923d9da3-c243-4ca1-ba1d-2b88ef27461b>', '<urn:uuid:345175d3-8e63-4c8c-ba23-6dff23f5adbf>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T00:20:10.754925	10	104	2415
62	How does promoting health and wellbeing in the workplace benefit both the company and its employees?	A workplace that promotes health and wellbeing can lead to multiple benefits: it increases employee job satisfaction and engagement, improves productivity, reduces sick leave, and increases employee retention.	"[""Healthy Communities - Healthy Workplaces\nA workplace that promotes the health and wellbeing of employees can increase employee job satisfaction and engagement. It has also been shown to improve productivity, reduce sick leave and increase employee retention.\nBarwon Health Support\nBarwon Health is the lead local agency in the Barwon region of the Cancer Council Workplace Achievement Program, which was initiated across the state in 2012.\nDo you want tailored support to develop your health and wellbeing program?\nBarwon Health can provide workplaces with one to one support to help you with:\n- Writing a health and wellbeing policy\n- Setting up a wellbeing committee or champions\n- Creating a health and wellbeing action plan\n- Setting up program evaluation\n- Providing opportunities to network and learn from other organisations\n- Connecting you with health and wellbeing experts\nTo get started contact the Workplace Health team to find out more.\nWorkplaces and the Achievement Program\nBarwon Health supports local workplaces in the Barwon region to implement the Healthy Workplaces Achievement Program. The Achievement Program is a state wide initiative supported by the Victorian Government and delivered by Cancer Council Victoria. There is no cost to register your workplace.\nThe Workplace Achievement Program gives you practical steps and easy-to-use tools and templates to promote and improve health and wellbeing in your workplace. It's more than just a program - it provides you with a framework for continuous improvement; you can work at your own pace; and, it’s completely free.\nIf you're already doing great things to promote and encourage health and wellbeing in your workplace, the Achievement Program helps you build on this and get the best results for your business. Backed by evidence, it will also help you communicate your ideas to management and get your workplace leaders and colleagues on board with the initiatives you're proposing. You will also enjoy ongoing support from health professionals. Read more about the program here.\nWorkplace Health Priority Areas\nFor more information on each of the benchmark areas visit the Healthy Workplaces Achievement Program.\nRegister for the Healthy Workplace Achievement Program to:\n- Create a positive, healthy culture\n- Increase employee morale, improve productivity and reduce sick leave\n- Build on the health and wellbeing initiatives you already have in place\n- Be recognised by the Victorian Government and position yourself as an employer of choice.\nJoin over 100 workplaces and 30,000 employees in Geelong who have already made the healthy choice\nWorkplace Health in the Barwon region\nJoin our quarterly Workplace Achievement Program eNewsletter. We provide important events, tips and resources that support local workplaces to achieve better health and wellbeing outcomes for their employees.\nSubscribe to the Workplace Health eNewsletter here\nClick below for our previous editions:\nThe Workplace Health Network provides opportunities for those involved in health and wellbeing initiatives to share knowledge, collaborate and inform future workplace health events across the region. The network is always open to new members from Achievement Program registered workplaces.\nBarwon Health host a number of events around the Workplace Achievement Program throughout the year. These events are designed to support you in implementing the benchmarks of the achievement program in your workplace. They are also a great opportunity to network with other businesses in the Geelong region.\nLast Modified: Wednesday, 04 January 2023""]"	['<urn:uuid:49f785b2-6512-46e3-9cab-a0bf3c20ce53>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T00:20:10.754925	16	28	550
63	I'm particularly interested in the party's stance during WWI - could you explain how World War I impacted the Socialist Party of America's membership and leadership?	The party's opposition to World War I caused a sharp decline in membership. The party lost some of its most prominent members who had supported America's entry into World War I, including Walter Lippmann, John Spargo, George Phelps Stokes, and William English Walling. These members briefly formed an organization called the National Party. Additionally, the party's best-known leader, Eugene Victor Debs, made an anti-war speech in 1918 calling for draft resistance, for which he was arrested under the Sedition Act of 1918, convicted, and sentenced to ten years in prison. He was later pardoned by President Warren G. Harding in 1921.	"['- Socialist Party of America\nInfobox Historical American Political Party\nparty name= Socialist Party of America\nparty articletitle= Socialist Party of America\nactive= 1901ndash 1973\nideology= American Socialism,\npreceded by= Social Democratic Party\nSocialist Labor Party\nSocialist Party USA\nDemocratic Socialists of America\ncolors = RedThe Socialist Party of America (SPA) was a socialist\npolitical partyin the United States. It was formed in 1901 by a merger between the three-year-old Social Democratic Party and a wing of the older Socialist Labor Party. It flourished in numerous ethnic enclaves from 1904 through 1912, with Eugene V. Debsas its presidential candidate. It splintered over World War Iand Russia\'s 1917 October Revolutionand was a minor political movement after 1920, often nominating Norman Thomasfor president.\nFrom 1901 to the onset of\nWorld War I, the Socialist Party had numerous elected officials. There were two Socialist members of Congress, Meyer Londonof New York Cityand Victor Bergerof Milwaukee(a part of the sewer socialismmovement); over 70 mayors, and many state legislators and city councilors. Its voting strength was greatest among recent Jewish, Finnish and German immigrants, coal miners, and former Populist farmers in the Midwest. [Shannon (1951)]\nEarly political perspectives ranged from radical socialism to social democracy, with New York party leader\nMorris Hillquitand Congressman Berger on the more social democratic or right wing of the party and radical socialists and syndicalists, including members of the Industrial Workers of the World(IWW) and the party\'s frequent candidate, Eugene V. Debs, on the left wing of the party. As well there were agrarianutopian-leaning radicals, such as Julius Waylandof Kansas, who edited the party\'s leading national newspaper, "" Appeal To Reason"" along with trade unionists; Jewish, Finnish, and German immigrants; and intellectuals such as Walter Lippmannand the Black activist/intellectual Hubert Harrison.\nThe party had a hostile relationship with the\nAmerican Federation of Labor(AFL). The AFL leadership was strongly opposed to the SPA, but moderate Socialists like Berger and Hillquit urged cooperation with the AFL in hopes of eventually forming a broader Labor Party. Their leading ally in the AFL was Max Hayes, president of the International Typographical Union. These efforts were bitterly spurned, however, by the majority of the Socialist Party, who held to either the IWWview or the Wayland view.\nThe party\'s opposition to\nWorld War Icaused a sharp decline in membership. An increase in the membership of its language federations from areas involved in the Bolshevik Revolution proved illusory, since these members were soon lost to the Communist Labor Party. The party also lost some of its most prominent members, who had been in favor of America\'s entry into World War I, including Walter Lippmann, John Spargo, George Phelps Stokes, and William English Walling. They briefly formed an outfit called the National Party, in an unrealised hope of merging with the remnants of Theodore Roosevelt\'s Progressive Party and the Prohibition Party.\nIn June 1918 the Party\'s best-known leader,\nEugene Victor Debsmade an anti-war speech [ Eugene V. Debs, ""The Canton, Ohio Speech, Anti-War Speech"", delivered June 16, 1918, first published 1918 in ""The Call"", [http://marxists.org/archive/debs/works/1918/canton.htm online at Marxists.org] , accessed 11 August 2006.] calling for draft resistance; he was arrested under the Sedition Act of 1918, convicted and sentenced to serve ten years in prison. He was pardoned by President Warren G. Hardingin 1921.\nExpulsion of Bolshevists\nIn January 1919\nVladimir Lenininvited the communistwing of the Socialist Party to join in the founding of the Communist Third International, the Comintern.\nThe Bolshevists held a conference in June 1919 to plan to regain control of the party by bringing delegations from the sections of the party that had been expelled to demand that they be seated. However, the language federations, eventually joined by\nCharles Ruthenbergand Louis Fraina, broke away from that effort and formed their own party, the Communist Party of America, at a separate convention in Chicago on September 2 1919.\nMeanwhile plans led by John Reed and\nBenjamin Gitlowto crash the Socialist Party convention went ahead. Tipped off, the incumbents called the police, who obligingly expelled the Bolshevists from the hall. The remaining Bolshevist delegates walked out and, meeting with the expelled delegates, formed the Communist Labor Partyon September 1, 1919. The Communist Labor Party merged with the Communist Party of America in 1921 to form the predecessor of the Communist Party USA.\nExpulsion of Socialists from the New York Assembly\nIn 1920, the\nNew York State Assemblyexpelled five Socialist members on the grounds that being a member of the Socialist Party constituted as disloyalty. These members included Louis Waldman, Samuel Orr, Charles Solomon, August Claessens and Sam Dewitt. This case was brought before the Supreme Court, and the members were permitted back into the Assembly.\nFrom 1904 to 1912, the Socialist Party ran\nEugene Debsfor President at each election. The best showing ever for a Socialist ticket was in 1912, when Debs gained 901,551 total votes, or 6% of the popular vote. In 1920 Debs ran again, this time from prison, and received 913,693 votes, 3.4% of the total.\nThe Socialist Party did not run a presidential candidate in 1924, but supported Senator\nRobert M. La Follette, Sr.and his ad-hoc Progressive Party. LaFollette\'s party disbanded after his death in 1925.\nIn 1928, the Socialist Party returned as an independent electoral entity under the leadership of\nNorman Thomas, a Protestant minister in New York City. Thomas repeatedly ran as the party\'s presidential candidate through 1948.\nA turn to the left\nThe party experienced a major growth spurt during the\nGreat Depression, primarily among youth. These youth leaders, however, were quickly won over to the proposition of reconciliation and reunification with the Communist Party, in keeping with new Popular Front policy of the Comintern. Leaders of the United Front faction included Reinhold Niebuhr, Andrew Biemiller, Daniel Hoan, and Gus Tyler. Most of these figures went on to become the founders of Americans for Democratic Action(ADA), a key Cold War liberal organization.\nThe ""militants"", as they were called, were triumphant at the Socialist Party\'s national convention in Detroit in June 1934, which precipitated the exodus of the opposing ""old guard""—led by\nLouis Waldmanand David Dubinsky—which favored the formation of a national Farmer-Labor Partythat would have been likely led by Huey Long. Fact|date=February 2007 After this fell through, in 1936 the old guard leaders formed the Social Democratic Federation and reluctantly endorsed Franklin Roosevelt.\nBy this time, however, the militants as well were on the Roosevelt bandwagon, in keeping with the dictates of the\nPopular Front. The party was then buttressed by the mass entry of the American followers of Leon Trotskyfrom the U.S Workers Party in keeping with the so-called French Turn, by which Trotskyists recruited to their revolutionaryperspectives. The revolutionary perspectives of the Trotskyists caused enough havoc, however, that they were expelled by 1938. The Socialist Party\'s youth group, the Young People\'s Socialist League, left with the Trotskyists.\nBy 1940, only a small committed core remained in the party which opposed\nFranklin D. Roosevelt\'s New Deal. In 1940 Norman Thomas was the only presidential candidate opposed to a pro-Soviet foreign policy. This also led Thomas to serve as an active spokesman for the isolationist America First Committeeduring 1941.\nThomas led his last presidential campaign in 1948, after which he became a critical supporter of the postwar liberal consensus. The party retained some pockets of local success, in cities such as\nMilwaukee, Bridgeport, Connecticut, and Reading, Pennsylvania. In New York City, they often ran their own candidates on the Liberal Party line. In 1956, the party reconciled and reunified with the Social Democratic Federation.\nIn 1958 the party admitted to its ranks the members of the Independent Socialist League led by\nMax Shachtman. Shachtman\'s young followers were able to bring new vigor into the party and helped propel it to play an active role in the civil rights movement as well as the early events of the New Left. Shachtman, however, successfully blocked merger of the party with the Jewish Labor Bundon account of that organization\'s historical anti-Zionism. [cite web\nauthorlink=Socialist Party of Rhode Island\ntitle= A Short History of the Socialist Party USA\nBy the late 1960s the most powerful figures in the Socialist Party of America were Max Shachtman and\nMichael Harrington, who agreed upon a parallel strategy of maintaining the Socialist Party as an independent third party that fielded its own candidates, and acting as a pressure group within the Democratic Party. The party itself had become divided into three caucuses. One was the Debs Caucus led by David McReynolds, which wanted to pursue the traditional position of the Socialist Party as an independent political party and held the most strongly ""leftist"" position within the group. Another was the ""centrist"" Coalition Caucus led by Michael Harrington, which also had a leftist orientation, but wanted to work within the Democratic Party to pull it to the left. Finally, the ""rightist"" Unity Caucus led by Max Shachtman were strong supporters of the Lyndon Johnson/ ""Scoop"" Jacksonwing of the Democratic Party that supported hawkish anti-Communismabroad and civil rightsand the Great Societyprogram domestically.Socialist Party of Rhode Island. (2000). [http://sp-usa.org/spri/sp_usa_history.htm A Short History of the Socialist Party USA] (web page). Accessed: June 13, 2006.] Drucker, Peter. (1994). Max Shachtman and His Left: A Socialist\'s Odyssey Through the ""American Century"". Atlantic Highlands, NJ: Humanities Press. 346 p. ISBN 0-391-03815-X]\nThis split was reflected in party members opinions about the\nVietnam Warand the New Left– Shachtman and his followers increasingly supported the war and greatly distrusted the New Left, Harrington was strongly opposed to the war, but was nevertheless suspicious of the New Left, while the Debs Caucus opposed the war and embraced the New Left. Conversely, of all the three groups, the Shachtmanitesmaintained the strongest tendency to Marxist orthodoxy (or their version of it) and democratic centralism, while the other two caucuses were more eclectic in their approach to socialism. This division manifest most strongly during the 1968 Democratic Convention, in which members of the Debs Caucus were among the protesters outside of the convention, while members of the Coalition and Unity Caucuses were among the convention delegates.\nBy 1972, the party was even more deeply divided, with the party newspaper, ""\nNew America"", running opposing articles on practically every issue. During the 1972 presidential election, each caucus supported a different candidate; the Debs Caucus supported the independent candidacy of Benjamin Spock, the Coalition Caucus supporting the liberal Democratic nominee George McGovern, and the Unity Caucus supporting the Democratic primary run of Henry M. ""Scoop"" Jackson, then declaring their neutrality between McGovern and Richard Nixonwhen Jackson failed to win the nomination.\nThe Debs Caucus finally broke with the party in 1972 to form the Union for Democratic Socialism. (David McReynolds had left the party in 1970, but rejoined the breakaway group.) The UDS became the\nSocialist Party USAin 1973 when all other factions had abandoned the name ""Socialist Party"". The Socialist Party USA developed into a small third party in U.S. politics, which now has about 1,000 members in good standing and regularly runs candidates for public office, though often these are more educational campaigns than they are serious attempts to win. [ [http://sp-usa.org/ncminutes/1006nc.html Minutes] of October 2006 Socialist Party National Committee meeting.]\nMichael Harrington and the Coalition Caucus left the party soon after. They became the\nDemocratic Socialist Organizing Committee(later the Democratic Socialists of America), which worked within the Democratic Party but in support of its left wing.\nThis left Shachtman and the Unity Caucus in unopposed control of the party (though Shachtman himself died very soon after). In 1973, this group renamed it the\nSocial Democrats USA. It evolved into more of a think tank than a political organization, with many of its members later holding important governmental offices in both Democratic and Republican administrations.\nEugene V. Debsand Job Harriman(87,945 votes, 0.6%)\n* 1904ndash Eugene V. Debs and\nBen Hanford(402,810 votes, 3.0%)\n* 1908ndash Eugene V. Debs and Ben Hanford (420,793 votes, 3.8%)\n* 1912ndash Eugene V. Debs and\nEmil Seidel(901,551 votes, 6.0%)\nAllan L. Bensonand George Kirkpatrick(590,524 votes, 3.2%)\n* 1920ndash Eugene V. Debs and\nSeymour Stedman(913,693 votes, 3.4%)\nNorman Thomasand James H. Maurer(267,478 votes, 0.7%)\n* 1932ndash Norman Thomas and James H. Maurer (884,885 votes, 2.2%)\n* 1936ndash Norman Thomas and\nGeorge A. Nelson(187,910 votes, 0.4%)\n* 1940ndash Norman Thomas and\nMaynard C. Krueger(116,599 votes, 0.2%)\n* 1944ndash Norman Thomas and\nDarlington Hoopes(79,017 votes, 0.2%)\n* 1948ndash Norman Thomas and\nTucker P. Smith(139,569 votes, 0.3%)\n* 1952ndash Darlington Hoopes and\nSamuel H. Friedman(20,065 votes, <0.1%)\n* 1956ndash Darlington Hoopes and Samuel H. Friedman (2,044 votes, <0.1%)\nVictor L. Berger\nAllan L. Benson\nElla Reeve Bloor*\nJames P. Cannon*\nEugene V. Debs\nElizabeth Gurley Flynn*\nWilliam Z. Foster*\nMaynard C. Krueger\nTheresa S. Malkiel\nMary E. Marcy\nKate Richards O\'Hare\nMary White Ovington\nA. Philip Randolph\nRose Pastor Stokes*\nFrank P. Zeidler†\n:(*) Left with founding of the\nCommunist Party USA:(†) Went on to join the Socialist Party USA\n* Bell Daniel. ""Marxian Socialism in the United States.""\nPrinceton University Press, 1967.\n* Harrington, Michael. ""Socialism"" 1970.\n* Robert Hyfler; ""Prophets of the Left: American Socialist Thought in the Twentieth Century""\nGreenwood Press. 1984.\n* Ira Kipnis; ""The American Socialist Movement, 1897-1912""\nColumbia University Press, 1952. Reprinted Haymarket Books. 2004.\n* Laslett John M., and Lipset, Seymour Martin, eds. ""Failure of a Dream? Essays in the History of American Socialism."" 1974.\n* Lipset, Seymour Martin and Gary Marks, ""It Didn’t Happen Here: Why Socialism Failed in the United States?"" New York: Norton, 2000.\n* H. Wayne Morgan; ""Eugene v. Debs: Socialist for President"". Greenwood Press, 1973\n* Miller, Sally M. ""Victor Berger and the Promise of Constructive Socialism, 1910-1920"". Greenwood, 1973.\n* Quint, Howard. ""The Forging of American Socialism."" 1953.\n* Nick Salvatore. ""Eugene V. Debs: Citizen and Socialist"" (2007), the standard scholarly biography\n* David A. Shannon. ""The Socialist Party Before the First World War: An Analysis"" ""\nThe Mississippi Valley Historical Review,"" Vol. 38, No. 2. (Sep., 1951), pp. 279-288. [http://links.jstor.org/sici?sici=0161-391X%28195109%2938%3A2%3C279%3ATSPBTF%3E2.0.CO%3B2-B in JSTOR]\n* Shannon, David A. ""The Socialist Party of America."" 1967.\n* Swanberg W. A. ""Norman Thomas: The Last Idealist."" 1976\n* Weinstein James. ""The Decline of Socialism in America: 1912-1925."" 1969.\n* [http://www.burnedbookspublishing.com George Ross Kirkpatrick\'s WAR - WHAT FOR? (1916)] attacking World War I.\n* [http://www.democracynow.org/article.pl?sid=04/06/21/1410236&mode=thread&tid=25 ""The Last Socialist Mayor""] . Frank Zeidler, Mayor of\nMilwaukee(1948-1960). Interviewer, Amy Goodman. Democracy Now!. Monday, June 21 2004. Retrieved May 12 2005.\n* [http://www.theorganizer.org/LP/USHistory/Bridgeport1.html ""Book Review: Bridgeport\'s Socialist New Deal, 1915-1936 by Cecelia Bucki, Urbana and Chicago: University of Illinois Press, 2001""] . Published by\nSocialist Organizer. Retrieved August 27, 2006.\n* [http://www.cresswellslist.com/ballots2/reading.htm Excerpts from the Reading Eagle, November 1911 and November 1935] . Articles on the\nReading, PennsylvaniaSocialist Party.\n* [http://www.marxisthistory.org/subject/usa/eam/socialistparty.html Socialist Party chronology] in [http://www.marxisthistory.org/subject/usa/eam/index.html Early American Marxism Archive] . Retrieved August 23, 2006.\n* [http://www.marxisthistory.org/subject/usa/eam/documentdownloads.html SPA Downloadable Documents 1897 - 1930] on Marxist Internet Archive. Retrieved\nApril 20 2005.\n* [http://www.marxisthistory.org/subject/usa/eam/spapubs.html Lists of SPA Publications 1897 - 1930] on Marxist Internet Archive. Retrieved\nApril 20 2005.\n* [http://www.marxisthistory.org/subject/usa/eam/spaofficials.html Lists of SPA Officials 1897 - 1936] . Retrieved May 29, 2006.\n* [http://www.marxisthistory.org/subject/usa/eam/spamembership.html List of SPA Membership figures 1899 - 1946] . Retrieved May 29, 2006.\n* [http://www.cresswellslist.com/sp_auctions/referenc.htm Socialist Party Reference Material] . Guide to campaign buttons and iconography of the SPA.\nWikimedia Foundation. 2010.\nLook at other dictionaries:\nSocialist Party of America — Die Sozialistische Partei Amerikas (Socialist Party of America, SPA) war eine sozialistische, politische Partei in den Vereinigten Staaten und amerikanischer Teil der Sozialistischen Internationalen. Sie wurde 1901 durch den Zusammenschluss der… … Deutsch Wikipedia\nSocialist Party Of America — (SPA) Founded in 1901 from factions of the Social Democratic Party, the Socialist Labor Party, Christian Democrats and others, the Socialist Party of America was committed to peaceful, democratic reforms as a means of achieving socialism. By… … Historical dictionary of Marxism\nSocialist Party Of America — (SPA) In 1901, the Social Democratic Party, led by Eugene V. Debs, joined with reformist elements of the Socialist Labor Party, led by Morris Hillquit, to establish the SPA. The SPA was committed to state ownership of the means of production… … Historical Dictionary of the Roosevelt–Truman Era\nNon-English press of the Socialist Party of America — Logo of the Socialist Party of America, established August 1901. For a number of decades after its establishment in August 1901, the Socialist Party of America produced or inspired a vast array of newspapers and magazines in an array different… … Wikipedia\nNational Socialist Party of America — This series is linked to the Politics and Elections series Part of the Politics series on Neo Fascism … Wikipedia\nNational Socialist Party of America v. Village of Skokie — National Socialist Party v. Skokie Supreme Court of the United States Decided June 14, 1977 … Wikipedia\nSocialist Party USA — Chairman Billy Wharton Stephanie Cholensky Founded January 1, 1973 ( … Wikipedia\nSocialist party (disambiguation) — Socialist Party may refer to: Action * Arab Socialist Action Party * Arab Socialist Action Party – Arabian Peninsula * Liberal Socialist Action Party * National Socialist Action Party * Sardinian Socialist Action Party * Senegalese Party of… … Wikipedia\nSocialist Party of New York — Chairman Lawrence Rockwood Senate leader None House leader … Wikipedia\nSocialist Party of Michigan — Chairman James Arnoldi Senate leader None House leade … Wikipedia']"	['<urn:uuid:544ba8a5-ab46-4f36-bb4e-9ba56d935aa3>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T00:20:10.754925	26	101	2879
64	What should I consider when looking for sustainable wood products, and what are the best certifications to look for?	When looking for sustainable wood products, it's important to check if they are certified and by whom. The Forest Stewardship Council (FSC) logo is one of the best certifications, indicating properly and responsibly managed woods. It's also good to look at certifications provided by your state. Additionally, the most sustainable woods are those that travel the least distance from harvest to consumer, and come from well-managed local forests.	['The most sustainable wood choices for your building projects are woods that are grown locally and are well managed. Depending on where you live, the best type of sustainable wood will vary. However, nature tends to be smarter than we are. If your local wood producer is managing certain types of trees, they will likely work well for your building projects.\nFor example, Mathew Kelty, who teaches about forest ecology at the University of Massachusetts, Amherst said “In the northwest, Douglass fir is everywhere. There is also a lot of western red cedar, which is soft, but doesn’t decay. It’s often used for shingles for that reason.” Kelty added that in the northeast region a lot of spruce and fir is used due to its abundance. DIY Resource: http://www.networx.com/article/managed-forests-make-for-more-sustainabl\nThe idea of sustainable wood products is larger than what specific woods to use. Sustainability in relation to wood products has more to do with appropriate land management, biodiversity of plants and animals, and the integrity of the soil according to Emily Boss, director of the Massachusetts Woodlands Institute. “We prefer to use the term ‘responsible management’ of wood over ‘sustainable,’” said Boss. “Responsible management is the goal,” she added.\nWhen you are looking for sustainable wood products, it is important to know if they are certified woods and by whom. One of the best certifications comes from the Forest Stewardship Council. The FCS logo is an excellent indicator that you have chosen woods that have been properly and responsibly managed. It is also a good idea when researching local sustainable wood sources to look at certifications your state provides. Boss said in Massachusetts, for example, a new certification was created approximately a year ago called the Commonwealth Quality program. There are also similar programs in Vermont and Maine, Boss said. “There are no quick answers (regarding sustainable wood products). But, FCS takes into account how the workers are being treated, indigenous people’s issues; there are a lot of variable when talking about sustainable wood products,” said Boss.\nThe best sustainable woods travel the least distance from harvest to consumer. “Local soft and hardwoods use less energy to transport. If you are supporting a local business, you can get to know then, know how high their land management standards are,” said Boss. Boss said that good land management also take invasive species such as bittersweet and and wild rose into account as they take over and are destructive to native trees and plants.\nBoss said that typically in the New England region, oak, white pine, sugar maple, red maple, and birch are the most commonly used woods for lumber production. It’s also good to know where the wood is being processed. In other words, do you have a local saw mill? DIY Resource: http://www.hometalk.com\nBeware of using tropical woods such as teak, rosewood or ebony for custom woodworking. Many tropical areas of the world where these woods are harvested have not been responsibly managed, and over-harvesting often causes environmental, political and social harm to indigenous peoples.\nTo sum up how to find the most sustainable woods, look for certified woods from managed forests that travel the shortest distance from harvest to consumer.']	['<urn:uuid:b46c9caa-dfa8-44bd-a9c9-e36d1684f406>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T00:20:10.754925	19	68	531
65	reason why older equines unable shed hair during warm months	Failure to shed the winter coat in older horses is a good indication of Cushing's disease. It is really the only condition that causes older horses not to shed their hair in summer.	['As summer sets in most horses have shed their winter coats. But some older horses are still clinging to their winter coat. Some have not shed it at all.\nFailure to shed the winter coat is a good indication that a horse has Cushing’s disease.\nCushing’s disease is being recognised more and more frequently in older horses and ponies. The condition is named because of its similarities to the human disease of the same name. But there are significant differences.\nEquine Cushing’s disease is associated with excessive cortisol production by the adrenal gland. In almost all cases, it is caused by increased activity in the intermediate lobe of the pituitary gland, which sits at the base of the brain. The underlying problem seems to lie with nerves that should limit the activity of the affected part of the gland.\nIn normal horses, ACTH, a hormone from the pituitary gland, stimulates the release of cortisol from the adrenal glands. The blood cortisol level is controlled by a complex « feed back” mechanism. Basically, an increase in blood cortisol inhibits further ACTH release, which in turn causes the cortisol level to fall.\nIn horses with Cushing’s disease the abnormal pituitary gland produces ACTH, and other related molecules, some of which increase the “potency” of ACTH. In addition, the abnormal gland does not respond to the normal feedback controls. Most of the signs seen with Cushing’s disease are due to the increased cortisol activity.\nOften the first sign that a horse is affected with Cushing’s disease is hirsutism, the development of an excessively long and often curly coat. Shedding of the coat that normally occurs in summer either does not occur at all or is greatly reduced.\nOther signs include excessive sweating, weight loss, poor performance, increased drinking and passing increased amounts of urine. Almost all affected animals go on to develop laminitis eventually. Because the laminitis in these cases is the result of internal problems rather than a momentary dietary indiscretion, it is often more difficult to treat than laminitis due to dietary causes.\nVarious tests have been used to help to confirm the diagnosis. Unfortunately the tests can sometimes be misleading. In most cases they are not necessary. There is really only one condition that causes older horses not to shed their hair in summer – and that is Cushing’s disease.\nSo, what can be done for horses with Cushing’s disease? Veterinarians have used a variety of medications, originally intended for human use, to treat Cushing’s disease in horses. The most effective seems to be a drug called pergolide. If the treatment is successful, as well as showing a general improvement in condition, a horse may start to shed its coat again.\nSome people have found that herbal mixtures containing chaste berry extracts have helped, but others disagree.\nClipping the excessive coat will make the horse more comfortable, and may reduce the risk of skin infections. Regular corrective trimming of the feet is likely to be necessary once laminitis develops.\nThe onset of Cushing’s disease need not necessarily mean a horse’s days are numbered. But once you spot the telltale signs have a word with your veterinarian to discuss the options that are available for managing the condition.']	['<urn:uuid:ae6446d4-c5f4-4d68-8aa1-bf5bccfacf31>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-13T00:20:10.754925	10	33	537
66	biomass renewable fuels production pathways comparison	There are multiple pathways for producing renewable fuels from biomass. For diesel engines, biomass can undergo fast pyrolysis to produce a high-oxygen content liquid that can be upgraded to distillate fuels through hydrogenation. Similarly, for gasoline engines, biomass pyrolysis followed by hydrogenation can produce drop-in fuels. Additionally, biomass can be used to produce hydrogen through several routes: pyrolysis or gasification of agricultural residues and consumer wastes can produce bio-oil that can be reformed to generate hydrogen, while certain biomass like lignocellulosic materials can be pretreated and fermented by microorganisms like Clostridium to directly produce hydrogen.	"[""NREL biofuels projects help overcome technical barriers and expand markets for renewable, biodegradable vehicle fuels. These new liquid fuels include higher-level ethanol blends, butanol, biodiesel, renewable diesel, and other biomass-derived fuels. NREL's biofuels research and development helps improve engine efficiency, reduce polluting emissions, and improve U.S. energy security by reducing petroleum dependency.\nBiofuels for Diesel Engines\nNREL's diesel biofuels research and development focuses on developing fuel quality standards and demonstrating compatibility with engines and emission control systems. Highly efficient heavy-duty diesel truck engines are the primary power source for global transportation of freight. Light-duty diesel-fueled passenger vehicles have much higher fuel economy than comparable gasoline-powered cars.\nBiodiesel use in the United States surpassed 1 billion gallons in 2011. This renewable fuel can be produced from vegetable oil, algal oil, animal fats, and waste grease by a process called transesterification. NREL's biodiesel research includes fuel quality test methods, fuel quality surveys, oxidation stability, cold weather operability, air pollutant emissions effects, and impact on emission control catalysts and filters. Research is focused on demonstrating vehicle and infrastructure compatibility for blends well beyond the current 5 volume percent, up to 20 volume percent.\nFischer-Tropsch (FT) diesel is made from carbon monoxide and hydrogen produced from carbon-containing sources such as natural gas, biomass, or coal. NREL's FT diesel research examines fuel properties and demonstrates the use of this fuel in vehicle fleets.\nHydroisomerized Fats and Oils\nAlso known as hydrocarbon renewable diesel, this fuel is produced by hydrogenation and isomerization of the same feedstocks used to make biodiesel. NREL investigates chemical composition and properties of this fuel in detail, as well as combustion and emissions impacts.\nBiomass Pyrolysis-Derived Diesel\nFast pyrolysis of biomass produces a high-oxygen-content liquid that can be upgraded to distillate fuels by hydrogenation. Research at NREL focuses on the basic properties of these fuels and what levels of oxygen can be tolerated in drop-in fuels.\nA variety of oxygenated molecules might be chemically derived from lignocellulosic biomass, which includes sustainable forest and agricultural waste, as well as perennial crops. NREL is evaluating some of these materials for performance as diesel fuels.\nBiofuels for Spark-Ignited Engines\nNREL researches and develops biofuels for spark-ignited or gasoline engines, which are currently the predominant power source for personal vehicles.\nEach year, the United States produces approximately 14 billion gallons of ethanol, and almost all gasoline sold in this country contains 10% of this corn-derived biofuel. NREL research focuses on developing 15% ethanol blends for conventional cars and on understanding the performance of flex-fuel vehicles that can operate on ethanol levels as high as 83%. The lab also is examining how higher level ethanol blends can be used to enable the development of very efficient, high compression ratio, direct injection, turbocharged engines.\nButanol and Long-Chain Alcohols\nThese fuel blendstocks can be produced from biomass via several processes. NREL research focuses on how blending butanol and long-chain alcohols into gasoline affects fuel properties such as octane number, vapor pressure, and distillation curve. The lab also characterizes impurities and tailpipe emissions.\nBiomass Pyrolysis-Derived Gasoline\nFast pyrolysis of biomass produces a high oxygen content liquid that can be upgraded to distillate fuels by hydrogenation. Research at NREL focuses on the basic properties of these fuels, as well as determining what levels of oxygen can be tolerated in drop-in fuels.\nNREL is evaluating several oxygenates that can be derived by chemical reactions from lignocellulosic biomass. Current research is examining oxygenated properties, fuel blend properties, and combustion.\nNREL's biofuels work helps advance the U.S. Department of Energy's 21st Century Truck Partnership, U.S. DRIVE, and Vehicle Technologies Office transportation energy savings goals. For information about related projects, see Advanced Combustion and Fuels and the Renewable Fuels and Lubricants (ReFUEL) Laboratory."", ""Hydrogen Production and Delivery\nResearchers at NREL are developing advanced processes to produce hydrogen economically from sustainable resources.\nBiological Water Splitting\nCertain photosynthetic microbes use light energy to produce hydrogen from water as part of their metabolic processes. Because oxygen is produced along with the hydrogen, photobiological hydrogen production technology must overcome the inherent oxygen sensitivity of hydrogen-evolving enzyme systems. NREL researchers are addressing this issue by screening for naturally occurring organisms that are more tolerant of oxygen and by creating new genetic forms of the organisms that can sustain hydrogen production in the presence of oxygen. Researchers are also developing a new system that uses a metabolic switch (sulfur deprivation) to cycle algal cells between the photosynthetic growth phase and the hydrogen production phase.\nContact: Maria Ghirardi\nNREL scientists are developing pretreatment technologies to convert lignocellulosic biomass into sugar-rich feedstocks that can be directly fermented to produce hydrogen, ethanol, and high-value chemicals. Researchers are also working to identify a consortium of Clostridium that can directly ferment hemicellulose to hydrogen. Other research areas involve bio-prospecting efficient cellulolytic microbes, such as Clostridium thermocellum, that can ferment crystalline cellulose directly to hydrogen to lower feedstock costs. Once a model cellulolytic bacterium is identified, its potential for genetic manipulations, including sensitivity to antibiotics and ease of genetic transformation, will be determined. NREL's future fermentation projects will focus on developing strategies to generate mutants that are blocked selectively from producing waste acids and solvents to maximize hydrogen yield.\nContact: Pin-Ching Maness\nConversion of Biomass and Wastes\nHydrogen can be produced via pyrolysis or gasification of biomass resources such as agricultural residues like peanut shells; consumer wastes including plastics and waste grease; or biomass specifically grown for energy uses. Biomass pyrolysis produces a liquid product (bio-oil) that contains a wide spectrum of components that can be separated into valuable chemicals and fuels, including hydrogen. NREL researchers are currently focusing on hydrogen production by catalytic reforming of biomass pyrolysis products. Specific research areas include reforming of pyrolysis streams and development and testing of fluidizable catalysts.\nContact: Richard French\nPhotoelectrochemical Water Splitting\nThe cleanest way to produce hydrogen is by using sunlight to directly split water into hydrogen and oxygen. Multijunction cell technology developed by the photovoltaic industry is being used for photoelectrochemical (PEC) light harvesting systems that generate sufficient voltage to split water and are stable in a water/electrolyte environment. The NREL-developed PEC system produces hydrogen from sunlight without the expense and complication of electrolyzers, at a solar-to-hydrogen conversion efficiency of 12.4% lower heating value using captured light. Research is underway to identify more efficient, lower cost materials and systems that are durable and stable against corrosion in an aqueous environment.\nSolar Thermal Water Splitting\nNREL researchers use the High-Flux Solar Furnace reactor to concentrate solar energy and generate temperatures between 1,000 and 2,000 degrees Celsius. Ultra-high temperatures are required for thermochemical reaction cycles to produce hydrogen. Such high-temperature, high-flux, solar-driven thermochemical processes offer a novel approach for the environmentally benign production of hydrogen. Very high reaction rates at these elevated temperatures give rise to very fast reaction rates, which significantly enhance production rates and more than compensate for the intermittent nature of the solar resource.\nContact: Judy Netter\nRenewable energy sources such as photovoltaics, wind, biomass, hydro, and geothermal can provide clean and sustainable electricity for our nation. However, renewable energy sources are naturally variable, requiring energy storage or a hybrid system to accommodate daily and seasonal changes. One solution is to produce hydrogen through the electrolysis—splitting with an electric current—of water and to use that hydrogen in a fuel cell to produce electricity during times of low power production or peak demand, or to use the hydrogen in fuel cell vehicles.\nResearchers at NREL's Energy Systems Integration Facility and Hydrogen Infrastructure Testing and Research Facility are examining the issues related to using renewable energy sources for producing hydrogen via the electrolysis of water. NREL tests integrated electrolysis systems and investigates design options to lower capital costs and enhance performance.\nLearn more about NREL's renewable electrolysis research.\nContact: Kevin Harrison\nHydrogen Dispenser Hose Reliability\nWith a focus on reducing costs and increasing reliability and safety, NREL performs accelerated testing and cycling of 700 bar hydrogen dispensing hoses at the Energy Systems Integration Facility using automated robotics to simulate field conditions. View the video of the robot, which mimics the repetitive stress of a person bending and twisting a hose to dispense hydrogen into a fuel cell vehicle's onboard storage tank. Researchers perform mechanical, thermal, and pressure stress tests on new and used hydrogen dispensing hoses. The hose material is analyzed to identify hydrogen infiltration, embrittlement, and crack initiation/propagation.\nContact: Kevin Harrison\nHydrogen Production and Delivery Pathway Analysis\nNREL performs systems-level analyses on a variety of sustainable hydrogen production and delivery pathways. These efforts focus on determining status improvements resulting from technology advancements, cost as a function of production volume, and the potential for cost reductions. Results help identify barriers to the success of these pathways, primary cost drivers, and remaining R&D challenges. NREL-developed hydrogen analysis case studies provide transparent projections of current and future hydrogen production costs. Learn more about NREL's systems analysis work.\nContact: Genevieve Saur\nHydroGEN Energy Materials Network\nNREL serves as the lead laboratory for the HydroGEN Energy Materials Network (EMN) consortium.\nRemarkable Stability of Unmodified GaAs Photocathodes during Hydrogen Evolution in Acidic Electrolyte, Journal of Materials Chemistry A (2016)\nSolar to Hydrogen Efficiency: Shining Light on Photoelectrochemical Device Performance, Energy and Environmental Science (2016)\nReversible GaInP2 Surface Passivation by Water Adsorption: A Model System for Ambient-Dependent Photoluminescence, Journal of Physical Chemistry C (2016)\nCO2-Fixing One-Carbon Metabolism in a Cellulose-Degrading Bacterium Clostridium thermocellum, Proceedings of the National Academy of Sciences (2016)\nPhosphoketolase Pathway Contributes to Carbon Metabolism in Cyanobacteria, Nature Plants (2016)""]"	['<urn:uuid:76354b21-9f6c-4f28-b739-41f26c308d63>', '<urn:uuid:13a02b09-8dfc-4538-bd22-78817cd111fe>']	open-ended	direct	short-search-query	distant-from-document	three-doc	expert	2025-05-13T00:20:10.754925	6	95	1578
67	I'm thinking about becoming a medical lab technician - what subjects should I study in school, and what personal qualities are important for success in this field?	You should focus on science subjects like biology, chemistry, and physics, along with math and computer technology. Biology and chemistry are particularly crucial as you'll need knowledge of organisms, cells, and chemical processes. As for personal qualities, the job requires dependability, attention to detail, integrity, and stress tolerance. You must also be adaptable, demonstrate initiative, and maintain strong self-control while working independently.	"[""What is a Medical Lab Technician?\nClinical laboratory technologists perform testing that is crucial to detecting and diagnosing diseases. They examine and analyze body fluids, tissues and cells and look for bacteria, parasites and other microorganisms. People in this field use microscopes and high tech equipment that is automated to perform many tests at once. Medical Lab technologists have knowledge in biology, chemistry and medicine. They also use mathematics to perform tests and must accurately document their findings. When there is an emergency, the technician has to work quickly to provide physicians with the results of their tests to save patients lives.\nReal People Profile\n||Vincent M. Carraway\n||Palmetto Health Richland\n||Bachelor of Science in biology with a minor in chemistry\nMasters in medical technology\nMasters in public health\n|Biology and organic chemistry\n|First Real Job:\n||Investigation of disease outbreaks\nFishing, canoeing, building/woodworking, gardening\nReal People Q&A\nAdvice for students interested in my job:\nDo your best in all subjects, especially science and math, but also those related to computers and communication.\nWhat subjects in school will students need to do my job?\nBiology, chemistry, physics, math, computer technology\nWhat I like most about my job?\nWhat I like best is investigating the unknown. Basically what we do is take a specimen that was collected from that patient and it's unknown what is causing that infection. We have to do various testing procedures to determine what's happening, if anything, to cause the patient's infection. It's like playing Sherlock Holmes sometimes because we don't know what we're expected to find. Sometimes the doctor can tell us what he suspects but often times he doesn't. He gives us the specimens and says I need all of the cultures and testing done to it, and we have to narrow it down to what probable cause for the infection, and then we have to do more testing to determine what antibiotics to use to treat that infection. It's interesting being part of the healthcare team because the doctors and nurses need that vital information to properly care for the patient.\nWhat is the biggest challenge in my job?\nWhile some of our testing may take hours, days or even weeks, it's important to remember that some of that information has to be relayed back to the physician very quickly. We have to prioritize what we are doing just like a child may have to prioritize their schoolwork. We have to put some things aside so we can do some testing quickly and get those results back to the patients and get back to the other testing after that. I think that is challenging.\nHow do you get a job like mine?\nStudy and keep up with your schoolwork because you are going to need all of that, math and science and computer skills, when you get to college, you're going to need that. When you are in college you are going to need to study the sciences. Most medical technologists get a bachelor's degree in one of the science fields and then they are going to study medical technology for a year after that. You are going to have to have good grades to get into college and that medical technology program. Good grades are going to be important to get that first job."", ""Medical and Clinical Laboratory Technicians\nThis occupation is expected to grow rapidly.\nPerform routine medical laboratory tests for the diagnosis, treatment, and prevention of disease. May work under the supervision of a medical technologist.\nSample Job Titles\nCertified Clinical Laboratory Technician\nClinical Laboratory Scientist\nClinical Laboratory Technician (Clinical Lab Technician)\nLaboratory Assistant (Lab Assistant)\nMedical Laboratory Technician (MLT)\nMedical Laboratory Technicians (Medical Lab Technician)\nActive Listening - Giving full attention to what other people are saying, taking time to understand the points being made, asking questions as appropriate, and not interrupting at inappropriate times.\nComplex Problem Solving - Identifying complex problems and reviewing related information to develop and evaluate options and implement solutions.\nCritical Thinking - Using logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems.\nInstructing - Teaching others how to do something.\nMonitoring - Monitoring/Assessing performance of yourself, other individuals, or organizations to make improvements or take corrective action.\nOperation Monitoring - Watching gauges, dials, or other indicators to make sure a machine is working properly.\nQuality Control Analysis - Conducting tests and inspections of products, services, or processes to evaluate quality or performance.\nReading Comprehension - Understanding written sentences and paragraphs in work related documents.\nScience - Using scientific rules and methods to solve problems.\nSpeaking - Talking to others to convey information effectively.\nArm-Hand Steadiness - The ability to keep your hand and arm steady while moving your arm or while holding your arm and hand in one position.\nCategory Flexibility - The ability to generate or use different sets of rules for combining or grouping things in different ways.\nControl Precision - The ability to quickly and repeatedly adjust the controls of a machine or a vehicle to exact positions.\nFinger Dexterity - The ability to make precisely coordinated movements of the fingers of one or both hands to grasp, manipulate, or assemble very small objects.\nInformation Ordering - The ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations).\nNear Vision - The ability to see details at close range (within a few feet of the observer).\nOral Comprehension - The ability to listen to and understand information and ideas presented through spoken words and sentences.\nOral Expression - The ability to communicate information and ideas in speaking so others will understand.\nProblem Sensitivity - The ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem.\nWritten Comprehension - The ability to read and understand information and ideas presented in writing.\nBiology - Knowledge of plant and animal organisms, their tissues, cells, functions, interdependencies, and interactions with each other and the environment.\nChemistry - Knowledge of the chemical composition, structure, and properties of substances and of the chemical processes and transformations that they undergo. This includes uses of chemicals and their interactions, danger signs, production techniques, and disposal methods.\nClerical - Knowledge of administrative and clerical procedures and systems such as word processing, managing files and records, stenography and transcription, designing forms, and other office procedures and terminology.\nComputers and Electronics - Knowledge of circuit boards, processors, chips, electronic equipment, and computer hardware and software, including applications and programming.\nCustomer and Personal Service - Knowledge of principles and processes for providing customer and personal services. This includes customer needs assessment, meeting quality standards for services, and evaluation of customer satisfaction.\nEducation and Training - Knowledge of principles and methods for curriculum and training design, teaching and instruction for individuals and groups, and the measurement of training effects.\nEnglish Language - Knowledge of the structure and content of the English language including the meaning and spelling of words, rules of composition, and grammar.\nMathematics - Knowledge of arithmetic, algebra, geometry, calculus, statistics, and their applications.\nMedicine and Dentistry - Knowledge of the information and techniques needed to diagnose and treat human injuries, diseases, and deformities. This includes symptoms, treatment alternatives, drug properties and interactions, and preventive health-care measures.\nPublic Safety and Security - Knowledge of relevant equipment, policies, procedures, and strategies to promote effective local, state, or national security operations for the protection of people, data, property, and institutions.\nConduct chemical analyses of body fluids, such as blood or urine, using microscope or automatic analyzer to detect abnormalities or diseases and enter findings into computer.\nAnalyze the results of tests or experiments to ensure conformity to specifications, using special mechanical or electrical devices.\nSet up, maintain, calibrate, clean, and test sterility of medical laboratory equipment.\nPrepare standard volumetric solutions or reagents to be combined with samples, following standardized formulas or experimental procedures.\nCollect blood or tissue samples from patients, observing principles of asepsis to obtain blood sample.\nSupervise or instruct other technicians or laboratory assistants.\nConduct blood tests for transfusion purposes and perform blood counts.\nInoculate fertilized eggs, broths, or other bacteriological media with organisms.\nObtain specimens, cultivating, isolating, and identifying microorganisms for analysis.\nExamine cells stained with dye to locate abnormalities.\nCommunicating with Supervisors, Peers, or Subordinates - Providing information to supervisors, co-workers, and subordinates by telephone, in written form, e-mail, or in person.\nDocumenting/Recording Information - Entering, transcribing, recording, storing, or maintaining information in written or electronic/magnetic form.\nEvaluating Information to Determine Compliance with Standards - Using relevant information and individual judgment to determine whether events or processes comply with laws, regulations, or standards.\nGetting Information - Observing, receiving, and otherwise obtaining information from all relevant sources.\nInspecting Equipment, Structures, or Material - Inspecting equipment, structures, or materials to identify the cause of errors or other problems or defects.\nInteracting With Computers - Using computers and computer systems (including hardware and software) to program, write software, set up functions, enter data, or process information.\nMaking Decisions and Solving Problems - Analyzing information and evaluating results to choose the best solution and solve problems.\nMonitor Processes, Materials, or Surroundings - Monitoring and reviewing information from materials, events, or the environment, to detect or assess problems.\nProcessing Information - Compiling, coding, categorizing, calculating, tabulating, auditing, or verifying information or data.\nUpdating and Using Relevant Knowledge - Keeping up-to-date technically and applying new knowledge to your job.\nAdaptability/Flexibility - Job requires being open to change (positive or negative) and to considerable variety in the workplace.\nAnalytical Thinking - Job requires analyzing information and using logic to address work-related issues and problems.\nAttention to Detail - Job requires being careful about detail and thorough in completing work tasks.\nCooperation - Job requires being pleasant with others on the job and displaying a good-natured, cooperative attitude.\nDependability - Job requires being reliable, responsible, and dependable, and fulfilling obligations.\nIndependence - Job requires developing one's own ways of doing things, guiding oneself with little or no supervision, and depending on oneself to get things done.\nInitiative - Job requires a willingness to take on responsibilities and challenges.\nIntegrity - Job requires being honest and ethical.\nSelf Control - Job requires maintaining composure, keeping emotions in check, controlling anger, and avoiding aggressive behavior, even in very difficult situations.\nStress Tolerance - Job requires accepting criticism and dealing calmly and effectively with high stress situations.""]"	['<urn:uuid:b4c1d48e-4232-46f3-be90-0fba3d402ac0>', '<urn:uuid:78e6f223-23c6-490d-91f9-5a6f45557002>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T00:20:10.754925	27	62	1758
68	how did family members cope with bennie dexter disappearance	The disappearance had a severe emotional impact on Bennie Dexter's family. His mother was particularly affected, with reports indicating that the uncertainty and not knowing took a significant toll on her over the years. She passed away unable to fulfill her wish of bringing her son's remains home. His sister Dollie continues to miss him every day, though she mentions being more at peace now and able to talk more openly about him. The Air Force continues to search for answers about what happened to Bennie, and he remains listed as a 'priority one' case as they continue to search for his remains.	"['THANK YOU MGBOOTH@HOTMAIL.COMPosted on 5/10/15 - by firstname.lastname@example.orgThank you for your offer to send the bracelet. The accounting for the lost is not over. Bennie is still listed as a ""priority one"" case and they continue to search for his remains. It is important that our children and grand children know about the Vietnam era from people who lived it. The bracelet, with Bennie\'s name, gives a more personal connection to the war. I would prefer that you either: start wearing it again (I have many people that still wear theirs until he is found); give it to a Grandchild telling them of the history; or simply put it back in your jewelry box or some other place of ""honor"". I am always cautious about responding or sending emails to unknown people. One again, thank you for remembering.MORE\nDollie Dexter Raymond\nPOW BraceletPosted on 5/9/15In my old jewelry box I cam across a POW Bracelet with Sgt. Bennie Dexter, 5-10-66 engraved into it. I remember buying the bracelet and wearing it, but had forgotten about it. If any famiily member would like to have the bracelet, please get in touch with me. My email address is email@example.com.MORE\nTHANK YOU FOR REMEMBERINGPosted on 1/15/15 - by Bennies SisterI am happy to see the recent posts. I know there are a lot of people who ""still remember"". The years took a toll on Mother and she died unable to fulfill my brothers wish of getting his remains home. We still do not have an accounting for my brother and many other vets. I continue to pray for our government and an accounting for all that were lost.MORE\nDollie Dexter Raymond\nBennie\'s motherPosted on 1/15/15 - by Greg Brown Gregb@bendcable.comI was only thirteen and living down the street from the Dexter\'s when he was captured. His mother started Operation Sweettooth to get packages to service people and you could see the toll over the years the not knowing took on her.MORE\nFinal Mission of A2C Bennie L. DexterPosted on 1/10/15 - by firstname.lastname@example.orgOn May 10, 1966, A2C Bennie L. Dexter was captured by communist forces while driving a jeep south on National Route 14 near the Darlac/Quang Duc Province boder, South Vietnam. His jeep was found at this location the next day and there was a Vietnamese witness to his capture. Subsequent intelligence reports confirmed Dexter\'s capture and named the location at which he was being held. Dexter was placed in Prisoner of War status. When American military involvement ended in Southeast Asia, Bennie Dexter was not released from prison, nor did his name appear on any lists provided by the Vietnamese. The United States government later declared him administratively ""dead"" because no specific information had been received that he was still alive. [Taken from pownetwork.org]MORE\nBENNIE - NOT FORGOTTENPosted on 12/10/14 - by Dollie Dexter RaymondI received an email this morning that someone had posted a remembrance to Bennie on this page. I didn\'t find any new postings. I decided to post myself. The statement ""Gone but NOT forgotten"" was very evident at our 50th class reunion this year. Every classmate at the reunion asked about Bennie. Some of the classmates are still wearing Bennie\'s bracelet. I have struggled through the years when talking about Bennie never coming home; no answers what happened to him and no remains. The Air Force continues to search for answers as to where, when , and what happened to Bennie. I am at peace these days and am able to talk more openly about Bennie although I miss him each and every day. Bennie is still in our hearts and we pray for closure. I am thankful that so many people have NOT FORGOTTEN.MORE\nBracelet found with fathers key belongings...Posted on 12/10/14 - by Danielle RosellisonMy dad, John Roselli, was drafted for Vietnam. He made it home, but sadly took his own life 11/10/2014. In a secret compartment of his desk, I found his passport, his SDB keys and Bennie\'s bracelet. I don\'t know you Bennie, and my dad never spoke of his time in Vietnam, but you sure did mean something special to my dad. I hope you\'re wearing bermuda shorts, sipping on drinks with umbrellas and shooting the shit with my dad somewhere.MORE\nUntil you come homePosted on 3/15/13 - by Victoria Davenport email@example.com\nI was blessed with the safe return of my family members who served and because of it I wanted to show my support for all the POWsMIAs by wearing a bracelet. I found Bennie in a Life Magazine article about the Wall and for some reason, his name touched a chord in my heart. I have worn the bracelet ever since and will do so until you come home. Until you all come home.\nMY BROTHER -NEVER EVER FORGOTTENPosted on 5/7/12 - by Your Sister - DollieAlthough it has been 46 years (Mother\'s Day - May 9th - 1966) since Bennie was taken as a POW he has ALWAYS been with me; in my heart, breath and soul. He was my BIG brother. He will always be with me and NEVER - EVER - FORGOTTEN. Our Mother is gone now too and I believe has found the peace she never had here on earth - always in \'pain\' over her first born child. I LOVE YOU MY BROTHER BENNIE; UNTIL WE MEET AGAIN.MORE\nPhotoPosted on 3/29/11 MORE\nMissing you Uncle BenniePosted on 6/12/10 - by Tammara Coleman firstname.lastname@example.orgMy entire life I waited for Uncle Bennie to come home. I think of him daily and wonder how our family would have been different if he had come home. So many years of sadness, wondering what happened to him. My Uncle is my hero.MORE\nNever ForgottenPosted on 7/20/09 - byIn 1969 I had several MIA/POW bracelets to chose from and as I scanned the metal bracelets his name caught my eye. I knew Bennie had been missing for quite awhile but I had faith and hope that he would come home. A few months later I wrote a letter to his mother and she wrote me back telling me more information about him. That was when I found out we had something in common....we shared the same middle name. At that moment I felt it wasn\'t an accident I chose his bracelet. I wore his bracelet from 1969 until 1979 when I went to work in a mill and felt it was too dangerous to wear. All these years I have been very proud to have worn his name on my wrist. I was quite moved by the postings on the wall website from his niece and fellow supporters of Bennie. I recently visited the moving wall and was quite overwhelmed with tears and sadness for Bennie and his family that he has not come home yet. He will be in my thoughts and prayers forever.MORE\nBennie DexterPosted on 5/9/07Intelligence gathered during the search stated that an airman matching Bennie Dexter\'s description was seen being escorted into the jungle by five armed Viet Cong (VC). Based on this information, and subsequent intelligence reports that confirmed Bennie Dexter\'s capture including the named location where he was being held, A2C Dexter\'s status was upgraded from AWOL to Prisoner of War.MORE\n40 years laterPosted on 3/24/07 - by Jean Underhill email@example.comI went to Hight School with Bennie and I always looked up to him. I just had the opportunity to go to the Wall and found a braclet with Bennie\'s name on it. I now wear it proundly in rememberance and I think of his sacrifice for our country daily.MORE\nDo not stand at my grave and weepPosted on 5/9/05 - by Bob RossDo not stand at my grave and weep.MORE\nI am not there; I do not sleep.\nI am a thousand winds that blow,\nI am the diamond glints on snow,\nI am the sun on ripened grain,\nI am the gentle autumn rain.\nWhen you awaken in the morning\'s hush\nI am the swift uplifting rush\nOf quiet birds in circled flight.\nI am the soft stars that shine at night.\nDo not stand at my grave and cry,\nI am not there; I did not die.\nMary Frye – 1932\nUntil forever is gone, I will remember youPosted on 2/11/05 - by Joni firstname.lastname@example.orgThough I never met you, I love you the way you love your brother or your best friend. I wear a bracelet with your name on it so that other people know that you were here. I don\'t know what happened to you back in 1966 and that is why I will always wear this bracelet. So no one can forget. I named my guitar after you too, so every time I play it I remember to thank you for making it possible for me to be free.MORE\nMy MIA braceletPosted on 6/30/04 - by Peggy Moseley email@example.comI wore the bracelet the whole time of the war up until the MIA\'s and POW\'s come home and looked at the list and never found his name. I just want his family to know that I did wear his bracelet.MORE\nYou are not ForgottenPosted on 6/1/03 - by Joseph S. ConlklinI would first like to say that I am very honored to be wearing a braclet with Bennie Dexters name on it. I am a 22 year old male that was given this braclet on June 31st, 2003. I am a son of a retired Marine and I did 3 years in the Navy, but this is not about me. I would like to say that I am very thankfull for all the men a women that have faught and still fight today. You all should be very proud of yourself\'s and I hope you are. We will miss all of you very much and may God look over you all and keep you safe.MORE\nFor those who came home...\nFor those who did not come home...\nFor those who have yet to come home...\n... We love and miss you all.\nNot ForgottenPosted on 2/14/03 - by Candace LokeyI have not forgotten you. I chair the Adoption Committee for The National League of Families of Prisoners of War and Missing in Action in Southeast Asia. We will always remember the 1,889 Americans still unaccounted for in Southeast Asia and the thousands of others that lost their lives. We will not stop our efforts until all of you are home where you belong.MORE\nWe need to reach the next generation so that they will carry on when our generation is no longer able. To do so, we are attempting to locate photographs of all the missing. If you are reading this remembrance and have a photo and/or memory of this missing American that you would like to share for our project, please contact me at:\nPO Box 206\nFreeport, PA 16229\nIf you are not familiar with our organization, please visit our web site at :\nFrom a student who won\'t forget.Posted on 8/26/01 - by Bobby OlsonYour sacrifice to this country will not be forgotten. I am very sorry I did not get to meet you. The traveling Wall came to our school and I learned a lot about the war and the sacrifices of those who gave their lives. I have a bracelet my teacher gave me with your name on it and I wear it in your honor.MORE\nMary Seetoo has worn your braceletPosted on 2/11/01 - by Mary SeetooI have kept your bracelet in honor of your greatest sacrifice to freedom. You are loved. I would be honored to return my bracelet to an existing family member if they are interested.MORE\nBracelet WearerPosted on 7/30/00 - by Patricia MyersI did not know you, but wear your bracelet in memory and to honor your sacrifice. You are not forgotten.MORE\nThe Wall of Faces\nBrought to you by the organization that built The Wall, the Vietnam Veterans Virtual Memorial Wall is dedicated to honoring, remembering and sharing the legacies of all those who died in the Vietnam War. Here you can go beyond the names on The Wall to see the faces, share the stories and read the remembrances posted by friends, neighbors, classmates and family members.\nAll of these photos will be showcased in The Education Center at The Wall on the National Mall in Washington, D.C. To learn more about the effort to collect these photos and ensure their faces will never be forgotten, visit www.buildthecenter.org.']"	['<urn:uuid:294379ee-930a-4f58-83f2-e961c7c638bf>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T00:20:10.754925	9	103	2080
69	What kind of specialized treatment options are available for young individuals with severe autism who haven't responded well to traditional care settings?	There is a Short-term Autism Intensive Residential Services Neurobehavioral Unit (STAIRS-NBU) that provides intensive, autism-specific treatment for children ages 6 to 21 who have severe behavioral challenges and skills deficits and haven't responded to traditional community-based or residential settings. The program involves comprehensive functional behavioral assessment, followed by 3-6 months of intensive clinical services at the Children's Intellectual and Developmental Disabilities Services campus. The treatment team includes Board Certified Behavioral Analysts, clinical psychologists, psychiatrists, special education teachers, nurses, speech therapists, and specially trained direct care providers who use Applied Behavior Analysis and other evidence-based approaches.	"['Devereux Advanced Behavioral Health supports children and adults with autism spectrum disorder in the development of communication, social and independence skills needed to realize fulfilling, productive and socially connected lives. Our programs and services utilize evidence-based strategies, including positive behavioral supports and applied behavior analysis.\nWe offer services throughout the lifespan including: initial assessments, residential treatment, community-based group homes, approved private schools, theFrapeutic foster care, wraparound services, respite, school and family consultation and training, transition services for young adults, and adult day programs.\nKey principles of our autism programs:\nAutism Services for Children and Adolescents\nDevereux Advanced Behavioral Health schools for children and adolescents are state-licensed, approved private schools, serving children with autism spectrum disorders and other intellectual/developmental disabilities or emotional/behavioral disorders.\nWhat is an Approved Private School?\nIn accordance with the Individuals with Disabilities Education Act (IDEA), all children with disabilities are entitled to receive a free appropriate public education (FAPE) in the least restrictive environment (LRE). The school district and the state will share in the cost to place a child in an approved private school if the child’s Individualized Education Program (IEP) team determines the educational needs of the child cannot be met in a general education environment. In addition, parents may choose to privately fund the tuition cost of an Approved Private School. While placements are typically within state, children may be placed in an out of state program if an in-state school is not available.\nDevereux offers the following approved private day and residential programs for children and adolescents in Colorado, Connecticut, Florida, Massachusetts, New York and Pennsylvania:\nSchool Program Overview:\nWe offer instructional methods consistent with the science of applied behavior analysis and designed to reflect the needs of each child, including:\nOur transition programs for adolescents and young adults focus on achieving independence in self-care routines; domestic, employment, community-based and recreational skills; human sexuality; mobility training; and self-determination skills. We use person-centered futures planning and community-based instruction to support our students as they prepare for adulthood.\nFamilies and caregivers play an integral role at Devereux. Collaboration is facilitated through frequent, informal communication among teachers and families, as well as scheduled meetings such as the IEP, Futures Planning, Parent-Teacher Organization and Quality Improvement meetings.\nWe provide families with assessment services using the ADOS-2 – the gold standard of assessment – as well as guidance on appropriate services and interventions, if an autism diagnosis is confirmed.\nOur Short-term Autism Intensive Residential Services Neurobehavioral Unit (STAIRS-NBU) provides intensive, autism-specific treatment for children, ages six to 21, with severe behavioral challenges and skills deficits who have not responded to treatment in traditional community-based or residential settings.\nSTAIRS-NBU employs evidence-based practices across a continuum of care. These practices include a foundation of Positive Behavioral Interventions and Supports delivered in a treatment environment that is trauma sensitive and trauma informed. The continuum begins with a comprehensive, functional behavioral assessment in the family’s home, followed by intensive clinical services for a period of three to six months on our Children’s Intellectual and Developmental Disabilities Services (CIDDS) center campus. The continuum of care concludes with discharge and after care service planning in partnership with Devereux CARES consultants and BHRS (Wrap Around) services. The STAIRS-NBU team of professionals includes Board Certified Behavioral Analysts (BCBA), a clinical psychologist, child psychiatrist, special education teacher, nurses, speech language therapist, residential unit supervisor, and direct care providers specially trained in Applied Behavior Analysis, Picture Exchange Communication System (PECS), and Individualized Positive Behavior Support. Click here for more information or contact Kelly McCool at (610) 431-8191 or Martin Berthold at (610) 431-8126.\nDevereux Poconos Behavioral Health Rehabilitation Services\nDevereux Massachusetts Consultants\nSince 2004, CARES Consultants have partnered with school districts, intermediate units and families across the region by providing workshop trainings, conferences, ongoing consultation, and program development. Workshops include:\nCARES consultants have extensive experience training school districts with students who have traditionally been placed in Approved Private Schools. We have a well developed model, which uses implementation effectiveness measures such as performance checklists and team supervision meetings to ensure a more consistent application of instructional techniques and long-term positive staff and student performance.\nDevereux Millwood Consultation\nStaff of Devereux Millwood provide a variety of consultation and other services to families and agencies with adolescents and adults with ASD. These include:\nDevereux offers innovative, stand-alone transition programs that provide instruction and support across work, community and home environments for students ages 18 and older to assist them with the transition to adulthood.\nThis program assists school districts in meeting the unique needs of transition-aged students who do not benefit from spending a full school day in the classroom. The transition model emphasizes independence, employment readiness, community-based instruction and positive behavior support planning. With the support of highly qualified Devereux specialists who work with the school’s IEP team, students develop appropriate and effective skills that will facilitate independence throughout their adult lives.\nDevereux Pocono School to Work Transition\nThe School to Work Transition program serves older students, ages 18 to 21, who attend a local school district, and need additional skill development and training to gain competitive employment. Devereux, in partnership with the student’s school, develops a transition plan as part of the IEP process. These plans include skill development, and hands on training/prevocational experience.\nDevereux Glen Ridge, College and Employment Coaching Program\nGlen Ridge is a college and post-secondary program directed toward young adults with learning disabilities and Asperger\'s Syndrome. The Glen Ridge program focuses on developing the skills needed to succeed in college, work environments and social settings. The program encourages and coaches autonomy in a safe, challenging and supportive setting. Glen Ridge is located near a variety of community colleges, universities and businesses in beautiful Western Connecticut.\nDevereux offers group homes and campus based living for adults with autism and other intellectual and developmental disabilities. We provide a comprehensive life enrichment program that promotes self-improvement, decision-making, task completion, community involvement, recreation and family collaboration. Our programs emphasize skill development in self-care, functional communication, social skills, anger management, problem-solving, independent living, self-esteem, recreation, and community safety using intensive, highly-structured positive behavioral support techniques that focus on developing and teaching age-appropriate behaviors.\nThrough Lifesharing, individuals live with ""host"" families who provide homes for individuals with disabilities. Lifesharing enriches the quality of life of individuals with autism and other disabilities, as they become part of a family and the community, and offers opportunities for people to have:\nDevereux Pocono Life Sharing Program: Newfoundland, PA (570) 839-6147\n- About Us\n- Who We Help\n- Supporting Devereux']"	['<urn:uuid:63a25f21-66ae-47e7-ae4f-df69f0749e83>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T00:20:10.754925	22	95	1087
70	Do BIOHM and Wild Earth both work with fungi?	Yes, BIOHM studies fungi's role in gut health and has access to the second-largest collection of fungi strains after the CDC, while Wild Earth uses koji, a filamentous fungus, to produce protein for pet food.	['- This month, Cleveland, Ohio-based microbiome startup BIOHM raised a $7.5 million equity financing round.\n- Middleland Capital’s VTC Ventures led the oversubscribed round.\n- Additional investment came from Felton Group, JobsOhio Growth Capital Fund, Aztec Capital Management, and others.\n- New funds will enable further development of BIOHM’s discovery platform that turns microbiome data into potential solutions for food and beverage startups, consumers and the medical field.\nWhy it matters:\nBrian Mixer, managing director at Middleland Capital, says he’s seen a growing interest in the gut microbiome over the last several years. “We’ve seen overwhelming research that demonstrates the fundamental importance of the microbiome in human health,” he tells AFN.\nThe gut microbiome is the environment in and around our stomachs and intestines and is made up of trillions of microbes including bacteria, fungi, viruses, and protozoa. Some of these microbes are good for a person’s health, some are not.\nMuch of the work done on the microbiome so far has focused on bacteria. However, the role of fungi could be equally important. It is therefore a very overlooked component of health and wellness.\nBIOHM co-founder Dr. Mahmoud Ghannoum has studied fungi for more than 40 years. His work eventually led him to the conclusion that fungal organisms are as essential to the microbiome as bacterial ones. In 2010, Dr. Ghannoum and his team released research about fungal communities in the gut and coined the term “mycobiome” to describe these communities.\nThe research also explored the critical relationship between bacteria and fungi when it comes to gut health.\nWhile these two things can work together to create a healthy microbiome, they can also team up to create a digestive biofilm. This is a plaque-like lining that forms on a person’s intestines and hides bad germs in the gut. Because of this, BIOHM believes gut health solutions must address both bacterial and fungal balance.\n“There’s growing research that [fungi] are a very important piece of the puzzle,” says Mixer. “You can’t just look at the bacteria, but how they work together in promoting both health and disease. When it comes to understanding the role of fungi, BIOMH is the leader.”\nHow BIOHM works:\nBIOHM’s business includes both direct-to-consumer and business-to-business products and services.\nBIOHM FX is a probiotic supplement for consumers that specifically addresses the aforementioned digestive biofilm in the gut. Once that lining is destroyed, the product releases good bacteria and fungi to neutralize the bad ones that were hiding out in the biofilm.\nIn the B2B realm, BIOHM partners with other companies to make products and ingredients based on its extensive library of microbiome data. BIOHM says it manages one of the most comprehensive gut health datasets and has access to the second-largest collections of fungi strains, after the Centers for Disease Control and Prevention (CDC). This enables “predictable and low-cast developments,” according to Mixer.\nBIOHM also works with partners on bioinformatics, testing, and clinical trial support. The company is currently embarking on a clinical extension directly for medical practitioners.\nMixer says the B2B pipeline and discovery platform were of particular interest to Middleland Capital.\n“They’re turning AI into powerful insights and working with some of the largest global ingredient companies. That validates that there’s real value in their dataset.”\nThat validation is no small matter.\nControversy often surrounds microbiome startups. They’ve been accused of fraud, raided by the FBI, and slammed for outrageous pricing on tests. All of this rolls up into concerns from both consumers and scientists about the strength of the science backing many of these companies.\nMixer believes BIOHM stands apart in this respect. “Proven science is an anomaly. There is a lot of buzz in the space, but not too many that are clinically backed by science.”\n“They built this state-of-the-art microbiome innovation platform and have a really exciting, robust B2B pipeline,” he adds. “They’re working with global industry leaders to create next-gen products using BIOHM’s database. We saw that robust pipeline validating that this is proven science, which is unique in the sector.”\nBIOHM’s location was also attractive to Middleland and VTC: Cleveland, Ohio has some of the world’s leading academic medical centers, such as Case Western University. It’s also an underdeveloped ecosystem for technology, including foodtech and ingredient tech.\nBoth of these factors work in BIOHM’s favor, suggests Mixer. The company can leverage expertise from the universities and scientific community, and there’s a real, untapped opportunity in the area’s venture ecosystem.\n“It’s an exciting market when you look at it from an investor perspective,” he says of the microbiome space. “Research will continue to get better.”', 'A guest post by John Cumbers, Founder and CEO of SynbioBeta.\nJohn Cumbers will be teaching the Specialization Track ‘The Frontiers of Synthetic Biology’ on July 26 at the TFF Academy.\nJohn Cumbers is the founder and CEO of SynBioBeta, a global innovation network and community of entrepreneurs, thought-leaders and investors. He is the co-author of “What’s Your Bio Strategy?” a book on the impact that synthetic biology is having on all business. John has been consulting to leading companies and investors on the impact of synthetic biology for over a decade and has received multiple awards and grants from NASA and the National Academy of Sciences for his work in the field.\nA super connector, John has devoted himself to helping those around him connect into the synthetic biology ecosystem and gain the resources that they need to break through scientific and commercial boundaries and see their innovations applied in the real world.\nWe’ve used domesticated microbes— “good” bacteria and fungi—to produce food and drink for millennia. Today, innovators around the world are using modern biological tools to expand microbial repertoires, creating foods in surprising new ways.\nThese efforts are not only potentially profitable, but less environmentally harmful than conventional means of food production. Thus, synthetic biology could not only disrupt the world’s largest commercial sector, but also quietly revolutionize how we interact with the natural world. Here, we’ll meet three companies — all of which will be represented at SynBioBeta — that epitomize this approach.\nThe Great Hop Forward\nThe Reinheitsgebot (German Beer Purity Law), which recently celebrated its 500th birthday, specifies that beer can be made of only barley, hops, and water. The 16th-century Bavarians didn’t know anything about yeast, the microbe that ferments the barley. Given that, what’s brewing at Berkeley Brewing Sciences would really have blown off their Lederhosen: modifying yeast so that hops are no longer necessary.\nBBS metabolically engineered brewer’s yeast to synthesize the aromatic compounds present in the essential oils of the hop flower. The altered strain serves double duty during brewing, fermenting sugar while flavoring the final product—giving new meaning to the term “microbrew.”\n“It’s a true drop-in replacement,” explains co-founder Charles Denby. “A brewer could get rid of their old yeast and add our yeast,” avoiding the need to add hops at the end of the brewing process.\n«Na und?» our ancient Bavarian Meisterbrauer might ask. «Was ist das Problem mit Hopfen?» First, hops are resource-intensive, requiring 50 pints of water to grow enough flowers to flavor one pint of beer—that’s 100 billion liters a year, not counting the energy, fertilizer, and transportation infrastructure required to pipe in all the water and truck away all the hops.\nHops are also inconsistent, with the components of the essential oil varying significantly from crop to crop. The scientists at BBS solved this problem by precisely engineering yeast’s metabolic pathways to produce just the right levels of terpenes.\nBBS has not yet partnered with a major producer to sell hop-less beer fermented with its new yeast, so it might be awhile before we taste our first MGD (Miller GMO Draft). Currently, Denby says, “we’re focusing on making awesome products that brewers will want, and then commercialize from there.” But recent taste tests at Lagunitas Brewing Company clearly show that the industry is interested.\nWhat’s next? Denby sees limitless potential for metabolic engineering of yeast: “Yeast already has a lot of tricks up its sleeve, and it already makes a rich and complex beer. More metabolic engineering will give us a richer beer, and that can be further refined.” Moreover, the general approach could be applied beyond brewing to any consumable that requires molecular enhancement.\nBBS’s strategy exemplifies a potential “killer app” for synthetic biology: producing economically important food additives without requiring wasteful production and transport of excess biomass.\nHowever, most people don’t drink beer every day, but they almost invariably consume something sweet. That’s why we’re enthusiastic about a company that is developing another microbially produced flavoring agent, this one targeting the $3 billion market for high-intensity sweeteners.\nAmai Proteins, based in Israel, is commercializing peptides that taste like sugar but are digested like proteins. The consumer benefits are immediately obvious: Sugar damages human health by promoting obesity and diabetes, and many synthetic sweeteners pose health risks of their own or have off flavors that discourage wider adoption. By contrast, Amai’s compounds contain negligible calories and don’t activate the insulin response.\nThe sweet peptides are based on naturally occurring plant proteins modified by a method called Agile Integrative Computational Protein Design (AI-CPD), a field in which founder Ilan Samish has done extensive academic work. This technique can be used to optimize commercially important features of a protein (e.g., stability, solubility, and shelf-life) while preserving or improving qualities of interest (in this case, sweetness).\nThe computationally tweaked proteins are expressed in a type of yeast. Although these microbes are technically genetically engineered, the desired products can be purified and legally sold as non-GMO. Once the manufacturing details are resolved, the company hopes to have a product in a premium market within two years.\nAnd as with BBS, Amai’s overall strategy is not limited to one gustatory property. “There are numerous proteins in food that can be modified quickly using AI-CPD,” Samish explains, anticipating much broader application of these principles to food science. “In general, making healthy food for the general population requires higher yields and stability, which can be offered via synthetic biology.”\nFungus for our best friends\nFood additives constitute only a small fraction of what we eat and drink. But what if we could harness the power of microbes, not merely to flavor or modify existing food, but to make the food itself?\nOne exciting effort in this direction is targeted not at humans but at our companion animals. Wild Earth, another Berkeley-based company, is reinventing pet food from the ground up, using a filamentous fungus known as koji to build protein from scratch. Koji serves as the basis of Japanese fermentation, giving us products as diverse as soy sauce, miso, and sake.\nIn contrast to the way those foods are made, however, Wild Earth does not use grain as a feedstock. Instead, they provide their koji with the simplest biochemical building blocks: beet sugar, hydrolyzed yeast extract, and even a bit of fertilizer.\n“Essentially,” says co-founder and CEO, Ryan Bethencourt, “we want to create a protein source from the basic elements.” The company’s scientists screened sixteen different koji strains before they found one that grew well in this simple nutrient broth. “We wanted koji that we could brew well, using minimal feedstock to make it cost-competitive.” he continues.\nIn home tests so far, the company’s early products have received rave reviews—but as dog lovers know, our canine companions do not have the most discriminating palates. Dogs will eat anything, right?\nWell, sure, they will, but it doesn’t mean they should. Much of today’s pet food is simply not what you’d want to feed your best friend: pet food manufacturers use parts of agricultural animals not considered fit for human consumption. This is less true of premium brands, but to the extent that those are made of healthier and more palatable ingredients, they are just as resource-intensive and potentially environmentally damaging as the meat we humans eat.\n“In the past, we thought of pet food as a waste product,” Bethencourt explains. “But when I looked into the mid-tier to premium pet food, where meat is being purpose-grown for consumption by dogs, I learned that one-third of the meat we consume goes to pets, and at least half is not waste product.”\nOnto this bleak landscape steps Wild Earth, with a solution that is both healthier than standard chow (in that it is well-defined, free of nasty bits, and incidentally vegan) and less environmentally intensive than premium haute cuisine pour chiens. Simply put, Bethencourt says, “Our goal is to use science to make the best pet food on the planet.” They plan to launch commercial treats in October of this year, and a dog food early in 2019.\nOnce commercial production is underway, what’s next? Bethencourt is already thinking beyond the nearly hundred million dog dishes in the US that could someday be filled with Wild Earth chow. “All animals need nutrition. Once we’ve shown we can make high-quality clean protein for pets, what’s to stop us from making great food for humans?”\nFungal production of protein from well-defined and easily standardized feedstocks could help address supply chain issues in emerging markets, as well as food security and fraud issues in developed nations. “Ultimately,” he predicts, “we could feed billions of people and billions of animals.”\nOne might be reminded of the science fiction series “The Expanse,” in which the self-reliant Belters of the outer solar system eat a fungally synthesized kibble.\n“Yes!” laughs Bethencourt in response, who is also a fan of the show. “We would love to be the people who supply humans with proteins as we leave Earth and start living in space.”\nSpace is as good a place as any to end our discussion of synthetic biology in food. With tools and techniques that are capable of improving human and animal health, protecting our environment, and changing how we eat and drink all around the world, it’s clear that not even the sky’s the limit.\nCurious to learn the about the fundamentals of synthetic biology, state-of-the-art applications, investments and methods to engineer biological systems? Join this Specialization Track with John Cumbers at the TFF Academy this July.']	['<urn:uuid:71bbfdb4-7bbc-49ad-9105-ca7a65e7f56e>', '<urn:uuid:6c14ad4e-95d2-4375-b4c5-332634af21ed>']	factoid	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-13T00:20:10.754925	9	35	2353
71	why build simulation models purpose	The primary objective of building simulation models is to better understand the controlling factors and processes in the system and use this information to predict the future behavior of the system under various sets of assumptions.	"['The ability to define what may happen in the future and to choose among alternatives lies at the heart of contemporary societies.Peter Bernstein\nUnit 14 Overview\nBack in Unit 2, we discussed why you would want to carry out a simulation. We noted that the primary objective of building simulation models was to better understand the controlling factors and processes in the system and use this information to predict (forecast) the future behavior of the system under various sets of assumptions.\nThe primary reason we would do so is so we can determine what we can do to influence the future behavior of the system. That is, our goal is often to use the simulation model to identify any necessary changes we can make to the system that will help make it perform the way that we want it to. Such changes range from changing the physical design of the system (e.g., increasing the number of scanning stations at a security checkpoint) to changing policies for how the system is run (e.g., changing the decision rules for opening and closing new stations).\nHence, simulation provides a mechanism by which alternative designs, plans, and/or policies can be evaluated without having to experiment on a real system, which may be prohibitively costly, time-consuming, or simply impractical to do. That is, it allows you to ask ""What if?"" questions about a “virtual” system without having to experiment on the actual system.\nTo accomplish this in GoldSim (or any simulation tool), what we want to do is: 1) build a simulation model; 2) run it multiple times using different inputs (representing different designs or policies); and then 3) compare the results of those different simulation runs to determine which set of inputs best meets our goals for the system.\nIn the last 13 Units we have learned how to build a model, change the inputs, run the model, and look at results. So conceptually, we have everything we need to carry this out and ask these “What if?” questions about the system we are modeling. Practically, however, doing so with the tools we have discussed can be difficult. Why? Because every set of inputs (and corresponding results) must be stored in a separate model file. This makes comparing different results difficult. Using what we have learned so far, in order to compare the results of different runs you would need to export the key results (e.g., to a spreadsheet) and compare them there. Moreover, since the different runs would be represented by different model files, there is no easy way to determine how the models are different (i.e., what are the differences in the inputs?).\nTo address this, GoldSim provides a powerful and flexible feature referred to as scenario modeling. GoldSim’s scenario modeling capability allows you to directly compare results generated by your model using different sets of input parameters. In effect, when you use this capability, your model can store (and subsequently compare) multiple sets of inputs and outputs.\nThis Unit discusses this important feature.\nIn particular, this Unit includes the following Lessons:\n• Introduction to scenario modeling;\n• Creating scenarios;\n• Understanding the Active Scenario;\n• Running scenarios;\n• Comparing scenario results;\n• Editing scenarios and adding new scenarios; and\n• Changing model logic, Time Series and other inputs between scenarios.\nThis Unit also includes one Exercise (in addition to several Examples that we will work through together). This Unit has a total of 10 Lessons (including this overview and a summary at the end).']"	['<urn:uuid:f61480d9-e149-4a8a-8243-e446e52b22c4>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-13T00:20:10.754925	5	36	583
72	united states freed slaves support programs after civil war	After the Civil War, the United States failed to provide adequate support for freed slaves. The approximately 4 million people freed from slavery were given no access to credit, decent education, or political participation. They faced prejudice, discrimination, and violence. Additionally, their family networks were broken up as family members were separated across different parts of the country, and their names had been changed, preventing them from building the social capital needed to succeed in American life.	"[""Skip to 0 minutes and 6 secondsHello. Today I'm with Todd Landman, Pro-Vice-Chancellor at the University of Nottingham, and a well-known social and political scientist himself who works on human rights around the world. Todd, we have a great raft of international human rights laws that would help countries, and to which countries have already subscribed, to build programs which they might use to eradicate slavery. But we have enormous differences country to country. What's going on there?\nSkip to 0 minutes and 34 secondsSo historically, we know there's been a proliferation of human rights laws, from the Universal Declaration of Human Rights all the way till today there's been an enactment, if you will, of many international instruments for the protection of human rights, within which there are lots of legal codes and frameworks and expectations of states who are members of those treaties to protect things like slavery, or to eradicate slavery. So freedom from servitude is a kind of language that we find in international law repeated over and over again. The problem is that countries then need to implement policies and have institutions in place that realize the objectives that are in those international treaties. And that's where the problem lies.\nSkip to 1 minute and 11 secondsBecause countries have different capacities, they're wealthy, they're poor, they're middle income countries, and they may have different policy preferences within their domestic politics which suggests that maybe slavery doesn't have the same importance as health care. So there are trade-offs that countries face on a daily basis about where to put oftentimes scarce resources to realize and protect the fundamental human rights of their citizens, and at the same time eradicate some of the worst forms of practices that we see around the world. So that variation, if you will, in the ability for countries to protect and eradicate slavery will be contingent on the resources they have, as well as the political will for doing something around it.\nSkip to 1 minute and 51 secondsSo when I think about countries around the world that have, let's say, a limitation on national resources. I understand you've done some work in Mauritania. So you went into Mauritania, you were working on a national plan to eradicate slavery, what happened? Well that's a very interesting story, because it perfectly illustrates what you were just saying about the juxtaposition of will, resources, and political frameworks. I was called upon by the Mauritanian government to come work with them to prepare a national plan for the eradication of slavery. Now, it's worth noting that, of course, Mauritania is one of the countries in the world which has the highest prevalence of slavery, and some of the most long-lasting slavery in the world.\nSkip to 2 minutes and 32 secondsIn other words, hereditary slavery has been operating there for we're not even sure how long, but at least 1,000 years. And now this first newly democratically elected government was asking for help. It was a wonderful moment, a very exciting moment to be alive, in a sense, if you're an abolitionist. And a lot of the world's larger democratic countries began to gather around and say, let us give you help to make this happen. Working through the World Bank, they organized a donor group, they organized a support group, they organized a group of constitutional experts and anti-slavery experts and people working on contingency plans about how you could transfer funds to support people coming out of slavery.\nSkip to 3 minutes and 17 secondsPart of the process was to educate the judiciary, so they would actually enforce those laws which had been in place for many years but never enforced. To train law enforcement to-- in fact put up billboards around the country to say, there's no more slavery in our country. Which is interesting, because while they had legally eradicated slavery four times, no one had bothered to tell the people in slavery. So slavery had continued for a very long time. We were going great guns along this path, and feeling pretty cocky and happy about it, when about nine months into the process there was a coup d'etat, that military took over.\nSkip to 3 minutes and 55 secondsI think too many of them were part of too many major controlling Hassaniya families who were deeply involved in and with hereditary forms of slavery. So the entire thing collapsed. My colleagues there disappeared, some of them we don't know what happened to them. So it's a pretty grim situation. And that was more than eight years ago, and we're now at a moment when it's just beginning to be talked about again in Mauritania. So it illustrates that way in which rights are out there, but you can't necessarily penetrate for political or cultural reasons.\nSkip to 4 minutes and 36 secondsSo that case of Mauritania is really interesting to me, because it's kind of an extreme case of a disruption, there's a military coup that actually disrupts any kind of progress that you might have made. But states that don't experience coups, let's say, on a regular basis, or have not experienced a coup also face a lot of challenges in dealing with the nature and extent of a problem like slavery.\nSkip to 4 minutes and 55 secondsAnd I was doing a project for a DFID, the Department for International Development here in the United Kingdom, and I went to Bangladesh to talk to local NGOs and government officials and foreign officials that had been working in that country on human rights, and how human rights could work into governance structures, local institutions, justice reform, all the sorts of things that you're talking about with respect to Mauritania. And one of the things I was interested in is how do you measure the nature and extent of a problem, a human rights problem.\nSkip to 5 minutes and 23 secondsAnd I was actually interviewing the head of the National statistical office in Bangladesh, and I said something about, what I'm really interested in is a way in which we can demonstrate the disproportional treatment of groups in society. And often when we look at issues of slavery, we're looking at the disproportionality of certain groups involved in slavery versus other groups. And I said, you run a national survey every year of 10,000 respondents in Bangladesh. Can you break that down by the ethnic groups that are in Bangladesh? He said, absolutely not. He said we have 45 different ethnic groups in this country.\nSkip to 5 minutes and 54 secondsWe would need a sample size so large for our statistical work that we could never actually tell you anything about the differences between some of those groups. So the actual cost of running a survey, the capacity of a government to run a survey that actually captures the true dimensions of the human population within a country, wouldn't even allow that country to even understand the full nature and extent of the problem before even doing something about it. So that is, again, another illustration of just the day-to-day work of governments, and how difficult it is to realize some of these idealistic objectives around human rights.\nSkip to 6 minutes and 26 secondsAnd yet, if the problem were not slavery, but something like a highly infectious form of diphtheria, you bet they'd go out there with a 40,000 sample and go after that because that could actually infect people who had power. In fact, people of the right ethnic groups and so forth. It's very interesting about that because people in slavery, of course, are people who don't get to vote, they don't get to have the power to have much influence on the people around them. It's interesting that you mention Bangladesh as well, because next door in India you actually see this almost schizophrenia of national plans or reactions to slavery that is often compared to what goes on in Bangladesh.\nSkip to 7 minutes and 9 secondsSo that-- and India is one of the few countries in the world which would operate what they would have called in the United States the 40 acres and a mule response. So there's actually, by law, for anyone who's certificated to have come out of situations of slavery, an immediate grant of funds, a secondary grant of funds, a special access to appropriated land, job training, a whole series, a whole raft of government provisions.\nSkip to 7 minutes and 37 secondsAnd yet, one of the things that we've discovered is that to access those grants, to have the opportunity to start a new life with a little bit of income, a little bit of stability, the chance to put your kids in school, is pretty much determined by who's walking alongside you from an NGO to make the legal representation before you can access that national plan for eradication. Otherwise, very often local officials simply sign your certificate, put it in their own pocket, take it around to their buddy in an office and collect your money and put in their pocket as well. So that raises a really interesting question for me around legal equality and social inequality.\nSkip to 8 minutes and 20 secondsSo legal equality, pristine legal equality can be granted through declaration, through law, through institutions, et cetera. But the actual realization of law butts up against social equality and social inequality. And one example I think from the United States that's also very telling, is after the Civil War and after the abolition of slavery the right to vote was being given to residents in the south of the United States. And the story goes that there was legal equality, people could vote. If your grandfather voted in an election, you could vote in an election. Well that immediately eliminated people who were slaves, because their grandfathers had not voted in elections. So, legally, if you took a test and qualified you could then vote.\nSkip to 8 minutes and 58 secondsBut the test was so difficult that even people with PhDs coming out of the Tuskegee University, for example, wouldn't be able to pass the test. So legally, on paper, it looked like everyone was equal and had access to the same set of rights. But we know socially that was not possible. Well and I have to say the United States is a classic negative example of how to do a plan for the eradication of slavery. Because at the end of the Civil War, there was no 40 acres and a mule for any freed slave family. And fundamentally the entire population, about 4 million people who had been in slavery were lifted up out of legal slavery but then dumped.\nSkip to 9 minutes and 39 secondsNo access to political participation, as you were saying, no access to credit, which might help them do some asset formation, no opportunity to really access decent education, socially set aside prejudice, discrimination, violence used to enforce all of that. And creating a situation in which that botched emancipation then echoed down through the generations. And I have to say, all the way into my life as going up in the deep South, you know, what I saw as a tiny child of segregation was just one of the last echoes of that. Well, I hope it was one of the last because it's certainly echoing today. It's echoing today.\nSkip to 10 minutes and 17 secondsAnd I think that one of the dimensions that you didn't mention was the breakup of family. So the social networks that non-slave communities were able to form as they came to America and settled and developed their family networks and communities et cetera, that wasn't available to freed slaves, because family members would be taken off to different parts of the country and they couldn't reconnect, their names had been changed, and there was no social capital to build the networks that one needs to succeed in American life.\nSkip to 10 minutes and 45 secondsSo that extra challenge as it were, the social inequality, the sort of being set aside, left behind et cetera, really did color the way in which things then happened leading up to the civil rights movement, and at least the promulgation of the Voting Rights Act and the 1960s. So if we're going to put together a national plan for the eradication of slavery for any country it has to have economic components, educational components, legal components, obviously, but even address pretty deep and textured questions of family relationships, and social relationships, and so forth. It's complex. And it's hard to find one that's anywhere near that in the world today.\nSkip to 11 minutes and 24 secondsSo far we've got a bag of tricks hardly ever assembled into a mechanism of liberation that's sound. So I think we'll probably just have to keep all of that in mind as we begin to move forward. And to look at ideas about how people come to freedom at the individual level. And listen to the narratives of freed slaves as we move forward in this course.\nA National Plan\nProfessor Todd Landman, Pro Vice Chancellor for the Social Sciences faculty at the University of Nottingham, is a leading expert on human rights, internationally renowned for his work on measurement and analysis.\nIn this film, he joins Kevin Bales to discuss ways in which to map, explain, and understand the variation in human rights abuse around the world. They discuss human rights contexts at the national level, and how human rights advocacy can be advanced through the tools of economics, political science, and international relations.\nTodd and Kevin also debate countries’ national contemporary antislavery plans. Today these include, for example, a slavery eradication plan for the government of Mauritania and the Brazilian National Plan for the Eradication of Slavery. The role of national governments in the eradication of slavery is crucial. But some nations will have the resources to eradicate slavery very quickly and an extreme shortage of political will. Poorer countries may have the best will in the world, but not enough money to take on the slaveholders. Government officials have yet to understand that slavery’s eradication is multi-dimensional. Slavery is a legal problem but also, to a greater or lesser extent in every country, a problem of economic development, migration, gender, prejudice and corruption.\nYou can also dip into a longer lecture by Todd, about the comparative politics of human rights, where he discusses the evolution of the international human rights regime, different kinds of human rights measures, and systematic ways in which to map, explain, and understand the variation in human rights abuse around the world.\nPlease then search online for particular countries’ responses to slavery. What has a country tried? Can you find announcements of its intentions to tackle a particular form of slavery? Or evidence that it has a national plan, a national hotline for reporting suspected slavery cases, a particular government official working on antislavery action, or any other mechanism for tackling slavery? Share any examples you find - whether of governments that seem to be taking particularly innovative action or those that aren’t engaging extensively with the issue.""]"	['<urn:uuid:4d3a8dda-730b-493e-b7d7-0c006fa9f8ee>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T00:20:10.754925	9	77	2558
73	italian renaissance museum restitution comparison leonardo da vinci mona lisa ownership history nazi looted madonna	The Madonna and Child Enthroned painting has a complex ownership history involving Nazi looting - it was stolen from Jacques Goudstikker by Hermann Göring during WWII, recovered by Allied forces in 1945, sold by the Dutch government, and finally returned to Goudstikker's heir after the Virginia Museum of Fine Arts' decision. In contrast, the Mona Lisa, painted by Da Vinci starting in 1503, was a privately commissioned work that has not faced ownership disputes of this nature.	['Madonna and Child Enthroned With Saints Nicholas of Tolentino and Sebastian” dates to the mid-1510s.\nIt may have taken 13 years, but another painting looted by the Nazis during World War II will be given back to the family of Jewish Dutch art dealer Jacques Goudstikker.\nThe board of trustees of the Virginia Museum of Fine Arts voted last week to return “Madonna and Child Enthroned With Saints Nicholas of Tolentino and Sebastian” to Marei von Saher, Goudstikker’s sole heir. Goudstikker had a vast collection, some of which was looted by Adolf Hitler’s deputy Hermann Göring.\nThe painting, dating to the mid-1510s, was recovered by Allied forces in 1945 and turned over to the Dutch, with the expectation that it would be returned to the Goudstikker family. Instead, the Dutch government sold it to an unknown buyer. The Richmond museum bought it from a New York gallery in 1958.\nVon Saher, who lives in New York, has resolved claims for more than 50 works of art from at least seven countries and more than a dozen museums in the United States and abroad, according to her attorney, Frank Lord. She first contacted the VMFA in 2005, when her claim against the Dutch government was still pending.\n“It took some time to resolve the claim, but the process was amicable and relations between the parties have been cordial,” Lord said in an email.\nVMFA director Alex Nyerges said: “For us it’s not even a choice — it’s an obligation. A work of art belongs to its owner, and any work that is illegally obtained, in this case stolen by the Nazis, belongs to the original owners.”\nNyerges said the process was complicated and lengthy because it involved trying to verify the history of the work. “We set out to do our research, and we determined it was very likely a Goudstikker painting,” he said. “We asked for their research to show us that this painting was not returned to the heirs when it should have been.”\nThe reclamation of Nazi-looted art is a challenge for museums around the world and one that has defied a global resolution, according to Jonathan Petropoulos, professor of history at Claremont McKenna College in California and an expert on art looted during World War II. Each claim has its own set of circumstances, and different governments have their own laws. A set of guidelines, known as the Washington Conference Principles, was created in 1998.\n“The VMFA did the right thing and acted in a manner consistent with the Washington Conference Principles,” Petropoulos said. “The [museum] merits praise for the decision.”\nGoudstikker escaped the Netherlands in 1940 with a logbook that detailed the works he had left behind. The document included an inventory number that was found on the back of the painting, Nyerges said.\nThe painting was thought to be the work of Lorenzo Costa but is now attributed to the Master of the Risen Magdalen. It depicts the Virgin Mary seated on a throne, holding the Christ child and flanked by Saints Nicholas of Tolentino and Sebastian. It has not been on view recently at the Richmond museum.\nThe painting is the third Nazi-looted work the VMFA has returned. The others were a 16th-century French painting returned to an Austrian family in 2004 and a Flemish portrait to a Polish family in 2005.\nThe VMFA also has been on the receiving end of a claim. In 2016, the Museum of Modern Art in New York determined that a major painting by German expressionist artist Ernst Ludwig Kirchner belonged to the same family that had given 200 works to the Richmond museum in a 2009 gift and purchase agreement.\nVon Saher continues to seek the return of other works, including two pieces at the Norton Simon Museum in Pasadena, Calif. An appeals court recently dismissed her claim, but according to her attorney, she is planning to file a new petition.\nVon Saher thanked the VMFA board for agreeing to return the work, saying in a statement, “It is always gratifying when an institution like the VMFA recognizes the importance of returning Nazi-looted artworks to their rightful owner.”', 'Leonardo Da Vinci is arguably history’s greatest multi-tasker. Possessor of a curious mind and keen intellect, Da Vinci studied the laws of science and nature, which greatly informed his work as a painter, sculptor, architect, inventor, military engineer and draftsman.\nYoung Leonardo received little formal education beyond basic reading, writing and mathematics instruction, but his artistic talents were evident from an early age. Around the age of 14, da Vinci began a lengthy apprenticeship with the noted artist Andrea del Verrocchio in Florence. He learned a wide breadth of technical skills including metalworking, leather arts, carpentry, drawing, painting and sculpting. It is thought that Verrocchio was so humbled by the superior talent of his pupil that he never picked up a paintbrush again.\nIn 1482, Florentine ruler Lorenzo de’ Medici commissioned Da Vinci to create a silver lyre and bring it as a peace gesture to Ludovico Sforza, who ruled Milan as its regent. After doing so, Da Vinci lobbied Ludovico for a job and sent the future Duke of Milan a letter that barely mentioned his considerable talents as an artist and instead touted his more marketable skills as a military engineer. Using his inventive mind, Da Vinci sketched war machines such as a war chariot with scythe blades mounted on the sides, an armored tank propelled by two men cranking a shaft and even an enormous crossbow that required a small army of men to operate. The letter worked, and Ludovico brought Da Vinci to Milan for a tenure that would last 17 years.\nDa Vinci began to seriously study anatomy and dissect human and animal bodies during the 1480s. His drawings of a fetus in utero, the heart and vascular system, sex organs and other bone and muscular structures are some of the first on human record. In addition to his anatomical investigations, Da Vinci studied botany, geology, zoology, hydraulics, aeronautics and physics. He filled dozens of notebooks with finely drawn illustrations and scientific observations. A man ahead of his time, da Vinci appeared to prophesize the future with his sketches of machines resembling a bicycle, helicopter and a flying machine based on the physiology of a bat.\nAround 1495, Ludovico commissioned Da Vinci to paint “The Last Supper” on the back wall of the dining hall inside the monastery of Milan’s Santa Maria delle Grazie. The masterpiece, which took approximately three years to complete, captures the drama of the moment when Jesus informs the Twelve Apostles gathered for Passover dinner that one of them would soon betray him.\nAfter brief stays in Mantua and Venice, Da Vinci returned to Florence. In 1502 and 1503, he briefly worked as a military engineer for Cesare Borgia, the illegitimate son of Pope Alexander VI and commander of the papal army. He traveled outside of Florence to survey military construction projects and sketch city plans and topographical maps. He designed plans to divert the Arno River away from rival Pisa in order to deny its wartime enemy access to the sea.\nDa Vinci started working in 1503 on what would become his most well known painting—and arguably the most famous painting in the world—the “Mona Lisa.” The privately commissioned work is characterized by the enigmatic smile of the woman in the half-portrait.\nDa Vinci moved to Rome in 1513. Giuliano de’ Medici, brother of newly installed Pope Leo X and son of his former patron, gave Da Vinci a monthly stipend along with a suite of rooms at his residence inside the Vatican. His new patron, however, also gave Da Vinci little work. Lacking large commissions, he devoted most of his time in Rome to mathematical studies and scientific exploration.\nAfter being present at a 1515 meeting between France’s King Francis I and Pope Leo X in Bologna, the new French monarch offered Da Vinci the title “Premier Painter and Engineer and Architect to the King.” Da Vinci did little painting during his time in France. One of his last commissioned works was a mechanical lion that could walk and open its chest to reveal a bouquet of lilies. He continued work on his scientific studies until his death at the age of 67 on May 2, 1519.']	['<urn:uuid:6ad77d4e-6bdb-40bf-b23a-330d1529ee4a>', '<urn:uuid:bed75671-9195-4387-9605-1c3b44c33308>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	expert	2025-05-13T00:20:10.754925	15	77	1389
74	What new perspectives on Darwin's early medical education in Edinburgh have emerged in recent historical research?	Recent historical research has reinterpreted Darwin's period as a medical student in Edinburgh, which was previously dismissed as merely a Victorian gap year. The period is now seen as critical because it brought Darwin's deeply-held anti-slavery convictions into direct conflict with the scientific racism that was emerging from Edinburgh's medical school at the time. This was particularly significant because Edinburgh was one of Europe's intellectual capitals for studying the origins and classification of humans.	"['Evolving views on Darwin\nCharles Darwin, founder of modern evolutionary theory, still\nexcites much literary interest 200 years after his birth.\nAn array of books greets the 200th anniversary of the man who put the idea of human evolution to the world. Alison Bashford considers a selection.\nCHARLES Robert Darwin is immortal. The more that new-wave creationists condemn his theory of evolution, the more oxygen the 200-year-old is given, and the more life is breathed into his On the Origin of Species by Means of Natural Selection, now 150 years old. For years, publishers have commissioned and authors have written Darwin books, from brilliant biographies to garden-variety expositions of evolution. I\'m not alone in needing to re-organise the bookshelves periodically: move along for more Darwin.\nAnd then 2009 rolls around, the bicentenary of his birth, and the sesquicentenary of Origin. It was always going to be a challenge for authors and publishers. But of course there are new ways to see Darwin. Our understanding of the past evolves with time, just like everything else on the planet.\nAn astonishing interpretation has emerged from the long-standing Darwin duo, biographers Adrian Desmond and James Moore, authors of the bestseller Darwin (1991). Their new book, Darwin\'s Sacred Cause, examines the young scientist and his strange, faltering views on evolution through the vivid context of anti-slavery. This was, unmistakably, the greatest social, political, and moral issue of Darwin\'s early years, of his milieu, and of his extended family.\nEvery biography of Darwin notes this, but Desmond and Moore see in it a new explanation for his theories. This is a book as brilliantly simple in concept as it is persuasive and substantiated in execution.\nBoth grandfathers Erasmus Darwin and Josiah Wedgwood were central to the abolitionist cause. This was not just a modish campaign, but a driving religious and political revulsion against the practice, the commerce, and the principle of slavery.\nJosiah, the ceramic magnate, was responsible for the mass production of the famous anti-slavery seal that became the brand of the movement, the chained slave on bended knee pleading: ""Am I not a Man, and a Brother?"" For these men, as for so many others, the aversion to slavery was political, visceral and spiritual in equal measure.\nFor Darwin, the authors argue, it was also intellectual. The inherited non-conformist Quaker and Unitarian message of human ""brotherhood"", of universality, of links not divisions between white and black people, predisposed Darwin to see connections, and to think quite literally about ancestors in common. Darwin eventually extended a version of this fundamental idea to all life, in his theory of evolution.\nHistorians typically put Darwin\'s ideas about humans last in his life story (Descent of Man was not published until 1871). But Desmond and Moore\'s completely new explanation places ideas about humans first. Darwin\'s anti-slavery conviction was not just background to the real story, it formed the real story. Anti-slavery and the great 19th-century race question are not contextual, then, but explanatory.\nAnd suddenly everything in the well-known life looks different. Darwin\'s period as a medical student in Edinburgh, for example, is usually written off as a Victorian gap year fun, but not that useful. Desmond and Moore see it as critical in bringing Darwin\'s deeply-held anti-slavery convictions into uncomfortable contact with the scientific racism just then emerging from Edinburgh\'s medical school: this was, after all, one of Europe\'s intellectual capitals for the study of the origins and classification of ""man"".\nFor seasoned Darwin followers this is sparkling new scholarship. For the unseasoned, this is history and biography at its most crafted.\nNew Yorker essayist Adam Gopnik will devour the Desmond and Moore book: it is the missing scholarly link between anti-slaver Darwin and the great emancipationist, Abraham Lincoln, who mounted a bloody civil war over the question of race and slavery. In his book on the pair, born the same day, Gopnik asks: what do Darwin and Lincoln say about, and to, the modern world?\nInitially, the birthday coincidence seemed a pinched premise for a joint biography. I was wary. And then, suddenly and early, Gopnik clinches it: 50 years ago, he writes, Marx and Freud would have been selected as key to the modern world. Right now, it\'s Darwin and Lincoln. He\'s quite correct ""liberal civilisation"" and free inquiry are among the most radical things around. ""Science and democracy still look like the hope of the world.""\nFor Gopnik, Lincoln and Darwin represent different shades of ""freedom"". Notwithstanding the US-tinged fascination with that particular concept, he had me on board. And except for the sections where he parallels the deaths of the most-loved child this threatened a minor mutiny I sailed through Angels and Ages, a long essay on modernity and its founding ideas.\nDarwin scholars always make much of his islands. And now the most famous island in the world (at least for the Victorians) is vying for top Darwinian place, next to the Galapagos: Britain. In a new look at the older Darwin\'s many years holed up in Down House, Kent, science writer Steve Jones suggests that England itself was his most important insular inspiration. The Kent-based years the rocks, insects, pigeons and worms of that place, that field need to figure as largely for us as they did for the bearded man.\nJones\' book is not quite what the jacket leads us to expect, however. He writes less about Charles Darwin, and more as Charles Darwin. It\'s a quirky book, a readable update on the ecology and biology of all the strange phenomena that engaged Darwin: orang-utans, insect-eating plants, consanguineous marriage, breeding dogs. I think this is what Jones means by ""bringing us up to the present"", or writing a natural history for the ""modern age"". In part this comes to us via Darwin\'s own work, and as promised, via his lesser-known English books (on earthworms, for example). But I wanted much more Darwin-in-Kent and less Jones-as-Darwin for the third millennium.\nJones is at his best describing the denuded state of English biodiversity since Darwin\'s time. Other authors, too, have chosen to view Darwin locally. Many readers have met the ""Australian"" Darwin before, through the person of Syms Covington, his servant and assistant, fictionalised in the stunning Mr Darwin\'s Shooter, republished for 2009. Also republished is F. W. and J. M. Nicholas\' Darwin in Australia, with an elegant new foreword by James Moore. This already appreciated book presents Darwin\'s diary comprehensively, and letters selectively, from his Australian ports of call in 1836: Hobart, Sydney, King George Sound.\nTom Frame\'s Evolution in the Antipodes takes a longer view, assessing not just Darwin in Australia then, but Darwinism in Australia ever since. Antipodean readers, he argues, came around to the idea of natural selection much more readily than British counterparts. This well-researched book will be of particular interest for those seeking a local history of the religious implications of Darwin\'s theory of evolution.\nAustralian writer Peter Macinnis takes a different tack and places Origin in its temporal context, tracing the science and technology of 1859 in Mr Darwin\'s Incredible Shrinking World. Australian geographer Patrick Armstrong presents another Darwin life story, focusing on the place of chance, luck, and fortune within it: a premise too attenuated to really work.\nHistorian Iain McCalman casts his local net much wider, writing not just about Darwin in Australia, but in the large and rich southern oceans. He draws us into Darwin\'s close relationships with fellow voyagers Thomas Huxley, Joseph Hooker and Alfred Russel Wallace. These four became friends and colleagues, some feisty and ambitious, some troubled and humble. Each travelled in the far southern regions as young men, as curious adventurers, and as the most important natural historians of their day.\nThis is splendid history; simultaneously intricate portraiture and sublime landscape.\nHooker was Darwin\'s great scientific and personal confidant, for years listening to his puzzling on natural selection. Huxley the man with the greatest link to Australia needed to be persuaded of this curious notion of the transmutation of species by natural selection, but then famously became the great public warrior for it.\nAnd the enigmatic Wallace, the outsider, the lowest placed of the four in all-important Victorian class terms, didn\'t need to be persuaded of ""Darwin\'s"" theory. He came up with the same idea himself, working painstakingly and imaginatively, isolated in the Malay archipelago. The simultaneous presentation of Wallace and Darwin\'s theories to the Linnaean Society in 1858 is the most famous story in the history of science. Hooker and Huxley were the strategists behind the device, which saved Darwin from being unwittingly trumped by Wallace.\nMcCalman\'s book is a completely fresh, personal, scientific and colonial backstory to that famous event. And it is a ""group biography"" with a structural simplicity of the highest order of elegance.\nSo much history of science is about clever people thinking things as if they were disembodied and nowhere. McCalman instead transports us to the places where these men thought: inspecting that platypus next to the creek at Warawalang; canoeing up the Sarawak River; a silent and solitary night on deck in the Coral Sea.\nWe feel the discomfort as young Huxley lashes his precious microscope to the table on the lolling Rattlesnake, sharing space with all those adolescent midshipmen, and thousands of cockroaches, his feet awash, and his heart anguished with lovesickness for clever Henrietta, his key Sydney discovery.\nDarwin\'s Armada places these four natural historians in their ""field"" the southern oceans and its magnificent islands. McCalman\'s case is that the significance of the southern ocean voyages have been overlooked in international Darwin scholarship. These islands need an intellectual, if not a geographical, place alongside Galapagos. The platypus really was as intriguing for Darwin as the famous tortoise.\nAlison Bashford is professor of modern history at the University of Sydney. In August she will join Harvard University\'s department of the history of science as chair of Australian studies.\nsend photos, videos & tip-offs to 0406 THE AGE (0406 843 243), or us.']"	['<urn:uuid:a89659af-652a-48f0-a1e8-8938f4eb2480>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T00:20:10.754925	16	74	1663
75	What's involved in concrete testing and foundation leveling?	For testing, methods include visual examination, laboratory sampling, non-destructive testing, chloride/sulphate content checks, and strength tests using rebound Schmidt hammer and ultrasonic measurements. For foundation leveling, you can either build up the low side with gravel fill or dig the high side into the slope and add a retaining wall. The ground slope should be assessed using a line level and string or builder's level.	"['Concrete Contractor Dallas - An Overview\nConcrete types and pouring a concrete piece foundation can be frightening. Your heart races because you know that any error, even a little one, can rapidly turn your slab into a huge mess, a mistake literally cast in stone.\nIn this short article, we\'ll stroll you through the slab-pouring procedure so you get it right the very first time. We\'ll pay specific focus on the hard parts where you\'re most likely to goof, like the best ways to make concrete.\nStill, putting a big concrete slab foundation isn\'t a job for a novice. If you haven\'t worked with concrete, begin with a small walkway or garden shed flooring prior to trying a garage-size piece foundation like this. Even if you\'ve got a couple of little jobs under your belt, it\'s a smart idea to find an experienced assistant. In addition to basic woodworking tools, you\'ll need a number of unique tools to complete large concrete types or a piece (see the Tool List below).\nThe bulk of the work for a brand-new piece remains in the excavation and form structure. If you have to level a sloped website or bring in a lot of fill, hire an excavator for a day to assist prepare the website Then figure on investing a day building the kinds and another putting the piece\nIn our area, hiring a concrete contractor to pour a 16 x 20-ft. piece like this one would cost $3,000 to $4,000. The quantity of cash you\'ll save money on a concrete slab expense by doing the work yourself depends primarily on whether you have to employ an excavator. Most of the times, you\'ll conserve 30 to HALF on concrete piece expense by doing your own work.\nAction 1: Prepare the site for the concrete slab in Dallas Texas\nDrive 4 stakes to roughly show the corners of the new piece. With the approximate size and place significant, use a line level and string or home builder\'s level to see how much the ground slopes. You can construct up the low side as we did, or dig the high side into the slope and add a low maintaining wall to hold back the soil.\nYour concrete piece will last longer, with less cracking and motion, if it\'s built on solid, well-drained soil. If you have sandy soil, you remain in luck. Just scrape off the sod and topsoil and include gravel fill if required. If you have clay or loam soil, you must remove enough to permit a 6- to 8-in. layer of compacted gravel under the new concrete.\nIf you need to remove more than a few inches of dirt, think about leasing a skid loader or working with an excavator. An excavator can also assist you get rid of excess soil.\nKeep in mind: Before you do any digging, call 811 or visit call811.com to set up to have your local energies find and mark buried pipelines and wires.\nAction 2: Construct strong, level kinds for a perfect slab around Dallas\nStart by picking straight type boards. Cut the two side kind boards 3 in. You\'ll nail the end boards in between the side boards to develop the appropriate size form.\nShow how to develop the forms. Procedure from the lot line to place the first side and level it at the wanted height. For speed and accuracy, utilize a home builder\'s level, a transit or a laser level to set the height of the types.\nBrace the kinds to ensure straight sides Freshly poured concrete can push type boards external, leaving your piece with a curved edge that\'s nearly impossible to fix. Place 2 × 4 stakes and 2 × 4 kickers every 2 ft. along the type boards for assistance.\nStretch a strong string (mason\'s line) along the top edge of the form board. As you set the braces, make sure the form board lines up with the string. Change the braces to keep the form board directly.\nShows measuring diagonally to set the 2nd type board perfectly square with the. Utilize the 3-4-5 technique. Procedure and mark a multiple of 3 ft. on one side. (In our case, this is 15 ft.) Then mark a multiple of 4 ft. on the adjacent side (20 ft. for our piece). Remember to determine from the exact same point where the 2 sides satisfy. Change the position of the unbraced form board till the diagonal measurement is a multiple of 5 (25 ft. in this case).\nSquaring the second type board is most convenient if you prop it level on a stack of 2x4s and slide it back and forth till the diagonal measurement is correct. Then drive a stake behind completion of the form board and nail through the stake into the form. Total the second side by leveling and bracing the form board.\nSet the third type board parallel to the first one. Leave the fourth side off until you\'ve hauled in and tamped the fill.\nSuggestion: Leveling the types is much easier if you leave one end of the kind board slightly high when you nail it to the stake. Change the height by tapping the stake on the high end with a maul until the board is completely level.\nAction 3: Develop the base and pack it.\nConcrete requirements reinforcement for additional strength and crack resistance. It\'s well worth the little additional cost and labor to set up 1/2-in. rebar (steel enhancing bar). You\'ll discover rebar in your home centers and at suppliers of concrete and masonry items (in 20-ft. lengths). You\'ll likewise need a package of tie wires and a tie-wire twisting tool to connect the rebar.\nCut and bend pieces of rebar to form the perimeter strengthening. Wire the border rebar to rebar stakes for support. You\'ll pull the grid up into the center of the concrete as you pour the slab.\nIf you\'ve never ever put a big slab or if the weather condition is hot and dry, makings concrete harden rapidly, divide this piece down the middle and fill the halves on various days to reduce the amount of concrete you\'ll need to end up at one time. Get rid of the divider before putting the 2nd half.\nMark the position of the door openings on the concrete kinds. Mark the place of the anchor bolts on the types. Place marks for anchor bolts 6 in. from each side of doors, 12 in. from corners and 6 ft. apart around the perimeter.\nStep 5: In Dallas Fort Worth Get ready for the concrete truck\nPouring concrete is fast-paced work. To minimize tension and avoid errors, ensure everything is ready prior to the truck arrives.\nTriple-check your concrete types to make sure they\'re square, level, straight and well braced. Have at least two contractor-grade wheelbarrows on hand and three or four strong assistants. Plan the path the truck will take. For large slabs, it\'s finest if the truck can back up to the concrete kinds. great post to read Avoid hot, windy days if possible. This kind of weather condition accelerates the solidifying procedure-- a slab can turn tough before you have time to trowel a nice smooth finish. If the projection calls for rain, reschedule the concrete shipment to a dry day. Rain will destroy the surface.\nTo figure the volume of concrete needed, increase the length by the width by the depth (in feet) to show up at the number of cubic feet. Divide the total by 27 and include 5 percent to compute the number of backyards of concrete you\'ll require. The air entrainment traps microscopic bubbles that help concrete hold up against freezing temperatures.\nAction 6: Pour and flatten the concrete to form a perfect concrete slab\nBe prepared to hustle when the truck gets here. Start by putting concrete in the concrete kinds farthest from the truck. Use wheelbarrows where required.\nConcrete is too heavy to shovel or push more than a couple of feet. Location the concrete close to its final spot and roughly level it with a rake. As quickly as the concrete is placed in the concrete forms, begin striking it off even with the top of the form boards with a straight, smooth 2 × 4 screed board.\nThe technique to simple screeding is to have a helper with a rake moving the concrete in front of the screed board. You want enough concrete to fill all voids, but not so much that it\'s hard to pull the board. About 1/2 to 1 in. Deep in front of the screed board is about. It\'s much better to make numerous passes with the screed board, moving a little concrete each time, than to aim to pull a great deal of concrete at once.\nStart bull-floating the concrete as soon as possible after screeding. Keep the prominent edge of the float just a little above the surface by raising or decreasing the float deal with. If the float angle is too high, you\'ll plow the damp concrete and produce low areas.\nStep 7: Drift and trowel for a smooth finish in Dallas\nAfter you smooth the piece with the bull float, water will ""bleed"" out of the concrete and sit on the surface area. When the slab is firm enough to resist an imprint from your thumb, begin hand-floating.\nYou can edge the piece prior to it gets company because you don\'t have to kneel on the slab. If the lawn edger sinks in and leaves a track that\'s more than 1/8 in. deep, wait on the piece to solidify somewhat prior to continuing.\nYou\'ll have to wait up until the concrete can support your weight to start grooving the slab. The kneeling board disperses your weight, allowing you to get an earlier start.\nGrooving produces a weakened spot in the concrete that allows the inevitable shrinkage breaking to occur at the groove rather than at some random area. Cut grooves about every 10 ft. in big slabs.\nWhen you\'re done grooving, smooth the concrete with a magnesium float. You may have to bear down on the float if the concrete is starting to solidify.\nFor a smoother, denser finish, follow the magnesium float with a steel trowel. Shoveling is one Get More Info of the more difficult actions in concrete completing. You\'ll need to practice to develop a feel for it. For an actually smooth finish, repeat the troweling action 2 or three times, letting the concrete harden a bit in between each pass. At first, hold the trowel practically flat, raising the leading edge simply enough to avoid gouging the surface. On each successive pass, lift the cutting edge of the trowel a little more. If you want a rougher, nonslip surface, you can skip the steel trowel completely. Instead, drag a push broom over the surface to produce a ""broom surface.""\nKeep concrete damp after it\'s put so it treatments slowly and develops optimal strength. The simplest method to make sure appropriate curing is to spray the finished concrete with treating compound. You can this content lay plastic over the concrete instead, although this can lead to discoloration of the surface.\nLet the finished slab harden over night prior to you carefully get rid of the type boards. Pull the duplex nails from the corners and kickers and pry up on the stakes with a shovel to loosen up and eliminate the types. Because the concrete surface area will be soft and simple to chip or scratch, wait for a day or 2 prior to building on the piece.', 'Prior to commencing repair of concrete structures, it is always advisable to investigate the possible causes of this degradation. Nothing will be gained by carrying out a repair in haste.\nCauses of deterioration may be divided into recurring and non-recurring:\n- If the recurrence of deterioration is acceptably low, then it is normally acceptable to restore the structure as nearly as possible to its original state.\n- If there is an unacceptable risk of recurrence, the structure should be repaired and the fundamental cause of deterioration should be eliminated or controlled to acceptable limits.\nOnce the cause of deterioration is known, a decision of the extent of repair is required. This includes the parameters of durability, strength, function, and appearance of the structure after the repair process is completed.\nAfter the above decision is made, the choice of repair material and repair technique could be investigated.\nCauses of the concrete deterioration of a concrete structure:\n- Structural Defects: Design or construction errors resulting in stress cracking or foundation movement.\n- Corrosion of Steel Reinforcement: Concrete deficiencies in cover, compaction, presence of chloride, rusting resulting in spalling (i.e. concrete cancer) or cracking of overlying concrete.\n- Chemical attack: Etching or softening of the surface may be caused by acid attack or concrete may crack or spall due to sulphate attack.\n- Fire damage: A fire can cause spalling and cracking.\n- Substances in Concrete Aggregates: Alkali aggregate or sulphate reaction causing concrete expansion with cracking and spalling. This occurs slowly over many years.\n- Thermal Contraction or Expansion: Concrete contracts and expands with variations of temperature and when the contraction or expansion is restrained cracking or spalling may occur.\n- Shrinkage During Curing: As concrete sets shrinkage occurs and if this is restrained cracking may occur soon after construction.\n- Structural Creep: Inadequate design may fail to allow for long term creep due to the shortening of columns or deflection of suspended floor slabs and beams causing cracking or damage.\n- Plastic Shrinkage: Evaporation of the water content of concrete during the curing process may cause fine plastic cracks if not fully protected by curing compounds.\n- Plastic Settlement: New concrete can hang on formwork or reinforcement causing cracks in columns, beams or walls.\n- Construction Faults: Lack of cover over steel reinforcement, honeycombing in concrete, voids in concrete, blow-holes in concrete surfaces.\nIdentification of the causes of the deterioration of the concrete structure, extent of damage and recommended remediation methods:\n- Visual inspection\n- Review of construction documentation such as specifications, drawings, geotechnical reports, quality control on concrete batching and concrete mix designs.\nDepending on the extent of deterioration testing may include any combination of the following:\n- Visual examination\n- Laboratory testing of samples\n- Non-destructive testing\n- Chloride content\n- Sulphate content\n- Concrete strength\n- Moisture content\n- Compression tests\n- Cover over steel\n- Reinforcement deterioration\n- Nature of cracking\n- Aggregate quality\n- Concrete mix design\n- Rate of deterioration\n- Rebound Schmidt hammer tests\n- Ultrasonic measurements\n- Pull-off tests\n- Break-off tests\n- Core tests\nGenerally experienced concrete repair technicians will identify the possible causes and the recommended solutions after a visual examination of the degraded areas.\nThis depends on the extent of damage and the feasibility of repair.\n- Extent of corrosion of reinforcement\n- Extent of loss of strength of reinforcement\n- Extend of loss of the bond between the reinforcement and the concrete\n- Extent of deficiency in concrete cover over reinforcement\n- Extent of deflection due to cracking in the tensioned areas.\n- Extent of honeycombing in concrete\n- Extent of porosity of concrete\n- Extent of damage and loss of strength due to sulphate attack.\n- Extent and width of cracking\nThis will determine to nature and type of repairs required or whether repairs are feasible.\nIf repairs are not feasible then complete replacement of the damaged structural element, such as, beams, columns, slabs, stairs etc may be unavoidable.\nTHE MORAL TO THE STORY IS THAT IMMEDIATE ACTION MUST BE TAKEN WHEN THE PROPERTY OWNER BECOMES AWARE OF THE EARLY SYMPTOMS OF CONCRETE DEGREDATION OR DETERIORATION IN ORDER TO AVOID A MASSIVE ESCALLATION IN THE COST OF REMEDIATION OR , IN THE EXTREME, THE NEED FOR COMPLETE REINSTATEMENT OF THE STRUCTURAL ELEMENTS.']"	['<urn:uuid:8e1f92df-be0d-47a9-b442-537efa187e6b>', '<urn:uuid:29fb4f3a-ad13-420f-9028-dd4c352377aa>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T00:20:10.754925	8	65	2648
76	What are timing chain symptoms and microplastics effects on organisms?	A failing timing chain typically produces a rattling noise during engine start or idling, alongside metal shavings in engine oil. The chain is crucial for valve timing and if damaged, causes rough idling and unbalanced power output. For microplastics, they affect organisms by impacting mortality, reproduction, metabolism, and gene expression. They're particularly harmful as they're mistaken for food by marine life, affecting photosynthesis in phytoplankton and containing harmful additives like phthalates and heavy metals that can be released into organisms.	['Table of Contents Hide\nIf you are experiencing an unusual sound when starting your vehicle, the timing chain is most likely the cause. A subdued, slapping noise, often punctuated by a soft click, is generally superimposed over a general rattle. The sound tends to be hollow, not loud, and is usually not indicative of a worn chain. In fact, it’s often a combination of worn sprockets and guides as well. Because a worn chain does not necessarily mean imminent failure, hands-on inspection is necessary.\nSymptoms of a bad timing chain\nA rattling noise coming from the engine when the engine is started or brakes are applied may indicate that the timing chain is failing. A timing chain is responsible for the timing of the opening and closing of each engine valve. When it breaks, the engine cannot get an adequate balance of air and fuel, causing an unbalanced power output. Other signs of a damaged timing chain include metal shavings in the engine oil, which are also indications of a bad timing chain.\nIf you notice a rattling noise coming from the engine while idling your car, you need to diagnose the cause of the problem. Rough idling may be the result of other problems, but if it doesn’t get better over time, it might be a timing chain problem. Another sign of a bad timing chain is engine rattle, which can happen when the engine is idling or not. If you notice any of these symptoms, you should take your car in for a quick inspection.\nThe timing chain is a vital part of your car’s engine. Failure can result in severe damage to your engine. A professional mechanic can replace your timing chain if you notice any of these warning signs. Timing chain problems are not uncommon, but preventive maintenance will keep your vehicle running at peak efficiency. You can save thousands of dollars on repairs by replacing the timing chain before it becomes a problem.\nCauses of a bad timing chain\nWhen your car makes a rattling noise, it may be the timing chain. This problem typically develops gradually and can be very subtle. But if it’s persistent, it could mean that your timing chain has failed. Listed below are some causes of this problem, and how to fix them. In the event that you’re experiencing this problem, contact a qualified service technician as soon as possible.\nA rattling noise: If your car makes a rattling noise while idling, it is most likely the timing chain is damaged or lax. A rattling noise will start and stop intermittently, but it is an indication that the timing chain is bad. The noise could be coming from the crankshaft, clutch, or transmission, or it could also be from the brakes.\nA misaligned or broken timing chain can cause an engine to overheat, or even fail. In a high mileage vehicle, you need to have it replaced between eighty thousand and one hundred thousand miles. A broken timing chain can result in expensive engine repair bills. Checking the timing chain is one of the most important steps you can take to avoid this problem. A broken timing chain is the most common cause of engine failure.\nWhen to replace a timing chain\nOne of the first things you should do if you notice your car making a loud noise is to check the timing chain. Generally, timing chain problems occur in vehicles that have 80,000 to 120,000 miles. If the noise persists or your engine is running poorly, you should consider replacing the entire timing chain. Many vehicles with high mileage will experience timing chain issues. For this reason, it is important to know the symptoms of timing chain failure so you can determine whether it is time to replace your car’s timing chain.\nThe first sign of a bad timing chain is a rattling noise that occurs before and after the engine warms up. The noise could also be coming from the rocker arms. If you hear these sounds, it’s a good indication that your timing chain is stretched and needs to be replaced. For more information on the causes of timing chain noises, read the rest of this article. Once you’ve done this, you’re well on your way to fixing the problem.\nThe next sign that you need to replace your timing chain is a warning light. The Check Engine Light will illuminate if your valve timing goes beyond preset limits or if your exhaust emissions are too high. Ignoring this warning could result in severe damage to your engine. In many cases, timing chain noises are an indication that your timing chain has failed, and you should replace it as soon as possible. Noises can be caused by one of three issues: an overly tight tensioner, a mis-adjusted sprocket, or excessive wear on the chain.', 'Discussing the effects of microplastics on our environment it is important to understand that microplastic particles from rinse-off and leave-on cosmetics usually end up in sewage, microplastics manage to escape from the wastewater treatment systems due to their small size. The separation of microplastics from other particles is difficult and limited. Mainly in underdeveloped and developing countries, according to (European Chemicals Agency, 2019) more than around 87% of the microplastics end up in biologically active sludge and aquatic environments (European Chemicals Agency, 2019), (Yurtsever, 2019b, 2019a), (Hidayaturrahman and Lee, 2019), (Li et al., 2020).\nThere are a lot of studies conducted which demonstrate the presence of microplastic particles literally everywhere: in the air we breathe, in bottled and tap water, in foodstuff like salt, in home dust and almost everywhere in the aquatic environment (surface, mildwater, benthos), in freshwater systems. Furthermore, it was detected that there are microplastic particles in all forms of marine species: from tiny organisms to predators.\nAccording to (Haegerbaeumer et al., 2019) and (Xu et al., 2020), there is proven effect of microplastics on our environment and life processes like mortality, development, reproduction, behaviour, cellular response, lifespan, egestion, regeneration, air and food consumption, even on metabolism, gene expression, neurotoxicity and carcinogenicity of aquatic species.\nDid you know that resembling the movement and the size of food, microplastics can be consumed by mistakes by zooplankton and small fish. Microplastics can be absorbed by phytoplankton and filter feeders; and by mistake by larger organisms ending up in the intestine. There are issues with photosynthesis function in phytoplankton species. Thus, microplastics are putting the life of microorganisms at a high risk which leads to the problems of the trophic chain.\nIndirect effects of microplastics should be also mentioned here, as they may contain additional harmful additives like phthalates, pigments or heavy metals, plasticizers, alkylphenols for functionality and conservation purposes. However, afterwards, these additives are released to the environment. Also, there are studies on the interaction of microplastics with inorganic pollutants (Xu et al., 2020). These pollutants and harmful additives may be released into the organisms, it happens more easily with small plastic nanoparticles. Nevertheless, detailed research on toxicity, and effects of plastic nanoparticles on the environment and organisms is needed in order to make specific conclusions.\nMoreover, in terms of humans themselves, further studies are needed in order to ensure toxicity of microplastics on the human organism. However, recent findings have shown the presence of microplastics in the human faces (Yan et al., 2020; Zhang et al., 2020). We inhale, swallow and consume microplastics in the direct way through air, water, and seafood.\nThere are significant gaps in analysis and scientific knowledge on the effects of micro- and nanoplastics on human organisms. A lot of researchers are worried about possible harmful effects of nanoplastics, as they are more likely to be absorbed and distributed in our organisms.\nActions against plastic microbeads\nHaving identified possible effects of microplastics on the environment, it can be said that microplastics may pose a greater risk than macroplastics. Therefore, the policies on exclusion of intentionally added microplastics from products (e.g. microbeads from cosmetic products) started by particular countries. In 2012, there started a global pressure against microbeads in cosmetics by NGOs, scientific findings, and intergovernmental organizations. The topic was thoroughly analyzed by environmental consultants and scientific institutes indicating that microbeads are an unnecessary source of plastic pollution.\nThus, in 2014 the state of Illinois was the first to ban non-biodegradable microbeads from personal care products, then 15 States followed this ban. Therefore, “Microbeads Free – Water Act” was introduced by the U.S. prohibiting manufacture (from July 2017) and sales (from July 2018) of intentionally added non-biodegradable plastic microbeads in rinse-off personal care products. Other countries also took actions, for instance, Canada followed the ban prohibiting the manufacture, import, or sale of toiletries containing microbeads.\nIn October 2019, China also introduced a ban to stop the production of household chemical products which contain plastic microbeads as of the 31th of December 2020, the full-scale ban will be implemented in December 2022.\nNon-European countries like New Zealand, South Korea and the Taiwan province have already banned microbeads usage in rinse-off cosmetics. Iceland joined the ban and signed a commitment. India is considering future restrictions. In Brazil the law was introduced in 2016 and approved in 2019, but has not been accepted as a public law yet.\nIn 2015 two out of six states of Australia signed an agreement to phase out microbeads.\nSouth Africa’s government is currently in discussion and consultation with the cosmetic industry for a microbeads’ phase-out in cosmetic products.\nRegarding the European Union, in 2013 Denmark was the first country to propose a European-wide ban on microbeads. A new directive 2014/893/EU published by the European Commission in 2014 prohibited rinse-off cosmetics that contain plastic microbeads to have the Ecolable sign. Therefore, the Netherlands, Austria, Luxemburg, Belgium, and Sweden joined the European ban on microbeads in personal care products. However. The European ban was not the straightforward policy, as it is in the USA and Canada due to the different EU’s structure.\nNordic countries were the pioneers of promoting the microplastics ban in cosmetics. In 2017, a full ban of microplastics in cosmetics was proposed by the Nordic council. In 2018, Sweden added solid plastic particles in cosmetics in the list of prohibited particles. Finnish government had to exclude microbeads in rinse-off cosmetics by June 2020. Norway being a niot full EU member, has prohibited personal care products containing microplastics to bare the Swan label (eco-friendly Nordic product), and the country is considering a full ban. Belgium signed a phase-out agreement with cosmetic industries which had to be implemented by the end of 2019.\nFrance aims to ensure water quality justifying the prohibition of microbeads by using the European Marine Strategy Framework Directive. In 2018, the national law banning microbeads in rinse-off cosmetics was introduced in France. Starting as of 2020, Italy joined the same restriction of microbeads usage in exfoliating and cleansing rinse-off cosmetics.\nThe UK has preferred to join a European-wide ban introducing the ban into the national legislation, UK’s prohibition is one of the strongest and most comprehensive.\nHowever, the microbeads (from 1 nm to 5 mm) phase-out in exfoliating and cleansing rinse-off cosmetics does not seem to be enough and does not lead to a risk reduction. The European Chemical Agency proposed a restriction of all microplastics (other than microbeads) that are added to rinse-off cosmetics to be initiated 4 years after the ban becomes effective and for leave-on cosmetics 6 years respectively.\nTaking everything into consideration, alternative solutions should be developed. BIOWEG is aiming to fully replace plastic microbeads in cosmetic and personal care products offering biodegradable, 100% bio-based microbeads to cosmetic industries.\nKeep following us on social media channels to know more.\nSource: (Anagnosti et al, 2021).\nSource: Anagnosti, L., Varvaresou, A., Pavlou, P., Protopapa, E., & Carayanni, V. (2021). Worldwide actions against plastic pollution from microbeads and microplastics in cosmetics focusing on European policies. Has the issue been handled effectively?. Marine Pollution Bulletin, 162, 111883. doi: 10.1016/j.marpolbul.2020.111883']	['<urn:uuid:758a27d1-e653-40fd-928c-aefe55bfbe4c>', '<urn:uuid:2f4165b1-af6c-40c1-8aed-386b48b0aa61>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T00:20:10.754925	10	80	1988
77	What's the best way to figure out if your racing tires have the right amount of air in them, and how can you tell if they're working properly during a race?	To determine optimal tire pressures, you should use a combination of tools and observations. The most important tools are a lap timer, driver feedback, and a probe-type tire pyrometer (not laser type). The pyrometer measures tread temperatures, which indicate how well the tire is performing. Street tires work best at 140-170°F, while race tires prefer 170-220°F. When measuring temperatures, you should check three points across each tire: inside, center, and outside. Ideally, you want to see a 5-10° drop from inside to center, and another similar drop from center to outside. If the center is too hot compared to the edges, your pressure is too high. If it's too cold, the pressure is too low. Additionally, during a race run, tires typically gain about 2 pounds of pressure per tire, and you may need different pressures for different length runs - higher pressure for short qualifying runs, lower for longer races. Track conditions, weather, and sun exposure can all affect optimal pressures, so there's no single 'right' pressure for all situations.	['Go Kart Tire Pressure – More than just keeping your tires round\nAn extremely popular topic at every kart race is tire pressures. How much? How little? Track conditions and weather will affect what air pressure you run as much as anything. Personally I like to think of tire pressure as a timing device. If you don’t have enough the kart won’t come in and the tires won’t have maximum grip for the conditions. If you have to much then the kart will get tight during the run or the tires will get to hot and the kart will start to slide which is commonly referred to as “greasy”. Just remember, more air pressure is more heat, less air pressure is less heat.\nOne important factor to consider when setting the air pressure is the length of the run. If it’s only a few laps, like qualifying, you might need more air. If it’s a 20 lap race then you’ll definitely need to set the air low enough that the kart won’t just come in for the first 5 laps and suck for the next 15! Best way to determine your air pressure is during practice. If you go out for practice and you run 8 laps you need to evaluate how the run went. Was it fast early then fell off? Was your last lap your fastest? If your last lap was the quickest then you know that you are closer to a long run go kart tire pressure which is great for a long final. If you only have 3-5 laps for qualifying then the higher air that was only fast for the first few laps looks to be the better number for that scenario.\nMy typical rule of thumb for air pressure gain is 2 pounds per tire during a run. If you set all the tires at 10 pounds and go out and run 6 laps check your pressures when you come off. Did they all go up 2 pounds? Did the outside rear tire gain more than the other three (which is typical)? If you come off the track and any of the tires gained more than 2 pounds then you probably need to drop air on that tire or tires for the next run. Running staggered tire pressures is very common, actually it’s necessary. If the track is predominately a left hand turn track the right rear will get a lot of abuse and should gain the most air pressure. As an example you’ll see a typical cold air pressure setup like this – RR 9, LR 9.5, RF 9.8, LF 10. The right rear is lowest because it is worked the hardest and gains the most air so to keep it around a 2 pound gain on track you have to start it lower than the others. In contrast, the left front is the tire used the least in this example so it has to be started higher to see a 2 pound gain. If the track is mainly a right hand turn track then you would swap these pressures so the LR would be the lowest set tire and the RF would be the highest.\nNow one thing that can throw a wrench into the works is new tires. Personally I think in most situations, especially if it is warm out and the track has good grip, you need to run lower air in qualifying on new tires than you think. My rationale for that is you have a cold tire, cold air in the tire and a cold wheel. As soon as you hit the track all three are increasing in temperature very quickly and you will see much more than a 2 pound gain. Once again the length of the run is extremely important in this decision. If you only have a 3 lap qualifying run then you need to make sure the air is high enough that the tire comes in before you get the checkered flag. If it’s a 5 minute session then you don’t want the air so high that the first lap is your best (unless it’s good enough for pole!). In my experience you shouldn’t run the air any higher on a new set of tires then you did on the practice set. Typically bumping up air on new tires (if qualifying isn’t super short) will make the kart tight way to soon to run an adequate lap time.\nNow setting air before you go on track can be a little tricky. If you set the air under the tent then push to the grid the sun will be increasing your go kart tire pressure as the tire heats up. I like to set my cold pressures under the tent then 5-10 minutes before I have to be on track I will put the kart out in the sun to let the tires heat up some and bleed the tires back down to my “cold” pressure settings. Unfortunately it’s as much art as science in getting it right but it’s very common to see a .5 pound increase in all the tires just putting the kart in the sun. Sure the tire was going to gain that when you hit the track but if the tire was only 60 degrees under the tent and 90 in the sun I think some of that pressure gain needs to be adjusted before hitting the track just due to the initial temperature difference.\nSo what about rain tires? More air if it’s really wet and less air if it’s really dry. If it’s a monsoon 18-22 pounds of air is a good range, if the track is drying then you’ll have to drop air. I’ve seen guys go as low as 10 pounds when they had to go out on a very dry track with rain tires.\nA final rule of thumb. Soft tires and/or high grip tracks require less air pressure, hard tires and/or low grip tracks and conditions need more air pressure. I’ve been to tracks where 20 pounds of air makes you fast and I’ve been to tracks where 7 pounds of air was all you needed and sometimes it’s the exact same track with only different conditions or tires! The only right answer on air pressure is whatever makes you fast for a given day. One question that comes up a lot is what’s better, air or nitrogen. The reason racers like nitrogen is the lack of moisture in compressed nitrogen. Water vapor expands when it is heated up so running regular air will cause the tire to gain more pressure over a run than running moisture free nitrogen, at least in theory. Here’s the main issue with running nitrogen, you have to adequately purge the air from your tires to get as much moisture laden air out of the tire as possible and replace it with the nitrogen. According to the racing professionals I have talked to (stock car and Indycar guys) it’s extremely hard to get all the air out of the tire and replace it with pure nitrogen. Most think it doesn’t make a huge difference because purging all the moisture from the tire is extremely difficult. Some professional race teams use a vacuum pump to suck the air out of the tire and another line that is fed from a nitrogen tank replaces the air once the tire is empty. Is nitrogen better than air? Will you see a difference over the course of a 20 lap run? I think the jury is still out.\nThat pretty much covers the basics of go kart tire pressure. As with anything in racing experience is the best teacher and keep notes on what did and didn’t work so you’ll know what you did the last time you were racing in those conditions.\nsource: Mark Dismore Jr. / Comet Kart Sales', 'Supermiata Using a Tire Pyrometer\ncopyright 949 Racing 2006-2023\nUsing a tire pyrometer\nI get asked all the time: “what pressures should I run?” My answer is usually “I don’t know, what were your pyrometer readings that day?” You spent $3200 on tires last year but don’t own a $100 pyrometer? Oh the shame.\nOne should determine optimum pressures primarily with a lap timer, driver feedback and pyrometer. Data acquisition helps but isn’t always necessary. The pressure gauge is just a meter and doesn’t tell you much about how the tire is working. In other words, using a timer, pyro and good driver, you could optimize pressures without a tire gauge, you just wouldn’t know what they are.\nThere is no such thing as an “ideal” or “best” pressure for all conditions. The finer you are trying to hone your setup for a particular condition, the more minute the variations. I recall a story by legendary Indy driver Rick Mears (a god). He was telling a journalist about having a car that was a bit loose in qualifying on a super high speed oval. So he deliberately drives a line that will induce understeer for one lap, thus increasing the temperature and making the right front tire grow in diameter by a few thousandths of an inch. Due to the ultra sensitive set up of an Indy car, that tiny stagger difference balanced the car perfectly for the next lap for a great qualifying lap. The tire cooled down after that lap but the pro driver got the job done by actively playing with tire pressures just with driving technique. Every single thing that happens at the track affects tread temps and what optimum might be for that lap. Suns comes out, wind picks up, tires going off, oil in dampers losing viscosity from heat and on and on.\nFor HPDE’s, autocross and most club racing, getting to within .5 psi of optimum in each tire is close enough. It is common to end up with a different hot and cold pressure on each tire. So optimum pressures for all four tires will rarely be the same when hot. Clockwise course with a few long fast right hand turns will obviously put more heat into the left side tires. So starting left side cold pressures might be below the right side cold pressures, but hot pressures higher than the right side. Running 2-3 qualifying or time attack laps? Your cold pressures will be different than if running a 35 minute sprint race or 3hr enduro even with the exact same setup. One driver likes the car loose and throws the car into the turn with lots of steering corrections. Another driver in the same car likes the car neutral, enters turns earlier and has fewer and smaller steering inputs. Both drivers run the same lap times. Same pressures? Of course not.\nWhat I am trying to indicate here is that no one can tell you what optimum pressure is so don’t ask. Get a pyrometer, learn how to use it and become your own expert.\nThe pyrometer measures tread temperatures. Any tire will develop its optimum grip when the rubber compound is heated above ambient temp. Street tires like to stay fairly cool, 140-170°. UHP, EHP, MHP tires (R1R, Star Spec etc) will work best at slightly higher temps, around 150-190°. Full race tires (Hoosier, V710, NT01 etc) will want in the 170-220° range. Race tire manufacturers will always provide optimum tread temp info for a given compound, just ask them or go to their website in the competition tire section. Tires will always cool a bit after coming off track no matter how quickly you get to them so as long as your procedure remains the same, you can still get excellent data. The relative value is what matters, how the temps changed when you did this or that adjustment.\nFor the pyrometer, use a probe type. The laser type only see the surface of the tire which cools too rapidly to get accurate measurements. The laser measures reflectivity so and older heat cycled tire might have a different relative temp reading than a freshly shaved race tire, even though the absolute tread temp is exactly the same.\nThe goal is to get all four tires with as close to perfect heating pattern as possible. Occasionally, I briefly manage that for a given track condition. Those settings always coincide with my fastest laps. Expect it to take a few sessions of tuning if you are starting from scratch. Highly recommend keeping a notebook will all your measurements, brief driving impressions, car set up and of course, lap times.\n- Without a cool down lap, get the car stopped safely but quickly in the pits\n- Start with the tires on the outside of the track, example: left tires on CW track\n- Start with the inside and work out; Inside, Center, Outside.\n- The shoulder measurement must be in the scuffed an heated part of the tread, usually 1/2~ 1″ inboard of the extreme edge of the tread. Not the shiny unused outer corner.\n- Put the probe into the exact same area of the tread on each tire and push in the same amount.\n- For a given car, always use the same sequence. IOW, for a Miata, I always start with the outside front, outside rear, coldside rear, coldside front. S2000’s generally heat the rears more so I start with the rear.\n- You should see a 5~10° drop from inside to center, then the same drop from center to outside shoulder. The total gradient that works best for radials on the loaded side of the car (outside of track) is usually 15-20° in even steps.\n- The outside tires (left side on clockwise course) are the important ones. It is possible for the more lightly loaded inside course tires to display a greater temp delta of 25-40° on cars with a lot of static camber and stiffer suspensions (more than OEM specs). This is normal and usually doesn’t need to be corrected.\nSome basic guidelines for what the temperatures mean:\nCenter too hot – example: I 170° – C 180° – O 170°\nToo high air pressure. This can be too high cold or “starting” pressure or excessive heat caused by sliding due to car setup or driving style.\nCenter too low – example: I 180° – C 160° – O 170°\nToo low air pressure. This can be too low cold or “starting” pressure, car setup or driving style.\nInside high or low – example: I 190° – C 175° – O 160°\nToo much toe out or negative camber can cause excessively high inside edge temps. The way to differentiate is if both LF and RF have the same delta, it’s toe. If the outside course tires have a much different delta than the inside course tires, it’s usually camber. Also, excessive toe out will usually show a scalloping or feathering of the tread where camber will not.\nOutside high or low – example: I 170° – C 180° – O 200°\nThis is the pattern most commonly seen on production based cars and is usually caused by insufficient negative camber. One band-aid fix is to help make it through the day is to overinflate the tires to raise center temps. This can help reduce the workload of the overheated outside shoulder\nWith the pyro, you’re seeing vehicle configuration, weather, setup, and driving style.\nExample: Too much front sway bar/spring and not enough camber? Front tires significantly hotter than rear, outside shoulders hotter than inside.\nExample: Rears like 50° hotter than fronts? Too much drifting\nExample: All tires low temps? First session, cold day on old tires\nExpect to see one outside tire slightly hotter than the rest if the track has more than one long duration turn in the main direction of travel. WSIR has two very high speed right hand turns on a CW track. The fastest way through in just about every rwd car is to have the tail out a bit so the left rear tire takes a beating and always runs 5-10° hotter than the rest. Buttonwillow CW13 OTOH, shows pretty even temps on a well set up rwd car because it has plenty of hard left handers too.\nBuying a pyrometer\nThere are two basic types; Probe and laser. The laser merely reads the radiant heat off the tire surface. Unless the specific laser is calibrated for that particular tire texture and reflectivity, the reading will not be true. The other problem with laser pyros is that they only read the surface. When you roll off track, the surface cools down quickly while the rubber and casing underneath retain their heat a bit longer. Always measure and record the surface at trackside, making sure to sample the same type of surface as the track. Lasers only work in real time when aimed at a tire while on track and recording to an onboard data logger. For this reason, you should choose a probe type pyrometer. They’re available everywhere. A quick google search will find plenty of deals.\nA few shops I like to frequent for this kind of stuff']	['<urn:uuid:d5879716-bedb-48f7-8c75-90c8b2af0c23>', '<urn:uuid:cb023981-b0fa-466d-aece-89117daaf5a5>']	open-ended	direct	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-13T00:20:10.754925	31	171	2859
78	public land survey system history modern satellite technology advantages disadvantages	The Public Land Survey System began with Jefferson's Land Ordinance of 1785, leading to the survey of 1.5 billion acres into townships and sections during the 1800s. The land was initially sold by the federal government for $1.25 per acre, with lot sizes of 320 acres available in installments by 1800. Modern satellite technology has transformed this system, utilizing both passive and active sensors. Passive sensors like radiometers offer advantages such as long heritage and sensitivity to surface conditions, particularly moisture and snow-water content. However, they require large pixel sizes due to low energy emission. Active sensors provide benefits like day/night coverage and all-weather capability (except during heavy rain), but have disadvantages including difficult data interpretation and coarse resolution except for Synthetic Aperture Radar.	"['To honor National Surveyors Week (March 17-23), we want to highlight a few land surveyors who have impacted our country. Notably, these surveyors also happened to be U.S. Presidents. They played a big role in making our country what it is today.\nPast presidents of the United States are often remembered for individual accomplishments, but three of the four presidents featured on Mt. Rushmore, also had an instrumental role in the way of creating America.\nThomas Jefferson was instrumental in creating the Public Land Survey System (PLSS) for the United States after the Louisiana Purchase. Jefferson proposed the Land Ordinance of 1785 that marked the beginning of the Public Land Survey System. At the time, the U.S. government wanted to distribute land to Revolutionary War soldiers as a reward for their services, and sell land as a way to raise money for the nation. However, before this could happen, the land needed to be surveyed, Jefferson enlisted in the help of Meriweather Lewis and William Clark.\nThere were 1.5 billion acres surveyed into townships and sections (1 square mile) during the 1800s. During that time the vast open land was sold by the federal government for $1.25 per acre. In 1800, the minimum lot was halved to 320 acres and settlers were allowed to pay in four installments.\nGeorge Washington was one of the most notable land surveyors in American history, laying the lines for the Western Virginia frontier. In 1749, George Washington was appointed to serve as the Surveyor General for Virginia at the age of 17. The role was essential because surveying the land promoted westward expansion for the growing country.\nAbraham Lincoln signed the law known as The Homestead Act of 1862. By May of that year, 11 states had left the Union, and this piece of legislation would continue to have regional and political overtones. The Homestead Act of 1862 allowed any citizen who did not fight in a war against the United States to purchase 160 acres of land.\nBetween 1805 and 1850, government surveyors subdivided Illinois into six-mile squares (called townships), 1-mile squares (called sections) and 160 ac “lots” (called quarter-sections), creating the “checkerboard patterns” seen while flying over Illinois.\nSurvey Technology at a Glance\n- 1780-1890 – Surveyors used the compass and chain to trace lines.\n- 1890-1975 – Over the course of 85 years, technology changed rapidly and surveyors started using steel tape and engineers transit to trace lines.\n- 1975-1985 – Surveyors started using Electronic Distance Measuring (EDM) devices to retrace the Public Land Survey System lines, where the original government surveyors placed them.\n- 1985-Present – As technology has continued to change, surveyors have had to learn new tools, including total stations and data collectors to help them retrace the PLSS lines.\n- 1990-present – Surveyors have continued to retrace PLSS lines, and follow in the footsteps of original government surveyors by using global positioning system(GPS), 3D laser scanning and UAV’s or drones to make measuring become faster and more accurate.\nA Tribute to Land Surveying\nAlthough the men carved into Mt. Rushmore were first and foremost great presidents, Washington, Jefferson and Lincoln were also noted surveyors – so it is only right to commemorate them. In March of 1984, Ronald Reagan proclaimed the third full week of March as National Surveyors Week as a way to pay tribute to professional surveyors and their contribution to society.\nIn his proclamation letter, Reagan stated: “I invite all Americans to look back at the historic contributions of surveying and look ahead to the new technologies which are constantly modernizing this honored and learned profession.”\nThe United States wouldn’t be where it is today without the work of our past surveyors and leaders who have literally paved the way for how America is today!', 'surveying ii ajith sir class6\nPost on 19-Jun-2015\nEmbed Size (px)\n- 1. Two Generic Types of Sensors A ""passive"" system generally consists of an array of small sensors or detectors which record the amount of electro-magnetic radiation reflected and/or emitted from the Earth\'s surface. An ""active"" system propagates its own electro- magnetic radiation and measures the intensity of the return signal.\n2. Examples of Passive Sensors Advanced Very High Resolution Radiometer (AVHRR)- Sea Surface Temperature Sea-viewing Wide Field-of-View Sensor (SeaWiFS)- Ocean Color 3. Why Sea-Surface-Temperature? Sea surface temperature (SST) is a key parameter for energy transfer between the oceans and the atmosphere and AVHRR- derived SST can deliver important data to meteorological and climatological models. AVHRR imagery is also useful for meteorological applications (for example, providing detailed imagery of cloud systems), and land applications such as vegetation mapping and monitoring. 4. Advantages of passive microwave remoteAdvantages of passive microwave remote sensingsensing Sees through clouds at lower frequencies Long heritage, various instruments since 1978 Emissivity sensitive to state of surface, particularly moisture Soil moisture Snow-water equivalent (water is ~80 as absorptive as ice at these frequencies, whereas in visible through infrared, water and ice have similar absorption coefficients) But, because of small amount of energy emitted, pixel size must be large 5. ACTIVE SENSORS (Detect the reflected or emitted electromagnetic radiation from natural sources.) PASSIVE SENSORS (Detect reflected responses from objects that are irradiated from artificially-generated energy sources such as radar.) Passive Non-Scanning Non-Imaging. (They are a type of profile recorder, ex. Microwave Radiometer. Magnetic sensor. Gravimeter. Fourier Spectrometer. Imaging. (Example of this are the cameras which can be: Monochrome, Natural Colour, Infrared etc.) Scanning Imaging. Image Plane scanning .Ex. TV Camera Solid scanner. Object Plane scanning. Ex. Optical Mechanical Scanner Microwave radiometer. Active Non-Scanning Non-Imaging. (They are a type of profile recorder, ex. Microwave Radiometer. Microwave Altimeter. Laser Water Depth Meter. Laser Distance Meter. Scanning Imaging. (It is a radar ex. Object Plane scanning: Real Aperture Radar. Synthetic Aperture Radar. Image Plane Scanning: Passive Phased Array Radar. 6. IoE 184 - The Basics of Satellite Oceanography. 1. Satellites and Sensors Sensors on satellites Passive sensors Wavelength Information Visible wavelength radiometers 400 nm - 1 m Solar radiation reflected by Earth surface Infrared (IR) radiometers about 10 m Thermal emission of the Earth Microwave radiometers 1.5 - 300 mm Thermal emission of the Earth in the microwave Active devices Altimeters 3 - 30 GHz Earth surface topography Scatterometers 3 - 30 GHz Sea surface roughness Synthetic aperture radars 3 - 30 GHz Sea surface roughness and movement 7. Microwave Remote Sensing: Principles and Applications. Advantages Day/night coverage. All weather except during periods of heavy rain. Complementary information to that in optical and IR regions. Disadvantages Data are difficult to interpret. Coarse resolution except for SAR. 8. Infrared Satellite Imagery These images were obtained with an AVHRR sensor (Advanced Very High Resolution Radiometer) carried on a NOAA Polar Orbiting Environmental Satellite (POES). Measures the amount of thermal infrared radiation given off by the surface of the ocean. The amount of thermal infrared radiation given off by an object is related to its temperature (dying embers give off less radiation than a hot fire and a person gives off even less). Thus by measuring the amount of radiation given off by the ocean we can calculate its temperature. 9. Infrared Satellite Imagery These images were obtained with an AVHRR sensor (Advanced Very High Resolution Radiometer) carried on a NOAA Polar Orbiting Environmental Satellite (POES). Measures the amount of thermal infrared radiation given off by the surface of the ocean. The amount of thermal infrared radiation given off by an object is related to its temperature (dying embers give off less radiation than a hot fire and a person gives off even less). Thus by measuring the amount of radiation given off by the ocean we can calculate its temperature. 10. Role of space measurements of the earth\'s topography. This information has wide applications: in geology (e.g., correlating with plate tectonics effects); In geophysics (eg., Distribution of is static imbalances); In climatology (e.g., Orographic barriers), In hydrology (e.g., Drainage basin characteristics); In glaciology (e.g., Ice sheet thicknesses); In ecology (e.g., Ecozone disposition); and In planetology (e.g., Comparative global hypsometry [frequency distribution of elevations]). High resolution regional coverage (on order of 100 m horizontal and 1 m vertical) is technically achievable even now. 11. Remote sensing applications Agriculture Mapping crop area Identifying diseases and crop stress Estimating crop yield in conjunction with models Detecting weeds and illicit crops pasture management 12. Remote sensing applications Cartography map revision geodesy and photogrammetry merging other data with remote sensing for map presentation use of stereo imagery for topographic mapping compile and update thematic maps of various resources 13. Remote sensing applications Engineering routing power lines designing transportation networks site selection 14. Remote sensing applications Erosion mapping mapping and monitoring eroded areas predicting potential erosion sites monitoring land degradation and desertification 15. Remote sensing applications Geology Identifying lineaments and other structural features mapping geomorphology and geobotany mineral and petroleum exploration analysing landform and drainage identifying rock types identifying oil seepage access planning and base map preparation 16. Remote sensing applications Hydrology detecting near-surface aquifers for ground water storage monitoring irrigation performance and usage supplement investigations for flood plain management monitoring on-farm water storage mapping current and potential salinity sites estimating soil moisture and surface temperature planning engineering constructions and monitoring their effectiveness 17. Remote sensing applications Meteorology routine atmospheric studies of temperature and weather patterns mapping cloud cover, patterns, composition and temperature weather forecasting flood prediction and monitoring storm warning and damage assessment locating and tracking cyclones monitoring bushfires mapping snow cover, run-off and melt rate detecting chemical and/or particulate composition of the atmosphere climate studies vertical temperature and humidity profiling deducing geopotential height and upper level wind velocity mapping cloud drift winds 18. Remote sensing applications Oceanography estimating sea surface temperature ocean colour mapping mapping of sea surface and sea floor topography detecting navigational hazards mapping ocean currents, wind and wave action detecting oil spills, thermal effluent or other pollution mapping fish populations and movements identifying upwelling areas of biological significance studies of sea ice and glacial movement 19. Remote sensing applications Renewable resources land cover inventory and monitoring modeling vegetation structure detecting land use changes mapping landform types mapping potential bushfire status assessing the impact of natural disasters such as fire or drought 20. Remote sensing applications Shallow water mapping bathymetric studies mapping turbidity and estimating suspended sediment concentration mapping chlorophyll content, such as for algal blooms mapping reef type and morphology monitoring sea grass distribution 21. Remote sensing applications Urban studies mapping extent of, and changes, in urban settlements studies of housing density and urban drainage land use planning']"	['<urn:uuid:ca89bea9-c365-48e6-961f-b49b6f03f2e1>', '<urn:uuid:8605f800-8320-429a-9ec7-6ecdc6b8495b>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T00:20:10.754925	10	124	1755
79	waste hierarchy main purpose reduce landfill	The waste hierarchy has served as the cornerstone of waste management policy, principally as a guide to move waste management away from landfilling into more environmentally desirable options.	['Rethinking the waste hierarchy\n24th October 2017\nThe term “rethink” is a particular favourite within our sector. Examples of reports and projects abound: rethinking waste, rethinking waste management, rethinking waste crime, rethinking organic waste, rethinking recyclability. What about rethinking the waste hierarchy? Many suggestions have been put forward, expanding the so-called 3Rs (reduce, reuse, recycle) to 5Rs, 6Rs (Rethink, Refuse, Reduce, Reuse, Recycle, Replace) and even 10Rs in an attempt to widen the reach of the hierarchy beyond the waste phase. Is it perhaps time to revisit this 1970s-vintage principle and ask whether it needs to be updated and/or supplemented with other priority frameworks?\nThe waste hierarchy originated as Lansink’s Ladder when Ad Lansink presented an order of preference for waste management options to the Dutch parliament in 1979. Since then the waste hierarchy has served as the cornerstone of waste management policy, principally as a guide to move waste management away from landfilling into more environmentally desirable options.\nHowever, with the evolution from waste management to resource management and the advent of the circular economy as its leading guiding principle, how does the waste hierarchy stand up in this new context? For example, it has been suggested that ‘prevention’, the first step in the hierarchy, is misplaced here. As experience in the UK has indicated, a product is de facto defined as a waste once it has been discarded, which is at odds with the concept of preventing a product from becoming waste in the first place. Furthermore, Van Ewijk and Stegemann have pointed out in Limitations of the waste hierarchy for achieving absolute reductions in material throughput (2016) that in addressing just the discard element of a product, the waste hierarchy does nothing to encourage practices further up the resource chain, for example to improve resource efficiency and resource productivity, and to minimise total material flow through the economy.\nAnother crucial aspect of the circular economy, that of encouraging sustainable consumption behaviours, is also untouched by the waste hierarchy, though the European Commission’s good practice guide Public Procurement for a Circular Economy – good practice and guidance (2017) adapts it as the basis for green procurement – reduce, reuse, recycle, recover. Interestingly, the implicit assumption here is that a product is ‘prevented’ from becoming waste, again suggesting that prevention is a strategy that should be separate from the ‘waste’ hierarchy.\nThe bottom line seems to be that the plus points of the waste hierarchy are its simplicity, clarity and enduring emotional and intuitive appeal, which suggests that tinkering with it, for example by expanding some steps or introducing new steps to force-fit other concepts would make it unwieldy and water down its powerful message. Better perhaps to accept that it holds good for a particular segment/side stream of the circular economy, and to introduce other priority frameworks to support it. Horses for courses.\nOne option would be to separate the resource chain into three or four discrete elements, and apply appropriate concepts to each. For example, production and manufacture would be characterised by a preference for secondary over virgin raw materials and renewable over brown energy, backed up by resource efficiency and resource productivity targets. The aim here would be to encourage “absolute material reductions through the concept of absolute decoupling” in the words of Van Ewijk and Stegemann.\nThe product phase would be supported by a hierarchy emphasising the primacy of designing for durability, modularity, repair and reuse, then for recyclability, and finally to promote safe disposal by avoiding the use of hazardous materials – linking with the previous stage to ensure that secondary raw materials and renewable energy are used in preference to virgin/fossil commodities.\nThe consumption phase would be characterised by a so-called “consumption hierarchy”, which could look something like this:\nThe discard phase would still be represented by the standard five-step waste hierarchy, except that prevention activities may be better addressed in a hierarchy further up the resource chain.\nThe waste hierarchy has had a huge influence in changing waste management practices for the better. The present approach attempts to replicate this success in other key elements of the resource chain, but each according to its own characteristics. In particular, disentangling prevention from the discard phase may give it the focus and attention it sorely lacks at the present time, sitting as it does within the waste hierarchy.Tweet\n- circular economy\n- no more waste\n- resource management\n- Waste Crime\n- waste hierarchy\n- waste management\n- waste management policy\n- zero waste to landfill']	['<urn:uuid:02708f05-d990-4589-9e2f-19cd52044e7b>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-13T00:20:10.754925	6	28	752
80	What are National Museum's different buildings and how much data plans cost there?	The National Museum complex has three main buildings: The Natural History Museum (featuring exhibits like Tree of Life and Lolong the crocodile), Fine Arts Museum (housing masterpieces by Filipino artists including Luna's Spoliarium), and Anthropology Museum (displaying cultural artifacts and ethnographic materials). For mobile data while visiting, you can get packages from Smart like allout30 (2 days, 300MB plus Facebook data for 30 pesos), gigasurf70 (7 days, 1GB plus YouTube data for 70 pesos), or gigaplus499 (30 days, 4GB plus YouTube data for 499 pesos). These come with varying amounts of calls and texts.	"['You land in the Philippines and have time to kill in Manila before hitting the islands. What do you do?\nHere’s what we’ll cover today:\n- Setting up your local sim card and ride-sharing apps\n- Seeing some of Manila’s cultural and historical sights\n- Experiencing Filipino bargain shopping\n- Blending in and staying safe\nGetting Out of the Airport\nWhen you arrive at Ninoy Aquino International Airport (NAIA) and make your way through customs, you’ll step out into a warm blanket of tropical humidity.\nFrom there, your options are to take an airport taxi or book a Grab.\nSide note: if you’re traveling light and only have a backpack, you can try Angkas—a motorcycle taxi service. Follow the link for a short article on what it is and how to use it.\nIf you want to get set up with either Grab or Angkas, a local sim card will help. (NAIA’s free wifi tends to cut out as soon as you leave the building.)\nGetting Set Up with a Sim Card\nAt the airport, you can get a free Smart or Globe sim card. Then you can buy credit or “load” (usually sold at airport stalls in 500 peso cards). Once you have your load, you can subscribe to a data package.\nI started using Smart because their free sim card comes with a bit of consumable data. Once that is used up, I usually go with one of the following packages by texting one of the codes below to 9999. The number in the code equals its cost in pesos.\n|Code||Days||Data||Call & Text|\n|allout30||2||300MB plus 100MB/day for Facebook||unlimited texts to all networks; 30 min of calls to Smart/TNT/Sun|\n|gigasurf70||7||1GB plus 300MB/day for YouTube||1000 texts to Smart/TNT/Sun|\n|allout99||7||1GB plus 100MB/day for Facebook||unlimited texts to all networks; 100 min of calls to Smart/TNT/Sun|\n|gigasurf299||30||2GB plus 1GB/day for YouTube||N/A|\n|gigaplus499||30||4GB plus 1GB/day for YouTube||unlimited texts to all networks; unlimited calls to Smart/TNT/Sun|\nI prefer Smart because they offer more data at better value and tend to have better speeds. However, their coverage isn’t quite as good as Globe. Check Globe’s prepaid promos here.\nMobile Phone Networks in the Philippines\nRegardless of the provider you choose, you will quickly learn that data in the Philippines is alright but it’s not super fast and not always super reliable.\nGlobe and Smart are the largest networks in the Philippines. Next are TNT and Sun, which—as you can see from the packages above—are connected to Smart.\nIf you have friends to visit in Manila and want to be able to call them, you should find out which network they are using.\nThings to do in Manila on a One—Day Stopover\nFirst stop: Baclaran\nWhy Baclaran? It’s near the airport and a great induction to the crazy that is Manila.\nIf you haven’t sorted out Grab or Angkas, count on an airport taxi to Baclaran costing somewhere around 100-200 pesos. (It’s roughly 4km from NAIA Terminal 1 and 7km from Terminal 3.)\nFor things to explore in this part of town, check out an earlier post on the Baclaran experience.\nNext stop: Intramuros\nHop on the LRT at Baclaran Station and ride to United Nations Avenue (14 minutes, leaves every 5 minutes, costs 20 pesos).\nThis will drop you off right at the corner of Rizal Park, where you can check out a massive map of the Philippines laid out in a something like a big pond, plus several historical monuments, and the National History Museum.\nFrom Rizal Park, you can head north to Intramuros (10-15 minute walk).\nA few sights to see in Intramuros, within walking distance of Rizal Park are:\n- National Commission for Culture and Arts or NCCA (A small, contemporary art museum with free entry)\n- San Agustin Church and Museum (A historical landmark, completed in 1607)\n- Casa Manila Intramuros (This restored 1850s house-turned-museum is a display of the Spanish colonial era)\n- Bahay Tsinoy (Museum of Chinese in Filipino life)\n- Manila Cathedral (a.k.a. the Minor Basilica and Metropolitan Cathedral of the Immaculate Conception)\n- Plaza Moriones (a pleasant public square where you can rest your weary feet)\n- Fort Santiago (the landmark defensive fortress is now a museum commemorating national hero José Rizal)\nBy the way, Intramuros is also where you will find the Bureau of Immigration. For those who want to extend their stay in the Philippines beyond 30 days, here’s how you can blend a drab visa renewal with some colorful exploration.\nWho is José Rizal?\nIn Intramuros, José Rizal is the man of the hour. But who is he?\nIn short, Rizal was a physician, Filipino nationalist, and writer who lived during the Spanish colonial era. He was executed in what is now Rizal Park for inciting rebellion. Today, he is remembered as one of the first martyrs and heroes of the Philippine Revolution.\nFinal stop: Chinatown\nIf you have the time and energy to continue your exploration of Manila, cross the bridge and head into Chinatown for food and other Chinese things.\nWhen you’re ready to leave Intramuros, head to LRT Central Station or catch a bus at Plaza Lawton. If you’re leaving from Chinatown, you might be closer to LRT Carriedo Station.\nIf you’re tired of walking or simply want to have this quintessential Filipino experience, you can jump on any jeepney with the names of these places hanging in its window.\nIf you’re the kind of tireless traveler who can keep wandering all day, you can venture from Chinatown into Manila’s notorious Divisoria Market.\nThink of it as Baclaran’s big brother. You can find literally everything here, and at the lowest prices.\nThings to Keep in Mind in Manila\nGetting Around Manila Takes Time\nTransport in Manila is not excellent. The MRT and LRT lines only cover certain areas and there is always traffic—whether motor vehicle or moving people.\nYou can use Grab (Southeast Asia’s Uber) but may find that there are sometimes no available drivers, and at other times your phone will have no signal or data.\nPro Tip: Try Angkas (Uber on two wheels). Download the app on your phone and call a motorcycle rider to pick you up and take you to your destination. Cheaper and generally faster than Grab because you can weave through traffic.\nThen there’s public transportation but navigating this complex network of tricycles, jeepneys, and busses is a feat entirely of its own and one I still struggle with at times.\nRegardless of your mode of transportation, roads are crowded with cars, bikes, buses, and jeeps. The metro is often crowded with people.\nUltimately, you’ll want to arrange a place to stay that is close to the sights you want to see and where you need to be the next day.\nPersonal Safety and Blending in\nPeople often ask me if I think Manila is safe. Personally, I have no reason to believe it is not. The worst confrontation I had in eight years here was having my phone snatched out of my pocket while getting on a crowded bus.\nSince then, I’ve learned to keep a closer eye on my belongings. But at the end of the day, I’ve found Filipinos to be friendly, welcoming, and decidedly non-confrontational.\nUltimately, I would say that simply not standing out goes a long way in making you a non-target for opportunist crime in any circumstances.\nSo, in closing, here are a few quick tips for personal safety and blending in when exploring Manila’s crazy side. I’m talking specifically about being in markets like Baclaran and Divosria, or on crowded public transport.\n- Keep your bag and valuables close, ideally in front of you.\n- Avoid keeping valuables in your pockets.\n- Don’t wear flashy, expensive jewelry or accessories such as watches.\n- Dress simply and practically. In general, when wandering Manila, it’s best to try and not flaunt yourself as a rich foreigner or naive tourist.\n- To get local prices, ask how much something costs in Tagalog by saying “Magkano ‘to?” or “Magkano po ito?” to be polite.\n- You can always be polite—and blend in—by adding “po” when addressing Filipinos: “Hello po”, “Thank you po”, “OK po.”, “Ang mahal naman po.” (It’s too expensive po).\nPro Tip: It’s “poh”, not “poe”.', ""- Luzon Philippines Museums\n- National Museum of the Philippines (Manila)\n- National Museum of Natural History\n- National Museum of Fine Arts\n- National Museum of Anthropology\n- Las Casas Filipinas de Acuzar (Bataan)\n- Ayala Museum (Makati)\n- BenCab Museum (Baguio)\n- Pinto Art Museum (Rizal)\n- San Agustin Museum (Intramuros, Manila)\n- Aguinaldo Shrine and Museum (Cavite)\n- The University of Sto. Tomas Museum of Arts and Sciences (Manila)\n- Palawan Special Battalion WW2 Memorial Museum (Puerto Princesa)\n- Visayas Philippines Museums\n- Motag Living Museum (Aklan)\n- Balay Negrense Museum (Silay)\n- Museo Sugbo (Cebu)\n- Camiña Balay na Bato (Iloilo)\n- The Negros Museum (Bacolod)\n- National Museum Bohol (Bohol)\n- Casa Gorordo (Cebu)\n- Mindanao Philippines Museums\n- D’Bone Collector Museum (Davao)\n- Capitol University Museum of Three Cultures (Cagayan de Oro)\n- Davao Museum of History and Ethnography (Davao)\n- Balangay Shrine (Butuan)\nPhotos by the National Museum of the Philippines and the Motag Living Museum\nSprawling across over 7,000 islands of Philippine geography are pristine beaches and marine sanctuaries. But besides the beautiful beaches in the Philippines, there are cultural museums spread throughout Luzon tourist spots, Visayas tourist spots, and Mindanao tourist spots.\nThese world-class museums in the Philippines showcase the country’s rich culture and history. With 300 years of Spanish colonization, several heritage provinces have witnessed wars and other cultural activities in the Philippines.\nDuring your museum visit, you’ll get introduced to the country’s colorful culture that is evident in Philippine festivals, arts, and food with Pampanga as the top culinary tourism destination in the Philippines. Amid the lush greenery are breathtaking Philippine mountains that are home to tarsiers, and diving spots, which serve as a natural habitat for whale sharks.\nThe best time to visit the Philippines is during the dry season, from December to May. While the Philippines is usually sunny all-year-round, there are days when visiting museums is one of the best rainy season activities/destinations. Read on to find out what awaits you in the famous museums in the Philippines.\nLuzon Philippines Museums\nFor first-time travelers, your gateway to the Philippines’ top tourist destinations is Ninoy Aquino International Airport (NAIA) or Manila Airport, one of the Philippine airports located in Manila, the capital city of the Philippines. It is located on the island of Luzon, the country’s largest island, home to notable spots like Palawan’s stunning beaches and the Summer Capital of the Philippines, Baguio.\nNational Museum of the Philippines (Manila)\nPhoto by the National Museum of the Philippines\nRecognized as the best museum in the Philippines, The National Museum (Pambansang Museo) of the Philippines is a neoclassical building that holds the best repository of historical artworks in the Philippines. Within the complex are three museums worth visiting:\nNational Museum of Natural History\nRepurposed in 2018, the National Museum of Natural History showcases notable Philippine flora and fauna exhibits such as the Tree of Life and Rainforest Diorama. Its biggest attraction is the skeleton display of Lolong, the Philippines' largest crocodile held in captivity.\nNational Museum of Fine Arts\nPhoto by the National Museum of the Philippines\nEstablished in the early 1920s, the National Museum of Fine Arts features priceless masterpieces of famous Filipino artists such as Guillermo Tolentino and Félix Resurrección Hidalgo. Inside, you can find the Spoliarium painting by renowned 19th-century Filipino painter Juan Luna.\nNational Museum of Anthropology\nFeaturing the cultural artifacts in the Philippines, the National Museum of Anthropology is home to ethnographic materials and agricultural tools such as baskets, textiles, and weapons. The museum showcases portraits of indigenous women and their livelihood, including a collection of jars and treasures.\nLas Casas Filipinas de Acuzar (Bataan)\nPhoto by Las Casas Filipinas de Acuzar\nSurrounded by century-old homes, Las Casas Filipinas de Acuzar is the first-ever heritage resort and museum featuring 22 colonial mansions reminiscent of 17th-century Hispanic-Filipino history. It is situated in Bagac, Bataan where historical events happened, including the World War II Death March.\nSome of the homes were replicas reconstructed brick by brick, including the Casa Cagayan and Hotel De Oriente, the Philippines’ first luxury hotel in Binondo. Las Casas Filipinas de Acuzar is one of the romantic places in the Philippines, where couples can have a lavish dinner under the stars while being serenaded by live kundiman.\nAyala Museum (Makati)\nPhoto by Ayala Museum\nNestled at the heart of the central business district in Makati City, Ayala Museum is among the private museums in the Philippines filled with art galleries. Inside one of the modern museums in Manila are several artworks of Fernando Amorsolo, the first-ever recognized National Artist of the Philippines.\nThis elegant museum in Metro Manila showcases the Diorama Experience or the visual narration of ancient and contemporary historical times with exquisite and intricate wood-carved installations made by craftsmen from Paete, Laguna.\nBenCab Museum (Baguio)\nPhoto by the Department of Tourism\nWith its chilly ambiance, Baguio, or the City of Pines is among the best places to visit in the Philippines. If you’re heading up north, drop by BenCab Museum included in Baguio tour packages. This cultural museum in the Philippines is notable for its art show venues, galleries, and exhibitions.\n- See destinations for solo travel in the Philippines\nThe museum holds National Artist Benedicto Cabrera’s (BenCab) annual collection of paintings, historical photos of the Philippines, and artworks depicting Cordillera and its indigenous people.\nPinto Art Museum (Rizal)\nPhoto by the Department of Tourism\nThe Pinto Art Museum is a famous art museum in the Philippines located in Antipolo City, a popular weekend getaway destination just 1.5 hours away from Manila. It is hailed as one of the world’s most Instagrammed places, so make sure you don’t miss it when booking day tours near Manila.\nAs one of the popular museums in Antipolo and tourist spots in Antipolo Rizal, it showcases Filipino artist Antonio Leaño’s architectural design and owner Dr. Joven Cuanang’s vast collection of local art. Visitors can take endless selfies at the museum’s labyrinth-like corridors and scenic lounges. It also houses one of the must-visit Antipolo coffee shops.\nSan Agustin Museum (Intramuros, Manila)\nPhoto by the San Agustin Museum\nFor those planning to take Manila tours, make your day tours in Manila special by strolling along the walled city of Intramuros. Apart from exploring the cobbled streets and architectural landmarks in the Philippines, your historical Intramuros Manila tour comes with a visit to the San Agustin Museum, one of the UNESCO heritage sites in the Philippines.\nSan Agustin Church is among the country’s oldest Catholic structures, which holds a monastery and museum. It features ecclesiastical art such as crucifixes, antique furniture, wooden statues, and altars.\nAguinaldo Shrine and Museum (Cavite)\nThis national shrine in Kawit, Cavite served as the site for the Philippine declaration of independence from Spain on June 12, 1898. As one of the historical shrines and museums in the Philippines, the Aguinaldo Shrine and Museum feature antique furniture intricately designed with motifs of the Philippine flag.\nThe shrine was also the ancestral birthplace of Emilio Aguinaldo, the Philippines’ first president. Currently, the museum showcases an interactive hologram of the former president and 2D wall exhibits depicting Philippine history.\nThe University of Sto. Tomas Museum of Arts and Sciences (Manila)\nPhoto by UST Museum\nEstablished in 1869, the University of Sto. Tomas Museum of Arts and Sciences is the oldest existing museum in the Philippines. It showcases cultural artifacts, including those preserved for more than 300 years.\nInside are ethnographic materials, memorabilia, religious images, oriental ceramics, and paintings from the 17th to 20th centuries. Hung on the walls are paintings and murals of Filipino artists such as Carlos “Botong” Francisco, Galo Ocampo, and Vicente Manansala.\nPalawan Special Battalion WW2 Memorial Museum (Puerto Princesa)\nPhoto by Palawan Special Battalion WW-II Memorial Museum\nOne of the historical landmarks in Puerto Princesa City is the Palawan Special Battalion WW2 Memorial Museum, which was established in honor of Palawan’s local hero and guerilla soldier Dr. Higinio Mendoza Sr. who lost his life fighting against the Japanese troops.\nThe museum also commemorates a thousand guerillas, known as the Fighting 1000 or Palawan Special Batallion who fought with Mendoza during the Second World War. Inside the museum are war relics such as bombs, bayonets, ammunition, and actual vehicles used back then.\nVisayas Philippines Museums\nSeveral Philippine tourist spots are nestled in the Visayas Island group, the central region in the Philippine archipelago. It consists of seven large islands, including Cebu, Bohol, Masbate, Negros, Samar, and Panay. Notable destinations like Boracay’s White Beach and the stunning geological formation of Chocolate Hills in Bohol are here.\nMotag Living Museum (Aklan)\nPhoto by Motag Living Museum\nBesides the world-famous beach of Boracay, there is a hidden cultural gem in Aklan province, the Motag Living Museum. This first-ever interactive museum in the Philippines lets you immerse in the traditional livelihood of Aklanons when the island was the main producer of dried fish, tobacco, kopra, and rice.\nThe living museum, a Bahay Kubo built with bamboo tadtad, was inspired by the Open Air Museum Ballenberg in Brienz, Switzerland, and interactive museums in South Korea. Upon arriving at the museum, farmers dressed in patadyong or local farming costumes will welcome you inside the hut where elder women grind rice and weave baskets.\nBalay Negrense Museum (Silay)\nPhoto by Balay Negrense Museum\nNegrense Museum or Balay Negrense is the former abode of Victor Fernandez Gaston, a sugar baron in the 19th century. It is located in Silay City, dubbed as the Paris of Negros Occidental due to its well-preserved European-inspired structures, that you can visit in Bacolod tours.\nThe Bahay-na-Bato residence soon turned into a lifestyle museum with 12 bedrooms and two floors. Its house pillars and floorboards are made of ironwood or “balayong”. Surrounding the spacious garden are flowers, old trees, and a water fountain with memorabilia, antique items, and clothes from the 20th century.\nMuseo Sugbo (Cebu)\nMost cultural and historic attractions in Visayas are found in Cebu, the Queen City of the South, or the oldest city in the Philippines. One of which is Museo Sugbo, also known as the Cebu provincial museum, which is included in our Cebu City tour and Cebu tour package.\nMuseo Sugbo used to be Carcel de Cebu or the provincial jail of Cebu. In 2008, the local government inaugurated Cebu Provincial Museum as the repository of the Cebuano heritage. Inside the museum are signatures of former Philippine presidents engraved in gold-plated metals. You can also find ceramics from the 1600 San Diego shipwreck.\nCamiña Balay na Bato (Iloilo)\nPhoto by Camiña Balay nga Bato in Iloilo\nIloilo is home to the Islas de Gigantes, one of the best islands in the Philippines. Aside from scenic countryside sights, the province also showcases iconic historical museums in the Philippines. Among the remarkable sites included in your Iloilo tours is the Camiña Balay na Bato, an ancestral house and museum in Iloilo.\nThe name, Camiña Balay na Bato, means “house by the river” and is the former residence of Fernando Avancena and his wife Eulalia Abaja. Inside the living museum is a dining area where you can feast on Ilonggo delicacies such as Pancit Molo (wonton soup) and hot chocolate with Biscocho or local biscuits.\nThe Negros Museum (Bacolod)\nPhoto by The Negros Museum\nConstructed in 1987, the Negros Museum features contemporary historical art in the Philippines displayed in Bacolod City where the colorful MassKara Festival is held. This attraction is among the notable destinations included in our Bacolod tours.\nThe Negros Museum depicts the Negrense lifestyle and culture through precious possessions and items loaned from ancestral houses of the province. There is a souvenir shop inside the museum where you can purchase memorabilia and keepsakes.\nNational Museum Bohol (Bohol)\nPhoto by National Museum Bohol\nBeing a haven of natural wonders, Bohol is home to the famous Chocolate Hills that are part of our Bohol tour packages. When exploring the province, don’t forget to check out other scenic Bohol tourist spots included in our Bohol tours. Among the must-visit historical attractions here is the National Museum Bohol, featuring a vast artwork collection of Boholano artists.\nThe museum used to be the site of the Provincial Capitol Building in 1860. However, it was reconstructed due to the 2013 earthquake. The museum gives insights into Bohol's prehistory times. It also houses the relevant works of the National Artist for Sculpture, Napoleon Abueva.\nCasa Gorordo (Cebu)\nOne of the museums in Cebu worth a visit is the Casa Gorordo, a two-storey house named after Juan Gorordo, the first native Cebuano bishop. The ancestral residence was built in the 19th century, and served as a home to the four generations of the Gorordo family from 1863 to 1979.\nInside are antique furniture, household items, paintings, decorative artworks, and farming tools. You can also find wooden carvings and plant cultivating tools used during the Spanish era.\nMindanao Philippines Museums\nKnown as the Philippines’ second-largest primary island, Mindanao is a mountainous region where you can find the conservation area for the Philippine Eagle, the country's national bird, in Davao City or Mindanao’s largest city.\nD’Bone Collector Museum (Davao)\nPhoto by D' Bone Collector Museum Inc.\nNot only does Davao serve as a sanctuary to vast living species, but also serves as a repository of animals in skeletal form. Over 700 specimens of bones of various animals around the world are displayed at the D’Bone Collector Museum, the first of its kind among the local museums in the Philippines.\nOpened in 2012, the museum was owned and managed by an American bone collector named Darrell Dean Blatchley. Some of the animal remains were collected from Indonesia, Russia, Thailand, and Canada.\nCapitol University Museum of Three Cultures (Cagayan de Oro)\nPhoto by Capitol University Museum of Three Cultures\nSet inside the prestigious campus of Capitol University in Cagayan de Oro, the Museum of Three Cultures preserves the significant cultural and historical artifacts of the Muslim Maranao and other indigenous tribes in Northern Mindanao.\nThe museum is divided into three parts: a Gallery tour, Art exhibitions, and a Souvenir shop. Inside you can find rare antiques, furniture, jewelry, musical instruments, and weapons. There is also a section dedicated to the Lumad arts and crafts.\nDavao Museum of History and Ethnography (Davao)\nDive into the cultural heritage of Mindanaoans at the Davao Museum of History and Ethnography, one of the famous museums in Davao. It features priceless artifacts, including a giant fossilized clam from Samal Island, trade jars, burial urns, handicrafts, and pottery.\nAs an ethnic research center, this museum also gives a glimpse of Davao’s historical timeline that is still relevant to this day. There are also plenty of dioramas depicting the life and healing rituals of the Bagobo tribe.\nBalangay Shrine (Butuan)\nDuring the Spanish era, trading in the Philippines was actively rampant. Balangay or the first wooden boats in Southeast Asia were mainly used for trading in Butuan City, Agusan del Norte.\nThe Balangay Shrine was built to preserve the remnants of the pre-Hispanic watercraft, dating back to 320 AD. Inside the museum are wooden coffins found in the same location as the Balangay boat. No other Balangays were excavated elsewhere in the Philippines, aside from the other one displayed at the National Museum in Manila.\nExplore the Best Museums in the Philippines\nIf you’re looking for date ideas or indoor recreational activities in the Philippines, start booking vacation packages for couples in the 5-star hotels in the Philippines and go museum-hopping to learn more about the country’s rich cultural heritage, one of the reasons why you should visit the Philippines.\nWhen booking a hotel in the Philippines, enrich your experience with Philippines long packages and go for an extended stay in the hotels in the Philippines. Find affordable rent a car in the Philippines for a convenient transfer from hotel to airport, and discover exhilarating travel packages to the Philippines to make the most of your trip.\nTour the Philippines with your loved ones and see the best places in the Philippines for family vacation when you book all inclusive family vacation packages. For Philippine tour packages and travel promos, visit Guide to the Philippines, an accredited Philippine travel agency. Don’t forget to check out the latest Philippines travel requirements for a hassle-free vacation.\nBest Palawan Guide: Top Tours, Where to Stay, How to Get Around\n20 Most Beautiful Sandbars in the Philippines: White Sand, Longest, Vanishing\n15 Best Tropical Bali-Like Resorts in the Philippines: Near Manila, Siargao, Cebu, Bohol\n10 Best Treehouse Resorts in the Philippines for a Scenic Getaway at Mountains, Beaches & Rivers\nOther interesting articles\n15 Best Philippines Food Tourism DestinationsIMPORTANT: The destinations featured in this article may have COVID-19 travel restrictions and requirements. Check our COVID-19 travel advisory page for details. While travelers often take inter...Read more\n15 Best Historic Hotels in the Philippines: Oldest Hotels & Ancestral Houses to Stay InPhotos by Las Casita Mercedes and Manila Hotel The Philippines is a relatively young country, but its history is dramatic and colorful. With more than 300 years of colonial history and cultural tie...Read more\n10 Best Family-Friendly Museums in ManilaDiscover the Philippines' history, arts, and culture with these museums in Manila. What are the best museums to visit in Manila and nearby cities today? What are the unique attractions and activities...Read more""]"	['<urn:uuid:d5acb6bd-0772-4307-bd49-c550ecfd1aaf>', '<urn:uuid:35912756-4221-4db2-83cf-f15712add3a2>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T00:20:10.754925	13	94	4266
81	How does AI help detect things in images, and how is it used for cancer?	CNN-based models are used for detecting and classifying objects in images, with region proposal networks generating bounding boxes over localized areas. In medical applications, this same deep learning technology has been adapted to analyze microscopic images of breast tumor biopsies, where it can predict cancer properties like aggressiveness and molecular subtypes by recognizing patterns in tissue samples stained with hematoxylin and eosin.	['On Generalizing Detection Models for Unconstrained Environments\nObject detection has seen tremendous progress in recent years. However, current algorithms don’t generalize well when tested on diverse data distributions. We address the problem of incremental learning in object detection on the India Driving Dataset (IDD). Our approach involves using multiple domain-specific classifiers and effective transfer learning techniques focussed on avoiding catastrophic forgetting. We evaluate our approach on the IDD and BDD100K dataset. Results show the effectiveness of our domain adaptive approach in the case of domain shifts in environments.\nObject detection has been a widely studied task in computer vision. It is focussed upon classifying objects present in an image and then regressing bounding boxes over the localized proposals. We have seen remarkable results with CNN based models[NIPS2012_4824] on the COCO dataset[DBLP:journals/corr/LinMBHPRDZ14] [DBLP:journals/corr/abs-1805-09300] [DBLP:journals/corr/abs-1904-08189] [DBLP:journals/corr/abs-1901-01892] [DBLP:journals/corr/abs-1904-11492]. Recently, [DBLP:journals/corr/abs-1906-02659] showed that when commonly used detectors are evaluated on nonstandard settings of objects in an environment, they tend to provide unusual predictions. This is also applicable for autonomous navigation systems operating in unstructured environments (e.g drivable areas except roads etc.) as well. Current detection methods don’t generalize well when they encounter diverse environmental conditions.\nWe witness variety of environmental conditions when it comes to driving such as weather changes, dynamic changes in the surrounding environment, etc. Current detectors have been tested on data obtained from structured environments which are often not representative of real-world conditions. As a result of which, the need for data obtained from nonstandard sources is felt the most for data-driven algorithms to improve and test their generalizing capabilities.\nAutonomous navigation algorithms must perform well on multiple domains especially the ones with corner cases for safety purposes. Most importantly, we want to be able to learn from a large standard data distribution to efficiently learn features in an embedding space and learn progressively from domain-specific data without having access to earlier used data.\nIn this paper, we address the problem of incremental learning and domain adaptation to some extent for object detectors to improve generalizing capabilities. Specifically, we tackle the problem of adapting from a standard data distribution to data obtained from the unstructured environment. We also provide baseline results on IDD and BDD100K for object detection task to compare our proposed methods.111Code for this work can be found here\n2 Related work\nObject Detection: Region proposal based methods introduced in [DBLP:journals/corr/GirshickDDM13] have been widely used as object detectors. It made use of selective search to reduce the number of bounding boxes. Spatial Pyramid Pooling Nets [DBLP:journals/corr/HeZR014] could generate a fixed-length representation in a dynamic manner irrespective of image scale. Fast RCNNs [DBLP:journals/corr/Girshick15] made use of regression for bounding box predictions. [NIPS2015_5638] made use of RPNs and introduced anchor boxes to deal with different aspect ratios and scales. SSD [DBLP:journals/corr/LiuAESR15] method runs a CNN on input image only once and calculates a feature map that doesn’t require proposal generation steps. Stereo RCNNs [DBLP:journals/corr/abs-1902-09738] extends the use of Faster RCNN with stereo images for 2D and 3D bounding box predictions. It is a region proposal based network that works without the need for point clouds. Our approach can also be extended for 3D object detection similarly but we still lack the diversified ground truth data (such as 3D bounding box coordinates or Lidar point clouds obtained from unconstrained environments) for 3D detections.\nLearning from multiple distributions: The concept of making generalizable deep learning models has been widely studied. This often involves retaining what the model has learned in the past and performing incremental learning on multiple domains. [inproceedings] used a GAN[NIPS2014_5423] to approximate the feature distribution in the source domain. [DBLP:journals/corr/LiH16e] [DBLP:journals/corr/abs-1808-06281] addressed the task of incremental learning with architectures that inhibit loss of learned knowledge.  made use of a larger network to train a smaller network to generate close predictions. [DBLP:journals/corr/CourtyFTR15] treated the task of domain adaptation as an optimal transport problem.\nFaster RCNN: It takes an RGB image as an input. The model consists of a feature extractor followed by a feature pyramid network (FPN) and region proposal network (RPN) for generating region proposals which are then used to detect objects. RPNs are more efficient than selective search. They perform a ranking of anchor boxes to reduce their number and propose those which most likely contain an object. Image features are generated by a backbone network which is then fed to an RPN along with images and targets for generating proposals. After RPN, we get proposed regions with different sizes. Region of Interest (ROI) classifier predicts the category label obtained by using ROI Pooling. RPN can output differently sized regions. ROI Pooling can simplify the problem by reducing the feature maps into the same size. The loss is the sum of classification and regression loss defined as:\nWe refer readers to [NIPS2015_5638] for further details about model architecture.\n4 Baseline Model\nWe use a region proposal based approach for the baseline model. For feature extraction which is used by RPN, we use ResNet50[DBLP:journals/corr/HeZRS15] followed by FPN pretrained on COCO. Linear layers after ROI Head were adjusted as per the number of classes. Our RPN generates 5 x 3 anchors per spatial location with 5 different sizes and 3 different aspect ratios. We used random horizontal flipping for augmenting the input data. Our baseline model is trained on a non HQ image set from IDD with batch size set to 4 for 5 epochs per camera orientation. It was optimized using SGD [Sutskever:2013:IIM:3042817.3043064] with momentum and weight decay set to 0.9 and 0.00004 respectively. The learning rate was initially set to 0.001 with the Cyclical learning rate scheduler[DBLP:journals/corr/Smith15a]. We use the same process for performing training on BDD100K. Results are shown in Table 1 and Table 2.\n5 Incremental Learning\nThe following section contains a description of training methodology and proposed transfer learning techniques aimed at minimizing catastrophic learning while adapting to target data distribution.\nOur task is to perform incremental learning on multiple diverse data distributions. The network initially learns the weights from a standard data distribution and the proposed techniques help in performing domain adaptation while remaining consistent with the already learned information. Once trained on one distribution, we don’t require already used data.\n5.1 Domain specific heads\nWe make use of two ROI heads which are combined with the common backbone and RPN for generating domain-specific predictions. We can also have more than two ROI heads depending upon the number of target domains we want to adapt to. The weights of RPN and feature extractor are shared across all domain-specific classifiers. Weight sharing allows the network to learn common features with the proposed techniques across all domains without any increment in the number of parameters. Domain-specific heads also help in cases where classes don’t overlap in both distributions, as in this case.\nAfter the addition of ROI Head to the baseline model, we train the head on to learn domain-specific weights. This is followed by progressive training of other components of the network to avoid catastrophic forgetting as proposed in [howard2018universal] to learn domain invariant features.\n5.2 Discriminative finetuning\nWe use different learning rates to train different layers of our network. As shown in [NIPS2014_5347], different layers of the network are responsible for capturing different types of information. Discriminative finetuning allows us to set the rate at which these different components of the network learn. Since the weights of the backbone and RPN are being shared for all tasks, we want to inhibit the loss of learned information. We use a higher learning rate for domain-specific components and a lower learning rate for components whose weights are being shared. Specifically, we require a lower learning rate for the backbone and RPN since feature extraction and generation of region proposals are common tasks across all domains and a higher learning rate for domain-specific ROI Heads. A general SGD update of a model’s parameters at time step looks like:\nwhere denotes learning rate and denotes gradient with respect to model’s objective function. We split model’s parameters into where contains parameters of the model at the -th layer and denotes the total number of layers of our network. The SGD update then becomes:\n5.3 Gradual unfreezing\nTraining the entire model on a different domain at once leads to catastrophic forgetting, which means the model adapts itself to the target domain on which it is being tuned compromising the performance on source domain on which it was trained. We overcome this issue by gradually unfreezing the components of the network with discriminative finetuning. We freeze all the components initially and unfreeze the domain ROI Head which is fine-tuned until convergence followed by progressive unfreezing and finetuning of FPN and RPN.\n5.4 Cyclical Learning Rate\nWe optimize our network using the Cyclical learning rate (CLR) as proposed in [smith2017cyclical]. Instead of having a gradually decreasing learning rate, as the training converges, we use CLR which cycles the learning rate between lower and upper bound. CLR helps in oscillating towards a higher learning rate wherever necessary. It prevents the network from converging at some poor local minima in loss landscape. We make use of triangular variation for our experiments.\nIn this section, we evaluate our proposed approach on two diverse datasets. One dataset denotes structured environments and the other one denotes unstructured and unconstrained environments to which we want to adapt. The later one simulates high traffic density, rural areas with no proper roads, classes usually not seen in other datasets posing a much harder task for current object detection models.\nIDD: We use IDD for target adaptation tasks. It provides data for object detection in two resolutions. The non HQ set consists of 27072 images taken from 5 different orientations of the camera with two resolutions 964x1280 and 1080x1920. The HQ set consists of 14722 images with two resolutions 720x1280 and 1080x1920. There are 15 classes for this task. Note that we only perform training and evaluation on the non HQ set of IDD. Results can be further improved if high res images from the HQ set are used to train the components of the network. The validation set consists of 10,225 high-resolution images.\nBerkeley Deep Drive: We use BDD100K [DBLP:journals/corr/abs-1805-04687] to denote data distribution obtained from structured environments. We only use the images and their respective ground truths for the detection task. There are 69863 images in train and 10000 in the validation set. We trained our proposed model over 12 classes.\n6.2 Training methodology\nThe proposed architecture has been shown in LABEL:fig:fig_model. This architecture is based on Faster RCNN. The backbone is a ResNet50 pretrained on COCO. We use a batch size of 16. We use the same baseline model with an additional ROI Head. We obtain four feature maps from the batch of images obtained by intermediate layers of backbone to perform multi-scale ROI aligning. These feature maps are shared across all components. The obtained feature maps are then fed to an RPN for generating region proposals followed by domain-specific ROI pooling and prediction layers. While training and inference, only the designated ROI Head is used for the respective domain. This model is trained in an end to end strategy and inference can be performed in a regular manner with learned weights.\nIn the following section, we evaluate the effects of each of the mentioned techniques along with the effect of varying learning rates. As per convention, we use BDDIDD to denote BDD100K as and IDD as . Since we have more data collected from structured environments [DBLP:journals/corr/abs-1803-06184] [DBLP:journals/corr/abs-1805-04687] [Geiger2012CVPR] , the results simulate learning from already existing data distributions to adapt to unstructured environment.\nAdding domain specific head Here we use the same baseline model for BDD. We add a domain-specific head as proposed in Figure 3. In this case, we only perform finetuning of this head on . Apart from the domain-specific head, the rest of the components of the network are kept frozen.\nBy introducing the domain-specific head and training it for 5 epochs, we see a considerable performance on without any performance decrement on . BDDIDD indicates that we use the baseline model trained on BDD100K with the specified method. While reporting for IDDBDD, we only change the domain-specific head, the rest of the network stays the same. The same model can achieve an mAP of 24.3% on IDD and 45.7% on BDD. Results are shown in Table 3\nDiscriminative finetuning and Gradual unfreezing We use the same network and weights as in the previous step. Here, we introduce both techniques. In Table 3, active components denote those components of the network whose weights are being updated during the training process. We experiment with different learning rates with progressive addition of active components with different bounds of learning rate during each step.\n|and||Epoch||Active components (with LR)||LR Range||mAP (%) at specified epochs|\n|BDDIDD||5||+ROI Head(1e-3)||1e-3, 6e-3||24.3|\n|BDDIDD||5,9||+RPN (1e-4)||1e-4, 6e-4||24.7, 24.9|\n|IDDBDD||Eval||+ROI head (1e-3)||-||45.3, 45.0|\n|BDDIDD||1,5,6,7||+RPN (1e-4)||1e-4, 6e-3||24.3, 24.9, 24.9, 25.0|\n|IDDBDD||Eval||+ROI head (1e-3)||-||45.7, 44.8, 44.7, 44.7|\n|BDDIDD||1,5,10||+ROI head (1e-3)||1e-4, 6e-3||24.9, 25.4, 25.9|\n|IDDBDD||Eval||+RPN (4e-4) +FPN(2e-4)||-||45.2, 43.9, 43.3|\nLearning rate plays a very crucial role in determining the performance increment on and retention of learned information. Domain-specific components require a higher learning rate as compared to shared components. The learning rate range plays a crucial role since it determines the rate at which weights change in all active components. In our experiments, we found this range 0.0001-0.006 for the learning rate to work well. As experimental results show, with a little decrement in performance on , our model retains near similar performance on after being trained on . In some cases, we saw an increment in performance on while maintaining the same performance on . This shows that these transfer learning techniques complement each other and are effective in inhibiting information loss while adapting to diverse target distributions.\nIn this paper, we use an incremental learning approach and demonstrate the effectiveness of our method on data obtained from unconstrained environments. The main motivation behind this work is to demonstrate the effectiveness of our approach and encourage further research into building detection systems that generalize well on uncommon data distributions which are well representative of diverse real-world conditions. These proposed approaches can also be extended to other computer vision tasks as well.\nThe author would like to thank Intel AI for providing access to AI Devcloud as part of the Student ambassador program.', 'The same technology that powers Siri and face recognition on your iPhone has also found success in medicine. By automatically analyzing microscopic images of breast tumor biopsies, artificial intelligence may one day help guide cancer treatments.\nThis particular type of AI is called deep learning, and over the last few years has become a part of our everyday lives. Its applications continue to expand to areas like language translation and self-driving cars, enabled by massive repositories of data. While deep learning was first applied to recognizing people, cars, and other everyday objects in photographs, it has more recently been adapted to study cancer. Our team of computer scientists and cancer researchers at the University of North Carolina at Chapel Hill used it to analyze types of breast cancer from microscopic images of tumor tissue.\nPOWER AND SHORTCOMINGS\nDeep learning is a method of learning a new representation for images or other data by recognizing patterns. Also called a neural network, it consists of multiple layers of features where the higher-level concepts are built upon the lower-level ones. Going up the hierarchy, the features increase in both scale and complexity. Similar to human visual processing, the low levels detect small structures such as edges. Intermediate layers capture increasingly complex properties like texture and shape. The top layers of the network are able to represent objects like people.\nLearning these patterns allows the computer to make predictions. After training on a large data set containing content labels, the model can predict these labels on new data that it was not trained with. For example, given images of people and the location of the faces in each, the model can find faces in new photos.\nThe key factor in successfully training a neural network is a large amount of labeled data. Many state-of-the-art models are trained with tens or hundreds of millions of labeled images. The most commonly used public data set is ImageNet, which has 1,000 classes of objects and scenes that were gathered from photo-sharing sites like Flickr. However, in the medical field, patient samples are scarce and expert annotations of these samples are expensive. Training a large model on a small data set simply results in overfitting; the model performs well with the data it was trained on, but gives poor results when predicting on newly presented data.\nHowever, there’s a shortcut to apply such large and powerful models to small data sets: transfer learning. The same network that has been trained on millions of photographs of objects and scenes can be adapted to many other applications, including microscopic images of tissue. The network computes a representation on the new set of images, and a new model is trained to make a prediction for each image.\nDeep transfer learning works because many of the elements of images are the same across domains. The low levels of the network capture small structures like edges but are not powerful enough to distinguish complex image classes. The upper layers are very specific to the images on which they were trained, capturing things like faces and bicycle tires, but do not work very well on disparate image sets, such as medical images. The middle layers, however, are sufficiently powerful and generalizable, making them well-suited to new applications.\nPREDICTING BREAST CANCER SUBTYPES\nAs a computer scientist, I work to bring these exciting advancements in deep learning to further breast cancer research. Our team studies cancer subtypes: smaller groups that a type of cancer is divided into based on certain characteristics of the tumor cells. I computed neural network features on microscopic images of breast tumors and trained models to predict different properties, including aggressiveness and molecular subtype. My models successfully predicted these properties on an independent test set and may one day help to guide treatment decisions.\nOur team worked with a data set of microscopic images of breast tumor tissue. Each tissue sample was stained with a pair of stains called hematoxylin and eosin—or H&E—to turn different tissue structures pink or blue. A pathologist reviews such images to detect cancer and assign a measure of its aggressiveness, called grade. Other samples from each tumor are processed in different ways to determine molecular properties that can help identify weaknesses of individual tumors, in order to select appropriate therapies.\nMy models predicted two molecular properties and grade. We then compared my predicted values by deep learning with those assessed by other techniques. Neither molecular property was previously known to be predictable from H&E.\nWhile our grade measure replicates a pathologist, the other two provide new insights and potential cost savings for laboratories with limited resources. Standard methods for assessing molecular subtypes are costly but critical in determining the best course of treatment for a patient. Our image-based methods may one day provide an alternative.\nThe pink and blue H&E images of breast tumors are wildly different than photos of dogs, people and cars, but the same methods still apply because similar shapes and textures are present. And those vacation photographs you posted on Flickr that became a part of the ImageNet data set were essential in training the models that we use to study breast cancer. Transfer learning makes deep learning possible for many new tasks—from cancer to climate change—and continues to improve your day-to-day interaction with technology along the way.']	['<urn:uuid:b3776ee2-96a6-4ab3-984b-7b39fbb3053b>', '<urn:uuid:00e3e1b0-e889-4383-8678-62303b436e1e>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T00:20:10.754925	15	62	3269
82	How should fuel gas leaks in cylinders be handled safely?	If gas is leaking from the valve stem, first close the valve and tighten the gland nut. If the leak continues, the cylinder must be taken out of use, tagged, and removed from the work area. When fuel gas is leaking from the cylinder valve and cannot be shut off, the cylinder should be properly tagged and removed from the work area, unless a regulator attached to the cylinder valve can stop the leak through the valve seal. If a leak develops at a fuse plug or other safety device, the cylinder should be removed from the work area.	['Welding, cutting and brazing present a host of safety and health risks, including fire, explosion, toxic atmospheres, ultraviolet radiation, and electric shock. If you work on sites where “hot work” is performed, you need to be familiar with the relevant standards, even if your employees perform no such work themselves. Let’s look at the basics.\nMoving, Transport, and Storage of Compressed Gas Cylinders: Cylinder valves must be kept closed when work is not being performed, when the cylinders are empty, or they are being moved.\nA chain, cylinder truck, or other steadying device should be used to keep cylinders from being knocked over while in use. Cylinders should be secured in a vertical position, when being transported in a vehicle. Regulators must be removed, and valve protection caps must be in place, before cylinders are moved. Employees should never lift cylinders by their valve protection caps.\nCylinders should be kept a safe distance from welding operations, or behind fire-resistant shields. They must not be placed in confined spaces, or where they might become part of an electrical circuit.\nCylinders, full or empty, must not be used as rollers or supports.\nUse of Fuel Gas: Before being connected to a regulator, a cylinder valve should be “cracked” (opened slightly and closed at once) by a person standing to the side of it. Do not “crack” the valve where the gas would reach welding work, sparks, flame, or another source of ignition.\nCylinder valves should always be opened slowly, and those on fuel gas cylinders opened not more than 1-1/2 turns. If a special wrench is required, it must be left in position, to allow the fuel gas flow to be shut off quickly. In the case of manifolded or coupled cylinders, at least one such wrench shall be kept close by. Nothing should be placed on top of a fuel gas cylinder that might damage the safety device, or impede the quick shutoff of the valve.\nFuel gas should be used through devices equipped with shutoff valves only if the pressure is reduced through a suitable regulator attached to the cylinder valve or manifold. Before a regulator is removed from a cylinder valve, the valve must always be closed, and the gas released from the regulator.\nIf gas is found to be leaking from the valve stem, the valve shall be closed, and the gland nut tightened. If that does not stop the leak, the cylinder must be taken out of use, tagged, and removed from the work area.\nIf fuel gas is leaking from the cylinder valve, and cannot be shut off, the cylinder should be properly tagged, and removed from the work area, except if a regulator attached to the cylinder valve will stop the leak through the valve seal.\nIf a leak should develop at a fuse plug or other safety device, the cylinder should be removed from the work area.\nFuel gas and oxygen manifolds: Fuel gas and oxygen manifolds must bear the name of the substance they contain in letters at least 1” high, painted on the manifold, or on a sign permanently affixed to it. The manifolds must be in safe, well-ventilated and accessible locations (never within enclosed spaces).\nOxygen and fuel gas hoses must not be used interchangeably, and a single hose having more than one gas passage must not be used. Hose couplings must be of a type that cannot be unlocked or disconnected without a rotary motion. Boxes used for the storage of gas hose should be ventilated, and hose must be inspected at the beginning of every shift, and removed from use if damaged or malfunctioning.\nTorches: Torches must be inspected at the beginning of each shift for leaking shutoff valves, hose couplings and tip connections. They may be lighted by friction lighters or other approved devices, but not by matches, or from hot work.\nRegulators and Gauges: Oxygen and fuel gas regulators and their gauges must be inspected regularly, and kept in good working order.\nOil and Grease Hazards: Cylinders, cylinder caps, valves, couplings, regulators, hose and apparatus must be kept free from oil and greasy substances, and must not be handled with oily hands or gloves.\nWelding Cables and Connectors: Use only cable without repair or splices within 10 feet from the cable end to which the electrode holder is connected. Cables with standard insulated connectors, or with splices having an insulating quality equal to that of the cable, are permitted. Cables in need of repair must not be used.\nSafe Operation of Equipment: All equipment should be operated in a manner consistent with the manufacturer’s instructions. OSHA requires that, when electrode holders are to be left unattended, the electrodes must be removed, and the holders placed or protected so that they cannot make electrical contact with employees or conducting objects.\nWhen the cutter or arc welder has occasion to leave his work, or to stop work for any appreciable length of time, or when the arc welding or cutting machine is to be moved, the equipment’s power supply must be shut off.\nWhenever practicable, arc welding and cutting operations are to be shielded by noncombustible or flameproof screens, as protection from the direct rays of the arc.\nNaturally, faulty or defective equipment shall be reported to the supervisor, and taken out of service until repaired, or determined to be in good operating condition.\nVentilation and Protection: Hot work operations involving toxic substances must not be performed without adequate ventilation. Employees exposed to the same atmosphere must be protected in the same manner as the welders and burners themselves.\nMechanical ventilation can be either a “general mechanical ventilation system” or a “local exhaust system.” General mechanical systems must produce the number of air changes necessary to maintain welding fumes and smoke within safe limits (as defined in subpart D of 29 CFR 1926).\nLocal exhaust ventilation consists of freely movable hoods, placed as close as practicable to the work. Such ventilation must remove fumes and smoke, so as to keep concentrations within safe limits.\nLocal exhaust ventilation or air line respirators are required, when hot work is to be performed in an enclosed space, and involves lead (other than as an impurity), metals coated with lead-bearing materials, cadmium, or mercury. Both local exhaust ventilation and air line respirators are required, when the work involves beryllium-containing base or filler metals.\nContaminated air from a workspace shall be exhausted into the open air, or otherwise clear of the intake air source. All air replacing that which has been withdrawn must be clean and respirable.\nOxygen must not be used for ventilation, comfort cooling, blowing dust from clothing, or for cleaning work areas.\nConfined spaces: Generally, the foregoing ventilation requirements must be observed whenever hot work is done in a confined space.\nBy way of exception, if ventilation cannot be provided without blocking the means of access, employees in the confined space shall be protected by air line respirators, with an employee outside the confined space being assigned to maintain communication with those working inside it, and to provide assistance in case of an emergency.\nWhen an employee must enter a confined space through a manhole or other small opening, means (e.g., safety belts and lifelines) must be used, and an attendant, with a pre-planned rescue procedure, stationed outside, to extricate the welder or cutter should the need arise.\nProtective equipment: Employees performing hot work, or working nearby, must wear suitable eye protective equipment. All who are exposed to radiation must be covered completely, to prevent harm from ultraviolet rays.\nFire prevention: Articles to undergo “hot work” must be moved to a designated location whenever practicable. No hot work shall be done where the application of flammable paints, or the presence of other flammable compounds or dust concentrations, creates a hazard.\nFor more complete details concerning safety and health requirements for “hot work,” refer to Subpart J of 29 CFR 1926, and the publications and other OSHA standards referred to in the same.']	['<urn:uuid:d98f5476-a7b6-4885-a0eb-a6041295832f>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T00:20:10.754925	10	99	1331
83	I'm interested in learning about soil experiments - what kind of interesting test involves burying cotton underwear in the ground?	This is called the 'Soil your Undies' test, which involves burying an all cotton pair of underwear two inches deep in the soil. The test demonstrates soil health and agricultural practices by showing how much of the original cotton underwear remains after being buried. The amount of decomposition indicates different levels of soil activity.	['View All Posts\nPosted on February 19, 2020 at 9:20 AM by Ryan Hiniker\nDrainage update 2/18/2020\nUpcoming Drainage Hearings or Meetings:\nI was fortunate enough to be able to attend a local Soil Health meeting. The event was called “Lets Talk Dirt”. The soil health gathering was sponsored by the USDA (United States Department of Agriculture), Blue Earth County SWCD (Soil and Water Conservation District), Crystal Waters Project, NRCS (Natural Resources Conservation Service)\nand the Minnesota Soil Health Coalition.\nThis town-hall style meeting was hosted in Lake Crystal. The intended audience for this soil health meeting was aimed at being a mixture of government conservation groups, local water quality groups and actual landowners or growers. Mother-Nature decided to throw us a monkey wrench with the snowy weather, but that didn’t stop some of us from attending.\nThe demonstrations presented by staff from the NRCS were great. The three demonstrations at the meeting included; a rainfall simulation test, the Slake test and the soiled undies test. The last one, I’m sure you’re laughing at right now, but that’s really what it is. So, let’s explain these three tests quickly.\nThe rainfall simulation test is a way of illustrating the soil erosion effects of a one-inch rainfall event on two different soil scenarios. One scenario is soil that is practicing the cover crop method with active plant growth and the other scenario is an example of conventional tillage situations that have no cover crops. It was amazing the visual differences between the two scenarios.\nThe second test was the Slake test. The Slake test is designed to measure the stability of soils. This test is incredibly simple in design and execution but delivers a huge visual impact to the difference soil health plays.\nThe third and final test was the “Soil your Undies” test. This involves burying an actual all cotton pair of underwear two inches deep in the soil. The difference in soil activity from different soils and different agricultural practices was illustrated by how much of the original cotton underwear remained after burying them in the soil. Again, this was a huge mind-opening demonstration, as the visual effects of the tests speak volumes for soil health and thinking outside-the-box.\nA huge thank you to Crystal Waters Project and the rest of the folks that made this meeting possible. Not the greatest weather or the biggest turn out, but overwhelmingly worth the time to attend.\nClick the links below to view these experiments in more depth. Slake test: https://www.youtube.com/watch?v=GOos10UyRwY\nRainfall simulation test: https://www.youtube.com/watch?v=4IIoyFYEcBc\nSoil your Undies test: https://www.youtube.com/watch?v=D5Xm7rZmm3c\nRecent Drainage Inspections:\nI didn’t make it out last week to do any inspections, but I did make it out to our local ski hill over the weekend. I haven’t been on downhill skies in over 20 years. The first few runs down the hill where a bit intimidating, but I was feeling confident in no time. Just like riding a bike, might take you awhile to get the feel of it, but once you learn how to, you don’t forget. The only thing I forgot was how much fun it was to be out flying down those slopes. I guess I might have to add downhill skiing back into my hobbies group.\nWe require that all repairs to a county drainage system (tile or open ditch) beauthorized by one us in the drainage office, either Craig or myself, before any repairs are made.\nDrainage Management Specialist']	['<urn:uuid:b605e1b3-01b5-48a1-84a1-f179cfe1b201>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T00:20:10.754925	20	54	575
84	going to study in china monthly stipend amount phd masters bachelors scholarship money how much	The monthly stipend amounts for scholarship students in China vary by degree level: Bachelor's students receive CNY 2500 RMB per month, Master's students receive CNY 3000 RMB per month, and PhD students receive CNY 3500 RMB per month. In addition to these monthly stipends, students also get free tuition, Chinese/English preparatory study, accommodation and medical insurance.	['CSC Scholarship also knows as the Chinese Government Scholarship fully funded scholarships for international students willing to pursue their studies in China. For admission in Undergraduate, Graduate, and Ph.D. Prepare your documents as scholarships for thousands of students are about to be open in 250+ Chinses institutions.\nWith the cooperation of the Chinese Scholarship Council (CSC) presented thousands of scholarships every year, and now September is going to bring hundreds of fully-funded scholarships in more than 250 Chinses universities. For study under CSC scholarships, eligible students can apply by just submitting scholarship form and providing duplicates of documents required.\nBasic Requirements for Chinese Government Scholarship 2023\nHost Country People’s Republic of China\nTarget Countries International Students\nAffiliated universities 250+ Chinese universities\nDepartment All Departments\nLanguage(s) English and Chinese\nCertificates IELTS or HSK certificates (Not mandatory)\nMedical Certificate Required\nUndergraduate Graduate and Ph.D. students under CGS scholarships will get free tuitions, Chinese/English preparatory study, accommodation and medical insurance with a monthly stipend as described.\n|Bachelors||CNY 2500 RMB/month||–||Student should be a high school graduate||<25 years|\n|Masters||CNY 3000 RMB/month||2-3 years max.||Student should be a Bachelor’s degree holder||<35 years|\n|PhD||CNY 3500 RMB/month||3-4 years max.||Student should be a Master’s degree holder||<40 years|\nRequired Documents List (Duplicate)\n- Application Form\n- Two Recommendation Letter\n- Highest Degree/Transcript/Diploma( Should be notarized )\n- IELTS and English proficiency certificate\n- Medical Examination Certificate\n- Research Proposal\n- Passport size Pictures (blue/Black) background\n- Study Plan\nOptional Documents for CSC Scholarship\n- Acceptance Letter\n- Students having an acceptance letter will be preferred by high ranked universities.\n- Note: Write an email to a professor at the host university for acceptance with your research interest. ( Check How to Find any Professors Email List for CSC Scholarship)\n- Police Clearance Certificate\n- Character Certificate from your college or university\n- Curriculum vitae: 1-2 pages\n- Experience Letter: professional experience letter\nFollowing steps are needed to complete your Scholarship form:\n- CGS scholarship form should be downloaded from the CSC website and written in Chinese or English language.\n- Recommendation letter from at least two teachers has to be uploaded at the CGS portal for application. Make sure to send a copy of the letter instead of the letter itself.\n- Before starting with your application process get your required documents (diploma, degrees, and transcripts) attested from authorized public officers or a lawyer.\n- Students can apply for scholarships through the CSC portal depending upon the universities’ deadline when scholarships will be announced.\nNote for CSC Scholarship\n- In case of non-availability of provisional degree certificate can be used.\n- Hope certificate can be sued in case of a remaining degree.\n- Semester result cards can be used instead of the transcript at application submission time but make sure to get a transcript before applying for a visa.\n- Attested documents other than in Chinese or English language should be translated in either of the language and attached.\n- IELTS or HSK certificates provision is not mandatory but the provision of them will enhance chances of getting a scholarship. English proficiency certificate from last university stating student’s last university was completed in the English language should be provided.\n- The medical examination form can be downloaded from http://www.csc.edu.cn/studyinchina. Carry out mentioned tests from nearby government hospital stamp your picture from the doctor and attach that picture with form photocopy.\n- Research paper abstract consisting of 800 words for MS programs should be provided with scholarship form. Abstract should be in English or Chinese language.\n- Apply for a passport and submit it with the CSC scholarship application form.\n- Photographs attached with application form can be blue or black background.\n- Police clearance form from local police station should be attached with the application form to ensure clearance of criminal records.\nPrint online completed form attach all your important notarized documents and send the envelope to the desired university.\nChinese Government Scholarship Universities List\n|NORTHWESTERN PLOY-TECHNICAL UNIVERSITY||Shanghai Normal University|\n|Zhejiang Sci-Tech University||Shanghai International Studies University|\n|Peking University||Shanghai University of Finance and Economics SUFE|\n|RENMIN UNIVERSITY OF CHINA||Shanghai University of Sport|\n|TSINGHUA UNIVERSITY||Shanghai Conservatory of Music|\n|BEIJING JIAOTONG UNIVERSITY||Shanghai University|\n|BEIJING UNIVERSITY OF TECHNOLOGY||Nanjing University|\n|Beihang University||Soochow University|\n|Beijing Institute of Technology||Southeast University|\n|University of Science and Technology Beijing||Nanjing University of Aeronautics and Astronautics|\n|Beijing University of Chemical Tech||Nanjing University of Science and Technology|\n|Beijing University of Posts and Telecommunications||China University of Geosciences Wuhan|\n|China Agricultural Univ CAU||Wuhan University of Technology|\n|Beijing Forestry University||Huazhong Agricultural University|\n|Beijing University of Chinese Medicine||Huazhong Normal University|\n|Beijing Normal University||Zhongnan University of Economics and Law|\n|Capital Normal University||Xiangtan University|\n|Capital Institute for Physical Educations||Hunan University, Changsha|\n|Beijing Foreign Studies University||Central South University, Changsha|\n|Beijing International Studies University||Hunan Normal University, Changsha|\n|Beijing Language and Culture University||Sunyat-Sen University|\n|Central Univ of Finance and Economics||Shantou University|\n|Capital Univ of Business and Economics||South China University of Technology|\n|University of International Business and Economics||South China Agricultural University|\n|China Foreign Affairs University||Guangzhou University of Chinese Medicine|\n|Beijing Sports University||South China Normal University|\n|Central Conservatory of Music||Guangxi University|\n|Central Academy of Fine Arts||Guangxi Medical University|\n|The Central Academy of Drama||Guangxi Normal University|\n|Beijing Film Academy||Guangxi University for Nationalities|\n|Minzu University||Sichuan University|\n|The China University of Political Science and Law||Chongqing University|\n|North China Electric Power University||Southwest Jiaotong University|\n|Nankai University||University of Electronic Science and Technology of China|\n|Tianjin University||Chongqing University of Posts and telecommunications|\n|Tianjin Univ of Science and Technology||Southwest University|\n|Tianjin Medical University||The Southwestern University of Finance and Economics|\n|Tianjin Univ of Traditional Chinese Medicine||GUIZHOU UNIVERSITY|\n|Tianjin Normal University||YUNNAN UNIVERSITY|\n|Tianjin Univ of Technology and Education||KUNMING UNIVERSITY OF SCIENCE AND TECHNOLOGY|\n|Tianjin Foreign Studies Univ TFSU||YUNNAN NORMAL UNIVERSITY|\n|Liaoning University||YUNNAN UNIVERSITY OF FINANCE & ECONOMICS|\n|Dalian Univ of Technology||XI’AN JIAOTONG UNIVERSITY|\n|Northeastern University||XIDIAN UNIVERSITY|\n|Dalian Maritime University||CHANG’AN UNIVERSITY|\n|China Medical University||NORTHWEST A&F UNIVERSITY|\n|Dalian Medical University||SHAANXI NORMAL UNIVERSITY|\n|Liaoning Normal University||XI’AN INTERNATIONAL STUDIES UNIVERSITY|\n|East China Univ of Science and Technology||Northeast Agriculture University|\n|The University of Shanghai for Science and Technology||Harbin Normal University|\n|Donghua University||Fudan University|\n|Shanghai University of Traditional Chinese Medicine||East China Normal University|\n|LANZHOU UNIVERSITY||China Univ of Mining and Technology|\n|LANZHOU UNIVERSITY OF TECHNOLOGY||Hohai University|\n|LANZHOU JIAOTONG UNIVERSITY||Jiangnan University|\n|NORTHWEST NORMAL UNIVERSITY||Nanjing University of Information Science and Technology|\n|NINGXIA UNIVERSITY||Nanjing Agricultural University|\n|XINJIANG UNIVERSITY||Nanjing University of Chinese Medicine|\n|SHIHEZI UNIVERSITY||Zhejiang University|\n|XINJIANG MEDICAL UNIVERSITY||Zhejiang University of Technology|\n|XINJIANG NORMAL UNIVERSITY||Zhejiang Normal University|\n|Ningbo University||China Academy of Art|\n|Shenyang Normal University||Anhui University|\n|Dalian University of Foreign Languages||University of Science and Technology of China USTC|\n|Dongbei University of Finance and Economics||Hefei University of Technology|\n|Jilin University||Anhui Normal University|\n|Yanbian University||Xiamen University|\n|Changchun University of Science and Technology||XIAMEN UNIVERSITY|\n|Northeast Normal University||Nanchang University|\n|Beihua University||Jingdezhen Ceramic Institute|\n|Jilin Normal University||Shandong University|\n|Heilongjiang University||Ocean University of China|\n|Harbin Institute of Technology||Shandong Normal University|\n|Harbin Engineering University||Wuhan University|\n|Jiamusi University||Huazhong University of Science and Technology|\n|Tongji University||Shanghai Jiao Tong University SJTU|\n|Zhengzhou University||Tiangong University|\nFrequently Asked Questions\nStudents from all around the world are eligible for CSC Scholarship.\nFirstly, choose the university of your choice, apply for admission directly and provide important documents. The next step is to apply online at CGS scholarship portal by completing all steps and get a hardcopy of the final draft. In the end, Provide your documents to the desired university before the last date.\nYou can apply for the scholarship online at CGS information management system for international students. For admission, apply to the university of your choice directly.\nAfter reviewing documents CSC will choose most eligible candidates for the provision of scholarships, CSC will choose host university and scholarship for and candidate and send the admission documents to the university. Students will not be allowed to change the university after accepting university and coming to China.\nScholarships will be open in September for more than 250 Chinese universities.\nA research plan states your plan for the tenure you are going to stay in China as well as how you are going to use your CSC scholarship. It also states the area for your research interest for Graduate and Ph.D. programs. Moreover, the information in your study plan should be comprehensive and brief.\nChinese universities offering CGS scholarships will be available on CSC official website along with details and the deadline for the application procedure.\n250+ Chinese universities are awarding CGS for international students and students can apply in as many universities as they want. The more universities to apply in the more your chances are to get accepted. You also need to apply for scholarships for each individual university. Moreover, you need to define priority for universities if you don’t than CSC will decide the best university for your program.\nSome universities give ease to students to apply for admission without application fee the list of these universities is available on CSC official website for information.']	['<urn:uuid:62155496-0195-488b-965e-aa7570a62ae2>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T00:20:10.754925	15	56	1458
85	paint spray equipment looking for ways to reduce paint waste save money	Nitrotherm technology can help reduce paint waste by replacing compressed air with Conditioned Nitrogen as the paint carrier medium. This technology has been proven to achieve significant paint savings, averaging 20% reduction in paint consumption. Additionally, it provides cosmetic quality improvements and increases productivity through shorter process cycles. These benefits have been demonstrated across various industries including automotive, consumer electronics, and furniture manufacturing.	['Nitrotherm replaces air by Conditioned Nitrogen as paint-carrier\nPhysical properties of Conditioned Nitrogen can result in paint-savings, quality improvements and productivity improvements\nApplications occur in automotive, consumer electronics, furniture\nUnderrated aspect of Nitrotherm technology is improved process control.\nSpray-painting systems basically consist of 3 elements: gun, paint and carrier medium. Over the past 20 – 30 years worldwide, paint consuming industries – such as automotive, consumer goods and equipment – emphasised the development of the gun and the paint performance. The paint carrier medium however received much less attention by process engineers and production line designers. As a result, the medium carrying the paint to the target usually still consists of compressed air at 5-7 bar and often of questionable quality.\nNitrotherm®technology was introduced and patented by Eurosider SAS in Italy about ten years ago. Essentially this technology replaces compressed air by Conditioned Nitrogen as the carrier for the paint. This Conditioned Nitrogen is produced in Nitrotherm units making use of – among others – hollow fibre technology and can be supplied to painting gun with the required pressure and flow rates.\nThe technology has been applied mainly in Europe so far and hundreds of units were installed for manual-, automatic gun- and recently also rotary bell painting lines. The benefits of the technology result from the physical properties of Conditioned Nitrogen as a paint-carrier medium and have been proven in many painting applications in automotive, consumer electronics, furniture etc. and include significant paint savings (avg. 20%), cosmetic quality improvement and productivity improvement from shorter process cycles.\nAt present, we see Nitrotherm-technology making its way into the Asian markets, in particular China’s booming automotive industry but also in Korea.\nOne attribute of the Nitrotherm technology that is highly underrated is the significantly improved process control through Nitrotherm. In my personal experience this aspect is of critical importance for the performance of the spray-painting process. This aspect is discussed in more detail below.\nWhen I was a young process engineer 30 years ago and working in Philips Electronics, I was specifically educated in (Statistical) Process Control which principles at that time became very popular in order to improve the yield of production processes and the quality of products. Huge improvements were realized in factories (for instance in the automotive industry) in the USA, Europe and Japan by applying these principles and techniques.\nIn manufacturing companies that I have been leading after that I always emphasized the importance of the principles of process control as a way of life. In my past 4 years leading a company in China however I also learned how poorly these principles in many Chinese factories are known and applied. This is well understandable because the Chinese industry has been growing incredibly fast over the past two decades and the entire focus was on volume and less on efficiency and quality. Process operators and –engineers were not educated and trained in the principles of process control and consequently many production processes perform poorly.\nEach production process and consequently each product property is exposed to variation. This brings us to the definition of Process Control which is to Manage Process Variation. Each process is defined by its process settings formalizedin the Standard Operating Procedure (SOP). No operator is allowed to change settings and the SOP can only be changed after formal release by technical management and based on new insight. However the numeric value of each process parameter – like temperature, pressure, humidity etc. – is subject to an intrinsic (or normal) variation around the numeric value of the setting due to variation in the process system that is not captured in the technical control loops. This intrinsic variation is called Process Capability and within this process capability the outcome (quality and yield) is fully predictable – see Chart I.\nIn the case a Process is ‘out-of-control’ it is not predictable and consequently the quality and yield performance cannot achieve good levels. Therefore it is mandatory that a process is brought technically ‘’ In-Control” . When that is achieved, Process Control is to manage the process variation within the boundaries of its process capability. This means that when an operator observes that a parameter is moving outside its boundaries (control limits) of normal variation , this is an indication that something went wrong (special cause) causing the system moving out-of-control. When this happens the reaction of the operator should never be to change the process settings. Instead the operator should sort out the root cause of the disturbance and solve the problem so the system can go back within its boundaries. These corrective actions are normally defined in a so called Out of Control Action Plan (OCAP)\nIf we are not satisfied with the current performance of the process in terms of yield and quality, this only can be improved by technical innovation resulting in improved process settings and/or better control systems that reduce the intrinsic variation within the system. This with other words brings an improved Process Capability- see chart 3 below.\nWithin the theory of process control as explained above, Nitrotherm technology should be seen as a technical innovation to better control the spray painting process. This besides the physical benefits of Nitrotherm as explained in the introduction. As explained with Nitrotherm the paint carrier is changed from compressed air into Conditioned Nitrogen. By doing this two important improvements are achieved:\nThe natural variation of the carrier parameters (Temperature, Pressure and Flow) is substantially reduced by the Nitrotherm unit. Moreover the Purity of the carrier is much higher (for example water and oil contamination is fully eliminated) and the carrier is provided with a pre-set Ionisation (or static electric charge).\nOperator (Human) is reduced as the Nitrotherm unit takes over the full control of the paint carrier medium.\nThese two elements will result in an overall much better controlled spray painting process which consequently brings an intrinsic higher yield and product quality. For this reason I see Nitrotherm technology as a real technical innovation in spray-painting that will become a future standard as paint-carrier medium in the many industries where spray-painting performance is of critical importance.\nFormer CEO of Perlos/Lite-On Mobile']	['<urn:uuid:5e4691fc-9921-4b71-b39a-0b1c20a78e4d>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T00:20:10.754925	12	63	1025
86	I've been studying Old Norse poetry and would love to know how the daughters of Ægir appear in the Poetic Edda - are there any specific examples of them being mentioned in the poems?	Yes, the daughters of Ægir appear notably in the poem Helgakviða Hundingsbana I, which describes Helgi's boat battling intense seas. The poem specifically mentions 'Kolga's sisters' crashing and depicts Ægir's daughters as trying to overthrow the 'stay-bridled sea-steeds.' The stanzas describe dramatic scenes of swells and bluffs appearing to burst as Helgi's crew fights through the waves with heightened sails.	"['Nine Daughters of Ægir and Rán\nIn Norse mythology, the goddess Rán and the jötunn Ægir both personify the sea, and together they have nine daughters who personify waves. Each daughter\'s name reflects poetic terms for waves. s attested in the Poetic Edda, compiled in the 13th century from earlier traditional sources; the Prose Edda, written in the 13th century by Snorri Sturluson; and in the poetry of skalds. Scholars have theorized that these daughters may be the same figures as the nine mothers of the god Heimdallr.\nThe names of Ægir and Rán\'s daughters occur commonly in Old Norse sources. Lists of their names appear twice in Skáldskaparmál, a section of the Prose Edda (for detail, see Prose Edda section below).\n|Blóðughadda||""Bloody-hair""||According to scholar John Lindow says, this name ""[refers] to reddish foam atop a wave"". Scholar Rudolf Simek says that ""the name does not appear to be too appropriate for a wave, but perhaps it was supposed to convey the wispy, thread-like appearance of the water streaming from the crest of the wave.""|\n|Bylgja||""Billow""||Employed as a common noun|\n|Dröfn or Bára||According to Anthony Faulkes, Dröfn means ""comber"", whereas Bára translates to ""wave"" According to Andy Orchard, \'Dröfn means \'foaming sea\'.||Bára replaces Dröfn in a list of the daughter in Skáldskaparmál. Dröfn also appears as a common noun.|\n|Hefring or Hevring||""Lifting""|\n|Hrönn||""Wave""||Employed as a common noun|\n|Uðr or Unn||""Wave""||Employed as a common noun, also appears as a name for Odin and as the name of a river|\nReferences to the waves as \'Ægir\'s daughters\' appear in the Poetic Edda. The poem Helgakviða Hundingsbana I describes how the hero Helgi\'s boat crashes through intense seas, in doing so referencing Rán, Ægir, and their daughters as personifications of the sea. For example, two sequential stanzas reference the wave daughters:\n- Once the longships regrouped, only\n- Kolga\'s sisters could be heard crashing.\n- a sound as if swells and bluffs were bursting.\n- Helgi had the high sails heightened,\n- the unfailing crew rallying through\n- the rollers, Ægir\'s dreaded daughters trying\n- to overthrow their stay-bridled sea-steeds.\nThe daughters are mentioned several times in the Prose Edda. Section 25 of Skáldskaparmál (""How shall sea be referred to?"") collects manners in which poets may refer to the sea, including ""husband of Ran"" and ""land of Ran and of Ægir\'s daughters"", but also ""father of Ægir\'s daughters"". The section contains the first of two instances of a list of the wave daughters (for discussion regarding their names, see Name section above).\nScholarly reception and interpretation\nSome scholars have linked the Nine Daughters of Ægir and Rán with the Nine Mothers of Heimdallr, an identification that would mean that Heimdallr was thus born from the waves of the sea. However, this connection has been questioned on the grounds that the names presented for the Nine Daughters of Ægir and Rán and the Nine Mothers of Heimdallr (as listed in Völuspá hin skamma) do not match. Scholar John Lindow comments that the identification of Heimdallr\'s mothers as Ægir and Rán\'s daughters do, however, match on the grounds that Ægir and Rán\'s daughters, like Heimdallr\'s mothers, are sisters, and that two separate traditions about Heimdallr\'s mothers may explain the differences between the two.\n- Children of Lir, children of the personified sea in Irish folklore\n- Lindow (2001:49).\n- Simek (2007 :38-39).\n- Faulkes (1995 :232).\n- Faulkes (1995 :233, 230).\n- Orchard (1997:34).\n- Faulkes (1995 :231, 141).\n- Faulkes (1995:233).\n- Faulkes (1995 :242).\n- Faulkes (1995 :257).\n- Dodds (2014:129).\n- Faulkes (1995 :91).\n- Faulkes (1995 :141).\n- Simek (2007:136).\n- Lindow (2002:169).\n- Dodds, Jeramy. Trans. 2014. The Poetic Edda. Coach House Books. ISBN 978-1-55245-296-7\n- Faulkes, Anthony. Trans. 1995 . Edda. Everyman. ISBN 0-460-87616-3\n- Lindow, John. 2002. Norse Mythology: A Guide to the Gods, Heroes, Rituals, and Beliefs. Oxford University Press. ISBN 0-19-515382-0\n- Orchard, Andy. 1997. Dictionary of Norse Myth and Legend. Cassell. ISBN 0-304-34520-2\n- Simek, Rudolf. 2007 . Translated by Angela Hall. Dictionary of Northern Mythology. D.S. Brewer. ISBN 0-85991-513-1']"	['<urn:uuid:a6d8bf6b-fa18-4ddb-b4ca-0d53a6bfa479>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T00:20:10.754925	34	60	675
87	education system food preparation environmental impact discuss	The education system teaches food preparation through a comprehensive curriculum that covers practical cooking skills, nutrition, and food science. Students learn to prepare various dishes like macaroni cheese, soup, and pastries while developing skills in food safety and ingredient understanding. This educational approach aligns with environmental sustainability goals, as modern food service operations implement various waste reduction initiatives. These include trayless dining to reduce food waste, recycling programs for materials like cardboard and glass, and the use of energy-efficient appliances. The combination of proper food education and sustainable practices helps create environmentally conscious cooks who understand both culinary skills and ecological responsibility.	"['Food Preparation & Nutrition\nThe Food and Nutrition department is a busy and popular department. Students acquire a range of excellent practical skills, working independently as well as part of a team. They gain nutritional knowledge, an understanding of the science behind food and the skills to enjoy the pleasure of cooking for themselves and others. It is our aim to prepare students with the skills needed to live healthily by preparing their own nutritious food at the end of Key Stage 3.\nKey Stage 3 Curriculum\nAll pupils study Food and Nutrition in Key Stage 3; they will learn how to use equipment safely and build their confidence by learning how to prepare a range of healthy meals such as macaroni cheese, soup, pasta salad, lasagne, and fish dishes. There will also be the chance to develop high level skills such as choux pastry, rough puff pastry and showstoppers such as lemon meringue pie. Food science is an important element of GCSE and as such we introduce the idea of investigative work from the outset; these lessons involve taste testing and understanding what is happening to ingredients in recipes; this understanding allows students to be able to swap ingredients successfully and make corrections to their dishes as they learn.\nTerm 1 introduces students to Food and Nutrition as well as basic knife skills, use of the cooker and healthy eating dishes such as pizza toast, fruit crumble and soup.\nBy Term 2, students will begin undertaking sensory analysis, conducting investigations into raising agents and considering food safety; as well as learning the importance of reducing sugar in diets and how to create a healthy lunch.\nIn Term 3, students begin investigations into bread, preparing a white sauce and the importance of cooking for fun.\nCuisine from around the world is experienced in Term 1, followed by an understanding of nutrition, how time plans keep us on track and the differences in seasonal cookery.\nTerm 2 explores ingredients such as shortcrust pastry, the whisking method of cake making, family meals and the importance of food waste and food packaging.\nIn Term 3, Students will use nutritional software to evaluate meals, investigate the shaping and coating of foods, consider differing meal options for certain age groups and culminate their learning in an exciting Bake-off.\nTerm 1 begins with a Pasta project, where students learn to make their own, followed by Pastry -shortcrust, rough puff and seasonal considerations.\nFunctions of ingredients such as eggs is explored in Term 2 along with vegetarian cookery, choux pastry, shaping pastries and an in-depth look into International cuisine.\nTerm 3 sees research into food-related health issues, a second showstopper recipe (lemon meringue pie) and finally, a skills showcase.\nKey Stage 4 Curriculum\nFood Preparation and Nutrition is a very popular and successful choice at GCSE. Building on the work done in KS3 Year 10 pupils study all aspects of food - nutrition, food science, food safety, food choice and provenance of food. Year 11 sees the completion of two non-examined assessments - a food science investigation and a food preparation assessment which culminates in a 3- hour practical task.\nThis is an exciting and creative course which focuses on practical cooking skills to ensure students develop a thorough understanding of nutrition, food provenance and the working characteristics of food materials. At its heart, this qualification focuses on nurturing practical cookery skills and developing a strong understanding of nutrition and food science.\nStudying Food and Nutrition in Key Stage 4 allows students to develop a range of transferable skills such as time management, independent working and group working. Further skills developed include critical awareness, knowledge of sustainability resources and a wider understanding of how individual needs are met by products designed for a specific target group.\nThis course will help students to consider the influence of lifestyle and consumer choice, understand the functional properties and chemical processes of ingredients, develop the ability to review and improve recipes and ultimately learn to justify and present their ideas.\nFor more information, please view the latest resources available below.\nClick the exam board logo below for the exam specification :', ""Your Sodexo Campus Dining Team is dedicated to sustainability. We know we can all work together to make a positive impact right here at Wayland Baptist University. In addition to the many initiatives we take on as a company, we have implemented the following programs specific to Wayland Baptist University.\nXprss Nap Dispensers save energy and waste. The napkins are made of 100% recycled paper and the dispenser will encourage customers to take (and waste) fewer napkins. Energy is saved because less power is used to recycle paper products than to create them from virgin material.\nAPEX combines technology and products designed to save water and energy, minimize the impact of products on the environment, and has a built-in method of measuring results. The APEX management approach uses a tablet PC and wireless technology to communicate with the system’s controller to download, process and analyze data to establish each foodservice operation’s “rack-to-guest ratio.” By monitoring and improving this ratio, the system helps reduce the amount of water and energy used at each facility, and improve total operational efficiency.\nWe are a trayless campus. Customers without trays typically waste less food and beverages. It is also a healthier approach to eating, as we all know that at times our eyes are bigger than our stomach. Trayless dining also reduces the amount of water and detergents needed to wash trays - and saves energy too!\nWeigh The Waste\nDid you know that 40% of the food produced in our country is wasted? Periodically throughout the school year, we conduct a Weigh the Waste event. During the event, we weigh all of the uneaten food from guest’s plates. We consolidate the findings and report back to the community. We tie this to an awareness campaign that informs guests about food waste and its impact to our environment and our communities. We hold frequent campaigns in which we compare our results (and hopefully see our waste decline).\nWaste Reduction Efforts Recycling\nOn campus we recycle the following products: cardboard, glass, aluminum, paper, plastic.\nThe Sustainability Management and Reporting Tool (SMART) is a comprehensive, yet easy to use dashboard that provides baseline sustainability performance, prioritizes initiatives, and tracks continuous improvement in four key areas - carbon and energy, water, waste, and healthy and sustainable food and environments.\nReducing waste is very important. It results in less food waste in landfills where it emits methane which is 21x more powerful than CO2. The LeanPath kitchen waste tracking system records and tracks all of the food waste from our kitchen. This helps us to identify and change practices which result in food waste. By tracking waste we inform our staff and focus our efforts. On average, Sodexo has experienced a 40% decrease in kitchen food waste when the LeanPath system is in place.\nIndividual packets of condiments are useful in some instances, but they result in extra waste. If you are dining on site or you can add the sauce to your item before you leave, please use the bulk dispensers to reduce our waste.\nRecycling Fryer Oil into Biofuel\nFrench fries to fuel. Indeed, our fryer oil is recycled into biodiesel that is used to power a variety of vehicles - from delivery fleets to farm equipment.\nChoose To Reuse\nChoose to Reuse is a reusable container program designed to reduce waste and help achieve Sodexo's Better Tomorrow commitments by replacing single-use disposable food and beverage containers with reusable containers.\nReusable Cup Discount\nThe environment and your pocketbook win! Bring your reusable cup to purchase a fountain soda on campus and receive a discount at every location, every time. Details and cups can be purchased in Pete's.\nWe offer china dishes in our dining locations. This reduces waste associated with using disposable containers. Dining in? Be sure to use china. We also use china at our catered functions on site.\nGreen Cleaning Green Seal\n92% of the chemicals Sodexo uses are “Green Seal” certified, concentrated or sustainable.\nEnergy Reduction Efforts Daylighting\nThe windows in our dining room allow us to enjoy a tremendous amount of natural light while dining. Natural light not only provides a more comfortable environment, it also reduces our energy usage by allowing us to turn off lights.\nTo save energy, we only turn our equipment on when it is needed and turn it off as soon as we are done using it for the day.\nEnergy Star Appliances\nENERGY STAR is a U.S. Environmental Protection Agency (EPA) voluntary program that helps businesses and individuals save money and protect our climate through superior energy efficiency. When replacing equipment we choose Energy Star rated appliances.\nEnergy Efficient Lighting\nWhere feasible we have replaced our incandescent bulbs with compact flourescent bulbs to reduce our energy consumption.\nNo Idling at Loading Docks\nWorking with our vendors, our catering staff and our campus facility services staff to make sure all vehicles are turned off while they are parked in our loading dock.\nWe work closely with local produce distributors to maximize the fruits and vegetables purchased from local farms. We also purchase from local dairies, and participate in farm to school initiatives. Tracking local purchases helps us to meet our Better Tomorrow Plan commitment of 20% local purchase by 2020.\nSustainable Fish and Seafood\nWe are committed to serving sustainable seafood and fish. By 2015 100% of our fish and seafood will be sustainably certified by the Marine Stewardship Council or the Best Aquaculture Practices. We do not purchase any at risk species.\nAll of our fluid milk is purchased from local dairies that do not use rBST.\nGoing forward by giving back. We are proud to feature 100% Transfair, Fair Trade USA certified aspretto coffee and 100% USDA certified organic and ethically sourced Numi teas. Everything that touches the product is green, from the 10% post-consumer fiber cups to the renewable resource stirrers to the fair trade sugar wrapped in recyclable paper and printed with vegetable dye.\nOrganic food is produced by farmers who emphasize the use of renewable resources and the conservation of soil and water to enhance environmental quality for future generations. Organic meat, poultry, eggs, and dairy products come from animals that are given no antibiotics or growth hormones. Organic food is produced without using most conventional pesticides; fertilizers made with synthetic ingredients or sewage sludge; bioengineering; or ionizing radiation. Before a product can be labeled ‘organic,’ a Government-approved certifier inspects the farm where the food is grown to make sure the farmer is following all the rules necessary to meet USDA organic standards. Look for the menu icon identifying organic foods.\nAll it takes are a few simple steps to more sustainable catered events. We feature menus that are locally grown, seasonal, organic, vegetarian and/or well balanced. We use china serviceware whenever possible, but if necessary, we try to use compostable and/or recyclable disposableware. We strive to eliminate bottled beverages when possible by replacing them with pitchers of drinks and reusable glassware.""]"	['<urn:uuid:0d387c61-1541-4051-bee5-1e4864216aec>', '<urn:uuid:c3a89965-182a-4e82-89e9-dfa173738187>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T00:20:10.754925	7	102	1850
88	When does the Holy Night fall in Ramadan?	According to different traditions, the Night of Qadr may fall on any of the odd dates of the last ten days of Ramadan (21st, 23rd, 25th, 27th, and 29th), though the majority of traditions indicate the 27th night as the most probable.	['Etymologically the word Laylat ul Qadr means the Night of Importance. However, Muslim scholars translate it in many other ways as well. For instance, this night is called as the Night of Decree, Night of Destiny, Night of Forgiveness, Night of Power, and so on and so forth.\nSignificance of Night of Qadr In Holy Quran\nThere are many other significant dates in the Islamic calendar that have been mentioned in the Quran but never have been a whole surah been dedicated for a day. Even, if we see Surah jummah, only three ayats in the final conclusion openly speak about it, the preceding ayats don’t have any mention of Jummah.\nHad it been of any lesser significance, Allah would not reveal an entire surah explaining the value of this night. As a matter of fact, the revelation of the Holy Quran began this night. Hence, this night has great significance. Allah underpins the value of the Night in Surah AL-Qadr, in the following words.\n- Undoubtedly, We sent it down in the blessed and valuable night.\n- And what you know, what the blessed night is?\n- The blessed and valuable Night is better than a thousand months.\n- Therein descend angels and Jibril (the Spirit) by the command of their Lord for every affair.\n- That is all peace till the rising of the dawn.\nMuslims all around the world spend these nights in Rukooh and Sujood. They ask for forgiveness for their sins. They ask for Duas, their unfulfilled wishes. They serve Allah with Ibadah and they serve themselves with loads of Duas. Therefore, Duas are an imperative part of this night.\nAs if the revelation of one surah did not suffice its importance, Allah dedicates six more verses of the beginning of Surah Dukhaan to endorse the importance of this night. The Quran says:\nIn the name of Allah, Most Gracious, Most Merciful.\n- By the Book that makes things clear;-\n- We sent it down during a Blessed Night: for We (ever) wish to warn (against Evil).\n- In the (Night) is made distinct every affair of wisdom,\n- By command, from Our Presence. For We (ever) send (revelations),\n- As Mercy from thy Lord: for He hears and knows (all things).\nWhat Is The Night Of Decree?\nHow To Spend The Last 10 Days Of Ramadan?\nThere are several traditions of duas from Prophet Muhammad (SAWW) which educates us with the importance of this night. Some of them have been mentioned in the points below. May Allah enlighten our hearts with the noor of emaan. Ameen\n↓ 1. Seek Forgiveness:\nThere is also a special link between this night and seeking forgiveness from God. A’isha asked the Prophet, “O Messenger of Allah! If I knew which night is Laylatul-Qadr, what should I say during it?” And he instructed her to say:\nWe also recommend that you go through these Islamic Quotes on Forgiveness.\n↓ 2. Offer Maximum Prayers:\nThe Prophet said: “Whoever prays during the night of Qadr with faith and hoping for its reward will have all of his previous sins forgiven.” (Bukhari and Muslim recorded from Abu Huraira).\nAisha (RA) reported: With the start of the last ten days of Ramadan, the Prophet (SAWW) used to tighten his waist belt (i.e. work harder) and used to pray the whole night, and used to keep his family awake for the prayers. (Bukhari) The nafli prayers are given below\nIbn ‘Uthaymeen said: “A person would attain the reward of the night, even if he has no knowledge of it. This is because the Prophet said ‘whoever stands (in prayer) during Laylatul-Qadr, with faith and hope, will be forgiven,’ and the Prophet did not make knowledge of the night a condition of their forgiveness. And had knowledge of the night become a necessary factor, the Prophet would have made this clear.”\n↓ 3. Ask for Goodness\nThe companion Anas ibn Malik reported: “Ramadan approached, so the Messenger of God said: ‘This month has come to you, and in it there is a night that is better than a thousand months. Whoever is deprived of it is deprived of all goodness, and no one is deprived of its goodness except one who is truly deprived.’ Sunan Ibn Majah, 1644. Here are Islamic Duas For Success That Every Muslim Should Know.\n↓ 4. Stay Energised & Don’t Overeat\nIf you want to stay up the whole night, it’s important that you take a nap either in the day time or after iftar.\n↓ 5. Give Charity\nTry giving a little charity every night because if you give charity on night of qadr, it will be similar to giving it every day for 84 years.\n↓ 6. Read Surah Ikhlas\nTry to read surah ikhlas every day in the last ten days at least three times, because that is equivalent to reading one-third of the Quran. If you get the chance to do this on the night of power, the reward will be multiplied for you.\n↓ 7. Pray Tahajjud\n↓ 8. Do Dhikr\nHere are some dhikr recommendations for laylatul qadr:\n↓ 9. Wake Your Family For Ibadat\nHow Should Menstruating Women Spend Laylatul Qadr?\nSigns of Night of Qadr\nThe Prophet Muhammad (SAWW) is reported to have said that the Night of Qadr is a gentle night and it is not hard, nor cold. It is a comforting night. Then he added, the sun rises on this day with soft rays and reddish complexion. Prophet Muhammad (SAWW) said that the amount of angels sent down on the Night of Qadr dulls the sun because the Arsh/ sky is filled with angels.\nBeing a person of sanity, one may ask how would we know about a night on the basis of the signs during the next day. The Prophet Muhammad (SAWW) explained that this night has been left mysterious, one should therefore never give up hope on the upcoming odd days. This way we\nWhen is Laylat ul Qadr 2020?\nAlthough according to different traditions the night of Qadr may fall in any of the odd dates of the last ten days of Ramadan, meaning it may be, 21st, 23rd, 25th, 27th, and 29th. However, the majority of traditions call the 27th night to be the most probable night. Having said that, according to the Georgian calendar Laylat ul Qadr will be Inshallah on the 19th of May 2020.\nPrayers for Night of Qadr:\nThere are specific namaz/prayers for the Nights of Qadr. In the book, Majmooa-e-oraad-o-wazaaif, authored by Molvi Ismat Ullah Hassan Zai, the following pattern of namaz is given as per the tradition of Messenger of Allah, Muhammad (SAWW).\n|Total number of rakats||Surahs to be recited in every rakat||Recitations after namaz|\n|Surah fatiha proceeded by Surah Qadr||Proceeded by Surah Surah Ikhlaas|\n|21st||Two||Once||Once||Durood shareef 70 times|\n|23rd||Four – two each||Once||Three times||–|\n|25th||Four- two each||Once||Five times||Kalima e Tayyiba 100 times|\n|27th||Twelve-four each||Once||Fifteen times||Recite istaghfaar 70 times|\n|29th||Four- two each||Once||Three times||Surah Al-Inshirah 70 times|\nYou may contact the author in the comment section for any confusion in the prayer procedure.\nMay Allah accept our prayers, forgive us and keep us under His blessings. Ameen']	['<urn:uuid:ee24522a-9874-4f88-951a-dd88e8b851fd>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T00:20:10.754925	8	42	1196
89	jazz chicago new orleans connection development	New Orleans served as jazz's birthplace where early musicians like Buddy Bolden played in brothels and bars. The connection between New Orleans and Chicago strengthened when Joe 'King' Oliver, who had become known in New Orleans by 1912, moved to Chicago. Oliver later invited Louis Armstrong to join his Creole Jazz Band in Chicago in 1922, when Chicago was the center of jazz. Armstrong made his first recordings while playing second cornet in Oliver's band in 1923, and the city became a new central base for jazz by 1927.	"['As Blacks migrated in the late 1800’s from the Mississippi Delta, they carried their songs to industrial centers like St. Louis and Chicago. The blues, spirituals accompanied by staccato handclaps and the foot-produced drum sounds of plantation dances all came together as Blacks congregated in the Northern cities.\nBy the 1880’s White listeners were already fascinated by “rag time,” a colloquialism for syncopation–rhythmic emphasis in unexpected places. It differed radically from European rhythms and the popular arrangements of folk tunes. The ragtime style coalesced and became a recognizable form in the 1890’s. An old timer in Texarkana remembered hearing parts of “Maple Leaf Rag” even before Scott Joplin had left to strike out on his own. St. Louis, with its Black-owned saloons and brothels, served as an important “school” for ragtime musicians. By 1900, it was becoming a national craze and gave the new decade its popular name, the “Gay The nineties”. The emergence of the commercial music market and music publishing firms in New York promoted a major cultural force.\nFrom a combination of blues and ragtime, jazz was born. Buddy Bolden is recognized as one of the first musicians to play the music later termed “jazz.” Born in 1868, he began playing while in his twenties around the streets of New Orleans. The cornet player led some successive bands, beginning in the brothels of New Orleans. From there the music spread to similar establishments in other cities and then to bars and dance halls. This music stirred such interest on both coasts that by 1917, Storyville, the New Orleans red-light district, employed hundreds of young jazz musicians. The pianist “Jelly Roll” Morton, considered a first-class composer of stomps and swings, became widely known for his sensitive touch. His songs provided a real challenge to music copyists because they were so difficult to arrange on paper. The closing of the district in 1917, due to reports of violence and syphilis among servicemen on leave, accelerated the spread of jazz. By 1927, it could be found in dozens of major cities with Chicago as the new central base.\nIn 1918, Chicago exploded with the sounds of Joe “King” Oliver. Oliver had become known in New Orleans as early as 1912. He made a modern musical statement here in the States, just as abstract art, imported from Paris, began to be accepted in New York City. Other “kings” to be found in Chicago at that time was Sidney Bechet on clarinet and “Jelly Roll” Moton on piano. The addition of Lil Hardin, who later became Mrs. Louis Armstrong, to “King” Oliver’s band not only provided a piano but also an important contact. Oliver and Armstrong toured together for about four years and wrote some songs together. Joe “King” Oliver brought many New Orleans players to Chicago.\nBoth Americans and Europeans were emotionally devastated by\nWorld War I, and jazz lifted them to musical highs they had never before experienced. It was fitting that a Black music, born of pain and linked to brothels and illegal liquor, should enrapture a world faced with the darkness of its soul.\nCharles Parker, Jr. was born in Kansas City on August 29, 1920, and grew to be the jazz world’s greatest idol. His technique and passion combined to make him one of the greatest jazz stylists ever. Boston drummer Roy Haynes has spoken of Parker’s spontaneity, his ability to react musically in an instant to anything that was going on around him — and to swing while doing it. In New York City, “Bird” joined musicians “Dizzy” Gillespie, Thelonius Monk, Kenny Clarke and Charlie Christian to create a harder-edged sound called bebop.\nManhattan’s 52nd Street flourished with jazz clubs in the 1940’s. Those white musicians who gravitated to the music had to enter the Black world to play with understanding. Parker said that music was a reflection of life experiences, that one had to live it to express it. Charles “Yardbird” Parker died at age 34 on March 12, 1955, but his influence continues to this day, as does the influence of all the others who have made and continue to make a mark in the world of jazz music.', 'Louis Daniel Armstrong (usually pronounced \'Louee\' in the French pronunciation with a silent s) (August 4, 19011 – July 6, 1971) (also known by the nicknames Satchmo and Pops) was an Americanjazzmusician. Armstrong was a charismatic, innovative performer whose musical skills and bright personality transformed jazz from a rough regional dance music into a popular art form. Probably the most famous jazz musician of the 20th century, he first achieved fame as a trumpeter, but towards the end of his career he was best known as a vocalist and was one of the most influential jazz singers.\nArmstrong was born to a poor family in New Orleans, Louisiana. The date of his birth is August 4, 1901. His youth was spent in poverty in a rough neighborhood of uptown New Orleans, as his father, William Armstrong, abandoned the family when Louis was an infant. He first learned to play the cornet (his first of which was bought with money loaned to him by the Karnofskys, a Russia-Jewishimmigrant family) in the band of the New Orleans Home for Colored Waifs, where he had been sent multiple times for general delinquency, most notably for a long term after (as police records show) firing a pistol into the air at a New Year\'s Eve celebration. He followed the city\'s frequent brass band parades and listened to older musicians every chance he got, learning from Bunk Johnson, Buddy Petit, and above all Joe \'King\' Oliver, who acted as a mentor and almost a father figure to the young Armstrong. Armstrong later played in the brass bands and riverboats of New Orleans, and first started traveling with the well regarded band of Fate Marable which toured on a steamboat up and down the Mississippi River; he described his time with Marable as \'going to the University\' since it gave him a much wider experience working with written arrangements. When Joe Oliver left town in 1919, Armstrong took Oliver\'s place in Kid Ory\'s band, regarded as the top hot jazz band in the city.\nIn 1922, Armstrong joined the exodus to Chicago, where he had been invited by Joe \'King\' Oliver to join his Creole Jazz Band. Oliver\'s band was the best and most influential hot jazz band in Chicago in the early 1920s, at a time when Chicago was the center of jazz. Armstrong made his first recordings, including taking some solos and breaks, while playing second cornet in Oliver\'s band in 1923.\nArmstrong was happy working with Oliver, but his wife, pianist Lil Hardin Armstrong, urged him to seek more prominent billing. He and Oliver parted amicably in 1924 and Armstrong moved to New York City to play with the Fletcher Henderson Orchestra, the top African American band of the day. Armstrong switched to the trumpet to blend in better with the other musicians in his section. During this time, he also made many recordings on the side arranged by an old friend from New Orleans, pianist Clarence Williams; these included small jazz band sides (some of the best pairing Armstrong with one of Armstrong\'s few rivals in fiery technique and ideas, Sidney Bechet) and a series of accompaniments for Blues singers.\nHe returned to Chicago in 1925 and began recording under his own name with his famous Hot Five and Hot Seven with such hits as \'Potato Head Blues\', \'Muggles\' (a reference to marijuana, for which Armstrong had a lifelong fondness), and \'West End Blues\', the music of which set the standard and the agenda for jazz for many years to come. Armstrong\'s trumpet introduction to \'West End Blues\' remains one of the most famous and celebrated improvisations in jazz history.\nArmstrong returned to New York in 1929, then moved to Los Angeles in 1930, then toured Europe. After spending many years on the road, he settled permanently in Queens, New York in 1943. Although subject to the vicissitudes of Tin Pan Alley and the gangster-ridden music business, he continued to develop his playing.\nDuring the subsequent thirty years, Armstrong played more than three hundred gigs a year. Bookings for big bands tapered off during the 1940\'s due to changes in public tastes: ballrooms closed, and there was competition from television and from other types of music becoming more popular than big band music. It became impossible to support and finance a 16-piece touring band.\nArmstrong kept up his busy tour schedule until a few years before his death. While in his later years, he would sometimes play some of his numerous gigs by rote, but other times would enliven the most mundane gig with his vigorous playing, often to the astonishment of his band. He also toured Africa, Europe, and Asia under sponsorship of the US State Department with great success and become known as \'Ambassador Satch\'. While failing health restricted his schedule in his last years, within those limitations he continued playing until the day he died.\nArmstrong died of a heart attack in 1971 at age 69, the night after playing a famous show at the Waldorf Astoria\'s Empire Room. He was interred in the Flushing Cemetery, Flushing, New York.\nWhat A Wonderful World ""By Louis Armstrong. By George David Weiss and Bob Thiele. For voice, piano and guitar chords. Piano Vocal. Standards. Difficulty: medium. Single. Vocal melody, piano accompaniment, lyrics, chord names and guitar chord diagrams. 4 pages. Published by Hal\nWhat A Wonderful World - Easy Piano ""By Louis Armstrong. By George David Weiss and Bob Thiele. For voice and easy piano. Hal Leonard Easy Adult Piano. Standards and Traditional Pop. Difficulty: easy-medium. Single. Vocal melody, piano accompaniment, lyrics, chord names and Easy Piano Notati\nDream A Little Dream Of Me ""By Nat """"King"""" Cole, Ella Fitzgerald, Louis Armstrong, and The Mamas & The Papas. For piano, voice, and guitar (chords only). Jazz; Love; Pop; Standards. 5 pages. Published by Hal Leonard - Digital Sheet Music""\nCool Yule ""By Louis Armstrong. Arranged by Kirby Shaw. For SSA choir and piano accompaniment (SSA). Hal Leonard Secular Christmas Choral. Choral, Jazz, Standards and Swing. Octavo. Chord names and optional jingle bell part. 8 pages. Published by Hal Leonard""\nLouis Armstrong Plays Standards - Trumpet ""(Artist Transcriptions - Trumpet). By Louis Armstrong. For trumpet. Hal Leonard Artist Transcriptions. Jazz, Standards and Swing. Difficulty: medium-difficult. Instrumental solo book. Standard notation, chord names, introductory text and discography. 37\nWhat A Wonderful World By Louis Armstrong. By Bob Thiele and George David Weiss. Arranged by Mark A. Brymer. SSA. Concert; Inspirational; Light Concert; Standards. Choral. 7 pages. Duration 2:20. Published by Hal Leonard - Digital Sheet Music\nAll Of Me ""By Frank Sinatra, Michael Buble, Willie Nelson, and Louis Armstrong. For piano, voice, and guitar (chords only). Jazz; Standards. PV. 8 pages. Published by Hal Leonard - Digital Sheet Music""']"	['<urn:uuid:f8093122-4583-4406-ae3a-bf5a5e1548b1>', '<urn:uuid:b8221d48-9145-49c5-9e33-f0d4dcec15f7>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T00:20:10.754925	6	89	1821
90	What happened to HNoMS Helge Ingstad during the exercise?	The Royal Norwegian Navy frigate HNoMS Helge Ingstad collided with the Malta-flagged oil tanker Sola TS during the exercise, suffering significant damage.	['Trident Juncture 2018 is a major NATO exercise which is focused on Northern Tier defense.\nIt also is an exercise for the Norwegians in mobilizing their society to deal with crises in the region in which NATO forces would be called upon to operate from Norwegian soil.\nA key part of the strategic shift facing the Nordics is the shift from expeditionary out of area operations to direct defense.\nAs we noted earlier this year when we released our report on the Nordics and the strategic shift:\nThe Russian seizure of Crimea and other aspects of its global activism have had a significant effect on the Nordics.\nThe Nordics are working mores closely together to deal with the strategic shift. And they are adding new capabilities to shape a more effective approach to crisis management and deterrence in depth.\nAnd the Norwegians, Swedes and Finns are clearly committed to a total defense concept whereby society is being mobilized to support defense in depth as well.\nThe direct defense focus is coming as well within the context of a significant shift within Europe itself with regard to the next phase of European development.\nIn an article entitled “The Coming of Hanseatic League 2.0,” Harald Malmgren highlighted this strategic political and economic shift as well:\nGerman authorities are now beginning to address the possibility of a different European context in coming years, as the pressures within the EU and Eurozone build towards a potential breakup over mutual financial commitments.\nItaly in particular is posing this financial issue as central to its demand for greater fiscal and banking autonomy.\nLess noticed in press and media is a parallel process of thinking among other European national militaries. Among the militaries of the Nordic nations there has already developed a Northern Europe Defense Cooperation group, which involves closely integrated operations of the four Nordic nations’ militaries with the UK and the US. This group has been joined de facto militarily by the Netherlands and Belgium. Its area of direct concern includes the security of the Arctic and virtually all of the Baltic Sea region, including the Baltic States.\nHow divisions between military thinking (clear desire to be part of the Nordic club) and political thinking in Germany (where Macron and Merkel have tried to work a closely relationship) and other European nations evolves is uncertain.\nMost likely there will be clashes among these disparate sources of power in Europe in the future,when economic and security challenges arise and require action.\nNorway sits in a geography where the greatest concentration of military force rests, namely the Kola Peninsula.\nEssentially with the Russians building out bases throughout their part of the Northern region inclusive of the Arctic, they are distributing their access points to put pressure on the Nordics and NATO in a crisis.\nThe reworking of the geography is sometimes referred as the militarization of the Arctic.\nBut it is better understood as enhanced base distribution by the Russians to provide operational points from which force can be applied in a crisis or as part of the diplomatic side of deterrence strategy.\nThis reworking of geography provides a backdrop of the challenges facing NATO which Trident Juncture 2018 is trying to address.\nAs Megan Eckstein noted in an article published by USNI News on November 7, 2018:\nThe massive exercise – which grew to include more than 50,000 personnel, 65 ships and 250 aircraft, including 14,000 American troops, a carrier strike group (CSG) and an amphibious ready group (ARG) – centered around a scenario of protecting Norway from an invasion at its borders by inserting reinforcements by air and by amphibious landing.\nA key piece of the amphibious landing was the Iwo Jima ARG and embarked 24th Marine Expeditionary Unit. The ARG/MEU stopped in Iceland for training on the way to Norway, but the two smaller ships, dock landing ship USS Gunston Hall (LSD-44) and amphibious transport dock USS New York (LPD-21), had to return to Iceland when heavy seas hampered their voyage to Norway.\nPersonnel on the LSD suffered minor injuries, and the well deck of the ship and an unsecured landing craft utility (LCU) were damaged in the heavy seas. The LPD returned to Iceland with the LSD as a precautionary measure. New York made it to Norway for the start of the exercise, but Gunston Hall did not.\nAccording to a 24th MEU news release, the Marines successfully landed ashore at Alvund, Norway, on Oct. 29.\nThe landing force included Marines from Battalion Landing Team 2nd Battalion, 2nd Marine Division, and 700 Marines pushed ashore with 12 amphibious assault vehicles, six light armored vehicles and 21 humvees.\n“We came to the North Atlantic looking for a challenge and Trident Juncture delivered; throughout the exercise the environment forced us to be flexible and adaptive,” Maj. Anthony Bariletti, the 24th MEU operations officer, said in the news release.\n“It is the adaptability that makes Marine Expeditionary Units such a lethal crisis response force. As Marines, we gain our lethality from the ability to operate as part of a naval integrated team. The ability to conduct amphibious operations in the premier core competency of our service and this exercise provided an outstanding opportunity for the 24th MEU to hone its skills and prepare for combat as a forward deployed, sea-based Marine Air-Ground Task Force.”\nAdditional amphibious landings took place in the exercise, including a French-led force that also included Dutch Marines and Finnish Coastal Jaegers as well as British soldiers with their Viking All Terrain Vehicles.\nIt should be noted that enhanced operational rhythm has consequences for the blue force as well. For example, the Royal Norwegian Navy frigate HNoMS Helge Ingstad (F313) collided with the Malta-flagged oil tanker Sola TS during the exercise with significant damage to the frigate.\nWithout placing blame in the Thursday collision, the increased operational tempo of warships in general are pushing navies around the world harder and adding stress to operations, Eric Wertheim, author of U.S. Naval Institute’s Combat Fleets, told USNI News on Thursday.\n“As you get ops tempo increasing, navies are going to be stressed. It’s not just the U.S.,” Werthheim said.\n“The more you drive your ships, the more you’re going to have accidents.”\nThis recent piece published by NATO highlights the key aspect of how the Norwegians addressed their role in the exercise.\nAlthough the article highlights that national resilience is a NATO wide trend, this clearly is not so.\nBut it is also clear that the Nordics are focused specifically on enhancing their national resilience for sure.\nNorway uses Exercise Trident Juncture to strengthen its national resilience\nTrident Juncture 18, NATO’s largest exercise in recent years, is also the Alliance’s first military exercise to include substantial civil preparedness elements and to practice cooperation between the military and the civilian authorities. Norway, which is hosting the exercise, is using the collective defence scenario not only to train its armed forces, but also to build up its ability to respond to a crisis of any kind.\nThis is fully in line with the commitment that all NATO Allies have undertaken to increase national resilience, which is a key element of NATO’s collective defence. Resilience is rooted in the Washington Treaty. At the Warsaw Summit in 2016, NATO leaders also pledged to enhance national resilience, including by improving civil preparedness.\nTo meet this pledge, Norway added an extra challenge to Trident Juncture 18, in the form of close interaction between the military participants and civilian crisis responders, such as the health service, the police, the fire department, and non-governmental organisations. Specific events have been integrated in the exercise programme, including simulated mass casualty incidents, evacuation drills, Chemical, Biological, Radiological and Nuclear (CBRN) emergencies, taking care of evacuated civilians, and crisis management.\nIn addition to contributing to Norway’s crisis management capacities, this also contributes to interoperability with other NATO Allies. Some of the forces participating in Trident Juncture have been involved in these events as well – for instance, Danish and French CBRN soldiers, part of NATO’s Spearhead Force (the Very High Readiness Joint Task Force) in 2019, have been part of a simulation where they had to give first aid to victims of a chemical attack, and decontaminate the area.']	['<urn:uuid:5107ec20-2b72-4fe2-801b-3ea3814ba52b>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T00:20:10.754925	9	22	1372
91	foods drinks make me less depressed natural ways	Several dietary changes can help improve mood naturally. Avoid processed foods, limit sugar, and eat foods high in protein plus plenty of fruits and vegetables. It's important not to skip meals. Additionally, you should restrict caffeine as it can reduce mood-balancing serotonin levels, and limit alcohol since it acts as a depressant. Fish oil, rich in omega-3 fatty acids, can also help combat depression.	['If you frequently feel worried, have trouble getting a good night’s sleep, feel sad or down, or have trouble experiencing feelings of joy or excitement, you are likely suffering with some level of anxiety or depression. While many people often feel sad or depressed from time to time, on-going depression can greatly impact your life and color the world as you see and experience it. However, you have the ability to combat those feelings if you make an effort to incorporate some healthier habits into your day. Here are some suggestions; you don’t have to implement all of them, just choose 1 or 2 to start with and see if your mood improves overall.\nExercise can serve as a mood booster and is a natural way to alleviate anxiety. When you exercise, your body releases endorphins, which results in a positive feeling and an uplift in attitude. Regular exercise can also reduce immune system chemicals that can cause you to feel more depressed. Experts recommend that you exercise three to five times a week. Even taking the stairs instead of an elevator or walking your dog can make a difference. It can be difficult to get motivated to exercise when you are feeling depressed, so consider asking a friend to be your workout buddy or join an exercise class.\nPractice Relaxation Techniques\nThe practice of relaxation techniques can reduce stress, improve your mood, and make you feel more energized. Consider practicing deep breathing by breathing deeply into your abdomen, do yoga, or meditate. When meditating, picture an image in your mind that elicits warm, happy feelings, or simply repeat a single word, such as “peace.” Meditation doesn’t have to be a formal practice, just sitting, deep breathing with your eyes closed and picturing some place that makes you calm can be very helpful at any point during your day.\nRestrict Alcohol and Caffeine\nIt has been shown that caffeine can reduce levels of serotonin in the brain. Serotonin is regarded by researchers as a chemical that is responsible for maintaining mood balance and a deficit of serotonin can lead to depression. While it is commonly thought that alcohol can help to alleviate anxiety and stress, it actually is a depressant and can ultimately negatively impact your mood. Limiting your intake of both of these beverages can help improve your mood.\nEat a Healthy Diet\nEating well and getting the nutrients your body needs is important to support overall wellness and can help improve mood. Therefore, even if you don’t feel like eating, make sure you don’t skip meals. When choosing what to eat, avoid processed and prepackaged food and limit the amount of sugar you consume. Eat foods that are high in protein and make sure your diet includes plenty of fruits and vegetables. It’s nearly impossible to eat too many vegetables so prepare carrots, apples, celery and other vegetables in the morning and enjoy them throughout the day. Also consider speaking with your doctor or an allergist to determine if there are certain foods you are allergic to that are negatively impacting your overall sense of wellbeing and mood.\nTake Fish Oil\nIt has been found that not consuming enough omega-3 fatty acids is associated with feelings of depression. Fish oil contains omega-3s and is an easy way to ensure you are getting enough omega-3s. The best way to get fish oil is by eating the fish itself, however, if you really don’t enjoy fish you can take a supplement to get the appropriate amount of these important nutrients. Check with your healthcare provider for additional information.\nKeep a Journal\nWriting down how your day is going, when you feel sad as well as when you feel happy, can help you sort through your feelings and determine if there are specific triggers that make you feel one emotion or another. Looking back and reviewing your journal you might just find that you are happier than you thought or getting involved in more activities. You might also notice that certain foods impact the way you feel and you might be able to eliminate that food in the future. It is also a great way to keep track of what you are eating and your exercise routine and people often report feeling better by keeping track.\nParticipate in a Meaningful Activity\nDo something that gives you a sense of purpose. This could mean volunteering at your local homeless shelter, tutoring children after school, volunteering for one of the many local non-profit organizations, or volunteering at your house of worship. Doing something meaningful and using your talents and skills to help others can make you feel more fulfilled and improve your outlook on life.\nWhen suffering with anxiety and depression, it can be very helpful to talk to someone about your feelings. Reach out to a friend or family member who can offer support. Talking to a mental health professional can also prove to be very beneficial. A professional is not only there to listen, but can help you sort out your feelings, find the root of your anxiety and depression, and suggest coping techniques. Reach us at Rockland Behavioral Health Response Team 24/7 at 845-517-0400.']	['<urn:uuid:381bd03e-aa00-4c17-bd77-5f0961f5563b>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T00:20:10.754925	8	64	864
92	I've been reading a lot about melting ice in Greenland and I'm worried. How much would the ocean levels go up if all that ice melts away?	If the Greenland ice sheet were to completely melt, it would raise global sea levels by over 20 feet. The melting is already accelerating - between 1992 and 2018, Greenland lost close to four trillion tons of ice. While the complete disintegration would likely not be abrupt, there could be a point beyond which its eventual collapse becomes irreversible for thousands of years.	['As world leaders gather at the United Nations Climate Change Conference (COP26) in Glasgow, Scotland, to take bolder action against climate change, human activity has already warmed the planet 1.1°C above pre-industrial levels.\nThe Intergovernmental Panel on Climate Change (IPCC) has warned that exceeding 2°C of warming could have catastrophic consequences and that we need to keep global warming to 1.5°C. The world is currently on track to surpass both of those limits. Under the most optimistic scenario, if all 140 countries that have announced net zero targets or are considering them actually reach these goals, as well as their more ambitious 2030 commitments under the Paris Agreement, warming could be limited to 1.8°C by 2100. But will overshooting 1.5°C push us over climate tipping points, triggering irreversible and abrupt changes? The IPCC’s latest report warned of that possibility, and UN Secretary-General Antonio Guterres recently said, “…time is running out. Irreversible climate tipping points lie alarmingly close.”\nWhat are the tipping points? And how close are they?\nA tipping point is the point at which small changes become significant enough to cause a larger, more critical change that can be abrupt, irreversible, and lead to cascading effects. The concept of tipping points was introduced by the IPCC 20 years ago, but then it was thought they would only occur if global warming reached 5°C. Recent IPCC assessments, however, suggested that tipping points could be reached between 1°C and 2°C of warming.\nHere are the major climate tipping points.\nGreenland ice sheet\nThe Greenland ice sheet contains enough water to raise global sea levels by over 20 feet and its melting is accelerating. From 1992 to 2018, it lost close to four trillion tons of ice. While its disintegration is not likely to be abrupt, there could come a point beyond which its eventual collapse is irreversible for millennia.\nA new study found that ice-sheet height and melting rates in the Jakobshavn basin, one of the fastest melting basins in Greenland, are destabilizing the ice sheet. Most of the melting occurs on the ice surface because of warming temperatures, but as the height of the ice sheet is reduced, the surface is exposed to warmer air at lower altitudes, which further speeds melting.\nIn addition, less snowfall leaves the ice surface darker so it absorbs more of the sun’s heat and warms faster. Scientists are not sure if a tipping point has been passed but the study found that there would likely be more melting in the near future. In other research, scientists speculated that the critical temperature range at which the Greenland ice sheet would go into irreversible disintegration is between 0.8°C and 3.2°C of warming above pre-industrial levels.\nThe West Antarctic Ice Sheet (WAIS)\nThe WAIS is vulnerable to collapse because it sits on bedrock below sea level and is affected by the ocean’s warming. A 2018 study found that the WAIS went from ice loss of almost 58.5 billion tons a year between 1992 and 1997 to 175 billion tons from 2012 to 2017. The Thwaites Glacier on West Antarctica’s Amundsen Sea has lost a trillion tons of ice since the early 2000s, and some scientists believe it could be headed for an irreversible collapse, which could threaten a large part of the WAIS and raise global sea levels by two feet or more.\nThe Pine Island glacier, also on the Amundsen Sea, is thinning rapidly as well. A new study found that current policies, heading for almost 3°C of warming, would result in an abrupt hastening of Antarctic ice loss after 2060, while other research suggests that the tipping point for the WAIS lies between 1.5°C and 2.0°C of warming.\nAnother new study found that if the WAIS melted, it could raise sea levels three feet more than previous projections of 10.5 feet; Antarctica as a whole contains enough ice to raise global sea levels by over 200 feet.\nAtlantic Meridional Overturning Circulation (AMOC)\nThe AMOC is one of the main global ocean currents and is critical to regulating climate. Cold salty water, which is dense and heavy, sinks deep into the ocean in the North Atlantic, and moves along the bottom until it rises to the surface near the equator, usually in the Pacific and Indian Oceans. Heat from the sun then warms the water, and evaporation leaves the water saltier. The warm salty water travels up the coast via the Gulf Stream, warming the U.S. East Coast and Western Europe. Once the water releases its heat and reaches the North Atlantic, it becomes cold and dense again, and the cycle, which can take water 1,000 years to complete, continues. But as glaciers and ice sheets melt, they add fresh, less dense water to the North Atlantic, which prevents the water from sinking and impedes circulation. This may be why AMOC has slowed 15 percent since the 1950s. A recent study found that the AMOC is in its weakest state in 1,000 years. Moreover, the latest climate models project that continued global warming could weaken the AMOC by 34 to 45 percent by 2100.\nIf the AMOC shuts down, it would cause significant cooling along the east coast of the U.S. and Western Europe. This, in turn, would alter rainfall patterns, make sea levels rise, cause more drying, and reduce agriculture in the U.K. It could also potentially set off other tipping points. And even if global warming is reversed, once shut down, the AMOC would not switch back on for a long time. Scientists believe this occurred during the last ice age when a glacial lake burst and poured freshwater into the Atlantic. As the AMOC shut down, the Northern Hemisphere entered a cold spell that lasted 1,000 years.\nWhile there are still many uncertainties, some studies suggest that the AMOC’s tipping point could be reached between 3°C and 5.5°C of warming.\nThe Amazon rainforest, the world’s largest tropical rainforest, stores 200 billion tons of carbon—equal to about five years of global carbon emissions from the burning of fossil fuels—and is home to millions of species of plants and wildlife. The moisture from the Amazon’s rainfall returns to the atmosphere from the soil through evaporation and from plants through transpiration. This self-sustaining process creates clouds and more rainfall.\nBecause of logging, ranching, mining, agriculture, and fires, the Amazon has lost about 17 percent of its tree cover and at the current rate of deforestation, could reach a loss of 27 percent by 2030. The policies of Brazil’s pro-development president, Jair Bolsonaro, have led to widespread clear-cutting and the rate of deforestation in Brazil is the highest since 2008.\nIf 20–25 percent of the Amazon were deforested, its tipping point could be crossed, according to one study. Fewer trees would mean less evapotranspiration, and without enough rainfall to sustain itself, the Amazon could start to die back. In other words, parts of the rainforest could transition into a savannah, a drier ecosystem characterized by grasslands and few trees. In the process, it would potentially release 90 gigatons of CO2, exacerbating climate change. Crossing this tipping point would also result in the loss of biodiversity and ecosystem services, affect global weather patterns, and threaten the lives of 30 million people, many Indigenous, who depend on the rainforest to survive. One study found that dieback would occur if we reach 3°C of warming.\nThe Amazon is already feeling the effects of climate change, as over the last century, temperatures in the region have increased 1°C to 1.5°C. The Amazon is experiencing longer and hotter dry seasons that make it more vulnerable to wildfires, reduced evapotranspiration in response to higher levels of CO2, and there are now more drought-tolerant tree species.\nScientists are unsure whether the Amazon has a single overall tipping point, or when exactly it might be reached, and the ecosystem has some ability to adapt to changing conditions. But fires and drought could cause local changes that spread drying conditions to other regions because of an overall reduction of moisture. Twenty-eight percent of the eastern part of the Amazon is already losing more carbon than it is absorbing due to deforestation. And some climate models predict that by 2035, the Amazon will be a permanent source of carbon.\nPermafrost is ground that remains frozen for two or more consecutive years and is composed of rock, soil, sediments, and ice. Some permafrost has been frozen for tens or hundreds of thousands of years. It is found in northern hemisphere lands without glaciers, including parts of Siberia, Alaska, northern Canada and Tibet. In the Southern Hemisphere, there is permafrost in parts of Patagonia, Antarctica and the Southern Alps of New Zealand.\nFourteen hundred billion tons of carbon are thought to be frozen in the Arctic’s permafrost, which is twice as much carbon as is currently in the atmosphere. But the Arctic is warming two times faster than the rest of the planet—it has already warmed 2°C above pre-industrial levels. As it warms and thaws the permafrost, microbes come out of hibernation and break down the organic carbon in the soil, releasing CO2 and methane, which then trigger even more warming and melting. The 2019 Arctic Report Card from NOAA found that the Arctic’s thawing permafrost could be releasing 300 to 600 million tons of carbon per year into the atmosphere.\nMethane stored in ice-like formations called hydrates are also found in permafrost in ocean sediments. This methane may be released as hydrates are thawed by warming seawater. Scientists recently discovered methane leaking from a giant ancient reservoir of methane below the permafrost of the Laptev Sea in the East Siberian Arctic Ocean.\nScientists don’t know exactly how much carbon could ultimately be released by thawing permafrost or when. According to one report, 2°C of warming could mean the loss of 40 percent of the world’s permafrost.\nEl Niño and La Niña are the warm and cool, naturally occurring weather patterns across the tropical Pacific—the El Niño-Southern Oscillation, or ENSO. Every two to seven years, the pattern alternates, bringing disruptions in temperature and precipitation. El Niño causes impacts around the world, such as more drought in India, Indonesia and Brazil, and flooding in Peru. As the ocean warms, it could push ENSO past a tipping point, which would make El Niño events more severe and frequent and could increase drought in the Amazon.\nTipping point interactions\nA recent study of the WAIS, the Greenland ice sheet, the AMOC, ENSO, and the Amazon rainforest tipping points found that they could interact with one another before temperatures reach 2°C. This interaction would enable tipping to occur at lower thresholds than previously expected. The risk analysis found that a cascade could potentially begin with the melting of the ice sheets because their critical thresholds are lower. For example, as the Greenland ice sheet releases fresh water into the North Atlantic, the AMOC could slow. This would result in less heat being transported towards the north. As the North got colder, it could potentially help stabilize the Greenland ice sheet. However, it would also result in warmer water in the Southern Ocean and this could lead to more drought in some parts of the Amazon while others get more rainfall. Changes in the AMOC could also trigger changes in ENSO, leading to a more permanent El Niño state, whose impacts could lower the critical threshold for Amazon dieback.\nThe scientists say that these changes would occur over long time scales, and that the limits of computing power make it impossible to represent each climate system’s tipping point or their interactions exactly.\nCan we avoid the climate tipping points?\nSeventy-three percent of people in G20 countries think Earth is close to climate tipping points, according to a Global Commons Alliance poll. And much research indicates that if we do not curb our carbon emissions immediately to keep global warming below 2°C, we are headed for irreversible and catastrophic conditions. But some experts are more sanguine.\nRobin Bell, a polar scientist at Columbia University’s Lamont-Doherty Earth Observatory, who specializes in ice sheet dynamics, doesn’t believe the ice sheets are at a tipping point yet.\n“The most recent science is suggesting that maybe some of the runaway mechanisms we were worried about, might not occur,” she said. “For example, in terms of the WAIS, pressure on the giant river of ice could keep it from flowing. It means either we just need to keep icebergs in the way, or maybe it’s something we can think about engineering. It’s not that we have to hold the whole thing back, we just have to put a little pressure on it, and it will possibly not collapse—the ice sheet may not be as bad as we thought and maybe we have some time to get our act together.”\nBell worries more about the social tipping points than the physical ones. Will they occur fast enough to forestall climate tipping points? Social tipping points are the points where many members of society quickly and dramatically change their behavior or thinking. A 2020 study proposed six social tipping points that could help stabilize Earth’s climate: removing fossil-fuel subsidies and incentivizing decentralized energy generation, building carbon-neutral cities, divesting from assets linked to fossil fuels, clarifying the moral implications of fossil fuels, expanding climate education and engagement, and making greenhouse gas emissions transparent.\n“The real question is: Is there the social will to act?” Bell said. “And it appears that the social will is emerging. We really are starting to have serious conversations. People from the individual scale to the government scale are taking action, and that’s what needs to happen.”\nSteve Cohen, senior vice dean of Columbia University’s School of Professional Studies and a professor in the Practice of Public Affairs at Columbia’s School of International and Public Affairs, places his hope in technology. “The most important driver of change in the modern world has been technology,” he said. “And it’s a pretty simple equation: technological change leads to economic change, leads to social and cultural change, which leads to political change.”\nTechnological change can be difficult to predict but can sometimes lead to rapid changes, said Cohen, citing the ubiquitous and indispensable smart phone as a prime example. “The phone is the most important thing you take when you leave the house because it’s a portable computer that you bring around with you. Would anybody have predicted that 25 years ago?”\nHe also puts great hope in young people. “If you look at polling data, young people by a huge margin understand the climate issue. And it cuts across ideology, cuts across everything. It you’re under 30, you know, there’s a climate crisis.”\nThe pledges countries make in Glasgow at COP26 and the policies they implement afterwards will ultimately determine how close the world will come to climate tipping points.\nGreta Thunberg, the 18-year-old Swedish climate activist with millions of young followers, went to Glasgow to join a climate strike and put pressure on politicians to get them to make real commitments to curb climate change.\n“We know that change is possible because we can look back in history and see that there have been massive changes in society that have been unprecedented,” Thunberg said. “If we felt like there wasn’t any hope, we wouldn’t be activists.”\nEarth Institute at Columbia University\nThis story is republished courtesy of Earth Institute, Columbia University http://blogs.ei.columbia.edu.\nHow close are we to climate tipping points? (2021, November 12)\nretrieved 12 November 2021\nThis document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no\npart may be reproduced without the written permission. The content is provided for information purposes only.']	['<urn:uuid:26e7f091-2914-4b6a-bfc3-f2165908d2df>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T00:20:10.754925	27	63	2613
93	What process control parameters optimize gold recovery in modern plants?	In modern gold recovery plants, optimal processing requires controlling multiple parameters. The key conditions include maintaining pH levels between 10-12, using appropriate sodium cyanide concentrations, and ensuring proper carbon particle size of 1.0-3.35mm. The contact time between carbon and pulp must be carefully managed, particularly in CIP/CIL tanks. Additionally, proper equipment operation and ventilation systems are essential for both process efficiency and environmental protection. These parameters must be well understood and controlled to achieve good plant operation and design.	"['Towards Representative Metallurgical Sampling and Gold - MDPI,4 May 2018 Traditional metallurgical testwork is critical for plant design and is an inherent part of . illustrates the need for better designed sampling and testwork to CIL/CIP. CIL/CIP, Flotation. CIL/CIP, Flotation. Very fine: 10–50 µm.\nCIP/CIL Cyanidation Plant for Gold Recovery: APT Modular Mineral,13 Jun 2014 CIP/CIL Cyanidation Plant for Gold Recovery: APT Modular Mineral Processing Plants. Appropriate Process Technologies The distinguishing feature of this plant is its triangular design, which was adopted Top comments.\nGold Processing Plant - Alibaba.com,8987 products Leaching equipment process CIL/CIP gold processing plant. US $1000.0-10000.0 / Set Industrial Super quality gold mineral processing plant.\ncil cip gold mineral separation plant new,Gold CIL (Carbon in Leach) Process is an efficient design of extracting and the design of new style quot;minimalistquot; gold plants. as CIP/CIL and flotation it was mineral machine for gold mining; good efficiency ball mill for grinding gold.\ndynamic simulation of the carbon-in- pulp and carbon-in - SciELO,the plant, and in the carbon-in-leach process leaching and adsorption occur the contact time between the carbon and the pulp is longer are the best carbon transfer Brazilian Journal of Chemical Engineering the conventional CIP and CIL tanks can also be us\nCIP / CIL Interstage Screening Multotec,Multotec manufactures a range of wedge wire cylinder screens for CIP and CIL tanks Mineral Processing Equipment The symmetrical, seamless design of our wedge wire interstage cylinders quick lead and delivery times, wherever your gold processing plant may\nGold CIP Production Line - HOT Mining,Gold Carbon-in-pulp (CIP) production is the sequential leach then absorption of gold from ore. ore, The most different from CIL process is in CIP process, gold leaching and The best conditions for CIP process in China: PH=10-12, sodium cyanide Usually, sm\nSmall Scale CIP/CIL Cyanidation Plant - Appropriate Process,The APT TriTank is available as a small scale CIP/CIL cyanidation plant. and even better way of using cyanidation as a gold extraction process, even at such The design has multiple benefits including enhanced metallurgical performance. APT encourages mini\nA New Age Gold Plant Flowsheet for the Treatment - Gekko Systems,parameters for the design of new style ""minimalist"" gold plants. . as CIP/CIL and flotation it was relegated to the treatment of low grade ores due to its low . good ventilation, preferably with forced extraction, is recommended as a precaution.\nGold CIP CIL Process Gold Extraction Optimization solutions - JXSC,Gold Carbon-in-pulp (gold CIP) production is the sequential leach then Mineral Processing Experiment · Process Design · After-Sales Service; Close less than 0.015%, the particle size of activated carbon is 1.0-3.35mm, the best choice of carbon .\nTri-K Gold Project Bankable Feasibility Study - Avocet Mining PLC,2 Jul 2018 METALLURGICAL TEST WORK AND PROCESS PLANT DESIGN 184 continued feasibility investigations have led to better definition of the project. 2.3 Mt/a open-pit mining operation and a gravity/CIL process gold recovery plant. .\ngold process modelling. ii. the effect of ore type on leaching and,\'Department of Chemical Engineering, The University of Melbourne, Victoria 3010, Australia It is essential to be able to predict processing outcomes on a CIPKIL plant as showed that the optimum distribution of carbon in a CIL or CIP tank was .. compari\n(PDF) Gold CIP and CIL process optimization in a capital constraint,Gold CIP and CIL process optimization in a capital constraint environment. Article (PDF Available) in advantage for a mineral processing plant. The design houses to design and size the plant,. and in most . obtain the best fit. The plant\ncip gold cyanidation for gold ore high capacity,Xinhai devotes to providing ""Turn-key Solution for Mineral Processing Plant"", namely ""design Gold Cil Processing Line That because the design of early equipment is not perfect, such as lower tank disk filter mineral separation plant best &m\nEmployee Search Mining Employment Services,Top Menu. Job Alerts · Meet The Team · Contact · Information · Employers · Job Seekers · News Process Manager : Manage all on site Safety, Training, Production and gold, CIP, CIL, crushing and grinding plants, flotatio\nCIL Archives - International Mining,17 Jan 2019 The study shows total gold production of 3.3 Moz over a 10-year of the mineral deposits to support the CIL plant design, and learned a lot from current operations. of 89.7%) and better financial returns than heap leach processing for CIL,\nGold Mining Process Development – Denver Mineral Engineers,determine the best recovery process and that metallurgical testing is almost always required Carbon-In-Pulp (CIP); Carbon-In-Leach (CIL); Carbon-In-Column (CIC) Where amalgamation plants could recover about 60% of the gold present, . not a testing laborato\nMODIFIED TITANIUM WHITE FROM MINERAL PROCESSING,9 Jul 2013 Thus, by using the best fit model, the Kaymaz plant was optimized for existing Carbon adsorption unit of a carbon in pulp (CIP) Gold Ore Cyanide .. the design of CIL and CIP circuits, Minerals Engineering, 24, 484–494.\nGold CIL & CIP Gold Leaching Process Explained CCD,2 Nov 2018 Developments in the Carbon-In-Pulp Process; Design Criteria; Commercial CIL is slightly different from another gold extraction process called CIP or Carbon-in-pulp (CIP) is the sequential leach then absorption of gold from ore. The first in\nA dynamic CIP/CIL process simulation using MATLAB SIMULINK,1 Jan 2008 dynamic CIL/CIP plant performance under varying operating conditions. This will .. In the process it leaches the gold from the ore particles into . present must be well understood to achieve good plant operation and design.\nmineral processing cost of cip system professional - Spoonful Eatery,pioneer gold cip plant equipment Mineral Processing EPC. processing system Machine for sale Find the Right and the Top Cil cip gold processing for your coal handling plant! The process design of gold leaching and carbon in pulp.\nGold CIL Production Line,Gold CIL Process,Gold CIL Plant-Beijing,Gold CIL (Carbon in Leach) Process is an efficient design of extracting and recovering gold from its ore. Gold CIP Production Line It has become the first choice of modern gold mining and processing plant design. The best conditions for CIL process in Chi\n100tpd20000 tpd customized gold ore cip machine plant supplier,China Gold CIP Production Line Plant , Gold Mining Machine. Gold cyanide leaching production plant, new complete CIL plant design . . best quality tpd gold cip processing plant for sale which gold processing is best rr classifier gold ore\nThe process design of gold leaching and carbon-in-pulp - SAIMM,gold plant will be that of the leaching and carbon-in-pulp leach and CIP/CIL circuits in order to ensure The process design of gold leaching and carbon-in-pulp circuits. △. 14 carbon. Cyanide air/oxygen. Lime flocculant. Run of mine ore. Process works\nFactors influencing the rate of gold cyanide leaching and adsorption,Minerals Engineering The percentage of annual global gold production by activated carbon-based processes demonstrated better gold recoveries than the traditional filtration/Merrill Crowe process, with lower capital and operating costs. This paper examines\ngold cil or cip machine gold leaching plant in su - lelogis1170.be,Response high quality cil gold plant , cil gold plant for mineral processing. Africa best price widely used gold cil process cip gold plant . Gold CIL (Carbon in Leach) Process is an efficient design of extracting and recovering gold from its\nAbout ZipaTank™ - About Us ZipaTank,The ZipaTank™ incorporates an elegant modular design with simple mechanical overflow system are fabricated into the panels under controlled factory conditions. at ground level, with the ZipaTank constructed from the top strake downwards. CIP/CIL processin\nGold Cip Plant Wholesale, Cip Plant Suppliers - Alibaba - Alibaba.com,1210 products Gold ore processing plant mining equipment gold cip cil flotation production line China CIP Gold Wash Plant , Gold Wash Plant of Flotation Production Line .. Elution Electrowinning Cell Gold CIP Plant With The Independent Design Flow Chart ..\nInsights from simulation of CIL/CIP circuit Part 2 – Simulation studies,Mineral Processing and Extractive Metallurgy model of a carbon in leach (CIL)/carbon in pulp (CIP) plant for recovery of gold and silver was detailed.\nGold-Precious Metals Conference - ALTA Metallurgical Services,PROCESS INTENSIFICATION IN GOLD PROCESSING Damian Connelly, Mineral Engineering Technical Services (METS) (Australia) SCALE CIP AND CIL PLANTS . equivalent to, or better than, those achieved with cyanide leaching.\n- gold cip related equipment for lead ore low noise\n- float tank cil gold plant\n- tpd gold mining equipment gold cil plant for sale\n- cil cil gold processing\n- minerals used in gold mining gold flotation cip\n- mineral mining equipment in china cil\n- gold cip plant mobile gold plant overall service\n- cil cip gold thickening and clarifying solutions cheap\n- gold cip related equipment for mica heavy\n- basic operation of cip gold plant\n- cil cip gold iron magnetic separation high frequency\n- cip plant gold cip gold recovery overall service\n- fine gold cip production line models\n- gold cip plant mineral separation plant low consumption\n- gold cip and gold mining equipment\n- gold copper lead zinc ore flotation production line gold cil\n- gold cip for fining ore portable\n- mineral mining equipment en china cil\n- cip gold cyanidation with mineral processing portable\n- cip gold cyanidation with ore dressing new\n- gold mine in saskatchewan\n- zenith wheeled jaw crusher plant\n- vibrating screens for sale mining\n- stone crusher process indiastone crusher process land\n- costos molinos mining\n- local mining supplies nc\n- pyro processing pyro\n- crusher products aggregate plants crushing plant\n- gold mining stamp mill for sale zimbabwe\n- agitation tank for fluorite for gold mining\n- fri china high quality ce spring cone crusher ore plant\n- pe ball mill for quarry and mining\n- froth flotation process explanation for 9th icsa\n- gold mining cambodia\n- impact mill china for sale\n- small size cassava starch processing plant\n- fi jaw modelcrusher plant sale in india\n- placer gold in malaysia\n- china mining gypsum\n- portable vibratory wash plants', 'Mar 17, 2017 Environmental & Social Project Construction Project Valuation & Optimization Gold CIL & CIP Gold Leaching Process Explained CCD This is a gold extraction process called cyanidation where carbon is added to the . one type of process equipment to another i.e., between leach tanks, thickeners,\nMar 23, 2017 Process Development; Selecting a Process; Gold Recovery; Amalgamation; Flotation; Leaching economics, expected recoveries and environmental considerations. . Jigs are the most common types of gravimetric equipment placed to This situation is sometimes called skin flotation. CIP circuit.\nApr 2, 2010 Cyanidation and gold extraction methods . Impacts of cyanide on human health and the environment . .. Carbon-in-pulp CIP by separation processes e.g. crushing, grinding, size-sorting, flotation and other potential for the reuse of the precious metals used in electronic equipment within Europe.\nENVIRONMENTAL AND COST OUTCOME FOR GOLD PLANTS. with the Merrill Crowe Process, CIP or CIL the use and further development of gravity concentration devices for the recovery of gold in modern gold plants went into decline.\nLow capital costs, low environmental impact and maximum return to . From experience testing a large range of materials at Gekko Systems Pty Ltd the optimum Within the gold mining industry the use of gravity equipment until recently was as CIPCIL and flotation it was relegated to the treatment of low grade ores due\nGold Ore Processing: Project Development and Operations, Second Edition, brings together all Clearly, there is little point in using the latest analytical methods and equipment and Gold deportment in final flotation concentrates is carried out in conjunction with . Safety, Process Control and Environmental Management.\n1-8 1.4 GOLD EXTRACTION AND BENEFICIATION PRACTICES 1-11 1.4.1 2-2 2.1.3 Environmental Setting 2-4 22.214.171.124 Surface Water 2-4 126.96.36.199 Geology . Four main techniques are used in the beneficiation of gold ore: cyanidation, flotation, Carbon-in-Leach » The CIL technique differs from CIP in that activated\nGold Ore Processing - 2nd Edition - ISBN: 9780444636584, 9780444636706 . Safety, Process Control and Environmental Management. Chapter Flotation of Gold and Gold-Bearing Ores. 1. Modeling CIP Circuits; 4. . Gold – A Key Enabler of a Circular Economy: Recycling of Waste Electric and Electronic Equipment.\nCarbon-In-Pulp CIP; Carbon-In-Leach CIL; Carbon-In-Column CIC. Refractory ore The prospectors gold pan is the most familiar gravity concentration device. To function The flotation process in general does not float free gold particles but is considered environmentally acceptable to emit large quantities of sulfur\nJohn Chadwick reports on advances in gold recovery, including .. environment, maximising process run-time and optimising Conventional flotation machines are typically limited to a .. Spectroscopy ICP-MS and verified by Fire Assay.\nGold and silver typically occur at very low concentrations in ores - less than 10 gt economically viable method of extracting the gold and silver from the ore. These reagents present risks to health and the environment, and are more expensive. and construct the necessary equipment to safely dissolve the solid cyanide\nGold cyanidation is a hydrometallurgical technique for extracting gold from low-grade ore by The ore is comminuted using grinding machinery. Depending on the ore, it is sometimes further concentrated by froth flotation or by . to reduce environmental impacts with third party audits of a companys cyanide management.\nHatch Gold capabilities include gold process development and design, gravity separation, flotation and refractory gold ore pretreatment. Design teams with extensive experience in cyanide-in-leach CIL, cyanide-in-pulp CIP, carbon elution, be significantly improved by proper design and process equipment selection.\nOct 8, 2014 Table 1.1: Plant Capacity, Gold Recovery, and Gold Production Table 2.10: Gekko Test Work Comparison of Recovery Methods Appendix 4 Mechanical Equipment List CGR. Continuous Gravity Recovery. CIL. Carbon-in-Leach. CIP . Option 3 – Gravity followed by Flotation and Intensive Cyanide\nof those specifically indicated are property of the Artisanal Gold Council and can . Although social and environmental problems are common in this trying to improve mining practices and reduce mercury use. 1. . exposure through inhalation of mercury vapor when safety equipment like .. center of the pan into to a cup.\nJun 3, 2009 Keywords: Gold ores, agglomeration, coal, environmental, fine particle processing The process has been applied successfully on gold recovery in some pilot plants 1 Cadzow, .. Flotation is performed in mechanical and pneumatic flotation machines. 1949, First application in carbon in pulp CIP.\nTypical range and profile of gold losses from CILCIP processing. • Common Experiences in benchmarking and process modelling in this environment, and possible future . explained as ore related or equipment related, without proper investigation. And . considered when performing flotation geometallurgical testing.\nMay 4, 2018 focusses on sampling and testwork for gold recovery determination. Equipment operation process can be particularly challenging in the gold environment and may recovery determination e.g., Leach, GRG and flotation methods. CIL: carbon-in-leach; CIP: carbon-in-pulp; POX: pressure oxidation;\nSGS is at the forefront of cyanide leaching technology for the gold mining industry. Quality, Health, Safety & Environment > . We have the equipment and expertise needed to design the most efficient and Our team includes world class professionals who have developed techniques such as CIP carbon in pulp, CIL\nApr 26, 2018 Tailings, mineral processing, arsenic, sulphide minerals, flotation, . Ore sample from the Kuikka 2 gold ore deposit was delivered to GTK for Batch flotation tests were carried out using an Outotec GTK LabCell flotation machine . using ICP-OESMS method by Labtium modified from methods SGS T01\npotential environmental impact of any waste solutions and decreases spectrometry ICP-OES to reduce the mercury to undetectable necessary to extract gold from complex ores which comprise gold in removed by dissolved air flotation 5. Hutchison .. of the equipment 0.01 ppm during the test period. Additionally\nDETAILED ENVIRONMENTAL IMPACT ASSESSMENT REPORT construction engineer M.Sc P.Bold and president of mineral processing association of Ore from Gatsuurt hard rock gold deposit with Flotation and Cyanidation. Design criteria of CIP leaching and Adsorption will be evaluated in the 2B phase of the.\nThe mineralogy determines the recovery process of gold. The devices developed and used for gravity 1.3 Flotation and subsequent processes refractory ore processing . During the CIP process activated carbon particles are added to the leached pulp . environment that is needed for flotation of a particular mineral.\ntives to amalgamation process for gold such as intensive cyanidation, borax Artisanal and small-scale mining contribute to environment contamination . cult, because finer particles of gold tend to float on the mercury not amal- gamating. pieces of equipment units and agitators as well as high costs due to cyanide.\ntechniques to recover the valuable mineral resources from the ore. The choice of a suitable technique, which is both environmentally sound and economically the substantial development of a sustainable gold mining sector in that .. Tank leaching: is a technique where crushedmilled ores or flotation concentrates are.\nXinhai Gold CIP Process: Fast Absorption Rates, High Recovery Rate 【Application】Flotation gold concentrate or amalgamation, gravitation Energy saving, Environmental protection and economical; Simple process equipment, Low cost;.\ngold occurring in alluvial and bed-rock deposits and the various methods used for environmentally damaging use of mercury amalgamation. This is due to both poor technique and, in some cases, poor equipment; African .. Froth flotation has the potential to be used for the recovery of fine gold. . and CIP cyanidation.\nFeb 1, 2012 environmental hazardous substances such as sulphur dioxide gas or sulfuric acid, Extraction of gold and silver from the argentopyrite argentite ore .. described a device of the 1940s, the “electronic coagulator” which electrochemically coagulation, absorption, adsorption, precipitation and flotation.']"	['<urn:uuid:4c873878-bfa7-4ad2-ab50-747a999f40f8>', '<urn:uuid:fdade971-9a41-408b-9f7e-8963e84a3002>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T00:20:10.754925	10	79	2890
94	modern agriculture young female farmers development impact climate change negative positive aspects	Modern agriculture is experiencing a transformation led by young female farmers who bring both positive developments and face significant challenges. On the positive side, these farmers are introducing sustainable practices, focusing on small diverse farming operations, and building strong community connections through local markets and CSA programs. They are highly educated (95% have post-secondary education) and are driven by environmental consciousness and desire for positive impact. However, they face serious challenges due to climate change, particularly in regions like the Caribbean where women farmers experience reduced yields, increased pest problems, and greater vulnerability to extreme weather events. The economic impact is substantial, with climate change projected to cause $22 billion in losses in the Caribbean region alone. Despite these challenges, many young women farmers are succeeding through collaborative approaches and innovative farming methods, though they continue to face barriers in accessing resources and technical support.	['The New Face of Farming\nLooking for a job? Check your local classifieds and you’ll find that many types of ads abound. But have you ever seen a “Farmers Wanted” ad? I haven’t. Nevertheless, the agricultural sector in Canada is in desperate need of help.\nIn the next ten years 75% of existing farmers are going to reach retirement age with few prospects of replacement. Pause to ponder the gravity of this situation. Who will grow our food? Will we become totally dependent on industrial farms and imported food? Are we comfortable with relying on imported labour? What do these things mean for food prices and quality? What implications do they have for the social and economic fabric of rural communities?\nLeave it to young people to face these questions head on. Despite an aging farmer population and uncertain finances, there is a small but steadily growing number of young farmers who have the courage, dedication, passion and ingenuity to turn conventional farming on its head and are making a go at it in new and exciting ways. Who are they and how are they doing it?\nToday’s new farmers include a diverse body of young people ranging from new immigrants, urbanites, scholars and those coming to farming from a previously established career. Of these new farmers, a growing proportion is made up of young women. Most of them are digging into their work for different reasons and in different ways than their predecessors. A survey taken by FarmStart, an Ontario-based organization dedicated to helping people pursue careers in agriculture, found that 68% of its interns grew up in the suburbs or city and 95% had at least some post-secondary education. Intention, rather than necessity, appears to drive these young people to what is still seen as an unusual career choice.\nMany young farmers share a deep frustration with the current state of the agricultural system and see their own labour as a means to enact tangible, positive impact. They find that amidst so much global turmoil, being able to grow healthy, sustainable food is a deeply satisfying undertaking. For Erica Lemieux, a 26 year old urban farmer in Toronto, this was the very reason she chose to farm. “Learning how to grow food is a small thing but it’s also a huge thing,” she told me. “Young people are realizing they’ve lost this basic skill and are choosing to reclaim this knowledge. In doing so, they’re rejecting the current agricultural system and reinventing what it means to farm.”\nAnd reinvent they do. While their motivations may be rooted in idealism, the actions of young farmers are intelligent, practical and forward-thinking. Today’s modern food system is characterized by mega-farms, mechanization and monocrops. While this system is profitable for some, we are seeing the dire ramifications of such a food system in the form of labour rights violations, ecosystem devastation, inhumane treatment of animals and a lack of transparency as to where our food comes from, who grows it and how. In many ways, the approach young farmers are taking today is bringing agriculture back to its roots – small, diverse farming operations with a human face.\nIt’s this approach that is making farming for young and new farmers both financially and ethically sound as they strive to make a living in a way that gives back to communities and the environment. According to Tarrah Young, a 34 year-old in her 10th year of farming, “people don’t pay for food but they will pay for the knowledge that they are doing good. Young farmers are invested in sustainability and contributing to the health of the planet and are building a loyal customer base around that.”\nTo truly sell that knowledge of health, sustainability and community, you have to live it. A common thread among young women farmers is their relationship-based approach to farming. This means knowing their plants, animals and soil through-and-through so that they can respond effectively when things are out of balance. This also means building relationships with the community to whom they sell and with whom they work.\nThese young women tend to sell their products locally – through a farmers’ market, a Community Shared Agriculture (CSA) program, or at the farm gate itself. And whether they are the lead farmer or are farming in partnership, they take a collaborative approach, bringing in friends, family, volunteers and paid or work-exchange interns into the fold. The steep financial costs of getting started at farming make this collaborative approach imperative for many.\nBut this collaborative and supportive spirit works both ways. Now that her farm is up and running (in its sixth year of operation), Caitlin Hall, a 30-year-old woman farming solo in Southern Ontario, explains the importance of community in agricultural contexts: “Farming by nature isn’t a solitary activity … so while on paper it’s just my place, I have a great network of people here.” She is happy being able to offer living quarters to several interns each year who have even started their own farm businesses using Caitlin’s land as an incubator space.\nSince most young women entering farming aren’t coming from farming backgrounds, a hands-on education is also an essential ingredient to farming successfully. There’s a steep learning curve associated with acquiring necessary knowledge and skills. While some universities and colleges offer courses in agriculture, farm-based education programs are quickly gaining in popularity across Canada. In Ontario, FarmStart offers a farm incubator program, skill-based trainings and farm business courses. THINKFarm is a similar organization in Nova Scotia. Some provinces even offer start-up grants for new farmers to help with initial capital costs.\nSexism and ageism are realities for many young woman farmers. Old-schoolers in the agriculture scene often don’t take them seriously and may be slow to recognize them as the primary farmer when working alongside male colleagues or partners. Despite such negative experiences, the women I interviewed often expressed their surprise at how readily they were accepted and supported by the older male farmers in their community. For Amy Lounder, the 31-year-old proprietor of a winter crop CSA in Nova Scotia, men in their fifties and sixties were her main source of support in her first year of farming. They had the expertise and tools to help her and to her surprise, they seemed to want to support anyone who was interested in farming, given that young people aren’t getting into it on a broad scale.\nBut all is not rosy for a young woman starting a career in farming. There are many challenges to test your resolve along the way. Making a living at it is tough, especially in the first few years. Personal affinities for hard work, the outdoors and multi-tasking are definitely important. But don’t forget passion! For many young farmers, a real love of farming is what helps them weather the job’s many challenges. Farmers tend to be a resilient bunch, and often see the silver lining behind every cloud. As Caitlin Hall notes, “it’s not the easiest way to make a living, but it’s entirely possible. The bonus is that you get a great place to live and great food to eat. Plus you work for yourself.”', '“…there is no social sector more invisible, less understood and less served, than that of rural women, despite the vital role they play in our rural communities…” Former President of Costa Rica, Mrs. Laura Chinchilla\nBlue Mountain Coffee is one of the most expensive specialty coffees in the world, and it caters to a hyper-niche market of luxury coffees, costing over US$58/lb. Its prized berries grow on the cool steep slopes of Jamaica’s Blue Mountain range above 3000ft where it fuels the livelihood of over 4,000 smallholder farmers. However, this space of luxury coffee production has not escaped the far-reaching effects of climate change.\nThe Blue Mountain coffee space has been invaded and disrupted by changing climatic conditions. The livelihoods of Jamaican women coffee farmers have been particularly affected by variable rainfall, extended dry seasons, reduced yields, and increased pests and diseases. Drought conditions have led to inconsistent production and poor coffee quality which reduces coffee volumes that can be sold and result in financial losses.\nMoreover, the unpredictability and inconsistency of climatic conditions have also increased the cost to maintain coffee farms, as the farm inputs required to increase yield and treat diseases such as the coffee leaf rust are often expensive.\nSimilar events have been documented globally where rural women experience a greater impact from changing climatic conditions as their dependence on natural resources for livelihood maintenance is often disrupted by hydro-meteorological events such as hurricanes, storms, floods, droughts, and landslides, among others.\nUnfortunately, even though the Latin America and Caribbean (LAC) region contributes less than 10% of global emissions, their economies, sectors, infrastructure, and people have been facing the consequences and adverse effects. According to the State of the Climate in Latin America and the Caribbean 2020 report, the passage of Hurricanes Eta and Iota wreaked havoc in Guatemala, Honduras, Nicaragua, and Costa Rica; Brazil, Bolivia, Paraguay, and Argentina’s Pantanal region experienced an unprecedented drought and fire season. The glaciers in the Chilean and Argentine Andes continue to retreat and the Caribbean region continues to face deficits in rainfall.\nMoreover, considering that many economies and livelihoods within the LAC region are dependent on climate-sensitive sectors such as agriculture, food, and nutrition security are also in the crosshairs of being adversely affected. It is expected that by 2050, Central America and the Caribbean will experience a one-fifth reduction in agricultural yield for beans and maize.\nOther areas that remain highly exposed and vulnerable include, human health, water resources, settlements, and biodiversity. Economically, the combined annual damages for Latin America and the Caribbean due to climate change impacts are estimated to be at US$100 billion by 2050, which is almost as much as Ecuador’s GDP.\nParticularly for the Caribbean region, approximately US$ 22 billion in losses is projected. Therefore, the devastating impacts on the economic, cultural, environmental, physical, and social fabric of countries will erode any gains and progress they have made in development.\nHowever, amidst these threats, the gendered face of climate change remains a critical area, as the difference in the capabilities of men and women to adapt to changing climatic conditions continues to generate concern. The recent IPCC assessment reported with high confidence on the unequal impact of climate change on men and women. This is primarily due to the gender inequality and inequity that influences the control of and access to assets, resources, services, and decision-making processes.\nThis disparity disproportionately affects women and increases their vulnerability to climate change impacts and disasters, as well as affects their ability to cope and adapt. ECLAC acknowledged that “women and children are 14 times as likely as men to die during a disaster”. Rural women, particularly in the Latin America and Caribbean region are expected to face the brunt of climate change impacts and disasters.\nRural women play a critical role within the food system, where on average they comprise of 43% of the agricultural labour force. Contributing to the food security of households and communities, their roles range from producers, planters, harvesters, farmworkers, rearing livestock, as well as domestic care. However, despite their contribution, the work of rural women is often unrecognized. Their reality is persistently characterised by poverty and structural inequalities, particularly since they have less access to productive resources. Climate change impacts and disasters tend to exacerbate these pervasive challenges, thus increasing their vulnerability.\nAccording to Oxfam, around 30% of rural women in Latin America own agricultural land, while access to technical assistance is only available to less than 5%. For example, in Brazil, and Guatemala rural women are faced with having less access to credit, technology, mechanization, land, and other assets, thus limiting their adaptation and decision-making capabilities. In Colombia, climate change has affected women coffee producers by exacerbating the spread of the coffee berry borer. Their ability to successfully manage this pest and apply adaptation measures has been hampered by their lack of access to technical expertise, information, and decision-making control. Similar experiences are also found within the Small Island Developing States of the Caribbean (SIDS) region where rural women in Haiti and other territories face socio-economic barriers that are rooted in gender inequality; increasing their vulnerability and heightening their risk to climate shocks and disasters.\nFor Jamaica’s women coffee farmers in the Blue Mountains, the impacts of climate change have also been fuelled by the disparity in receiving advisory services. Based on a pilot survey conducted by IWCA Jamaica (JAWiC), a woman coffee farmer for over 10 years perceived that, “people [wider industry stakeholders] more want to help the men or look to the men” with regard to farm management. Women also noted barriers in accessing resources, technical training, and engaging in opportunities which allows upward mobility in the coffee value chain, as well as the ability to occupy spaces of leadership within their communities.\nAccording to another local coffee farmer, there was a “lack of recognition, lack of resources such as training, workshops, mentorship, and lack of support” for women in the coffee industry. Likewise, a woman organic coffee farmer who has operated in the industry for over 20 years as a roaster and exporter observed that “men want to be the ones to tell you what to do…. and think women don’t know anything”. This gender dynamic and uneven power relation illustrate that some women perceive their contribution to the coffee industry as ‘lesser than’, compared to men.\nHowever, ignoring women’s contribution to rural livelihoods and limiting opportunities to access resources not only lowers a nation’s economic potential, but also weakens its resilience to climate change impacts. Therefore, addressing the root causes of gender inequality in the region is vital to securing sustainable rural livelihoods.\nFor women coffee farmers in Jamaica, there have been efforts to effect change by facilitating rural women coffee farmers with access to opportunities such as training, technical knowledge, financing, technology, and productive resources. Empowering rural women to tackle the impacts of climate change not only requires a gendered lens in reforming institutions and investment in services, but also tackling deep-seated cultural, socio-economic, and patriarchal norms that limit the full expression of women’s productive capacity and decision-making power. Last but not least, it will allow mankind to continue enjoying one of the best coffees in the world.']	['<urn:uuid:48133c1a-caf5-4141-b845-94fdada7e758>', '<urn:uuid:3b5e55b6-8857-4c43-941f-d3d3c25b6a1a>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T00:20:10.754925	12	145	2398
95	What makes synthetic diamond materials special when it comes to cleaning contaminated water?	Synthetic diamond materials, particularly boron-doped diamond electrodes, are effective for water decontamination because they can produce strong oxidizing species that enable fast mineralization of organic contaminants. They are also specifically used for water disinfection processes.	['Synthetic Diamond Films: Preparation, Electrochemistry, Characterization, and Applications\nDescriptionThe book gives an overview on the current development status of synthetic diamond films and their applications. Its initial part is devoted to discuss the different types of conductive diamond electrodes that have been synthesized, their preparation methods, and their chemical properties and characterization. The electrochemical properties of diamond films in different scientific areas, with special attention in electroanalysis, are further described. Different strategies to modify these electrodes are also discussed as important technologies with ability to change their electrochemical characteristics for a more specific electroanalytical use. The second part of the book deals with practical applications of diamond electrodes to the industry, organic electrosynthesis, electrochemical energy technology, and biotechnology. Special emphasis is made on the properties of these materials for the production of strong oxidizing species allowing the fast mineralization of organics and their use for water disinfection and decontamination. Recent biotechnological development on biosensors, microelectrodes, and nanostructured electrodes, as well as on neurochemistry, is also presented. The book will be written by a large number of internationally recognized experts and comprises 24 chapters describing the characteristics and theoretical fundaments of the different electrochemical uses and applications of synthetic diamond films.\nPREFACE TO THE WILEY SERIES ON ELECTROCATALYSIS AND ELECTROCHEMISTRY.\nList of Contributors.\nPart I. Synthesis of Diamond Films.\n1. Electrochemistry on Diamond: History and Current Status (John C. Angus).\n2. Synthesis of Diamond Films (Vadali V. S. S. Srikanth and Xin Jiang).\n3. Types of Conducting Diamond Materials and their Properties (Marco A. Quiroz and Erick R. Bandala).\nPart II. Electrochemistry of Diamond Films.\n4. Electrochemistry of Diamond (Yuri Pleskov).\n5. Applications of Polycrystalline and Modified Functional Diamond Electrodes (Yasuaki Einaga and Akira Fujishima).\n6. Diamond Ultramicroelectrodes and Nanostructured Electrodes (Katherine B. Holt).\nPart III. Electroanalytical Applications.\n7. Electroanalytical Applications of Diamond Films (Weena Siangproh, Amara Apilux, Pimkwan Chantarateepra and Orawon Chailapakul).\n8. Cathodic Pretreatment of Boron-Doped Diamond Electrodes and Their Use in Electroanalysis (Leonardo S. Andrade, Giancarlo R. Salazar-Banda, Romeu C. Rocha-Filho and Orlando Fatibello-Filho).\nPart IV. Industrial Applications.\n9. Use of Boron-Doped Diamond Electrode in Electrochemical Generation and Applications of Ferrate (Virender K. Sharma, Enric Brillas, Ignasi Sirés and Karel Bouzek).\n10. Electrochemical Oxidation of Organic Compounds Induced by Electro-generated Free Hydroxyl Radicals on BDD Electrode (Agnieszka Kapalka, Helmut Baltruschat and Christos Comninellis).\n11. Modeling of Electrochemical Process for Water Treatment using Diamond Films (Onofrio Scialdone).\n12. Production of Strong Oxidizing Substances with BDD Anodes (Ana Sánchez-Carretero, Cristina Sáez, Pablo Cañizares, and Manuel A. Rodrigo).\n13. Ozone Generation Using Boron – Doped Diamond Electrodes (Yunny Meas, Luis A. Godinez and Erika Bustos).\n14. Application of Synthetic Diamond Films to Electro-Oxidation Processes (Marco Panizza).\n15. Fabrication and Application of Ti/BDD for Wastewater Treatment (Xueming Chen and Guohua Chen).\n16. Application of Diamond Films to Water Disinfection (Jessica H. Bezerra Rocha and Carlos A. Martínez-Huitle).\n17. Fenton-Electrochemical Treatment of Wastewaters for the Oxidation of Organic Pollutants using BDD (Enric Brillas).\n18. Electrochemical Energy Storage and Energy Conversion Systems with Diamond Films (Juan M. Peralta-Hernández, Aracely Hernández-Ramírez, Jose L. Guzmán-Mar, Luis Hinojosa-Reyes, Giancarlo R. Salazar-Banda and Carlos A. Martínez-Huitle).\n19. Use of Diamond Films in Organic Electrosynthesis (Siegfried R. Waldvogel, Axel Kirste and Stamo Mentizi).\nPart V. Bioelectrochemical Applications.\n20. Diamond Sensors for Neurochemistry (Bhavik Anil Patel).\n21. DNA Modified Diamond Films (Nianjun Yang and Christoph E. Nebel).']	['<urn:uuid:5519208f-d628-41f7-88ed-b706bdbbcd59>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T00:20:10.754925	13	35	557
96	What steps should be taken when dealing with a clarinet cork that is too thick on new instruments or joints to ensure smooth assembly?	First verify that the cork, not the wood, is causing the problem. Then, either use 240 grain sandpaper or a fingernail file to carefully remove some of the cork until achieving a firm but smooth fit. Apply cork-grease before assembling the joints together.	['1.Barrel gets stuck: Humidity can enter the wood and increase the diameter of the top-joint tenon. This can cause problems, because the connection between the top-joint and barrel becomes too tight, which makes it hard or even impossible to take the barrel off. The solutions are:\n1. Hold the barrel very tight in your hand and instead of trying to turn the barrel, wiggle it upwards.\n2. Bend the barrel in all 4 directions, like you would like to break it off. Don´t try it to hard and not on Greenline instruments! Greenline tenons can break! .\n3. Lay a leather belt around the barrel in a sling and try to turn it.\n4. Just wait a day or two and have the instrument dry by itself. Keep it in a dry environment.\n2.Barrel wobbles If the Barrel wobbles, lay either a threat, some tape or paper around the top-joint tenon, before you push on the barre.\nIf a barrel cracks at the top or bottom, where the tenons of the mouthpiece or top-joint enters, the crack might be caused by physical force. To prevent that from happening, be always careful not to use to much force when you ensemble the clarinet. Don?t tilt the different parts just turn or push them in straight.\nIf a crack goes all the way into the bore, humidity would have caused it. It can nicely be glued and the barrel will be fine again. As an emergency solution, put some tape around the barrel to make it seal again. Do not use oil or grease on the crack, because that will make it difficult if not impossible to glue afterwards.\n4. Loose sockets When the humidity is very low and the wood shrinks metal tenons can loose their bound to the wood and become loose. If that?s happening it can easily be tightened again. If the metal receiver was glued on with a thermo glue or shell lack, the metal tenon needs just to be heated up with a torch. If in doubt ask an experienced repairman do it for you.\n5. Barrel versus tuning rings To change the length of an instrument just with a barrel will have its biggest effect on the notes close to the barrel. A better solution might be to combine it with some tuning rings. This way, the instrument can be prolonged just a bit at the barrel and some more at the lower-joint. A more even tuning will be possible.\n6. Zoom barrels If you turn a zoom barrel to fare apart, you might not get it to work again. Don´t use force, but give it to someone who knows how to handle it.\n7. The original barrel The original barrel was reamed together with the top-joint of the instrument when it was built. Therefor is there a good chance that it will have a very good and smooth connection to the top-joint in the beginning. Over time both joints might develop differently and the bores of both joints might become oval. In some cases this might cause problems. Try to turn the barrel in different positions and see if it improves it?s performance. If so just remember the position and keep it that way. There is no reason to exchange the barrel just because of this.\n8. The cork is to thick New instruments or joints might have a cork which is to thick to go on smoothly. First make sure that it is really the cork and not the wood that?s causing the problem. If you ensemble the pieces and the cork is to thick, just take either a 240 grain piece of sandpaper and take of some of the cork. Alternatively use a fingernail file and remove some cork. Do it carefully until you have a firm but smooth fit. Apply some cork-grease before you put the joints together.\n9.The tenons can not get all the way in This often happens to new instruments. The instrument was dried to it?s minimum humidity level. Afterwards the body was turned to its dimensions as soon as you start playing the humidity level will increase. This causes the wood to increase as well and the tenons will expand. The Metal rings on the receiver side will prevent, that the receiver can expand as well. You will notice that the joints get nicely together until about 4-5 mm before the end and then tighten up. That?s the area where the wood of the tenon is to thick. Take a thin strap of sandpaper or fold a piece of sandpaper to make it more stable and remove a layer of wood of the tenon just above the cork. Do it carefully. If it still binds to it again. At one point you will notice some progress but it will not be enough. You will now also notice, that some areas, where you sanded, the wood showed the typical sanding marks. Other will have a shiny surface. These are the areas which are still too thick. Continue to sand just these areas down until you have the fit you like.\n10. The cork is loose Lately many tenon corks on new instruments become loose. If you can remove the cork gently in one piece, do so. Clean the loose cork surface with alcohol or gasoline and apply a thin layer of contact cement on the wood and the cork. Waite for some minutes until it feels dry and then press it tightly back into place. If only one end is loose, clean only this bit on the cork and wood, apply the contact cement, wait and press it into place.']	['<urn:uuid:c59adf8a-dc8a-407b-a14d-40c71a64f34f>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T00:20:10.754925	24	43	941
97	I have allergies - what filters remove particles and can ozone help?	For allergies, HEPA filters are most effective as they eliminate at least 99.97% of dust particles sized 0.3 microns, including dust mites, mold, and pet hair. Activated carbon filters also help by removing odors and gaseous pollutants. Ozone is not recommended, as scientific evidence shows it's generally ineffective at safe concentrations for indoor air cleaning. Additionally, ozone exposure can be harmful - it can damage the lungs and cause symptoms like chest pain, coughing, shortness of breath, and throat irritation even at relatively low amounts.	['Table of Contents_\nSome consumers may confuse air filters with air purifiers because air filters often reside within air purifiers. However, to understand the difference between an air filter vs. an air purifier, you may need to understand the way each works because there are several types of air purifiers (including lots of top-rated air purifiers) just as there are a couple of types of air filters.\nSpeaking of different purification devices, you’ll want to learn about air purifiers vs vacuum purifiers, electrostatic air purifiers vs HEPA, and allergen removes vs air purifiers. In case you’re interested in other units.\nThe filtration methods vary between air filters and air purifiers unless you are comparing filters with a filter-based purifier. Both air cleaning technologies eliminate microscopic particles, including pet dander, dust particles, and mold spores. However, when using a filter, the device catches larger particles as the air passes through it. Meanwhile, air purifiers eliminate contaminants using electricity, ionization, or UV light. You may also be interested in the pros and cons of a diffuser vs. air purifier.\nMany air purifiers, like the finest air purifiers for pets, come outfitted with their air filters. However, some air filters are placed within a house’s ductwork. To find the best air purifier for your home, you can do a comparison of the various types of air purifiers. Although the air conditioners and furnaces come with their filters, some consumers have additional filters added. The main kinds of filters that are used include HEPA and activated carbon. Many consider HEPA filters the best option for cleaning airborne particulates, while activated carbon filters work well against household odors. Air purifiers work better than house plants when masking odors.\nHEPA filters will help with pollutants, while activated carbon ones will help with odors.\nHigh-efficiency particulate air (HEPA) filters eliminate at least 99.97% of larger dust particles size 0.3 microns in diameter. These filters can eliminate dust mites, mold, pet hair, or other organic compounds while the air flows through them. Some brands offer “HEPA-type” filters, which do not meet the industry standard requirements for an actual HEPA air filter.\nThe type of filter that works best on odors and gaseous pollutants features activated carbon as its material. These filters work well in eliminating odors from tobacco smoke, cigarette smoke cooking, and mold, among other indoor air pollution. Like HEPA filters, these filter larger particles from the air, but they do not target a specific size.\nSeveral types of air cleaners exist that improve indoor air quality. However, many of these produce ozone, even the ones that aren’t referred to as ozone generators. Electrostatic precipitators, ionizers, and ozone generators each feature unique processes. Electrostatic precipitators typically make up the best options within this group. Ionizers cause particles to stick to nearby surfaces, including clothes or your body. As the name implies, ozone generators produce toxic ozone, which is tolerable in small amounts. Just be sure to ventilate the room while using any of these options.\nThe primary type of electronic air purifier that removes airborne particulates using air filtrate that traps particles is an electrostatic precipitator, which should not be confused with the electrostatic filters for furnaces. Electrostatic precipitation occurs when the particulate matter passes through the air purifier and receives an electrical charge. The particles are then attracted to an oppositely charged collection plate and kept on the surface. This type of air purifier is cleanable, so it does not feature disposable filters. Some air purifiers utilize filters and electrostatic precipitation, resulting in fewer particles.\nSome air purifiers emit ozone, which can be toxic at higher levels.\nWhat do air purifiers filter?\nAir purifiers catch solid particulate matter or gaseous molecules. The type of air purifier affects what is eliminated, but some air purifiers provide both methods in one device.\nCan I use both an air purifier and humidifier together?\nYou may use both an air purifier and humidifier, especially if you are worried about viruses in the air. The air purifier should eradicate some of the larger particles containing the virus, while the humidifier may reduce the infectiousness of viruses if kept at the right level.\nDo air filters improve air quality?\nAir filters reduce the particulate matter in the air, which increases indoor air quality. However, they do need to be changed or cleaned every so often to remain effective. To check whether you have a washable or disposable filter, you should consult your user’s manual.\nSTAT: According to several studies (Lapidus et al., 2013), increasing the humidity in the air to a range of 40-60 percent reduces the infectiousness of viruses. (source)', '- Does ozone kill mildew?\n- Is it safe to clean CPAP with ozone?\n- What are the side effects of too much ozone?\n- Can ozone hurt you?\n- How much does ozone cleaning cost?\n- How do you get rid of ozone?\n- How is ozone exposure treated?\n- What are the symptoms of ozone exposure?\n- How does ozone kill mold?\n- Does ozone kill mold and bacteria?\n- Does ozone treatment really work?\n- What happens if you breathe in ozone?\n- Can you get ozone poisoning?\n- How long does ozone take to kill mold?\n- Is ozone treatment FDA approved?\n- Is ozone safe to breathe?\n- Can you smell ozone?\n- How long does it take for ozone to kill bacteria?\nDoes ozone kill mildew?\nAccording to the proponents of ozone generators the amazing benefits are so great that atomic oxygen will not only remove the musty odor associated with mold, but will kill the mold as well.\nEven low levels of ozone can damage the lungs and other parts of the respiratory system..\nIs it safe to clean CPAP with ozone?\nThe FDA reported that devices claiming to clean, disinfect or sanitize CPAP devices or accessories (such as masks, tubing, headgear) using ozone gas or UV light are not legally marketed for this use by the FDA in the U.S. According to the FDA, it is unknown if these CPAP cleaners are safe and effective.\nWhat are the side effects of too much ozone?\nPeople exposed to elevated levels of ozone may experience a variety of symptoms. The most common symptom is a feeling of irritation in the eyes, nose and throat. Some people may also experience respiratory or heart symptoms such as shortness of breath, chest pain, and wheezing.\nCan ozone hurt you?\nIf ozone is inhaled, it reacts with compounds found in the body and can cause harm to the lungs. If a person inhales a small amount of ozone, it can cause side effects, such as coughing, shortness of breath, chest pains, and irritation of the throat.\nHow much does ozone cleaning cost?\nThe cost for ozone treatment starts at $600, which generally covers about 1,000 square feet, Calamus said.\nHow do you get rid of ozone?\nAir cleaning refers to using methods like air filters and gas absorbing materials to remove pollutants out of the air. The use of air filters that include activated carbon is very effective at removing ozone from the air.\nHow is ozone exposure treated?\nNo specific treatment is available for individuals exposed to ozone, though some have suggested that oral intake of Vitamin E is beneficial to the chronic ambient exposure most experience.\nWhat are the symptoms of ozone exposure?\nWhat are ozone’s acute physiological and symptom effects?Cough.Throat irritation.Pain, burning, or discomfort in the chest when taking a deep breath.Chest tightness, wheezing, or shortness of breath.\nHow does ozone kill mold?\nProcess. High ozone shock treatment involves using an ozone generator with a timer to create lethal levels of ozone in an enclosed odour ridden or mold-affected room or building for a short period of time, between one and several hours.\nDoes ozone kill mold and bacteria?\nOzone kills bugs, mold, bacteria, plants, pets, and people, if any breathe it long enough. Ozone kills bugs, mold, bacteria, plants, pets, and people, if any breathe it long enough.\nDoes ozone treatment really work?\nAvailable scientific evidence shows that, at concentrations that do not exceed public health standards, ozone is generally ineffective in controlling indoor air pollution. The concentration of ozone would have to greatly exceed health standards to be effective in removing most indoor air contaminants.\nWhat happens if you breathe in ozone?\nWhen inhaled, ozone can damage the lungs. Relatively low amounts can cause chest pain, coughing, shortness of breath and throat irritation. Ozone may also worsen chronic respiratory diseases such as asthma and compromise the ability of the body to fight respiratory infections.\nCan you get ozone poisoning?\nWhether in its pure form or mixed with other chemicals, ozone can be harmful to health. When inhaled, ozone can damage the lungs. Relatively low amounts of ozone can cause chest pain, coughing, shortness of breath and, throat irritation.\nHow long does ozone take to kill mold?\nFor a while it was somewhat common to see mold remediation specifications call for flooding the work area with ozone for 24 to 48 hours following the gross removal and cleanup.\nIs ozone treatment FDA approved?\nOzone therapy. … In April 2016, the FDA prohibited all medical uses of ozone, “In any medical condition for which there is no proof of safety and effectiveness”, stating “Ozone is a toxic gas with no known useful medical application in specific, adjunctive, or preventive therapy.\nIs ozone safe to breathe?\nConclusions. Whether in its pure form or mixed with other chemicals, ozone can be harmful to health. When inhaled, ozone can damage the lungs. Relatively low amounts of ozone can cause chest pain, coughing, shortness of breath and, throat irritation.\nCan you smell ozone?\nWhat they’re really smelling is ozone. In fact, the word ozone is derived from the Greek verb ozein, which means “smell.” This is particularly fitting given the strong aroma of this chemical compound. Ozone has a distinctive smell that humans can detect even in small concentrations — as few as 10 parts per billion.\nHow long does it take for ozone to kill bacteria?\nOzone can effectively kill bacteria as well as Giardia, Cryptosporidium, parasites, fungi, molds and other microbes. The ozonation standard for killing most organisms is to maintain 0.3 ppm for a three-minute contact time. The dosage and contact time can vary.']	['<urn:uuid:2a703d1e-9855-4497-a8ed-cf7055c3057a>', '<urn:uuid:b5c3faa6-959e-4583-bb5a-198bfc117658>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T00:20:10.754925	12	85	1717
98	I'm researching crystals - how do they make pure strontium ruthenate?	High-quality crystals of strontium ruthenate are synthesized using a floating zone method in a controlled atmosphere with ruthenium as flux.	"['The unit cell of the layered perovskite structure of strontium ruthenate. Ruthenium ions are red, strontium ions are blue, and oxygen ions are green.\n|Except where noted otherwise, data are given for materials in their standard state (at 25 °C (77 °F), 100 kPa)|\nStrontium ruthenate (SRO) is an oxide of strontium and ruthenium with the chemical formula Sr2RuO4. It was the first reported perovskite superconductor that did not contain copper. Strontium ruthenate is structurally very similar to the high-temperature cuprate superconductors, and in particular, is almost identical to the lanthanum doped superconductor (La, Sr)2CuO4. However, the transition temperature for the superconducting phase transition is 0.93 K, which is much lower than the corresponding value for cuprates.\nHigh-quality crystals of strontium ruthenate are synthesized using a floating zone method in a controlled atmosphere with ruthenium as flux. The perovskite structure can be deduced based on powder x-ray diffraction measurements. Strontium ruthenate behaves as a conventional Fermi liquid at temperatures below 25 K.\nSuperconductivity in SRO was first observed by Yoshiteru Maenu and his group in 1994 when they were looking for high temperature superconductors with structures similar to the cuprates. Unlike the cuprates, SRO displays superconductivity even in the absence of doping. The superconducting order parameter in SRO has been shown to exhibit signatures of time-reversal symmetry breaking, and hence, it can be classified as an unconventional superconductor.\nSr2RuO4 is believed to be a fairly two-dimensional system, with superconductivity occurring primarily on the Ru-O plane. The electronic structure of Sr2RuO4 is characterized by three bands derived from the Ru t2g 4d orbitals, namely, α, β and γ bands, of which the first is hole-like while the other two are electron-like. Among them, the γ band arises mainly from the dxy orbital, while the α and β bands emerge from the hybridization of dxz and yz orbitals. Due to the two-dimensionality of Sr2RuO4, its Fermi surface consists of three nearly two-dimensional sheets with little dispersion along the crystalline c-axis.\nEarly proposals have suggested that superconductivity is dominant in the γ band. In particular, the order parameter in the momentum space exhibits k-dependence phase winding which is characteristic of time-reversal symmetry breaking. This peculiar single-band superconducting order is expected to give rise to appreciable spontaneous supercurrent at the edge of the sample. Such an effect is closely associated with the topology of the Hamiltonian describing Sr2RuO4 in the superconducting state, which is characterized by a nonzero Chern number. However, scanning probes have so far failed to detect expected time-reversal symmetry breaking fields generated by the supercurrent, off by orders of magnitude. This has led some to speculate that superconductivity arises dominantly from the α and β bands instead. Such a two-band superconductor, although having k-dependence phase winding in its order parameters on the two relevant bands, is topologically trivial with the two bands featuring opposite Chern numbers. Therefore it could possibly give a much reduced if not completely cancelled supercurrent at the edge.\n- Maeno, Yoshiteru; H. Hashimoto; et al (1994). ""Superconductivity in a layered perovskite without copper"". Nature 372 (6506): 532–534. Bibcode:1994Natur.372..532M. doi:10.1038/372532a0. Retrieved 16 April 2012.\n- Yanoff, Brian (2000). Temperature dependence of the penetration depth in the unconventional superconductor Sr2RuO4. University of Illinois at Urbana-Champaign.\n- Wooten, Rachel. ""Strontium Ruthenate"". University of Tennessee-Knoxville. Retrieved 16 April 2012.\n- Maenu, Yoshiteru; Maurice Rice; Manfred Sigrist (2001). ""The intriguing superconductivity of Strontium Ruthenate"". Physics Today 54 (1): 42. Bibcode:2001PhT....54a..42M. doi:10.1063/1.1349611. Retrieved 16 April 2012.\n- Kapitulnik, Aharon; Jing Xia; Elizabeth Schemm Alexander Palevski (May 2009). ""Polar Kerr effect as probe for time-reversal symmetry breaking in unconventional superconductors"". New Journal of Physics 11 (5): 055060. arXiv:0906.2845. Bibcode:2009NJPh...11e5060K. doi:10.1088/1367-2630/11/5/055060. Retrieved 17 April 2012.\n- Hicks, Clifford W.; et al. (2010). ""Limits on superconductivity-related magnetization in Sr2RuO4 and PrOs4Sb12 from scanning SQUID microscopy"". Physical Review B 81 (21). arXiv:1003.2189. Bibcode:2010PhRvB..81u4501H. doi:10.1103/PhysRevB.81.214501. Retrieved 22 December 2012.\n- Raghu, S.; Marini, Aharon; Pankratov, Steve; Rubio, Angel (2010). ""Hidden Quasi-One-Dimensional Superconductivity in Sr2RuO4"". Physical Review Letters 105 (13): 136401. arXiv:1003.3927. Bibcode:2010PhRvL.105b6401B. doi:10.1103/PhysRevLett.105.026401.']"	['<urn:uuid:bc5a9ed0-00e4-4ec2-9c18-c0586ec1ad2f>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T00:20:10.754925	11	20	665
99	best grow potatoes tomatoes polytunnel disease prevention seasonal timing certified seed	In a polytunnel, potatoes can be planted in early spring when soil warms up, yielding harvest around end of April. Tomatoes should be started in spring in pots or modules for summer growing. For disease prevention, it's crucial to use only certified seed potatoes and disease-free tomato transplants, never using tablestock potatoes as seed since they can carry harmful diseases. Additionally, practice crop rotation with non-solanaceous crops, and ensure proper ventilation by closing polytunnel doors before sunset and opening at first light.	['Spring arrives in the polytunnel four to six weeks earlier than it does outside, and so spring is when the benefits of tunnel growing will be most obvious to you. Outside things are struggling to get going, wind-swept and frost-burnt, but inside the tunnel it already feels like summer. As soon as the days lengthen noticeably, the tunnel will be up and running even though conditions outside might leave a lot to be desired: so make a start with early sowings and enjoy harvesting your autumn-sown plants, while your neighbours are still waiting for their soil to warm up.\nVentilation can increase as the days grow longer, but nights can still be very cold so make sure that the doors are closed at least an hour before sunset and are opened again at first light. Fleece (floating row cover) protection for tender plants can be removed, but replace it if heavy frost is forecast and take it off the next morning.\nWhen daylight hours begin to stretch in February, the soil in the polytunnel will warm up much earlier than the soil outside. As soon as you can leave your fingers in the soil comfortably (or for the scientifically-minded, as soon as the soil temperature at 8cm depth reaches 8C/46F) then plant some first early new potatoes. These will be ready around the end of April, providing a welcome new flavour in the ‘hungry gap’ and leaving the space clear for hot weather crops.\nOnce things really start warming up in the tunnel in March you can push on with sowing as you would outside, except four weeks earlier. Start to sow summer salad leaves now, such as lettuce, rocket, radishes, spinach, sorrel, spring onions and lambs lettuce, and also salad roots such as carrot and beetroot. Aim to sow salad crops in small patches every two to three weeks throughout the growing season for a succession of tender young leaves at their very best; you need never settle for limp salad again.\nLegumes such as peas and broad beans are well worth giving some space to since they will be ready at least a month before their outdoor relatives, as will early short varieties of sweetcorn and even parnethocarpic varieties of courgette (Zucchini), which need no pollination. These can be lifted in time to make space for winter planting, but make sure that you leave yourself room for your summer crops such as tomatoes and aubergines. These slots need not be empty, as you can use them for quick-maturing crops such as radish and pea-shoots – an extravagance if you buy your seed, but much less so if you save your seeds yourself – or the space can be occupied by a suitable green manure plant such as buckwheat, rye or phacelia. Tunnel soil should never be left empty for long during the growing season, as it dries out incredibly quickly and the vital soil life perishes. If you don’t want to use green manure, mulch the area with organic material such as straw or grass clippings.\nYou can also use the tunnel as you would a greenhouse, for sowing and bringing on seedlings for outside planting later in the season – except that with a polytunnel you have much more room than in a small greenhouse, so you can make much greater use of ‘sowing under glass’. Large seeds such as peas or pumpkins this can prove a great temptation to mice, so plant them in trays or modules and place them in a mouse-free area (such as a suspended shelf) until they are well established, or be prepared to suffer some losses.\nSome plants that will occupy the tunnel during the hottest part of the summer need to be planted in spring in pots or modules, such as tomatoes and tomatillos, chillis, melons and watermelons, cucumbers and aubergines. Getting as early a start as possible with these plants is vital if they are to set and ripen fruit properly in time to come out for winter planting, so if necessary get them started in a heated propagator or sunny windowsill, and give them as much light as you possibly can.\nKeep an eye on soil moisture levels and increase watering accordingly; as soon as the risk of hard frosts recedes reinstall any automatic watering systems that were stored for the winter. Spring clean your tunnel by washing all equipment and staging down, and clean the cover inside and out using a plant-safe organic detergent such as Citrox. Remove all weeds from the tunnel and check under low plants for slugs; combined with removing any visible slugs of any size shortly after dark once or twice a week will help to keep the tunnel relatively slug-free. Because polytunnels crop so intensively it is important that you support the living soil as much as possible, so top-dress any beds that were not done in autumn or winter with 7.5cm (3″) of compost or well-rotted manure. Feed all the beds with a dressing of fish, blood and bone meal or a similar all-purpose slow release organic fertiliser, and water it in well.\nGlobe artichokes, autumn-planted peas and broad beans, radishes, parsley, spinach, chard, salsify, early potatoes, endive, scorzonera, rhubarb, strawberries. Winter salad plants will start to grow again, then bolt as light levels increase. With regular cropping of leaves and pinching out of flowering stems, harvesting can be prolonged until new plantings become ready.', 'Susan Smith P. Ag., Ministry of Agriculture\nSolanaceous crops such as potatoes and tomatoes are hosts to late blight, the most destructive disease of potatoes in British Columbia. The pathogen causing late blight, Phytophthora infestans, thrives and produces spores under humid or moist environmental conditions. It causes infection only when free water is present on plants. Our wet weather conditions and cool to moderate temperatures make Lower Mainland BC an area where late blight can progress and spread very rapidly in Solanaceous crops grown both commercially and in home gardens.\nOn potatoes, symptoms of late blight first appear as large, soft, water-soaked spots with a light green halo on the leaves. Symptoms will also commonly develop on stems and leaf petioles which turn brown or black. In humid, wet conditions, a white, fuzzy mould can be seen on the underside of the leaves. As the disease progresses, the whole plant may die back to the ground. If late blight is present at harvest, tubers may become infected resulting in a firm brown rot that starts at the skin and eventually affects the whole tuber. Although symptoms appear slightly differently on tomatoes, the organism causing late blight is the same one seen in potatoes.\nIn addition to persisting in the soil and on infected plant tissues for a period of time, the late blight pathogen can cause infection through structures called sporangia during cool, wet, or humid weather. Sporangia can easily spread to distances of up to 20 km via water, wind, and wind-blown rain and cause infection in healthy Solanaceous plants. For this reason, when late blight is in an area, commercial potato growers are at risk of greater crop losses from disease outbreaks in the field and later when potatoes are in storage.\nPotatoes and tomatoes are popular crops for the home garden and community gardens throughout the Lower Mainland. Late blight is a disease of both potato and tomato that, if not managed for, can easily be spread from gardens to commercial potato fields in the area. This can also be a means of introducing new and more vigorous strains (genotypes) of the late blight pathogen. To minimize and hopefully prevent this from occurring, there are steps that gardeners should be taking to help reduce the pressure of harmful and costly po- tato diseases to local growers.\nIn 2014, the BC Ministry of Agriculture and Agriculture and Agri-Food Canada – through Growing Forward 2, a federal-provincial-territorial initiative – were pleased to fund an education and awareness program for home and community gardeners. The BC Potato and Vegetable Growers Association developed information materials and delivered outreach events to raise awareness of harmful plant pests of commonly grown solanaceous crops. In addition, management tips were provided to reduce the risks to neighbouring commercial potatoes. The brochure developed can be found on the BC Certified Seed Potato Growers website\nPrevention of Solanaceous diseases starts by planting disease-free tomato transplants and potato seed. Grow- ers are strongly encouraged to only plant certi ed seed potatoes and to never plant potatoes grown for tablestock or potatoes purchased for eating. This should be avoided because a potato tuber is a part of its mother plant and can carry its diseases and viruses. Certified seed potatoes (both the mother plant and its tubers) are inspected and tested to ensure that they do not exceed accepted levels of critical diseases. Non-certified potatoes have the potential to carry many economically harmful diseases – and when used as seed, can be the source of problematic viruses that could spread to nearby potato crops. Using certified seed potatoes is a good start for a successful disease-free crop.\nOther important practices for preventing and minimizing the severity of late blight in potatoes and tomatoes include:\n- Minimizing humidity by avoiding overhead irrigation\n- Growing short-season varieties during periods where weather conditions are less favourable to late blight\n- Preventively applying available, registered fungicides suitable for organic production (Note: always read and follow product label directions carefully)\n- Learning to identify the symptoms of late blight\n- Carrying out regular crop monitoring, from plant emergence to harvest, looking for late blight symptoms on the leaves, petioles, and stems of plants. Note: when disease is suspected, avoid spread of inoculum (i.e. spores) to healthy plants. Do not enter clean elds or planted areas with clothes, boots, gloves, pruning, tying equipment, vehicles, etc. that were used in planted areas infected with late blight\n- Removing infected plants and carrying out a thorough clean up of infected plant materials, unwanted tubers, fruits, and volunteer plants. Dispose by either burning or removing in sealed bags. Do not dispose of diseased plant materials in your compost.\n- Practicing crop rotation with non-solanaceous crops to minimize build-up of inoculum in the field.\nFor more information, please contact Susan Smith at the BC Ministry of Agriculture by email: Susan.L.Smith@ gov.bc.ca.\nSusan Smith, P. Ag., is Industry Specialist, Vegetables and Organics with the BC Ministry of Agriculture.']	['<urn:uuid:997dc9fc-12e4-4ab3-a53c-18f56a7adaa0>', '<urn:uuid:ee983639-9bc6-479d-881a-3e3284a832d5>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T00:20:10.754925	11	82	1732
100	blood agar dry steam sterilization comparison	Blood agar medium preparation requires high-pressure steam sterilization at 121°C and 15 psi for 15 minutes, while dry heat sterilization would require much higher temperatures (160-180°C) and longer time (2 hours) to achieve the same sterilization effect. Steam sterilization is preferred as it has stronger penetrating power and more reliable sterilization effect.	['The most widely used enriched medium for the growth of Fastidious organisms (those microbes that requires some extra nutrients and only grow in enriched medium and not on an ordinary medium are known as fastidious microbes) is one n only our Blood Agar Medium (BAM).\nThe Blood Agar Medium is the simplest yet powerful enriched medium that supports the growth of almost all the microbes and extremely easy to prepare in the Laboratory.\nIn this Article, I’m gonna explain the preparation of Blood agar medium from the Trypticase soy agar Base…..\nPRINCIPLE OF BLOOD AGAR MEDIUM\nBacteria are commonly cultured on a Blood agar medium to obtain the discrete colonies of the fastidious as well as non – Fastidious bacteria present in the specimen or to get the information about cultural characteristics of bacteria on a solid medium, colony morphology and patterns of growth and especially the Hemolytic property of the bacterium etc. This Solid enriched medium, used in bacteriology laboratory constitutes the 7 essential components which are described below –\n⇒ Pancreatic digest of casein – The Pancreatic Digest of Casein is a rich source of Nutrients, Vitamins, and Nitrogenous substances that support the rapid growth of the bacterial cell.\n⇒ Papaic Digest of Soybean – Similar to Pancreatic Digest of Casein, the Papaic Digest of Soybean is the Rich source of protein or peptides and nutrients that support the rapid growth of the bacterial cell.\n⇒ Dextrose – The Dextrose present in the Trypticase Soy Agar (TSA) Medium provides a rich source of carbohydrate for the rapid growth of the Bacterial Cell.\n⇒ Dipotassium hydrogen phosphate – It acts as the buffer in the medium that helps to maintain the stable pH of the Trypticase Soy Agar (TSA) medium.\n⇒ Sodium Chloride – It maintains the osmotic pressure in the agar medium so that the movement of molecules takes place in and out of the bacterial cell. It must be present in right proportion otherwise it will lead to the lysis of the bacterial cell.\n⇒ Blood – Blood is the key component of BA medium which makes the medium more nutritious by providing additional nutrients and growth factors required by fastidious organisms for the optimum growth. Moreover, it also helps in visualizing the hemolytic reactions (if any). Usually, 5% Blood Agar medium is used in laboratories for routine culture.\n⇒ Agar-Agar – It is often called as Agar, is a complex polysaccharide, a carbohydrate consisting of 3, 6-Anhydro-L-galactose and D-galactopyranose, free of nitrogen, produced from various red-purple algae belonging to Gelidium, Gracilaria, Gigastina etc. It liquefies on heating to 96 °C and hardens into a jelly on cooling at 40-45 °C.\nThe final pH of the solutions is adjusted to 7.2 – 7.6 preferably the 7.4 at 25 °C. The above-mentioned components can be modified by adding the various substances in a variety of ways as per the results required so that the rapid and satisfactory growth of the bacterial cell takes place.\nThe above-mentioned components can be modified by adding the various substances in a variety of ways as per the requirements of the bacterium so that the rapid and satisfactory growth of the bacterial cell takes place.\nREQUIREMENTS FOR PREPARING BLOOD AGAR MEDIUM (BAM)\n- Sterile Conical Flask / Erlenmeyer Flask\n- Pancreatic digest of casein\n- Papaic digest of soybean meal\n- Dextrose (Glucose)\n- Dipotassium hydrogen phosphate\n- Sodium Chloride\n- Sheep’s Blood\n- Measuring Cylinder\n- 1N HCl\n- 1N NaOH\n- pH Strip\n- Weighing Scale\n- Distilled Water\n- Butter Paper\nPROCEDURE FOR THE PREPARATION OF BLOOD AGAR MEDIUM USING TRYPTICASE SOY AGAR BASE\n⇒ Weigh the quantity of Peptone, Beef Extract, and Sodium Chloride using the weighing scale for 1000 ml of Trypticase Soy Agar medium as follows:\nCOMPONENTS QUANTITY (in grams)\nPancreatic digest of casein 17.0\nPapaic digest of soybean meal 3.0\nDextrose (Glucose) 2.5\nDipotassium hydrogen phosphate 2.5\nSodium Chloride 5.0\nPut the butter paper on the weighing scale and transfer the required quantity of Pancreatic digest of casein on to the paper using the spatula. Repeat the step to obtain the required quantity of Papaic digest of soybean meal, dextrose, Dipotassium Hydrogen Phosphate, sodium chloride, and Agar-Agar powder.\nNote: Use the separate piece of butter paper to avoid the errors in measurements.\nAlternatively, You can use the commercially available Trypticase Soy Agar (TSA) media powders and simply weigh the mixture of content as prescribed on the carton or box of the manufacturer. As per HiMedia, its 45 grams for 1000 ml medium.\n⇒ Take a clean and dry Conical Flask/ Erlenmeyer flask.\nNote: The size of the flask should be at least 1.5 times larger than the quantity of media you are preparing, for e.g. use 1500 or 2000 ml flask to prepare 1000ml of the solid medium.\n⇒ Pour 500 ml of distilled water to the flask and add the weighed quantity of Pancreatic digest of casein, Papaic digest of soybean meal, dextrose, Dipotassium Hydrogen Phosphate, and sodium chloride.\n⇒ Now add the weighed quantity of Agar-Agar to the above solution.\n⇒ Mix well the content and Heat it with continuous agitation to dissolve the constituents.\n⇒ Now add more distilled water to the medium and make the volume 950 ml.\nNOTE: At this point, you have to make the volume 950 ml not 1000ml as later 50 ml of blood will be added later which will make the total volume 1000 ml.\n⇒ Check the pH of the solution using pH strip, it should be 7.4 ± 0.2. If required, adjust the pH by adding either 1N HCl (acid) or 1N NaOH (base) as per the case.\n⇒ Mix well the content and apply the Non-absorbent cotton plug to the flask.\n⇒ Autoclave the content at 121 °C and 15 psi pressure for 15 minutes.\n⇒ Allow the content to cool down to 45 – 50 °C and to this carefully add 50 ml of sterile defibrinated blood aseptically and mix well gently.\nNOTE: Avoid formation of air bubbles in the medium while adding blood. Also, before adding the blood to the agar medium, warm it to the room temperature.\nNow, Pour about 15 ml of this medium in the sterile Petri plates under the strict aseptic atmosphere (preferably in Laminar Air Flow) and allowed it to cool at room temperature. Store the prepared media plates in the refrigerator at 4 °C.\n⇒ Use the prepared media plates to inoculate the specimen to be cultured and then place it in the incubator at optimum temperature.\nGROWTH OF VARIOUS BACTERIA ON BLOOD AGAR MEDIUM\nUSES OF BLOOD AGAR MEDIUM\nThe Blood Agar medium is commonly used for the isolation and cultivation of Streptococcus species, Neisseria species, and other fastidious microorganisms.\nThe BA medium is also used to differentiate bacteria on the basis of their hemolytic property i.e. β-hemolysis, α-hemolysis or γ-hemolysis (a.k.a. non-hemolytic).', 'Views:25 Author:Site Editor Publish Time: 2020-09-16 Origin:Site\nGenerally speaking, sterilization is mainly divided into moist heat sterilization and dry heat sterilization. We will analyze these two methods today.\nAt the same temperature, warm heat has a better sterilization effect than dry heat. The reasons are: ①The temperature required for protein coagulation is related to its water content. The greater the water content, the lower the temperature required for coagulation. The moist heat sterilized bacterial protein absorbs water, and it is easy to solidify in the hot dry air at the same temperature. ②The steam emits a lot of latent heat during the warm sterilization process, which accelerates the increase of humidity. Therefore, the temperature required for moist heat sterilization is lower than that of dry heat. If the temperature is the same, the time required for moist heat sterilization is shorter than that of dry heat. ③The penetrating power of damp heat is greater than that of dry heat, so that the deep part can reach the sterilization temperature, so damp heat is more effective than dry heat.\nMoist heat sterilization methods include:\n(1) Boiling method: Boiling at 100°C for 5 minutes can kill the propagule of common bacteria. Many spores need to be boiled for 5-6 hours before they die. Adding 2% sodium carbonate to water can increase its boiling point to 105°C. It can not only promote the killing of spores, but also prevent metal utensils from rusting. The boiling method can be used to disinfect drinking water and general equipment (knives, scissors, syringes, etc.).\n(2) Circulating steam sterilization method: Use steam at about 100°C for disinfection. Generally, a circulating steam sterilizer (the principle of which is equivalent to a steamer in our country) is used. Heating for 15 to 39 minutes can kill bacterial propagation. The packaging of sterilized items should not be too large or too tight to facilitate steam penetration.\n(3) Intermittent sterilization method: the use of repeated circulating steam to achieve the purpose of sterilization. Generally, using a steam sterilizer, heating at 100°C for 15-30 minutes, can kill the propagules; but there are still spores remaining. After taking it out, put it in a 37°C incubator overnight to allow the spores to develop into propagules, and then steam it again the next day for more than three consecutive times. This method is suitable for the sterilization of nutrients that are not resistant to high temperature (such as serum culture medium).\n(4) Pasteurization: a method of using heat to kill pathogenic bacteria or general miscellaneous bacteria in the liquid without seriously damaging its quality. Created by Pasteur to disinfect alcohol, hence the name. Heat 61.1～62.8℃ for half an hour, or 71.7℃ for 15～30 seconds. It is often used to sterilize milk and alcohol.\n(5) High-pressure steam sterilization: Pressure steam sterilization is carried out in a special pressure steam sterilizer, which is the most commonly used and most reliable method in heat sterilization. Its advantages are strong penetrating power, reliable sterilization effect, and can kill all microorganisms.\nThe pressure sterilizers currently in use can be divided into two categories: lower exhaust pressure sterilizers and pre-vacuum pressure sterilizers. It is suitable for the sterilization of high temperature and water resistant items.\nDry heat sterilization requires higher temperature and longer time than moist heat sterilization.\n(1) Dry roasting: Use a dry oven to heat at 160-180℃ for 2 hours to kill all microorganisms, including spores. Mainly used for the sterilization of glassware and porcelain.\n(2) Scorching and incineration: Scorching is the direct use of flame to kill microorganisms, which is suitable for sterilization of heat-resistant metal equipment such as inoculation needles in microbiological laboratories. Incineration is a thorough disinfection method, but it is limited to the disposal of waste contaminated items, such as useless clothing, paper, garbage, etc. Incineration should be carried out in a dedicated incinerator.\n(3) Infrared radiation: Infrared radiation is an electromagnetic wave with a wavelength of 0.77 to 1000 microns, which has a good thermal effect, especially the thermal effect with a wavelength of 1 to 10 microns. It is also considered a dry heat sterilization. Infrared rays are produced by infrared light bulbs and do not need to be conducted by air, so the heating speed is fast, but the thermal effect can only be produced on the irradiated surface, so the front, back, left and right of an object cannot be uniformly heated. The sterilization effect of infrared rays is similar to that of dry heat, and the temperature and time required for sterilization in an infrared oven are also the same as those for dry baking. It is mostly used for the sterilization of medical devices.\nPeople who are exposed to infrared rays for a long time will feel eye fatigue and headache; long-term exposure will cause eye damage. Therefore, workers should at least wear protective glasses that can prevent infrared rays.\n(4) Microwave: Microwave is an electromagnetic wave with a wavelength of 1 mm to 1 meter. It has a high frequency and can penetrate glass, plastic film, ceramics and other materials, but cannot penetrate metal surfaces. Microwave can cause the disordered polar molecules in the medium to move back and forth according to the frequency of the wave under the action of the microwave field, collide and rub each other to generate heat, and the temperature of the medium can rise accordingly, so it can be played at a lower temperature. To disinfection. It is generally believed that in addition to thermal effects, its sterilization mechanism also has electromagnetic resonance effects and field dedication effects. There are two kinds of microwaves commonly used in disinfection: 2450MHZ and 915MHZ. Microwave irradiation is mostly used in food processing. In the hospital, it can be used to sterilize laboratory supplies, non-metallic equipment, food utensils, medicine cups and other supplies in sterile wards.\nLong-term microwave irradiation can cause systemic reactions such as opacity of the eyes, testicular damage and neurological disorders, so the door must be closed before operation.']	['<urn:uuid:5f1fb592-19c5-46c7-a908-831cbe6813d9>', '<urn:uuid:b9b9e5a5-0ef6-4408-b305-91520ac47f28>']	factoid	direct	short-search-query	distant-from-document	comparison	expert	2025-05-13T00:20:10.754925	6	52	2144
